- en: Deploying Applications on the Cloud with OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we showed you how to develop microservices with WildFly
    Swarm. In this chapter, you will learn how to deploy those services to the cloud,
    and you will use OpenShift to achieve that. However, why should we bother? What
    are the features and benefits of cloud computing? Let's start by saying a few
    words about it.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the next section, you need to be provided with some important
    information. This chapter describes a lot of theoretical concepts that explain
    the internals of OpenShift. Don't worry if some of those concepts sound too complex
    to configure because (spoiler alert!), in the end, OpenShift will do most of the
    stuff for you. The goal of this chapter is to provide you with knowledge, which
    will allow you to understand all the magic that OpenShift can do, and. In later
    chapters, modify, and reconfigure that behavior. So, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OK. Let's start at the beginning. So, what actually is cloud computing?
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing is an IT paradigm that advocates the use of shared pools of
    configurable resources and services provided over the internet. Those services
    are provided on demand, rapidly, and with minimal management. As a result, cloud
    computing allows for flexible architecture, optimized resource usage, and the
    possibility of decoupling the infrastructure provider from a consumer, enabling
    the separation of concerns. Let's examine those statements in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Resources provisioned on demand give you, as a developer or an architect, flexibility
    in configuring your technical infrastructure. Starting the project is cheap, as
    you don't have to begin with infrastructure investments. Also, when your project
    is in production, computing resources may be scaled automatically to fill your
    application's demands. In each case, you are paying only for the resources used.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, cloud computing introduces the separation of concerns. The cloud
    provider becomes specialized in provisioning the infrastructure. The developers
    are provided with the interface of the cloud infrastructure and will use it for
    the development. As a result, they don't need to be concerned with the details
    of the infrastructure configuration as long as they comply with the interface
    provided by the cloud. Cloud providers, on the other hand, have to compete to
    provide the most convenient and robust cloud infrastructure while minimizing the
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will describe the cloud interface provided by OpenShift.
    Before that, we will describe the following two cloud infrastructure characteristics,
    which will enable the use of a consistent set of concepts during the following
    chapters: deployment and service models. Let''s start with the first one.'
  prefs: []
  type: TYPE_NORMAL
- en: The cloud infrastructure deployment model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cloud deployment model specifies how cloud infrastructure is organized.
    Let's look at commonly used models.
  prefs: []
  type: TYPE_NORMAL
- en: The public cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **public cloud** is a model in which services are provided over a network
    for public usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Public cloud computing is sometimes described as an evolution of computing
    resources into utility such as electricity. Using the example of a power provider
    may give you an alternative insight into the topics described in this paragraph:
    the power distribution company is responsible, among other, for ensuring that
    the network can deal with demand peaks, or provide network redundancy in case
    of traction breakdown. However, in the end, we don''t think about it too much—we
    insert the plug into the socket when needed and get billed only for electricity
    we used.'
  prefs: []
  type: TYPE_NORMAL
- en: We shouldn't stretch that analogy too far though. Computing resources are obviously
    not a commodity like electricity, and they cannot be provided to everyone in a
    uniform manner because of different customer requirements.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, not all customers are able, for various reasons, to move their
    workloads to the public cloud. One of the reasons may be the need for a higher
    level of security or greater control over the architecture. Such customers can
    take advantage of the private cloud discuss it.
  prefs: []
  type: TYPE_NORMAL
- en: The private cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the **private cloud**, the computing resources used to create the cloud are
    dedicated to a single customer. The resources may be located on-site or off-site,
    owned by the customer, or managed by a third party. All of the resources are provisioned
    for a single client and are not shared by multiple clients. Such a configuration
    allows for greater control and security but requires the company to invest in
    and manage the cloud resources, removing the decoupling benefit of the public
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **hybrid cloud** is a model in which public and private clouds are connected
    together. As a result, you can take advantage of both solutions. For example,
    you can run services that deal with sensitive data on the private cloud, but use
    the public cloud to scale other services.
  prefs: []
  type: TYPE_NORMAL
- en: We would like our cloud provider to provide cloud abstraction that will enable
    users to see a homogeneous cloud view, abstracting away the infrastructure details
    of the hybrid cloud. We will return to this thought later, when we describe the
    architecture of OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now focus on another cloud characteristic, that is, the **service model**.
  prefs: []
  type: TYPE_NORMAL
- en: The service model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you learned at the beginning of this chapter, the cloud provider is responsible
    for providing computing resources on demand. Computing resources can be provisioned
    in different ways, and characterized by different levels of abstraction. This
    feature of cloud computing infrastructure is called a service model. Let's describe
    the commonly used models.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as a Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In **Infrastructure as a Service** (**IaaS**), a customer is able to install
    arbitrary applications (including operating systems) on the provisioned resources.
    The customer does not control the infrastructure that is provided by the cloud
    provider. For example, the customer may get access to a remote virtual machine,
    which they can fully operate.
  prefs: []
  type: TYPE_NORMAL
- en: Platform as a Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In **Platform as a Service** (**PaaS**), as its name suggests, the cloud provider
    is responsible for providing the customer with a ready-for-use the platform on
    which the customer can deploy and run their applications. Let's suppose that you
    want to use the database. In this case, the platform provider is responsible for
    giving you access to an up-to-date and configured database on which you can start
    working immediately. You don't have to mess with the entire configuration, and
    can start working with the database (or any other technology; WildFly Swarm, for
    example) straightaway.
  prefs: []
  type: TYPE_NORMAL
- en: Software as a Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, there is **Software as a Service** (**SaaS**). In this model, the customer
    is able to use the application provided by the cloud provider. An example of SaaS
    may be disc storage, email, or an office suite provided over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: OK, now that we've clarified the nomenclature, we can finally dig into OpenShift
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenShift architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have been talking in abstract terms. In this section, we will give
    you an overview of the OpenShift architecture. As a result, you will gain a practical
    understanding of the functioning cloud PaaS infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a bird''s-eye view architecture diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a028067-a462-464b-b063-d3b65ab55eb6.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram sketches the layers of OpenShift architecture. **Docker**
    runs on top of the operating system and provides layers of containers. Containers
    are lightweight, standalone, and executable pieces of software (*Further reading*,
    link 2) that can be run anywhere on the cloud. Those containers are orchestrated
    by **Kubernetes**, which provides a unified view of heterogeneous computing resources.
    Finally, **OpenShift** builds on top of **Kubernetes**, providing developer tools
    that automate most configuration tasks. If this short description sounded cryptic
    to you, don't worry. Everything will be clear by the end of this chapter. Let's
    start with containers.
  prefs: []
  type: TYPE_NORMAL
- en: Containerization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said before, in cloud computing, the cloud provider is responsible for
    providing the server resources to users on demand. However, what does that actually
    mean? How are those resources provided? How is the server with all the necessary
    configurations created, and how it is isolated from other users? To understand
    this, we must understand how containers work.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are basically a type of **virtualization**. Let's discuss this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtualization is a technology that allows the running of many isolated virtual
    machines on one server. A virtual machine is an emulation of a computer system
    governed by the virtualization application running on the server's operating system.
    The user to whom the virtual machine has been provisioned has access to the fully
    operational host. The fact that the host is not physical, and that it shares the
    resources of the physical server with other virtual machines, is being abstracted
    away from the user.
  prefs: []
  type: TYPE_NORMAL
- en: Another key feature of virtualization is the ability to serialize the virtual
    machine into the image. This feature enables portability. The image can be moved
    to different servers, thus ensuring that the state of the virtual machine will
    be preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Why is virtualization important to the cloud provider, then?
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the cloud provider is able to provide access to the virtual machine
    to the user. As a result, the user will obtain access to a fraction of server
    resources. From the user's perspective, they will have access to the isolated
    server
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the user has the ability to use a preconfigured image of the platform
    that they want to use. Do you need Fedora with WildFly AS? Here is your configured
    image. We will run it on our server, and you are ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, the cloud provider is able to optimize the resource usage. There can
    be many virtual machines running on one server, minimizing the idle time.
  prefs: []
  type: TYPE_NORMAL
- en: Fourthly, Virtual machines can be moved freely between different servers when
    necessary. If more resources are needed, some virtual machines can be transferred
    to another server. Again, from the user's point of view, you will have access
    to an isolated preconfigured server and won't have to worry about the details.
  prefs: []
  type: TYPE_NORMAL
- en: Then, what about the implementation?
  prefs: []
  type: TYPE_NORMAL
- en: Some of you may identify virtualization with hardware (of full) virtualization.
    In this architecture, the virtualization application is responsible for emulating
    the whole operating system with all the necessary processes and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: This solution has some performance problems, most of them resulting from the
    fact that the operating system was initially designed to run on a physical host.
    Firstly, to start the virtual machine the whole OS has to be started and, as a
    result, the startup time may be substantial (minutes). Secondly, OS processes
    and libraries have to be duplicated in each virtual machine, which leads to non-optimal
    resources usage.
  prefs: []
  type: TYPE_NORMAL
- en: Let's think about it from the cloud provider's perspective, taking special consideration
    of the microservices architecture that we described in [Chapter 1](77a69da4-3aff-46c3-aa00-fa4d02db5061.xhtml),
    *Java EE and Modern Architectural Methodologies*. We would like to have a solution
    that will enable us to provide a large number of ephemeral virtual machines. We
    would like to ensure that they can be started and stopped immediately, optimizing
    the use of resources, and store image data effectively. It turns out that we need
    another tool. Let's discuss containers.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers are the implementation of a system-level virtualization (or paravirtualization).
    In this kind of virtualization, the operating system is not emulated on each virtual
    machine. Instead, virtual machines share the same operating system instance, using
    the tools provided by it to achieve isolation. As a result, in this model, we
    can think of virtual machines as isolated user-space instances running on top
    of the same operating system. Such instances are called container.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d290ce5b-8edd-49c8-b9ce-f9e85d271b98.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram highlights the main differences between full virtualization
    and containers. Containers share the same operating system and, as you will learn
    in the next sections, are able to share common libraries effectively. In the preceding
    diagram, the horizontal line represents the layers that have to be created when
    the new virtual machine/container is created.
  prefs: []
  type: TYPE_NORMAL
- en: Before we describe the Docker implementation of containers, let's talk about
    the isolation tools that the Linux kernel provides.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel isolation tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Linux kernel provides a wide array of tools that enable the isolation of
    processes, and put a resource usage limit on those groups. The main tools that
    enable that (and are used by Docker containers) are namespaces (isolation), and
    `cgroups` (limits). Let's learn more about it.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each kernel process can be assigned to a namespace—processes with the same namespace
    share the same view some of the system resources. For example, a PID namespace
    provides the ability to isolate processes—processes in the same PID namespace
    can see each other, but cannot see processes from different namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: There is a group of namespaces in Linux kernel that provides PID, network, mount
    point, and username isolation.
  prefs: []
  type: TYPE_NORMAL
- en: cgroups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `cgroups` are responsible for limiting the resource usage for the group
    of processes. `cgroups` allow you to assign processes to a number of groups and
    configure a resource quota for those groups. The resources that can be controlled
    are, among others, CPU, memory usage, network, and disk bandwidths. In the event
    of resource congestion, the `cgroups` mechanism will make sure that the group
    won't exceed its quota for that resource.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of containers, a group may be created for each container. As a result,
    we would be able to provide a quota for all of the containers. We may, for example,
    assign 1/5 of the CPU to one of the containers. This would guarantee that, in
    the case of a congestion, this container would have access to that amount of the
    CPU cycles. As a result, we are able to guarantee resource access. The `cgroups`
    limit is only enforced during the congestion. In our example, if all other containers
    are idle, the container that was assigned a fraction of CPU quota may use more
    CPU cycles.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker implementation of containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have just learned that the Linux kernel provides tools that enable isolation
    of resources, laying the ground for the following the implementation of system-level
    virtualization. But we have to ask ourselves some questions: which applications
    will be run inside the container? Which libraries and files will be visible in
    individual containers?'
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, we will introduce you to Docker images. As we have
    hinted at before, virtual machines in hardware virtualization can be stored as
    images, which make it possible to store the state of the virtual machine and,
    as a result, allow for the creation of reusable preconfigured virtual machines
    and portability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same feature concerns containers and the implementation that we will use:
    Docker. As you will learn in the following sections, this idea has been developed
    to a whole different level, providing us with an efficient and convenient image
    ecosystem and, as a result it, provides a base environment for our cloud infrastructure.
    However, let''s start at the beginning.'
  prefs: []
  type: TYPE_NORMAL
- en: Images and containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Docker nomenclature, there is a distinction between an image and a container.
    An image is an immutable, unambiguously identifiable collection of files and metadata.
    A container, on the other hand, is a runtime instance of an image. There can be
    many container instances of the same image, each of which is mutable and has its
    own state.
  prefs: []
  type: TYPE_NORMAL
- en: Let's make it clear it with the help of an example. You can start a Fedora distribution
    in a container. To do so, you have to download and build a Fedora image. After
    the build, the image will be located on your machine and will contain the Fedora
    distribution. As we mentioned in the preceding paragraph, this image is an immutable
    template that can be used to start a container. When you start the container based
    on the Fedora image and log in to it, you will see that you have access to the
    bare Fedora distribution. You can, among other things, install software and create
    files there. When you do this, you modify only that specific container. If you
    run another container from the same Fedora image, you will again have access to
    the bare Fedora distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example gives you a bird's-eye view of the behavior of containers
    and images. Let's now take a closer look at the architecture of both.
  prefs: []
  type: TYPE_NORMAL
- en: Images are described by `Dockerfiles`. A `Dockerfile` is a text file, which
    contains a list of commands that instruct you on how to assemble an image.
  prefs: []
  type: TYPE_NORMAL
- en: Images have a layered structure. The commands executed in a `Dockerfile` result
    in the creation of additional layers—every subsequent layer is different from
    the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: Every image must derive from another image (possibly, an explicitly empty *scratch*
    image), and it adds its layers on top of it. Image layers are built directly on
    top of the kernel code.
  prefs: []
  type: TYPE_NORMAL
- en: Let's clarify all those concepts by taking a look at a practical example. We
    will create a bunch of simple images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we will create a number of files in the local directory from which
    we will build the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4495867f-c8f7-4ee7-b602-fa939e73ef28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create the first image now (note that we are aiming at architecture
    description; so, although we will briefly explain the commands used, refer to
    (Further reading, link 1) if you are interested in the details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `Dockerfile` represents the base image. It derives from the `centos:7`
    image, which is a bare Centos distribution, adds the `tomek` user (`#2`), and
    switches the previous user to `tomek` so that all subsequent commands will be
    run as this user (`#3`), therefore the directory from any further commands will
    be executed to `tomek`'s `homedir`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build an image from `Dockerfile`, we have to execute the following
    command in the directory in which the image is located:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, we have tagged the image as base. As a result, we
    are able to refer to it by its base name. Let''s move on to the second Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding image derives from the base image created before that. It copies
    directories `A` (`#2`) and `B` (`#3`) from our local filesystem to the image.
    Similarly, let''s build and tag the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, take a look at the last image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It derives from the middle image (`#1`) and copies the `C` directory (`#2`)
    from the local filesystem to the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram presents the layers of the top image. We have included
    all the commands from the image hierarchy so that you can take a look at how the
    top image is assembled from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34b0d337-fa20-4cf8-bf9c-b461bb638d3f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you will notice in the preceding diagram, each command from all the `Dockerfiles`
    is translated into an additional layer. The base image, for example, derives from
    the `centos:7` images and, as a result, its first layer is added on top of `centos`
    layers. Similarly, the top image is based on the middle image, and as a result,
    the layer resulting from the execution of the `COPY C` command is added on top
    of layers from the middle image.
  prefs: []
  type: TYPE_NORMAL
- en: We now understand how `Dockerfiles` translate to the image layer structure.
    However, why is this structure important? It is important because it enables images
    to share the layers. Let's find out how.
  prefs: []
  type: TYPE_NORMAL
- en: When we build the image, the commands from the `Dockerfile` will be executed
    and all the layers will be created. As you saw in the preceding example, images
    are connected to each other and can share layers. A layer that is used by multiple
    images has to be created only once.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the middle and top images share all the layers from
    the middle image. If we decide to build the middle image, all its layers will
    also be created. If we later build the top image, none of the middle layers have
    to be created again.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why such sharing of layers is possible, we must understand how
    the layers influence filesystem behavior after the container is started.
  prefs: []
  type: TYPE_NORMAL
- en: When we start the container based on an image, we are inserting another layer
    on top of an image from which the container was started. The container will write
    its changes to this layer, but the result won't change the image, which is immutable.
    To understand why this is the case, we must understand how layers influence filesystem
    behavior at runtime. Let's find out.
  prefs: []
  type: TYPE_NORMAL
- en: When Docker starts a container, it creates a filesystem that will be mounted
    as the root filesystem of the container. Such a filesystem overlays all the layers
    from the image and the writable layer, creating the filesystem that appears to
    combine the files and directories from all the layers that constitute the image.
    Let's return to our example to show how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create three containers based on the images from our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b04ba0e-cf32-43ce-8d89-99676233e13e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the top rectangle represents the images that we have
    created. The middle image has added two layers corresponding to directories `A`
    and `B`. The top image has added one layer with the `C` directory. There are three
    containers. Two containers (**2** and **3**) are based on the top image, and **CONTAINER1**
    is based on the middle image. The rectangles at the bottom represent each container's
    writable layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since all layers have been combined into one filesystem, all containers will
    see the whole operating system distribution. The content of the home directory
    will be different for **CONTAINER1**, **CONTAINER2**, and **CONTAINER3**: **CONTAINER1**
    will see only `A` and `B` directories in its `home` folder, whereas **CONTAINER2**
    and **CONTAINER3** will see `A`, `B`, and `C` directories.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's explain how this overlay is implemented. When a container reads a file,
    the storage driver (a component responsible for the union filesystem implementation)
    looks for that file starting from the top layer. If the file is not found, it
    moves to the layer below it. The cycle repeats till either the file is found or
    there are no more layers. If a file is not found in any of the layers, we will
    get a `File not found` error.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that **CONTAINER1** wants to read a `~/C/c.txt` file. The storage
    driver starts searching in **CONTAINER3**'s writable layer. Since the file is
    not there, it moves to the `COPY C` layer from the top image. The file is found
    there and is read.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if **CONTAINER1** wants to read the same file?
  prefs: []
  type: TYPE_NORMAL
- en: The storage driver starts from **CONTAINER1**'s writable layer. Again, it cannot
    find the file, but this time it moves to the `COPY B` layer, which is the top
    layer of the middle image from which the container was created. The file cannot
    be found there, nor in any layers below it. We will end up with a `File not found`
    a message.
  prefs: []
  type: TYPE_NORMAL
- en: What if **CONTAINER1** and **CONTAINER2** want to read `~/B/b.txt`?
  prefs: []
  type: TYPE_NORMAL
- en: After reading the supposition in the preceding paragraph, you will know that
    both files can be read. Note, however, that both containers were reading the same
    file. The `"COPY B"` layer is reused by the middle and top images, and the `b.txt`
    file reads from the same image for both containers. Thanks to layers, containers
    are able to reuse data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what about writing to a file?
  prefs: []
  type: TYPE_NORMAL
- en: The storage controller uses a copy-on-write strategy when writing a file to
    a filesystem. The driver looks for a file in all the layers, again from top to
    bottom. If the file is present in the container's writable layer, it can be directly
    open for write. If it is present in one of the image's layers, it is copied to
    a writable layer and opened for write.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s return to our example. Let''s suppose that **CONTAINER1** wants to write
    to the `~/A/a.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32e39a1d-0bd7-4f77-b630-ecb5c1eac610.png)'
  prefs: []
  type: TYPE_IMG
- en: The storage driver has found the `~/A/A.txt` file in **COPY A** layer and copied
    it to the writable layer of **CONTAINER1**. Subsequent reads and writes of the
    `~/A/A.txt` file from **CONTAINER1** will read/write the file from **CONTAINER1**'s
    writable layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that **CONTAINER3** wants to write to the `~/A/A.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd7edaf7-abb0-4548-8adb-59f579baa3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: The situation is analogous; the file is copied to **CONTAINER3**'s writable
    layer. Let's look at the current situation. Each container has had access to the
    Fedora distribution and modified some of its parts.
  prefs: []
  type: TYPE_NORMAL
- en: '**CONTAINER1** and **CONTAINER3** still share most of the data, as only the
    files modified by the given container are copied to its writable layer.'
  prefs: []
  type: TYPE_NORMAL
- en: As you will noticed in the preceding diagram, the implementation of Docker images
    provides an effective way of storing multiple images on the same host. Then, what
    about container startup time? The container uses the resources of the underlying
    Linux kernel and reuses the image layers (provided that they are present—if not,
    they are downloaded only once). Because of that, starting the container means
    creating a writable layer and running the container processes using Kernel isolation
    features. As you are able to see, such processes are very lightweight compared
    to the hardware virtualization. As a result, containers can be started and stopped
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: By this point, we have described the implications of Docker image architecture
    on container virtualization performance. You may have doubts regarding the performance
    of such an implementation in other circumstances. Surely, a layered filesystem
    would have a performance penalty if we decided to run a database on it. That's
    a good point that has to be clarified. A Docker-layered filesystem is used to
    work effectively with Docker containers. The layered filesystem is not meant to
    store data that requires high performance; this is the role of volumes, which
    we will learn about in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There may be different implementations of a storage driver. For example, copy
    on write strategy may be implemented on the file or page-cache level. Docker provides
    a number of implementations, and choosing the correct one depends on your use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in the architecture of specific storage drivers, or you
    are researching which driver is best for your use case, refer to the Docker documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Docker registries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s return to the build process of our example container. When we were building
    the base image for the first time, the following could be seen in the build log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As it turns out, the `fedora:26` images were downloaded from the `docker.io`
    server. Which service that enables users to download images?
  prefs: []
  type: TYPE_NORMAL
- en: Docker images, just like Maven artifacts or operating system packages, create
    an ecosystem of interconnected reusable entities. Just like in a Maven or operating
    system scenario, we need a service that will store and distribute such images.
    Such service is called a Docker registry.
  prefs: []
  type: TYPE_NORMAL
- en: Docker provides a default registry called DockerHub. DockerHub is a publicly
    available free registry. If not configured, Docker will use DockerHub as the default
    registry.
  prefs: []
  type: TYPE_NORMAL
- en: Docker wrap-up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you can see, Docker is a tool that provides a number of capabilities that
    are essential building blocks for our cloud architecture, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The isolation implemented on the operating system level, combined with a layered
    filesystem implementation, makes it possible to share server resources effectively
    and allows for immediate container startup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image ecosystem provides a vast number of images that can be downloaded
    and used immediately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The containers run from the same images and operate on different Docker environments,
    providing consistency in all your environments. As you will learn in the rest
    of this chapter, this is the key feature of a dynamic cloud environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these features make Docker containers a great building block for cloud infrastructure.
    However, we need more than that. First, there will be a lot of containers in nontrivial
    environments. We are aiming for a dynamic environment, which enables, among other
    things, automatic scaling, high availability, and continuous integration as we
    expect a large number of those containers to be started and stopped within short
    periods of time. Finally, no matter how cool and efficient Docker images are,
    we would preferably like to generate them automatically during the build of our
    applications. All of those issues are resolved for you by OpenShift. Let's continue
    learning about the OpenShift stack. The next thing that you need to learn is orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just learned about a tool that enables us to provide the containers—lightweight,
    virtual machines using operating system-level virtualization and providing us
    with isolation, effective resource usage, immediate creation time, and the most
    repeatable behavior across different environments. This is the first layer in
    our cloud environment, but it is not our target platform. In a more complex production
    system, we would have to manage a large number of containers and, obviously, we
    don't want to do it manually. We will need a tool that will manage our containers
    in a clever way. Let's meet Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to show you the interface that Kubernetes provides, let's introduce
    its main architectural concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes and master services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes creates a cluster from a group of computers. The computers that constitute
    the cluster can be heterogeneous. A cluster may be created on your laptop, a group
    of workstations, or virtual machines. Also, all types of worker machines can be
    mixed in one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes nomenclature, each worker machine is a node and the whole cluster
    is governed by the master node. Each node runs the Kubelet, which is a daemon
    that enables communication with the master and the Docker engine so that Kubernetes
    can deploy containers on it.
  prefs: []
  type: TYPE_NORMAL
- en: The master, on the other hand, is a group of services that coordinate the whole
    cluster. From the user's point of view, the most important part of the master
    is its REST API service, which provides an endpoint that allows users to interact
    with the whole cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram presents a sample Kubernetes cluster, which we will use
    in the further description. The master services are represented by the blue circle
    and each node is represented by a rectangle. The cluster consists of two workstations
    (cyan), two virtual machines (green), and one laptop. Each node runs Kubelet so
    that it can connect with the master, and the Docker engine so that it can start
    containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ebf00e0-807c-47cd-8b39-d7f8eda63e95.png)'
  prefs: []
  type: TYPE_IMG
- en: Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, containers are ephemeral which means that they may be started
    and stopped often. If the container data is not committed, it will be erased when
    the container is stopped. As a result, we will need another tool for the storage
    of data. In Kubernetes, such a functionality is provided by volumes. Volume is
    a persistent storage implementation, which has an independent life cycle and can
    be mounted in a number of containers. We will discuss volumes in detail in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes, a pod is a group of containers and volumes, all of which share
    the same IP address among the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: All content of the pod is guaranteed to run on the same host. As a result, a
    pod can be thought of as an atomic unit of deployment and scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: The pod concept is needed in order to provide us with the ability to implement
    decoupled containers. With pods, we are able to collocate a bunch of containers
    with different functionalities that have to be located together (and possibly
    share data). Those distinct functionalities can be encapsulated in each container.
    If, on the other hand, a container would have to be used as an atomic unit of
    deployment, we may be forced to collocate distinct functionalities in one container
    in order to ensure that they will be deployed and scaled together, breaking good
    design principles of low coupling and high cohesion.
  prefs: []
  type: TYPE_NORMAL
- en: In a large number of scenarios, a pod will contain only one container, and this
    will be perfectly fine. Pods don't have to comprise many containers, but you are
    provided with such a possibility so that you can use it when it is necessary for
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already know about the building blocks of a Kubernetes cluster. Now it''s
    time to look at the thing that interests us the most: deploying applications to
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you want to deploy an application to the Kubernetes cluster, you have
    to create the deployment object that contains information about it. Among other
    things, Kubernetes must know which containers constitute the pod and how many
    replicas of the pod have to be created. Given that knowledge, Kubernetes will
    decide on which node''s application pods will be deployed, and it will deploy
    them there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f856f535-ef0b-4f97-b3aa-a84dcc0bd159.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the deployment, which requires that the pod be replicated
    on three hosts, has been created. Let's assume that Kubernetes has decided that
    the pod will be run on **WORKSTATION1** and the two virtual machines. The deployment
    object has become a part of the master model. What do we actually mean by that?
  prefs: []
  type: TYPE_NORMAL
- en: It has to be strongly emphasized that deployment isn't an operation that will
    finish after execution, leaving no further impact on the cluster status. Instead,
    the deployment effectively adds objects to the description of the desired state
    of the cluster. It is the role of Kubernetes to make sure that this state will
    be maintained.
  prefs: []
  type: TYPE_NORMAL
- en: We have hinted that Kubernetes master provides the REST API. This API allows
    for the creation of an object describing the desired state of the cluster. The
    deployment is one of those objects. As a result, deploying an application to the
    cluster is the equivalent of adding one more deployment object to the cluster
    description.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes monitors the state of the cluster and is responsible for making sure
    that it is equivalent to that description. To clarify it a little more, let's
    look at a few simple examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that one node in the cluster has gone down. As a result, the
    group of pods that were deployed on it has to be moved to different nodes so that
    the number of deployed pods matches the description of the deployment. To present
    it in our example, let''s suppose that **VIRTUAL MACHINE 1** has gone down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a73437f2-7de1-4cae-93e2-50c56c5bad7e.png)'
  prefs: []
  type: TYPE_IMG
- en: In such a scenario, master would find out that the **VIRTUAL MACHINE 1** node
    has failed and, in order to keep the number of pod replicas aligned with the description,
    it will deploy one more pod on another machine—**LAPTOP 1**, in our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, what about deploying a new version of an application? We would have to
    change the version in the deployment description. Kubernetes will find out that
    the version has changed. It will roll down the pods with the preceding version
    of an application and start pods with the new application. Let''s present this
    example in our diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5be6545f-50ca-4703-8de4-e11bc0ee2684.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes has undeployed the pods that constitute the preceding application,
    replaced the old deployment object with the new one, chosen on which nodes the
    updated pods have to be deployed, and, finally, performed deployment on those
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Because this chapter is aimed at explaining OpenShift architecture, we are using
    general theoretical examples. We will show you how scaling and deployments are
    done in practical examples in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key point to understand now is the principle on which Kubernetes operates:
    it is responsible for keeping the cluster synchronized with the desired state
    described by the user.'
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may already have noticed that there is a problem with the dynamic pod deployments
    described in the preceding section. How are we supposed to connect to the application
    if we have no idea where its pods are located? To solve this problem, Kubernetes
    introduced the service concept.
  prefs: []
  type: TYPE_NORMAL
- en: A service is an object that monitors the group of pods that constitutes an application.
    It contains search criteria that define which pods are the part of an application,
    and monitors the cluster so that it knows where these pods are located. A service
    has its own IP address, which is visible from outside the cluster. As a result,
    the client only has to know the address of a service. The actual cluster status
    (number of pods and their location) is abstracted away from the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s present it again in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/952d5674-b267-4779-9183-6afe3088210e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the call to the master has created the service object
    in the master's object model, which has resulted in the creation of the service.
    The created service has an IP address, which is reachable by the external clients.
    When invoked, the service will load balance the invocation to one of the pods
    from the cluster. Note that the service constantly monitors the state of the cluster.
    If, for example, there is a failure in one of the nodes, the service will learn
    the new locations of the pods and will continue to work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example combines together all the concepts that we have introduced. Let''s
    recap them: Kubernetes creates a cluster of heterogeneous worker machines. All
    machines in a cluster have to be able to run Docker containers and communicate
    with Kubernetes master services that govern all the clusters. The unit of deployment
    is a pod, which consists of one or more containers and can have many replicas.
    A user creates the description of such deployments using the master''s API, and
    Kubernetes is responsible for ensuring that the actual cluster state matches this
    description. To do that, it has to, among other things, perform health checks
    of nodes and the application, and redeploy them as necessary. It also has to react
    to all model changes and modify the cluster accordingly. Owing to all the reasons
    mentioned in this paragraph, application pods can be located on different nodes,
    and those locations can change dynamically. As a result, Kubernetes introduces
    the service concept, which provides the proxy for the external clients to the
    pods that constitute the application to which the client wants to connect.'
  prefs: []
  type: TYPE_NORMAL
- en: Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each object in a OpenShift cluster model can have any number of labels, which
    are basically key-value properties. This is a very simple, yet powerful, feature.
  prefs: []
  type: TYPE_NORMAL
- en: We will need a way to be able to categorize different types of objects. We can
    build a directory structure for that, but the problem with that solution is that
    it's not flexible enough. A directory structure provides one view of objects,
    but there may be many of them, depending on the user or on the current usage scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Labels, on the other hand, provide full flexibility. Any object can have any
    number of labels applied. Those labels can be used in queries to find an OpenShift
    object based on a wide range of characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s take a look at a service object again. The service has
    to find all the pods that run the application represented by the service. The
    service finds those pods by querying the labels: it''s a common practice for the
    pod to have an app label set to the name of an application that runs on it. As
    a result, the service can query for pods with an appropriate app label.'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know the most important concepts regarding the architecture of Kubernetes,
    it is time to look at the big picture of the interface that it provides. Kubernetes
    takes the group of heterogeneous worker machines and provides its user with a
    view of a homogeneous container execution environment. Let's think about this
    for a moment. As a user, you tell Kubernetes, *I want to deploy my application,
    which consists of N pods.* Kubernes will place those somewhere in the cluster.
    If pods have to be scaled up or down, or if there are failures, Kubernetes will
    take care of moving those pods between the underlying machines, but the technical
    details of those machines are abstracted away from the user. With Kubernetes,
    a user sees the cluster as a pool of container execution resources. Furthermore,
    this view is the same for all the clusters that you may want to run in your environment.
    The Kubernetes development cluster on your laptop will have the same interface
    as your production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must also emphasize the role that containers play—Docker guarantees that
    two containers built from the same image will behave identically. Combining that
    fact with the role that Kubernetes plays, we are able to see the cloud view provided
    by Docker and Kubernetes: a pool of container execution computing resources, which
    further guarantees repeatable behavior on all environments on which it is deployed.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we covered the powerful cloud abstraction that is
    provided by Kubernetes and Docker. On the other hand, we hinted in a number of
    places that in order to use Kubernetes effectively, you have to, among other things,
    deal directly with Kubernetes object configurations or Docker files. As we wrote
    before, we would preferably like to have a tool that will abstract those things
    away from us, make them either happen automatically, or be configured using easy-to-use
    tools. Here is where OpenShift steps in. OpenShift adds another layer of abstraction
    on top of Kubernetes, providing it with additional cluster model features, such
    as builds, or tools, such as web console. As you will see, the layer added by
    OpenShift makes all the cloud operations very simple and effectively allows you
    to focus on the development of your code.
  prefs: []
  type: TYPE_NORMAL
- en: Returning for a moment to the cloud computing types section at the beginning
    of this chapter, we can say that Kubernetes and Docker provide you with IaaS,
    whereas OpenShift transforms it into powerful programmer-oriented PaaS.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn about the most important features provided by OpenShift in the
    following paragraphs. Now, let's quickly highlight the most important features
    of OpenShift. We will start with the build infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The build infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous paragraphs, we suggested that direct Docker images are abstracted
    away by OpenShift. Now it's time to look at it a little more closely.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift provides builds and deployment abstractions. Builds are responsible
    for creating images, and deployments are responsible for deploying those images
    to the cluster, providing the abstraction above Kubernetes objects, such as replication
    controllers.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first questions that come to mind now is how are those images built?
    There are a bunch of options, but the most interesting one for us is source-to-image
    build. As its name suggests, this kind of build is responsible for automatically
    turning your code into a Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, your interaction with OpenShift may be configured as follows:
    you write your application and push the changes into the GitHub repository. This
    triggers the source-to-image build, which creates a Docker image, and the creation
    of the Docker image may trigger automatic deployment. Also, all those steps may
    be integrated into Jenkins based on a continuous delivery pipeline. As you can
    see, OpenShift build tools allow you to concentrate only on code development.
    The interaction with cloud will be done automatically by the build infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: You will learn more about deployments and about builds (including source-to-image
    and pipeline builds) in [Chapter 9](e5cfc996-0fbf-40e8-a89b-fc9ee72c8861.xhtml),
    *Configuring Continuous Integration Using Jenkins*.
  prefs: []
  type: TYPE_NORMAL
- en: Management of projects and users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This feature is not as fresh as the preceding one, but it's still very important.
    In order to make OpenShift able to work in an enterprise environment, the concept
    of the project is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides the namespace concept, which enables it to separate the
    cluster into a group of virtual clusters. This concept namespace doesn't implement
    access control though. As a result, OpenShift creates the notion of a project,
    which is a Kubernetes namespace identified by specific annotation and implementing
    access control policies based on users and user groups.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn more about security microservice applications in [Chapter 10](2ad0780e-aeb3-40a8-8af7-f2d26341bb35.xhtml),
    *Providing Security Using Keycloak*.
  prefs: []
  type: TYPE_NORMAL
- en: Minishift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already said a lot of good things about OpenShift. It is high time we
    saw how it works in practice. However, how are you actually able to do that? As
    a developer, you have access to OpenShift Online. It is a publicly available OpenShift
    cloud where anyone can open an account and test OpenShift itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also another option: Minishift. Minishiftis a tool that starts a virtual
    machine on your local computer and creates an OpenShift cluster inside it. As
    a result, it enables you to try and test a fully featured OpenShift cluster on
    your local machine. This is the option that we will use in this book. Let''s start
    by installing it.'
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are able to download its latest version from the GitHub page. You also have
    to install the virtual machine that you will use and configure your environment
    variables accordingly. The process is very simple and takes a few minutes to complete.
    The details of the particular installation steps differ a bit between operating
    systems. They are described thoroughly in the attached installation guide.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After you have installed the cluster, you can start it using the minishift
    start command. It is a good practice to boost the default parameters to provide
    enough memory and disk space for the services that we will develop and use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After you run the preceding command, you have to wait few minutes for the cluster
    to start:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f11a892-ad0c-4edd-95aa-6511ba0fa8b1.png)'
  prefs: []
  type: TYPE_IMG
- en: After minishift has started, we can access it using the provided address in
    the startup log. The first screen is a login screen. On this screen, you can use
    any credentials (as Minishift is a test tool) and click the Login button. After
    you do this, you will see the `web console`, which is one way of managing the
    OpenShift cluster. Let's learn more about it.
  prefs: []
  type: TYPE_NORMAL
- en: Web console
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `web console` is a graphical tool that enables you to view and manage the
    content of an OpenShift project. From a technical point of view, the console is
    a graphical interface that provides convenient abstraction over the OpenShift
    REST API, which it uses to modify the cluster model according to user operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the main console window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b37e1787-e8e8-4a6f-9b1c-ae4b14f0fb06.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding screenshot, the console allows you to manage
    projects, view their content, and modify it. The overview (presented in the preceding
    screenshot) contains an application deployed in the petstore namespace. The menu
    on the left allows you to view and modify different aspects of the cluster, such
    as builds, deployments, or persistent resources.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `web console` extensively in the following chapters, where you
    will be able to take a look at most of its features and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: YAML notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the majority of configuration can be done using the graphical interface,
    sometimes it will be necessary to edit the internal representation of OpenShift
    objects: YAML.'
  prefs: []
  type: TYPE_NORMAL
- en: Each object in an OpenShift model can be represented using this kind of notation.
    If you click on Applications | Deployments, choose one of them, click on Actions
    in the top-right corner, you will be able to choose the Edit YAML option. This
    applies to all objects in the console.
  prefs: []
  type: TYPE_NORMAL
- en: We will be performing this from time to time when such an edit is necessary,
    informing you about the meaning of the performed operation.
  prefs: []
  type: TYPE_NORMAL
- en: CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it is more convenient to use a command-line tool instead of graphical
    interface. OpenShift provides it too. The OpenShift CLI implements the `oc` command-line
    tool, which allows for managing the cluster from the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: You can install the CLI using the instruction provided in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that you have to do in order to use `oc` is to log in to the
    cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You will be asked for your credentials, and will have to provide the same credentials
    that were used to create your project in the web console.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of operations that the `oc` tool provides. We will use it
    extensively to get and describe operations. Let's introduce them now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get` operation allows you to obtain available information about the availability
    of a given type of object; let''s invoke the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The tool will suggest a type of object that you can inspect; let''s take a
    look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82c54c38-0cf9-4a6e-8f63-66f9b80e74e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Wow! That''s a lot, but don''t worry, you will learn a lot about most of these
    in the next chapters. Let''s use the `oc get` command to inspect the services
    available in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffcd64b9-21ac-44ea-b131-e980aa585215.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also take advantage of labels. If you write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Then you will be able to see all kinds of objects associated with the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code, we are able to list the objects that
    we are interested in using the `get` command. If we want to get some more information
    about them, we need to use the `oc describe` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7f26657-45ae-42e4-9737-f5c2f03d58de.png)'
  prefs: []
  type: TYPE_IMG
- en: The describe command allows you to read all the information about the given
    type of object.
  prefs: []
  type: TYPE_NORMAL
- en: You have now learned all the essential information needed to understand OpenShift.
    Now, it's finally time to try it.
  prefs: []
  type: TYPE_NORMAL
- en: Catalog-service on the OpenShift example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot of theory in this chapter and have introduced many concepts
    that will help you to better understand how OpenShift works under the hood. It
    is finally time to try it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: We will deploy a `catalog-service` with a `h2` database. In this example, we
    will only use the web console and deploy the applications from the book's code
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples reference: `chapter6/catalog-service-openshift-h2`'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start. Let's enter the host address. You will find it in the log of the
    minishift start command. After entering it into the web browser, you will be welcomed
    by the user login screen. Let's enter our Username and Password.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be directed to the welcome screen, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2414d182-561f-4292-b79a-9a2e2dd6f7f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enter petstore as the name of the project. In order to deploy catalog-service
    to OpenShift, we will use the source-to-image build using the CLI. To start with,
    ensure that you have logged in to the cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command creates a bunch of OpenShift objects that are necessary
    to start an OpenShift build.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is time to start an application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We are fully aware that, at this point, these commands look cryptic. For some
    time, we will use them as a magical service deployment spell. Don''t worry though,
    in [Chapter 8](988f96cb-b157-41a0-8c0c-81c5ec440927.xhtml), *Scaling and Connecting
    Your Services*, they will be fully explained, and you will be able to understand
    each part of the process. To give you a quick overview now: we are making OpenShift
    create a build of our service directly from source code. In order to do that,
    we have to specify the GitHub repository. Because our book repository contains
    many subdirectories, we have to specify the subdirectory in which this example
    is located—we use `--context-dir` for that. We also provide the name, using the
    `--name` command.'
  prefs: []
  type: TYPE_NORMAL
- en: For now, let's use the web console to check whether the application has been
    deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Login to the web console again and navigate to Builds | Builds on the left-hand
    side. You will be able to see that the build has indeed started:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2df1108-d9ad-44bb-81e5-fdf606af338c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Wait till the build has finished, then navigate to Applications | Services,
    and select catalog-service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2aaea04d-8621-43f9-b60a-92ff25c8207f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding screenshot, OpenShift has deployed the application
    on one pod and made a service for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can check the application, we have to do one more simple thing. The
    services are not visible outside of the cluster, and as a result, in order to
    use them, we have to expose them on the address visible from the external network.
    OpenShift provides a tool that enables us to use these routes. In the preceding
    view, click on the Create route link, don''t change anything, and click the Create
    button. After that, you will be able to see the external address of the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebb4abc6-5a4f-45f3-96f0-8581f9d95df9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we are ready to check the operation of our service. Copy the external
    address of the host and add the REST path to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f65364c-e200-4736-9f47-311f28d1fc4f.png)'
  prefs: []
  type: TYPE_IMG
- en: It works. Congratulations! You have deployed your first service in OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned a lot about OpenShift architecture. You were introduced
    to cloud computing and was provided the most essential information about it. Later,
    you learned about the architecture of OpenShift: a Kubernetes cluster using Docker
    images with an OpenShift layer, making cluster easy, to use and allowing developers
    to focus on coding.'
  prefs: []
  type: TYPE_NORMAL
- en: Later in the chapter, you started your own local OpenShift instance using the
    Minishift tool and deployed your first service on it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to configure persistent storage for
    microservices deployed in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/engine/reference/builder/](https://docs.docker.com/engine/reference/builder/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.docker.com/](https://www.docker.com/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://openshift.io/](https://openshift.io/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/minishift/minishift](https://github.com/minishift/minishift)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://docs.openshift.org/latest/minishift/getting-started/installing.html](https://docs.openshift.org/latest/minishift/getting-started/installing.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://docs.openshift.com/enterprise/3.1/cli_reference/get_started_cli.html](https://docs.openshift.com/enterprise/3.1/cli_reference/get_started_cli.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
