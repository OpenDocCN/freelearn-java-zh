- en: Data Processing Using the Lambda Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes the Lambda pattern, which is not to be confused with
    AWS Lambda functions. The Lambda architecture consists of two layers, typically
    used in data analytics processing. The two layers include a speed layer to calculate
    data in near-real time and a batch layer that processes vast amounts of historical
    data in batches.
  prefs: []
  type: TYPE_NORMAL
- en: Because serverless platforms allow us to scale horizontally very quickly, and
    since it's simple to store large amounts of data, the Lambda pattern is well suited
    for a serverless implementation. Lambda architectures are relatively new, coming
    onto the scene with the advent of big data processing and the desire to see the
    results of processing sooner than was previously available using batch systems
    such as Hadoop. This type of architecture or pattern is especially interesting
    since there are so many components involved in making it work, which we'll walk
    through using an example application that will calculate average prices for the
    cryptocurrencies Bitcoin and Ethereum.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you can expect to have learned the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A thorough understanding of the Lambda architecture and when it may be appropriate
    to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What tooling and options are available when designing a Lambda architecture
    in a serverless environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a speed layer for processing a stream of cryptocurrency prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to develop a batch layer for processing historical prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternate implementations and tooling when building a serverless Lambda architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the lambda architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To the best of my knowledge, Nathan Martz, author of Apache Storm, first introduced the
    lambda architecture in a 2011 blog post. You can read the post yourself at [http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html).
    In this post, Nathan proposes a new type of system that can calculate historical
    views of large datasets alongside a real-time layer that can answer queries for
    real or near-real-time data. He labels these two layers the batch layer and the
    real-time layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Lambda architecture was derived from trying to solve the problem of answering
    queries for data that is continuously updated. It''s important to keep in mind
    the type of data we''re dealing with here. Streaming data in this context are
    factual records. Some examples of streaming factual data are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The temperature at a given location at a given time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An HTTP log record from a web server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The price of Bitcoin from a given exchange at a given time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine the case where a temperature sensor is taking measurements in
    a given location and sending those readings somewhere every 5 seconds. If the
    temperature reading on January 31, 2018, at 12:00:00 was 75.4 °F, that fact should
    never change. A reading 5 seconds later may be 75.5 °F, but that does not nullify
    the prior reading. In this and other cases, we are working with an append-only
    data stream of facts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our temperature analogy, imagine that we need to answer questions about
    this data such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What was the average weekly temperature since 2010?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What was the average monthly temperature since 2000?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What were the daily high and low temperatures over the past year?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the temperature trend over the past 2 hours?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not only are we working with immutable data, but there is a time domain to consider
    as well in the queries to which we need to respond. If we had a naive implementation,
    we could store each piece of data in a relational database and perform queries
    on demand. There are 31,540,000 seconds in a year. If our system was uploading
    measurements every 5 seconds, that is 6,308,000 each year. Now, assume that we
    need to keep track of 10,000 different sensors around the world. This means our
    system would be adding 63,080,000,000 new records each year.
  prefs: []
  type: TYPE_NORMAL
- en: The initial challenge using a relational database would simply be finding a
    subset of those records for a particular location, for example, `SELECT * FROM
    temperatures where location_id = 1234`. Of course, we can undertake this type
    of query quickly using indexes, but there are significant limitations and trade-offs
    when dealing with billions or trillions of rows.
  prefs: []
  type: TYPE_NORMAL
- en: The second challenge would be performing calculations to get the right answer
    (that is, average temperature by week, high-low temperature each day). If our
    query was pulling data from 20 years ago until today, that would mean a lot of
    disk access and a significant load on the database, presuming the analytical query
    could be done in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, there are systems that can deal with this level of scale, such as
    data warehouses or NoSQL data stores. However, data warehouses are not designed
    for real-time queries. NoSQL systems may be better at storing large amounts of
    data, but they lack flexibility or ability when it comes to running calculations
    on that data.
  prefs: []
  type: TYPE_NORMAL
- en: What is the solution when we have the level of scale of a data warehouse, a
    continually updated data stream and the requirement to serve queries in real time?
    This is where the Lambda pattern can help. Comprised of a batch layer and speed
    layer, the Lambda pattern is designed to solve this problem of responding to queries
    in real time, pulling data from both the batch layer and speed layer outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, the view layer is a separate component of this architecture.
    As mentioned at the start of this chapter, there are many moving parts and components
    to a serverless implementation of the lambda pattern. For this reason, I won't
    be discussing the view layer in much detail so we can focus on the data portion
    of this pattern, which is more in tune with this chapter's theme. For a discussion
    of frontend applications in serverless systems, I'll refer you to [Chapter 2](svrls-dsnptn-bstprac_ch02.html),
    *A Three-Tier Web Application Using RE**ST*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9358ca3b-b4c3-4c7e-9faf-8fd1b0e46415.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Batch layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the lambda architecture was proposed, Hadoop was already widely adopted
    and regarded as a proven technology. Hadoop is a linearly scalable system and
    can easily churn through terabytes of data in a reasonable amount of time to calculate
    nearly anything from your dataset. Here, *reasonable* may mean a job that runs
    for a few hours in the middle of the night so that new views of your data are
    ready first thing in the morning.
  prefs: []
  type: TYPE_NORMAL
- en: Using our temperature monitoring analogy, a day's worth of data will require
    a new batch job run if we need to calculate the average temperature in the month
    or year. Also, imagine we wanted to calculate trends day by day, month by month,
    or year by year. Whenever a new batch of daily temperatures is completed, our
    system would need to perform some work to calculate the pre-materialized views.
    By pre-calculating all of this data, any query would just look up the answer on
    demand without needing to calculate anything.
  prefs: []
  type: TYPE_NORMAL
- en: Speed layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A batch layer by itself isn't anything new. The problem we're trying to solve
    here is answering queries quickly and up to date with a real-time view of our
    data stream. The magic with the Lambda pattern is the combination of the batch
    layer in conjunction with a speed layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The speed layer is a constantly updating system that processes new data from
    the data stream in real time. The calculations here will be the same as in the
    batch later, but it only works on a small subset of data as it arrives from the
    data stream. For example, to get the daily high temperature for a given location
    since 2015 in response to such a query, our system would do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch daily high temperatures from January 1, 2015, until yesterday from the
    batch layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetch the high temperature from today from the speed layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the two datasets into one to present back to the user
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also note that in such a system, we could go even further. Our view layer could
    display the historical data in one area of the application and present the real-time
    information separately, which could be continually updated using a WebSocket connection
    or polling. By separating out these two layers, many options open up regarding
    application development and interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda serverless architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the overall design and theme of a lambda architecture remain the same
    as a traditional system, there are variations and adaptations that we need to
    make. Perhaps more importantly, there are many different ways to implement this
    pattern using serverless systems or, at the very least, managed services.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data producers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any system must start with data to process. On serverless platforms, there
    are multiple choices for streaming systems. Azure, Google Compute, and AWS all
    offer some form of streaming systems. I mentioned these in [Chapter 6](svrls-dsnptn-bstprac_ch06.html), *Asynchronous
    Processing with the Messaging Pattern,* when discussing the differences between
    queues and streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure**: Event Hubs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS**: Kinesis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Compute Cloud**: Cloud Dataflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's worth briefly touching on the topic of queues versus streams again. As
    mentioned in [Chapter 6](svrls-dsnptn-bstprac_ch06.html), *Asynchronous Processing
    with the Messaging Pattern*, one of the main differentiators is that queues are
    primarily designed for once-only processing. That is, once a message is pulled
    from a queue, no other consumer will see it. Data in a stream, on the other hand,
    has a given lifetime and cannot be removed by any consumer of that data. For example,
    a stream can set data to expire after 24 hours, or after 30 days. At any point
    in time, one or more readers can come along and begin reading data from the stream.
    It's up to the readers to keep track of where they are in the history of a stream.
    A new reader may start at the beginning of the stream, in the middle, or at the
    end.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since there are two distinct layers in this pattern, storage choices will likely
    be different since the two layers are drastically different in their data requirements.
    The batch layer requires extreme scalability and should perform well for a high
    number of concurrent reads during batch processing. The speed layer, on the other
    hand, doesn't need to store as much data but should be extremely fast for both
    reads and writes.
  prefs: []
  type: TYPE_NORMAL
- en: In many examples of this pattern, you'll see references to **Hadoop Filesystem**
    (**HDFS**) for storing historical data and NoSQL databases for real-time data.
    While it's near impossible to say what you should pick, it is possible to speak
    to some of your options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud storage systems such as AWS S3 or Google Cloud Storage were designed
    to fill a similar role as HDFS, that is, to store practically as much data as
    you need. The advantages of storing plain files on services such as this are that
    it''s straightforward, requires almost no management, and is very durable. What
    I like about using flat or structured files is that it becomes possible to process
    the data later and store it in a different system such as a database. Also, there
    are a myriad of serverless or hosted systems that you can leverage that read data
    from these systems. Focusing only on AWS, the following systems can perform batch
    processing or analytical queries on S3 data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic MapReduce** (**EMR)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DynamoDB or other NoSQL are also options for historical data. Azure Cosmos DB
    and Google Bigtable are other services that I cannot speak to directly but that
    are options if you're building on top of those cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: At least in the case of DynamoDB, special consideration should be made since
    read and write throughput needs to be carefully considered to maintain a workable
    system.
  prefs: []
  type: TYPE_NORMAL
- en: For the speed layer, there are also multiple tools you can use. DynamoDB is
    a viable choice since it's linearly scalable and you should have a fair idea of
    the read and write capacity needed. Managed Redis services such as AWS ElastiCache
    or Azure Redis Cache are also decent choices since Redis is exceptionally performant
    and the dataset is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Computation in the speed layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It makes sense that, since we're using serverless systems, our serverless functions
    will perform any computations necessary. Serverless functions are a natural choice,
    for what should be obvious reasons at this point. Running functions on demand
    or in response to new data arriving on our streams is incredibly simple. Additionally,
    FaaS allows us to scale horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: In a lambda architecture, we may need to calculate many different metrics at
    the same time, from the same data stream. Doing this from a `single` serverless
    function could be possible. However, in the case where the computation is heavier,
    we may need to split out the computation into multiple functions, each calculating
    their own set of metrics from the same stream. Numerous readers allow us to scale
    out horizontally and provide the flexibility needed when data changes or new metrics
    need to be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: While serverless functions are a natural choice and easily understood, there
    are other options. On AWS, it's possible to use Spark Streaming from within the
    EMR system. Spark Streaming is purpose-built for this type of workload. In the
    case that your data stream outgrows the limitations of cloud functions such as
    Lambda, moving to Spark Streaming is a good alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Computation in the batch layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many lambda architecture systems will rely on Hadoop or Spark for the batch
    layer. Since we don't want to manage a cluster ourselves, we'll have to pick some
    other serverless system or, at the very least, a managed service. There are a
    variety of options here.
  prefs: []
  type: TYPE_NORMAL
- en: First, it's possible to implement our MapReduce system entirely using serverless
    technologies. You'll read about this in [Chapter 8,](svrls-dsnptn-bstprac_ch08.html) The *MapReduce
    Pattern*[](svrls-dsnptn-bstprac_ch08.html). If you'd rather not build a MapReduce
    system, there are other services that you can leverage. Both Spark and Hadoop
    are available within AWS EMR. HDInsight from Azure provides the same or similar
    functionality to EMR.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing is a solved problem nowadays, and you should have no problems
    finding a solution that works for your needs. Since there are so many options
    for batch processing, you may find it challenging to narrow down your choices.
  prefs: []
  type: TYPE_NORMAL
- en: Processing cryptocurrency prices using lambda architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, our example application will perform a single task of reading
    prices in real time for a variety of cryptocurrencies and calculating the average
    prices by the minute, hour, and day. Of course, this isn't all that useful in
    the real world because there is so much data on cryptocurrencies already. However,
    this presents an excellent scenario and dataset for an example application to
    illustrate this pattern. As is usual in this book, I'll build the application
    on top of AWS with Python. It's also important to note that none of the concepts
    are unique to AWS or Python and that this example application is portable to other
    languages and cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for this chapter at [https://github.com/brianz/serverless-design-patterns/tree/master/ch7](https://github.com/brianz/serverless-design-patterns/tree/master/ch7).[](https://github.com/brianz/serverless-design-patterns/tree/master/ch7)
  prefs: []
  type: TYPE_NORMAL
- en: System architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of this is perhaps the most complex in this book, even though
    the implementation itself is relatively simple. As mentioned earlier in this chapter,
    there are many moving parts in a lambda architecture since it is two distinct
    systems that work side by side, with each having its own unique set of services.
    I show a high-level architecture in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: This design is somewhat contrived and overly simplified. A system with true
    scale likely would not be able to get away with a serverless function that calculated
    historical prices by year, as I do in the batch layer. To fit this example application
    into a single chapter, I had to make the system relatively simple to demonstrate
    the pattern and techniques. While this may be an oversimplification of an actual
    big data system, it does show that the design works on a serverless platform and
    is even applicable to a decent-sized data problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd62972d-2a1f-42c0-90a2-9f48ff2c47f0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I'll break this down piece by piece and describe how each layer and component
    works. While there may be a lot going on in this design, each part is quite simple.
    Most of the complexity of this system comes from the system setup and concepts,
    instead of application code.
  prefs: []
  type: TYPE_NORMAL
- en: Data producer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, we need to have some data to process. In a real system where you are
    creating the data or wish to send some data *out* from your system, this isn't
    much of an issue. In our case, we need to pull data in real time from somewhere,
    and I've chosen the public API from GDAX ([https://www.gdax.com](https://www.gdax.com)),
    which is a digital exchange for cryptocurrencies from Coinbase. This API is suitable
    for our example application mainly because there are many transactions and there
    is a WebSocket endpoint that we can subscribe to. A simple script that subscribes
    to the GDAX WebSocket API and publishes those messages to our Kinesis stream will
    serve as our data producer.
  prefs: []
  type: TYPE_NORMAL
- en: Speed layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The speed layer is also relatively simple. Each message published to the Kinesis
    stream will trigger a Lambda function. This Lambda function will just write the
    data to DynamoDB so that we can serve data in real time for any queries. With
    this design, we're set up for a decent amount of real-time load and concurrency.
    The data producer will deliver data at a reasonably fast rate, perhaps a few messages
    per second. If there was a burst of traffic and the speed layer started seeing
    tens or hundreds of messages per second, it would not be a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Since serverless functions and databases such as DynamoDB scale linearly, the
    speed layer can absorb practically any amount of real-time traffic. There are,
    of course, provisioning and throughput concerns, as well as maximum concurrent
    limits to contend. However, these issues are just configuration settings that
    you can quickly change and increase as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Batch layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our batch layer gets a bit more interesting. Some of the details are AWS-centric,
    but you can carry the general idea across cloud providers. AWS Kinesis Firehose
    is another version of Kinesis that is designed to transport data in batches to
    various locations. In this architecture, we'll set up the Kinesis Firehose stream
    to ingest data from the primary Kinesis stream.  I'll also configure the Firehose
    stream to deliver batches of messages to S3 every minute.
  prefs: []
  type: TYPE_NORMAL
- en: If you have heard about AWS Kinesis but the term Kinesis Firehose is new to
    you, don't worry. Kinesis Firehose's specialty is loading data into various services,
    such as S3 and Redshift. A plain Kinesis stream captures data and makes it available
    for consumption, but that consumption is your responsibility. Kinesis Firehose
    is useful when you'd like to dump the streaming data to S3 or Redshift automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a new file being delivered to S3 every minute, we can set up a Lambda
    function to trigger on that event and perform some work. The work here will be
    reading the list of messages, calculating the average price per currency, and
    writing out a new file back to S3\. If you follow the flow of time and data, you
    should be able to see that we can extend this pattern down at different time increments
    - minutes, hours, days, months, and even years. The general flow and set of triggers
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Every minute, a Lamba function reads data stored on S3 and calculates the average
    price for the last minute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every hour, a Lamba function reads data stored on S3 and calculates the average
    price for the last hour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every day, a Lamba function reads data stored on S3 and calculates the average
    price for the previous day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since data is stored on S3, nothing is stopping this system from evolving to
    create an entirely new batch layer using more powerful tools such as Spark and
    Athena.
  prefs: []
  type: TYPE_NORMAL
- en: AWS resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In my opinion, the most complicated part of this entire system is set up all
    of the various resources and the interplay between them all. If you count up the
    number of resources we need to make this system work, the list quickly grows:'
  prefs: []
  type: TYPE_NORMAL
- en: Two S3 buckets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One Kinesis stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One Kinesis Firehose stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One DynamoDB table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four Lambda functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple IAM roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not only do we need to create all of the preceding resources, but we need to
    ensure they can communicate with one another. As is often the case with building
    with AWS, much of the work involved in managing a stack such as this is getting
    permissions correct so that the Lambda function can read/write from/to the right
    S3 buckets, DynamoDB table, or Kinesis stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see proof of this if we use `cloc` to count the lines of code in this
    application. Looking at the following output, you''ll see that the amount of configuration
    code in `serverless.yml` is higher than the application code, with 165 lines of
    YAML configuration to 128 lines of Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'I will walk through some interesting bits of the `serverless.yml` file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: First, we need to ensure our Lambda functions will have access to the various
    resources. In the preceding `iamRoleStatements`, I'm giving various permissions
    for all Lambda functions in this stack to the two S3 buckets we'll use as well
    as DynamoDB. This shouldn't come as a big surprise. Our Lambda functions will
    be reading and writing data from and to S3\. Likewise, our speed layer will be
    writing new records to DynamoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I'll walk through how to create a Kinesis stream that we can write to
    from our data producer, which, in turn, forwards messages on to a Kinesis Firehose
    delivery stream. Be warned; this is a bit raw and can seem complicated. I'll break
    this down bit by bit, hopefully in an order that is comprehensible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is creating a Kinesis stream. This part is straightforward using
    the `Resources` section of `serverless.yml`, which is straight-up CloudFormation.
    In this case, we only need a single shard since our application throughput is
    reasonably small. If the amount of data you''re pushing is larger, you can add
    additional throughput by increasing the number of shards. The following code snippet
    is the `resources` section from `serverless.yml` and shows how I''m creating the
    Kinesis stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up is the bit which is a bit more complicated. Kinesis Firehose is still
    a Kinesis stream, that behaves a bit differently as I mentioned earlier. In a
    standard Kinesis stream, you are responsible for doing something with the messages
    that producers push onto the stream. Kinesis Firehose, on the other hand, will
    automatically deliver a batch of messages to some destination. Your choices for
    final destinations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Elasticsearch service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can create a Kinesis Firehose stream by adding some CloudFormation code
    in the `Resources` block. What we need to create via CloudFormation is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Firehose stream that received data from the previous Kinesis stream and batch
    write data to S3 every 60 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An IAM role that grants access to Firehose to read/write to/from our S3 buckets
    and also read the Kinesis stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This CloudFormation code is a bit verbose. Rather than putting the entire code
    block here, I'll refer you to the GitHub repository. You can read the full details
    of setting up the Kinesis Firehose stream at the following URL: [https://github.com/brianz/serverless-design-patterns/blob/master/ch7/serverless/serverless.yml#L47-L113](https://github.com/brianz/serverless-design-patterns/blob/master/ch7/serverless/serverless.yml#L47-L113).
  prefs: []
  type: TYPE_NORMAL
- en: The data source for this Firehose stream is the primary Kinesis stream, which
    I named `GdaxKinesisStream`. You can see the configuration to use this stream
    as a data source in the `KinesisStreamSourceConfiguration` and `DeliveryStreamType`
    keys. These two settings say that we're going to be using a Kinesis stream as
    a data source, as opposed to putting data directly on this Firehose stream via
    API calls. It also tells the Firehose stream where to find this source Kinesis
    stream via the `KinesisStreamSourceConfiguration`, which can be a bit confusing.
  prefs: []
  type: TYPE_NORMAL
- en: Here, `KinesisStreamSourceConfiguration` is comprised of two keys, `KinesisStreamARN`
    and `RoleARN`. The former, `KinesisStreamARN`, refers to the location of the Kinesis
    stream we're connecting to. `RoleARN`, on the other hand, has to do with permissions.
    This referenced role must permit for the reading of the source Kinesis stream.
    It's a bit too much to cover here, but if you look at the entirety of the configuration,
    it should make some amount of sense.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've taken care of the input source, we need to set up the S3 destination
    configuration in the `S3DestinationConfiguration` key. This configuration is analogous
    to that of the source stream; we need to give our Firehose stream the data on
    where to write data with the `BucketARN` and also give it a role with the necessary
    access.
  prefs: []
  type: TYPE_NORMAL
- en: The other interesting and important part of the `S3DestinationConfiguration`
    is that it's configured to write data to S3 every 60 seconds or every 5 MB, whichever
    comes first. Since the GDAX WebSocket feed isn't all that chatty, we can count
    on hitting the 60-second limit before the buffer in Firehose reaches 5 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we can turn our attention to the Lambda functions that will be running
    our application code. I''ve implemented four different Lambda functions, which
    will handle the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Single events from the Kinesis stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 objects created every 60 seconds from Kinesis Firehose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 objects created from the aggregated minute views of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 objects created from the aggregated hour views of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The configuration of these four Lambda functions is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the first function, `ProcessPrice`, is triggered upon delivery
    of a message onto our Kinesis stream. Once this function executes, its job is
    done. There is no other interaction with this function and any other function.
  prefs: []
  type: TYPE_NORMAL
- en: The next three functions work in coordination. This process starts when `CalculateMinuteView`
    is triggered when the Firehose stream delivers a new batch of messages every 60
    seconds to S3\. This function will calculate the average prices using all of the
    delivered messages and upload a new file to S3 named `MM-minute.json`, where `MM`
    is a numerical representation of the minute calculated (`00, 01...59`).
  prefs: []
  type: TYPE_NORMAL
- en: Once we reach the end of an hour, this function will write a file named `59-minute.json`.
    Since that file signifies the end of an hour, we can trigger the `CalculateHourlyView`
    function to calculate the average prices for the past hour. This function produces
    files named `HH-hour.json`, where `HH` represents the 24 hours in a day `(00,
    01...23)`. The same strategy holds true for hours and days. Once a file named
    `23-hour.json` arrives, it's time to calculate the daily average prices from `CalculateDailyView`.
  prefs: []
  type: TYPE_NORMAL
- en: Data producer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example code shows what a simple client application looks like
    if you''re interested in prices for Bitcoin and Ethereum. This code doesn''t do
    anything other than deserializing the JSON payload from each message and printing
    it to the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this code, I''ll see payloads printing out whenever there is a buy
    or sell transaction for either of the two currencies to which I''ve subscribed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To get this data into our system, we need a few extra lines of code to publish
    data into an AWS Kinesis stream. The changes to the preceding client code are
    quite simple. Upon receiving a new message, I''ll check whether the payload is
    of the correct type, merely by looking for the time key. Some messages that we
    get back are confirmations to our subscription and do not include the time key,
    indicating some other message other than a trade. The following code shows these
    changes and how I use the `boto3` library to publish data to a Kinesis stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This part of our application is completely standalone. I ran this small script
    on an EC2 instance inside of a `screen` session, so the code continued to run
    when I logged out. This implementation isn't suitable for a real production system,
    but it worked just fine for the few days that I ran it. If I were doing this for
    an actual production system, I'd run this code with some daemon management systems
    such as `supervisord`, `upstart`, or `runit`.
  prefs: []
  type: TYPE_NORMAL
- en: Speed layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our speed layer is the simplest part of the entire system. With the configuration
    in place—a `single` Lambda function to execute whenever a new stream message arrives—the
    only work we need to do is decode the data from the message payload, calculate
    the DynamoDB partition key, and write it to DynamoDB. The following code block
    shows all of this work in the `single` function, which is processing a single
    message from AWS Kinesis:'
  prefs: []
  type: TYPE_NORMAL
- en: This code is all located in a single `handler.py` function, which goes against
    a best practice of splitting up application code and decoupling application logic
    from the cloud-specific bits. However, this application code is for demonstration
    purposes, and I can get away with breaking some rules for the sake of clarity
    and brevity. If there were a real system rather than a demo, I would be much more
    deliberate and organized with this code. Some of the import statements will be
    used in the batch layer code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this code is that every single GDAX transaction we''re interested
    in ends up in our DynamoDB table. The following screenshot shows a subset of the
    data stored. With the data in DynamoDB, our view layer can quickly look up a set
    of rows for a particular time range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cc2640e-7f08-4798-8e9d-a8ab7510e8b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Prices for DynamoDB are more dependent on the reads and writes you need for
    this system. Even though there is no code to trim out data we no longer need (that
    is, data that is historical and handled the batch layer), it's not a huge concern.
    Still, if there were a production system, you'd want to consider some techniques
    on expiring data that is no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: Batch layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas our speed layer is only interacting with DynamoDB, our batch layer will
    just be interacting with S3\. There are two distinct types of functions in this
    layer—the functions that respond to S3 objects that arrive from Firehose, and
    functions that respond to S3 objects coming from Lambda functions. There isn't
    all that much difference, but it's important to point out the two different categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block, taken from `handler.py`, shows the application code
    that comprises our batch layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Batch processing starts with the `minute` function. Again, this function starts
    when a new file arrives in S3 from our Firehose stream. At this point, we can
    be assured that the `minute` function is invoked once a minute based on our configuration.
    We don't need to go through this line by line, but we can look at the final results
    as well as some small tricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice the line that splits data using `split(''|||'')`. Firehose will
    concatenate records together before delivering to any data source. The AWS documentation
    on Kinesis Firehose explicitly states this, but it''s still easy to overlook:'
  prefs: []
  type: TYPE_NORMAL
- en: '"For data delivery to Amazon S3, Kinesis Firehose concatenates multiple incoming
    records based on buffering configuration of your delivery stream and then delivers
    them to Amazon S3 as an S3 object. You may want to add a record separator at the
    end of each record before you send it to Kinesis Firehose so that you can divide
    a delivered S3 object to individual records."'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding has been quoted from [https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html](https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html).[](https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html)
  prefs: []
  type: TYPE_NORMAL
- en: If you look back up at the data producer, you can see that we're appending the
    `'|||'` string to each message. With this delimiter, it's possible for us to break
    apart individual messages in this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final results that the `minute` function uploads to S3 have the following
    form. For each currency, it includes the list of individual buy prices along with
    the average price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our `CalculateHourlyView` executes once a file named `59-minute.json` arrives
    in S3\. The arrival of this S3 object signifies the end of an hour, so it's time
    to calculate the average prices for the entire hour. That work for the hour calculation,
    as well as the calculation of the daily average, is all wrapped up in the `_aggregate_prices`
    function. By using the prefix of the S3 object that triggers the functions, we'll
    only scan through a subset of the S3 records that are used in the calculation
    of the averages. For example, when a new S3 object arrives with a key name of
    `$BUCKET/2018/03/19/04/59-minute.json`, our application code can pick apart the `$BUCKET/2018/03/19/04`
    prefix and only scan files in that location. When an S3 object arrives named `$BUCKET/2018/03/19/23-hour.json`,
    the daily function will scan through files with the `$BUCKET/2018/03/19` prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After running this system for a few days, I was able to produce many S3 files
    as expected, as well as keeping track of each trade in DynamoDB. As I mentioned
    earlier in this chapter, implementing some view layer wasn't feasible for this
    example system. However, querying DynamoDB is quite easy, provided primary keys
    and sort keys are set up correctly. Any view layer that needed to get historical
    data could easily grab files from S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows listings of S3 files for a given hour. Each
    file has the format shown earlier with individual prices, along with the pre-computed
    average for each currency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84eb37ec-4bcb-429d-9d44-11d7ae582ff8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking one level up in the S3 hierarchy, we can see the hour files that contain
    the average prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea568325-0b31-4753-9ead-2547ea41f0ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the Lambda pattern at a conceptual level as well
    as in detail. I walked through an example implementation with serverless technologies
    to calculate average cryptocurrency prices for different time slices. Our example
    application was fed by a simple script that receives data from the GDAX WebSocket
    API. This data producer script published data to a single AWS Kinesis stream,
    which, in turn, triggered a series of events, ultimately resulting in real-time
    updates to DynamoDB and triggering batch jobs to calculate historical views of
    the minute, hourly, and daily average prices for multiple cryptocurrencies.
  prefs: []
  type: TYPE_NORMAL
- en: I discussed when the Lambda pattern may be appropriate and the types of data
    for which it's well suited. We talked through various systems and services that
    one may leverage when building a lambda architecture using serverless technologies.
    I introduced AWS Kinesis and AWS Kinesis Firehose, which are streaming systems
    you may leverage for real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: While the details of a Lambda pattern implementation can be quite intricate,
    readers should have a decent understanding of its advantages, disadvantages, and
    when they should consider it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, I'll cover the MapReduce pattern and work through an example
    where we will build our very own serverless implementation of this pattern.
  prefs: []
  type: TYPE_NORMAL
