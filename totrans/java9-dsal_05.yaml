- en: Chapter 5. Efficient Searching – Binary Search and Sorting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is searching? Searching is trying to locate a given value in a collection
    of values. For example, you are given an array of integers, and your problem is
    to check whether the integer *5* is in that array. This is a search problem. In
    addition to just deciding whether the element *5* is in the array, we may be interested
    in its location as well when it is found. This is also a search problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting take on it would be to imagine a dictionary, that is, an
    array of values and associated values. For example, you have an array of names
    of students and their marks, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Marks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Tom | 63 |'
  prefs: []
  type: TYPE_TB
- en: '| Harry | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| Merry | 65 |'
  prefs: []
  type: TYPE_TB
- en: '| Aisha | 85 |'
  prefs: []
  type: TYPE_TB
- en: '| Abdullah | 72 |'
  prefs: []
  type: TYPE_TB
- en: '| … | ... |'
  prefs: []
  type: TYPE_TB
- en: The list continues. Suppose, our system lets the student view his/her own marks.
    They would type their name and the system would show their marks. For simplicity,
    let's assume that there are no duplicate names. Now, we have to search for the
    name provided and return the corresponding values. This is, thus, another search
    problem. As we will see, search problems are quite ubiquitous in programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Search algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient searching in a sorted list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some sorting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you are given an array of values and you are required to check whether
    a particular value is in that array, what is the most natural way to find that
    element? Well, it seems that the natural way is to go through each element one
    by one and check whether they match the given value. If any one does, we have
    found that element and we can return the index; if not, we can return `-1` at
    the end of processing all the elements to report that such an element could not
    be found. This is what we would call a **linear search**. The following demonstrates
    a linear search algorithm in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `linearSearch` takes an array of values and a value to search
    in it, and returns the index if the value is found. If not, it returns a special
    value, `-1`. The program simply goes through each element and checks whether the
    current element matches with the value to lookup; if it does, then it just returns
    the current index, otherwise it keeps looking. At the end of the array, it returns
    the special value, `-1`. Now the following piece of code should return `-1` in
    the first case and the value `5` in the second case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we want to solve the student-marks problem described in the introduction
    of this chapter, we just need to have the marks of the students stored in a different
    array in the same order, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write a function to search for a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we look for the name of the students in the list of students. If the
    name is found, the corresponding index would be assigned to the variable index
    and the value would be greater than or equal to zero. In such a case, we return
    the marks stored in the same index as of the marks array. If it is not found,
    we return null. To lookup the marks for Merry, for example, we call as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We correctly obtain her marks, that are, `65`.
  prefs: []
  type: TYPE_NORMAL
- en: What is the complexity of linear search? We have a `for` loop that moves through
    each element of an array of length *n* (say); in the worst case, we would go through
    all the elements, so the worst case complexity is *θ(n)*. Even on an average,
    we would be visiting half the elements before we hit the correct element, so the
    average case complexity is *θ(n/2) = θ(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: Binary search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is a linear search the best we can do? Well, it turns out that if we are looking
    at an arbitrary array, this is what we have to do. After all, in an arbitrary
    array, there is no way to know whether an element is there without potentially
    looking at all of them. More specifically, we cannot say for sure that some element
    does not exist, without verifying all the elements. The reason is that the value
    of one element does not say anything about the values of the other elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, what information can one element have about other elements in an array?
    One way to make elements have information about the other elements is to have
    a sorted array instead of just an arbitrary array. What is a sorted array? A sorted
    array is an array that has all the elements arranged in order of their values.
    When an array is sorted, every element contains the information that everything
    on the left is smaller than that particular element, and everything on the right
    is bigger (or the other way round if the order of the elements is opposite, but
    we will consider the arrays that are sorted in an increasing order from left to
    right). This information can, amazingly, make this search a lot faster. Here is
    what we do:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the middle element of the array. If it matches the element we are searching
    for, we are done.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the middle element is smaller than the value we are searching for, search
    on the subarray on the right of the current array. This is because everything
    on the left is even smaller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the middle element is bigger than the value we are searching for, search
    only in the left sub-array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To avoid creating copies of the array while creating a sub-array, we just pass
    on the whole array, but we remember the start and end positions we are looking
    at. The start is included in the range and end is excluded. So, only elements
    on the right of the start position and the left of the end position are included
    in the subarray being searched. The following figure gives a visual understanding
    of binary search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary search](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Binary Search.'
  prefs: []
  type: TYPE_NORMAL
- en: An arrow representing moving one element to another during the search.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, before implementing this algorithm, we need to understand the concept
    of `Comparable`. `Comparable` is an interface in the Java standard library that
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Any class implementing this interface has to compare a different object with
    itself. It is required that the type parameter, *T*, must be instantiated with
    the same class that implements it, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `compareTo` method intends to compare an object of the same type. If the
    current object (the one that the `this` reference refers to) is smaller than the
    object passed, `compareTo` must return a negative value. If the object passed
    is smaller, the method must return a positive value. Otherwise, if they are equal,
    it must return `0`. The following conditions are required to be fulfilled by the
    `compareTo` method:'
  prefs: []
  type: TYPE_NORMAL
- en: If `a.compareTo(b) == 0`, then `a.equals(b)` must be `true`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `a.compareTo(b) < 0` and b`.compareTo(c) < 0`, then `a.compareTo(c) <0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `a.compareTo(b) <0`, then `b.compareTo(a) > 0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `b.equals(c)` is true and `a.compareTo(b) <0`, then `a.compareTo(c) <0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `b.equals(c)` is true and `a.compareTo(b) >0`, then `a.compareTo(c) >0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basically, the conditions are the same for a total order where equality is represented
    by the equals method. It basically generalizes the concept of the `<` and `<=`
    operators, which are there for numbers. Of course, the `compareTo` method for
    the `Wrapper` objects are implemented exactly as the `<` and `<=` operators are
    on the primitives inside them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we write the search function to do the search, as per the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that in this case, we have mandated that the objects in the array must
    be comparable so that we can know if an object is greater than or less than another
    object. It does not matter exactly how this relationship is determined; the array
    must be sorted using the same comparison – that the comparison between two consecutive
    elements will make sure the element on the left is smaller than the one on the
    right, as provided by `Comparable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `if` condition checks whether the array passed is empty, if so, obviously
    the element to be searched is not found and we return `-1` to represent this.
    Then, we find the `midIndex` and recursively search for the element in either
    the left or the right subarray. Once we have this function, we create another
    wrapper function to run the search without having to mention the start and the
    end positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Complexity of the binary search algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In every step, we are partitioning the total array into two parts, barring
    the one element that we are comparing. In the worst case, that is, the case where
    the element searched is not present in the array, we will have to get down all
    the way to the point where we are dealing with an empty array, in which case we
    return `-1` and stop recursing. We must have had an array of only one element
    in the previous step. For our analysis, we will consider this step as the last
    step. So, let''s have a sorted array of *n* elements and *T(.)* is the time required
    for searching in the array. So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In general, the two search branches in every step would be of different sizes,
    one potentially being of size one less than that of the other part. But we will
    ignore these small differences, which hardly matter for a large *n*. Hence, we
    will work with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s assume that *n* is an integral power of *2* so that *n = 2m* for
    some integer *m*. So, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take another function *S(.)*, such that *S(m) = T(2m)* for all *m*.
    Then, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the formula for an arithmetic progression. And hence, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have the asymptotic complexity of *T(n)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The function, *T(n)*, grows only as fast as the logarithm of the size of the
    array passed, which is really slow. This makes binary search an extremely efficient
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sort of field test the algorithms, we run both linear and binary search
    algorithms on an array of a hundred million elements with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: On my computer, the linear search took 282 milliseconds and binary search took
    0 milliseconds. Also, note that the value we are looking for is expected to be
    quite near to the beginning of the array; in case of values near the middle, a
    binary search would have an even higher advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, so we are convinced that if we have a sorted array, it takes a lot less
    time to find an element in it. But how do we get a sorted array? An arbitrary
    array is unlikely to be sorted. The algorithm of getting a sorted array out of
    an arbitrary array while keeping all the elements same, that is, by only rearranging
    the elements of an input array, is called sorting. There are a lot of algorithms
    for sorting. But in this chapter, we will start with a few simple ones that are
    not efficient. In the next chapter, we will explore efficient sorting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Selection sort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most natural algorithm for sorting. We choose each position of
    the array and find the element in the array that belongs in that position. The
    functional definition of selection sort is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the minimum element in an array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swap this element with the first element of the array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sort the rest of the array after the first element recursively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finding the minimum element has the functional structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the array as a composition of the first element and the rest of the
    array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the index of the minimum element in the rest of the array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare this element with the first element. If this element is smaller than
    the first element, then it is the minimum element in the entire array. Otherwise,
    the first element is the minimum element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of making copies of the array, we represent subarrays by simply storing
    the starting index we want to consider, and then we work on the index recursively.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we write a function to find the index of the minimum element in a given
    array, starting from a position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check whether the start position is the last position in the array,
    in which case we just simply return the start position as there are no more elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We find the index of the minimum element in the array to the right of the current
    start position and compare that element with the current start element. We return
    whichever has the minimum element, as demonstrated in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `swap` function swaps or interchanges two elements in the array at the
    given positions. This function is pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `findMin` and `swap` functions in our repository, we can finally put
    down the `selectionSort` algorithm. We start off by passing the position zero
    as the value of the start parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Firstly, there is no sorting to be done if the array is empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just find the index of the minimum element and swap the current position
    with the index of the minimum position. This will put the minimum element in the
    current position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we recursively sort the rest of the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can write a wrapper function to just do `selectionSort` without having
    to pass a start index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test our code by creating an arbitrary array and then sorting it using
    our algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note how all the elements are repositioned in ascending order, which means that
    the array is sorted.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The form of selection sort shown is not functional in a strict sense because
    we are modifying the contents of an array. A truly functional sort of an array
    will make a copy of the array on every modification. However, this is very expensive.
    On the other hand, thinking of the algorithm in terms of smaller versions of the
    same problem, like we have done, does make the algorithm simpler to understand.
    I tried to hit a sweet spot where I have the simplicity of the recursive algorithm,
    but don't have to keep creating copies of the array.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of the selection sort algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To compute the complexity of the selection sort algorithm, first we have to
    compute the complexity of the `findMin` and `swap` functions. Let''s start with
    the `findMin` function. As with any recursive function, we start with assuming
    that with an array of length *n* (in this case, the effective length of the array,
    starting from the start position), it takes us *T(n)* time to compute the `findMin`
    function. While recursively calling itself, it passes on an effective array of
    length *n-1*. So, we have the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's move on to the `swap` function. It has no recursion and no loops,
    so the complexity is constant or *θ(1)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are ready to compute the complexity of the function `selectionSort`.
    Say, for an effective length *n* of an array, the time taken is *T(n)*. It calls
    itself with effective length *n-1*, it also calls `findMin` and `swap` functions,
    which are *θ(n)* and *θ(1)*, respectively. So, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that some expressions that are *θ (n)* have been written as *θ (n)* itself.
    It should be read as, "*Some function of n that has the asymptotic complexity,
    θ (n)*." It turns out, for computing complexity of *T(n)*, we don''t have to actually
    know the actual expression, we can simply put *Cn* and *D* for functions that
    are *θ (n)* and *θ (1)*, respectively, where *C* and *D* are constants. So, we
    form the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, *T(n-1) - T(n-2) = C(n-1) + D* and so on. If we stack these equations,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding both sides, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of the selection sort algorithm](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, the selection sort has a complexity of *θ(n2)*, where *n* is the size of
    the array being sorted. Now, we will see the next sorting algorithm, which is
    the insertion sort.
  prefs: []
  type: TYPE_NORMAL
- en: Insertion sort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In selection sort, we first selected a position and then found the element
    that should sit there. In the insertion sort, we do the opposite; we first select
    an element and then insert the element into position where it should sit. So,
    for every element, we first find out where it should be and then we insert the
    element in the right place. So, we first see how to insert an element into a sorted
    array. The idea is that we are given an array of sorted elements and we are supposed
    to insert another element in the correct position, so that the resulting array
    remains sorted. We will consider a simpler problem. We are given an array that
    is sorted except for the last element. Our job is to insert the last element in
    the correct position. The recursive way to achieve this insertion is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the element to be inserted is bigger than the end element of the sorted array,
    it belongs in the end and the insertion is complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, we swap the last element with the element to be inserted and recursively
    insert this element in the rest of the smaller array in front of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We do it with the following function. The function takes an array and a position
    that represents this last position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If the last position or the `valueIndex` is `0`, there is nothing to be done,
    as the element is already in the correct position, that is, `0`. There is no array
    to the left of `valueIndex` in this case. If not, we compare the last element
    to the previous element. Since the array on the left is presumed to be sorted,
    the previous element is the largest element in the sorted part of the array. If
    the last element is bigger than even this one, there is nothing more to be done.
    If not, we swap the last element with the previous one and run the insertion recursively
    on the array with one less element. The last element has moved to the previous
    position and it must now be compared with the element before that, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the insertion function available for sorted arrays, we are now ready to
    write the algorithm for an insertion sort. In every step of the insertion sort,
    we consider a boundary in the array. Everything on the left of the boundary has
    already been sorted. Our job in the current step is to insert the element at the
    boundary index into the left sorted array, which we achieve using the `insertElementSorted`
    function. We implement this sort with the following simple strategy. In any step,
    we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first sort the left-hand side of the boundary so that our assumption about
    it being sorted is achieved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we invoke the `insertElementSorted` function to insert the current boundary
    element in the sorted array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, when `boundary` is zero, it means that there is no array to be sorted
    and we simply return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Complexity of insertion sort
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To compute the complexity of insertion sort, we must first compute it for the
    `insertElementSorted` function. Let the time taken for an array of effective length
    (that is, from *zero to boundary-1*), *n* be *T(n)*. From there, we recursively
    call it with *n-1*. So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now assume that the time taken for sorting an array of *n* elements
    is *S(n)*. Apart from the base case, it calls itself with one less argument and
    then calls the `insertElementSorted` function with an array of effective length
    *n-1*. Thus, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, when *n* is large, *T(n) = θ(n)*; hence, it can be approximated by *An*
    where *A* is a constant. So, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is true for all *n*, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Summing both sides, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of insertion sort](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, insertion sort has the same asymptotic complexity as selection sort.
  prefs: []
  type: TYPE_NORMAL
- en: Bubble sort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another interesting sorting algorithm is a bubble sort. Unlike the previous
    algorithms, this one works at a very local level. The strategy is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan through the array, searching pairs of consecutive elements that are ordered
    wrongly. Then find *a j*, such that *array[j+1] < array[j]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever such a pair is found, swap them and continue searching until the end
    of the array and then back from the beginning again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop when a scan through the entire array does not even find a single pair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code that does this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The flag, `sorted`, keeps track of whether any inverted pairs were found during
    a scan. Each iteration of the `while` loop is a scan through the entire array,
    the scan being done inside the `for` loop. In the `for` loop, we are, of course,
    checking each pair of elements, and if an inverted pair is found, we swap them.
    We stop when `sorted` is `true`, that is, when we have not found a single inverted
    pair in the entire array.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see that this algorithm will indeed sort the array, we have to check two
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: When there are no inverted pairs, the array is sorted. This justifies our stopping
    condition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: This is, of course, true because when there are no inverted pairs, we have that
    for all *j< array.length-1*, we have *array[j+1]>=array[j]*. This is the definition
    of an array being in an increasing order, that is, the array being sorted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Irrespective of the input, the program will eventually reach the preceding condition
    after a finite number of steps. That is to say that we need the program to finish
    in a finite number of steps. To see this, we need to understand the concept of
    **inversions**. We will explore them in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inversions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inversion in an array is a pair of elements that are wrongly ordered. The pair
    may be close together or very far apart in the array. For example, take the following
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'How many inversions does the array have? Let us count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In this listing, every element is compared with the elements following it. There
    is an inversion when there is a greater-than sign, highlighted by bold characters.
    Counting the bold ones, we see there are 10 inversions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For any input array, there is a number of inversions. In a sorted array, the
    number of inversions would be zero. Now, think about what happens to the number
    of inversions when a swap is made. A swap interchanges a pair of consecutive elements,
    thus breaking one inversion (the swap happens only when there is an inversion
    between consecutive elements). To see this more clearly, consider the following
    case of a swap between *j* and *j+1* indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Let's take the *j^(th)* element first. Let it have *x* number of inversions
    with the left part of the array. Since these elements are on the left, all inversions
    of this type are with elements greater than the *j^(th)* element. When the *j^(th)*
    element moves to the *(j+1)^(th)* position, they still remain to the left, and
    the only element added to the left of the *j^(th)* element is the element it is
    swapped with. Hence, no changes to the number of inversion happens to the *j^(th)*
    element, other than the one due to the *(j+1)^(th)* element. The same logic can
    be applied to the inversions with it in the right part of the array, and also
    to both sides of the array for the *(j+1)^(th)* element. Because of the swap,
    one inversion is broken between *j^(th)* and *(j+1)^(th)* elements. Hence, the
    number of inversions reduce by exactly one in each inversion. This means the number
    of swaps in bubble sort would be exactly equal to the number of inversions in
    the input array, which is finite. And since each scan through the array requires
    a swap in the previous scan, the total number of scans is at most one more that
    the number of swaps; this is finite too. This makes sure that the algorithm always
    finishes.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of the bubble sort algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand the complexity of a bubble sort, we have to count the number of
    steps. Is the number of steps equal to the number of swaps? The answer is, not
    really. In case of asymptotic analysis, we must always count the step that happens
    a maximum number of times. In this case, that step is comparison. How many comparisons
    are there per scan of the array? *n-1* of course. So, now the analysis of complexity
    is reduced to the number of scans we need to sort the array.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what happens to the maximum element after the first scan. Let's say
    the maximum element is at the index *j*. So, it will be compared with the element
    at *j+1*. For simplicity, let's assume that all elements are different. Now, since
    it is the maximum element, the element at the *j+1* position will be less than
    it, and hence it will be swapped. Now the maximum element is at the position,
    *j+1*, and is being compared with the element at position, *j+2*, and the same
    thing happens. It will continue until the maximum element is at the end of the
    array. If the elements are not unique, the same will happen to the rightmost maximum
    element. In the next cycle, the maximum element will already be at the end of
    the array, and we will hit the second maximum (or another maximum element) somewhere
    in the array. Now, since one maximum element is at the end of the array, we can
    think of the rest of the array apart from the last element. In this array, the
    current maximum is the maximum and it will reach the end of the current part of
    the array at the end of the current scan.
  prefs: []
  type: TYPE_NORMAL
- en: This shows that at the end of each scan, at least one element reaches the correct
    final position without altering the correct positions of the ones that got there
    before the scan, which means that at the end of *n* scans, all of the elements
    would be in the correct position and the array would be sorted. That is to say
    that after at most *n* scans, the bubble sort would be complete. In each of those
    scans, there are *O(n)* operations. So, the worst case complexity of bubble sort
    is *O(n2).*
  prefs: []
  type: TYPE_NORMAL
- en: This is not the end of this analysis; we still have to show that there is a
    case that takes that many steps, and only then can we have a theta bound on the
    worst case. We take the case where all the elements are sorted in the opposite
    order, that is, they are in a decreasing order and are all distinct. In such a
    case, every element has an inversion with all the others. This means that each
    one of the *n* elements have an inversion with *n-1* other elements, that is,
    *n(n-1)* inversions in total. But since each inversion would be counted twice,
    once from each of the elements that are members of the inversion, it is actually
    *n(n-1)/2*. Now, note that the maximum number of swaps that can be done in one
    scan is *n-1*, which will happen if every comparison results in a swap because
    there are *n-1* comparisons per scan. So, we will need at least *(n(n-1)/2)/(n-1)
    = n/2* scans to complete all the swaps, each requiring *n-1* comparisons. So,
    the complexity is at least *n(n-1)/2 = Ω(n2)*. Of course then, the worst case
    is at least this much complex because the worst case is, by definition, the most
    complex case.
  prefs: []
  type: TYPE_NORMAL
- en: So, the worst case is both *O(n2)* and *Ω(n2)*, that is to say that it is *θ(n2)*.
  prefs: []
  type: TYPE_NORMAL
- en: A problem with recursive calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A problem with recursive calls is that they are expensive; a method invocation
    entails considerable overhead on the processor. It is, in general, better to avoid
    invoking methods if you want to improve performance to the last bit. On top of
    that, there is a limit to the depth of function calls that you can go to, before
    the program breaks. This is because a program has a stack to enable method invocation
    semantics, that actually gets a new element containing all variables and the position
    of the current instruction to the stack. This stack does not grow indefinitely,
    but instead is fixed in size; usually, it can hold a few thousand values, which
    means that if your method invocation is deeper than that, it will break and the
    program will exit with an error. This means that our insertion sort will break
    for an array containing more than a few thousand entries. On the other hand, it
    is generally easier to explain an algorithm in a functional form. To balance between
    these two aspects, we need to be able to convert to and from the recursive and
    non-recursive versions of the same algorithm. We will do this step by step from
    the simplest form to the more complicated form.
  prefs: []
  type: TYPE_NORMAL
- en: Tail recursive functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A recursive function is called a tail recursive function if all the recursive
    calls to itself in the function are the last operations. I say it like there are
    multiple calls and all of them must be the last operations. How is that possible?
    I mean there can be different calls from different conditional branches of the
    code inside the function. However, whenever the function calls itself that must
    be the last operation in that conditional branch of the function. For example,
    take our binary search algorithm again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that the function calls itself in two different conditional branches. However,
    in each branch, the recursive call is the last operation. There is nothing to
    be done after the call to itself. This is a tail recursive function.
  prefs: []
  type: TYPE_NORMAL
- en: Tail recursive functions can be turned into a loop absolutely mechanically.
    In fact, all functional language compilers do this automatically during compiler
    optimization. The Java compiler, however, does not do this because Java generally
    prefers loops over recursion in the code, at least until very recently. But we
    can do this conversion by ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is that since there are no more operations after the recursive call,
    the program does not have to remember the values of the variables of the calling
    function. So, they can simply be overwritten by the values of the same variables
    of the called function instead, and we just have to process the code of the function
    again. So, the following is the mechanical procedure to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Wrap the entire content in an infinite `while` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Replace all recursive calls by updating the values of the parameters to the
    values that are passed in the recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows this update in the binary search algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note that we updated only those arguments that changed, which is only one update
    per branch in this case. This will produce the exact same result as the earlier
    function, but now it would not cause a stack overflow. This conversion is not
    really required in case of a binary search though, because you need only *lg n*
    steps to search an array of length *n*. So, if your allowed depth of invocation
    is *1000*, then you can search in an array of maximum size of *21000* elements.
    This number is way more than the total number of atoms in the entire universe,
    and hence we will never be able to store an array of that enormous size. But the
    example shows the principle of converting a tail recursion into a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is the `insertElementSorted` function, used in our insertion
    sort algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that there is no operation pending after the recursive call to itself.
    But we need to be a little more careful here. Note that the invocation only happens
    inside a code branch. The else case is implicit here, which is `else { return;
    }`. We need to make it explicit in our code first, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use our old technique to make it non-recursive, that is, to wrap
    it in an infinite loop and replace recursive calls with argument updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the exact same result as the previous recursive version of the function.
    So, the corrected steps would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, make all implicit branches explicit and all implicit returns explicit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap the entire content in an infinite while loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace all recursive calls by updating the values of the parameters to the
    values that are passed in the recursive calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-tail single recursive functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By single recursion, I mean that the function invokes itself at most once per
    conditional branch of the function. They may be tail-recursive, but they are not
    always so. Consider the example of the recursion of our insertion sort algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the function calls itself only once, so it is a single recursion.
    But since we have a call to `insertElementSorted` after the recursive call to
    itself, it is not a tail recursive function, which means that we cannot use the
    earlier method. Before doing this though, let''s consider a simpler example. Take
    the factorial function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: First, note that the function is singly recursive, because there is at most
    one recursive call per branch of the code. Also, note that it is not tail recursive
    because you have to do a multiplication after the recursive call.
  prefs: []
  type: TYPE_NORMAL
- en: To convert this into a loop, we must first figure out the actual order of the
    numbers being multiplied. The function calls itself until it hits `0`, at which
    point, it returns `1`. So, the multiplication actually starts from `1` and then
    accumulates the higher values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since it accumulates the values on its way up, we need an accumulator (that
    is a variable storing one value) to collect this value in a loop version. The
    steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, make all implicit branches explicit and all implicit returns explicit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an accumulator of the same type as the return type of the function. This
    is to store intermediate return values. The starting value of the accumulator
    is the value returned in the base case of the recursion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the starting value of the recursion variable, that is, the one that is
    getting smaller in each recursive invocation. The starting value is the value
    that causes the next recursive call to fall in the base case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The exit value of the recursion variable is the same as the one passed to the
    function originally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a loop and make the recursion variable your loop variable. Vary it from
    the start value to the end value calculated earlier in a way to represent how
    the value changes from higher depth to lower depth of recursion. The higher depth
    value comes before the lower depth value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the recursive call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What is the initial value of the accumulator `prod`? It is the same as the
    value that is returned in the exit branch of the recursion, that is, `1`. What
    is the highest value being multiplied? It is `x`. So we can now convert it to
    the following loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s consider the `insertionSort` algorithm. What is the accumulator?
    It is the same thing that would be the final output, that is, an array of sorted
    elements. What is the starting value? It is the same that is returned in the recursive
    version in the exit branch. This is an array of length zero. What is the final
    value? The array of sorted elements of the length provided to sort. Again, just
    like our recursive version, we represent these partial arrays with, simply, a
    boundary value. So, the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Note that in this case, the function `return` type is `void`. But what we are
    really returning is the sorted array; we just resorted to modifying the same array
    to avoid creating duplicate arrays.
  prefs: []
  type: TYPE_NORMAL
- en: The most general case is the multiple recursion, that is, the function calls
    itself multiple times in the same conditional branch of the function. This case
    cannot be done without a stack. In case of a recursive call, we use the method-invocation
    stack. Otherwise, we can even use an external stack. Since we do not have such
    an example in this chapter, we defer its explanation to the next chapter, where
    we will have an example.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to do an efficient search in an ordered array. This
    search is called a binary search. You also learned some methods of obtaining an
    ordered array out of an unordered one. This process is called sorting. We saw
    three basic algorithms of sorting. Although they are not particularly efficient,
    they are simple to understand the concept. You also learned how to convert a recursive
    algorithm to a non-recursive one that uses a loop. In the next chapter, we will
    see efficient sorting algorithms.
  prefs: []
  type: TYPE_NORMAL
