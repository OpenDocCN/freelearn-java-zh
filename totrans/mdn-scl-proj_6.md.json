["```java\ntrait AirlineWrapper {  }\n```", "```java\nlazy val session = { SparkSession.builder()..getOrCreate() }\n```", "```java\ncase class AirlineCarrier(uniqueCarrierCode: String)\n```", "```java\ncase class Flight(monthOfFlight: Int, /* Number between 1 and 12 */\n                 dayOfFlight: Int, /*Number between 1 and 31 */\n                 uniqueCarrierCode: String,\n                 arrDelay: Int, /* Arrival Delay - Field # 15*/\n                 depDelay: Int, /* Departure Delay - Field # 16 */\n                 originAirportCodeOfFlight: String, /* An identification number assigned by US DOT to identify a unique airport. */\n                destAirportCodeOfFlight: String, /* An identification number assigned by US DOT to  identify a unique airport.*/\n                carrierDelay: Int, /* Field # 25*/\n                weatherDelay: Int, /* Field # 26*/\n                lateAircraftDelay: Int /* Field # 29*/\n)  \n```", "```java\ncase class Airports(iataAirportCode: String, airportCity: String, airportCountry: String)\n```", "```java\nval airportsData: String = loadData(\"data/airports.csv\") \n```", "```java\nval carriersData: String = loadData(\"data/airlines.csv\") \n```", "```java\nval faaFlightsData: String = loadData(\"data/faa.csv\")\n```", "```java\ndef loadData(dataset: String) = {\n                     //Get file from resources folder\n                    val classLoader: ClassLoader = getClass.getClassLoader\n                    val file: File = new File(classLoader.getResource(dataset).getFile)\n                    val filePath = file.getPath\n                    println(\"File path is: \" + filePath)\n                   filePath\n } \n```", "```java\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.SparkConf\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.rdd.RDD\n```", "```java\ndef buildDataFrame(dataSet: String): RDD[Array[String]] = {\n//def getRows2: Array[(org.apache.spark.ml.linalg.Vector, String)] = {\ndef getRows2: RDD[Array[String]] = {session.sparkContext.\ntextFile(dataSet).flatMap {\n                        partitionLine =>  \n                        partitionLine.split(\"\\n\").toList\n                        }.map(_.split(\",\"))\n } \n   //Create a dataframe by transforming an Array of a tuple of Feature \n   Vectors and the Label\n   val dataFrame = session.createDataFrame(getRows2).\n   toDF(bcwFeatures_IndexedLabel._1, bcwFeatures_IndexedLabel._2)\n   //dataFrame\n   //val dataFrame = session.createDataFrame(getRows2)\n   getRows2\n}\n```", "```java\n/*\nImport the MongoDB Connector Package\n*/\n\nimport com.mongodb.spark._\nimport com.mongodb.spark.config._\nimport org.bson.Document\n```", "```java\nobject Aircraft extends AirlineWrapper {\n```", "```java\ndef main(args: Array[String]): Unit = {\n\n}\n```", "```java\nobject Aircraft extends AirlineWrapper {\n      def main(args: Array[String]): Unit = {\n\n}\n\n}\n```", "```java\ncase class FlightsData(flightYear: String, /* 1 */ \nflightMonth : String, /* 2 */ \nflightDayOfmonth : String, /* 3 */ \nflightDayOfweek : String, /* 4 */ \nflightDepTime : String, /* 5 */ \nflightCrsDeptime : String, /* 6 */ \nflightArrtime : String, /* 7 */ \nflightCrsArrTime : String, /* 8 */ \nflightUniqueCarrier : String,/* 9 */ \nflightNumber : String, /* 10 */ \nflightTailNumber : String, /* 11 */ \nflightActualElapsedTime : String, /* 12 */ \nflightCrsElapsedTime : String, /* 13 */ \nflightAirTime : String, /* 14 */ \nflightArrDelay : String, /* 15 */ \nflightDepDelay : String, /* 16 */ \nflightOrigin : String, /* 17 */ \nflightDest : String, /* 18 */ \nflightDistance : String, /* 19 */ \nflightTaxiin : String, /* 20 */\nflightTaxiout : String, /* 21 */ \nflightCancelled : String, /* 22 */ \nflightCancellationCode : String, /* 23 */ \nflightDiverted : String, /* 24 */ \nflightCarrierDelay : String, /* 25 */ \nflightWeatherDelay : String, /* 26 */ \nflightNasDelay : String, /* 27 */ \nflightSecuritDelay : String, /* 28 */ \nflightLateAircraftDelay : String, /* 29 */ record_insertion_time: String, /* 30 */ uuid : String /* 31 */\n                    )\n```", "```java\nval airFrame: DataFrame = session.read .format(\"com.databricks.spark.csv\") .option(\"header\", true).option(\"inferSchema\", \"true\").option(\"treatEmptyValuesAsNulls\", true) .load(\"2008.csv\")\n```", "```java\nprintln(\"The schema of the raw Airline Dataframe is: \")\nairFrame.printSchema()\n```", "```java\nimport org.apache.spark.sql.functions._\n```", "```java\nairFrame.createOrReplaceTempView(\"airline_onTime\")\n```", "```java\nprint(\"size of one-time dataframe is: \" + airFrame.count())\n```", "```java\nairFrame.createOrReplaceTempView(\"airline_ontime\")\nprint(\"size of one-time dataframe is: \" + airFrame.count()) \n```", "```java\nairFrame.createOrReplaceTempView(\"airline_ontime\") \nprint(\"size of one-time dataframe is: \" + airFrame.count()) \n```", "```java\nval airFrameJSON: Dataset[String] = clippedAirFrameForDisplay.toJSON\n```", "```java\nprintln(\"Airline Dataframe as JSON is: \")\nairFrameJSON.show(10) \n```", "```java\nairFrameJSON.rdd.saveAsTextFile(\"json/airlineOnTimeDataShort.json.gz\", classOf[org.apache.hadoop.io.compress.GzipCodec]) \n```", "```java\nclippedAirFrameForDisplay.write.format(\"parquet\").save(\"parquet/airlineOnTimeDataShort.parquet\")\n```", "```java\nval airlineOnTime_Json_Frame: DataFrame = session.read.json(\"json/airlineOnTimeDataShort.json.gz\")\nprintln(\"JSON version of the Airline dataframe is: \")\nairlineOnTime_Json_Frame.show()\n```", "```java\nval airlineOnTime_Parquet_Frame: DataFrame = session.read.format(\"parquet\").load(\"parquet/airlineOnTimeDataShort.parquet\") \n```", "```java\n println(\"Parquet version of the Airline dataframe is: \")\n airlineOnTime_Parquet_Frame.show(10) \n```", "```java\nMongoSpark.save( airlineOnTime_Parquet_Frame.write.option(\"collection\", \"airlineOnTimeData\").mode(\"overwrite\") )\n```"]