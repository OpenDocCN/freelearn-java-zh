<html><head></head><body>
		<div id="_idContainer035">
			<h1 id="_idParaDest-27"><em class="italic"><a id="_idTextAnchor028"/>Chapter 2</em>: JIT, HotSpot, and GraalJIT</h1>
			<p><a id="_idTextAnchor029"/>In the previous chapter, we learned about C1 and C2 compilers and the kind of code optimizations and de-optimizations that C2 compilers perform at runtime.</p>
			<p>In this chapter, we will deep dive into the C2 just-in-time compilation and introduce Graal's just-in-time compilation. <strong class="bold">Just-In-Time</strong> (<strong class="bold">JIT</strong>) compilation is one of the key innovations that enabled Java to compete with traditional <strong class="bold">ahead-of-time</strong> (<strong class="bold">AOT</strong>) compilers. As we learned in the previous chapter, JIT compilation evolved with C2 compilers in JVM. The C2 JIT compiler constantly profiles code execution and applies various optimizations and de-optimizations at runtime to compile/recompile the code.</p>
			<p>This chapter will be a hands-on session, where we will take a sample code and analyze how the C2 JIT compiler works and introduce Graal JIT.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understand how the JIT compiler works</li>
				<li>Learn how code is optimized by JIT by identifying HotSpots</li>
				<li>Use profiling tools to demonstrate how the JIT compiler works</li>
				<li>Understand how GraalVM JIT works on top of JVM JIT</li>
			</ul>
			<p>By the end of this chapter, you will have a clear understanding of the internal workings of the JIT compiler and how GraalVM extends it further. We will be using sample Java code and profiling tools such as JITWatch to gain a deeper understanding of how JIT works.</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor030"/>Technical requirements</h1>
			<p>To follow the instructions given in this chapter, you will require the following:</p>
			<ul>
				<li>All the source code referred to in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Supercharge-Your-Applications-with-GraalVM/tree/main/Chapter02">https://github.com/PacktPublishing/Supercharge-Your-Applications-with-GraalVM/tree/main/Chapter02</a>.</li>
				<li>Git (<a href="https://github.com/git-guides/install-git">https://github.com/git-guides/install-git</a>)</li>
				<li>Maven (<a href="https://maven.apache.org/install.html">https://maven.apache.org/install.html</a>)</li>
				<li>OpenSDK (<a href="https://openjdk.java.net/">https://openjdk.java.net/</a>) and JavaFX (<a href="https://openjfx.io/">https://openjfx.io/</a>)</li>
				<li>JITWatch (<a href="https://www.jrebel.com/blog/understanding-java-jit-with-jitwatch#:~:text=JITWatch%20is%20a%20log%20analyser,to%20the%20Adopt%20OpenJDK%20initiative">https://www.jrebel.com/blog/understanding-java-jit-with-jitwatch#:~:text=JITWatch%20is%20a%20log%20analyser,to%20the%20Adopt%20OpenJDK%20initiative</a>)</li>
				<li>The Code in Action video for this chapter can be found at <a href="https://bit.ly/3w7uWlu">https://bit.ly/3w7uWlu</a>. </li>
			</ul>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor031"/>Setup environment</h1>
			<p>In this section, we will set up all the prerequisite tools and environments that are required to follow on with the rest of the chapter.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor032"/>Installing OpenJDK Java</h2>
			<p>You can install OpenJDK from <a href="https://openjdk.java.net/install/">https://openjdk.java.net/install/</a>. This URL has detailed instructions on how to install <a id="_idIndexMarker097"/>OpenJDK. We also require JavaFX. Please refer to <a href="https://openjfx.io/">https://openjfx.io/</a> for more details on how to install JavaFX.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor033"/>Installing JITWatch</h2>
			<p>JITWatch is one of the most <a id="_idIndexMarker098"/>widely used log analysis and visualization tools for understanding the behavior of the JIT compiler. This is also widely used in analyzing the code and identifying opportunities for better performance tuning.</p>
			<p>JITWatch is an active open source project hosted at <a href="https://github.com/AdoptOpenJDK/jitwatch">https://github.com/AdoptOpenJDK/jitwatch</a>.</p>
			<p>The typical commands for installing JITWatch are as follows:</p>
			<p class="source-code">git clone git@github.com:AdoptOpenJDK/jitwatch.git</p>
			<p class="source-code">cd jitwatch</p>
			<p class="source-code">mvn clean install -DskipTests=true</p>
			<p class="source-code">./launchUI.sh</p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor034"/>Taking a deep dive into HotSpot and the C2 JIT compiler</h1>
			<p>In the previous chapter, we<a id="_idIndexMarker099"/> walked through the evolution of JVM and how the C2 JIT compiler evolved. In this section, we will dig deeper into the JVM C2 JIT compiler. Using sample code, we will go through the optimizations that the JIT compiler performs at runtime. To appreciate the Graal JIT compiler, it is very important to understand how the C2 JIT compiler works.</p>
			<p>Profile-guided optimization is the key principle for JIT compilers. While AOT compilers can optimize the static code, most of the time, that is just not good enough. It's important to understand the runtime characteristics of the application to identify opportunities for optimization. JVM has a built-in profiler that dynamically instruments the application to profile some key parameters and to identify opportunities for optimizations. Once identified, it will compile that code to the native language and switch from running the interpreted code to faster-compiled code. The optimizations are based on profiling and educated assumptions that are made by JVM. If any of these assumptions are<a id="_idIndexMarker100"/> incorrect, JVM will de-optimize and switch back to running interpreted code. This is called <strong class="bold">Mixed Mode Execution</strong>.</p>
			<p>The following diagram shows a flow of how JVM performs the profile-guided optimizations and switches between different modes of execution:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B16878_Figure_2.01.jpg" alt="Figure 2.1 – JIT compilation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – JIT compilation</p>
			<p>The Java source code (<strong class="source-inline">.java</strong>) is <a id="_idIndexMarker101"/>compiled to bytecode (<strong class="source-inline">.class</strong>), which is an intermediate representation of the code. JVM starts to run the bytecode with the inbuilt interpreter. The interpreter uses a bytecode to machine code mapping, converts the bytecode instructions to machine code one statement at a time, and then executes it.</p>
			<p>While JVM executes these instructions, it also tracks the number of times a method is invoked. When the method invocation count of a particular method exceeds the compiler threshold, it spins off a compiler to compile that method on a separate compilation thread. There are two <a id="_idIndexMarker102"/>types of compilers that JVM uses to compile the code: C1 (client) and C2 (server) JIT compilers. The compiled code is stored in the code cache, so that the next time that method is invoked, JVM will execute the code from the code cache instead of interpreting it. JIT compilers perform various optimizations to the code, and hence, over time, the application performs better. The rest of this section will walk through these various components in detail.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor035"/>Code cache</h2>
			<p>The code cache is an area in JVM where JVM stores the compiled native methods (also referred to as <strong class="source-inline">nmethod</strong>). The code <a id="_idIndexMarker103"/>cache is set to a static size and it might become full after a while. Once the code cache is full, JVM cannot compile or store any more code. It is very important to tune the code cache for optimum performance. Four key parameters help us to fine-tune JVM performance, with the optimum code cache:</p>
			<ul>
				<li><strong class="source-inline">-XX:InitialCodeCacheSize</strong>: The initial size of the code cache. The default size is 160 KB (varies based on the JVM version).</li>
				<li><strong class="source-inline">-XX:ReservedCodeCacheSize</strong>: The maximum size the code cache can grow to. The default size is 32/48 MB. When the code cache reaches this limit, JVM will throw a warning: <strong class="source-inline">CodeCache is full. Compiler has been disabled.</strong>. JVM offers the <strong class="source-inline">UseCodeCacheFlushing</strong> option to flush the code cache when the code cache is full. The code cache is also flushed when the compiled code is not hot enough (when the counter is less than the compiler threshold).</li>
				<li><strong class="source-inline">-XX:CodeCacheExpansionSize</strong>: This is the expansion size when it scales up. Its default value is 32/64 KB.</li>
				<li><strong class="source-inline">-XX:+PrintCodeCache</strong>: This option can be used to monitor the usage of the code cache. </li>
			</ul>
			<p>Since Java 9, JVM segments the code cache into three segments:</p>
			<ul>
				<li><strong class="bold">Non-native method segment</strong>: This segment contains the JVM internal code (such as the bytecode interpreter). The default size of this segment is 5 MB. This can be changed using the <strong class="source-inline">-XX:NonNMethodCodeHeapSize</strong> flag.</li>
				<li><strong class="bold">Profiled code</strong>: This segment contains the compiled profiled code. This code is not completely optimized and has instrumentation that the profiler uses to optimize the code further. The default size is 122 MB. This can be changed using the <strong class="source-inline">-XX:ProfiledCodeHeapSize</strong> flag.</li>
				<li><strong class="bold">Non-profiled code</strong>: This is the fully optimized<a id="_idIndexMarker104"/> code, where even the instrumentation is removed. This can be changed using the <strong class="source-inline">-XX:NonProfiledCodeHeapSize</strong> flag.</li>
			</ul>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor036"/>Compiler threshold</h2>
			<p>The compilation threshold is a factor<a id="_idIndexMarker105"/> that helps JVM decide when to perform JIT compilations. When JVM detects that a method execution has reached  a compilation threshold, JVM will instigate the appropriate compiler to perform the compilation (more on this later in this section, where we will walk through the various types of JIT compilers and tiered compilation).</p>
			<p>Deciding the compilation threshold is based on two key variables. These variables come with a default value for each JVM, but can also be changed with appropriate command-line arguments. These variables are very critical in tuning the performance of JVM and should be used carefully. These two variables are as follows:</p>
			<ul>
				<li><strong class="bold">Method invocation counter</strong>: This counts the number of times a particular method is invoked.</li>
				<li><strong class="bold">Loop counter</strong>: This refers to the number of times a particular loop has completed execution (what is referred to as branching back). Sometimes, this is also referred to as Backedge Threshold or Backedge Counter.</li>
			</ul>
			<p>JVM profiles these two variables at runtime and, on this basis, decides whether that method/loop needs to be compiled. When a compilation threshold is reached, JVM spins off a compilation thread to compile that particular method/loop.</p>
			<p>The compilation threshold can be changed using the <strong class="source-inline">-XX:CompilationThreshold=N</strong> flag as an argument while executing the code. The default value of <strong class="source-inline">N</strong> is <strong class="source-inline">1500</strong> for the client compiler and <strong class="source-inline">10000</strong> for the server compiler.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor037"/>On-stack replacement</h2>
			<p>The methods that reach the compilation threshold are compiled by the JIT compilers, and the next time the method is called, the compiled machine code is called. This improves performance over time. However, in cases of long-running loops that reach the loop counter threshold (Backedge Threshold), the <a id="_idIndexMarker106"/>compilation thread initiates code compilation. Once the code that is in the loop is compiled, the execution is stopped and resumed with the compiled code frame. This process is called <strong class="bold">On-Stack Replacement (OSR)</strong>. Let's look at the following example.</p>
			<p>The following code snippet just discusses how OSR works. To keep it simple, the code simply shows a long-running loop where we are just calculating the total number of times the loop runs. In this case, the <strong class="source-inline">main()</strong> method is never entered, so even after the compilation threshold is reached and the code is compiled, the compiled code cannot be used as the interpreter continues to execute, unless the code is replaced. This is where OSR helps in optimizing such code:</p>
			<p class="source-code">public class OSRTest {</p>
			<p class="source-code">    public static void main(String[] args) {</p>
			<p class="source-code">        int total = 0;</p>
			<p class="source-code">        //long running loop</p>
			<p class="source-code">        for(int i=0; i &lt; 10000000; i++) {</p>
			<p class="source-code">            </p>
			<p class="source-code">            //Perform some function</p>
			<p class="source-code">            total++;</p>
			<p class="source-code">        }</p>
			<p class="source-code">        System.out.println("Total number of times is "+ total);</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>The following<a id="_idIndexMarker107"/> flowchart shows how the OSR works in this case:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B16878_Figure_2.02.jpg" alt="Figure 2.2 – OSR flowchart&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – OSR flowchart</p>
			<p>Let's see how this works:</p>
			<ol>
				<li>The interpreter starts executing the code.</li>
				<li>When the compiler threshold is reached, JVM spins off a compiler thread to compile the method. In the meantime, the interpreter continues to execute the statement.</li>
				<li>Once the compilation<a id="_idIndexMarker108"/> thread comes back with the compiled code (compilation frame), JVM checks whether the interpreter is still executing the code. If the interpreter is still executing the code, it will pause and perform OSR, and execution starts with the compiled code.</li>
			</ol>
			<p>When we run this code with the <strong class="source-inline">-XX:PrintCompilation</strong> flag on, this is the output that shows that JVM performed OSR (the % attribute indicates that it performed OSR):</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B16878_Figure_2.03.jpg" alt="Figure 2.3 – OSR log screenshot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – OSR log screenshot</p>
			<p>Please refer to the next section to understand the log format in detail.</p>
			<h3>XX:+PrintCompilation</h3>
			<p><strong class="source-inline">XX:+PrintCompilation</strong> is a very powerful<a id="_idIndexMarker109"/> argument that can be passed to understand how the JIT compilers are kicking in and optimizing the code. Before we run our code with this argument, let's first understand the output format.</p>
			<p><strong class="source-inline">XX:+PrintCompilation</strong> produces a log list of parameters separated by blank spaces in the following format:</p>
			<p class="source-code">&lt;Timestamp&gt; &lt;CompilationID&gt; &lt;Flag&gt; &lt;Tier&gt; &lt;ClassName::MethodName&gt; &lt;MethodSize&gt; &lt;DeOptimization Performed if any&gt;</p>
			<p>Here is an example snapshot of the output:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16878_Figure_2.04.jpg" alt="Figure 2.4 – Print compilation log format&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Print compilation log format</p>
			<p>Let's look at what these <a id="_idIndexMarker110"/>parameters mean:</p>
			<ul>
				<li><strong class="source-inline">Timestamp</strong>: This is the time in milliseconds since JVM started.</li>
				<li><strong class="source-inline">CompilationID</strong>: This is an internal identification number used by JVM in the compilation queue. This will not necessarily be in a sequence, as there are background compilation threads that might reserve some of the IDs.</li>
				<li><strong class="source-inline">Flags</strong>: The compiler flags are very important parameters that are logged. This suggests what attributes are applied by the compiler. JVM prints a comma-separated string of five possible characters to indicate five different attributes that are applied to the compiler. If none of the attributes are applied, it is shown as a blank string. The five attributes are as follows:<p>a. <strong class="bold">On-Stack Replacement</strong>: This is represented by the <strong class="source-inline">%</strong> character. OSR is explained earlier in this section. This attribute suggests that an OSR compilation is triggered as the method is looping over a large loop.</p><p>b. <strong class="bold">Exception Handler</strong>: This is represented by the <strong class="source-inline">!</strong> character. This indicates that the method has an exception handler.  </p><p>c. <strong class="bold">Synchronized method</strong>: This is represented by the <strong class="source-inline">s</strong> character. This indicates that the method is synchronized.  </p><p>d. <strong class="bold">Blocking Mode</strong>: This is represented by the <strong class="source-inline">b</strong> character. This indicates that the compilation occurred in blocking mode. This means that the compilation did not happen in the background.</p><p>e. <strong class="bold">Native</strong>: This is represented by the <strong class="source-inline">n</strong> character. This indicates that the code is compiled to the native method.</p></li>
				<li><strong class="source-inline">Tier</strong>: This indicates which tier of compilation is performed. Refer to the <em class="italic">Tiered compilation</em> section for more details.</li>
				<li><strong class="source-inline">MethodName</strong>: This column lists the<a id="_idIndexMarker111"/> method that is being compiled.</li>
				<li><strong class="source-inline">MethodSize</strong>: This is the size of the method.</li>
				<li><strong class="source-inline">Deoptimization performed</strong>: This shows any de-optimizations that may be performed. We will discuss this in detail in the next section.</li>
			</ul>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor038"/>Tiered compilation</h2>
			<p>In the previous chapter, we <a id="_idIndexMarker112"/>briefly covered compilation tiers/levels. In this section, we will go into more details. Client compilers kick in early when the compiler threshold is reached. The server compiler kicks in based on the profiling. The recent versions of JVM use a combination of both compilers for optimum performance. However, the user can specifically use one of the compilers with the <strong class="source-inline">-client</strong>, <strong class="source-inline">-server</strong>, or <strong class="source-inline">-d64</strong> arguments. The default behavior of JVM is to use tiered compilation, which is the most optimum JIT compilation. With tiered compilation, the code is first compiled by the client compiler and, based on the profiling, if the code gets hotter (hence the name HotSpot), the server compiler kicks in and recompiles the code. This process was explained in the previous section by means of the flowchart.</p>
			<p>Tiered compilation brings in more optimization as the code gets complicated and runs for longer. There are instances where JIT compilation runs more optimally and faster than AOT compilation. While AOT compilation brings in optimization, during the build phase, it does not have the intelligence to optimize/deoptimize itself based on runtime profiling. Runtime profiling, optimizing, and deoptimizing are the key advantages of JIT compilation.</p>
			<p>There are three versions of JIT compilers:</p>
			<ul>
				<li><strong class="bold">Client 32-bit</strong>: The 32-bit client<a id="_idIndexMarker113"/> version is for applications that we are running on 32-bit operating systems. For a 64-bit operating system, both 32-bit and 64-bit versions of the JIT compilers can be used. Typically, 32-bit versions (both client and server) are ideal for smaller heap sizes (smaller footprint). This <a id="_idIndexMarker114"/>version of the compiler can be explicitly invoked using the <strong class="source-inline">-client</strong> argument:<p class="source-code"><strong class="bold">java -client -XX:+PrintCompilation &lt;Class File&gt;</strong></p><p><strong class="source-inline">-XX:PrintCompilation</strong> logs the compilation process to the console. This helps in understanding how the compiler is working.</p></li>
				<li><strong class="bold">Server 32-bit</strong>: The 32-bit server <a id="_idIndexMarker115"/>version JIT compiler is ideal for 32-bit operating<a id="_idIndexMarker116"/> systems and applications that have a smaller footprint and do not perform extensive operations on <strong class="source-inline">long</strong> or <strong class="source-inline">double</strong> variables. This version of the compiler can be explicitly invoked using the <strong class="source-inline">-server</strong> argument.</li>
				<li><strong class="bold">Server 64-bit</strong>: The 64-bit server <a id="_idIndexMarker117"/>version JIT compiler is for 64-bit operating systems and is ideal for large applications. They have a larger footprint and are not as fast as 32-bit compilers. However, 64-bit compilers can perform faster and better. This version of the compiler can be explicitly invoked using the <strong class="source-inline">-d64</strong> argument.</li>
			</ul>
			<p>Server compilers are up to 4x slower in compiling than client compilers. However, they do generate a faster running application (up to 2x).</p>
			<p>There are five tiers/levels of compilation levels as listed next. A compilation log can be used to find which method was compiled to what level, by means of compilation print:</p>
			<ul>
				<li><strong class="bold">Level 0 – Interpreted code</strong>: This the standard<a id="_idIndexMarker118"/> interpreter mode, where the JIT is still not activated. The JIT gets activated based on the compilation threshold.</li>
				<li><strong class="bold">Level 1 – Simple C1 compiled code</strong>: This is a <a id="_idIndexMarker119"/>basic no-profile compilation of the code. The compiled code will not have any instrumentation.</li>
				<li><strong class="bold">Level 2 – Limited C1 compiled code</strong>: In this level, basic counters are instrumented. The counter will help JVM decide to<a id="_idIndexMarker120"/> move to the next level, L2. Sometimes, when the C2 compiler is busy, JVM will use this level as an intermediate before promotion to Level 3.</li>
				<li><strong class="bold">Level 3 – Full C1 compiled code</strong>: In this level, the code is fully instrumented and profiled. This <a id="_idIndexMarker121"/>detailed profiling will help decide further optimization with L4. This level adds up to 25-30% of overhead to the compiler and performance.</li>
				<li><strong class="bold">Level 4 – C2 compiled code</strong>: This is the <a id="_idIndexMarker122"/>most optimized compilation of the code, where<a id="_idIndexMarker123"/> all the optimization is applied. However, while profiling, if JVM finds that the context of optimization has changed, it will deoptimize and replace the code with L0 or L1 (for trivial methods).</li>
			</ul>
			<p>Let's now look at how the Java HotSpot compiler performs tiered compilation. The following diagram shows the various tiers and flow patterns of compilation:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B16878_Figure_2.05.jpg" alt="Figure 2.5 – Tiered compilation patterns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Tiered compilation patterns</p>
			<p>Let's understand what each <a id="_idIndexMarker124"/>flow indicates:</p>
			<ul>
				<li><strong class="bold">A</strong>: This is the normal pattern of how JVM works. All the code starts with L0 and escalates to L3 when the compilation threshold is reached. At L3, the code is compiled with complete detailed profiling instrumentation. The code is then profiled at runtime, when it reaches the threshold, and then the code is re-compiled with the C2 compiler (L4), with maximum optimization. C2 compilers require detailed data regarding the control flow so as to take decisions concerning optimization. Later in this section, we will walk through all the optimizations that the C2 compiler (JIT) performs. It is possible, however, that the optimizations are invalid, due to changes in the flows or the context of optimization. In this case, JVM will deoptimize and bring it back to L0.</li>
				<li><strong class="bold">B: C2 Busy</strong>: C2 compilation is performed on a separate compilation thread and the compilation activities are queued. When the compilation threads are all busy, JVM will not follow the normal flow, as this may affect the overall performance of the application. Instead, JVM will escalate to L2, where at least the counters are instrumented, and at a later point, when the code reaches the higher threshold, it will escalate to L3 and L4. At any point, JVM can deoptimize or invalidate the compiled code.</li>
				<li><strong class="bold">C: Trivial Code</strong>: Sometimes, JVM will compile the code to L3 and realize that the code does not require any optimization, as it is very straightforward/simple, based on the profiling. In this case, it will bring it down to L1. That way, the execution of the code will be faster. The more we instrument the code, the more overhead we are <a id="_idIndexMarker125"/>putting on the execution. It is typically observed that L3 adds anywhere between 20-30% overhead to execution, due to instrumentation code.</li>
			</ul>
			<p>We can look at how JVM behaves using the <strong class="source-inline">-XX:+PrintCompilation</strong> option. Here is an example of a normal flow:</p>
			<p class="source-code">public class Sample {</p>
			<p class="source-code">    public static void main(String[] args) {</p>
			<p class="source-code">        Sample samp = new Sample();</p>
			<p class="source-code">        while (true) {</p>
			<p class="source-code">        for(int i=0; i&lt;1000000; i++) {</p>
			<p class="source-code">            samp.performOperation();</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p class="source-code">    public void performOperation() {</p>
			<p class="source-code">        int sum = 0;</p>
			<p class="source-code">        int x = 100;</p>
			<p class="source-code">        performAnotherOperation();</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public void performAnotherOperation() {</p>
			<p class="source-code">        int a = 100;</p>
			<p class="source-code">        int b = 200;</p>
			<p class="source-code">        for(int i=0; i&lt;1000000; i++) {</p>
			<p class="source-code">            int x = a + b;</p>
			<p class="source-code">            int y = (24*25) + x;</p>
			<p class="source-code">            int z = (24*25) + x;</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>For this code, when<a id="_idIndexMarker126"/> we execute <strong class="source-inline">java</strong> with <strong class="source-inline">-XX:+PrintCompilation</strong>, the following log is generated on the console. The log can be redirected to a log file using the <strong class="source-inline">+LogCompilation</strong> flag:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B16878_Figure_2.06.jpg" alt="Figure 2.6 – Log showing tiered compilation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Log showing tiered compilation</p>
			<p>In this screenshot, you can see how the <strong class="source-inline">main()</strong> method moves from L0-&gt;L3-&gt;L4, which is the normal flow (A). As JVM performs optimizations and de-optimizations, jumping between these various levels of compilation, it reaches the most optimum, stable point. This is one of the greatest <a id="_idIndexMarker127"/>advantages that JIT compilers have over AOT compilers. The JIT compiler uses the runtime behavior to optimize the code execution (not just the semantic/static code optimizations). If you run this with JITWatch, we can see a clearer representation. The following screenshot shows the JITWatch tool compile chain when we run it by the <strong class="source-inline">Sample.java</strong> snippet:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B16878_Figure_2.07.jpg" alt="Figure 2.7 – JITWatch tiered compilation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – JITWatch tiered compilation</p>
			<p>The previous screenshot shows that <strong class="source-inline">Sample::main()</strong> is compiled with the C1-L3 compiler. <strong class="source-inline">Sample::Sample()</strong> (default constructor) is inlined and <strong class="source-inline">Sample::performOperation()</strong> is also inlined into <strong class="source-inline">Sample::main()</strong>. <strong class="source-inline">Sample::performAnotherOperation()</strong> is also compiled. This is the first level of optimization: </p>
			<p class="source-code">JITWatch Tiered Compiliation for Sample::main() method</p>
			<p>The following screenshot shows how various compilers are run on each of the methods:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B16878_Figure_2.08.jpg" alt="Figure 2.8 – JITWatch tiered compilation of main()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – JITWatch tiered compilation of main()</p>
			<p>This screenshot<a id="_idIndexMarker128"/> shows how the <strong class="source-inline">main()</strong> method is optimized. Since the <strong class="source-inline">main()</strong> method has a long loop, OSR has occurred two times: once when the C1 compiled code is replaced, and the second time when the C2 compiled code is replaced. In each case, it has performed inlining. You can see what optimizations the C1 and C2 compilers performed in the following screenshot:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B16878_Figure_2.09.jpg" alt="Figure 2.9 – JITWatch tiered compilation of main() – OSR-L3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – JITWatch tiered compilation of main() – OSR-L3</p>
			<p>In the previous screenshot, we can see that <strong class="source-inline">Sample::performAnotherOperation()</strong> is compiled and <strong class="source-inline">Sample::performOperation()</strong> is inlined into <strong class="source-inline">Sample::main()</strong>. The following screenshot shows further optimization that is performed by inlining <strong class="source-inline">Sample:performAnotherOperation()</strong> into <strong class="source-inline">Sample::performOperation()</strong>:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B16878_Figure_2.10.jpg" alt="Figure 2.10 – JITWatch tiered compilation of main() – OSR-L4&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – JITWatch tiered compilation of main() – OSR-L4</p>
			<p>Let's now look at how the JIT <a id="_idIndexMarker129"/>compiler optimizes the S<strong class="source-inline">ample::performAnotherOperation()</strong> method:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B16878_Figure_2.11.jpg" alt="Figure 2.11 – JITWatch tiered compilation of performAnotherOperation()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – JITWatch tiered compilation of performAnotherOperation()</p>
			<p>As we can see in the previous screenshot, <strong class="source-inline">Sample::performAnotherOperation()</strong> has gone through various optimizations and OSRs, as it runs a long loop. The code is inlined into <strong class="source-inline">Sample::performOperation()</strong> as it hits the compiler threshold. The following screenshots reveal how <strong class="source-inline">Sample::performAnotherOperation()</strong> is compiled and inlined into <strong class="source-inline">Sample::performOperation()</strong>.</p>
			<p>Let's now look at how the JIT compiler <a id="_idIndexMarker130"/>compiles the <strong class="source-inline">Sample::performOperation()</strong> method:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B16878_Figure_2.12.jpg" alt="Figure 2.12 – JITWatch tiered compilation of performOperation() &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – JITWatch tiered compilation of performOperation() </p>
			<p>The following screenshot shows the C1 compilation chain view for the <strong class="source-inline">performOperation()</strong> method:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B16878_Figure_2.13.jpg" alt="Figure 2.13 – JITWatch tiered compilation of performOperation() – C1 compilation chain view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – JITWatch tiered compilation of performOperation() – C1 compilation chain view</p>
			<p>The previous screenshot shows that <strong class="source-inline">Sample::performAnotherOperation()</strong> is compiled as it hits<a id="_idIndexMarker131"/> the compiler threshold, and the following screenshot shows that the compiled code is inlined into <strong class="source-inline">Sample::performOperation()</strong>:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B16878_Figure_2.14.jpg" alt="Figure 2.14 – JITWatch tiered compilation of performOperation() – C2 compilation chain view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14 – JITWatch tiered compilation of performOperation() – C2 compilation chain view</p>
			<p>JITWatch can be used to gain detailed understanding of how the C1 an C2 compilers behave, and how the optimizations are performed. This helps in reflecting on the application code, and proactively updating the source code for better runtime performance. To get a better understanding of how the C2 compiler optimizes the code, let's now look at the various types of code optimizations that JVM applies during compilation. </p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor039"/>Understanding the optimizations performed by JIT</h1>
			<p>This section will cover the various optimization techniques that the JIT compilers employ at the various levels of compilation.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor040"/>Inlining</h2>
			<p>Calling a method is an expensive<a id="_idIndexMarker132"/> operation for JVM. When the program calls a method, JVM has to create a new stack frame for that method, copy all the values into the stack frame, and execute the code. Once the method completes, the stack frame has to be managed post-execution. One of the best practices in object-oriented programming is to access the object members through access methods (getters and setters). </p>
			<p>Inlining is one of the most effective optimizations performed by JVM. JVM replaces the method call with the actual content of the code. </p>
			<p>If we run our previous code with the following command, we can see how JVM performs inlining:</p>
			<p class="source-code">java -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining Sample</p>
			<p>In this case, the call to the <strong class="source-inline">performOperation()</strong> method is replaced with the content of the inline <strong class="source-inline">main()</strong> method. After inlining effectively, the <strong class="source-inline">main()</strong> method will look like this:</p>
			<p class="source-code">public static void <a id="_idTextAnchor041"/>main(String[] args) {</p>
			<p class="source-code">        Sample samp = new Sample();</p>
			<p class="source-code">        while (true) {</p>
			<p class="source-code">        for(int i=0; i&lt;1000000; i++) {</p>
			<p class="source-code">            samp.performOperation();</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p>Inlining can be disabled using the <strong class="source-inline">-XX:-Inline</strong> flag.</p>
			<p>JVM decides to<a id="_idIndexMarker133"/> inline the code, based on the number of calls made to the method and the size of the method, and if the method is frequently called (hot), and the size of the method is &lt;325 bytes. Methods that are &lt;35 bytes are inlined by default. These numbers can be changed with the <strong class="source-inline">-XX:+MaxFreqInlineSize</strong> and <strong class="source-inline">-XX:+MaxInlineSize</strong> flags from the command line, respectively.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor042"/>Monomorphic, bimorphic, and megamorphic dispatch</h2>
			<p>Polymorphism is one of the<a id="_idIndexMarker134"/> key object-oriented concepts that provides a way to dynamically load classes based on a context, and the behavior is decided dynamically. Interfaces and inheritance are two<a id="_idIndexMarker135"/> of the most widely used polymorphism implementations. However, this comes with a performance<a id="_idIndexMarker136"/> overhead, as JVM loads the class/interface implementations dynamically. Inlining the implementations becomes a challenge.</p>
			<p>One of the things that JVM profiles is the number of times a particular implementation is called and how many derived class/interface implementations really exist for a given base class or interface. If the profiler identifies only one implementation, then it's called monomorphic. If it finds two, then <a id="_idIndexMarker137"/>it's called bimorphic, and megamorphic means there are multiple implementations.</p>
			<p>Based on the profiling, the JIT compiler identifies which specific derived class object (or interface implementation) is used and takes the decision on inlining that specific implementation, to overcome the performance overheads of polymorphism. Monomorphic and bimorphic are<a id="_idIndexMarker138"/> easy to inline. The JIT profiler tracks the execution paths and identifies<a id="_idIndexMarker139"/> in which context which implementation is used and performs inlining. Megamorphic implementations <a id="_idIndexMarker140"/>are complex to inline. The following code snippet shows polymorphism. We will use this code to understand the performance overhead:</p>
			<p class="source-code">public interface Shape {</p>
			<p class="source-code">    String whichShapeAreYou();</p>
			<p class="source-code">}</p>
			<p class="source-code">public class Circle implements Shape {</p>
			<p class="source-code">    public String whichShapeAreYou() { return "I am Circle";} </p>
			<p class="source-code">}</p>
			<p class="source-code">public class Square implements Shape {</p>
			<p class="source-code">    public String whichShapeAreYou() { return "I am Square";} </p>
			<p class="source-code">}</p>
			<p class="source-code">public class Triangle implements Shape {</p>
			<p class="source-code">    public String whichShapeAreYou() { return "I am Triangle";} </p>
			<p class="source-code">}</p>
			<p class="source-code">public static void main(String[] args) {</p>
			<p class="source-code">    //Some code and logic here</p>
			<p class="source-code">    switch (circleType) {</p>
			<p class="source-code">        case 0:</p>
			<p class="source-code">            shape = new Circle();</p>
			<p class="source-code">            break;</p>
			<p class="source-code">        case 1:</p>
			<p class="source-code">            shape = new Square();</p>
			<p class="source-code">            break;</p>
			<p class="source-code">        case 2:</p>
			<p class="source-code">            shape = new Triangle();</p>
			<p class="source-code">            break;</p>
			<p class="source-code">        default:</p>
			<p class="source-code">            System.out.println("Invalid shape");</p>
			<p class="source-code">            break;</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>In the previous code, we<a id="_idIndexMarker141"/> have defined an interface called <strong class="source-inline"><a id="_idIndexMarker142"/></strong><strong class="source-inline">Shape</strong>, and we <a id="_idIndexMarker143"/>have three implementations of the interface, namely, <strong class="source-inline">Circle</strong>, <strong class="source-inline">Square</strong>, and <strong class="source-inline">Triangle</strong>. And, we are using a switch to initialize the right class. There are two optimization scenarios here:</p>
			<ul>
				<li>If the JIT knows that a particular implementation is used, it optimizes the code and might perform an inline. This is called a <a id="_idIndexMarker144"/>monomorphic dispatch.</li>
				<li>If, let's say, the decision is to be taken based on a particular variable or a configuration, JIT will profile, which is the most optimistic assumption it can take, and only those classes and inline them, and may also use an uncommon trap. In case the implementation class that is used is different from what is assumed, the JIT will deoptimize.</li>
			</ul>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor043"/>Dead code elimination</h2>
			<p>The JIT compiler identifies<a id="_idIndexMarker145"/> the code that is<a id="_idIndexMarker146"/> never executed or not required while profiling. This is called dead code, and the JIT compiler eliminates this from the execution. Modern IDEs identify dead code; this is purely based on the static code analysis performed. JIT compilers not only eliminate such trivial code, but also eliminate the code based on the control flow at runtime. Dead code elimination is one of the most effective ways to improve performance.</p>
			<p>Let's take the following code as an example:</p>
			<p class="source-code">/**</p>
			<p class="source-code"> * DeadCodeElimination</p>
			<p class="source-code"> */</p>
			<p class="source-code">public class DeadCodeElimination {</p>
			<p class="source-code">    public void calculateSomething() {</p>
			<p class="source-code">        int[] arrayOfValues = new int[1000000];</p>
			<p class="source-code">    </p>
			<p class="source-code">        int finalTotalValue = 0;</p>
			<p class="source-code">    </p>
			<p class="source-code">        for (int i=0; i&lt; arrayOfValues.length; i++) {</p>
			<p class="source-code">            finalTotalValue = calculateValue(arrayOfValues[i]);</p>
			<p class="source-code">        }</p>
			<p class="source-code">  //"Do some more activity here, but never use final Total            count");</p>
			<p class="source-code">    }</p>
			<p class="source-code">    </p>
			<p class="source-code">    public int calculateValue(int value) {</p>
			<p class="source-code">        //use some formula to calucalte the value</p>
			<p class="source-code">        return value * value;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public static void main(String[] args) {</p>
			<p class="source-code">        DeadCodeElimination obj = new DeadCodeElimination();</p>
			<p class="source-code">        for (int i=0; i&lt; 10000; i++) {</p>
			<p class="source-code">            obj.calculateSomething();</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">    </p>
			<p class="source-code">}</p>
			<p>In this code, the <strong class="source-inline">calculateSomething()</strong> method has some logic. Let's look at the previous code snippet. The <strong class="source-inline">finalTotalValue</strong> variable is initialized and later, the total is calculated<a id="_idIndexMarker147"/> by calling the <strong class="source-inline">calculateValue()</strong> method in a loop, but assuming that <strong class="source-inline">finalTotalValue</strong> is never used after calculation. The initialization code, the array heap allocation code, and the loop that calls the <strong class="source-inline">calculateValue()</strong> method, are all dead code. JIT understands this at runtime and removes it completely.</p>
			<p>JIT takes these decisions based on profiling and whether the code is reachable. It might remove unnecessary <strong class="source-inline">if</strong> statements (especially null checks; if the object is never seen to be null – this technique is sometimes referred to as Null Check Elimination). It will replace this with what is called "uncommon trap" code. If this execution ever reaches this trap code, it will then deoptimize.</p>
			<p>Another case where "uncommon trap" code is placed by eliminating the code is by predicting branches. Based on the <a id="_idIndexMarker148"/>profiling, JIT assumes and predicts a branch code (<strong class="source-inline">if</strong>, <strong class="source-inline">switch</strong>, and so on) that may never be executed, and eliminates that code.</p>
			<p>Common Subexpression Elimination is another effective technique that JIT uses to eliminate code. In this technique, an intermediate subexpression is removed to save the number of instructions.</p>
			<p>Later, in the <em class="italic">Escape analysis</em> section, we will <a id="_idIndexMarker149"/>also see some code elimination techniques, based on the escape analysis that JIT performs.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor044"/>Loop optimization – Loop unrolling</h2>
			<p>Loop unrolling is another <a id="_idIndexMarker150"/>effective optimization technique. This is more effective in smaller loop body and large number of iterations. The technique involves looking to reduce the iterations in the loop by replacing the code. Here is a very simple example:</p>
			<p class="source-code">for (int i=0; i&lt; arrayOfValues.size; i++) {</p>
			<p class="source-code">    somefunction(arrayOfValues[i]);</p>
			<p class="source-code">}</p>
			<p>This can be rolled into the following:</p>
			<p class="source-code">for (int i=0; i&lt; arrayOfValues.size; i+=4) {</p>
			<p class="source-code">    somefunction (arrayOfValues[i]);</p>
			<p class="source-code">    somefunction (arrayOfValues[i+1]);</p>
			<p class="source-code">    somefunction (arrayOfValues[i+2]);</p>
			<p class="source-code">    somefunction (arrayOfValues[i+3]);</p>
			<p class="source-code">}</p>
			<p>In this example, the JIT compiler decides to reduce the iterations by 1/4, by calling <strong class="source-inline">somefunction()</strong> four times instead of once. This has a significant performance improvement, as the number<a id="_idIndexMarker151"/> of jump statements goes down by 1/4. Of course, the decision on four is taken based on the size of the array so that the array reference does not go out of bounds.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor045"/>Escape analysis</h2>
			<p>Escape analysis is <a id="_idIndexMarker152"/>one of the most advanced optimizations that the JIT compiler performs. This is controlled with the <strong class="source-inline">-XX:+DoEscapeAnalysis</strong> flag from the command line. This is enabled by default.</p>
			<p>In the previous chapter, we went through the various memory areas in the <em class="italic">Memory subsystem</em> section. Heap and stack are two of the most important memory areas. The heap memory area is accessible across various threads in JVM. A heap is not thread-safe. When multiple threads access the data that is stored in the heap, it is recommended to write thread-safe code by obtaining synchronization locks. This blocks the other threads from accessing the same data. This has performance implications.</p>
			<p>Stack memory is thread-safe, as it is allocated for that particular method call. Only the method thread has access to this area, hence there is no need to worry about obtaining synchronization locks or the absence of blocking threads.</p>
			<p>The JIT compiler performs a detailed analysis of the code to identify code where we are allocating variables in the heap, but only using them in a specific method thread, and takes decisions on allocating these variables to the "stack area" instead of the "heap area." This is one of the most complex optimizations that JIT compilers perform and has a huge impact on performance. JIT might decide to store the variables in PC registers for even faster access.</p>
			<p>JIT also looks for the use of <strong class="source-inline">synchronized</strong> and tracks. If it's called by a single thread, JIT decides to ignore <strong class="source-inline">synchronized</strong>. This has a significant impact on performance. <strong class="source-inline">StringBuffer</strong> is one of the objects that is thread-safe and has a lot of synchronized methods. If an instance of <strong class="source-inline">StringBuffer</strong> is not used outside a single method, JIT decides to ignore <strong class="source-inline">synchronized</strong>. This technique is sometimes referred to as "lock elision."</p>
			<p>In cases where a synchronized lock<a id="_idIndexMarker153"/> cannot be ignored, the JIT compiler looks to combine the <strong class="source-inline">synchronized</strong> blocks. This <a id="_idIndexMarker154"/>technique is known as lock coarsening. This technique looks for subsequent <strong class="source-inline">synchronized</strong> blocks. Here is an example:</p>
			<p class="source-code">public class LockCoarsening {</p>
			<p class="source-code">    public static void main(String[] args) {</p>
			<p class="source-code">        synchronized (Class1.class) {</p>
			<p class="source-code">            ....</p>
			<p class="source-code">        }</p>
			<p class="source-code">        synchronized (Class1.class) {</p>
			<p class="source-code">            ....</p>
			<p class="source-code">        }</p>
			<p class="source-code">        synchronized (Class2.class) {</p>
			<p class="source-code">            ....</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>In this example, two subsequent synchronized blocks are trying to obtain a lock on the same class. The JIT compiler will combine these two blocks into one.</p>
			<p>JIT performs a similar analysis for variables that are created with a loop and never used outside the loop. There is a very sophisticated technique called "scalar replacement," where the JIT profiles for objects that are created, but only a few member variables are used in the object that is not used. JIT will decide to stop creating the objects and replace them with the member variables directly. Here is a very simple example:</p>
			<p class="source-code"> class StateStoring {</p>
			<p class="source-code">    final int state_variable_1;</p>
			<p class="source-code">    final int state_variable_2;</p>
			<p class="source-code">    public StateStoring(int val1, int val2) {</p>
			<p class="source-code">        this.state_variable_1 = val1;</p>
			<p class="source-code">        this.state_variable_2 = val2;</p>
			<p class="source-code">    }</p>
			<p class="source-code"> }</p>
			<p>The <strong class="source-inline">StateStoring</strong> class is a simple class, where we are storing the state of an object with two members – <strong class="source-inline">state_variable_1</strong> and <strong class="source-inline">state_variable_2</strong>. JIT profiles this for various iterations and checks whether this object was created and never used outside the scope. It might decide not to even create the object, instead replacing the object getters and setters with <a id="_idIndexMarker155"/>actual scalars (local variables). That way, entire object creation and destruction (which is a very expensive process) can be avoided.</p>
			<p>Here is a more advanced example, and this time let's see how JITWatch shows the escape analysis:</p>
			<p class="source-code">public class EscapeAnalysis2 {</p>
			<p class="source-code">    public void createNumberofObjects     (int numberOfArraysToCreate, int numberOfCellsInArray) {</p>
			<p class="source-code">        for (int i=0; i&lt; numberOfArraysToCreate; i++) {</p>
			<p class="source-code">            allocateObjects(numberOfCellsInArray);</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">    private void allocateObjects(int numberOfCellsInArray) {</p>
			<p class="source-code">        </p>
			<p class="source-code">        int[] arrayObj = new int[numberOfCellsInArray];</p>
			<p class="source-code">        for (int i=0; i&lt; numberOfCellsInArray; i++) {</p>
			<p class="source-code">        //Heap allocation, which could have been easily a local               stack allocation</p>
			<p class="source-code">            Integer dummyInt = new Integer(i);</p>
			<p class="source-code">            arrayObj[i] = dummyInt.intValue();</p>
			<p class="source-code">        }</p>
			<p class="source-code">        return;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public static void main(String[] args) {</p>
			<p class="source-code">        EscapeAnalysis2 obj = new EscapeAnalysis2();</p>
			<p class="source-code">        obj.createNumberofObjects(100000, 10);</p>
			<p class="source-code">    }</p>
			<p class="source-code">    </p>
			<p class="source-code">}</p>
			<p>In this code snippet, the <strong class="source-inline">allocateObjects()</strong> method is<a id="_idIndexMarker156"/> creating an array (on the heap) and adding that value to the array. The <strong class="source-inline">dummyInt</strong> variable's scope is limited to the <strong class="source-inline">for</strong> loop in the <strong class="source-inline">allocateObjects()</strong> method. There is no need to have these objects created in the heap. After performing the escape analysis, JIT identifies that these variables can be put in a stack frame instead.</p>
			<p>The following JITWatch screenshot demonstrates this:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B16878_Figure_2.15.jpg" alt="Figure 2.15 – JITWatch escape analysis – 1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15 – JITWatch escape analysis – 1</p>
			<p>In this screenshot, the <a id="_idIndexMarker157"/>bytecode that allocates <strong class="source-inline">dummyInt</strong> is struck off to indicate that heap allocation for that variable is not required:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B16878_Figure_2.16.jpg" alt="Figure 2.16 – JITWatch escape analysis – 2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16 – JITWatch escape analysis – 2</p>
			<p>The previous screenshot shows the optimization that is performed by C2/Level 4, where it removes allocation of the variable.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor046"/>Deoptimization</h1>
			<p>In the previous section, we looked at various optimization techniques that the JIT compiler performs. The JIT compiler <a id="_idIndexMarker158"/>optimizes the code with some assumptions that it makes, based on the profiling. Sometimes, these assumptions may be not correct in a different context. When JIT stumbles upon these scenarios, it deoptimizes the code and goes back to using an interpreter to run the code. This is called Deoptimization and has an impact on performance.</p>
			<p>There are two scenarios where Deoptimization occurs:</p>
			<ul>
				<li>When the code is "non-entrant"</li>
				<li>When the code is "zombie"</li>
			</ul>
			<p>Let's understand these scenarios with the help of examples.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor047"/>Non-entrant code</h2>
			<p>There are two cases where the code becomes non-entrant:</p>
			<ul>
				<li><strong class="bold">Assumptions made during polymorphism</strong>: As we discussed in the section on monomorphic dispatch, polymorphism <a id="_idIndexMarker159"/>has a significant performance overhead on JVM. One of the optimizations that JIT performs is assuming a particular implementation of the interface/base class, and inlines that particular implementation of the interface/class. This is done based on the context and the control path that the JIT observed during profiling. When the assumption is invalid, JIT generates a deoptimization trap, and this optimized code is rendered "non-entrant". We can see when the JIT is making the code non-entrant with the <strong class="source-inline">-XX:+PrintCompilation</strong> flag.</li>
				<li><strong class="bold">Escalations that happen during tiered compilation</strong>: When JIT decides to move between the level discussed in the <em class="italic">Tiered compilation</em> section, it marks the code that was optimized in the previous level as non-entrant, and the next level of the compilation process will optimize the code and replace the previous optimizations. This happens when JVM replaces the code that is compiled by C1 with the code that C2 has compiled. The following screenshot demonstrates an example when we run out <strong class="source-inline">Sample.java</strong>:</li>
			</ul>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B16878_Figure_2.17.jpg" alt="Figure 2.17 – Tiered compilation escalation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17 – Tiered compilation escalation</p>
			<p>In the preceding screenshot, we <a id="_idIndexMarker160"/>can see the tiered compilation in action (the third column shows the tier number) and the optimization that is done. </p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor048"/>Zombie code</h2>
			<p>In most cases, some objects are<a id="_idIndexMarker161"/> created in the heap of the code that is marked as non-entrant. Once the GC reclaims all these objects, then JVM will mark the methods of those classes as zombie code. JVM then removes this compiled zombie code from the code cache. As we discussed in the <em class="italic">Taking a deep dive into hotspot and the C2 JIT</em> section, it's very important to keep the code cache optimum, as this has a significant impact on performance.</p>
			<p>As we saw in tiered compilation, deoptimization is performed when any of the assumptions that the Java JIT made is challenged by the control flow at runtime. In the next section, we will briefly introduce the Graal JIT compiler, and how it plugs into JVM.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor049"/>Graal JIT and the JVM Compiler Interface (JVMCI)</h1>
			<p>In the previous sections, as we <a id="_idIndexMarker162"/>walked through the various features and advancements that JIT compilers <a id="_idIndexMarker163"/>underwent, it is very clear that C2 is very sophisticated. However, C2 compiler implementation has its downsides. C2 is implemented in the C/C++ language. While C/C++ is fast, it is not type-safe and it does not have garbage collection. Hence, the code becomes very complex. C2 implementation is very complex, as it has become more and more complex to change the code for new enhancements and bug fixes.</p>
			<p>In the meantime, Java has matured to run as fast as C/C++ in many cases. Java is type-safe with garbage collection. Java is simpler and easier to manage than C/C++. The key advantages of Java are its<a id="_idIndexMarker164"/> exception handling capabilities, memory management, better IDE/profiling, and tooling support. The JIT compiler is nothing but a program that takes in a bytecode, <strong class="source-inline">byte[]</strong>, optimizes it, compiles it, and returns an array of machine code, <strong class="source-inline">byte[]</strong>. This can easily be<a id="_idIndexMarker165"/> implemented in Java. What we need is a JVM interface that can provide the protocol for implementing the custom compiler logic. This will help open up JVM for the new implementations of JIT compilers.</p>
			<p>JDK enhancement proposal JEP243 (<a href="https://openjdk.java.net/jeps/243">https://openjdk.java.net/jeps/243</a>) is a proposal to provide a compiler interface that will enable writing a compiler in Java and extending JVM to use it dynamically.</p>
			<p>JEP243 was added in Java 9. This is one of the most significant enhancements to JVM. JVMCI is an implementation of JEP243. JVMCI provides the required extensibility to write our own JIT compilers. JVMCI provides the API that is required to implement custom compilers and configure JVM to call these custom compiler implementations. The JVMCI API provides the following capabilities:</p>
			<ul>
				<li>Access to VM data structures, which is required to optimize the code</li>
				<li>Managing the compiled code following optimization and deoptimization</li>
				<li>Callbacks from JVM to execute the compilation at runtime</li>
			</ul>
			<p>A JVMCI can be executed with the following command-line flags:</p>
			<p class="source-code">-XX:+UnlockExperimentalVMOptions </p>
			<p class="source-code">-XX:+EnableJVMCI </p>
			<p class="source-code">-XX:+UseJVMCICompiler </p>
			<p class="source-code">-Djvmci.Compiler=&lt;name of compiler&gt;</p>
			<p>Graal is an implementation of JVMCI, which brings all the key features and optimizations that are required for a modern Java runtime. Graal is wholly implemented in Java. Graal is much more than just a JIT compiler. Here is a quick comparison between the Graal JIT and the JVM JIT (C2):</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B16878_Table_2.1.jpg" alt=""/>
				</div>
			</div>
			<p>The next chapter will go into <a id="_idIndexMarker166"/>more detail on Graal architecture, and <a href="B16878_04_Final_SK_ePub.xhtml#_idTextAnchor077"><em class="italic">Chapter 4</em></a>, <em class="italic">Graal Just-In-Time Compiler</em>, will go deeper<a id="_idIndexMarker167"/><a id="_idIndexMarker168"/> into how Graal JIT works, and how it builds on top of Java JIT and brings in more advanced optimizations and support for Polyglot.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor050"/>Summary</h1>
			<p>In this chapter, we went into a lot of detail on how the JIT compiler works and discussed the tiered compilation patterns that JVM uses to optimize the code. We also walked through various optimization techniques with a number of sample code examples. This provided a good understanding of the internal workings of JVM.</p>
			<p>JVMCI provides the extensibility to build custom JIT compilers on JVM. Graal JIT is an implementation of JVMCI.</p>
			<p>This chapter provided the basis for understanding how JVM works, and how JIT compilation optimizes the code at runtime. This is key in understanding how the Graal JIT compiler works.</p>
			<p>In the next chapter, we will understand how the Graal VM architecture is built on the JVM architecture, and how it extends it to support Polyglot. </p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor051"/>Questions</h1>
			<ol>
				<li value="1">What is a code cache?</li>
				<li>What are the various flags that can be used to optimize a code cache?</li>
				<li>What is the compiler threshold?</li>
				<li>What is on-stack replacement? </li>
				<li>What is tiered compilation? What are the various patterns of tiered compilation?</li>
				<li>What is inlining?</li>
				<li>What is monomorphic dispatch?</li>
				<li>What is loop unrolling?</li>
				<li>What is escape analysis?</li>
				<li>What is Deoptimization?</li>
				<li>What is JVMCI?</li>
			</ol>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor052"/>Further reading</h1>
			<ul>
				<li><em class="italic">Introduction to JVM Languages </em>(<a href="https://www.packtpub.com/product/introduction-to-jvm-languages/9781787127944">https://www.packtpub.com/product/introduction-to-jvm-languages/9781787127944</a>)</li>
				<li>Java SDK documentation (<a href="https://docs.oracle.com">https://docs.oracle.com</a>)</li>
				<li>GraalVM documentation (<a href="https://docs.oracle.com/en/graalvm/enterprise/19/guide/overview/compiler.html">https://docs.oracle.com/en/graalvm/enterprise/19/guide/overview/compiler.html</a>)</li>
				<li>JITWatch documentation (<a href="https://github.com/AdoptOpenJDK/jitwatch">https://github.com/AdoptOpenJDK/jitwatch</a>)</li>
			</ul>
		</div>
	</body></html>