<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Clustering</h1></div></div></div><p>This chapter will cover WildFly's clustering capabilities. The term cluster is used to describe a system split over several machines. Having the components of a system synchronize over multiple machines generally improves performance and availability.</p><p>Clustering serves as an essential component to providing scalability and high availability to your applications. One major benefit of using clustering is that you can spread the traffic load across several AS instances via <a id="id899" class="indexterm"/>
<span class="strong"><strong>load balancing</strong></span>.</p><p>Load balancing is an orthogonal aspect of your enterprise application and is generally achieved by using a properly configured web server in front of the application server. For this reason, load balancing is discussed in the next chapter while, in this chapter, we will discuss the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">All available options to set up a WildFly cluster either using a standalone configuration or a domain of servers</li><li class="listitem" style="list-style-type: disc">How to effectively configure the various components required for clustering</li><li class="listitem" style="list-style-type: disc">The<a id="id900" class="indexterm"/> <span class="strong"><strong>JGroups</strong></span> subsystem, which is used for the underlying communication between nodes</li><li class="listitem" style="list-style-type: disc">The <a id="id901" class="indexterm"/><span class="strong"><strong>Infinispan</strong></span> subsystem, which handles the cluster consistency using its advanced data grid platform</li><li class="listitem" style="list-style-type: disc">The <a id="id902" class="indexterm"/><span class="strong"><strong>Messaging</strong></span> subsystem, which uses the HornetQ clusterable implementation</li></ul></div><div class="section" title="Setting up a WildFly cluster"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec38"/>Setting up a WildFly cluster</h1></div></div></div><p>For the benefit of <a id="id903" class="indexterm"/>impatient readers, we will immediately show you how to get a cluster of WildFly nodes up and running.</p><p>All you have to do to shape a new server profile is create a new XML configuration file. As the standalone server holds just a single profile, you will likely want to use either the configuration file named <code class="literal">standalone-ha.xml</code> or <code class="literal">standalone-full-ha.xml</code>. Both of these ship with WildFly. This configuration file contains all the clustering subsystems.</p><p>On the other hand, a <a id="id904" class="indexterm"/>domain server is able to store multiple profiles in the core <code class="literal">domain.xml</code> configuration file, hence you can use this file both for clustered domains and for nonclustered domain servers.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note59"/>Note</h3><p>Clustering and domains<a id="id905" class="indexterm"/> are two separate concepts, the functionality of each does not overlap. While the aim of <a id="id906" class="indexterm"/>clustering is to provide scalability, load balancing, and high availability, a domain is a logical grouping of servers that share a centralized domain configuration and can be managed as a single unit.</p></div></div><p>We will now describe the different ways to assemble and start a cluster of standalone servers and domain servers.</p><div class="section" title="Setting up a cluster of standalone servers"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec92"/>Setting up a cluster of standalone servers</h2></div></div></div><p>Configuring WildFly clusters<a id="id907" class="indexterm"/> for standalone servers can be broken down into two main possibilities:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A cluster of WildFly nodes running on different machines</li><li class="listitem" style="list-style-type: disc">A cluster of WildFly nodes running on the same machine</li></ul></div><p>We will look at each of these in turn.</p><div class="section" title="A cluster of nodes running on different machines"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec67"/>A cluster of nodes running on different machines</h3></div></div></div><p>If you decide<a id="id908" class="indexterm"/> to install each WildFly server on a dedicated machine, you are <span class="emphasis"><em>horizontally scaling</em></span> your cluster. In terms of configuration, this requires the least effort—all you have to do is bind the server to its IP address in the configuration file, and start the server using the <code class="literal">standalone-ha.xml</code> configuration. Let's build an example with a simple, two-node cluster as illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/6232OS_08_01.jpg" alt="A cluster of nodes running on different machines"/></div><p>Open the <code class="literal">standalone-ha.xml</code> file on each WildFly distribution, and navigate to the <code class="literal">interfaces</code> section. Within<a id="id909" class="indexterm"/> the nested interface element, insert the IP address of the standalone server. For the first machine (<code class="literal">192.168.10.1</code>), we will define the following:</p><div class="informalexample"><pre class="programlisting">&lt;interfaces&gt;
        &lt;interface name="management"&gt;
            &lt;inet-address value="192.168.10.1"/&gt;
        &lt;/interface&gt;
        &lt;interface name="public"&gt;
            &lt;inet-address value="192.168.10.1"/&gt;
        &lt;/interface&gt;
&lt;/interfaces&gt;</pre></div><p>On the second machine (<code class="literal">192.168.10.2</code>), we will bind to the other IP address:</p><div class="informalexample"><pre class="programlisting">&lt;interfaces&gt;
        &lt;interface name="management"&gt;
            &lt;inet-address value="192.168.10.2"/&gt;
        &lt;/interface&gt;
        &lt;interface name="public"&gt;
            &lt;inet-address value="192.168.10.2"/&gt;
        &lt;/interface&gt;
&lt;/interfaces&gt;</pre></div><p>This is the only thing you need to change in your configuration. To start the cluster, you have to start your standalone server using the <code class="literal">standalone-ha.xml</code> configuration file as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./standalone.sh -c standalone-ha.xml</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note60"/>Note</h3><p>Rather than updating the <code class="literal">standalone-ha.xml</code> file with the IP address of each server, you can use the <code class="literal">-b</code> option, which allows you to provide the binding IP address on server startup. In addition, you can use the <code class="literal">-bmanagement</code> flag to specify the management-interface address. Using these options, the previous configuration for the first server can be rewritten as:</p><div class="informalexample"><pre class="programlisting">standalone.sh -c standalone-ha.xml –b 192.168.10.1 -bmanagement 192.168.10.1</pre></div><p>For the second server, it can be rewritten as:</p><div class="informalexample"><pre class="programlisting">standalone.sh -c standalone-ha.xml –b 192.168.10.2 -bmanagement 192.168.10.2</pre></div></div></div><p>Within a few<a id="id910" class="indexterm"/> seconds, your servers will be running; however, we have not mentioned any details relating to clustering nodes in the console. This is because, in WildFly, the core services are only started on demand. This means the clustering services are started only when the server detects that they are required and are stopped when no longer required. Hence, simply starting the server with a configuration that includes the clustering subsystems will not initiate the clustering services. To do this, we will need to deploy a cluster-enabled application.</p><p>So, in order to verify our installation, we will deploy a bare-bones, cluster-enabled, web application named <code class="literal">Example.war</code>. To enable clustering of your web applications, you must mark them as <span class="emphasis"><em>distributable</em></span> in the <code class="literal">web.xml</code> descriptor:</p><div class="informalexample"><pre class="programlisting">&lt;web-app&gt;
   &lt;distributable/&gt;
&lt;/web-app&gt;</pre></div><p>When you have deployed the application to both machines, you will see that the clustering services are now started and that each machine is able to find other members within the cluster, as follows:</p><div class="mediaobject"><img src="graphics/6232OS_08_02.jpg" alt="A cluster of nodes running on different machines"/></div></div><div class="section" title="A cluster of nodes running on the same machine"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec68"/>A cluster of nodes running on the same machine</h3></div></div></div><p>The<a id="id911" class="indexterm"/> second variant of the standalone configuration comes into play when your server nodes are located (all or some of them) on the same machine. This scenario generally applies when you are scaling your architecture <span class="emphasis"><em>vertically</em></span> by adding more hardware resources to your computer.</p><p>Configuring server nodes on the same machine obviously requires duplicating your WildFly distribution on your filesystem. In order to avoid port conflicts between server distributions, you have to choose between the following two options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Define multiple IP address on the same machine</li><li class="listitem" style="list-style-type: disc">Define a port offset for each server distribution</li></ul></div><div class="section" title="Setting up a cluster on the same machine using multiple IP addresses"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec08"/>Setting up a cluster on the same machine using multiple IP addresses</h4></div></div></div><p>This is also known as <a id="id912" class="indexterm"/>
<span class="strong"><strong>multihoming</strong></span><a id="id913" class="indexterm"/> and requires a small amount of configuration to get working. Each operating system uses a different approach to achieve this. Illustrating the possible ways to configure multihoming is outside the scope of this book but, if you are interested in multihoming, we have provided links with detailed instructions on how to set up multihoming on Linux<a id="id914" class="indexterm"/> and Windows.</p><p>If you are using Linux, this tutorial describes in detail how to assign multiple IPs to a single network interface, also known as <a id="id915" class="indexterm"/>
<span class="strong"><strong>IP aliasing</strong></span>:</p><p>
<a class="ulink" href="http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/">http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/</a>
</p><p>Windows users<a id="id916" class="indexterm"/> can refer to the following blog that details how to set up multihoming in Windows 7:</p><p>
<a class="ulink" href="http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html">http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html</a>
</p><p>Once you have <a id="id917" class="indexterm"/>set up your network interface correctly, you will need to update your <code class="literal">standalone-ha.xml</code> file. You need to bind each IP to a different WildFly instance, just as we did when setting up the multiple-host cluster. Within the configuration file, navigate to the <code class="literal">interfaces</code> section and, within the nested <code class="literal">interface</code> element, insert the IP address to be bound to that standalone server:</p><div class="informalexample"><pre class="programlisting">&lt;interfaces&gt;
        &lt;interface name="management"&gt;
            &lt;inet-address value="192.168.10.2"/&gt;
        &lt;/interface&gt;
        &lt;interface name="public"&gt;
            &lt;inet-address value="192.168.10.2"/&gt;
        &lt;/interface&gt;
&lt;/interfaces&gt;</pre></div><p>In this example, the first server distribution is bound to the IP Address <code class="literal">192.168.10.1</code> and the second one to <code class="literal">192.168.10.2</code>. (remember that you can also use the <code class="literal">-b</code> and <code class="literal">-bmanagement</code> switches described earlier).</p><p>The following figure depicts this scenario:</p><div class="mediaobject"><img src="graphics/6232OS_08_06.jpg" alt="Setting up a cluster on the same machine using multiple IP addresses"/></div></div><div class="section" title="Setting up a cluster on the same machine using port offset"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec09"/>Setting up a cluster on the same machine using port offset</h4></div></div></div><p>Configuring<a id="id918" class="indexterm"/> multihoming is <a id="id919" class="indexterm"/>not always a viable choice, as it requires a relative amount of network administration experience. A simpler and more straightforward option is to define a port offset for each of your cluster members. By defining a port offset for each server, all the default-server binding interfaces will shift by a fixed number, hence you will not have two servers running on the same ports, causing port conflicts.</p><p>When using port offset, you will bind each server to the same IP address. So, for all your server distributions, you will configure the <code class="literal">standalone-ha.xml</code> file as follows:</p><div class="informalexample"><pre class="programlisting">&lt;interfaces&gt;
        &lt;interface name="management"&gt;
            &lt;inet-address value="192.168.10.1"/&gt;
        &lt;/interface&gt;
        &lt;interface name="public"&gt;
            &lt;inet-address value="192.168.10.1"/&gt;
        &lt;/interface&gt;
&lt;/interfaces&gt;</pre></div><p>You will then leave the first server configuration unchanged. It will use the default socket-binding ports:</p><div class="informalexample"><pre class="programlisting">&lt;socket-binding-group name="standard-sockets" default-interface="public" port-offset="0"&gt;
...
&lt;/socket-binding-group&gt;</pre></div><p>For the second server configuration, you will specify a <code class="literal">port-offset</code> value of <code class="literal">150</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&lt;socket-binding-group name="standard-sockets" default-interface="public" port-offset="150"</strong></span>
...
&lt;/socket-binding-group&gt;</pre></div><p>Your cluster <a id="id920" class="indexterm"/>configuration is now complete. You can verify this by starting each server distribution by passing it as an argument to the configuration file as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>standalone.sh -c standalone-ha.xml</strong></span>
</pre></div><p>From the following screenshot, you can see that a port offset of 150 has been applied:</p><div class="mediaobject"><img src="graphics/6232OS_08_07.jpg" alt="Setting up a cluster on the same machine using port offset"/></div></div></div></div><div class="section" title="Setting up a cluster of domain servers"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec93"/>Setting up a cluster of domain servers</h2></div></div></div><p>When you are configuring a <a id="id921" class="indexterm"/>domain cluster, you will find that the clustering subsystems are already included within the main configuration file <code class="literal">domain.xml</code>.</p><p>As a matter of fact, the WildFly domain deals with clustering just as another profile used by the application server. Opening the <code class="literal">domain.xml</code> file, you will see that the application server ships with the following four profiles:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">default</code> profile for nonclustered environments</li><li class="listitem" style="list-style-type: disc">The <code class="literal">ha</code> profile for clustered environments</li><li class="listitem" style="list-style-type: disc">The <code class="literal">full</code> profile with all the subsystems for nonclustered environments</li><li class="listitem" style="list-style-type: disc">The <code class="literal">full-ha</code> profile with all the subsystems for clustered environments</li></ul></div><p>So, in order to use clustering on a domain, you have to first configure your server groups to point to one of the <code class="literal">ha</code> profiles.</p><p>Let's look at an example configuration that uses two server groups. The following code snippet is from <code class="literal">domain.xml</code>:</p><div class="informalexample"><pre class="programlisting">&lt;server-groups&gt;
<span class="strong"><strong>    &lt;server-group name="main-server-group" profile="ha"&gt;</strong></span>
        &lt;jvm name="default"&gt;
            &lt;heap size="64m" max-size="512m"/&gt;
        &lt;/jvm&gt;
<span class="strong"><strong>            &lt;socket-binding-group ref="ha-sockets"/&gt;</strong></span>
    &lt;/server-group&gt;
<span class="strong"><strong>    &lt;server-group name="other-server-group" profile="ha"&gt;</strong></span>
        &lt;jvm name="default"&gt;
            &lt;heap size="64m" max-size="512m"/&gt;
        &lt;/jvm&gt;
<span class="strong"><strong>        &lt;socket-binding-group ref="ha-sockets"/&gt;</strong></span>
    &lt;/server-group&gt;
&lt;/server-groups&gt;</pre></div><p>As highlighted <a id="id922" class="indexterm"/>in the <code class="literal">socket-binding-group</code> element, we are referencing the <code class="literal">ha-sockets</code> group, which contains all socket bindings used for the cluster. Have a look at the following code:</p><div class="informalexample"><pre class="programlisting">&lt;socket-binding-group name="<span class="strong"><strong>ha-sockets</strong></span>" default-interface="public"&gt;
    &lt;socket-binding name="ajp" port="8009"/&gt;
    &lt;socket-binding name="http" port="8080"/&gt;
    &lt;socket-binding name="https" port="8443"/&gt;
    &lt;socket-binding name="jgroups-mping" port="0" multicast-address="230.0.0.4" multicast-port="45700"/&gt;
    &lt;socket-binding name="jgroups-tcp" port="7600"/&gt;
    &lt;socket-binding name="jgroups-tcp-fd" port="57600"/&gt;
    &lt;socket-binding name="jgroups-udp" port="55200" multicast-address="230.0.0.4" multicast-port="45688"/&gt;
    &lt;socket-binding name="jgroups-udp-fd" port="54200"/&gt;
    &lt;socket-binding name="modcluster" port="0" multicast-address="224.0.1.105" multicast-port="23364"/&gt;
    &lt;socket-binding name="txn-recovery-environment" port="4712"/&gt;
    &lt;socket-binding name="txn-status-manager" port="4713"/&gt;
    &lt;outbound-socket-binding name="mail-smtp"&gt;
        &lt;remote-destination host="localhost" port="25"/&gt;
    &lt;/outbound-socket-binding&gt;
&lt;/socket-binding-group&gt;</pre></div><p>Next, we need to define the servers that are part of the domain (and of the cluster). To keep things simple, we will reuse the domain server list that is found in the default <code class="literal">host.xml</code> file, as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">&lt;servers&gt;
    &lt;server name="server-one" group="main-server-group"&gt;
        &lt;jvm name="default"&gt;
    &lt;/server&gt;
    &lt;server name="server-two" group="main-server-group" auto-start="true"&gt;
        &lt;socket-bindings port-offset="150"/&gt;
    &lt;/server&gt;
    &lt;server name="server-three" group="other-server-group" auto-start="false"&gt;
        &lt;socket-bindings port-offset="250"/&gt;
    &lt;/server&gt;
&lt;/servers&gt;</pre></div><p>We do not need <a id="id923" class="indexterm"/>to specify the socket-binding group for each server, as this was configured in the <code class="literal">domain.xml</code> file. If we want to override the socket-binding group, then we can add the following to the <code class="literal">host.xml</code> file:</p><div class="informalexample"><pre class="programlisting">&lt;servers&gt;
    ...
    &lt;server name="server-one" group="other-server-group" auto-start="false"&gt;
        &lt;socket-bindings socket-binding-group="ha-sockets"/&gt;
    &lt;/server&gt;
&lt;/servers&gt;</pre></div><p>The following figure shows an overview of this configuration:</p><div class="mediaobject"><img src="graphics/6232OS_08_08.jpg" alt="Setting up a cluster of domain servers"/></div><p>Your clustered domain can now be started using the standard batch script (<code class="literal">domain.sh</code> or <code class="literal">domain.bat</code>). The <a id="id924" class="indexterm"/>server groups will now point to the <code class="literal">ha</code> profile and form a cluster of two nodes.</p><div class="section" title="Troubleshooting the cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec69"/>Troubleshooting the cluster</h3></div></div></div><p>Communication <a id="id925" class="indexterm"/>via nodes in a cluster is achieved <a id="id926" class="indexterm"/>via UDP and multicasts.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note62"/>Note</h3><p>A multicast is a protocol by which data is transmitted simultaneously to all hosts that are part of the multicast group. You can think about multicast as a radio channel where only those tuned to a particular frequency receive the data.</p></div></div><p>If you are having problems, typically it is due to one of the following reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The nodes are behind a firewall. If your nodes are on different machines, then it is possible that the firewall is blocking the multicasts. you can test this by disabling the firewall for each node or adding the appropriate rules.</li><li class="listitem" style="list-style-type: disc">You are using a home network or are behind a gateway. Typically, home networks will redirect any UDP traffic to the <a id="id927" class="indexterm"/><span class="strong"><strong>Internet Service Provider</strong></span> (<span class="strong"><strong>ISP</strong></span>), which is then either dropped by the ISP or just lost. To fix this, you will need to add a route to the firewall/gateway that will redirect any multicast traffic back on to the local network instead.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip28"/>Tip</h3><p>
<span class="strong"><strong>Mac OS X</strong></span>
</p><p>If you are using a Mac, you may get a <span class="strong"><strong>java.io.IOException: Network is unreachable</strong></span> error when trying to start a domain in the <code class="literal">ha</code> mode. To get around this, you will need to create a proper network route to use UDP as follows:</p><div class="informalexample"><pre class="programlisting">sudo route add 224.0.0.0 127.0.0.1 -netmask 240.0.0.0</pre></div></div></div><p>To allow you to check whether your machine is set up correctly for multicast, JGroups ships with two test applications that can be used to test IP multicast communication. The test classes are <code class="literal">McastReceiverTest</code> and <code class="literal">McastSenderTest</code>.</p><p>In order to test multicast communication on your server, you should first navigate to the location of the <code class="literal">jgroups</code> JAR within the <code class="literal">modules</code> directory, shown here:</p><p>
<code class="literal">JBOSS_HOME/modules/system/layers/base/org/jgroups/main</code>
</p><p>Within this directory, you will find <code class="literal">jgroups-3.4.3.Final.jar</code>, which contains the test programs.</p><p>Now, run the <code class="literal">McastReceiverTest</code> by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -classpath jgroups-3.4.3.Final.jar org.jgroups.tests.McastReceiverTest -mcast_addr 224.10.10.10 -port 5555</strong></span>
</pre></div><p>On the same machine, but in a different terminal, run the <code class="literal">McastSenderTest</code> command, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -classpath jgroups-3.4.3.Final.jar org.jgroups.tests.McastSenderTest -mcast_addr 224.10.10.10 -port 5555</strong></span>
</pre></div><p>If multicast <a id="id928" class="indexterm"/>works correctly, you should be able to type in the <code class="literal">McastSenderTest</code> window<a id="id929" class="indexterm"/> and see the output in the <code class="literal">McastReceiverTest</code> window, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/6232OS_08_09.jpg" alt="Troubleshooting the cluster"/></div><p>You should perform this test on each machine in the cluster. Once you have done this, you need to ensure that UDP communication works between each machine in the cluster by running <code class="literal">McastSenderTest</code> on one machine and <code class="literal">McastReceiverTest</code> on the other.</p><p>Finally, if you are experiencing issues with the default multicast address or port, you can change it by modifying the <code class="literal">jgroups-udp</code> socket-binding group within the <code class="literal">domain.xml</code> file:</p><div class="informalexample"><pre class="programlisting">&lt;socket-binding-groups&gt;
    ...
    &lt;socket-binding-group name="ha-sockets" default-interface="public"&gt;
        ...
        &lt;socket-binding name="jgroups-udp" port="55200" multicast-address="${jboss.default.multicast.address:230.0.0.4}" multicast-port="45688"/&gt;
        ...
    &lt;/socket-binding-group&gt;
&lt;/socket-binding-groups&gt;</pre></div></div></div></div></div>
<div class="section" title="Configuring the WildFly cluster"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec39"/>Configuring the WildFly cluster</h1></div></div></div><p>WildFly supports<a id="id930" class="indexterm"/> clustering out of the box. There are several libraries that work together to provide support for clustering. The following figure shows the basic clustering architecture adopted by WildFly:</p><div class="mediaobject"><img src="graphics/6232OS_08_10.jpg" alt="Configuring the WildFly cluster"/></div><p>The JGroups library is core to WildFly clustering. It provides the communication channels between nodes of the cluster using a multicast transmission. These channels are created upon deployment of a clustered application and are used to transmit sessions and contexts around the cluster.</p><p>Another important component of clustering is<a id="id931" class="indexterm"/> <span class="strong"><strong>Infinispan</strong></span>. Infinispan handles the replication of your application data across the cluster by means of a distributed cache.</p><div class="section" title="Configuring the JGroups subsystem"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec94"/>Configuring the JGroups subsystem</h2></div></div></div><p>Within the<a id="id932" class="indexterm"/> realm of JGroups, nodes are commonly referred to as<a id="id933" class="indexterm"/> <span class="strong"><strong>members</strong></span>, and clusters are referred to as <a id="id934" class="indexterm"/>
<span class="strong"><strong>groups</strong></span>.</p><p>A node<a id="id935" class="indexterm"/> is a<a id="id936" class="indexterm"/> process running on a host. JGroups keeps track of all processes within a group. When a node joins a group, the system sends a message to all existing members of that group. Likewise, when a node leaves or crashes, all the other nodes of that group are notified.</p><p>As we outlined earlier in the chapter, the processes (nodes) of a group can be located on the same host, or on different machines on a network. A member can also be part of multiple groups. The following figure illustrates a detailed view of JGroups architecture:</p><div class="mediaobject"><img src="graphics/6232OS_08_11.jpg" alt="Configuring the JGroups subsystem"/></div><p>A JGroups process broadly consists of three parts, namely a <span class="strong"><strong>Channel</strong></span>, the <span class="strong"><strong>Building Blocks</strong></span>, and the <span class="strong"><strong>Protocol Stack</strong></span>.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>Channel</strong></span><a id="id937" class="indexterm"/> is a simple socket-like interface used by application programmers to build reliable group communication applications.</li><li class="listitem" style="list-style-type: disc">The <span class="strong"><strong>Building Blocks</strong></span><a id="id938" class="indexterm"/> collectively form an abstraction interface layered on top of channels, which can be used instead of channels whenever a higher level interface is required.</li><li class="listitem" style="list-style-type: disc">The <span class="strong"><strong>Protocol Stack</strong></span><a id="id939" class="indexterm"/> contains a number of protocol layers in a bidirectional list. All messages sent have to pass through all the protocols. A layer does not <span class="emphasis"><em>necessarily</em></span> correspond to a transport protocol. For example, a fragmentation layer might break up a message into several smaller messages, adding a header with an ID to each fragment, and reassemble the fragments on the receiver's side.</li></ul></div><p>In the previous figure, when sending a message, the <code class="literal">PING</code> protocol is executed first, then <code class="literal">MERGE2</code>, followed by <code class="literal">FD_SOCK</code>, and finally, the <code class="literal">FD</code> protocol. When the message is received, this order would be reversed, which means that it would meet the <code class="literal">FD</code> protocol first, then <code class="literal">FD_SOCK</code>, followed by <code class="literal">MERGE2</code>, and finally up to <code class="literal">PING</code>. In WildFly, the JGroups configuration is found within the JGroups subsystem in the main <code class="literal">standalone-ha.xml/domain.xml</code> configuration file.</p><p>Within the JGroups subsystem, you can find the list of configured transport stacks. The following code snippet shows the default UDP stack used for communication between nodes:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem  default-stack="udp"&gt;
    &lt;stack name="udp"&gt;
        &lt;transport type="UDP" socket-binding="jgroups-udp"/&gt;
        &lt;protocol type="PING"/&gt;
        &lt;protocol type="MERGE3"/&gt;
        &lt;protocol type="FD_SOCK" socket-binding="jgroups-udp-fd"/&gt;
        &lt;protocol type="FD_ALL"/&gt;
        &lt;protocol type="VERIFY_SUSPECT"/&gt;
        &lt;protocol type="pbcast.NAKACK2"/&gt;
        &lt;protocol type="UNICAST3"/&gt;
        &lt;protocol type="pbcast.STABLE"/&gt;
        &lt;protocol type="pbcast.GMS"/&gt;
        &lt;protocol type="UFC"/&gt;
        &lt;protocol type="MFC"/&gt;
        &lt;protocol type="FRAG2"/&gt;
        &lt;protocol type="RSVP"/&gt;
    &lt;/stack&gt;
    ...
&lt;/subsystem&gt;</pre></div><p>UDP is the <a id="id940" class="indexterm"/>default protocol for JGroups and uses multicast (or, if not available, multiple unicast messages) to send and receive messages. A <a id="id941" class="indexterm"/>multicast UDP socket can send and receive datagrams from multiple clients. Another feature of multicast is that a client can contact multiple servers with a single packet, without knowing the specific IP address of any of the hosts.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note63"/>Note</h3><p>Switching to the TCP protocol is as easy as changing the <code class="literal">default-stack</code> attribute:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem  default-stack="tcp"&gt;</pre></div></div></div><p>TCP stacks are typically used when IP multicasting cannot be used for some reason. For example, when you want to create a network over a WAN. We will cover TCP configuration later in this chapter.</p><p>A detailed description of all JGroups protocols<a id="id942" class="indexterm"/> is beyond the scope of this book but, for convenience, you can find a short description of each in the following table. To find out more about these protocols, or about JGroups, you can refer to the <a id="id943" class="indexterm"/>JGroups site at <a class="ulink" href="http://jgroups.org/manual/html/index.html">http://jgroups.org/manual/html/index.html</a>.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Category</p>
</th><th style="text-align: left" valign="bottom">
<p>Usage</p>
</th><th style="text-align: left" valign="bottom">
<p>Protocols</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Transport</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id944" class="indexterm"/>is responsible for sending and receiving messages across the network</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">IDP</code>, <code class="literal">TCP</code>, and <code class="literal">TUNNEL</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Discovery</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id945" class="indexterm"/> is used to discover active nodes in the cluster and determine who the coordinator is</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">PING</code>, <code class="literal">MPING</code>, <code class="literal">TCPPING</code>, and <code class="literal">TCPGOSSIP</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Failure detection</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id946" class="indexterm"/>one is used to poll cluster nodes to detect node failures</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">FD</code>, <code class="literal">FD_SIMPLE</code>, <code class="literal">FD_PING</code>, <code class="literal">FD_ICMP</code>, <code class="literal">FD_SOCK</code>, and <code class="literal">VERIFY_SUSPECT</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Reliable delivery</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id947" class="indexterm"/> ensures that messages are actually delivered in the right order (FIFO) to the destination node</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">CAUSAL</code>, <code class="literal">NAKACK</code>, <code class="literal">pbcast</code>.<code class="literal">NAKACK</code>, <code class="literal">SMACK</code>, <code class="literal">UNICAST</code>, and <code class="literal">PBCAST</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Group membership</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id948" class="indexterm"/> is used to notify the cluster when a node joins, leaves, or crashes</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">pbcast</code>.<code class="literal">GMS</code>, <code class="literal">MERGE</code>, <code class="literal">MERGE2</code>, and <code class="literal">VIEW_SYNC</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Flow control</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id949" class="indexterm"/> used to adapt the data-sending rate to the data-receipt rate among the nodes</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">FC</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Fragmentation</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id950" class="indexterm"/>fragments messages larger than a certain size and unfragments them at the receiver's side</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">FRAG2</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>State transfer</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id951" class="indexterm"/>one synchronizes the application state (serialized as a byte array) from an existing node with a newly-joining node</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">pbcast</code>.<code class="literal">STATE_TRANSFER</code> and <code class="literal">pbcast</code>.<code class="literal">STREAMING_STATE_TRANSFER</code>
</p>
</td></tr></tbody></table></div><div class="section" title="Customizing the protocol stack"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec70"/>Customizing the protocol stack</h3></div></div></div><p>If you <a id="id952" class="indexterm"/>want to customize your transport configuration at a lower level, then you can override the default properties used by JGroups or even the single protocol properties. For example, the following configuration can be used to change the default send or receive buffer used by the JGroups UDP stack:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem  default-stack="udp"&gt;
  &lt;stack name="udp"&gt;
    &lt;transport type="UDP" socket-binding="jgroups-udp" diagnostics-socket-binding="jgroups-diagnostics"&gt;
      &lt;property name="ucast_recv_buf_size"&gt;50000000&lt;/property&gt;
      &lt;property name="ucast_send_buf_size"&gt;1280000&lt;/property&gt;
      &lt;property name="mcast_recv_buf_size"&gt;50000000&lt;/property&gt;
      &lt;property name="mcast_send_buf_size"&gt;1280000&lt;/property&gt;
    &lt;/transport&gt;
    ...
  &lt;/stack&gt;
&lt;/subsystem&gt;</pre></div><p>If you want to have a look at all the properties available within the JGroups subsystem, either at transport level or at the protocol level, you can consult the JGroups XSD file, <code class="literal">jboss-as-jgroups_2_0.xsd</code>, found in the <code class="literal">JBOSS_HOME/docs/schema</code> folder of your server distribution.</p></div></div></div>
<div class="section" title="Configuring the Infinispan subsystem"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec40"/>Configuring the Infinispan subsystem</h1></div></div></div><p>One of the<a id="id953" class="indexterm"/> requirements of a cluster is <a id="id954" class="indexterm"/>that data is synchronized across its members. This is because, should there be a failure of a node, the application and its session can continue on other members of the cluster. This is known as<a id="id955" class="indexterm"/> <span class="strong"><strong>High Availability</strong></span>.</p><p>WildFly uses Infinispan as the distributed caching solution behind its clustering functionality. Although Infinispan is embedded within the application server, it can also be used as a standalone data-grid platform.</p><p>We will now quickly look at Infinispan's configuration, which is found in the Infinispan subsystem within the main <code class="literal">standalone-ha.xml</code> or <code class="literal">domain.xml</code> configuration file.</p><p>The following is the backbone of the Infinispan configuration:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem &gt;
    &lt;cache-container name="server" aliases="singleton cluster" default-cache="default" module="org.wildfly.clustering.server"&gt;
    &lt;transport lock-timeout="60000"/&gt;
    &lt;replicated-cache name="default" mode="SYNC" batching="true"&gt;
        &lt;locking isolation="REPEATABLE_READ"/&gt;
    &lt;/replicated-cache&gt;
    &lt;/cache-container&gt;
    &lt;cache-container name="web" default-cache="dist" module="org.wildfly.clustering.web.infinispan"&gt;
        ...
    &lt;/cache-container&gt;
    &lt;cache-container name="ejb" aliases="sfsb" default-cache="dist" module="org.wildfly.clustering.ejb.infinispan"&gt;
        ...
    &lt;/cache-container&gt;
    &lt;cache-container name="hibernate" default-cache="local-query" module="org.hibernate"&gt;
        ...
    &lt;/cache-container&gt;
&lt;/subsystem&gt;</pre></div><p>One of the key <a id="id956" class="indexterm"/>differences between the standalone Infinispan configuration and the Infinispan subsystem within WildFly is that the WildFly configuration exposes multiple <code class="literal">cache-container</code> elements, while the native configuration file contains configurations for a single cache container.</p><p>Each <a id="id957" class="indexterm"/>
<code class="literal">cache-container</code> element <a id="id958" class="indexterm"/>contains one or more caching policies, which define how data is synchronized for that specific cache container. The following caching strategies can be used by cache containers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Local</strong></span>: In <a id="id959" class="indexterm"/>this caching mode, the entries are stored on the local node only, regardless of whether a cluster has formed. Infinispan typically operates as a local cache.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Replication</strong></span>: In this <a id="id960" class="indexterm"/>caching mode, all entries are replicated to all nodes. Infinispan typically operates as a temporary data store and doesn't offer increased heap space.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Distribution</strong></span>: In<a id="id961" class="indexterm"/> this caching mode, the entries are distributed to a subset of the nodes only. Infinispan typically operates as a data grid providing increased heap space.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Invalidation</strong></span>: In <a id="id962" class="indexterm"/>this caching mode, the entries are stored in a cache store only (such as a database) and invalidated from all nodes. When a node needs the entry, it will load it from a cache store. In this mode, Infinispan operates as a distributed cache, backed by a canonical data store, such as a database.</li></ul></div><p>In the following <a id="id963" class="indexterm"/>sections, we will have a more detailed look at some of the cache configurations, such as <code class="literal">session</code> caches (the <code class="literal">web</code> cache and the <code class="literal">SFSB</code> cache) and the <code class="literal">hibernate</code> cache. Understanding these is essential if you are to configure your clustered applications properly.</p><div class="section" title="Configuring session cache containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec95"/>Configuring session cache containers</h2></div></div></div><p>In this <a id="id964" class="indexterm"/>section, we will look at the caching configuration for the HTTP session and for stateful and singleton-session beans. The <a id="id965" class="indexterm"/>way the caches are configured for these three is very similar. For this reason, we will discuss them together and show the similarities between them. So, here is the <code class="literal">cache-container</code> configuration for the <code class="literal">web</code> cache, the <code class="literal">ejb</code> cache, and the <code class="literal">server</code> cache. The <code class="literal">web</code> cache refers to the HTTP session cache, the <code class="literal">ejb</code> cache relates to stateful session beans (SFSBs), and the <code class="literal">server</code> cache relates to the singleton-session beans:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem &gt;
    
    &lt;cache-container name="server" aliases="singleton cluster" default-cache="<span class="strong"><strong>default</strong></span>" module="org.wildfly.clustering.server"&gt;
        &lt;transport lock-timeout="60000" /&gt;
        &lt;replicated-cache name="<span class="strong"><strong>default</strong></span>" mode="SYNC" batching="true"&gt;
            &lt;locking isolation="REPEATABLE_READ" /&gt;
        &lt;/replicated-cache&gt;
    &lt;/cache-container&gt;

    &lt;cache-container name="web" default-cache="<span class="strong"><strong>dist</strong></span>" module="org.wildfly.clustering.web.infinispan"&gt;
        &lt;transport lock-timeout="60000" /&gt;
        &lt;distributed-cache name="<span class="strong"><strong>dist</strong></span>" mode="ASYNC" batching="true" l1-lifespan="0" owners="2"&gt;
            &lt;file-store /&gt;
        &lt;/distributed-cache&gt;
    &lt;/cache-container&gt;

    &lt;cache-container name="ejb" aliases="sfsb" default-cache="<span class="strong"><strong>dist</strong></span>" module="org.wildfly.clustering.ejb.infinispan"&gt;
        &lt;transport lock-timeout="60000" /&gt;
        &lt;distributed-cache name="<span class="strong"><strong>dist</strong></span>" mode="ASYNC" batching="true" l1-lifespan="0" owners="2"&gt;
            &lt;file-store /&gt;
        &lt;/distributed-cache&gt;
    &lt;/cache-container&gt;
&lt;/subsystem&gt;</pre></div><p>The configuration for each container can contain one or more caching strategy elements. These<a id="id966" class="indexterm"/> elements are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">replicated-cache</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">distributed-cache</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">invalidation-cache</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">local-cache</code></li></ul></div><p>Each of these <a id="id967" class="indexterm"/>cache elements can be defined zero or more times. To specify which cache element to use for the cache container, simply <a id="id968" class="indexterm"/>reference the name of the cache as the property of the <code class="literal">default-cache</code> attribute. In the next section, we will explore in detail the differences between these cache modes. Within each cache definition, you may have noticed the <code class="literal">locking</code> attribute that corresponds to the equivalent database isolation levels. Infinispan supports the following isolation levels:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">NONE</code>: No<a id="id969" class="indexterm"/> isolation level means no transactional support.</li><li class="listitem" style="list-style-type: disc"><code class="literal">READ_UNCOMMITTED</code>: The <a id="id970" class="indexterm"/>lowest isolation level, dirty reads are allowed, which means one transaction may see uncommitted data from another transaction. Rows are only locked during the writing of data, not for reads.</li><li class="listitem" style="list-style-type: disc"><code class="literal">READ_COMMITTED</code>: The <a id="id971" class="indexterm"/>transaction acquires read and write locks on all retrieved data. Write locks are released at the end of the transaction, and read locks are released as soon as the data is selected.</li><li class="listitem" style="list-style-type: disc"><code class="literal">REPEATABLE_READ</code>: This<a id="id972" class="indexterm"/> is the default isolation level used by Infinispan. The transaction acquires read and write locks on all retrieved data and is kept until the end of the transaction. Phantom reads can occur. Phantom reads are when you execute the same query in the same transaction and get a different number of results.</li><li class="listitem" style="list-style-type: disc"><code class="literal">SERIALIZABLE</code>: The<a id="id973" class="indexterm"/> strictest isolation level. All transactions occur in an isolated fashion as if they are being executed serially (one after the other), as opposed to concurrently.</li></ul></div><p>Another element nested within the cache configuration is <code class="literal">file-store</code>. This element configures the path in which to store the cached data. The default data is written in the <code class="literal">jboss.server.data.dir</code> directory under a folder with the same name as the cache container.</p><p>For example, the following figure shows the default <code class="literal">file-store</code> path for the standalone <code class="literal">web</code> cache container:</p><div class="mediaobject"><img src="graphics/6232OS_08_12.jpg" alt="Configuring session cache containers"/></div><p>If you wish, you can customize the <code class="literal">file-store</code> path using the <code class="literal">relative-to</code> and <code class="literal">path</code> elements, just as we did in <a class="link" href="ch02.html" title="Chapter 2. Configuring the Core WildFly Subsystems">Chapter 2</a>, <span class="emphasis"><em>Configuring the Core WildFly Subsystems</em></span>, for the path element:</p><div class="informalexample"><pre class="programlisting">&lt;cache-container name="web" default-cache="dist" module="org.wildfly.clustering.web.infinispan"&gt;
    &lt;distributed-cache name="dist" mode="ASYNC" batching="true" l1-lifespan="0" owners="2"&gt;
        &lt;file-store <span class="strong"><strong>relative-to="jboss.server.data.dir" path="my-cache"</strong></span>/&gt;
    &lt;/distributed-cache&gt;
&lt;/cache-container&gt;</pre></div><p>Before <a id="id974" class="indexterm"/>moving on, let's briefly look at the way messages are sent between each node.</p><p>Data synchronization across members can be done via synchronous messages (<code class="literal">SYNC</code>) or asynchronous messages (<code class="literal">ASYNC</code>), which are defined as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Synchronous </strong></span>messaging <a id="id975" class="indexterm"/>is the least efficient of the two, as each node needs to wait for a message acknowledgement from other cluster members. However, synchronous mode is useful if you have a need for high consistency.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Asynchronous</strong></span> messaging <a id="id976" class="indexterm"/>is the quicker of the two, the flip side being that consistency suffers. Asynchronous messaging is particularly useful when HTTP session replication and sticky sessions are enabled. In this scenario, a session is always accessed from the same cluster node. Only when a node fails is the data accessed from a different node.</li></ul></div><p>The SYNC and ASYNC properties are set in the <code class="literal">mode</code> attribute of the cache element:</p><div class="informalexample"><pre class="programlisting">&lt;distributed-cache name="dist" mode="ASYNC"    batching="true" l1-lifespan="0" owners="2"&gt;</pre></div></div><div class="section" title="Choosing between replication and distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec96"/>Choosing between replication and distribution</h2></div></div></div><p>When using<a id="id977" class="indexterm"/> <span class="strong"><strong>replicated</strong></span> caching, Infinispan<a id="id978" class="indexterm"/> will store every entry on every node in the cluster grid. This means that entries added to any one of these cache instances will be replicated to all other cache instances in the cluster, and any entry can be retrieved from any cache. The arrows indicate the direction in which data is being replicated. In the following figure, you can see that session data from Node 1 is being copied to Nodes 2, 3, and 4:</p><div class="mediaobject"><img src="graphics/6232OS_08_13.jpg" alt="Choosing between replication and distribution"/></div><p>The scalability of replication is a function of cluster size and average data size. If we have many nodes and/or large data sets, we hit a scalability ceiling.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note64"/>Note</h3><p>If <code class="literal">DATA_SIZE * NUMBER_OF_HOSTS</code> is smaller than the memory available to each host, then replication is a viable choice.</p></div></div><p>On the other hand, when using<a id="id979" class="indexterm"/> <span class="strong"><strong>distributed</strong></span> caching, Infinispan will store every cluster entry on a subset of the nodes in the grid.</p><p>Distribution makes use of a consistent-hash algorithm to determine where entries should be stored within the cluster. You can configure how many copies of a cache entry are maintained <a id="id980" class="indexterm"/>across the cluster. The value you choose here is a balance between performance and durability of data. The more copies you maintain, the lower the performance, but the lower the risk of losing data due to server outages.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note65"/>Note</h3><p>You can use the <code class="literal">owners</code> parameter (with a default value of <code class="literal">2</code>) to define the number of cluster-wide copies for each cache entry:</p><div class="informalexample"><pre class="programlisting">&lt;distributed-cache name="dist" mode="ASYNC" batching="true" l1-lifespan="0" owners="2"&gt;
    &lt;file-store/&gt;
&lt;/distributed-cache&gt;</pre></div></div></div><p>The following figure shows how the session data will be replicated across the nodes when the <code class="literal">owners</code> parameter is set to <code class="literal">2</code>. Each node replicates its session data to two other nodes:</p><div class="mediaobject"><img src="graphics/6232OS_08_14.jpg" alt="Choosing between replication and distribution"/></div><p>The choice between replication and distribution depends largely on the cluster size. For example, replication provides a quick and easy way to share states across a cluster; however, it only performs well in small clusters (fewer than ten servers). This is due to the increased number of replication messages that need to be sent as cluster size increases. In a distributed cache, several copies of an entry are maintained across nodes in order to provide <a id="id981" class="indexterm"/>redundancy and fault tolerance. The number of copies saved is typically far fewer than the number of nodes in the cluster. This means a distributed cache provides a far greater degree of scalability than a replicated cache.</p></div><div class="section" title="Configuring the hibernate cache"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec97"/>Configuring the hibernate cache</h2></div></div></div><p>The <a id="id982" class="indexterm"/>hibernate cache container <a id="id983" class="indexterm"/>is a <a id="id984" class="indexterm"/>key part of your configuration as it handles the caching of your data tier. WildFly uses hibernate as the default JPA implementation, so the concepts described in this chapter apply both to hibernate applications and to JPA-based applications. Hibernate caches are conceptually different from session-based caches. They are based on the assumption that you have a permanent storage for your data (the database) This means that it is not necessary to replicate or distribute copies of the entities across the cluster in order to achieve high availability. You just need to inform your nodes when data has been modified so that the entry in the cache can be invalidated. If a cache is configured for invalidation rather than replication, every time data is changed in a cache, other caches in the cluster receive a message that their data is now stale and should be evicted from memory.</p><p>The benefit of this is twofold. First, network traffic is minimized as invalidation messages are very small compared to replicating updated data. Second, caches in the cluster will only need to do database lookups when data is stale. Whenever a new entity or collection is read from database, it's only cached <span class="strong"><strong>locally</strong></span> in order to reduce traffic between nodes:</p><div class="informalexample"><pre class="programlisting">&lt;cache-container name="hibernate" default-cache="local-query" module="org.hibernate"&gt;
    &lt;transport lock-timeout="60000"/&gt;
    &lt;local-cache name="local-query"&gt;
        &lt;transaction mode="NONE"/&gt;
        &lt;eviction strategy="LRU" max-entries="10000"/&gt;
        &lt;expiration max-idle="100000"/&gt;
    &lt;/local-cache&gt;
    ...
&lt;/cache-container&gt;</pre></div><p>The <code class="literal">local-query</code> cache<a id="id985" class="indexterm"/> is configured by default to store up to 10,000 entries in an LRU vector. Each entry will be evicted from the cache automatically if it has been idle for 100,000 milliseconds, as per the <code class="literal">max-idle</code> attribute.</p><p>The following is a summary of the eviction strategies supported by Infinispan:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">NONE</code>: This <a id="id986" class="indexterm"/>value disables the eviction thread</li><li class="listitem" style="list-style-type: disc"><code class="literal">UNORDERED</code>: This <a id="id987" class="indexterm"/>is now deprecated. Using this value will cause the LRU to be used</li><li class="listitem" style="list-style-type: disc"><code class="literal">LRU</code>: This<a id="id988" class="indexterm"/> value causes evictions to occur based on a <a id="id989" class="indexterm"/><span class="strong"><strong>least-recently-used</strong></span> pattern</li><li class="listitem" style="list-style-type: disc"><code class="literal">LIRS</code>: This <a id="id990" class="indexterm"/>value addresses shortcomings of LRU. Eviction relies on inter-reference recency of cache entries</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip29"/>Tip</h3><p>To read more about <a id="id991" class="indexterm"/>how LIRS works, see the Infinispan documentation at <a class="ulink" href="http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies">http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies</a>.</p></div></div><p>Once the <a id="id992" class="indexterm"/>local cache <a id="id993" class="indexterm"/>entity is updated, the cache will send a message to other members of the cluster, telling them that the entity has been modified. This is when the <code class="literal">invalidation-cache</code> comes into play. Take a look at the following code:</p><div class="informalexample"><pre class="programlisting">&lt;invalidation-cache name="entity" mode="SYNC"&gt;
     &lt;transaction mode="NON_XA"/&gt;
     &lt;eviction strategy="LRU" max-entries="10000"/&gt;
     &lt;expiration max-idle="100000"/&gt;
&lt;/invalidation-cache&gt;</pre></div><p>The default configuration for the invalidation cache uses the same eviction and expiration settings as for local query cache. The maximum number of entries is set to 10,000 and the idle time before expiration to 100,000 milliseconds. The invalidation cache can also be configured to be synchronous (<code class="literal">SYNC</code>) or asynchronous (<code class="literal">ASYNC</code>). If you configure your invalidation cache to be synchronous, then your cache will be blocked until all caches in the cluster receive responses to invalidation messages. On the other hand, an asynchronous invalidation cache does not block and wait for a response, which results in increased performance. By default, hibernate is configured to use <code class="literal">REPEATABLE_READ</code> as the cache isolation level. For most cases, the default isolation level of <code class="literal">REPEATABLE_READ</code> will suffice. If you want to update it to, say, <code class="literal">READ_COMMITTED</code>, then you will need to add the following to your configuration:</p><div class="informalexample"><pre class="programlisting">&lt;invalidation-cache mode="SYNC" name="entity"&gt;
    ...
<span class="strong"><strong>    &lt;locking isolation="READ_COMMITTED"/&gt;</strong></span>
&lt;/invalidation-cache&gt;</pre></div><p>The last bit of configuration we are going to look at within the Infinispan subsystem is the <code class="literal">timestamp</code> cache. The <code class="literal">timestamp</code> cache<a id="id994" class="indexterm"/> keeps track of the last time each database table was updated.</p><p>The <code class="literal">timestamp</code> cache<a id="id995" class="indexterm"/> is strictly related to the query cache. It is used to store the result set of a query run against the database. If the query cache is enabled, before a query run, the query cache is checked. If the timestamp of the last update on a table is greater than the time the query results were cached, the entry is evicted from the cache and a fresh database lookup is made. This is referred to as a cache miss. Have a look at the following code:</p><div class="informalexample"><pre class="programlisting">&lt;replicated-cache name="timestamps" mode="ASYNC"&gt;
          &lt;transaction mode="NONE"/&gt;
          &lt;eviction strategy="NONE"/&gt;
&lt;/replicated-cache&gt;</pre></div><p>By default, the <code class="literal">timestamps</code> cache is configured with asynchronous replication as the clustering mode. Since<a id="id996" class="indexterm"/> all cluster nodes must store all timestamps, local or invalidated cache types are not allowed, and no eviction/expiration is allowed either.</p><div class="section" title="Using replication for the hibernate cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec71"/>Using replication for the hibernate cache</h3></div></div></div><p>There <a id="id997" class="indexterm"/>may be situations when you want to replicate your entity cache across other cluster nodes, instead of using local caches and invalidation. This may be the case when:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Queries executed are quite expensive</li><li class="listitem" style="list-style-type: disc">Queries are likely to be repeated in different cluster nodes</li><li class="listitem" style="list-style-type: disc">Queries are unlikely to be invalidated out of the cache (Hibernate invalidates query results from the cache when one of the entity classes involved in the query's <code class="literal">WHERE</code> clause changes)</li></ul></div><p>In order to switch to a replicated cache, you have to configure your <code class="literal">default-cache</code> attribute, as shown in the following code snippet, as well as add the relevant <code class="literal">replicated-cache</code> configuration:</p><div class="informalexample"><pre class="programlisting">&lt;cache-container name="hibernate" default-cache="replicated-cache" module="org.hibernate"&gt;
    &lt;replicated-cache name="replicated-cache" mode="SYNC"&gt;
        &lt;locking isolation="REPEATABLE_READ"/&gt;
    &lt;/replicated-cache&gt;
&lt;/cache-container&gt;</pre></div></div></div><div class="section" title="Advanced Infinispan configuration"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec98"/>Advanced Infinispan configuration</h2></div></div></div><p>Until now, we looked <a id="id998" class="indexterm"/>at the essential components required to get working with a clustered application. Infinispan has a wealth of options available to further customize your cache.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip30"/>Tip</h3><p>For more<a id="id999" class="indexterm"/> information on advanced configuration of Infinispan via the Infinispan subsystem, you can check out the documentation at <a class="ulink" href="http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html">http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html</a>
</p></div></div><div class="section" title="Configuring the Infinispan transport"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec72"/>Configuring the Infinispan transport</h3></div></div></div><p>The <a id="id1000" class="indexterm"/>Infinispan subsystem <a id="id1001" class="indexterm"/>relies on the JGroups subsystem to transport cached data between nodes. JGroups uses UDP as the default transport protocol as defined by the <code class="literal">default-stack</code> attribute in the JGroups subsystem:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem  default-stack="udp"&gt;
    ...
&lt;/subsystem&gt;</pre></div><p>You can, however, configure a different transport for each cache container. If you want to use TCP as the transport protocol for the web cache container, then you can add the <code class="literal">stack</code> attribute and set it to <code class="literal">tcp</code>:</p><div class="informalexample"><pre class="programlisting">&lt;cache-container name="web" default-cache="dist"&gt;
  &lt;transport lock-timeout="60000" stack="tcp"/&gt;
&lt;/cache-container&gt;</pre></div><p>The default UDP transport is usually suitable for large clusters. It may also be suitable if you are using replication or invalidation, as it minimizes opening too many sockets.</p><p>To learn about <a id="id1002" class="indexterm"/>the differences between TCP and UDP, please refer to this external link at <a class="ulink" href="http://www.skullbox.net/tcpudp.php">http://www.skullbox.net/tcpudp.php</a>.</p></div><div class="section" title="Configuring the Infinispan threads"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec73"/>Configuring the Infinispan threads</h3></div></div></div><p>It is important <a id="id1003" class="indexterm"/>to note that the thread-pool subsystem has been deprecated in WildFly 8. It is quite likely that it will be removed completely in WildFly 9. The configuration in this section can still be used in WildFly 8, but you will need to add the threads subsystem to your configuration file. Take a look at the following code:</p><div class="informalexample"><pre class="programlisting">&lt;extensions&gt;
    ...
    &lt;extension module="org.jboss.as.threads"/&gt;
&lt;/extensions&gt;</pre></div><p>Just as you can for <a id="id1004" class="indexterm"/>JGroups transport, you can externalize your Infinispan thread configuration, moving it into the thread-pool subsystem. The following thread pools can be configured per cache-container:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Thread pool</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>transport</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id1005" class="indexterm"/> gives the size of the bounded thread pool whose threads are responsible for transporting data across the network</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>listener-executor</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1006" class="indexterm"/>gives the size of the thread pool used for registering and getting notified when some cache events take place</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>replication-queue-executor</p>
</td><td style="text-align: left" valign="top">
<p>This gives <a id="id1007" class="indexterm"/>the size of the scheduled replication executor used for replicating cache data</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>eviction-executor</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1008" class="indexterm"/>gives the size of the scheduled executor service used to periodically run eviction cleanup tasks</p>
</td></tr></tbody></table></div><p>Customizing the thread pool may be required in some cases, for example, you may want to apply a cache replication algorithm. You may then need to choose the number of threads used for replicating data. In the following example, we are externalizing the thread pools of the web <code class="literal">cache-container</code> by defining a maximum of 25 threads for the bounded-queue-thread-pool and five threads for replicating data:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem &gt;

  &lt;cache-container name="web" default-cache="repl" listener-executor="infinispan-listener" eviction-executor="infinispan-eviction" replication-queue-executor="infinispan-repl-queue"&gt;
    &lt;transport executor="infinispan-transport"/&gt;
  &lt;/cache-container&gt;
&lt;/subsystem&gt;
...
&lt;subsystem &gt;
  &lt;thread-factory name="infinispan-factory" priority="1"/&gt;
  &lt;bounded-queue-thread-pool name="infinispan-transport"/&gt;
    &lt;core-threads count="1"/&gt;
    &lt;queue-length count="100000"/&gt;
    &lt;max-threads count="25"/&gt;
    &lt;thread-factory name="infinispan-factory"/&gt;
  &lt;/bounded-queue-thread-pool&gt;
  &lt;bounded-queue-thread-pool name="infinispan-listener"/&gt;
    &lt;core-threads count="1"/&gt;
    &lt;queue-length count="100000"/&gt;
    &lt;max-threads count="1"/&gt;
    &lt;thread-factory name="infinispan-factory"/&gt;
  &lt;/bounded-queue-thread-pool&gt;
  &lt;scheduled-thread-pool name="infinispan-eviction"/&gt;
    &lt;max-threads count="1"/&gt;
    &lt;thread-factory name="infinispan-factory"/&gt;
  &lt;/scheduled-thread-pool&gt;
  &lt;scheduled-thread-pool name="infinispan-repl-queue"/&gt;
    &lt;max-threads count="5"/&gt;
    &lt;thread-factory name="infinispan-factory"/&gt;
  &lt;/scheduled-thread-pool&gt;
&lt;/subsystem&gt;</pre></div></div></div></div>
<div class="section" title="Clustering the messaging subsystem"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec41"/>Clustering the messaging subsystem </h1></div></div></div><p>We will <a id="id1009" class="indexterm"/>conclude this chapter by discussing the messaging subsystem.</p><p>The <a id="id1010" class="indexterm"/>JMS provider used in WildFly is <a id="id1011" class="indexterm"/>
<span class="strong"><strong>HornetQ</strong></span>. In order to share the message processing load, HornetQ servers can be grouped together in a cluster. Each active node in the cluster contains an active HornetQ server. HornetQ manages its own messages and handles its own connections. Behind the scenes, when a node forms a cluster connection to another node, a core bridge connection is created between them. Once the connection has been established, messages can flow between each of the nodes.</p><p>Clustering is automatically enabled in HornetQ if one or more <code class="literal">cluster-connection</code> elements are defined. The following example is taken from the default <code class="literal">full-ha</code> profile:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem &gt;
    &lt;hornetq-server&gt;
        ...
        &lt;cluster-connections&gt;
             &lt;cluster-connection name="my-cluster"&gt;
                 &lt;address&gt;jms&lt;/address&gt;
                 &lt;connector-ref&gt;http-connector&lt;/connector-ref&gt;
                 &lt;discovery-group-ref discovery-group-name="dg-group1"/&gt;
             &lt;/cluster-connection&gt;
        &lt;/cluster-connections&gt;
    &lt;/hornetq-server&gt;
&lt;/subsystem&gt;</pre></div><p>Now, let's look<a id="id1012" class="indexterm"/> at how to configure <a id="id1013" class="indexterm"/>the <code class="literal">cluster-connection</code>. The following is a typical cluster connection configuration. You can either update the default <code class="literal">cluster-connection</code>, or you can add your own <code class="literal">cluster-connection</code> element within your <code class="literal">&lt;hornetq-server&gt;</code> definition.</p><div class="informalexample"><pre class="programlisting">&lt;subsystem &gt;
    &lt;hornetq-server&gt;
        ...
        &lt;cluster-connections&gt;
             &lt;cluster-connection name="my-cluster"&gt;
                 &lt;address&gt;jms&lt;/address&gt;
                 &lt;connector-ref&gt;http-connector&lt;/connector-ref&gt;
                 &lt;discovery-group-ref discovery-group-name="dg-group1"/&gt;
                 &lt;retry-interval&gt;500&lt;/retry-interval&gt;
                 &lt;forward-when-no-consumers&gt;false&lt;/forward-when-no-consumers&gt;
                 &lt;max-hops&gt;1&lt;/max-hops&gt;
             &lt;/cluster-connection&gt;
        &lt;/cluster-connections&gt;
    &lt;/hornetq-server&gt;
&lt;/subsystem&gt;</pre></div><p>The <code class="literal">cluster-connection</code> <code class="literal">name</code> attribute<a id="id1014" class="indexterm"/> obviously defines the cluster connection name, which we are going to configure. There can be zero or more cluster connections configured in your messaging subsystem.</p><p>The <code class="literal">address</code> element<a id="id1015" class="indexterm"/> is a mandatory parameter and determines how messages are distributed across the cluster. In this example, the cluster connection will only load balance the messages that are sent to addresses that start with <code class="literal">jms</code>. This cluster connection will, in effect, apply to all JMS queue and topic subscriptions. This is because they map to core queues that start with the substring <code class="literal">jms</code>.</p><p>The <code class="literal">connector-ref</code> element<a id="id1016" class="indexterm"/> references the connector, which has been defined in the <code class="literal">connectors</code> section of the messaging subsystem. In this case, we are using the http connector (see <a class="link" href="ch03.html" title="Chapter 3. Configuring Enterprise Services">Chapter 3</a>, <span class="emphasis"><em>Configuring Enterprise Services</em></span>, for more information about the available connectors).The <code class="literal">retry-interval</code> element determines the interval in milliseconds between the message retry attempts. If a cluster connection is attempted and the target node has not been started, or is in the process of being rebooted, then connection attempts from other nodes commence only once the time period defined in the <code class="literal">retry-interval</code> has elapsed.</p><p>The <code class="literal">forward-when-no-consumers</code> element, when set to <code class="literal">true</code>, will ensure that each incoming message <a id="id1017" class="indexterm"/>is distributed <a id="id1018" class="indexterm"/>round robin even though there may not be a consumer on the receiving node.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note66"/>Note</h3><p>You can actually specify the connection load-balancing policy within the <code class="literal">connection-factory</code> element. The out-of-the-box policies are<a id="id1019" class="indexterm"/> <span class="strong"><strong>Round-Robin</strong></span> (<code class="literal">org.hornetq.api.core.client.loadbalance.RoundRobinConnectionLoadBalancingPolicy</code>) and<a id="id1020" class="indexterm"/> <span class="strong"><strong>Random</strong></span> (<code class="literal">org.hornetq.api.core.client.loadbalance.RandomConnectionLoadBalancingPolicy</code>). You can also add your own policy by implementing the <code class="literal">org.hornetq.api.core.client.loadbalance.ConnectionLoadBalancingPolicy</code> interface.</p><p>The following example shows how to use the random policy for a connection factory:</p><div class="informalexample"><pre class="programlisting">&lt;connection-factory name="InVmConnectionFactory"&gt;
    ...
    &lt;connection-load-balancing-policy-class-name&gt;org.hornetq.api.core.client.loadbalance.RandomConnectionLoadBalancingPolicy
    &lt;/connection-load-balancing-policy-class-name&gt;
&lt;/connection-factory&gt;</pre></div></div></div><p>Finally, the optional <code class="literal">max-hops</code> value is set to <code class="literal">1</code> (default), which is the maximum number of times a message can be forwarded between nodes. A value of <code class="literal">1</code> means messages are only load balanced to other HornetQ serves, which are directly connected to this server. HornetQ can also be configured to load-balance messages to nodes that are indirectly connected to it, that is, the other HornetQ servers are intermediaries in a chain.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip31"/>Tip</h3><p>You can also refer to <code class="literal">jboss-as-messaging_2_0.xsd</code> for the full list of available parameters. This can be found in the <code class="literal">JBOSS_HOME/docs/schema</code> folder of your server distribution.</p></div></div><div class="section" title="Configuring messaging credentials"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec99"/>Configuring messaging credentials</h2></div></div></div><p>If you <a id="id1021" class="indexterm"/>try to<a id="id1022" class="indexterm"/> start a cluster where nodes use the <code class="literal">full-ha</code> profile, you will get an error logged to the console, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ERROR [org.hornetq.core.server] (default I/O-1) HQ224018: Failed to create session: HornetQClusterSecurityException[errorType=CLUSTER_SECURITY_EXCEPTION message=HQ119099: Unable to authenticate cluster user: HORNETQ.CLUSTER.ADMIN.USER]</strong></span>
</pre></div><p>This is because, when attempting to create connections between nodes, HornetQ uses a cluster user and cluster password. As you can see in the default configuration, you are required to update the password value:</p><div class="informalexample"><pre class="programlisting">&lt;subsystem &gt;
    &lt;hornetq-server&gt;
        &lt;cluster-password&gt;${jboss.messaging.cluster.password:<span class="strong"><strong>CHANGE ME!!</strong></span>}&lt;/cluster-password&gt;
        &lt;journal-file-size&gt;102400&lt;/journal-file-size&gt;
        ...
    &lt;/hornetq-server&gt;
&lt;/subsystem&gt;</pre></div><p>Once you have changed this password,  start your cluster, and you should see a successful bridge between nodes:</p><div class="mediaobject"><img src="graphics/6232OS_08_15.jpg" alt="Configuring messaging credentials"/></div></div></div>
<div class="section" title="Configuring clustering in your applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec42"/>Configuring clustering in your applications</h1></div></div></div><p>We will <a id="id1023" class="indexterm"/>now complete our journey through the clustering system by looking at how to cluster the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Session beans</li><li class="listitem" style="list-style-type: disc">Entities</li><li class="listitem" style="list-style-type: disc">Web applications</li></ul></div></div>
<div class="section" title="Clustering session beans"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec43"/>Clustering session beans</h1></div></div></div><p>In <a class="link" href="ch03.html" title="Chapter 3. Configuring Enterprise Services">Chapter 3</a>, <span class="emphasis"><em>Configuring Enterprise Services</em></span>, we discussed the difference between <span class="strong"><strong>Stateless Session Beans</strong></span> (<span class="strong"><strong>SLSBs</strong></span>), <span class="strong"><strong>Stateful Session Beans</strong></span> (<span class="strong"><strong>SFSBs</strong></span>), and <span class="strong"><strong>Singleton Session Beans</strong></span>.</p><p>SLSBs are<a id="id1024" class="indexterm"/> not able to retain state between invocations, so<a id="id1025" class="indexterm"/> the <a id="id1026" class="indexterm"/>main benefit of clustering an SLSB is to balance the load between an array of servers:</p><div class="informalexample"><pre class="programlisting">@Stateless
@Clustered
public class ClusteredBean {
   public void doSomething() {
   // Do something
   }
}</pre></div><p>If you want to further specialize your SLSB, then you can choose the load-balancing algorithm <a id="id1027" class="indexterm"/>used to distribute the load between your EJBs. The following are the available load-balancing policies for your SLSB:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Load-balancing policy</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">RoundRobin</code>
</p>
</td><td style="text-align: left" valign="top">
<p>It is <a id="id1028" class="indexterm"/>the default load-balancing policy. The smart proxy cycles through a list of WildFly Server instances in a fixed order.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">RandomRobin</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Under<a id="id1029" class="indexterm"/> this policy, each request is redirected by the smart proxy to a random node in the cluster.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">FirstAvailable</code>
</p>
</td><td style="text-align: left" valign="top">
<p>It <a id="id1030" class="indexterm"/>implies a random selection of the node, but subsequent calls will stick to that node until the node fails. The next node will again be selected randomly.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">FirstAvailableIdenticalAllProxies</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1031" class="indexterm"/>is the same as <code class="literal">FirstAvailable</code>, except that the random node selection will then be shared by all dynamic proxies.</p>
</td></tr></tbody></table></div><p>Then, you can apply the load-balancing policy as in the following example:</p><div class="informalexample"><pre class="programlisting">@Clustered(loadBalancePolicy="FirstAvailable")</pre></div><p>In JBoss AS 7, you were required to annotate your SFSB with <code class="literal">@Clustered</code> in order to replicate the state of the SFSB. In WildFly, this is not the case, as SFSB are configured to have passivation enabled by default. This means that as long as you annotate your bean with <code class="literal">@Stateful</code>, and you are using a server profile that supports high availability, your SFSB will have its state replicated across servers. Have a look at the following code: </p><div class="informalexample"><pre class="programlisting">@Stateful
public class ClusteredBean {
  public void doSomething() {
  // Do something
  }
}</pre></div><p>To disable passivation/replication, you can simply set <code class="literal">passivationCapable</code> to <code class="literal">false</code>, as shown here:</p><div class="informalexample"><pre class="programlisting">@Stateful(passivationCapable=false)
public class ClusteredBean {
   public void doSomething() {
   // Do something
   }
}</pre></div><p>By default, SFSBs use the cache container named <code class="literal">ejb</code>, which replicates sessions across all nodes. Should your application server node fail while sessions are running, the EJB proxy will detect it and choose another node where session data has been replicated. You can, however, reference a custom cache container used by your SFSB with the <code class="literal">@org.jboss.ejb3.annotation.CacheConfig</code> annotation. Have a look at the following code:</p><div class="informalexample"><pre class="programlisting">@Stateful
@CacheConfig(name="custom-ejb")
public class ClusteredBean {
  ...
}</pre></div><p>The<a id="id1032" class="indexterm"/> following is the corresponding cache container that uses a <a id="id1033" class="indexterm"/>distributed cache:</p><div class="informalexample"><pre class="programlisting">&lt;cache-container name="custom-ejb" default-cache="dist" module="org.wildfly.clustering.ejb.infinispan" aliases="sfsb"&gt;
    &lt;distributed-cache name="dist" batching="true" mode="ASYNC" owners="3"&gt;
        &lt;locking isolation="REPEATABLE_READ"/&gt;
        &lt;file-store/&gt;
    &lt;/distributed-cache&gt;
&lt;/cache-container&gt;</pre></div></div>
<div class="section" title="Clustering entities"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec44"/>Clustering entities</h1></div></div></div><p>As <a id="id1034" class="indexterm"/>entities sit deep in the backend, they do not need to be considered<a id="id1035" class="indexterm"/> with regard to load-balancing logic or session replication. However, it is useful to cache your entities to avoid roundtrips to the database. The EJB3 persistence layer implementation in WildFly is Hibernate 4.3.5. The Hibernate framework includes a complex cache mechanism, which is implemented both at the Session level and at the SessionFactory level.</p><p>The cache used in the Session level is called the first-level cache and only has session scope. This cache is cleared as soon as the Hibernate session using it is closed. Hibernate uses second-level caching to store entities or collections retrieved from the database. It can also store the results of recent queries. It is the second-level cache that we need to cluster, as this cache is used across sessions.</p><p>Enabling the second-level cache for your enterprise applications is relatively straightforward. If you are using JPA, then all you need to do to enable the second-level cache is add the following to your <code class="literal">persistence.xml</code> configuration file:</p><div class="informalexample"><pre class="programlisting">&lt;shared-cache-mode&gt;ENABLE_SELECTIVE&lt;/shared-cache-mode&gt;
&lt;properties&gt;

    &lt;property name="hibernate.cache.use_second_level_cache" value="true"/&gt;
    &lt;property name="hibernate.cache.use_minimal_puts" value="true"/&gt;
&lt;/properties&gt;</pre></div><p>The first element, <code class="literal">shared-cache-mode</code>, is the JPA 2.x way of specifying whether the entities and <a id="id1036" class="indexterm"/>entity-related state of a persistence unit will be cached. The <code class="literal">shared-cache-mode</code> element has five possible values, as indicated in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Shared Cache mode</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">ALL</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1037" class="indexterm"/>value causes all entities and entity-related states and data to be cached.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">NONE</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id1038" class="indexterm"/> value causes caching to be disabled for the persistence unit.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">ENABLE_SELECTIVE</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1039" class="indexterm"/>value allows caching if the <code class="literal">@Cacheable</code> annotation is specified on the entity class.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">DISABLE_SELECTIVE</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id1040" class="indexterm"/>value enables the cache and causes all entities to be cached except those for which <code class="literal">@Cacheable(false)</code> is specified.</p>
</td></tr></tbody></table></div><p>The property named <code class="literal">hibernate.cache.use_minimal_puts</code> performs some optimization on the second-level cache by reducing the amount of writes in the caches at the cost <a id="id1041" class="indexterm"/>of some additional reads. This is beneficial when clustering your entities, as the put operation is very expensive as it activates cache replication listeners.</p><p>In addition, if you plan to use the Hibernate query cache in your applications, you need to activate it with a separate property, as follows:</p><div class="informalexample"><pre class="programlisting">&lt;property name="hibernate.cache.use_query_cache" value="true"/&gt;</pre></div><p>For the sake of completeness, we will also include the configuration needed to use Infinispan as a caching provider for native Hibernate applications. This is the list of properties you have to add to your <code class="literal">hibernate.cfg.xml</code>:</p><div class="informalexample"><pre class="programlisting">&lt;property name="hibernate.cache.region.factory_class" value="org.hibernate.cache.infinispan.JndiInfinispanRegionFactory"/&gt;
&lt;property name="hibernate.cache.infinispan.cachemanager" value="java:CacheManager/entity"/&gt;
&lt;property name="hibernate.transaction.manager_lookup_class" value="org.hibernate.transaction.JBossTransactionManagerLookup"/&gt;
&lt;property name="hibernate.cache.use_second_level_cache" value="true"/&gt;
&lt;property name="hibernate.cache.use_minimal_puts" value="true"/&gt;</pre></div><p>As you can see, the configuration is more verbose because you have to tell Hibernate to use Infinispan as a caching provider. This requires setting the correct Hibernate transaction factory using the <code class="literal">hibernate.transaction.factory_class</code> property.</p><p>The <code class="literal">hibernate.cache.infinispan.cachemanager</code> property exposes the cache manager used by Infinispan. By default, Infinispan binds the cache manager responsible for the second-level cache to the JNDI name <code class="literal">java:CacheManager/entity</code>.</p><p>Finally, the <code class="literal">hibernate.cache.region.factory_class</code> property tells Hibernate to use Infinispan's second-level <a id="id1042" class="indexterm"/>caching integration, which uses <code class="literal">CacheManager</code>, as <a id="id1043" class="indexterm"/>defined previously, as the source for the Infinispan cache's instances.</p></div>
<div class="section" title="Caching entities"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec45"/>Caching entities</h1></div></div></div><p>Unless <a id="id1044" class="indexterm"/>you have set <code class="literal">shared-cache-mode</code> to <code class="literal">ALL</code>, Hibernate will not<a id="id1045" class="indexterm"/> automatically cache your entities. You have to select which entities or queries need to be cached. This is definitely the safest option since indiscriminate caching can hurt performance. The following example shows how to do this for JPA entities using annotations:</p><div class="informalexample"><pre class="programlisting">import javax.persistence.*;
import org.hibernate.annotations.Cache;
import org.hibernate.annotations.CacheConcurrencyStrategy;

@Entity
@Cacheable
@Cache(usage = CacheConcurrencyStrategy.TRANSACTIONAL, region ="properties") 

public class Property {

@Id
@Column(name="key")
private String key;

@Column(name="value")
private String value;

// Getter &amp; setters omitted for brevity
}</pre></div><div class="section" title="Using JPA annotations"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec100"/>Using JPA annotations</h2></div></div></div><p>The <code class="literal">@javax.persistence.Cacheable</code> annotation dictates whether this entity class should <a id="id1046" class="indexterm"/>be cached in the second-level cache. This <a id="id1047" class="indexterm"/>is only applicable when the <code class="literal">shared-cache-mode</code> is<a id="id1048" class="indexterm"/> not set to <code class="literal">ALL</code>.</p></div><div class="section" title="Using Hibernate annotations"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec101"/>Using Hibernate annotations</h2></div></div></div><p>The <code class="literal">@org.hibernate.annotations.Cache</code> annotation is the older annotation used to achieve <a id="id1049" class="indexterm"/>the same purpose as <code class="literal">@Cacheable</code>. You can still use it to define which strategy Hibernate should use to control concurrent <a id="id1050" class="indexterm"/>access to cache contents.</p><p>The <code class="literal">CacheConcurrencyStrategy.TRANSACTIONAL</code> property provides support for Infinispan's<a id="id1051" class="indexterm"/> fully-transactional JTA environment.</p><p>If there is a chance that your application data is read but never modified, you can apply the <code class="literal">CacheConcurrencyStrategy.READ_ONLY</code> property that does not evict data from the cache (unless performed programmatically):</p><div class="informalexample"><pre class="programlisting">@Cache(usage=CacheConcurrencyStrategy.READ_ONLY)</pre></div><p>Finally, the last attribute is the caching region that defines where entities are placed. If you do not specify a cache region for an entity class, all instances of this class will be cached in the <code class="literal">_default</code> region. Defining a caching region can be useful if you want to perform a fine-grained management of caching areas.</p></div></div>
<div class="section" title="Caching queries"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec46"/>Caching queries</h1></div></div></div><p>The <a id="id1052" class="indexterm"/>query cache can be used to cache the result set of a query. This <a id="id1053" class="indexterm"/>means that if the same query is issued again, it will not hit the database but return the cached value.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note67"/>Note</h3><p>The query cache does not cache the state of the actual entities in the result set; it caches only the identifier values and results of the value type.</p></div></div><p>In the following example, the query result set named <code class="literal">listUsers</code> is configured to be cached using the <code class="literal">@QueryHint</code> annotation inside a <code class="literal">@NamedQuery</code> annotation:</p><div class="informalexample"><pre class="programlisting">@NamedQueries(
{
@NamedQuery(
name = "listUsers",
query = "FROM User c WHERE c.name = :name",
hints = { @QueryHint(name = "org.hibernate.cacheable", value =
"true") }
)
})
public class User {

@Id
@Column(name="key")
private String key;

@Column(name="name")
private String name;

...
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note68"/>Note</h3><p>Overuse <a id="id1054" class="indexterm"/>of the query cache may reduce your application's performance, so use it wisely. First, the query cache will increase the<a id="id1055" class="indexterm"/> memory requirements if your queries (stored as key in the query cache map) are made up of hundreds of characters.</p><p>Second, and more important, the result of the query cache is invalidated each time there's a change in one of the tables you are querying. This can lead to a very poor hit ratio of the query cache. Therefore, it is advisable to turn off the query cache unless you are querying a table that is seldom updated.</p></div></div></div>
<div class="section" title="Clustering web applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec47"/>Clustering web applications</h1></div></div></div><p>Clustering<a id="id1056" class="indexterm"/> web applications requires the least effort. As we <a id="id1057" class="indexterm"/>touched upon earlier, all you need to do to switch on clustering in a web application is add the following directive in the <code class="literal">web.xml</code>:</p><div class="informalexample"><pre class="programlisting">&lt;web-app&gt;
<span class="strong"><strong>    &lt;distributable/&gt;</strong></span>
&lt;/web-app&gt;</pre></div><p>By default, clustered web applications will use the web cache contained in the Infinispan configuration. You also have the option of setting up a specific cache per deployment unit. This can be achieved by adding the <code class="literal">replication-config</code> directive to the <code class="literal">jboss-web.xml</code> file and specifying the cache name to use:</p><div class="informalexample"><pre class="programlisting">&lt;jboss-web&gt;
  &lt;replication-config&gt;
<span class="strong"><strong>     &lt;cache-name&gt;web.dist&lt;/cache-name&gt;</strong></span>
  &lt;/replication-config&gt;
&lt;/jboss-web&gt;</pre></div><p>The <a id="id1058" class="indexterm"/>previous configuration should obviously reference <a id="id1059" class="indexterm"/>a cache defined in the main configuration file:</p><div class="informalexample"><pre class="programlisting">&lt;cache-container name="web" default-cache="repl"&gt;
   &lt;alias&gt;standard-session-cache&lt;/alias&gt;

   &lt;distributed-cache mode="ASYNC" name="web.dist" batching="true"&gt;
        &lt;locking isolation="REPEATABLE_READ"/&gt;
        &lt;file-store/&gt;
   &lt;/distributed-cache&gt;
&lt;/cache-container&gt;</pre></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec48"/>Summary</h1></div></div></div><p>In this chapter, we looked at a lot of configuration options around clustering. There was a lot of information to take in but, in summary, we will mention the following key points.</p><p>A WildFly cluster can be composed of either standalone nodes or as part of a domain of servers .The clustering subsystem is defined in the <code class="literal">standalone-ha.xml</code> and <code class="literal">standalone-full-ha.xml</code> configurations files.</p><p>There are three main components required for clustering: JGroups, Infinispan, and messaging. JGroups provides communication between nodes in a cluster. By default, JGroups uses UDP multicast messages to handle the cluster life cycle events.</p><p>Within enterprise applications, there are several caches that need to be configured in order to achieve consistency of data. There are four cache containers configured by default in WildFly. These are the singleton session bean cluster <code class="literal">cache-container</code>, the SLSB <code class="literal">cache-container</code>, the web <code class="literal">cache-container</code>, and the Hibernate <code class="literal">cache-container</code>.</p><p>The singleton cluster (server) <code class="literal">cache-container</code> is configured to replicate singleton session bean data across nodes in the cluster. The SFSB's (ejb) <code class="literal">cache-container</code> is configured to replicate stateful session bean data across nodes in the cluster. The web <code class="literal">cache-container</code> is configured to replicate HTTP session data across nodes in the cluster. The Hibernate <code class="literal">cache-container</code> uses a more complex approach by defining a <code class="literal">local-query</code> strategy to handle local entities. An <code class="literal">invalidation-cache</code> is used when data is updated and other cluster nodes need to be informed. Finally, a <code class="literal">replicated-cache</code> is used to replicate the query timestamps.</p><p>Lastly, we looked at the messaging subsystem, which can be easily clustered by defining one <code class="literal">cluster-connection</code> element. This will cause messages to be transparently load-balanced across your JMS servers.</p><p>In the next chapter we will look at load balancing, the other half of the story when it comes to configuring high availability.</p></div></body></html>