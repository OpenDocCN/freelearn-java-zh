- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: ETL Data Load – A Batch-Based Solution to Ingesting Data in a Data Warehouse
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ETL数据加载 - 在数据仓库中摄入数据的基于批次的解决方案
- en: In the previous chapters, we discussed various foundational concepts surrounding
    data engineering, starting with the different types of data engineering problems.
    Then, we discussed various data types, data formats, data storage, and databases.
    We also discussed the various platforms that are available to deploy and run data
    engineering solutions in production.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了围绕数据工程的各种基础概念，从不同类型的数据工程问题开始。然后，我们讨论了各种数据类型、数据格式、数据存储和数据库。我们还讨论了可用于在生产环境中部署和运行数据工程解决方案的各种平台。
- en: In this chapter, we will learn how to architect and design a batch-based solution
    for low to medium-volume data ingestion from a data source to a **data warehouse**.
    Here, we will be taking a real-time use case to discuss, model, and design a data
    warehouse for such a scenario. We will also learn how to develop this solution
    using a Java-based technical stack and run and test our solution. By the end of
    this chapter, you should be able to design and develop an **extract, transform,
    load** (**ETL**)-based batch pipeline using Java and its related stack.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何架构和设计一个从数据源到**数据仓库**的低到中等数据摄入的基于批次的解决方案。在这里，我们将讨论一个实时用例，讨论、建模和设计一个适用于此类场景的数据仓库。我们还将学习如何使用基于Java的技术栈开发此解决方案，并运行和测试我们的解决方案。到本章结束时，您应该能够使用Java及其相关堆栈设计并开发一个**提取、转换、加载**（**ETL**）基于批次的管道。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding the problem and source data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解问题和源数据
- en: Building an effective data model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建有效的数据模型
- en: Designing the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Implementing and unit testing the solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施和单元测试解决方案
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code files for this chapter in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter04/SpringBatchApp/EtlDatawarehouse](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter04/SpringBatchApp/EtlDatawarehouse).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的GitHub仓库中找到本章的所有代码文件：[https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter04/SpringBatchApp/EtlDatawarehouse](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter04/SpringBatchApp/EtlDatawarehouse).
- en: Understanding the problem and source data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解问题和源数据
- en: Data engineering often involves collecting, storing, and analyzing data. But
    nearly all data engineering landscapes start with ingesting raw data into a data
    lake or a data warehouse. In this chapter, we will be discussing one such typical
    use case and build an end-to-end solution for the problem discussed in the following
    section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程通常涉及收集、存储和分析数据。但几乎所有数据工程领域都是从将原始数据摄入数据湖或数据仓库开始的。在本章中，我们将讨论这样一个典型用例，并构建一个针对以下章节讨论的问题的端到端解决方案。
- en: Problem statement
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Company XYZ is a third-party vendor that provides services for building and
    maintaining data centers. Now, Company XYZ is planning to build a data center
    monitoring tool for its customer. The customer wants to see various useful metrics,
    such as the number of incidents reported for any device on an hourly, monthly,
    or quarterly basis. They also want reports on closure ratios and average closure
    duration. They are also interested in searching incidents based on the type of
    device or incident type. They are also interested to find time-based outage patterns
    to predict seasonal or hourly usage surges for any set of resources. These reports
    need to be generated once every 12 hours. To generate such reports, a data warehouse
    needs to be built, and data must be ingested and stored daily in that data warehouse
    so that such reports can easily be generated.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 公司XYZ是一家提供数据中心建设和维护服务的第三方供应商。现在，公司XYZ计划为其客户开发一个数据中心监控工具。客户希望看到各种有用的指标，例如任何设备在每小时、每月或每季度报告的事件数量。他们还希望看到关于关闭比率和平均关闭时间的报告。他们还希望根据设备类型或事件类型搜索事件。他们还希望找到基于时间的故障模式，以预测任何一组资源的季节性或每小时使用激增。这些报告需要每12小时生成一次。为了生成此类报告，需要构建一个数据仓库，并且必须每天将数据摄入并存储在该数据仓库中，以便可以轻松生成此类报告。
- en: To create the solution for this data engineering problem, we must analyze the
    four dimensions of data (refer to the *Dimensions of data* section in [*Chapter
    1*](B17084_01.xhtml#_idTextAnchor014), *Basics of Modern Data Architecture*) in
    this use case. Our first question would be, *What is the velocity of data?* The
    answer to this question helps us to determine whether it is a real-time or batch
    processing problem. Although there is not much information about the input frequency
    of data as per the problem statement, it is clearly stated that the report needs
    to be generated after every 12 hours or twice daily. Irrespective of the incoming
    speed of data, if the frequency in which the downstream system needs data is more
    than an hour, we can safely decide that we are dealing with a batch-based data
    engineering problem (please refer to the *Types of data engineering problems*
    section in [*Chapter 1*](B17084_01.xhtml#_idTextAnchor014), *Basics of Modern
    Data Architecture*).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为这个数据工程问题创建解决方案，我们必须分析数据在这个用例中的四个维度（参考[*第一章*](B17084_01.xhtml#_idTextAnchor014)，*现代数据架构基础*中的*数据维度*部分）。我们的第一个问题将是，*数据的速度是多少？*
    这个问题的答案帮助我们确定它是一个实时还是批量处理问题。尽管根据问题描述，关于数据输入频率的信息不多，但明确指出报告需要在每12小时或每天两次后生成。无论数据到达的速度如何，如果下游系统需要数据的时间频率超过一小时，我们可以安全地决定我们正在处理一个基于批量的数据工程问题（请参考[*第一章*](B17084_01.xhtml#_idTextAnchor014)，*现代数据架构基础*中的*数据工程问题类型*部分）。
- en: Our second question would be, *What is the volume of the data? Is it huge? Is
    there a chance that this can grow into hundreds of terabytes in the future?* These
    questions generally help us choose the technology that we should use. If the volume
    is huge (in terabytes or hundreds of terabytes), only then should we choose **big
    data** technologies to solve our problem. A lot of times, architects tend to use
    big data in a non-big data use case, which makes the solution unsustainable and
    expensive in terms of cost, maintenance, and time. In our case, the data that
    needs to be ingested is incident log data. Such data is usually not huge. However,
    an architect should get confirmation about the data that will be sent for ingestion.
    In this case, let’s suppose that the customers responded and said that the data
    will be sent every couple of hours as a flat file, consisting of a Delta of the
    incidents that have either been newly logged or updated. This would mean that
    our datasets will be either in a small file or a medium-sized file. This means
    that as an architect, we should choose a non-big data-based solution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个问题将是，*数据量是多少？它是否巨大？未来有可能增长到数百个太字节吗？* 这些问题通常帮助我们选择我们应该使用的科技。如果数据量巨大（以太字节或数百个太字节计），那么我们才应该选择**大数据**技术来解决我们的问题。很多时候，架构师倾向于在非大数据用例中使用大数据，这使得解决方案在成本、维护和时间上都不可持续且昂贵。在我们的案例中，需要摄取的数据是事件日志数据。这类数据通常并不巨大。然而，架构师应该确认将要发送进行摄取的数据。在这种情况下，让我们假设客户已经回应并表示数据将以每两小时为一个平面文件的形式发送，包含已新记录或更新的事件的Delta。这意味着我们的数据集将要么是小型文件，要么是中等大小的文件。这意味着作为一个架构师，我们应该选择一个非大数据基础的解决方案。
- en: Understanding the source data
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解源数据
- en: The third important question that an architect must ask is, *What is the variety
    of the data? Is it structured, unstructured, or semi-structured?* This question
    often helps to determine how such data can be processed and stored. If the data
    is unstructured, then we need to store it in a NoSQL database, but structured
    data can be stored in RDBMS databases. There is another question related to **data
    variety** – that is, *What is the format of the data? Is it in CSV format, JSON
    format, Avro format, or Parquet format? Is the data compressed when received?*
    Often, these questions help determine the techniques, technologies, processing
    rules, and pipeline design required to process and ingest the data. In our case,
    since it is not mentioned in the initial requirement, we need to ask the customers
    these questions. Let’s suppose our customers agree to send the data in the form
    of CSV files. So, in this case, we are dealing with structured data and the data
    is coming as a CSV file without any compression. As it is structured data, it
    is apt for us to use a relational data model or RDBMS database to store our data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 架构师必须提出的第三个重要问题是，*数据的多样性是什么？它是结构化、非结构化还是半结构化？* 这个问题通常有助于确定如何处理和存储此类数据。如果数据是非结构化的，那么我们需要将其存储在NoSQL数据库中，但结构化数据可以存储在RDBMS数据库中。还有一个与**数据多样性**相关的问题——即，*数据的格式是什么？是CSV格式、JSON格式、Avro格式还是Parquet格式？接收到的数据是否被压缩？*
    通常，这些问题有助于确定处理和摄取数据所需的技巧、技术、处理规则和管道设计。在我们的案例中，由于初始要求中没有提到，我们需要向客户提出这些问题。假设我们的客户同意以CSV文件的形式发送数据。因此，在这种情况下，我们正在处理结构化数据，数据以CSV文件的形式到来，没有任何压缩。由于它是结构化数据，我们非常适合使用关系数据模型或RDBMS数据库来存储我们的数据。
- en: 'This brings us to the final question regarding the dimension of the data: *What
    is the veracity of the data?* Or, in simpler terms, *What is the quality of the
    data that we receive? Is there too much noise in the data?* Of all the data engineering
    solutions that fail to solve a customer problem, the majority fail because of
    a lack of time spent analyzing and profiling the source data. Understanding the
    nature of the data that is coming is very important. We must ask, and be able
    to answer, the following kinds of questions at the end of the analysis:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了关于数据维度的最后一个问题：*数据的真实性如何？* 或者，用更简单的话说，*我们接收到的数据质量如何？数据中是否有太多的噪声？* 在所有未能解决客户问题的数据工程解决方案中，大多数失败是因为在分析源数据时投入的时间不足。理解即将到来的数据的性质非常重要。我们必须在分析结束时提出并能够回答以下类型的问题：
- en: Does the source data contain any junk characters that need to be removed?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据是否包含需要删除的垃圾字符？
- en: Does it contain any special characters?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否包含任何特殊字符？
- en: Does the source data contain non-English characters (such as French or German)?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据是否包含非英语字符（如法语或德语）？
- en: Do any numeric columns contain null values? Which can or cannot be nullable
    columns?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有任何数值列包含空值？哪些列可以是或不可以是可空列？
- en: Is there something unique with which we can determine each record?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有什么独特之处可以用来确定每条记录？
- en: And the list goes on.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表还在继续。
- en: To analyze the source data, we should run a data profiling tool such as Talend
    Open Studio, DataCleaner, or AWS Glue DataBrew to analyze and visualize various
    metrics of the data. This activity helps us understand the data better.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析源数据，我们应该运行数据概要分析工具，例如Talend Open Studio、DataCleaner或AWS Glue DataBrew，以分析和可视化数据的各种指标。这项活动有助于我们更好地理解数据。
- en: 'Here, we will analyze the CSV data file that we need to ingest for our use
    case using the DataCleaner tool. Follow these steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用DataCleaner工具分析我们需要用于用例的CSV数据文件。请按照以下步骤操作：
- en: First, you can download DataCleaner Community Edition by going to [https://datacleaner.github.io/downloads](https://datacleaner.github.io/downloads).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您可以通过访问[https://datacleaner.github.io/downloads](https://datacleaner.github.io/downloads)下载DataCleaner社区版。
- en: 'Then, unzip the downloaded ZIP file in the desired installation folder. Based
    on your operating system, you can start DataCleaner using either the `datacleaner.sh`
    command or the `datacleaner.cmd` file present under the root installation folder.
    You will see a home screen, as shown in the following screenshot. Here, you can
    start a new data profiling job by clicking the **Build new job** button:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在所需的安装文件夹中解压缩下载的ZIP文件。根据您的操作系统，您可以使用根安装文件夹下的`datacleaner.sh`命令或`datacleaner.cmd`文件启动DataCleaner。您将看到一个主屏幕，如下面的截图所示。在这里，您可以点击**构建新作业**按钮开始一个新的数据概要分析作业：
- en: '![Figure 4.1 – DataCleaner welcome screen ](img/B17084_04_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – DataCleaner欢迎屏幕](img/B17084_04_001.jpg)'
- en: Figure 4.1 – DataCleaner welcome screen
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – DataCleaner欢迎屏幕
- en: 'Then, a dialog will pop up, where you can select the data store, as shown in
    the following screenshot. Here, we will browse for and select our input file called
    `inputData.csv`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将弹出一个对话框，您可以在其中选择数据存储库，如图下所示。在这里，我们将浏览并选择名为`inputData.csv`的输入文件：
- en: '![Figure 4.2 – DataCleaner – the Select datastore popup ](img/B17084_04_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – DataCleaner – 选择数据存储库弹出窗口](img/B17084_04_002.jpg)'
- en: Figure 4.2 – DataCleaner – the Select datastore popup
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – DataCleaner – 选择数据存储库弹出窗口
- en: Once the datastore is selected, we will see our data source at the top of the
    left pane. We should be able to see the column names of our CSV as well.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了数据存储库，我们将在左侧面板的顶部看到我们的数据源。我们应该能够看到我们CSV文件的列名。
- en: 'Now, we will drag and drop our data source `inputData.csv` file to the right
    pane, which is the pipeline building canvas. To profile the data, DataCleaner
    provides various analyzer tools under the **Analyze** menu, which is visible in
    the left pane. For our use case, we will be using **String analyzer**:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将把我们的数据源`inputData.csv`文件拖放到右侧面板，即管道构建画布。为了分析数据，DataCleaner在左侧面板的**分析**菜单下提供了各种分析工具，如图所示。对于我们的用例，我们将使用**字符串分析器**：
- en: '![Figure 4.3 – Creating an analysis pipeline ](img/B17084_04_003.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 创建分析管道](img/B17084_04_003.jpg)'
- en: Figure 4.3 – Creating an analysis pipeline
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 创建分析管道
- en: '**String analyzer** analyzes various string-related metrics such as the NULL
    count, blank count, white spaces, character case, and so on. The following screenshot
    shows the various configuration options of a **String analyzer**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**字符串分析器**分析各种与字符串相关的指标，如NULL计数、空白计数、空白字符、字符大小写等。以下截图显示了**字符串分析器**的各种配置选项：'
- en: '![Figure 4.4 – Adding String analyzer ](img/B17084_04_004.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 添加字符串分析器](img/B17084_04_004.jpg)'
- en: Figure 4.4 – Adding String analyzer
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 添加字符串分析器
- en: We will add another analyzer called `incidentNumber`, `deviceSerialNum`, `eventCode`,
    and `loggedTime` to be an eligible entry for our data warehouse.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加另一个名为`incidentNumber`、`deviceSerialNum`、`eventCode`和`loggedTime`的分析器，使其成为我们数据仓库的合格条目。
- en: 'If any of this information is missing, such a record will not add value to
    the problem that we are trying to solve. Here, **Completeness analyzer** will
    help us determine whether we need special checks to handle these constraints and
    drop records if these fields are blank. The following screenshot shows the various
    configuration options of **Completeness analyzer**:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果缺少任何此类信息，此类记录将不会为我们要解决的问题增加价值。在这里，**完整性分析器**将帮助我们确定我们是否需要特殊检查来处理这些约束，并在这些字段为空时删除记录。以下截图显示了**完整性分析器**的各种配置选项：
- en: '![Figure 4.5 – Adding the Completeness analyzer ](img/B17084_04_005.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 添加完整性分析器](img/B17084_04_005.jpg)'
- en: Figure 4.5 – Adding the Completeness analyzer
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 添加完整性分析器
- en: 'The final profiling pipeline for our use case can be seen in the following
    screenshot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用例的最终概要分析管道如图下所示：
- en: '![Figure 4.6 – Final analyzer pipeline ](img/B17084_04_006.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 最终分析管道](img/B17084_04_006.jpg)'
- en: Figure 4.6 – Final analyzer pipeline
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 最终分析管道
- en: 'Once we execute this pipeline, **Analysis results** will be generated, as shown
    in the following screenshot:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们执行此管道，**分析结果**将生成，如图下所示：
- en: '![Figure 4.7 – Analysis results ](img/B17084_04_007.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – 分析结果](img/B17084_04_007.jpg)'
- en: Figure 4.7 – Analysis results
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 分析结果
- en: Such data profiling can provide us with various pieces of information about
    the data, which can help us adjust our tools, technologies, and transformations
    to create an effective and successful data engineering solution. As shown in the
    preceding screenshot, we can infer that the total data size is 300 rows. Out of
    these, 53 are open incidents. The resolution comments can have spaces in them,
    all `deviceSerialNum` values are lowercase, and `status` values are uppercase.
    Such information helps us make effective decisions while designing the solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据概要分析可以为我们提供有关数据的各种信息，这有助于我们调整我们的工具、技术和转换，以创建一个有效且成功的数据工程解决方案。如图所示，我们可以推断出总数据量是300行。其中，53行是开放事件。解决评论中可以有空白，所有`deviceSerialNum`值都是小写，而`status`值都是大写。此类信息有助于我们在设计解决方案时做出有效的决策。
- en: For the brevity of this discussion, we are only showing one form of data profiling
    for a source data file. However, we can do the same for other kinds of datasets.
    In this use case, you can do similar data profiling for the data in the `device_dm.csv`
    and `event_dm.csv` files.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论的简洁性，我们只展示了一种源数据文件的数据概要分析形式。然而，我们也可以对其他类型的数据集进行同样的分析。在这个用例中，你可以对`device_dm.csv`和`event_dm.csv`文件中的数据进行类似的数据概要分析。
- en: Now that we have understood the requirements and have a fair idea of the source
    data, in the next section, we will discuss how to design the model so that it
    can store the ingested data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了需求，并对源数据有了相当的了解，在下一节中，我们将讨论如何设计模型，以便它可以存储导入的数据。
- en: Building an effective data model
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建有效的数据模型
- en: From our previous discussion and after analyzing the data, we have concluded
    that our data is structured, so it’s suitable for being stored in a relational
    data model. From the requirements, we have gathered that our final data store
    should be a data warehouse. Keeping these two basic factors in mind, let’s learn
    about relational data warehouse schemas.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前的讨论和数据分析中，我们得出结论，我们的数据是有结构的，因此适合存储在关系型数据模型中。根据需求，我们收集到我们的最终数据存储应该是一个数据仓库。考虑到这两个基本因素，让我们了解关系型数据仓库方案。
- en: Relational data warehouse schemas
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关系型数据仓库方案
- en: 'Let’s explore the popular relational data warehouse schemas that we can consider
    when creating our data model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索在创建我们的数据模型时可以考虑的流行关系型数据仓库方案：
- en: '**Star schema**: This is the most popular data warehouse schema type. As shown
    in the following diagram, there is a **Fact Table** in the middle where each record
    represents a fact or an event that has happened over time:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**星型模式**：这是最流行的数据仓库方案类型。如图所示，中间有一个**事实表**，每个记录代表一个随时间发生的事实或事件：'
- en: '![Figure 4.8 – Star schema ](img/B17084_04_008.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – 星型模式](img/B17084_04_008.jpg)'
- en: Figure 4.8 – Star schema
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 星型模式
- en: This **Fact Table** consists of various dimensions whose details need to be
    looked up from associated lookup tables called **dimension tables**. This **Fact
    Table** is associated with each dimension table using a foreign key. The preceding
    diagram shows what a star schema looks like. Since there is a **Fact Table** in
    the middle surrounded by multiple dimension tables on various sides, its structure
    looks like a star, hence its name.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个**事实表**包含各种维度，其详细信息需要从相关的查找表中查找，这些查找表被称为**维度表**。这个**事实表**通过外键与每个维度表相关联。前面的图示显示了星型模式的外观。由于中间有一个**事实表**被多个侧面的维度表包围，其结构看起来像一颗星，因此得名。
- en: '**Snowflake schema**: This is an extension of the star schema. Just like the
    star schema, here, there is a **Fact Table** in the middle and multiple dimension
    tables around it. However, in the snowflake schema, each dimension table further
    references other child dimension tables, making the structure look like a snowflake:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**雪花模式**：这是星型模式的一个扩展。就像星型模式一样，在这里，中间有一个**事实表**，周围有多个维度表。然而，在雪花模式中，每个维度表进一步引用其他子维度表，使得结构看起来像雪花：'
- en: '![Figure 4.9 – Snowflake schema ](img/B17084_04_009.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 雪花模式](img/B17084_04_009.jpg)'
- en: Figure 4.9 – Snowflake schema
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 雪花模式
- en: Here, we can see how each dimension table is connected to the child dimension
    tables using foreign key relations, making the structure look like a snowflake,
    hence its name.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到每个维度表是如何通过外键关系与子维度表连接的，这使得结构看起来像雪花，因此得名。
- en: '**Galaxy schema**: A galaxy schema is a schema that consists of more than one
    fact table. Here, one or more dimension tables are shared by multiple fact tables.
    This schema can be visualized as a collection of two or more star schemas, hence
    its name.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**银河模式**：银河模式是一种包含多个事实表的方案。在这里，一个或多个维度表被多个事实表共享。这种方案可以看作是两个或更多星型模式的集合，因此得名。'
- en: Evaluation of the schema design
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方案设计的评估
- en: For our use case, we need to evaluate what schema design best fits our use case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们需要评估哪种方案设计最适合我们的用例。
- en: The first question that we should ask is, *In our use case, do we need multiple
    fact tables?* Since our fact table only consists of device events or incidents,
    we can only have one fact table. This eliminates any chance of having a galaxy
    schema as our candidate data model. Now, we must determine whether a star schema
    or a snowflake schema is suitable for our use case.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该问的第一个问题是，*在我们的用例中，我们需要多个事实表吗？* 由于我们的事实表只包含设备事件或事故，我们只能有一个事实表。这消除了我们拥有银河模式作为候选数据模型的可能性。现在，我们必须确定星型模式或雪花模式是否适合我们的用例。
- en: 'To choose between those two alternatives, let’s look at the following columns
    of our `inputData.csv` file:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要在这两种选择之间做出选择，让我们看看我们的 `inputData.csv` 文件中的以下列：
- en: '`incidentNumber`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`incidentNumber`'
- en: '`deviceSerialNo`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deviceSerialNo`'
- en: '`eventCode`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eventCode`'
- en: '`loggedTime`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loggedTime`'
- en: '`closureTime`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`closureTime`'
- en: '`status`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`status`'
- en: '`assignedTo`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assignedTo`'
- en: '`resolutionComments`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resolutionComments`'
- en: By looking at the column names for this file, we can say that this is the device
    event log file. This implies that the data from the `inputData.csv` file needs
    to be ingested into our central fact table. But first, we need to determine whether
    we need to reference only dimension tables, which are complete in themselves,
    or whether our dimensions table needs to do further lookups in another set of
    dimension tables.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看此文件的列名，我们可以断定这是一个设备事件日志文件。这意味着来自 `inputData.csv` 文件的数据需要被摄入到我们的中心事实表中。但是首先，我们需要确定我们是否只需要引用完整的维度表，或者我们的维度表是否需要在另一组维度表中进行进一步的查找。
- en: Let’s begin by figuring out the candidate dimensions from the dataset present
    in `inputData.csv`. It is important to remember that candidate dimensions are
    determined by the purpose or goal of building the data warehouse. The purpose
    of the data warehouse we are building is to obtain metrics on `eventType`, device
    over different time intervals such as hourly, monthly, and quarterly, and closure
    duration metrics.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从确定 `inputData.csv` 中存在的数据集的候选维度开始。重要的是要记住，候选维度是由构建数据仓库的目的或目标决定的。我们正在构建的数据仓库的目的是获取
    `eventType`、设备在不同时间间隔（如小时、月和季度）上的指标以及关闭持续时间指标。
- en: In our case, `deviceSerialNo` and `eventCode` can correspond to two dimensions
    called `incidentNumber` will vary in each fact record, so it’s not a candidate
    for dimension. `status`, `loggedTime`, and `closureTime` will vary from record
    to record, so they are best suited for being facts and not dimensions. Since we
    are not doing any analysis on the `assignedTo` and `resolutionComment` fields,
    we can ignore those columns in our data model. In a real-world scenario, usually,
    incoming source data files contain hundreds of columns. However, only a small
    percentage of those columns are useful for solving a problem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，`deviceSerialNo` 和 `eventCode` 可以对应于两个称为 `incidentNumber` 的维度，这个维度在每条事实记录中都会变化，因此它不是一个维度的候选者。`status`、`loggedTime`
    和 `closureTime` 会从一条记录变化到另一条记录，因此它们最适合作为事实而不是维度。由于我们不对 `assignedTo` 和 `resolutionComment`
    字段进行任何分析，我们可以在我们的数据模型中忽略这些列。在现实世界的场景中，通常，传入的源数据文件包含数百列。然而，其中只有一小部分列对于解决问题是有用的。
- en: It is always advised to ingest only the columns that you need. This saves space,
    complexity, and money (remember that a lot of solutions these days are deployed
    on cloud platforms or are candidates for future migration to cloud platforms,
    and cloud platforms follow the principle of pay for what you use, so you should
    only ingest data that you intend to use). Apart from these, our requirements need
    us to mark every event on an hourly, monthly, and quarterly basis so that aggregations
    can easily be run on these intervals and hourly, monthly, and quarterly patterns
    can be analyzed. This interval tagging can be derived from `loggedTime` while
    saving the record. However, `hour`, `month`, and `quarter` can be stored as derived
    dimensions associated with our central fact table.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总是建议只摄入你需要的列。这可以节省空间、复杂性和金钱（记住，如今许多解决方案都是部署在云平台上，或者未来有迁移到云平台的可能，云平台遵循按使用付费的原则，因此你应该只摄入你打算使用的数据）。除此之外，我们的需求要求我们按小时、月和季度对每个事件进行标记，以便可以轻松地对这些间隔进行聚合，并分析小时、月和季度的模式。这种间隔标记可以在保存记录时从
    `loggedTime` 中提取，然而，`hour`、`month` 和 `quarter` 可以作为与我们的中心事实表关联的派生维度存储。
- en: 'Hence, from our analysis, it is clear that our fact table only references those
    dimension tables that are complete in themselves. So, we can conclude that we
    will be using a star schema for our data modeling with the following set of tables:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从我们的分析来看，很明显我们的事实表只引用那些自身完整的事实表。所以，我们可以得出结论，我们将使用以下一系列表来构建我们的星型模式数据模型：
- en: '`DEVICE_EVENT_LOG_FACT`: This is the centralized fact table, which consists
    of each incident entry'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEVICE_EVENT_LOG_FACT`：这是一个集中式的事实表，它由每个事件条目组成'
- en: '`DEVICE_DIMENSION`: This is the dimension table, which consists of device lookup
    data'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEVICE_DIMENSION`：这是一个维度表，它包含设备查找数据'
- en: '`EVENT_DIMENSION`: This is the dimension table, which consists of event lookup
    data'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EVENT_DIMENSION`：这是一个维度表，它包含事件查找数据'
- en: '`HOUR_DIMENSION`: This is the dimension table, which consists of static hour
    lookup data'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HOUR_DIMENSION`：这是一个维度表，它包含静态小时查找数据'
- en: '`MONTH_DIMENSION`: This is the dimension table, which consists of static month
    lookup data'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MONTH_DIMENSION`：这是一个维度表，它包含静态月份查找数据'
- en: '`QAURTER_DIMENSION`: This is the dimension table, which consists of static
    quarter lookup data'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QUARTER_DIMENSION`：这是一个维度表，它包含静态季度查找数据'
- en: 'The following diagram depicts the detailed star schema data model of the data
    warehouse that we are building:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了我们正在构建的数据仓库的详细星型模式数据模型：
- en: '![Figure 4.10 – Data model for our data warehouse ](img/B17084_04_010.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图4.10 – 我们数据仓库的数据模型](img/B17084_04_010.jpg)'
- en: Figure 4.10 – Data model for our data warehouse
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 我们数据仓库的数据模型
- en: 'Now, let’s understand the tables and their columns shown in the preceding diagram:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解前面图表中显示的表及其列：
- en: 'In the `DEVICE_EVENT_LOG_FACT` table, the following is happening:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`DEVICE_EVENT_LOG_FACT`表中，以下情况正在发生：
- en: We use `eventLogId` as the primary key, which maps to `incidentNumber` from
    our file
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`eventLogId`作为主键，它映射到文件中的`incidentNumber`。
- en: We have foreign key fields for the `DEVICE_DIMENSION`, `EVENT_DIMENSION`, `HOUR_DIMENSION`,
    `MONTH_DIMENSION`, and `QUARTER_DIMENSION` tables
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为`DEVICE_DIMENSION`、`EVENT_DIMENSION`、`HOUR_DIMENSION`、`MONTH_DIMENSION`和`QUARTER_DIMENSION`表设置了外键字段
- en: '`eventTimestamp`, `closurestatus`, and `closureDuration` are all facts for
    each row in the fact table'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`eventTimestamp`、`closurestatus`和`closureDuration`都是事实表中每一行的事实'
- en: The columns of `DEVICE_DIMENSION` and `EVENT_DIMENSION` are determined by the
    need as well as the data/attributes available for the device and events in the
    input files – that is, `device_dm.csv` and `event_dm.csv`. However, the primary
    keys of these two tables (`deviceId` and `eventId`) should be system-generated
    sequence numbers that are assigned to a record. The primary key in these two tables
    is the reference column for the foreign key relationship with the fact table.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEVICE_DIMENSION`和`EVENT_DIMENSION`的列由需求以及输入文件（即`device_dm.csv`和`event_dm.csv`）中设备和事件的数据/属性决定。也就是说，这两个表的主键（`deviceId`和`eventId`）应该是系统生成的序列号，分配给记录。这两个表的主键是事实表外键关系中的参考列。'
- en: Apart from the device and event, we have designed three other dimension tables
    denoting hours of the day (`HOUR_DIMENSION`), month (`MONTH_DIMENSION`), and quarter
    (`QUARTER_DIMENSION`) of the year. These are static lookup tables, and their data
    will always remain constant over time.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了设备和事件之外，我们还设计了三个其他维度表，表示一天中的小时（`HOUR_DIMENSION`）、月份（`MONTH_DIMENSION`）和季度（`QUARTER_DIMENSION`）。这些是静态查找表，其数据将随着时间的推移始终保持不变。
- en: The next design decision that needs to be made in terms of the data model is
    the decision to choose a database. Various **Relational Database Management Systems**
    (**RDBMSs**) are well suited for a data warehouse, such as Snowflake, AWS Redshift,
    PostgreSQL, and Oracle. While the first two options are cloud-based data warehouses,
    the other two options can be run both on-premises and in the cloud. For our use
    case, we should choose a database that is cost-effective as well as future-compatible.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据模型方面，接下来需要做出的设计决策是选择数据库。各种**关系数据库管理系统（RDBMS**）非常适合数据仓库，例如Snowflake、AWS Redshift、PostgreSQL和Oracle。虽然前两种选项是基于云的数据仓库，但其他两种选项可以在本地和云中运行。对于我们的用例，我们应该选择既经济高效又兼容未来的数据库。
- en: Out of these choices, we will choose PostgreSQL since it is a free database
    that is powerful and feature-rich to host a data warehouse. Also, our application
    may be migrated to the cloud in the future. In that case, it can easily be migrated
    to AWS Redshift, as AWS Redshift is based on industry-standard PostgreSQL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些选择中，我们将选择PostgreSQL，因为它是一个功能强大且丰富的免费数据库，适合托管数据仓库。此外，我们的应用程序未来可能迁移到云端。在这种情况下，它可以轻松迁移到AWS
    Redshift，因为AWS Redshift基于行业标准的PostgreSQL。
- en: Now that we have designed our data model and chosen our database, let’s go ahead
    and architect the solution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设计好了数据模型并选择了数据库，让我们继续设计解决方案。
- en: Designing the solution
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: 'To design the solution for the current problem statement, let’s analyze the
    data points or facts that are available to us right now:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计当前问题声明的解决方案，让我们分析我们现在可用的数据点或事实：
- en: The current problem is a batch-based data engineering problem
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前问题是基于批处理的数据工程问题
- en: The problem at hand is a data ingestion problem
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前的问题是数据摄取问题
- en: Our source is CSV files containing structured data
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的源是包含结构化数据的CSV文件
- en: Our target is a PostgreSQL data warehouse
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的目标是PostgreSQL数据仓库
- en: Our data warehouse follows a star schema, with one fact table, two dynamic dimension
    tables, and three static dimension tables
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数据仓库遵循星型模式，包含一个事实表，两个动态维度表和三个静态维度表
- en: We should choose a technology that is independent of the deployment platform,
    considering that our solution can be migrated to the cloud in the future
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到我们的解决方案未来可能迁移到云端，我们应该选择一个与部署平台无关的技术
- en: For the context and scope of this book, we will explore optimum solutions based
    on Java-based technologies
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于本书的背景和范围，我们将基于Java技术探索最佳解决方案
- en: Based on the preceding facts, we can conclude that we have to build three similar
    data ingestion pipelines – one for the fact table and two others for the dynamic
    dimension tables. At this point, we must ask ourselves, *What happens to the file
    if the file ingestion is successful or if it fails? How do we avoid reading the
    file again?*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述事实，我们可以得出结论，我们必须构建三个类似的数据摄取管道 – 一个用于事实表，另外两个用于动态维度表。在这个时候，我们必须问自己，*如果文件摄取成功或失败，文件会发生什么？我们如何避免再次读取文件？*
- en: 'We will read the file from the `input` folder and ingest it into the data warehouse.
    If it fails, we will move the file to an `error` folder; otherwise, we will move
    it to an `archive` folder. The following diagram shows our findings and provides
    an overview of our proposed solution:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`input`文件夹中读取文件并将其摄取到数据仓库中。如果失败，我们将文件移动到`error`文件夹；否则，我们将文件移动到`archive`文件夹。以下图表展示了我们的发现，并提供了我们提出的解决方案的概述：
- en: '![Figure – 4.11 – Solution overview ](img/B17084_04_011.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 – 4.11 – 解决方案概述](img/B17084_04_011.jpg)'
- en: Figure – 4.11 – Solution overview
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 – 4.11 – 解决方案概述
- en: However, this proposed solution is a 10,000-feet view of it. There are still
    many questions that have been left unanswered in this solution. For instance,
    there are no details about the ingestion processes or the technology that we should
    use to solve this problem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个提出的解决方案只是一个高层次的概述。在这个解决方案中，仍有许多问题尚未得到解答。例如，关于摄取过程或我们应该使用什么技术来解决这个问题，没有详细的说明。
- en: First, let’s try to decide on a technology based on the facts we have at hand.
    We need to find a Java-based ETL technology that supports batch ingestion. Also,
    it should have easy-to-use JDBC support to write and read data from PostgreSQL.
    We also need to have a scheduler to schedule the batch ingestion job and should
    have a retry ability mechanism. Also, our data is not huge, so we want to avoid
    big data-based ETL tools.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们根据我们手头的事实尝试决定一个技术。我们需要找到一个支持批处理摄取的基于Java的ETL技术。它还应该具有易于使用的JDBC支持，以便从PostgreSQL写入和读取数据。我们还需要一个调度器来安排批处理摄取作业，并应该有一个重试机制。此外，我们的数据量不大，所以我们想避免基于大数据的ETL工具。
- en: 'Spring Batch fits all these requirements. Spring Batch is an excellent Java-based
    ETL tool for building batch jobs. It comes with a job scheduler and a job repository.
    Also, since it is a part of the Spring Framework, it can easily be integrated
    with various tools and technologies with Spring Boot and Spring integration. The
    following diagram shows the high-level components of the Spring Batch architecture:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Batch符合所有这些要求。Spring Batch是一个基于Java的优秀的ETL工具，用于构建批处理作业。它包含一个作业调度器和作业存储库。此外，由于它是Spring框架的一部分，它可以轻松地与Spring
    Boot和Spring集成等工具和技术集成。以下图表显示了Spring Batch架构的高级组件：
- en: '![Figure 4.12 – Spring Batch architecture ](img/B17084_04_012.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12 – Spring Batch架构](img/B17084_04_012.jpg)'
- en: Figure 4.12 – Spring Batch architecture
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – Spring Batch架构
- en: 'The preceding diagram denotes how a Spring Batch job works. Let’s look at the
    various steps that a Spring Batch job goes through to be executed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表表示Spring Batch作业的工作方式。让我们看看Spring Batch作业在执行过程中经历的各个步骤：
- en: The Spring Batch job uses Spring’s job scheduler to schedule a job.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spring Batch作业使用Spring的作业调度器来安排作业。
- en: '**Spring Job Scheduler** runs **Job Launcher**, which, in turn, executes a
    **Spring Batch job**. It also creates a job instance at this point and persists
    this information in the job repository database.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Spring作业调度器**运行**作业启动器**，它反过来执行**Spring Batch作业**。它还在此处创建一个作业实例，并将此信息持久化到作业仓库数据库中。'
- en: The `batch_job_instance`
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_job_instance`'
- en: '`batch_job_execution`'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_job_execution`'
- en: '`batch_job_execution_params`'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_job_execution_params`'
- en: '`batch_step_execution`'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_step_execution`'
- en: '`batch_job_execution_context`'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_job_execution_context`'
- en: '`batch_step_execution_context`'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`batch_step_execution_context`'
- en: The **Spring Batch job**, which is executed by the **Job Launcher**, initiates
    individual steps to perform the job. Each step performs a specific task to achieve
    the overall aim of the job.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由**作业启动器**执行的**Spring Batch作业**启动单个步骤以执行作业。每个步骤执行特定任务以实现作业的整体目标。
- en: While there is a **Job Execution Context** present across all the steps of a
    job instance, there is a **Step Execution Context** present in each execution
    step.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然在作业实例的所有步骤中都有一个**作业执行上下文**，但在每个执行步骤中都有一个**步骤执行上下文**。
- en: Usually, a Spring Batch configuration helps stitch together each step in the
    desired sequence to create the Spring Batch pipeline.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，Spring Batch配置有助于将每个步骤按所需顺序连接起来，以创建Spring Batch管道。
- en: Each step, in turn, reads the data using `Reader` or `ItemReader`, processes
    the data using `Processor` or `ItemProcessor`, and writes the processed data using
    `Writer` or `ItemWriter`.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一步，依次使用`Reader`或`ItemReader`读取数据，使用`Processor`或`ItemProcessor`处理数据，并使用`Writer`或`ItemWriter`写入处理后的数据。
- en: 'Now that we have a fair understanding of the Spring Batch architecture, we
    will architect our ingestion pipeline using the Spring Batch job framework. The
    following diagram shows the architecture of our data ingestion pipeline:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Spring Batch架构有了相当的了解，我们将使用Spring Batch作业框架来设计我们的数据摄取管道。以下图表显示了我们的数据摄取管道的架构：
- en: '![Figure 4.13 – Solution architecture ](img/B17084_04_013.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图4.13 – 解决方案架构](img/B17084_04_013.jpg)'
- en: Figure 4.13 – Solution architecture
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – 解决方案架构
- en: 'Let’s look at this solution in more detail:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这个解决方案：
- en: Like all **Spring Batch jobs**, the **Spring Job Scheduler** schedules a **Job
    Launcher**, which instantiates the Spring job.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像所有**Spring Batch作业**一样，**Spring作业调度器**安排一个**作业启动器**，该启动器实例化Spring作业。
- en: In our use case, we will use a total of three sequential and two conditional
    steps to complete the job.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的用例中，我们将使用总共三个顺序步骤和两个条件步骤来完成作业。
- en: In the first steps, the application looks at whether there is a new file in
    the input folder or `JobExecutionContext` and marks **ExitStatus** as **COMPLETED**.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步中，应用程序检查输入文件夹中是否有新文件或`JobExecutionContext`，并将**退出状态**标记为**完成**。
- en: If `JobExecutionContext`) from **Landing Zone** to **Processing Zone**.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`JobExecutionContext`从**着陆区**到**处理区**。
- en: Upon completing the second step, the third step (**Spring Process Step**) is
    initiated. The third step transforms and loads the data into a data warehouse.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成第二步后，第三步（**Spring处理步骤**）被启动。第三步将数据转换并加载到数据仓库中。
- en: Upon completing the third step, **Spring Archival Step** is initiated, which
    moves the processed file from the process folder to the archive folder.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成第三步后，启动**Spring存档步骤**，该步骤将处理后的文件从处理文件夹移动到存档文件夹。
- en: However, if **String Process Step** fails, **Spring Error Handler Step** will
    be initiated, where it moves the file from the **Processing Zone** folder to the
    **Error Zone** folder.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，如果**字符串处理步骤**失败，将启动**Spring错误处理步骤**，其中将文件从**处理区域**文件夹移动到**错误区域**文件夹。
- en: In this section, we learned how to logically divide the solution using the available
    facts and data points and come up with an optimal architecture for the problem.
    We also learned how the effectiveness of each solution is dependent on the technology
    stack that we choose.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用可用的事实和数据点逻辑上划分解决方案，并为问题提出一个最佳架构。我们还学习了每个解决方案的有效性取决于我们选择的技术堆栈。
- en: In the next section, we will learn how to implement and test our solution using
    Spring Batch and related technologies.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用Spring Batch和相关技术实现和测试我们的解决方案。
- en: Implementing and unit testing the solution
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现和单元测试解决方案
- en: In this section, we will build the Spring Batch application to implement the
    solution that we designed in the preceding section. We will also run and test
    the solution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建Spring Batch应用程序来实现我们在上一节中设计的解决方案。我们还将运行和测试该解决方案。
- en: First, we must understand that different jobs will have their own schedules.
    However, the dimension tables need to be loaded before the fact table, because
    the dimension tables are the lookup tables.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须理解不同的作业将有自己的计划。然而，维度表需要在事实表之前加载，因为维度表是查找表。
- en: For the brevity of our discussion, we will only implement the Spring Batch application
    for the fact table. In this implementation, we will load the device data and event
    data from CSV to the table manually. However, you can follow the lead of the discussion
    by implementing the solution and developing two different Spring Batch applications
    for the device and event dimension tables. In this implementation, we will assume
    that the device and event data have already been loaded into the data warehouse.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论的简洁性，我们将只实现事实表的Spring Batch应用程序。在这个实现中，我们将手动将设备数据和事件数据从CSV文件加载到表中。然而，您可以通过实现解决方案并开发两个不同的Spring
    Batch应用程序来跟踪设备和事件维度表来跟随讨论的引导。在这个实现中，我们假设设备数据和事件数据已经加载到数据仓库中。
- en: 'You can do that manually by executing the DMLs present at the following GitHub
    link: [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SQL/chapter4_ddl_dml.sql](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SQL/chapter4_ddl_dml.sql).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行以下GitHub链接中提供的DML来手动完成此操作：[https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SQL/chapter4_ddl_dml.sql](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SQL/chapter4_ddl_dml.sql).
- en: 'We need to begin by creating a Spring Boot Maven project and adding the required
    Maven dependencies. The following Maven dependencies should be added to the `pom.xml`
    file, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要首先创建一个Spring Boot Maven项目并添加所需的Maven依赖项。以下Maven依赖项应添加到`pom.xml`文件中，如下所示：
- en: '[PRE0]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Two Spring dependencies are added here: `spring-boot-starter-batch` is added
    for Spring Batch and `spring-boot-starter-jdbc` is added for communicating with
    the `postgreSQL` database (which is used as a data warehouse and the Spring Batch
    repository database). Apart from this, the JDBC driver for PostgreSQL and the
    logging dependencies are added.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里添加了两个Spring依赖项：`spring-boot-starter-batch`用于Spring Batch，`spring-boot-starter-jdbc`用于与`postgreSQL`数据库（用作数据仓库和Spring
    Batch存储库数据库）通信。除此之外，还添加了PostgreSQL的JDBC驱动程序和日志依赖项。
- en: 'As per our architecture, let’s start by creating the entry point of the Spring
    Boot application, which is the `Main` class, and initializing the job scheduler.
    The following code denotes our `Main` class:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的架构，让我们首先创建Spring Boot应用程序的入口点，即`Main`类，并初始化作业调度器。以下代码表示我们的`Main`类：
- en: '[PRE1]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `@SpringBootApplication` annotation denotes that this class is the entry
    point of the Spring Boot application. Also, please note that the `@EnableScheduling`
    annotation denotes that this application supports Spring job scheduling. A method
    with the `@Scheduled` annotation helps perform the scheduled function at the configured
    schedule interval.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`@SpringBootApplication`注解表示这个类是Spring Boot应用程序的入口点。此外，请注意，`@EnableScheduling`注解表示这个应用程序支持Spring作业调度。带有`@Scheduled`注解的方法有助于在配置的计划时间间隔内执行计划功能。'
- en: 'The Spring Batch job scheduler supports all of the three formats as shown in
    the following block of code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Batch作业调度器支持以下代码块中显示的所有三种格式：
- en: '[PRE2]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `fixedDelayString` makes sure that there is a delay of *n* milliseconds
    between the end of a job and the beginning of another job. `fixedRateString` runs
    the scheduled job every *n* milliseconds, while `cron` schedules the job using
    some cron expression. In our case, we are using a `cron` expression to schedule
    the `perform()` method.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`fixedDelayString`确保在作业结束和另一个作业开始之间有*n*毫秒的延迟。`fixedRateString`每*n*毫秒运行计划中的作业，而`cron`使用某些cron表达式来安排作业。在我们的情况下，我们使用cron表达式来安排`perform()`方法。
- en: The `perform()` method adds a job parameter called `JobID` and triggers a Spring
    Batch job called `etlJob` using `jobLauncher`. `jobLauncher` is an auto-wired
    bean of the `JobLauncher` type.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`perform()` 方法添加了一个名为 `JobID` 的作业参数，并使用 `jobLauncher` 触发名为 `etlJob` 的 Spring
    Batch 作业。`jobLauncher` 是 `JobLauncher` 类型的自动装配实例。'
- en: The `etlJob` field in the `EtlDatawarehouseApplication` class, as shown earlier,
    is also auto-wired and hence is a Spring bean.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`EtlDatawarehouseApplication` 类中的 `etlJob` 字段也是自动装配的，因此它是一个 Spring 实例。
- en: 'Next, we will explore the Spring Batch configuration file where the `etlJob`
    bean is created:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨创建 `etlJob` 实例的 Spring Batch 配置文件：
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the class is annotated with `@Configuration` and `@EnableBatchProcessing`.
    This ensures that the `BatchJobConfiguration` class is registered as a configuration
    bean in Spring, as well as a couple of other batch-related bean components, such
    as `JobLauncher`, `JobBuilderFactory`, `JobRepository`, and `JobExplorer`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，该类被注解为 `@Configuration` 和 `@EnableBatchProcessing`。这确保了 `BatchJobConfiguration`
    类被注册为 Spring 中的一个配置实例，以及一些其他与批量相关的实例组件，例如 `JobLauncher`、`JobBuilderFactory`、`JobRepository`
    和 `JobExplorer`。
- en: The `etlJob()` function uses `JobBuilderFactory` to create the step pipeline,
    as described during the design phase. The `etlJob` pipeline starts with the `fileCheck`
    step. If the exit status of the `fileCheck` step is `STOPPED`, the batch job ends;
    otherwise, it moves to the next step – that is, `fileMoveToProcess`. The next
    step is `processFile`. On returning `COMPLETED` from the `processFile` step, the
    `moveToArchive` step is invoked. However, on returning `ExitStatus` as `FAILED`,
    the `moveToError` step is invoked.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`etlJob()` 函数使用 `JobBuilderFactory` 创建步骤管道，正如设计阶段所描述的。`etlJob` 管道从 `fileCheck`
    步骤开始。如果 `fileCheck` 步骤的退出状态是 `STOPPED`，则批量作业结束；否则，它将移动到下一个步骤——即 `fileMoveToProcess`。下一个步骤是
    `processFile`。从 `processFile` 步骤返回 `COMPLETED` 后，将调用 `moveToArchive` 步骤。然而，如果返回
    `ExitStatus` 为 `FAILED`，则调用 `moveToError` 步骤。'
- en: However, we can create an `etlJob` bean. To do so, we need to create all the
    step beans that are stitched together to form the batch job pipeline. Let’s begin
    by looking at how to create the `fileCheck` bean.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以创建一个 `etlJob` 实例。为此，我们需要创建所有拼接在一起形成批量作业管道的步骤实例。让我们首先看看如何创建 `fileCheck`
    实例。
- en: 'To create the `fileCheck` bean, we have written the following two classes:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 `fileCheck` 实例，我们编写了以下两个类：
- en: '`FileCheckConfiguration`: A configuration class where the `fileCheck` bean
    is initialized.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FileCheckConfiguration`：一个配置类，其中初始化了 `fileCheck` 实例。'
- en: '`FileCheckingTasklet`: A `Tasklet` class for the `fileCheck` step. `Tasklet`
    is meant to perform a single task within a step.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FileCheckingTasklet`：用于 `fileCheck` 步骤的 `Tasklet` 类。`Tasklet` 的目的是在步骤内执行单个任务。'
- en: '`FileCheckingTasklet` is a `Tasklet`, so it will implement a `Tasklet` interface.
    The code will be similar to the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`FileCheckingTasklet` 是一个 `Tasklet`，因此它将实现 `Tasklet` 接口。代码将类似于以下内容：'
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Tasklet` contains only one method – `execute()` – that must be implemented.
    It has the following type signature:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tasklet` 只包含一个必须实现的方法——`execute()`，它具有以下类型签名：'
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In `FileCheckingTasklet`, we wish to check whether any file is present in the
    landing zone or not. Our main aim for using this `Tasklet` is to change the `EXITSTATUS`
    property of the task based on whether the file is present or not. Spring Batch
    provides an interface called `StepExecutionListener` that enables us to modify
    `EXITSTATUS` based on our requirements. This can be done by implementing the `afterStep()`
    method of `StepExecutionListener`. The interface definition of `StepExecutionListener`
    looks as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `FileCheckingTasklet` 中，我们希望检查是否在目标区域中存在任何文件。我们使用此 `Tasklet` 的主要目的是根据文件是否存在来更改任务的
    `EXITSTATUS` 属性。Spring Batch 提供了一个名为 `StepExecutionListener` 的接口，使我们能够根据我们的需求修改
    `EXITSTATUS`。这可以通过实现 `StepExecutionListener` 的 `afterStep()` 方法来完成。`StepExecutionListener`
    的接口定义如下：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So, our `FileCheckingTasklet` will look similar to the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 `FileCheckingTasklet` 将类似于以下内容：
- en: '[PRE7]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let’s understand the logic that we want to execute in this `Tasklet`.
    We want to list all the files in the landing zone directory. If no files are present,
    we want to set `EXITSTATUS` to `STOPPED`. If we find one or more files, we want
    to set `EXITSTATUS` to `COMPLETED`. If an error occurs while listing the directory,
    we will set `EXITSTATUS` to `FAILED`. Since we can modify `EXITSTATUS` in the
    `afterStep()` method, we will write our logic in that method. However, we want
    to configure our landing zone folder in our application. We can do that by using
    a configuration POJO called `EnvFolderProperty` (we will discuss the code of this
    class later in this chapter). Here is the logic of the `afterstep()` method:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解我们想要在这个`Tasklet`中执行的逻辑。我们想要列出着陆区目录中的所有文件。如果没有文件存在，我们想要将`EXITSTATUS`设置为`STOPPED`。如果我们找到一个或多个文件，我们想要将`EXITSTATUS`设置为`COMPLETED`。如果在列出目录时发生错误，我们将设置`EXITSTATUS`为`FAILED`。由于我们可以在`afterStep()`方法中修改`EXITSTATUS`，因此我们将在这个方法中编写我们的逻辑。然而，我们想要在我们的应用程序中配置我们的着陆区文件夹。我们可以通过使用一个名为`EnvFolderProperty`的配置POJO来实现这一点（我们将在本章后面讨论这个类的代码）。以下是`afterstep()`方法的逻辑：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Since we don’t want to do any other processing in this `Tasklet`, we will let
    the `execute()` method pass with a `RepeatStatus` of `FINISHED`. So, our full
    code for `FileCheckingTasklet` will look as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不想在这个`Tasklet`中执行任何其他处理，我们将让`execute()`方法通过一个`RepeatStatus`为`FINISHED`的状态通过。因此，`FileCheckingTasklet`的完整代码如下：
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let’s see how we can use `FileCheckingTasklet` to create the `fileCheck`
    step. In the `FileCheckConfiguration` configuration class, first, we create a
    bean for `FileCheckingTasklet`, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何使用`FileCheckingTasklet`来创建`fileCheck`步骤。在`FileCheckConfiguration`配置类中，首先，我们为`FileCheckingTasklet`创建一个bean，如下所示：
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we use this bean to create the `fileCheck` step bean, as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用这个bean创建`fileCheck`步骤bean，如下所示：
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, the full code for the `FileCheckConfiguration` configuration class
    will look as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`FileCheckConfiguration`配置类的完整代码如下：
- en: '[PRE12]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding steps, we learned how to create a step using `Tasklet` and
    the `StepExecutionListener` interface and instantiate and utilize them using Spring’s
    powerful annotations, such as `@Bean`, `@Configuration`, and `@AutoWired`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们学习了如何使用`Tasklet`和`StepExecutionListener`接口创建步骤，并使用Spring的强大注解（如`@Bean`、`@Configuration`和`@AutoWired`）实例化和利用它们。
- en: Important note
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Spring Batch provides various listeners (listener interfaces) to intercept,
    listen, and react to the Spring Batch job flow at different levels. If you are
    interested, you can learn more about Spring Batch listeners at [https://howtodoinjava.com/spring-batch/spring-batch-event-listeners/](https://howtodoinjava.com/spring-batch/spring-batch-event-listeners/).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Batch提供了各种监听器（监听器接口），可以在不同的级别拦截、监听并对Spring Batch作业流程做出反应。如果您感兴趣，您可以在[https://howtodoinjava.com/spring-batch/spring-batch-event-listeners/](https://howtodoinjava.com/spring-batch/spring-batch-event-listeners/)了解更多关于Spring
    Batch监听器的信息。
- en: Now, we will move on to the next step, called `moveFileToProcess`, and see how
    we can implement its design. Again, to implement the `moveFileToProcess` step,
    we will be writing a configuration file called `FileMoveToProcessConfiguration`
    and a tasklet file called `FileMoveToProcessTasklet`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续到下一个步骤，称为`moveFileToProcess`，并看看我们如何实现其设计。同样，为了实现`moveFileToProcess`步骤，我们将编写一个名为`FileMoveToProcessConfiguration`的配置文件和一个名为`FileMoveToProcessTasklet`的任务文件。
- en: 'First, let’s build our `FileMoveToProcessTasklet` tasklet. To build our tasklet,
    we will define the tasks that we want to achieve while using it. Here are the
    tasks that we want to accomplish using this tasklet:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建我们的`FileMoveToProcessTasklet`任务。为了构建我们的任务，我们将定义在使用它时想要完成的任务。以下是我们要使用此任务完成的任务：
- en: List the files present in the landing zone
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出着陆区中存在的文件
- en: Move one file at a time to the process zone
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次移动一个文件到处理区
- en: Add the destination full file path (the file path in the processing zone) as
    the key-value entry to `JobExecutionContext`
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将目标完整文件路径（处理区的文件路径）作为键值条目添加到`JobExecutionContext`
- en: Just like the previous tasklet that we developed, `FileMoveToProcessTasklet`
    will also implement the `Tasklet` and `StepExecutionListener` interfaces.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前开发的那个任务`FileMoveToProcessTasklet`一样，它也将实现`Tasklet`和`StepExecutionListener`接口。
- en: 'The following code shows our implementation of the `execute()` function of
    the `Tasklet` interface:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了我们对`Tasklet`接口的`execute()`函数的实现：
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, we list the files from the landing zone (the read directory path) and
    if the list of files is not empty, we get the first file and move it to the destination
    path. Here, we create the destination path by appending the filename and file
    separator to the processing directory.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从着陆区（读取目录路径）列出文件，如果文件列表不为空，我们就获取第一个文件并将其移动到目标路径。在这里，我们通过将文件名和文件分隔符附加到处理目录来创建目标路径。
- en: 'Once the file is successfully moved to the destination, we set the value of
    the `filepath` instance variable as the destination path where the file has been
    moved.  We will use this in our implementation of the `afterStep()` method. Now,
    let’s look at the implementation of the `afterStep()` method, as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件成功移动到目标位置，我们将`filepath`实例变量的值设置为文件已移动到的目标路径。我们将在我们的`afterStep()`方法实现中使用它。现在，让我们看看`afterStep()`方法的实现，如下所示：
- en: '[PRE14]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the `afterStep()` method implementation, we store two key-value entries (`filePath`
    and `filePathName`) in `JobExecutionContext` if `filePath` is not null (which
    means at least one file was present in the landing zone during tasklet execution
    and has been successfully moved to the processing zone by the tasklet).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在`afterStep()`方法实现中，如果`filePath`不为空（这意味着在任务执行期间至少有一个文件存在于着陆区，并且已被任务成功移动到处理区），我们在`JobExecutionContext`中存储两个键值条目（`filePath`和`filePathName`）。
- en: 'Now, let’s see the full code for the `FileMoveToProcessTasklet` class:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看`FileMoveToProcessTasklet`类的完整代码：
- en: '[PRE15]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The source code of `FileMoveToProcessConfiguration` will be very similar to
    `FileCheckConfiguration`, which we discussed earlier.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`FileMoveToProcessConfiguration`的源代码将与之前讨论的`FileCheckConfiguration`非常相似。'
- en: 'The source code of `FileMoveToProcessConfiguration` is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`FileMoveToProcessConfiguration`的源代码如下：'
- en: '[PRE16]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, we will learn how to develop the `processFile` step. This is an important
    step as all the transformation and ingestion happens here. This step follows a
    typical `SpringBatch` step template, where there is an `ItemReader`, `ItemProcessor`,
    and `ItemWriter`. They are stitched together to form the step pipeline.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习如何开发`processFile`步骤。这是一个重要的步骤，因为所有的转换和摄取都发生在这里。这个步骤遵循典型的`SpringBatch`步骤模板，其中有一个`ItemReader`、`ItemProcessor`和`ItemWriter`。它们被缝合在一起形成步骤管道。
- en: 'First, let’s look at the source code for building the step pipeline in the
    `processFile()` method of the `ProcessFileConfiguration` configuration class:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看`ProcessFileConfiguration`配置类中`processFile()`方法构建步骤管道的源代码：
- en: '[PRE17]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we are building the step from an `ItemReader` bean called `csvRecordReader`,
    which reads the records from the CSV file and returns a set of `EventLogODL` POJO
    objects. An `ItemProcessor` bean called `deviceEventProcessor`, which reads each
    `EventLogODL` POJO and transforms them into `DeviceEventLogFact` POJOs, and an
    `ItemWriter` bean called `jdbcWriter`, which reads each record as a `DeviceEventLogFact`
    POJO and persists them in the PostgreSQL data warehouse. We also mention `chunk`
    while building the pipeline while using `chunkSize` as a configurable parameter
    (for learning purposes, we will test with a `chunkSize` of `1`).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在从名为`csvRecordReader`的`ItemReader` Bean构建步骤，该Bean从CSV文件中读取记录并返回一组`EventLogODL`
    POJO对象。一个名为`deviceEventProcessor`的`ItemProcessor` Bean，它读取每个`EventLogODL` POJO并将它们转换成`DeviceEventLogFact`
    POJOs，以及一个名为`jdbcWriter`的`ItemWriter` Bean，它读取每个记录作为`DeviceEventLogFact` POJO并将它们持久化到PostgreSQL数据仓库。我们在构建管道时也提到了`chunk`，同时使用`chunkSize`作为可配置参数（为了学习目的，我们将使用`chunkSize`为`1`进行测试）。
- en: 'Before we explain how to develop the `ItemReader` bean, let’s look at the source
    code of the `EventLogODL` POJO class:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释如何开发`ItemReader` Bean之前，让我们看看`EventLogODL` POJO类的源代码：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let’s look at the method that creates the `ItemReader` bean:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看创建`ItemReader` Bean的方法：
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, we are using Spring Batch’s inbuilt `FlatFileItemReader` to read the CSV
    source file. Since we need to dynamically read `filePathName` from `jobExecutionContext`,
    which we set in the previous step, we used the `@SetScope` annotation, which changes
    the default bean scope from a singleton to a step-specific object. This annotation
    is especially helpful for late binding where we want to read some parameters dynamically
    from `JobExecutionContext` or `StepExecutionContext`. Also, we are creating a
    delimited tokenizer with `fieldNames` and a `BeanWrapperFieldSetMapper` to map
    each record to the `EventLogODL` POJO and set the corresponding properties of
    the `FlatfileItemReader` instance.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Spring Batch内置的`FlatFileItemReader`来读取CSV源文件。由于我们需要从`jobExecutionContext`中动态读取`filePathName`，这是我们之前设置的，所以我们使用了`@SetScope`注解，它将默认的bean作用域从单例改为步骤特定的对象。这个注解在需要从`JobExecutionContext`或`StepExecutionContext`动态读取某些参数的后期绑定中特别有用。此外，我们创建了一个带有`fieldNames`的分隔符标记化器，以及一个`BeanWrapperFieldSetMapper`来将每条记录映射到`EventLogODL`
    POJO，并设置`FlatfileItemReader`实例的相应属性。
- en: Troubleshooting tips
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除技巧
- en: In an ideal world, all data is perfect, and our job should run fine every time.
    But we don’t live in an ideal world. What happens if the data is corrupted? What
    happens if a few records in the file are not following the proper schema? How
    do we handle such situations?
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想的世界里，所有数据都是完美的，我们的作业应该每次都能正常运行。但现实并非如此。如果数据损坏会怎样？如果文件中的几个记录没有遵循正确的模式会怎样？我们如何处理这种情况？
- en: 'There is no simple answer. However, Spring Batch gives you few capabilities
    to handle failures. If the file itself is corrupted and not readable or the file
    doesn’t have proper read and write permissions, then it will go to a `faultTolerance`
    during building the step. This can either be done by using the `skipLimit()`,
    `skip()`, and `noSkip()` methods or using a custom `SkipPolicy`. In our example,
    we can add a fault tolerance to the `processFile` method of the `ProcessFileConfiguration`
    class and skip certain kinds of exceptions while ensuring few other types of exceptions
    cause a step failure. An example is shown in the following code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 没有简单的答案。然而，Spring Batch为你提供了一些处理失败的能力。如果文件本身损坏且不可读，或者文件没有适当的读写权限，那么在构建步骤时将进入`faultTolerance`状态。这可以通过使用`skipLimit()`、`skip()`和`noSkip()`方法或使用自定义的`SkipPolicy`来实现。在我们的例子中，我们可以在`ProcessFileConfiguration`类的`processFile`方法中添加容错性，并跳过某些类型的异常，同时确保其他类型的异常导致步骤失败。以下代码展示了示例：
- en: '`return stepBuilderFactory.get("processFile")`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`return stepBuilderFactory.get("processFile")`'
- en: '`        .<EventLogODL, DeviceEventLogFact>chunk(chunkSize)`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`        .<EventLogODL, DeviceEventLogFact>chunk(chunkSize)`'
- en: '`        .reader(csvRecordReader)`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`        .reader(csvRecordReader)`'
- en: '`.faultTolerant().skipLimit(20).skip(SAXException.class).noSkip(AccessDeniedException.class)`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`.faultTolerant().skipLimit(20).skip(SAXException.class).noSkip(AccessDeniedException.class)`'
- en: '`        .processor(deviceEventProcessor)`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`        .processor(deviceEventProcessor)`'
- en: '`        .writer(jdbcWriter)`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`        .writer(jdbcWriter)`'
- en: '`        .build();`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`        .build();`'
- en: As we can see, we can add fault tolerance by chaining the `faultTolerant()`
    method in `stepBuilderFactory.build()`. Then, we can chain the `skip()` method
    so that it skips 20 errors of the `SAXException` type and use the `noSkip()` method
    to ensure `AccessDeniedException` will always cause a **Step Failure**.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们可以在`stepBuilderFactory.build()`中通过链式调用`faultTolerant()`方法来增加容错性。然后，我们可以链式调用`skip()`方法，使其跳过20个`SAXException`类型的错误，并使用`noSkip()`方法确保`AccessDeniedException`总会导致**步骤失败**。
- en: 'Now, let’s see how we can develop our custom `ItemProcessor`. The source code
    of the custom `ItemProcessor`, called `DeviceEventProcessor`, is shown in the
    following block:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何开发我们的自定义`ItemProcessor`。名为`DeviceEventProcessor`的自定义`ItemProcessor`的源代码如下所示：
- en: '[PRE20]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As you can see, we have implemented the `ItemProcessor` interface, where we
    need to implement the `process()` method. To convert the `EventLogODL` POJO into
    a `DeviceEventLogFact` POJO, we have created a delegate component called `DeviceEventLogMapper`.
    The source code of `DeviceEventLogMapper` is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们已经实现了`ItemProcessor`接口，其中我们需要实现`process()`方法。为了将`EventLogODL` POJO转换为`DeviceEventLogFact`
    POJO，我们创建了一个名为`DeviceEventLogMapper`的代理组件。`DeviceEventLogMapper`的源代码如下：
- en: '[PRE21]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since we are developing the code for the fact table, we need various primary
    keys to be fetched from different dimension tables to populate our `DeviceEventLogFact`
    POJO. Here, we are dynamically creating a query by using `jdbcTemplate` to fetch
    the dimension primary keys from the data warehouse and populating the `DeviceEventLogFact`
    POJO from its `resultset`. The complete source code for `DeviceEventLogMapper`
    is available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SpringBatchApp/EtlDatawarehouse/src/main/java/com/scalabledataarchitecture/etl/steps/DeviceEventLogMapper.java](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SpringBatchApp/EtlDatawarehouse/src/main/java/com/scalabledataarchitecture/etl/steps/DeviceEventLogMapper.java).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在为事实表编写代码，我们需要从不同的维度表中获取各种主键来填充我们的 `DeviceEventLogFact` POJO。在这里，我们通过使用
    `jdbcTemplate` 动态创建查询来从数据仓库中获取维度主键，并从其 `resultset` 中填充 `DeviceEventLogFact` POJO。`DeviceEventLogMapper`
    的完整源代码可在 GitHub 上找到，链接为 [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SpringBatchApp/EtlDatawarehouse/src/main/java/com/scalabledataarchitecture/etl/steps/DeviceEventLogMapper.java](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SpringBatchApp/EtlDatawarehouse/src/main/java/com/scalabledataarchitecture/etl/steps/DeviceEventLogMapper.java)。
- en: 'Finally, we will create an `ItemWriter` called `jdbcwriter` in the `ProcessFileConfiguration`
    class, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在 `ProcessFileConfiguration` 类中创建一个名为 `jdbcwriter` 的 `ItemWriter`，如下所示：
- en: '[PRE22]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, our source code for the `ProcessFileConfiguration` class looks as
    follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的 `ProcessFileConfiguration` 类的源代码如下所示：
- en: '[PRE23]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that we have built our code, we will configure our properties in the `application.yaml`
    file, which is present in the `resource` folder, as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了代码，我们将在 `resource` 文件夹中存在的 `application.yaml` 文件中配置我们的属性，如下所示：
- en: '[PRE24]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As shown in the `.yaml` file, we have to mention the `spring.datasource` properties
    so that Spring JDBC can automatically auto-wire the data source component.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如 `.yaml` 文件所示，我们必须提及 `spring.datasource` 属性，以便 Spring JDBC 可以自动自动连接数据源组件。
- en: 'Our final code structure will look as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的代码结构如下所示：
- en: '![Figure 4.14 – Project structure of the code ](img/B17084_04_014.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – 代码的项目结构](img/B17084_04_014.jpg)'
- en: Figure 4.14 – Project structure of the code
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 代码的项目结构
- en: We can run and test our program by running our Spring Boot application from
    our favorite IDE by running the `Main` class – that is, `EtlDataWarehouseApplication`.
    We must install Postgres, create the schema and the database tables, and populate
    all the dimension tables before we run our Spring Batch application here. Detailed
    run instructions can be found in this book’s GitHub repository.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在最喜欢的 IDE 中运行 `Main` 类（即 `EtlDataWarehouseApplication`）来运行和测试我们的程序。在运行我们的
    Spring Batch 应用程序之前，我们必须安装 Postgres，创建模式和数据表，并填充所有维度表。详细的运行说明可以在本书的 GitHub 仓库中找到。
- en: 'Once we have run our application and placed our data in the landing zone, it
    gets ingested into our data warehouse fact table, and the CSV files get moved
    to the archival zone, as shown in the following screenshot:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行了应用程序并将数据放置在着陆区，它就会被导入我们的数据仓库事实表，CSV 文件会被移动到存档区，如下面的截图所示：
- en: '![Figure 4.15 – Data ingested in the fact table after the Spring Batch job
    runs ](img/B17084_04_015.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – Spring Batch 作业运行后事实表中的数据导入](img/B17084_04_015.jpg)'
- en: Figure 4.15 – Data ingested in the fact table after the Spring Batch job runs
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – Spring Batch 作业运行后事实表中的数据导入
- en: 'We can also see the batch-related tables, which contain various run statistics,
    as shown in the following screenshot:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到与批处理相关的表，其中包含各种运行统计信息，如下面的截图所示：
- en: '![Figure 4.16 – Batch job execution log ](img/B17084_04_016.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16 – 批处理作业执行日志](img/B17084_04_016.jpg)'
- en: Figure 4.16 – Batch job execution log
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 批处理作业执行日志
- en: 'The preceding screenshot shows the batch execution log for the different batch
    jobs that have run. We can learn more about a specific job by looking at the batch
    step execution log, as shown in the following screenshot:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了已运行的各个批处理作业的批执行日志。我们可以通过查看批步骤执行日志来了解更多关于特定作业的信息，如下面的截图所示：
- en: '![Figure 4.17 – Step execution log for the batch jobs ](img/B17084_04_017.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.17 – 批处理作业的步骤执行日志](img/B17084_04_017.jpg)'
- en: Figure 4.17 – Step execution log for the batch jobs
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 – 批处理作业的步骤执行日志
- en: With that, we have analyzed, architected, designed, developed, and tested a
    batch-based ETL data ingestion pipeline successfully. As mentioned in the *Technical
    requirements* section, the detailed source code is available in this book’s GitHub
    repository.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经成功分析了、架构设计、开发并测试了一个基于批次的ETL数据摄取管道。正如在*技术要求*章节中提到的，详细的源代码可以在本书的GitHub仓库中找到。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to analyze a data engineering requirement from
    scratch, draw a definite conclusion, and extract facts that will help us in our
    architectural decision-making process. Next, we learned how to profile source
    data and how such an analysis helps us build better data engineering solutions.
    Going further, we used facts, requirements, and our analysis to build a robust
    and effective architecture for a batch-based data engineering problem with a low
    or medium volume of data. Finally, we mapped the design to build an effective
    ETL batch-based data ingestion pipeline using Spring Batch and test it. Along
    the way, you learned how to analyze a data engineering problem from scratch and
    how to build similar pipelines effectively for when you are presented with a similar
    problem next time around.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何从头开始分析数据工程需求，得出明确的结论，并提取出有助于我们在架构决策过程中的事实。接下来，我们学习了如何分析源数据，以及这种分析如何帮助我们构建更好的数据工程解决方案。进一步地，我们利用事实、需求和我们的分析，为具有低或中等数据量的基于批次的
    数据工程问题构建了一个稳健且有效的架构。最后，我们将设计映射到构建一个有效的使用Spring Batch的ETL批处理数据摄取管道，并对其进行测试。在这个过程中，你学习了如何从头开始分析数据工程问题，以及如何在下次遇到类似问题时有效地构建类似的管道。
- en: Now that we have successfully architected and developed a batch-based solution
    for medium- and low-volume data engineering problems, in the next chapter, we
    will learn how to build an effective data engineering solution for dealing with
    huge data volumes. In the next chapter, we will discuss an interesting use case
    for building an effective batch-based big data solution.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功架构和开发了一个针对中等和低量数据工程问题的基于批次的解决方案，在下一章中，我们将学习如何构建一个有效的数据工程解决方案来处理大量数据。在下一章中，我们将讨论构建一个有效的基于批次的大数据解决方案的有趣用例。
