- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ETL Data Load – A Batch-Based Solution to Ingesting Data in a Data Warehouse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed various foundational concepts surrounding
    data engineering, starting with the different types of data engineering problems.
    Then, we discussed various data types, data formats, data storage, and databases.
    We also discussed the various platforms that are available to deploy and run data
    engineering solutions in production.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to architect and design a batch-based solution
    for low to medium-volume data ingestion from a data source to a **data warehouse**.
    Here, we will be taking a real-time use case to discuss, model, and design a data
    warehouse for such a scenario. We will also learn how to develop this solution
    using a Java-based technical stack and run and test our solution. By the end of
    this chapter, you should be able to design and develop an **extract, transform,
    load** (**ETL**)-based batch pipeline using Java and its related stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem and source data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an effective data model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing and unit testing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code files for this chapter in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter04/SpringBatchApp/EtlDatawarehouse](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter04/SpringBatchApp/EtlDatawarehouse).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem and source data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data engineering often involves collecting, storing, and analyzing data. But
    nearly all data engineering landscapes start with ingesting raw data into a data
    lake or a data warehouse. In this chapter, we will be discussing one such typical
    use case and build an end-to-end solution for the problem discussed in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Company XYZ is a third-party vendor that provides services for building and
    maintaining data centers. Now, Company XYZ is planning to build a data center
    monitoring tool for its customer. The customer wants to see various useful metrics,
    such as the number of incidents reported for any device on an hourly, monthly,
    or quarterly basis. They also want reports on closure ratios and average closure
    duration. They are also interested in searching incidents based on the type of
    device or incident type. They are also interested to find time-based outage patterns
    to predict seasonal or hourly usage surges for any set of resources. These reports
    need to be generated once every 12 hours. To generate such reports, a data warehouse
    needs to be built, and data must be ingested and stored daily in that data warehouse
    so that such reports can easily be generated.
  prefs: []
  type: TYPE_NORMAL
- en: To create the solution for this data engineering problem, we must analyze the
    four dimensions of data (refer to the *Dimensions of data* section in [*Chapter
    1*](B17084_01.xhtml#_idTextAnchor014), *Basics of Modern Data Architecture*) in
    this use case. Our first question would be, *What is the velocity of data?* The
    answer to this question helps us to determine whether it is a real-time or batch
    processing problem. Although there is not much information about the input frequency
    of data as per the problem statement, it is clearly stated that the report needs
    to be generated after every 12 hours or twice daily. Irrespective of the incoming
    speed of data, if the frequency in which the downstream system needs data is more
    than an hour, we can safely decide that we are dealing with a batch-based data
    engineering problem (please refer to the *Types of data engineering problems*
    section in [*Chapter 1*](B17084_01.xhtml#_idTextAnchor014), *Basics of Modern
    Data Architecture*).
  prefs: []
  type: TYPE_NORMAL
- en: Our second question would be, *What is the volume of the data? Is it huge? Is
    there a chance that this can grow into hundreds of terabytes in the future?* These
    questions generally help us choose the technology that we should use. If the volume
    is huge (in terabytes or hundreds of terabytes), only then should we choose **big
    data** technologies to solve our problem. A lot of times, architects tend to use
    big data in a non-big data use case, which makes the solution unsustainable and
    expensive in terms of cost, maintenance, and time. In our case, the data that
    needs to be ingested is incident log data. Such data is usually not huge. However,
    an architect should get confirmation about the data that will be sent for ingestion.
    In this case, let’s suppose that the customers responded and said that the data
    will be sent every couple of hours as a flat file, consisting of a Delta of the
    incidents that have either been newly logged or updated. This would mean that
    our datasets will be either in a small file or a medium-sized file. This means
    that as an architect, we should choose a non-big data-based solution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the source data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third important question that an architect must ask is, *What is the variety
    of the data? Is it structured, unstructured, or semi-structured?* This question
    often helps to determine how such data can be processed and stored. If the data
    is unstructured, then we need to store it in a NoSQL database, but structured
    data can be stored in RDBMS databases. There is another question related to **data
    variety** – that is, *What is the format of the data? Is it in CSV format, JSON
    format, Avro format, or Parquet format? Is the data compressed when received?*
    Often, these questions help determine the techniques, technologies, processing
    rules, and pipeline design required to process and ingest the data. In our case,
    since it is not mentioned in the initial requirement, we need to ask the customers
    these questions. Let’s suppose our customers agree to send the data in the form
    of CSV files. So, in this case, we are dealing with structured data and the data
    is coming as a CSV file without any compression. As it is structured data, it
    is apt for us to use a relational data model or RDBMS database to store our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the final question regarding the dimension of the data: *What
    is the veracity of the data?* Or, in simpler terms, *What is the quality of the
    data that we receive? Is there too much noise in the data?* Of all the data engineering
    solutions that fail to solve a customer problem, the majority fail because of
    a lack of time spent analyzing and profiling the source data. Understanding the
    nature of the data that is coming is very important. We must ask, and be able
    to answer, the following kinds of questions at the end of the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the source data contain any junk characters that need to be removed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it contain any special characters?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the source data contain non-English characters (such as French or German)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do any numeric columns contain null values? Which can or cannot be nullable
    columns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there something unique with which we can determine each record?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And the list goes on.
  prefs: []
  type: TYPE_NORMAL
- en: To analyze the source data, we should run a data profiling tool such as Talend
    Open Studio, DataCleaner, or AWS Glue DataBrew to analyze and visualize various
    metrics of the data. This activity helps us understand the data better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will analyze the CSV data file that we need to ingest for our use
    case using the DataCleaner tool. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you can download DataCleaner Community Edition by going to [https://datacleaner.github.io/downloads](https://datacleaner.github.io/downloads).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, unzip the downloaded ZIP file in the desired installation folder. Based
    on your operating system, you can start DataCleaner using either the `datacleaner.sh`
    command or the `datacleaner.cmd` file present under the root installation folder.
    You will see a home screen, as shown in the following screenshot. Here, you can
    start a new data profiling job by clicking the **Build new job** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.1 – DataCleaner welcome screen ](img/B17084_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – DataCleaner welcome screen
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, a dialog will pop up, where you can select the data store, as shown in
    the following screenshot. Here, we will browse for and select our input file called
    `inputData.csv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2 – DataCleaner – the Select datastore popup ](img/B17084_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – DataCleaner – the Select datastore popup
  prefs: []
  type: TYPE_NORMAL
- en: Once the datastore is selected, we will see our data source at the top of the
    left pane. We should be able to see the column names of our CSV as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will drag and drop our data source `inputData.csv` file to the right
    pane, which is the pipeline building canvas. To profile the data, DataCleaner
    provides various analyzer tools under the **Analyze** menu, which is visible in
    the left pane. For our use case, we will be using **String analyzer**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Creating an analysis pipeline ](img/B17084_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Creating an analysis pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '**String analyzer** analyzes various string-related metrics such as the NULL
    count, blank count, white spaces, character case, and so on. The following screenshot
    shows the various configuration options of a **String analyzer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Adding String analyzer ](img/B17084_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Adding String analyzer
  prefs: []
  type: TYPE_NORMAL
- en: We will add another analyzer called `incidentNumber`, `deviceSerialNum`, `eventCode`,
    and `loggedTime` to be an eligible entry for our data warehouse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If any of this information is missing, such a record will not add value to
    the problem that we are trying to solve. Here, **Completeness analyzer** will
    help us determine whether we need special checks to handle these constraints and
    drop records if these fields are blank. The following screenshot shows the various
    configuration options of **Completeness analyzer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Adding the Completeness analyzer ](img/B17084_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Adding the Completeness analyzer
  prefs: []
  type: TYPE_NORMAL
- en: 'The final profiling pipeline for our use case can be seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Final analyzer pipeline ](img/B17084_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Final analyzer pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we execute this pipeline, **Analysis results** will be generated, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Analysis results ](img/B17084_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Analysis results
  prefs: []
  type: TYPE_NORMAL
- en: Such data profiling can provide us with various pieces of information about
    the data, which can help us adjust our tools, technologies, and transformations
    to create an effective and successful data engineering solution. As shown in the
    preceding screenshot, we can infer that the total data size is 300 rows. Out of
    these, 53 are open incidents. The resolution comments can have spaces in them,
    all `deviceSerialNum` values are lowercase, and `status` values are uppercase.
    Such information helps us make effective decisions while designing the solution.
  prefs: []
  type: TYPE_NORMAL
- en: For the brevity of this discussion, we are only showing one form of data profiling
    for a source data file. However, we can do the same for other kinds of datasets.
    In this use case, you can do similar data profiling for the data in the `device_dm.csv`
    and `event_dm.csv` files.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the requirements and have a fair idea of the source
    data, in the next section, we will discuss how to design the model so that it
    can store the ingested data.
  prefs: []
  type: TYPE_NORMAL
- en: Building an effective data model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From our previous discussion and after analyzing the data, we have concluded
    that our data is structured, so it’s suitable for being stored in a relational
    data model. From the requirements, we have gathered that our final data store
    should be a data warehouse. Keeping these two basic factors in mind, let’s learn
    about relational data warehouse schemas.
  prefs: []
  type: TYPE_NORMAL
- en: Relational data warehouse schemas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s explore the popular relational data warehouse schemas that we can consider
    when creating our data model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Star schema**: This is the most popular data warehouse schema type. As shown
    in the following diagram, there is a **Fact Table** in the middle where each record
    represents a fact or an event that has happened over time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Star schema ](img/B17084_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Star schema
  prefs: []
  type: TYPE_NORMAL
- en: This **Fact Table** consists of various dimensions whose details need to be
    looked up from associated lookup tables called **dimension tables**. This **Fact
    Table** is associated with each dimension table using a foreign key. The preceding
    diagram shows what a star schema looks like. Since there is a **Fact Table** in
    the middle surrounded by multiple dimension tables on various sides, its structure
    looks like a star, hence its name.
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowflake schema**: This is an extension of the star schema. Just like the
    star schema, here, there is a **Fact Table** in the middle and multiple dimension
    tables around it. However, in the snowflake schema, each dimension table further
    references other child dimension tables, making the structure look like a snowflake:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Snowflake schema ](img/B17084_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Snowflake schema
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see how each dimension table is connected to the child dimension
    tables using foreign key relations, making the structure look like a snowflake,
    hence its name.
  prefs: []
  type: TYPE_NORMAL
- en: '**Galaxy schema**: A galaxy schema is a schema that consists of more than one
    fact table. Here, one or more dimension tables are shared by multiple fact tables.
    This schema can be visualized as a collection of two or more star schemas, hence
    its name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of the schema design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our use case, we need to evaluate what schema design best fits our use case.
  prefs: []
  type: TYPE_NORMAL
- en: The first question that we should ask is, *In our use case, do we need multiple
    fact tables?* Since our fact table only consists of device events or incidents,
    we can only have one fact table. This eliminates any chance of having a galaxy
    schema as our candidate data model. Now, we must determine whether a star schema
    or a snowflake schema is suitable for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To choose between those two alternatives, let’s look at the following columns
    of our `inputData.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`incidentNumber`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deviceSerialNo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eventCode`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loggedTime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`closureTime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`status`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assignedTo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resolutionComments`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the column names for this file, we can say that this is the device
    event log file. This implies that the data from the `inputData.csv` file needs
    to be ingested into our central fact table. But first, we need to determine whether
    we need to reference only dimension tables, which are complete in themselves,
    or whether our dimensions table needs to do further lookups in another set of
    dimension tables.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by figuring out the candidate dimensions from the dataset present
    in `inputData.csv`. It is important to remember that candidate dimensions are
    determined by the purpose or goal of building the data warehouse. The purpose
    of the data warehouse we are building is to obtain metrics on `eventType`, device
    over different time intervals such as hourly, monthly, and quarterly, and closure
    duration metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, `deviceSerialNo` and `eventCode` can correspond to two dimensions
    called `incidentNumber` will vary in each fact record, so it’s not a candidate
    for dimension. `status`, `loggedTime`, and `closureTime` will vary from record
    to record, so they are best suited for being facts and not dimensions. Since we
    are not doing any analysis on the `assignedTo` and `resolutionComment` fields,
    we can ignore those columns in our data model. In a real-world scenario, usually,
    incoming source data files contain hundreds of columns. However, only a small
    percentage of those columns are useful for solving a problem.
  prefs: []
  type: TYPE_NORMAL
- en: It is always advised to ingest only the columns that you need. This saves space,
    complexity, and money (remember that a lot of solutions these days are deployed
    on cloud platforms or are candidates for future migration to cloud platforms,
    and cloud platforms follow the principle of pay for what you use, so you should
    only ingest data that you intend to use). Apart from these, our requirements need
    us to mark every event on an hourly, monthly, and quarterly basis so that aggregations
    can easily be run on these intervals and hourly, monthly, and quarterly patterns
    can be analyzed. This interval tagging can be derived from `loggedTime` while
    saving the record. However, `hour`, `month`, and `quarter` can be stored as derived
    dimensions associated with our central fact table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, from our analysis, it is clear that our fact table only references those
    dimension tables that are complete in themselves. So, we can conclude that we
    will be using a star schema for our data modeling with the following set of tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DEVICE_EVENT_LOG_FACT`: This is the centralized fact table, which consists
    of each incident entry'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEVICE_DIMENSION`: This is the dimension table, which consists of device lookup
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EVENT_DIMENSION`: This is the dimension table, which consists of event lookup
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HOUR_DIMENSION`: This is the dimension table, which consists of static hour
    lookup data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MONTH_DIMENSION`: This is the dimension table, which consists of static month
    lookup data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QAURTER_DIMENSION`: This is the dimension table, which consists of static
    quarter lookup data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the detailed star schema data model of the data
    warehouse that we are building:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Data model for our data warehouse ](img/B17084_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Data model for our data warehouse
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s understand the tables and their columns shown in the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `DEVICE_EVENT_LOG_FACT` table, the following is happening:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `eventLogId` as the primary key, which maps to `incidentNumber` from
    our file
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We have foreign key fields for the `DEVICE_DIMENSION`, `EVENT_DIMENSION`, `HOUR_DIMENSION`,
    `MONTH_DIMENSION`, and `QUARTER_DIMENSION` tables
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`eventTimestamp`, `closurestatus`, and `closureDuration` are all facts for
    each row in the fact table'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The columns of `DEVICE_DIMENSION` and `EVENT_DIMENSION` are determined by the
    need as well as the data/attributes available for the device and events in the
    input files – that is, `device_dm.csv` and `event_dm.csv`. However, the primary
    keys of these two tables (`deviceId` and `eventId`) should be system-generated
    sequence numbers that are assigned to a record. The primary key in these two tables
    is the reference column for the foreign key relationship with the fact table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the device and event, we have designed three other dimension tables
    denoting hours of the day (`HOUR_DIMENSION`), month (`MONTH_DIMENSION`), and quarter
    (`QUARTER_DIMENSION`) of the year. These are static lookup tables, and their data
    will always remain constant over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next design decision that needs to be made in terms of the data model is
    the decision to choose a database. Various **Relational Database Management Systems**
    (**RDBMSs**) are well suited for a data warehouse, such as Snowflake, AWS Redshift,
    PostgreSQL, and Oracle. While the first two options are cloud-based data warehouses,
    the other two options can be run both on-premises and in the cloud. For our use
    case, we should choose a database that is cost-effective as well as future-compatible.
  prefs: []
  type: TYPE_NORMAL
- en: Out of these choices, we will choose PostgreSQL since it is a free database
    that is powerful and feature-rich to host a data warehouse. Also, our application
    may be migrated to the cloud in the future. In that case, it can easily be migrated
    to AWS Redshift, as AWS Redshift is based on industry-standard PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have designed our data model and chosen our database, let’s go ahead
    and architect the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To design the solution for the current problem statement, let’s analyze the
    data points or facts that are available to us right now:'
  prefs: []
  type: TYPE_NORMAL
- en: The current problem is a batch-based data engineering problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem at hand is a data ingestion problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our source is CSV files containing structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our target is a PostgreSQL data warehouse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our data warehouse follows a star schema, with one fact table, two dynamic dimension
    tables, and three static dimension tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should choose a technology that is independent of the deployment platform,
    considering that our solution can be migrated to the cloud in the future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the context and scope of this book, we will explore optimum solutions based
    on Java-based technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the preceding facts, we can conclude that we have to build three similar
    data ingestion pipelines – one for the fact table and two others for the dynamic
    dimension tables. At this point, we must ask ourselves, *What happens to the file
    if the file ingestion is successful or if it fails? How do we avoid reading the
    file again?*
  prefs: []
  type: TYPE_NORMAL
- en: 'We will read the file from the `input` folder and ingest it into the data warehouse.
    If it fails, we will move the file to an `error` folder; otherwise, we will move
    it to an `archive` folder. The following diagram shows our findings and provides
    an overview of our proposed solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure – 4.11 – Solution overview ](img/B17084_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure – 4.11 – Solution overview
  prefs: []
  type: TYPE_NORMAL
- en: However, this proposed solution is a 10,000-feet view of it. There are still
    many questions that have been left unanswered in this solution. For instance,
    there are no details about the ingestion processes or the technology that we should
    use to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s try to decide on a technology based on the facts we have at hand.
    We need to find a Java-based ETL technology that supports batch ingestion. Also,
    it should have easy-to-use JDBC support to write and read data from PostgreSQL.
    We also need to have a scheduler to schedule the batch ingestion job and should
    have a retry ability mechanism. Also, our data is not huge, so we want to avoid
    big data-based ETL tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spring Batch fits all these requirements. Spring Batch is an excellent Java-based
    ETL tool for building batch jobs. It comes with a job scheduler and a job repository.
    Also, since it is a part of the Spring Framework, it can easily be integrated
    with various tools and technologies with Spring Boot and Spring integration. The
    following diagram shows the high-level components of the Spring Batch architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Spring Batch architecture ](img/B17084_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Spring Batch architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram denotes how a Spring Batch job works. Let’s look at the
    various steps that a Spring Batch job goes through to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: The Spring Batch job uses Spring’s job scheduler to schedule a job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Spring Job Scheduler** runs **Job Launcher**, which, in turn, executes a
    **Spring Batch job**. It also creates a job instance at this point and persists
    this information in the job repository database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `batch_job_instance`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_job_execution`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_job_execution_params`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_step_execution`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_job_execution_context`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_step_execution_context`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Spring Batch job**, which is executed by the **Job Launcher**, initiates
    individual steps to perform the job. Each step performs a specific task to achieve
    the overall aim of the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While there is a **Job Execution Context** present across all the steps of a
    job instance, there is a **Step Execution Context** present in each execution
    step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usually, a Spring Batch configuration helps stitch together each step in the
    desired sequence to create the Spring Batch pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each step, in turn, reads the data using `Reader` or `ItemReader`, processes
    the data using `Processor` or `ItemProcessor`, and writes the processed data using
    `Writer` or `ItemWriter`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have a fair understanding of the Spring Batch architecture, we
    will architect our ingestion pipeline using the Spring Batch job framework. The
    following diagram shows the architecture of our data ingestion pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Solution architecture ](img/B17084_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Solution architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at this solution in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Like all **Spring Batch jobs**, the **Spring Job Scheduler** schedules a **Job
    Launcher**, which instantiates the Spring job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our use case, we will use a total of three sequential and two conditional
    steps to complete the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the first steps, the application looks at whether there is a new file in
    the input folder or `JobExecutionContext` and marks **ExitStatus** as **COMPLETED**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If `JobExecutionContext`) from **Landing Zone** to **Processing Zone**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon completing the second step, the third step (**Spring Process Step**) is
    initiated. The third step transforms and loads the data into a data warehouse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon completing the third step, **Spring Archival Step** is initiated, which
    moves the processed file from the process folder to the archive folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, if **String Process Step** fails, **Spring Error Handler Step** will
    be initiated, where it moves the file from the **Processing Zone** folder to the
    **Error Zone** folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we learned how to logically divide the solution using the available
    facts and data points and come up with an optimal architecture for the problem.
    We also learned how the effectiveness of each solution is dependent on the technology
    stack that we choose.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to implement and test our solution using
    Spring Batch and related technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and unit testing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build the Spring Batch application to implement the
    solution that we designed in the preceding section. We will also run and test
    the solution.
  prefs: []
  type: TYPE_NORMAL
- en: First, we must understand that different jobs will have their own schedules.
    However, the dimension tables need to be loaded before the fact table, because
    the dimension tables are the lookup tables.
  prefs: []
  type: TYPE_NORMAL
- en: For the brevity of our discussion, we will only implement the Spring Batch application
    for the fact table. In this implementation, we will load the device data and event
    data from CSV to the table manually. However, you can follow the lead of the discussion
    by implementing the solution and developing two different Spring Batch applications
    for the device and event dimension tables. In this implementation, we will assume
    that the device and event data have already been loaded into the data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do that manually by executing the DMLs present at the following GitHub
    link: [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SQL/chapter4_ddl_dml.sql](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SQL/chapter4_ddl_dml.sql).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to begin by creating a Spring Boot Maven project and adding the required
    Maven dependencies. The following Maven dependencies should be added to the `pom.xml`
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Two Spring dependencies are added here: `spring-boot-starter-batch` is added
    for Spring Batch and `spring-boot-starter-jdbc` is added for communicating with
    the `postgreSQL` database (which is used as a data warehouse and the Spring Batch
    repository database). Apart from this, the JDBC driver for PostgreSQL and the
    logging dependencies are added.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As per our architecture, let’s start by creating the entry point of the Spring
    Boot application, which is the `Main` class, and initializing the job scheduler.
    The following code denotes our `Main` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `@SpringBootApplication` annotation denotes that this class is the entry
    point of the Spring Boot application. Also, please note that the `@EnableScheduling`
    annotation denotes that this application supports Spring job scheduling. A method
    with the `@Scheduled` annotation helps perform the scheduled function at the configured
    schedule interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spring Batch job scheduler supports all of the three formats as shown in
    the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `fixedDelayString` makes sure that there is a delay of *n* milliseconds
    between the end of a job and the beginning of another job. `fixedRateString` runs
    the scheduled job every *n* milliseconds, while `cron` schedules the job using
    some cron expression. In our case, we are using a `cron` expression to schedule
    the `perform()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `perform()` method adds a job parameter called `JobID` and triggers a Spring
    Batch job called `etlJob` using `jobLauncher`. `jobLauncher` is an auto-wired
    bean of the `JobLauncher` type.
  prefs: []
  type: TYPE_NORMAL
- en: The `etlJob` field in the `EtlDatawarehouseApplication` class, as shown earlier,
    is also auto-wired and hence is a Spring bean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will explore the Spring Batch configuration file where the `etlJob`
    bean is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the class is annotated with `@Configuration` and `@EnableBatchProcessing`.
    This ensures that the `BatchJobConfiguration` class is registered as a configuration
    bean in Spring, as well as a couple of other batch-related bean components, such
    as `JobLauncher`, `JobBuilderFactory`, `JobRepository`, and `JobExplorer`.
  prefs: []
  type: TYPE_NORMAL
- en: The `etlJob()` function uses `JobBuilderFactory` to create the step pipeline,
    as described during the design phase. The `etlJob` pipeline starts with the `fileCheck`
    step. If the exit status of the `fileCheck` step is `STOPPED`, the batch job ends;
    otherwise, it moves to the next step – that is, `fileMoveToProcess`. The next
    step is `processFile`. On returning `COMPLETED` from the `processFile` step, the
    `moveToArchive` step is invoked. However, on returning `ExitStatus` as `FAILED`,
    the `moveToError` step is invoked.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can create an `etlJob` bean. To do so, we need to create all the
    step beans that are stitched together to form the batch job pipeline. Let’s begin
    by looking at how to create the `fileCheck` bean.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the `fileCheck` bean, we have written the following two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FileCheckConfiguration`: A configuration class where the `fileCheck` bean
    is initialized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileCheckingTasklet`: A `Tasklet` class for the `fileCheck` step. `Tasklet`
    is meant to perform a single task within a step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileCheckingTasklet` is a `Tasklet`, so it will implement a `Tasklet` interface.
    The code will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Tasklet` contains only one method – `execute()` – that must be implemented.
    It has the following type signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In `FileCheckingTasklet`, we wish to check whether any file is present in the
    landing zone or not. Our main aim for using this `Tasklet` is to change the `EXITSTATUS`
    property of the task based on whether the file is present or not. Spring Batch
    provides an interface called `StepExecutionListener` that enables us to modify
    `EXITSTATUS` based on our requirements. This can be done by implementing the `afterStep()`
    method of `StepExecutionListener`. The interface definition of `StepExecutionListener`
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our `FileCheckingTasklet` will look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s understand the logic that we want to execute in this `Tasklet`.
    We want to list all the files in the landing zone directory. If no files are present,
    we want to set `EXITSTATUS` to `STOPPED`. If we find one or more files, we want
    to set `EXITSTATUS` to `COMPLETED`. If an error occurs while listing the directory,
    we will set `EXITSTATUS` to `FAILED`. Since we can modify `EXITSTATUS` in the
    `afterStep()` method, we will write our logic in that method. However, we want
    to configure our landing zone folder in our application. We can do that by using
    a configuration POJO called `EnvFolderProperty` (we will discuss the code of this
    class later in this chapter). Here is the logic of the `afterstep()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we don’t want to do any other processing in this `Tasklet`, we will let
    the `execute()` method pass with a `RepeatStatus` of `FINISHED`. So, our full
    code for `FileCheckingTasklet` will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s see how we can use `FileCheckingTasklet` to create the `fileCheck`
    step. In the `FileCheckConfiguration` configuration class, first, we create a
    bean for `FileCheckingTasklet`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use this bean to create the `fileCheck` step bean, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the full code for the `FileCheckConfiguration` configuration class
    will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding steps, we learned how to create a step using `Tasklet` and
    the `StepExecutionListener` interface and instantiate and utilize them using Spring’s
    powerful annotations, such as `@Bean`, `@Configuration`, and `@AutoWired`.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Spring Batch provides various listeners (listener interfaces) to intercept,
    listen, and react to the Spring Batch job flow at different levels. If you are
    interested, you can learn more about Spring Batch listeners at [https://howtodoinjava.com/spring-batch/spring-batch-event-listeners/](https://howtodoinjava.com/spring-batch/spring-batch-event-listeners/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on to the next step, called `moveFileToProcess`, and see how
    we can implement its design. Again, to implement the `moveFileToProcess` step,
    we will be writing a configuration file called `FileMoveToProcessConfiguration`
    and a tasklet file called `FileMoveToProcessTasklet`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s build our `FileMoveToProcessTasklet` tasklet. To build our tasklet,
    we will define the tasks that we want to achieve while using it. Here are the
    tasks that we want to accomplish using this tasklet:'
  prefs: []
  type: TYPE_NORMAL
- en: List the files present in the landing zone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move one file at a time to the process zone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the destination full file path (the file path in the processing zone) as
    the key-value entry to `JobExecutionContext`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like the previous tasklet that we developed, `FileMoveToProcessTasklet`
    will also implement the `Tasklet` and `StepExecutionListener` interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows our implementation of the `execute()` function of
    the `Tasklet` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First, we list the files from the landing zone (the read directory path) and
    if the list of files is not empty, we get the first file and move it to the destination
    path. Here, we create the destination path by appending the filename and file
    separator to the processing directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the file is successfully moved to the destination, we set the value of
    the `filepath` instance variable as the destination path where the file has been
    moved.  We will use this in our implementation of the `afterStep()` method. Now,
    let’s look at the implementation of the `afterStep()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the `afterStep()` method implementation, we store two key-value entries (`filePath`
    and `filePathName`) in `JobExecutionContext` if `filePath` is not null (which
    means at least one file was present in the landing zone during tasklet execution
    and has been successfully moved to the processing zone by the tasklet).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see the full code for the `FileMoveToProcessTasklet` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The source code of `FileMoveToProcessConfiguration` will be very similar to
    `FileCheckConfiguration`, which we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code of `FileMoveToProcessConfiguration` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will learn how to develop the `processFile` step. This is an important
    step as all the transformation and ingestion happens here. This step follows a
    typical `SpringBatch` step template, where there is an `ItemReader`, `ItemProcessor`,
    and `ItemWriter`. They are stitched together to form the step pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s look at the source code for building the step pipeline in the
    `processFile()` method of the `ProcessFileConfiguration` configuration class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are building the step from an `ItemReader` bean called `csvRecordReader`,
    which reads the records from the CSV file and returns a set of `EventLogODL` POJO
    objects. An `ItemProcessor` bean called `deviceEventProcessor`, which reads each
    `EventLogODL` POJO and transforms them into `DeviceEventLogFact` POJOs, and an
    `ItemWriter` bean called `jdbcWriter`, which reads each record as a `DeviceEventLogFact`
    POJO and persists them in the PostgreSQL data warehouse. We also mention `chunk`
    while building the pipeline while using `chunkSize` as a configurable parameter
    (for learning purposes, we will test with a `chunkSize` of `1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we explain how to develop the `ItemReader` bean, let’s look at the source
    code of the `EventLogODL` POJO class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the method that creates the `ItemReader` bean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using Spring Batch’s inbuilt `FlatFileItemReader` to read the CSV
    source file. Since we need to dynamically read `filePathName` from `jobExecutionContext`,
    which we set in the previous step, we used the `@SetScope` annotation, which changes
    the default bean scope from a singleton to a step-specific object. This annotation
    is especially helpful for late binding where we want to read some parameters dynamically
    from `JobExecutionContext` or `StepExecutionContext`. Also, we are creating a
    delimited tokenizer with `fieldNames` and a `BeanWrapperFieldSetMapper` to map
    each record to the `EventLogODL` POJO and set the corresponding properties of
    the `FlatfileItemReader` instance.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting tips
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, all data is perfect, and our job should run fine every time.
    But we don’t live in an ideal world. What happens if the data is corrupted? What
    happens if a few records in the file are not following the proper schema? How
    do we handle such situations?
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no simple answer. However, Spring Batch gives you few capabilities
    to handle failures. If the file itself is corrupted and not readable or the file
    doesn’t have proper read and write permissions, then it will go to a `faultTolerance`
    during building the step. This can either be done by using the `skipLimit()`,
    `skip()`, and `noSkip()` methods or using a custom `SkipPolicy`. In our example,
    we can add a fault tolerance to the `processFile` method of the `ProcessFileConfiguration`
    class and skip certain kinds of exceptions while ensuring few other types of exceptions
    cause a step failure. An example is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`return stepBuilderFactory.get("processFile")`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        .<EventLogODL, DeviceEventLogFact>chunk(chunkSize)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        .reader(csvRecordReader)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`.faultTolerant().skipLimit(20).skip(SAXException.class).noSkip(AccessDeniedException.class)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        .processor(deviceEventProcessor)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        .writer(jdbcWriter)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`        .build();`'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can add fault tolerance by chaining the `faultTolerant()`
    method in `stepBuilderFactory.build()`. Then, we can chain the `skip()` method
    so that it skips 20 errors of the `SAXException` type and use the `noSkip()` method
    to ensure `AccessDeniedException` will always cause a **Step Failure**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how we can develop our custom `ItemProcessor`. The source code
    of the custom `ItemProcessor`, called `DeviceEventProcessor`, is shown in the
    following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have implemented the `ItemProcessor` interface, where we
    need to implement the `process()` method. To convert the `EventLogODL` POJO into
    a `DeviceEventLogFact` POJO, we have created a delegate component called `DeviceEventLogMapper`.
    The source code of `DeviceEventLogMapper` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Since we are developing the code for the fact table, we need various primary
    keys to be fetched from different dimension tables to populate our `DeviceEventLogFact`
    POJO. Here, we are dynamically creating a query by using `jdbcTemplate` to fetch
    the dimension primary keys from the data warehouse and populating the `DeviceEventLogFact`
    POJO from its `resultset`. The complete source code for `DeviceEventLogMapper`
    is available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SpringBatchApp/EtlDatawarehouse/src/main/java/com/scalabledataarchitecture/etl/steps/DeviceEventLogMapper.java](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter04/SpringBatchApp/EtlDatawarehouse/src/main/java/com/scalabledataarchitecture/etl/steps/DeviceEventLogMapper.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will create an `ItemWriter` called `jdbcwriter` in the `ProcessFileConfiguration`
    class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, our source code for the `ProcessFileConfiguration` class looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have built our code, we will configure our properties in the `application.yaml`
    file, which is present in the `resource` folder, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the `.yaml` file, we have to mention the `spring.datasource` properties
    so that Spring JDBC can automatically auto-wire the data source component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final code structure will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Project structure of the code ](img/B17084_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Project structure of the code
  prefs: []
  type: TYPE_NORMAL
- en: We can run and test our program by running our Spring Boot application from
    our favorite IDE by running the `Main` class – that is, `EtlDataWarehouseApplication`.
    We must install Postgres, create the schema and the database tables, and populate
    all the dimension tables before we run our Spring Batch application here. Detailed
    run instructions can be found in this book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have run our application and placed our data in the landing zone, it
    gets ingested into our data warehouse fact table, and the CSV files get moved
    to the archival zone, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Data ingested in the fact table after the Spring Batch job
    runs ](img/B17084_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Data ingested in the fact table after the Spring Batch job runs
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the batch-related tables, which contain various run statistics,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Batch job execution log ](img/B17084_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Batch job execution log
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows the batch execution log for the different batch
    jobs that have run. We can learn more about a specific job by looking at the batch
    step execution log, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Step execution log for the batch jobs ](img/B17084_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Step execution log for the batch jobs
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have analyzed, architected, designed, developed, and tested a
    batch-based ETL data ingestion pipeline successfully. As mentioned in the *Technical
    requirements* section, the detailed source code is available in this book’s GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to analyze a data engineering requirement from
    scratch, draw a definite conclusion, and extract facts that will help us in our
    architectural decision-making process. Next, we learned how to profile source
    data and how such an analysis helps us build better data engineering solutions.
    Going further, we used facts, requirements, and our analysis to build a robust
    and effective architecture for a batch-based data engineering problem with a low
    or medium volume of data. Finally, we mapped the design to build an effective
    ETL batch-based data ingestion pipeline using Spring Batch and test it. Along
    the way, you learned how to analyze a data engineering problem from scratch and
    how to build similar pipelines effectively for when you are presented with a similar
    problem next time around.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully architected and developed a batch-based solution
    for medium- and low-volume data engineering problems, in the next chapter, we
    will learn how to build an effective data engineering solution for dealing with
    huge data volumes. In the next chapter, we will discuss an interesting use case
    for building an effective batch-based big data solution.
  prefs: []
  type: TYPE_NORMAL
