- en: '*Chapter 9*: Distributed Logging, Tracing, and Monitoring'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A microservice application often runs multiple microservices on a varied range
    of multiple hosts. For upstream consumers, the API gateway provides a one-stop-shop
    interface to access all the application endpoints. Any request to the API gateway
    is dispersed to one or more microservices. This distributed diffusion of request
    handling escalates challenges in maintaining microservices-based applications.
    If any anomaly or error occurs, it is hard to dig which microservice or distributed
    component is at fault. In addition, any effective microservices implementation
    must handle the maintenance challenges proactively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed logging**: How we can implement the log aggregation for distributed
    microservices so that application logs can be accessed and indexed in one place?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed tracing**: How we can trace the execution of a user request that
    could be dispersed onto multiple microservices running on multiple host environments?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed monitoring**: How we can continuously monitor the key performance
    indicators for all the service components to get a holistic picture of the system''s
    health?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By collecting these three different kinds of data – logging, tracing, and monitoring
    – we enhance the system observability. By accessing this telemetry data at any
    point in time, we can intuitively and precisely get a complete context of what
    and how a request was executed in the system. To learn more about observability,
    we will explore the microservices patterns on distributed logging, tracing, and
    monitoring with the hands-on `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have good knowledge of implementing these
    observability patterns in the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the commands and technical instructions in this chapter are run on Windows
    10 and macOS. Code examples covered in this chapter are available in the book's
    GitHub repository at [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tools need to be installed and set up in the development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java SDK**: Version 13 or above (we used Java 14).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maven**: This is optional and only required if you would like to use Maven
    as the build system. However, we recommend having Maven set up on any development
    machine. Instructions to download and install Maven can be found at [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development IDE**: Based on your preferences, any Java-based IDE can be used,
    but for the purpose of writing this chapter, IntelliJ was used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git**: Instructions to download and install Git can be found at [https://git-scm.com/downloads](https://git-scm.com/downloads).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PostgreSQL**: Instructions to download and install PostgreSQL can be found
    at [https://www.postgresql.org/download/](https://www.postgresql.org/download/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDB**: MongoDB Atlas provides a free online database-as-a-service with
    up to 512 MB storage. However, if a local database is preferred, then instructions
    to download and install can be found at [https://docs.mongodb.com/manual/administration/install-community/](https://docs.mongodb.com/manual/administration/install-community/).
    We used a local installation for writing this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**REST client**: Any HTTP REST client can be used. We used the Advanced REST
    Client Chrome plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker**: Instructions to download and install Docker can be found at https://docs.docker.com/get-docker/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed logging in Micronaut microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in the chapter introduction, in a microservices-based application,
    a user request is executed on multiple microservices running on different host
    environments. Therefore, the log messages are spread across multiple host machines.
    This brings a unique challenge to a developer or admin maintaining the application.
    If there's a failure, then it will be hard to zero down on the issue as you have
    to sign into multiple host machines/environments, grep the logs, and put them
    together to make sense.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will dive into log aggregation for distributed logging in
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log aggregation, as the name suggests, is combining the logs produced by various
    microservices and components in the application. Log aggregation typically involves
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log producer**: This is any microservice or a distributed component that''s
    producing logs while executing the control flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log dispatcher**: The log dispatcher is responsible for collecting the logs
    produced by the log producer and dispatching them to the centralized storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log storage**: The log storage persists and indexes the logs produced by
    all the application components and microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log visualizer**: The log visualizer provides a user interface for accessing,
    searching, and filtering the logs stored in the log storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the `pet-clinic` application context, we will implement the **ELK** (short
    for **Elasticsearch**, **Logstash**, **Kibana**) Stack for distributed logging.
    Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Distributed logging using the ELK Stack'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.1_B16585_Fixed_edited.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Distributed logging using the ELK Stack
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the ELK Stack is used to implement distributed logging
    in the `pet-clinic` application. **Logstash** dispatches the logs into the **Elasticsearch**
    engine, which is then used by **Kibana** to provide a user interface.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how we can set up an ELK Stack in a dockerized
    container.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up ELK in Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To set up ELK in Docker, follow these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Check out `docker-elk` from [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-elk](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open any Bash terminal (we used Git Bash).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change directory to where you have checked out `docker-elk`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `docker compose up –d` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for Docker to download the images and instantiate the ELK container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding instructions will boot up an ELK app in Docker. You can verify
    the installation by going to the Docker Dashboard | **Containers / Apps**, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Verifying ELK in the Docker Dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.2_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Verifying ELK in the Docker Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can verify ELK instantiation. By default, Elasticsearch runs on port
    `9200`, Logstash on `5000`, and Kibana on port `5601`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will modify our `pet-clinic` microservice to dispatch
    the logs to the Logstash instance.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Logstash with Micronaut microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To integrate Logstash into the `pet-clinic` microservice, we will leverage Logback.
    We will introduce a new appender to Logback that can dispatch the logs to the
    previously created Logstash instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the locally checked out `docker-elk` directory, you can verify that Logstash
    is configured with the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In `logstash.config`, we have the following three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input`: Logstash has the power to aggregate more than 50 different kinds of
    log sources. `input` configures Logstash for one or more input sources. In our
    configuration, we are enabling `tcp` input on port `5000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter`: Logstash''s `filter` provides an easy way to transform the incoming
    logs into filter-defined log events. These events are then pushed to log storage.
    In the preceding configuration, we are using a `grok` filter along with `mutate`
    to add extra information (`app_name` and `app_port`) to the log events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output`: The `output` section configures the receiving sources so that Logstash
    can push the log events to the configured output sources. In the preceding configuration,
    we are configuring the standard output and Elasticsearch to receive the produced
    log events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have booted up an ELK Docker instance with Logstash configured to
    receive, transform, and send log events into Elasticsearch. Next, we will make
    required amends in the `pet-clinic` microservices so that logs can be shipped
    to Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring microservices for distributed logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to make the `pet-clinic` microservice aggregate and ship logs to Logstash,
    we need to add the following `logstash-logback-encoder` dependency to all the
    microservice `pom.xml` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: By importing `logstash-logback-encoder`, we can leverage the `net.logstash.logback.appender.LogstashTcpSocketAppender`
    class in `logback.xml`. This class provides the `logstash` appender, which can
    ship logs to the Logstash server from the microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify `logback.xml` for all microservices by adding the Logstash appender
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Logstash appender will help in shipping the logs to `localhost:5000` and
    as we are running Logstash in a Docker container, we provide the address as `host.docker.internal`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we need to add the appender to the root level by using `appender-ref`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we need to define two properties for `app_name` and `app_port`.
    These are the filter configurations that will be used by Logstash to create the
    desired log events with app information. This is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we have added the required properties for the
    `pet-owner` microservice. We need to add similar properties in all the services
    so Logstash can generate service-specific log events.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the distributed logging in the pet-clinic application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To verify that Logstash is receiving the logs from all the microservices in
    the `pet-clinic` application, we would need to re-build the Docker images and
    redeploy the `pet-clinic` application. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal in the `pet-owner` microservice root directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the terminal in the `pet-clinic` microservice root directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the terminal in the `pet-clinic-reviews` microservice root directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the terminal in the `pet-clinic-concierge` microservice root directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open any Bash terminal and change the directory to where you have checked out
    the `pet-clinic` `docker-compose.yml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Run `docker compose up –d`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Wait for Docker to finish booting up the `pet-clinic` stack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the `pet-clinic` application is instantiated and running in Docker, we
    need to configure Kibana to index and show the logs. To index logs in Kibana,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Kibana at `http://localhost:5601` and log in using Elasticsearch
    credentials as mentioned in the `.env` file in the `docker-elk` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the home page and click on the **Connect to your Elasticsearch** **index**
    hyperlink. After clicking on **Connect to your Elasticsearch** **index**, Kibana
    will provide a setup page to connect your index (see the following screenshot):![Figure
    9.3 – Connecting the Elasticsearch index in Kibana
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_9.3_B16585_Fixed.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.3 – Connecting the Elasticsearch index in Kibana
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Kibana provides an intuitive user interface to connect with your Elasticsearch
    index. Click on the highlighted portion in the screenshot and follow the steps
    as presented by Kibana.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the setup page loads, enter `logstash` in the **Index Patterns** textbox.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Next step** button and select **@timestamp** in the configure
    settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click on **Create index pattern**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a successful index connection, you can go to the **Discover** page and
    view the application logs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Viewing the application logs in Discover'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.4_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Viewing the application logs in Discover
  prefs: []
  type: TYPE_NORMAL
- en: On the `app_name` and `app_port`, we can drill down on both the parameters to
    see the logs to a specific microservice.
  prefs: []
  type: TYPE_NORMAL
- en: So now, we have implemented an ELK Stack distributed logging that intuitively
    provides a common place to live access the microservices' logs. If there's any
    fault in any microservice, you can directly access Kibana and view/search the
    logs. As you add more microservice instances and components to your runtime topology,
    ELK will simplify the log management.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into distributed tracing and how we can implement
    distributed tracing in the `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing in Micronaut microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed tracing is the capability of the system to track and observe the
    execution flow of a request in distributed systems by collecting data as the request
    furthers from one service component to another. This trace data compiles metrics
    such as the time taken at each service along with end-to-end execution flow. Time
    metrics can help to zero down performance issues such as which service component
    is a bottleneck to the execution flow and why.
  prefs: []
  type: TYPE_NORMAL
- en: 'A trace is a Gantt chart-like data structure that stores the trace information
    in spans. Each span will keep a trace for the execution flow in a particular service
    component. Furthermore, a span can have a reference to parent span and child spans.
    Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Distributed tracing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.5_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Distributed tracing
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see the traces/spans for loading the `foo`
    page on the user interface app. It first calls `foo` object, which in turn calls
    `Bars` and `Bazs` for `foo`, respectively. The time taken for the whole execution
    will be the cumulative total of execution times at various service components.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will implement a distributed tracing solution in the
    `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing distributed tracing in Micronaut
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to get hands-on with distributed tracing in Micronaut, we will implement
    Zipkin-based tracing in the `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will run a Zipkin instance in Docker. To run Zipkin in Docker, perform the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open any Bash terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `docker run -d -p 9411:9411 openzipkin/zipkin` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for Docker to download and instantiate Zipkin on port `9411`. After successful
    instantiation, you can verify Zipkin by accessing `http://localhost:9411/zipkin`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will begin with the `pet-clinic-concierge` service, which is the API
    gateway. Add the following dependencies to the `pet-clinic-concierge` POM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By importing the preceding dependencies, we can leverage Micronaut as well as
    third-party code artifacts for distributed tracing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To enable the distributed tracing, we also need to amend `application.properties`.
    Add the following properties related to Zipkin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding application properties for Zipkin are added at the root level.
    In `url`, we specified a Docker instance of Zipkin running on localhost. Furthermore,
    in `sampler.probability`, we specify the value as `1` that will enable the tracing
    for all the user requests. This probability can be reduced to any value between
    0 and 1 wherein 0 means never sample and 1 means sample every request.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to tag the controller methods for spans. For managing the spans,
    we have the following two tags in Micronaut:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a. `@NewSpan`: This will create a new span beginning from the method it''s
    tagged on.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. `@ContinueSpan`: This will continue the previous span.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since all the client controllers in `pet-clinic-concierge` are the interfacing
    points to any upstream consumers, we will use `@NewSpan` on these methods so that
    a new trace can begin. The following are the span-related changes in `OwnerResourceClientController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Similar changes to annotate client controller methods should be made in all
    the other clients for the `pet-owner`, `pet-clinic`, and `pet-clinic-reviews`
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to modify the `pet-clinic` microservice for distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the pet-clinic microservice for distributed tracing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuing with distributed tracing changes, we need to make the required amends
    in the `pet-owner`, `pet-clinic`, and `pet-clinic-reviews` microservices project
    POM and application properties as explained in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, to continue the tracing, we need to annotate controller methods
    with `@ContinueSpan` tags. Refer to the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`@ContinueSpan` must be annotated on all the controller methods in all the
    microservices (excluding `pet-clinic-concierge`, which is an API gateway). `@ContinueSpan`
    will continue the span/trace from the previous span/trace. In `pet-clinic-concierge`,
    we annotate the `createOwner()` method with `@NewSpan`, and in the `pet-owner`
    microservice, we use `@ContinueSpan`. Using these tags in tandem will trace the
    end-to-end execution flow.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will verify the end-to-end trace for an HTTP request
    in the `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the distributed tracing in the pet-clinic application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To verify the distributed tracing in the `pet-clinic` application, you must
    have the `pet-clinic` microservice running. We will fetch a list of owners via
    the API gateway. For this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to `http://localhost:32584/api/owners` in any browser tab or REST client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to Zipkin to verify the trace for the preceding HTTP `GET` call at
    `http://localhost:9411/zipkin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Run Query** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the `get /api/owners` request in the returned results and click **Show**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After successfully performing these steps, you will see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – GET owners distributed tracing in Zipkin'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.6_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – GET owners distributed tracing in Zipkin
  prefs: []
  type: TYPE_NORMAL
- en: Zipkin provides an intuitive user interface for accessing the request execution
    traces. You can see at first the request reaches `pet-clinic-concierge`, which
    is further passed on to the `pet-owner` microservice. In total it took approximately
    948 ms to complete the request with the majority of the time spent on the `pet-owner`
    microservice.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will focus on distributed monitoring and how to implement
    distributed monitoring in the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed monitoring in Micronaut microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring is simply recording the key performance metrics to enhance visibility
    into the application state. By recording and surfacing the system performance
    metrics such as CPU usage, thread pools, memory usage, and database connections
    for all the distributed components, it can provide a holistic picture of how a
    microservice system is performing at a given point in time. The distributed nature
    of microservices requires a shift in how the system is monitored. Instead of relying
    on the host environment monitoring tools, we need a unified monitoring solution
    that can combine performance metrics from various services and present a one-stop
    interface. In this section, we will explore how to implement such a distributed
    monitoring solution for the `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement distributed monitoring, we will use a very popular stack of Prometheus
    and Grafana. Let''s look at our system components for distributed monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_9.7_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Distributed monitoring using Prometheus and Grafana
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, the `pet-clinic` microservice will be communicating
    the metrics to the **Prometheus** server and **Grafana** will get the metrics
    to present the user interface. Prometheus configurations will be stored in a YAML
    file.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will begin with setting up Prometheus and Grafana in
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Prometheus and Grafana in Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we instantiate Prometheus and Grafana in Docker, we need to define configurations
    for Prometheus so that it can pull required metrics from the `pet-clinic` microservice.
    You can check out `docker-prometheus docker-compose` and `prometheus.yml` from
    [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once checked out locally, you can review the `prometheus.yml` file to be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In `prometheus.yml`, we mainly need to configure `scrape_configs`. This will
    be responsible for invoking the microservice endpoints to get the metrics. We
    can specify the `pet-clinic` microservice in the targets. In addition, you can
    note that the scrape interval is `10` seconds. This will configure Prometheus
    to fetch metrics every 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's set up our distributed monitoring stack in Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up Prometheus and Grafana in Docker, follow these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Check out `docker-prometheus` from [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open any Bash terminal (we used Git Bash).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change directory to where you have checked out `docker-prometheus`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `docker compose up –d`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for Docker to download the images and instantiate the Prometheus app container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These instructions will boot up the monitoring app in Docker. You can verify
    the installation by going to the Docker Dashboard and **Containers / Apps**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how we can integrate the `pet-clinic` microservice
    into Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring microservices for distributed monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To configure the `pet-clinic` microservice for distributed monitoring, we need
    to update the `project` POM with `Micrometer` dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following dependencies to the `pet-owner` project POM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By importing the `micronaut-micrometer` dependencies, we can leverage a distributed
    monitoring toolkit in the `pet-owner` microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'To expose service metrics for Prometheus, we need to expose the `metrics` endpoint
    in all the `pet-clinic` microservices. We will add a new controller called `PrometheusController`
    to the `com.packtpub.micronaut.web.rest.commons` package as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`PrometheusController` will expose `prometheusMeterRegistry.scrape()` on the
    `/metrics` endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: '`prometheusMeterRegistry.scrape()` will provide service performance metrics
    as configured in the `application.properties` file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to configure the `application.properties` file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In `application.properties`, we are enabling the metrics and exporting the metrics
    in Prometheus format. Furthermore, since we are providing our custom `/metrics`
    endpoint, we are disabling the `metrics` and `prometheus` endpoints in the application
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we need to modify the project POM, add `PrometheusController`, and
    update the application properties for the `pet-clinic`, `pet-clinic-reviews`,
    and `pet-clinic-concierge` microservices. Afterward, we need to rebuild the Docker
    images for all service projects running the `mvn compile jib:dockerBuild` command
    in the terminal. Once the Docker images are built and uploaded to the local Docker
    repository, we need to decommission the old `pet-clinic` application in Docker
    and rerun `docker compose up –d` to re-instantiate the modified `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will verify the distributed monitoring implementation
    in the `pet-clinic` application.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the distributed monitoring in the pet-clinic application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To verify the distributed monitoring in the `pet-clinic` application, you must
    have the `pet-clinic` and Prometheus applications running in Docker. You need
    to follow these instructions to verify integration between Prometheus and the
    `pet-clinic` application:'
  prefs: []
  type: TYPE_NORMAL
- en: Access the `/metrics` endpoints for all the microservices to verify that services
    are exposing metrics to Prometheus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the `pet-owner` metrics by accessing `http://localhost:32581/metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the `pet-clinic` metrics by accessing `http://localhost:32582/metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the `pet-clinic-reviews` metrics by accessing `http://localhost:32583/metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify the `pet-clinic-concierge` metrics by accessing `http://localhost:32584/metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `http://localhost:9090/graph` and check whether you can see the
    `system_cpu_usage` metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After successful completion of the preceding steps, you will see the following
    screen:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Accessing the system CPU usage graph for the pet-clinic application
    in Prometheus'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.8_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Accessing the system CPU usage graph for the pet-clinic application
    in Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can verify that the `pet-clinic` microservice
    is able to expose the performance metrics on the endpoint and Prometheus can invoke
    the `/metrics` endpoints. We can see the system CPU usage graph in Prometheus
    graphs but as a system admin or developer, you probably need a system dashboard
    with all the metric graphs in one place.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following instructions, we will integrate Grafana with the Prometheus
    server:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `http://localhost:3000/` and log in with the username as `admin`
    and password as `pass`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After logging in, navigate to `http://localhost:3000/datasources`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Add data source** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **Time series databases** list, select **Prometheus**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `http://prometheus:9090`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the rest of the values as the defaults.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Save and test** button. You should get a successful message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the adjacent **Dashboards** tab and click on **Prometheus 2.0 stats**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After successful completion of these steps, you should see the following dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Prometheus dashboard in Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.9_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Prometheus dashboard in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, Grafana provides a very intuitive, unified
    dashboard for accessing the vital system metrics for all the service components
    in the `pet-clinic` application. One-stop access to this telemetry data is very
    handy in addressing any performance issues and system failures in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored what distributed monitoring is and how we can implement
    distributed monitoring using Prometheus and Grafana in the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began with distributed logging and why it is important in
    any microservice implementation. We implemented an ELK Stack for distributed logging
    in the `pet-clinic` application. Furthermore, we dived into using the Kibana user
    interface for connecting to the Elasticsearch application logs index.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we explored what distributed tracing is and how to implement distributed
    tracing using Zipkin in the Micronaut framework. We also verified the trace of
    an HTTP call in the Zipkin user interface.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we dived into the world of distributed monitoring and implemented
    a distributed monitoring solution for the `pet-clinic` application using a Prometheus
    and Grafana stack.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter enhanced your Micronaut microservices journey with the observability
    patterns that are distributed logging, distributed tracing, and distributed monitoring
    by enabling you with hands-on knowledge on how to implement these patterns in
    the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to implement an IoT solution in the
    Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is distributed logging in microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you run an ELK Stack in Docker?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you implement distributed logging in the Micronaut framework?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you connect to a Docker Logstash from the Micronaut microservice?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is distributed tracing in microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you implement distributed tracing in the Micronaut framework?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is distributed monitoring in microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you run a Prometheus and Grafana stack in Docker?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you implement distributed monitoring in the Micronaut framework?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
