<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Concurrency"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Concurrency</h1></div></div></div><p>Concurrency was one of the chief design goals of Clojure. Considering the concurrent programming <a class="indexterm" id="id306"/>model in Java (the comparison with Java is due to it being the predominant language on the JVM), it is not only too low level, but rather tricky to get right that without strictly following the patterns, one is more likely to shoot oneself in the foot. Locks, synchronization, and unguarded mutation are recipes for the concurrency pitfalls, unless exercised with extreme caution. Clojure's design choices deeply influence the way in which the concurrency patterns can be achieved in a safe and functional manner. In this chapter, we will discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The low level concurrency support at the hardware and JVM level</li><li class="listitem" style="list-style-type: disc">The concurrency primitives of Clojure—atoms, agents, refs and vars</li><li class="listitem" style="list-style-type: disc">The built-in concurrency that features in Java safe, and its usefulness with Clojure</li><li class="listitem" style="list-style-type: disc">Parallelization with the Clojure features and reducers</li></ul></div><div class="section" title="Low-level concurrency"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Low-level concurrency</h1></div></div></div><p>Concurrency<a class="indexterm" id="id307"/> cannot be achieved without explicit hardware support. We discussed about SMT and the multi-core processors in the previous chapters. Recall that every processor core has its own L1 cache, and several cores share the L2 cache. The shared L2 cache provides a fast mechanism to the processor cores to coordinate their cache access, eliminating the comparatively expensive memory access. Additionally, a processor buffers the writes to memory into something known as a <span class="strong"><strong>dirty write-buffer</strong></span>. This <a class="indexterm" id="id308"/>helps the processor to issue a batch of memory update requests, reorder the instructions, and determine the final value to write to memory, known<a class="indexterm" id="id309"/> as <span class="strong"><strong>write absorption</strong></span>.</p><div class="section" title="Hardware memory barrier (fence) instructions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec53"/>Hardware memory barrier (fence) instructions</h2></div></div></div><p>Memory access reordering is great for a sequential (single-threaded) program performance, but it is hazardous for the concurrent programs where the order of memory access in one thread may disrupt the expectations in another thread. The processor needs the means of synchronizing the access in such a way that memory reordering is either compartmentalized in code segments that do not care, or is prevented where it might have undesirable consequences. The hardware supports such a safety measure in terms of a "memory barrier" (also known as "fence").</p><p>There<a class="indexterm" id="id310"/> are several kinds of memory barrier instructions found in different architectures, with potentially different performance characteristics. The compiler (or the JIT compiler in the case of the JVM) usually knows about the fence instructions on the architectures that it runs on. The common fence instructions are read, write, acquire, and release barrier, and more. The barriers do not guarantee the latest data, rather they only control the relative ordering of memory access. Barriers cause the write-buffer to be flushed after all the writes are issued, before the barrier is visible to the processor that issued it.</p><p>Read and write barriers control the order of reads and writes respectively. Writes happen via a write-buffer; but reads may happen out of order, or from the write-buffer. To guarantee the correct ordering, acquire, and release, blocks/barriers are used. Acquire and release are considered "half barriers"; both of them together (acquire and release) form a "full barrier". A full barrier is more expensive than a half barrier.</p></div><div class="section" title="Java support and the Clojure equivalent"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/>Java support and the Clojure equivalent</h2></div></div></div><p>In Java, the <a class="indexterm" id="id311"/>memory barrier instructions are inserted by<a class="indexterm" id="id312"/> the higher level coordination primitives. Even though fence instructions are expensive (hundreds of cycles) to run, they provide a safety net that makes accessing shared variables safe within the critical sections. In Java, the <code class="literal">synchronized</code> keyword marks a "critical section", which can be executed by only one thread at a time, thus making is a tool for "mutual exclusion". In Clojure, the equivalent of Java's <code class="literal">synchronized</code> is the <code class="literal">locking</code> macro:</p><div class="informalexample"><pre class="programlisting">// Java example
synchronized (someObject) {
    // do something
}
;; Clojure example
(locking some-object
  ;; do something
  )
  </pre></div><p>The <code class="literal">locking</code> macro builds upon two special forms, <code class="literal">monitor-enter</code> and <code class="literal">monitor-exit</code>. Note that the <code class="literal">locking</code> macro is a low-level and imperative solution just like Java's <code class="literal">synchronized</code> – their use is not considered idiomatic Clojure. The special forms <code class="literal">monitor-enter</code> and <code class="literal">monitor-exit</code> respectively enter and exit the lock object's "monitor" – they are even lower level and not recommended for direct use.</p><p>Someone<a class="indexterm" id="id313"/> measuring the performance of the code that uses<a class="indexterm" id="id314"/> such locking should be aware of its single-threaded versus the multi-threaded latencies. Locking in a single thread is cheap. However, the performance penalty starts kicking in when there are two or more threads contending for a lock on the same object monitor. A lock is acquired on the monitor of an object called the "intrinsic" or "monitor" lock. Object equivalence (that is, when the <code class="literal">=</code> function returns as true) is never used for the purpose of locking. Make sure that the object references are the same (that is, when <code class="literal">identical?</code> returns as true) when locking from different threads.</p><p>Acquiring a monitor lock by a thread entails a read barrier, which invalidates the thread-local cached data, the corresponding processor registers, and the cache lines. This forces a reread from the memory. On the other hand, releasing the monitor lock results in a write barrier, which flushes all the changes to memory. These are expensive operations that impact parallelism, but they ensure consistency of data for all threads.</p><p>Java supports a <code class="literal">volatile</code> keyword for the data members in a class that guarantees read and write to an attribute outside of a synchronized block that would not be reordered. It is interesting to note that unless an attribute is declared <code class="literal">volatile</code>, it is not guaranteed to be visible in all the threads that are accessing it. The Clojure equivalent of Java's <code class="literal">volatile</code> is the metadata called <code class="literal">^:volatile-mutable</code> that we discussed in <a class="link" href="ch03.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span>. An example of <code class="literal">volatile</code> in Java and Clojure is as follows:</p><div class="informalexample"><pre class="programlisting">// Java example
public class Person {
    volatile long age;
}
;; Clojure example
(deftype Person [^:volatile-mutable ^long age])</pre></div><p>Reading and writing a <code class="literal">volatile</code> data requires read-acquire or write-release respectively, which means we need only a half-barrier to individually read or write the value. Note that due to a half-barrier, the read-followed-by-write operations are not guaranteed to be atomic. For example, the <code class="literal">age++</code> expression first reads the value, then increments and sets it. This makes two memory operations, which is no more a half-barrier.</p><p>Clojure 1.7 introduced a first class support for the volatile data using a new set of functions: <code class="literal">volatile!</code>, <code class="literal">vswap!</code>, <code class="literal">vreset!,</code> and <code class="literal">volatile?</code> These functions define volatile (mutable) data and work with that. However, make a note that these functions do not work with the volatile fields in <code class="literal">deftype</code>. You can see how to use them as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a (volatile! 10))
#'user/a
user=&gt; (vswap! a inc)
11
user=&gt; @a
11
user=&gt; (vreset! a 20)
20
user=&gt; (volatile? a)
true</pre></div><p>Operations <a class="indexterm" id="id315"/>on volatile data are not atomic, which is <a class="indexterm" id="id316"/>why even creating a volatile (using <code class="literal">volatile!</code>) is considered potentially unsafe. In general, volatiles may be useful where read consistency is not a high priority but writes must be fast, such as real-time trend analysis, or other such analytics reporting. Volatiles may also be very useful when writing stateful transducers (refer to <a class="link" href="ch02.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span>), serving as very fast state containers. In the next sub-section, we will see the other state abstractions that are safer (and mostly slower) than volatiles.</p></div></div></div>
<div class="section" title="Atomic updates and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Atomic updates and state</h1></div></div></div><p>It is a <a class="indexterm" id="id317"/>common use case to read a data element, execute some logic, and <a class="indexterm" id="id318"/>update with a new value. For single-threaded programs, it bears no consequences; but for concurrent scenarios, the entire operation must be carried out in a lockstep, as an atomic operation. This case is so common that many processors support this at the hardware level using a special Compare-and-swap (CAS) instruction, which is much cheaper than locking. On x86/x64 architectures, the instruction is called CompareExchange (CMPXCHG).</p><p>Unfortunately, it is possible that another thread updates the variable with the same value that the thread, which is working on the atomic update, is going to compare the old value against. This is known as the "ABA" problem. The set of instructions such as "Load-linked" (LL) and "Store-conditional" (SC), which are found in some other architectures, provide an alternative to CAS without the ABA problem. After the LL instruction reads the value from an address, the SC instruction to update the address with a new value will only go through if the address has not been updated since the LL instruction was successful.</p><div class="section" title="Atomic updates in Java"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Atomic updates in Java</h2></div></div></div><p>Java has a<a class="indexterm" id="id319"/> bunch of built-in lock free, atomic, thread safe compare-and-swap abstractions for the state management. They live in the <code class="literal">java.util.concurrent.atomic</code> package. For primitive types, such as boolean, integer, and long, there are the <code class="literal">AtomicBoolean</code>, <code class="literal">AtomicInteger</code>, and <code class="literal">AtomicLong</code> classes respectively. The latter two classes support additional atomic add/subtract operations. For atomic reference updates, there are the <code class="literal">AtomicReference</code>, <code class="literal">AtomicMarkableReference</code>, and <code class="literal">AtomicStampedReference</code> classes for the arbitrary objects. There is also a support available for arrays where the array elements can be updated atomically—<code class="literal">AtomicIntegerArray</code>, <code class="literal">AtomicLongArray</code>, and <code class="literal">AtomicReferenceArray</code>. They are easy to use; here is the example:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.atomic.AtomicReference)
(def ^AtomicReference x (AtomicReference. "foo"))
(.compareAndSet x "foo" "bar")
(import 'java.util.concurrent.atomic.AtomicInteger)
(def ^AtomicInteger y (AtomicInteger. 10))
(.getAndAdd y 5)</pre></div><p>However, where<a class="indexterm" id="id320"/> and how to use it is subjected to the update points and the logic in the code. The atomic updates are not guaranteed to be non-blocking. Atomic updates are not a substitute to locking in Java, but rather a convenience, only when the scope is limited to a compare and swap operation for one mutable variable.</p></div><div class="section" title="Clojure's support for atomic updates"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Clojure's support for atomic updates</h2></div></div></div><p>Clojure's atomic <a class="indexterm" id="id321"/>update abstraction is called "atom". It uses <code class="literal">AtomicReference</code> under the hood. An operation on <code class="literal">AtomicInteger</code> or <code class="literal">AtomicLong</code> may be slightly faster than on the Clojure <code class="literal">atom</code>, because the former uses primitives. But neither of them are too cheap, due to the compare-and-swap instruction that they use in the CPU. The speed really depends on how frequently the mutation happens, and how the JIT compiler optimizes the code. The benefit of speed may not show up until the code is run several hundred thousand times, and having an atom mutated very frequently will increase the latency due to the retries. Measuring the latency under actual (or similar to actual) load can tell better. An example of using an atom is as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a (atom 0))
#'user/a
user=&gt; (swap! a inc)
1
user=&gt; @a
1
user=&gt; (compare-and-set! a 1 5)
true
user=&gt; (reset! a 20)
20</pre></div><p>The <code class="literal">swap!</code> function provides a notably different style of carrying out atomic updates than the <code class="literal">compareAndSwap(oldval, newval)</code> methods. While <code class="literal">compareAndSwap()</code> compares and sets the value, returning true if it's a success and false if it's a failure, <code class="literal">swap!</code> keeps on trying to update in an endless loop until it succeeds. This style is a popular pattern that is followed among Java developers. However, there is also a potential pitfall associated with the update-in-loop style. As the concurrency of the updaters gets higher, the performance of the update may gradually degrade. Then again, high concurrency on the atomic updates raises a question of whether or not uncoordinated updates was a good idea at all for the use-case. The <code class="literal">compare-and-set!</code> and <code class="literal">reset!</code> are pretty straightforward.</p><p>The function<a class="indexterm" id="id322"/> passed to <code class="literal">swap!</code> is required to be pure (as in side effect free), because it is retried several times in a loop during contention. If the function is not pure, the side effect may happen as many times as the retries.</p><p>It is noteworthy that atoms are not "coordinated", which means that when an atom is used concurrently by different threads, we cannot predict the order in which the operations work on it, and we cannot guarantee the end result as a consequence. The code we write around atoms should be designed with this constraint in mind. In many scenarios, atoms may not be a good fit due to the lack of coordination—watch out for that in the program design. Atoms support meta data and basic validation mechanism via extra arguments. The following examples illustrate these features:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a (atom 0 :meta {:foo :bar}))
user=&gt; (meta a)
{:foo :bar}
user=&gt; (def age (atom 0 :validator (fn [x] (if (&gt; x 200) false true))))
user=&gt; (reset! age 200)
200
user=&gt; (swap! age inc)
IllegalStateException Invalid reference state  clojure.lang.ARef.validate (ARef.java:33)</pre></div><p>The second important thing is that atoms support is adding and removing watches on them. We will discuss watches later in the chapter.</p><div class="section" title="Faster writes with atom striping"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec18"/>Faster writes with atom striping</h3></div></div></div><p>We know<a class="indexterm" id="id323"/> that atoms present contention when multiple threads try to update the state at the same time. This implies that atoms have great performance when the writes are infrequent. There are some use cases, for example metrics counters, where the writes need to be fast and frequent, but the reads are fewer and can tolerate some inconsistency. For such use cases, instead of directing all the updates to a single atom, we can maintain a bunch of atoms where each thread updates a different atom, thus reducing contention. Reads from these atoms cannot be guaranteed to be consistent. Let's develop an example of such a counter:</p><div class="informalexample"><pre class="programlisting">(def ^:const n-cpu (.availableProcessors (Runtime/getRuntime)))
(def counters (vec (repeatedly n-cpu #(atom 0))))
(defn inc! []
  ;; consider java.util.concurrent.ThreadLocalRandom in Java 7+
  ;; which is faster than Math/random that rand-int is based on
  (let [i (rand-int n-cpu)]
    (swap! (get counters i) inc)))
(defn value []
  (transduce (map deref) + counters))</pre></div><p>In the previous example, we created a vector called <code class="literal">counters</code> of the same size as the number of CPU cores in the computer, and initialize each element with an atom of initial value 0. The function called <code class="literal">inc!</code> updates the counter by picking up a random atom from <code class="literal">counters</code>, and incrementing the value by 1. We also assumed that <code class="literal">rand-int</code> distributes the picking<a class="indexterm" id="id324"/> up of atom uniformly across all the processor<a class="indexterm" id="id325"/> cores, so that we have almost zero contention. The <code class="literal">value</code> function simply walks over all the atoms and adds up their <code class="literal">deref</code>'ed values to return the counter value. The example uses <code class="literal">clojure.core/rand-int</code>, which depends on <code class="literal">java.lang.Math/random</code> (due to Java 6 support) to randomly find out the next counter atom. Let's see how we can optimize this when using Java 7 or above:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.ThreadLocalRandom)
(defn inc! []
  (let [i (.nextLong (ThreadLocalRandom/current) n-cpu)]
    (swap! (get counters i) inc)))</pre></div><p>Here, we <code class="literal">import</code> the <code class="literal">java.util.concurrent.ThreadLocalRandom</code> class, and define the <code class="literal">inc!</code> function to pick up the next random atom using <code class="literal">ThreadLocalRandom</code>. Everything else remains the same.</p></div></div></div>
<div class="section" title="Asynchronous agents and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Asynchronous agents and state</h1></div></div></div><p>While<a class="indexterm" id="id326"/> atoms are synchronous, agents are the asynchronous <a class="indexterm" id="id327"/>mechanism in Clojure to effect any change in the state. Every agent is associated with a mutable state. We pass a function (known as "action") to an agent with the optional additional arguments. This function gets queued for processing in another thread by the agent. All the agents share two common thread pools—one for the low-latency (potentially CPU-bound, cache-bound, or memory-bound) jobs, and one for the blocking (potentially I/O related or lengthy processing) jobs. Clojure provides the <code class="literal">send</code> function for the low-latency actions, <code class="literal">send-off</code> for blocking actions, and <code class="literal">send-via</code> to have the action executed on the user-specified thread-pool, instead of either of the preconfigured thread pools. All of <code class="literal">send</code>, <code class="literal">send-off</code>, and <code class="literal">send-via</code> return immediately. Here is how we can use them:</p><div class="informalexample"><pre class="programlisting">(def a (agent 0))
;; invoke (inc 0) in another thread and set state of a to result
(send a inc)
@a  ; returns 1
;; invoke (+ 1 2 3) in another thread and set state of a to result
(send a + 2 3)
@a  ; returns 6

(shutdown-agents)  ; shuts down the thread-pools
;; no execution of action anymore, hence no result update either
(send a inc)
@a  ; returns 6</pre></div><p>When we inspect<a class="indexterm" id="id328"/> the Clojure (as of version 1.7.0) source code, we <a class="indexterm" id="id329"/>can find that the thread-pool for the low-latency actions is named as <code class="literal">pooledExecutor</code> (a bounded thread-pool, initialized to max '2 + number of hardware processors' threads), and the thread-pool for the high-latency actions is named as <code class="literal">soloExecutor</code> (an unbounded thread pool). The premise of this default configuration is that the CPU/cache/memory-bound actions run most optimally on a bounded thread-pool, with the default number of threads. The I/O bound tasks do not consume CPU resources. Hence, a relatively larger number of such tasks can execute at the same time, without significantly affecting the performance of the CPU/cache/memory-bound jobs. Here is how you can access and override the thread-pools:</p><div class="informalexample"><pre class="programlisting">(import 'clojure.lang.Agent)
Agent/pooledExecutor  ; thread-pool for low latency actions
Agent/soloExecutor  ; thread-pool for I/O actions
(import 'java.util.concurrent.Executors)
(def a-pool (Executors/newFixedThreadPool 10))  ; thread-pool with 10 threads
(def b-pool (Executors/newFixedThreadPool 100)) ; 100 threads pool
(def a (agent 0))
(send-via a-pool a inc)  ; use 'a-pool' for the action
(set-agent-send-executor! a-pool)  ; override default thread-pool
(set-agent-send-off-executor! b-pool)  ; override default pool</pre></div><p>If a program carries out a large number of I/O or blocking operations through agents, it probably makes sense to limit the number of threads dedicated for such actions. Overriding the <code class="literal">send-off</code> thread-pool using <code class="literal">set-agent-send-off-executor!</code> is the easiest way to limit the thread-pool size. A more granular way to isolate and limit the I/O actions on the agents is to use <code class="literal">send-via</code> with the thread-pools of appropriate sizes for various kinds of I/O and blocking operations.</p><div class="section" title="Asynchrony, queueing, and error handling"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/>Asynchrony, queueing, and error handling</h2></div></div></div><p>Sending <a class="indexterm" id="id330"/>an<a class="indexterm" id="id331"/> action to an agent returns immediately<a class="indexterm" id="id332"/> without blocking. If the agent is not already busy in executing any action, it "reacts" by enqueuing the action that triggers the execution of the action, in a thread, from the respective thread-pool. If the agent is busy in executing another action, the new action is simply enqueued. Once an action is executed from the action queue, the queue is checked for more entries and triggers the next action, if found. This whole "reactive" mechanism of triggering actions obviates the need of a message loop, polling the queue. This is only possible, because the entry points to an agent's queue are controlled.</p><p>Actions <a class="indexterm" id="id333"/>are <a class="indexterm" id="id334"/>executed asynchronously on agents, which<a class="indexterm" id="id335"/> raises the question of how the errors are handled. Error cases need to be handled with an explicit, predefined function. When using a default agent construction, such as <code class="literal">(agent :foo)</code>, the agent is created without any error handler, and gets suspended in the event of any exception. It caches the exception, and refuses to accept any more actions. It throws the cached exception upon sending any action until the agent is restarted. A suspended agent can be reset using the <code class="literal">restart-agent</code> function. The objective of such suspension is safety and supervision. When the asynchronous actions are executed on an agent and suddenly an error occurs, it will require attention. Check out the following code:</p><div class="informalexample"><pre class="programlisting">(def g (agent 0))
(send g (partial / 10))  ; ArithmeticException due to divide-by-zero
@g  ; returns 0, because the error did not change the old state
(send g inc)  ; throws the cached ArithmeticException
(agent-error g)  ; returns (doesn't throw) the exception object
(restart-agent g @g)  ; clears the suspension of the agent
(agent-error g)  ; returns nil
(send g inc)  ; works now because we cleared the cached error
@g  ; returns 1
(dotimes [_ 1000] (send-off g long-task))
;; block for 100ms or until all actions over (whichever earlier)
(await-for 100 g)
(await g)  ; block until all actions dispatched till now are over</pre></div><p>There are two optional parameters <code class="literal">:error-handler</code> and <code class="literal">:error-mode, which</code> we can configure on an agent to have finer control over the error handling and suspension as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">(def g (agent 0 :error-handler (fn [x] (println "Found:" x))))  ; incorrect arity
(send g (partial / 10))  ; no error encountered because error-handler arity is wrong
(def g (agent 0 :error-handler (fn [ag x] (println "Found:" x))))  ; correct arity
(send g (partial / 10))  ; prints the message
(set-error-handler! g (fn [ag x] (println "Found:" x)))  ; equiv of :error-handler arg
(def h (agent 0 :error-mode :continue))
(send h (partial / 10))  ; error encountered, but agent not suspended
(send h inc)
@h  ; returns 1
(set-error-mode! h :continue)  ; equiv of :error-mode arg, other possible value :fail</pre></div></div><div class="section" title="Why you should use agents"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Why you should use agents</h2></div></div></div><p>Just as the "atom" implementation uses only compare-and-swap instead of locking, the underlying "agent" specific implementation uses mostly the compare-and-swap operations. The<a class="indexterm" id="id336"/> agent implementation uses locks only when dispatching action in a transaction (discussed in the next section), or when restarting an agent. All the actions are queued and dispatched serially in the agents, regardless of the concurrency level. The serial nature makes it possible to execute the actions in an independent and contention-free manner. For the same agent, there can never be more than one action being executed. Since there is no locking, reads (<code class="literal">deref</code> or <code class="literal">@</code>) on agents are never blocked due to writes. However, all the actions are independent of each other—there is no overlap in their execution.</p><p>The implementation goes so far as to ensure that the execution of an action blocks other actions, which follow in the queue. Even though the actions are executed in a thread-pool, actions for the same agent are never executed concurrently. This is an excellent ordering guarantee that also extends a natural coordination mechanism, due to its serial nature. However, note that this ordering coordination is limited to only a single agent. If an agent action sends actions to two other agents, they are not automatically coordinated. In this situation, you may want to use transactions (which will be covered in the next section).</p><p>Since agents distinguish between the low-latency and blocking jobs, the jobs are executed in an appropriate kind of thread-pools. Actions on different agents may execute concurrently, thereby making optimum use of the threading resources. Unlike atoms, the performance of the agents is not impeded by high contention. In fact, for many cases, agents make a lot of sense due to the serial buffering of actions. In general, agents are great for high volume I/O tasks, or where the ordering of operations provides a win in the high contention scenarios.</p></div><div class="section" title="Nesting"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec59"/>Nesting</h2></div></div></div><p>When an<a class="indexterm" id="id337"/> agent action sends another action to the same agent, that is a case of nesting. This would have been nothing special if agents didn't participate in STM transactions (which will be covered in the next section). However, agents do participate in STM transactions and that places certain constraints on agent implementation that warrants a second-layer buffering of actions. For now, it should suffice to say that the nested sends are queued in a thread-local queue instead of the regular queue in the agent. The thread-local queue is visible only to the thread in which the action is executed. Upon executing an action, unless there was an error, the agent implicitly calls the equivalent of <code class="literal">release-pending-sends</code> function, which transfers the actions from second level thread-local queue to the normal action queue. Note that nesting is simply an implementation detail of agents and <a class="indexterm" id="id338"/>has no other impact.</p></div></div>
<div class="section" title="Coordinated transactional ref and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Coordinated transactional ref and state</h1></div></div></div><p>We saw in <a class="indexterm" id="id339"/>an earlier section that an atom provides <a class="indexterm" id="id340"/>atomic read-and-update operation. What if we need to perform an atomic read-and-update operation across two or even more number of atoms? This clearly poses a coordination problem. Some entity has to watch over the process of reading and updating, so that the values are not corrupted. This is what a ref provides—a <span class="strong"><strong>Software Transactional Memory</strong></span> (<span class="strong"><strong>STM</strong></span>) based system that takes care of concurrent atomic read-and-update operations across multiple refs, such that either all the updates go through, or in the case of failure, none does. Like atoms, on failure, refs retry the whole operation from scratch with the new values.</p><p>Clojure's STM implementation is coarse grained. It works at the application level objects and aggregates (that is, references to aggregates), scoped to only all the refs in a program, constituting the "Ref world". Any update to a ref can only happen synchronously, in a transaction, in a <code class="literal">dosync</code> block of code, within the same thread. It cannot span beyond the current thread. The implementation detail reveals that a thread-local transaction context is maintained during a lifetime of a transaction. The same context ceases to be available, the moment the control reaches another thread.</p><p>Like the other reference types in Clojure, reads on a ref are never blocked by the updates, and vice versa. However, unlike the other reference types, the implementation of ref does not depend on a lock-free spinning, but rather, it internally uses locks, a low-level wait/notify, a deadlock detection, and the age-based barging.</p><p>The <code class="literal">alter</code> function is used to read-and-update the value of a ref, and <code class="literal">ref-set</code> is used to reset the value. Roughly, <code class="literal">alter</code> and <code class="literal">ref-set,</code> for the refs, are analogous to <code class="literal">swap!</code> and <code class="literal">reset!</code> for the atoms. Just like <code class="literal">swap!</code>, <code class="literal">alter</code> accepts a function (and arguments) with no side effects, and may be retried several times during the contention. However, unlike with the atoms, not only <code class="literal">alter</code> but also <code class="literal">ref-set</code> and simple <code class="literal">deref</code>, may cause a transaction to be retried during the contention. Here is a very simple example on how we may use a transaction:</p><div class="informalexample"><pre class="programlisting">(def r1 (ref [:a :b :c]))
(def r2 (ref [1 2 3]))
(alter r1 conj :d)  ; IllegalStateException No transaction running...
(dosync (let [v (last @r1)] (alter r1 pop) (alter r2 conj v)))
@r1  ; returns [:a :b]
@r2  ; returns [1 2 3 :c]
(dosync (ref-set r1 (conj @r1 (last @r2))) (ref-set r2 (pop @r2)))
@r1  ; returns [:a :b :c]
@r2  ; returns [1 2 3]</pre></div><div class="section" title="Ref characteristics"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec60"/>Ref characteristics</h2></div></div></div><p>Clojure <a class="indexterm" id="id341"/>maintains the <span class="strong"><strong>Atomicity</strong></span>, <span class="strong"><strong>Consistency</strong></span>, and <span class="strong"><strong>Isolation</strong></span> (<span class="strong"><strong>ACI</strong></span>) characteristics in a transaction. This overlaps with A, C, and I of the ACID guarantee that many databases provide. Atomicity implies that either all of the updates in a transaction will complete successfully or none of them do. Consistency means that the transaction must maintain general correctness, and should honor the constraints set by the validation—any exception or validation error should roll back the transaction. Unless a shared state is guarded, concurrent updates on it may lead a multi-step transaction into seeing different values at different steps. Isolation implies that all the steps in a transaction will see the same value, no matter how concurrent the updates are.</p><p>The Clojure refs use something known as <span class="strong"><strong>Multi Version Concurrency Control</strong></span> (<span class="strong"><strong>MVCC</strong></span>) to provide <span class="strong"><strong>Snapshot Isolation</strong></span> to the transactions. In MVCC, instead of locking (which could block the transactions), the queues are maintained, so that each transaction can occur using its own snapshot copy, taken at its "read point", independent of other transactions. The main benefit of this approach is that the read-only out-of-transaction operations can go through without any contention. Transactions without the ref contention go through concurrently. In a rough comparison with the database systems, the Clojure ref isolation level is "Read Committed" for reading a Ref outside of a transaction, and "Repeatable Read" by default when inside the transaction.</p></div><div class="section" title="Ref history and in-transaction deref operations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"/>Ref history and in-transaction deref operations</h2></div></div></div><p>We discussed earlier that both, read and update operations, on a ref, may cause a transaction<a class="indexterm" id="id342"/> to be retried. The reads in a transaction <a class="indexterm" id="id343"/>can be configured to use the ref history in such a manner that the snapshot isolation instances are stored in the history queues, and are used by the read operations in the transactions. The default, which is not supposed to use the history queues, conserves heap space, and provides strong consistency (avoids the staleness of data) in the transactions.</p><p>Using the ref history reduces the likelihood of the transaction retries caused by read contention, thereby providing a weak consistency. Therefore, it is a tool for performance optimization, which comes at the cost of consistency. In many scenarios, programs do not need strong consistency—we can choose appropriately if we know the trade-off, and what we need. The snapshot isolation mechanism in the Clojure ref implementation is backed by the adaptive history queues. The history queues grow dynamically to meet the read requests, and do not overshoot the maximum limit that is set for the ref. By default, the history is not enabled, so we need to specify it during the initialization or set it later. Here is an example of how to use the history:</p><div class="informalexample"><pre class="programlisting">(def r (ref 0 :min-history 5 :max-history 10))
(ref-history-count r)  ; returns 0, because no snapshot instances are queued so far
(ref-min-history r)  ; returns 5
(ref-max-history r)  ; returns 10
(future (dosync (println "Sleeping 20 sec") (Thread/sleep 20000) (ref-set r 10)))
(dosync (alter r inc))  ; enter this within few seconds after the previous expression
;; The message "Sleeping 20 sec" should appear twice due to transaction-retry
(ref-history-count r)  ; returns 2, the number of snapshot history elements
(.trimHistory ^clojure.lang.Ref r)
(ref-history-count r)  ; returns 0 because we wiped the history
(ref-min-history r 10)  ; reset the min history
(ref-max-history r 20)  ; reset the max history count</pre></div><p>Minimum/maximum history limits are proportional to the length of the staleness window of the data. It<a class="indexterm" id="id344"/> also depends on the relative latency<a class="indexterm" id="id345"/> difference of the update and read operations to see what the range of the min-history and the max-history works well on a given host system. It may take some amount of trial and error to get the range right. As a ballpark figure, read operations only need as many min-history elements to avoid the transaction retries, as many updates can go through during one read operation. The max-history elements can be a multiple of min-history to cover for any history overrun or underrun. If the relative latency difference is unpredictable, then we have to either plan a min-history for the worst case scenario, or consider other approaches.</p></div><div class="section" title="Transaction retries and barging"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec62"/>Transaction retries and barging</h2></div></div></div><p>A <a class="indexterm" id="id346"/>transaction can internally be in one <a class="indexterm" id="id347"/>of the five distinct states—Running, Committing, Retry, Killed, and Committed. A transaction can be killed for various reasons. Exceptions are the common reasons for killing a transaction. But let's consider the corner case where a transaction is retried many times, but it does not appear to commit successfully—what is the resolution? Clojure supports age-based barging, wherein an older transaction automatically tries to abort a younger transaction, so that the younger transaction is retried later. If the barging still doesn't work, as a last resort, the transaction is killed after a hard limit of 10,000 retry attempts, and then the exception is thrown.</p></div><div class="section" title="Upping transaction consistency with ensure"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec63"/>Upping transaction consistency with ensure</h2></div></div></div><p>Clojure's transactional consistency is a good balance between performance and safety. However,<a class="indexterm" id="id348"/> at times, we may need the <span class="strong"><strong>Serializable</strong></span> consistency in order to preserve the correctness of the transaction. Concretely, in the face of the transaction retries, when a transaction's correctness depends on the state of a ref, in the transaction, wherein the ref is updated simultaneously in another transaction, we have a condition called "write skew". The Wikipedia entry on the write skew, <a class="ulink" href="https://en.wikipedia.org/wiki/Snapshot_isolation">https://en.wikipedia.org/wiki/Snapshot_isolation</a>, describes it well, but let's see a<a class="indexterm" id="id349"/> more concrete example. Let's say we want to design a flight simulation system with two engines, and one of the system level constraints is not to switch off both engines at the same time. If we model each engine as a ref, and certain maneuvers do require us to switch off an engine, we must ensure that the other engine is on. We can do it with <code class="literal">ensure</code>. Usually, <code class="literal">ensure</code> is required when maintaining a consistent relationship (invariants) across the refs is necessary. This cannot be ensured by the validator functions, because they do not come into play until the transaction commits. The validator functions will see the same value hence cannot help.</p><p>The write-skew can be solved using the namesake <code class="literal">ensure</code> function that essentially prevents a ref from modification by other transactions. It is similar to a locking operation, but in practice, it provides better concurrency than the explicit read-and-update operations, when the retries are expensive. Using <code class="literal">ensure</code> is quite simple—<code class="literal">(ensure ref-object).</code> However, it may be performance-wise expensive, due to the locks it holds during the transaction. Managing performance with <code class="literal">ensure</code> involves a trade-off between the retry latency, and the lost throughput due to the ensured state.</p></div><div class="section" title="Lesser transaction retries with commutative operations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec64"/>Lesser transaction retries with commutative operations</h2></div></div></div><p>Commutative<a class="indexterm" id="id350"/> operations are independent of the order in which they are applied. For example, incrementing a counter ref c1 from transactions t1 and t2 would have the same effect irrespective of the order in which t1 and t2 commit their changes. Refs have a special optimization for changing functions that are commutative for transactions—the <code class="literal">commute</code> function, which is similar to <code class="literal">alter</code> (same syntax), but with different semantics. Like <code class="literal">alter</code>, the <code class="literal">commute</code> functions are applied atomically during the transaction commit. However, unlike <code class="literal">alter</code>, <code class="literal">commute</code> does not cause the transaction retry on contention, and there is no guarantee about the order in which the <code class="literal">commute</code> functions are applied. This effectively makes <code class="literal">commute</code> nearly useless for returning a meaningful value as a result of the operation. All the commute functions in a transaction are reapplied with the final in transaction ref values during the transaction commit.</p><p>As we can see, commute reduces the contention, thereby optimizing the performance of the overall transaction throughput. Once we know that an operation is commutative and we are not going to use its return value in a meaningful way, there is hardly any trade-off deciding on whether to use it—we should just go ahead and use it. In fact, a program design, with respect to the ref<a class="indexterm" id="id351"/> transactions, with commute in mind, is not a bad idea.</p></div><div class="section" title="Agents can participate in transactions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec65"/>Agents can participate in transactions</h2></div></div></div><p>In the <a class="indexterm" id="id352"/>previous section on agents, we discussed how agents work with the queued change functions. Agents can also participate in the ref transactions, thereby making it possible to combine the use of refs and agents in the transactions. However, agents are not included in the "Ref world", hence a transaction scope is not extended till the execution of the change function in an agent. Rather, the transactions only make sure that the changes sent to the agents are queued until the transaction commit happens.</p><p>The <span class="emphasis"><em>Nesting</em></span> sub-section, in the earlier section on agents, discusses about a second-layer thread-local queue. This thread-local queue is used during a transaction to hold the sent changes to an agent until the commit. The thread-local queue does not block the other changes that are being sent to an agent. The out-of-transaction changes are never buffered in the thread-local queue; rather, they are added to the regular queue in the agent.</p><p>The participation of agents in the transactions provides an interesting angle of design, where the coordinated and independent/sequential operations can be pipelined as a workflow for better throughput and performance.</p></div><div class="section" title="Nested transactions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec66"/>Nested transactions</h2></div></div></div><p>Clojure transactions are nesting aware and they compose well. But, why would one need a nested <a class="indexterm" id="id353"/>transaction? Often, independent units of code may have their own low-granularity transactions that a higher level code can make use of. When the higher level caller itself needs to wrap actions in a transaction, nested transactions occur. Nested transactions have their own lifecycle and run-state. However, an outer transaction can abort an inner transaction on the detection of failure.</p><p>The "ref world" snapshot <code class="literal">ensure</code>s and <code class="literal">commute</code>s are shared among all (that is, outer and inner) levels of a nested transaction. Due to this, the inner transaction is treated as any other ref change operation (similar to <code class="literal">alter</code>, <code class="literal">ref-set</code> and so on) within an outer transaction. The watches and internal lock implementation are handled at the respective nesting level. The detection of contention in the inner transactions causes a restart of not only the inner but also the outer transaction. Commits at all the levels are effected as a global state finally when the outermost transaction commits. The watches, even though tracked at each individual transaction level, are finally effected during the commit. A closer look at the nested transaction implementation shows that nesting has little or no impact on the performance <a class="indexterm" id="id354"/>of transactions.</p></div><div class="section" title="Performance considerations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec67"/>Performance considerations</h2></div></div></div><p>Clojure Ref is<a class="indexterm" id="id355"/> likely to be the most complex reference type implemented yet. Due to its characteristics, especially its transaction retry mechanism, it may not be immediately apparent that such a system would have good performance during the high-contention scenarios. </p><p>Understanding its nuances and best ways of use should help:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We do not use changes with the side effects in a transaction, except for possibly sending the I/O changes to agents, where the changes are buffered until the commit. So by definition, we do not carry out any expensive I/O work in a transaction. Hence, a retry of this work would be cheap as well.</li><li class="listitem" style="list-style-type: disc">A change function for a transaction should be as small as possible. This lowers the latency and hence, the retries will also be cheaper.</li><li class="listitem" style="list-style-type: disc">Any ref that is not updated along with at least one more ref simultaneously needs not be a ref—atoms would do just fine in this case. Now that the refs make sense only in a group, their contention is directly proportional to the group size. Small groups of refs used in the transactions lead to a low contention, lower latency, and a higher throughput.</li><li class="listitem" style="list-style-type: disc">Commutative functions provide a good opportunity to enhance the transaction throughput without any penalty. Identifying such cases and designing with commute in mind can help performance significantly.</li><li class="listitem" style="list-style-type: disc">Refs are very coarse grained—they work at the application aggregate level. Often a program may need to have more fine-grained control over the transaction<a class="indexterm" id="id356"/> resources. This can be enabled by Ref striping, such as Megaref (<a class="ulink" href="https://github.com/cgrand/megaref">https://github.com/cgrand/megaref</a>), by providing a scoped view on the associative refs, thereby allowing higher concurrency.</li><li class="listitem" style="list-style-type: disc">In the high contention scenarios in which the ref group size in a transaction cannot be small, consider using agents, as they have no contention due to the serial nature. Agents may not be a replacement for the transactions, but rather we can employ a pipeline consisting of atoms, refs, and agents to ease out the contention versus latency concerns.</li></ul></div><p>Refs and transactions have an intricate implementation. Fortunately, we can inspect the source code, and browse through available online and offline resources.</p></div></div>
<div class="section" title="Dynamic var binding and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Dynamic var binding and state</h1></div></div></div><p>The fourth<a class="indexterm" id="id357"/> kind among the Clojure's reference types is the dynamic var. Since Clojure 1.3, all the vars are static by default. A var must be explicitly declared so<a class="indexterm" id="id358"/> in order to be dynamic. Once declared, a dynamic var can be bound to new values on per-thread basis. Binding on different threads do not block each other. An example is shown here:</p><div class="informalexample"><pre class="programlisting">(def ^:dynamic *foo* "bar")
(println *foo*)  ; prints bar
(binding [*foo* "baz"] (println *foo*))  ; prints baz
(binding [*foo* "bar"] (set! *foo* "quux") (println *foo*))  ; prints quux</pre></div><p>As the dynamic binding is thread-local, it may be tricky to use in multi-threaded scenarios. Dynamic vars have been long abused by libraries and applications as a means to pass in a common argument to be used by several functions. However, this style is acknowledged to be an anti-pattern, and is discouraged. Typically, in the anti-pattern dynamic, vars are wrapped by a macro to contain the dynamic thread-local binding in the lexical scope. This causes problems with the multi-threading and lazy sequences.</p><p>So, how can the dynamic vars be used effectively? A dynamic var lookup is more expensive than looking up a static var. Even passing a function argument is performance-wise much cheaper than looking up a dynamic var. Binding a dynamic var incurs additional cost. Clearly, in performance sensitive code, dynamic vars are best not used at all. However, dynamic vars may prove to be useful to hold a temporary thread-local state in a complex, or recursive call-graph scenario, where the performance does not matter significantly, without being advertised or leaked into the public API. The dynamic var bindings can nest and unwind like a stack, which makes them both attractive and suitable for such tasks.</p></div>
<div class="section" title="Validating and watching the reference types"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Validating and watching the reference types</h1></div></div></div><p>Vars (both static and dynamic), atoms, refs, and agents provide a way to validate the value being<a class="indexterm" id="id359"/> set as state—a <code class="literal">validator</code> function that accepts<a class="indexterm" id="id360"/> new value as argument, and returns the logical as true if it succeeds, or throws exception/returns logical as false (the false and nil values) if there's an error. They all honor what the validator function returns. If it is a success, the update goes through, and if an error, an exception is thrown instead. Here is the syntax on how the validators can be declared and associated with the reference types:</p><div class="informalexample"><pre class="programlisting">(def t (atom 1 :validator pos?))
(def g (agent 1 :validator pos?))
(def r (ref 1 :validator pos?))
(swap! t inc)  ; goes through, because value after increment (2) is positive
(swap! t (constantly -3))  ; throws exception
(def v 10)
(set-validator! (var v) pos?)
(set-validator! t (partial &lt; 10)) ; throws exception
(set-validator! g (partial &lt; 10)) ; throws exception
(set-validator! r #(&lt; % 10)) ; works</pre></div><p>Validators<a class="indexterm" id="id361"/> cause actual failure within a reference type while <a class="indexterm" id="id362"/>updating them. For vars and atoms, they simply prevent the update by throwing an exception. In an agent, a validation failure causes agent failure, and needs the agent to restart. Inside a ref, the validation failure causes the transaction to rollback and rethrow the exception.</p><p>Another mechanism to observe the changes to the reference types is a "watcher". Unlike validators, a watcher is passive—it is notified of the update after the fact. Hence, a watcher cannot prevent updates from going through, because it is only a notification mechanism. For transactions, a watcher is invoked only after the transaction commit. While only one validator can be set on a reference type, it is possible to associate multiple watchers to a reference type on the other hand. Secondly, when adding a watch, we can specify a key, so that the notifications can be identified by the key, and be dealt accordingly by the watcher. Here is the syntax on how to use watchers:</p><div class="informalexample"><pre class="programlisting">(def t (atom 1))
(defn w [key iref oldv newv] (println "Key:" key "Old:" oldv "New:" newv))
(add-watch t :foo w)
(swap! t inc)  ; prints "Key: :foo Old: 1 New: 2"</pre></div><p>Like validators, the watchers are executed synchronously in the thread of the reference type. For atoms and refs, this may be fine, since the notification to the watchers goes on, the other threads may proceed with their updates. However in agents, the notification happens in the same thread where the update happens—this makes the update latency higher, and the throughput potentially lower.</p></div>
<div class="section" title="Java concurrent data structures"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Java concurrent data structures</h1></div></div></div><p>Java has<a class="indexterm" id="id363"/> a number of mutable data structures that are meant for concurrency and thread-safety, which implies that multiple callers can safely access these data structures at the same time, without blocking each other. When we need only the highly concurrent access without the state management, these data structures may be a very good fit. Several of these employ lock free algorithms. We discussed about the Java atomic state classes in the <span class="emphasis"><em>Atomic updates and state section</em></span>, so we will not repeat them here. Rather, we will only discuss the concurrent queues and other collections. </p><p>All of these data structures live in the <code class="literal">java.util.concurrent</code> package. These concurrent data structures are tailored to leverage<a class="indexterm" id="id364"/> the JSR 133 "Java Memory Model and <a class="indexterm" id="id365"/>Thread Specification Revision" (<a class="ulink" href="http://gee.cs.oswego.edu/dl/jmm/cookbook.html">http://gee.cs.oswego.edu/dl/jmm/cookbook.html</a>) implementation that first appeared in Java 5.</p><div class="section" title="Concurrent maps"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec68"/>Concurrent maps</h2></div></div></div><p>Java has a mutable concurrent hash map—<code class="literal">java.util.concurrent.ConcurrentHashMap</code> (CHM in short). The concurrency level can be optionally <a class="indexterm" id="id366"/>specified when instantiating the class, which is 16 by default. The CHM implementation internally partitions the map entries into the hash buckets, and uses multiple locks to reduce the contention on each bucket. Reads are never blocked by writes, therefore they may be stale or inconsistent—this is countered by built-in detection of such situations, and issuing a lock in order to read the data again in the synchronized fashion. This is an optimization for the scenarios, where reads significantly outnumber writes. In CHM, all the individual operations are near constant-time unless stuck in a retry loop due to the lock contention.</p><p>In contrast with Clojure's persistent map, CHM cannot accept <code class="literal">null</code> (<code class="literal">nil</code>) as the key or value. Clojure's immutable scalars and collections are automatically well-suited for use with CHM. An important thing to note is that only the individual operations in CHM are atomic, and exhibit strong consistency. As CHM operations are concurrent, the aggregate operations provide a rather weak consistency than the true operation-level consistency. Here is how we can use CHM. The individual operations in CHM, which provide a better consistency, are safe to use. The aggregate operations should be reserved for when we know its consistency characteristics, and the related trade-off:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.ConcurrentHashMap)
(def ^ConcurrentHashMap m (ConcurrentHashMap.))
(.put m :english "hi")                    ; individual operation
(.get m :english)                           ; individual operation
(.putIfAbsent m :spanish "alo")    ; individual operation
(.replace m :spanish "hola")         ; individual operation
(.replace m :english "hi" "hello")  ; individual compare-and-swap atomic operation
(.remove m :english)                     ; individual operation
(.clear m)    ; aggregate operation
(.size m)      ; aggregate operation
(count m)    ; internally uses the .size() method
;; aggregate operation
(.putAll m {:french "bonjour" :italian "buon giorno"})
(.keySet m)  ; aggregate operation
(keys m)      ; calls CHM.entrySet() and on each pair java.util.Map.Entry.getKey()
(vals m)       ; calls CHM.entrySet() and on each pair java.util.Map.Entry.getValue()</pre></div><p>The <code class="literal">java.util.concurrent.ConcurrentSkipListMap</code> class (CSLM in short) is another concurrent mutable map data structure in Java. The difference between CHM and CSLM is that CSLM offers a sorted view of the map at all times with the O(log N) time complexity. The <a class="indexterm" id="id367"/>sorted view has the natural order of keys by default, which can be overridden by specifying a Comparator implementation when instantiating CSLM. The implementation of CSLM is based on the Skip List, and provides navigation operations.</p><p>The <code class="literal">java.util.concurrent.ConcurrentSkipListSet</code> class (CSLS in short) is a concurrent mutable set based on the CSLM implementation. While CSLM offers the map API, CSLS behaves as a set data structure while borrowing features of CSLM.</p></div><div class="section" title="Concurrent queues"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec69"/>Concurrent queues</h2></div></div></div><p>Java has a built-in implementation of several kinds of mutable and concurrent in-memory queues. The<a class="indexterm" id="id368"/> queue data structure is a useful tool for buffering, producer-consumer style implementation, and for pipelining such units together to form the high-performance workflows. We should not confuse them with durable queues that are used for similar purpose in the batch jobs for a high throughput. Java's in-memory queues are not transactional, but they provide atomicity and strong consistency guarantee for the individual queue operations only. Aggregate operations offer weaker consistency.</p><p>The <code class="literal">java.util.concurrent.ConcurrentLinkedQueue</code> (CLQ) is a lock-free, wait-free unbounded "First In First Out" (FIFO) queue. FIFO implies that the order of the queue elements will not change once added to the queue. CLQ's <code class="literal">size()</code> method is not a constant time operation; it depends on the concurrency level. Few examples of using CLQ are here:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.ConcurrentLinkedQueue)
(def ^ConcurrentLinkedQueue q (ConcurrentLinkedQueue.))
(.add q :foo)
(.add q :bar)
(.poll q)  ; returns :foo
(.poll q)  ; returns :bar</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Queue</p>
</th><th style="text-align: left" valign="bottom">
<p>Blocking?</p>
</th><th style="text-align: left" valign="bottom">
<p>Bounded?</p>
</th><th style="text-align: left" valign="bottom">
<p>FIFO?</p>
</th><th style="text-align: left" valign="bottom">
<p>Fairness?</p>
</th><th style="text-align: left" valign="bottom">
<p>Notes</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>CLQ</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Wait-free, but the size() is not constant time</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ABQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>The capacity is fixed at instantiation</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>DQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>The elements implement the Delayed interface</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LBQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>The capacity is flexible, but with no fairness option</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PBQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>The elements are consumed in a priority order</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>–</p>
</td><td style="text-align: left" valign="top">
<p>–</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>It has no capacity; it serves as a channel</p>
</td></tr></tbody></table></div><p>In the <code class="literal">java.util.concurrent</code> package, <code class="literal">ArrayBlockingQueue</code> (ABQ), <code class="literal">DelayQueue</code> (DQ), <code class="literal">LinkedBlockingQueue</code> (LBQ), <code class="literal">PriorityBlockingQueue</code> (PBQ), and <code class="literal">SynchronousQueue</code> (SQ) implement the <code class="literal">BlockingQueue</code> (BQ) interface. Its Javadoc describes the <a class="indexterm" id="id369"/>characteristics of its method calls. ABQ is a fixed-capacity, FIFO queue backed by an array. LBQ is also a FIFO queue, backed by the linked nodes, and is optionally bounded (default <code class="literal">Integer.MAX_VALUE</code>). ABQ and LBQ generate "Back pressure" by blocking the enqueue operations on full capacity. ABQ supports optional fairness (with performance overhead) in the order of the threads that access it.</p><p>DQ is an unbounded queue that accepts the elements associated with the delay. The queue elements cannot be null, and must implement the <code class="literal">java.util.concurrent.Delayed</code> interface. Elements are available for removal from the queue only after the delay has been expired. DQ can be very useful for scheduling the processing of the elements at different times.</p><p>PBQ is unbounded and blocking while letting elements be consumed from the queue as per priority. Elements have the natural ordering by default that can be overridden by specifying a Comparator implementation when instantiating the queue.</p><p>SQ is not really a queue at all. Rather, it's just a barrier for a producer or consumer thread. The producer blocks until a consumer removes the element and vice versa. SQ does not have a capacity. However, SQ supports optional fairness (with performance overhead), in the order, in which the threads access it.</p><p>There are some new concurrent queue types introduced after Java 5. Since JDK 1.6, in the <code class="literal">java.util.concurrent</code> package Java has <span class="strong"><strong>BlockingDeque</strong></span> (<span class="strong"><strong>BD</strong></span>) with <span class="strong"><strong>LinkedBlockingDeque</strong></span> (<span class="strong"><strong>LBD</strong></span>) as the only available implementation. BD builds on BQ by adding the <span class="strong"><strong>Deque</strong></span> (<span class="strong"><strong>double-ended queue</strong></span>) operations, that is, the ability to add elements and consume the elements from both the ends of the queue. LBD can be instantiated with an optional capacity (bounded) to block the overflow. JDK 1.7 introduced <span class="strong"><strong>TransferQueue</strong></span> (<span class="strong"><strong>TQ</strong></span>) with <span class="strong"><strong>LinkedTransferQueue</strong></span> (<span class="strong"><strong>LTQ</strong></span>) as the only implementation. TQ extends the concept of SQ in such a way that the producers and consumers block a queue of elements. This will help utilize the producer and consumer threads better by keeping them busy. LTQ is an unbounded implementation of TQ where the <code class="literal">size()</code> method is<a class="indexterm" id="id370"/> not a constant time operation.</p></div><div class="section" title="Clojure support for concurrent queues"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec70"/>Clojure support for concurrent queues</h2></div></div></div><p>We covered the <a class="indexterm" id="id371"/>persistent queue in <a class="link" href="ch02.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span> earlier. Clojure has a built-in <code class="literal">seque</code> function that builds over a BQ implementation (LBQ by default) to expose a write-ahead sequence. The sequence is potentially lazy, and the write-ahead buffer throttles how many elements to realize. As opposed to the chunked sequences (of chunk size 32), the size of the write-ahead buffer is controllable and potentially populated at all times until the source sequence is exhausted. Unlike the chunked sequences, the realization doesn't happen suddenly for a chunk of 32 elements. It does so gradually and smoothly.</p><p>Under the hood, Clojure's <code class="literal">seque</code> uses an agent to the backfill data in the write-ahead buffer. In the arity-2 variant of <code class="literal">seque</code>, the first argument should either be a positive integer, or an instance of BQ (ABQ, LBQ, and more) that is preferably bounded.</p></div></div>
<div class="section" title="Concurrency with threads"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Concurrency with threads</h1></div></div></div><p>On<a class="indexterm" id="id372"/> the JVM, threads are the de-facto fundamental instrument of concurrency. Multiple threads live in the same JVM; they share the heap space, and compete for the resources.</p><div class="section" title="JVM support for threads"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec71"/>JVM support for threads</h2></div></div></div><p>The JVM <a class="indexterm" id="id373"/>threads are the Operating System threads. Java wraps an underlying OS thread as an instance of the <code class="literal">java.lang.Thread</code> class, and builds up an API around it to work with threads. A thread on the JVM has a number of states: New, Runnable, Blocked, Waiting, Timed_Waiting, and Terminated. A thread is instantiated by overriding the <code class="literal">run()</code> method of the <code class="literal">Thread</code> class, or by passing an instance of the <code class="literal">java.lang.Runnable</code> interface to the constructor of the <code class="literal">Thread</code> class. </p><p>Invoking the <code class="literal">start()</code> method of a <code class="literal">Thread</code> instance starts its execution in a new thread. Even if just a single thread runs in the JVM, the JVM would not shut down. Calling the <code class="literal">setDaemon(boolean)</code> method of a thread with argument <code class="literal">true</code> tags the thread as a daemon that can be automatically shut down if no other non-daemon thread is running.</p><p>All Clojure functions implement the <code class="literal">java.lang.Runnable</code> interface. Therefore, invoking a function in a new thread is very easy:</p><div class="informalexample"><pre class="programlisting">(defn foo5 [] (dotimes [_ 5] (println "Foo")))
(defn barN [n] (dotimes [_ n] (println "Bar")))
(.start (Thread. foo5))  ; prints "Foo" 5 times
(.start (Thread. (partial barN 3)))  ; prints "Bar" 3 times</pre></div><p>The <code class="literal">run()</code> method does not accept any argument. We can work around it by creating a higher order <a class="indexterm" id="id374"/>function that needs no arguments, but internally applies the argument <code class="literal">3</code>.</p></div><div class="section" title="Thread pools in the JVM"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec72"/>Thread pools in the JVM</h2></div></div></div><p>Creating<a class="indexterm" id="id375"/> threads leads to the Operating System API calls, which is not always a cheap operation. The general practice is to create a pool of threads that can be recycled for different tasks. Java has a built-in support for threads pools. The interface called <code class="literal">java.util.concurrent.ExecutorService</code> represents the API for a thread pool. The most common way to create a thread pool is to use a factory method in the <code class="literal">java.util.concurrent.Executors</code> class:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.Executors)
(import 'java.util.concurrent.ExecutorService)
(def ^ExecutorService a (Executors/newSingleThreadExecutor))  ; bounded pool
(def ^ExecutorService b (Executors/newCachedThreadPool))  ; unbounded pool
(def ^ExecutorService c (Executors/newFixedThreadPool 5))  ; bounded pool
(.execute b #(dotimes [_ 5] (println "Foo")))  ; prints "Foo" 5 times</pre></div><p>The previous example is equivalent of the examples with raw threads that we saw in the previous sub-section. Thread pools are also capable of helping to track the completion, and the return value of a function, executed in a new thread. An ExecutorService accepts an instance of the <code class="literal">java.util.concurrent.Callable</code> instance as an argument to several methods that launch a task, and return <code class="literal">java.util.concurrent.Future</code> to track the final result. </p><p>All the Clojure functions also implement the <code class="literal">Callable</code> interface, so we can use them as follows:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.Callable)
(import 'java.util.concurrent.Future)
(def ^ExecutorService e (Executors/newSingleThreadExecutor))
(def ^Future f (.submit e (cast Callable #(reduce + (range 10000000)))))
(.get f)  ; blocks until result is processed, then returns it</pre></div><p>The thread pools described here are the same as the ones that we saw briefly in the Agents section earlier. Thread pools need to be shut down by calling the <code class="literal">shutdown()</code> method when no longer required.</p></div><div class="section" title="Clojure concurrency support"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec73"/>Clojure concurrency support</h2></div></div></div><p>Clojure has some nifty built-in features to deal with concurrency. We already discussed about the <a class="indexterm" id="id376"/>agents, and how they use the thread pools, in an earlier section. There are some more concurrency features in Clojure to deal with the various use cases.</p><div class="section" title="Future"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec19"/>Future</h3></div></div></div><p>We saw <a class="indexterm" id="id377"/>earlier in this section how to use the Java API to launch a new thread, to execute a function. Also, we learned how to get the result back. Clojure has a built-in support called "futures" to do these things in a much smoother and integrated manner. The basis of the futures is the function <code class="literal">future-call</code> (it takes a <code class="literal">no-arg</code> function as an argument), and the macro <code class="literal">future</code> (it takes the body of code) that builds on the former. Both of them immediately start a thread to execute the supplied code. The following snippet illustrates the functions that work with the future, and how to use them:</p><div class="informalexample"><pre class="programlisting">;; runs body in new thread
(def f (future (println "Calculating") (reduce + (range 1e7))))
(def g (future-call #(do (println "Calculating") (reduce + (range 1e7)))))  ; takes no-arg fn
(future? f)                  ; returns true
(future-cancel g)        ; cancels execution unless already over (can stop mid-way)
(future-cancelled? g) ; returns true if canceled due to request
(future-done? f)         ; returns true if terminated successfully, or canceled
(realized? f)               ; same as future-done? for futures
@f                              ; blocks if computation not yet over (use deref for timeout)</pre></div><p>One of the interesting aspects of <code class="literal">future-cancel</code> is that it can sometimes not only cancel tasks that haven't started yet, but may also abort those that are halfway through execution:</p><div class="informalexample"><pre class="programlisting">(let [f (future (println "[f] Before sleep")
                (Thread/sleep 2000)
                (println "[f] After sleep")
                2000)]
  (Thread/sleep 1000)
  (future-cancel f)
  (future-cancelled? f))
;; [f] Before sleep  ← printed message (second message is never printed)
;; true  ← returned value (due to future-cancelled?)</pre></div><p>The previous scenario happens because Clojure's <code class="literal">future-cancel</code> cancels a future in such a way that if the execution has already started, it may be interrupted causing <code class="literal">InterruptedException</code>, which, if not explicitly caught, would simply abort the block of code. Beware <a class="indexterm" id="id378"/>of exceptions arising from the code that is executed in a future, because, by default, they are not reported verbosely! Clojure futures use the "solo" thread pool (used to execute the potentially blocking actions) that we discussed earlier with respect to the agents.</p></div><div class="section" title="Promise"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec20"/>Promise</h3></div></div></div><p>A promise is <a class="indexterm" id="id379"/>a placeholder for the result of a computation that may or may not have occurred. A promise is not directly associated with any computation. By definition, a promise does not imply when the computation might occur, hence realizing the promise.</p><p>Typically, a promise originates from one place in the code, and is realized by some other portion of the code that knows when and how to realize the promise. Very often, this happens in a multi-threaded code. If a promise is not realized yet, any attempt to read the value blocks all callers. If a promise is realized, then all the callers can read the value without being blocked. As with futures, a promise can be read with a timeout using <code class="literal">deref</code>. </p><p>Here is a very simple example showing how to use promises:</p><div class="informalexample"><pre class="programlisting">(def p (promise))
(realized? p)  ; returns false
@p  ; at this point, this will block until another thread delivers the promise
(deliver p :foo)
@p  ; returns :foo (for timeout use deref)</pre></div><p>A promise is a very powerful tool that can be passed around as function arguments. It can be stored in a reference type, or simply be used for a high level coordination.</p></div></div></div>
<div class="section" title="Clojure parallelization and the JVM"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Clojure parallelization and the JVM</h1></div></div></div><p>We <a class="indexterm" id="id380"/>observed in <a class="link" href="ch01.html" title="Chapter 1. Performance by Design">Chapter 1</a>, <span class="emphasis"><em>Performance by Design</em></span> that parallelism is a function<a class="indexterm" id="id381"/> of the hardware, whereas concurrency is a function of the software, assisted by the hardware support. Except for the algorithms that are purely sequential by nature, concurrency is the favored means to facilitate parallelism, and achieve better performance. Immutable and stateless data is a catalyst to concurrency, as there is no contention between threads, due to absence of mutable data.</p><div class="section" title="Moore's law"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec74"/>Moore's law</h2></div></div></div><p>In 1965, Intel's cofounder, Gordon Moore, made an observation that the number of transistors<a class="indexterm" id="id382"/> per square inch on Integrated Circuits doubles every 24 months. He also predicted that the trend would continue for 10 years, but in practice, it has continued till now, marking almost half a century. More transistors have resulted in more computing power. With a greater number of transistors in the same area, we need higher clock speed to transmit signals to all of the transistors. Secondly, transistors need to get smaller in size to fit in. Around 2006-2007, the clock speed that the circuitry could work with topped out at about 2.8GHz, due to the heating issues and the laws of physics. Then, the multi-core processors were born.</p></div><div class="section" title="Amdahl's law"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec75"/>Amdahl's law</h2></div></div></div><p>The multi-core<a class="indexterm" id="id383"/> processors naturally require splitting up computation in order to achieve parallelization. Here begins a conflict—a program that was made to be run sequentially cannot make use of the parallelization features of the multi-core processors. The program must be altered to find the opportunity to split up computation at every step, while keeping the cost of coordination in mind. This results in a limitation that a program can be no more faster than its longest sequential part (<span class="emphasis"><em>contention</em></span>, or <span class="emphasis"><em>seriality</em></span>), and the coordination overhead. This characteristic was described by Amdahl's law.</p></div><div class="section" title="Universal Scalability Law"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec76"/>Universal Scalability Law</h2></div></div></div><p>Dr Neil Gunther's Universal Scalability Law (USL) is a superset of Amdahl's Law that makes<a class="indexterm" id="id384"/> both: <span class="emphasis"><em>contention (α)</em></span> and <span class="emphasis"><em>coherency (β)</em></span> the first class concerns in quantifying the scalability very closely to the realistic parallel systems. Coherency implies the coordination overhead (latency) in making the result of one part of a parallelized program to be available to another. While Amdahl's Law states that contention (seriality) causes performance to level off, USL goes to show that the performance actually degrades with excessive parallelization. USL is described with the following formula:</p><p>C(N) = N / (1 + α ((N – 1) + β N (N – 1)))</p><p>Here, C(N) implies relative capacity or throughput in terms of the source of concurrency, such as physical processors, or the users driving the software application. α implies the degree of contention because of the shared data or the sequential code, and β implies penalty <a class="indexterm" id="id385"/>incurred for maintaining the consistency of shared data. I would encourage you to pursue USL further (<a class="ulink" href="http://www.perfdynamics.com/Manifesto/USLscalability.html">http://www.perfdynamics.com/Manifesto/USLscalability.html</a>), as this is a very important resource for studying the impact of concurrency on scalability and the performance of the systems.</p></div><div class="section" title="Clojure support for parallelization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec77"/>Clojure support for parallelization</h2></div></div></div><p>A <a class="indexterm" id="id386"/>program that relies on mutation cannot parallelize its parts without creating contention on the mutable state. It requires coordination overhead, which makes the situation worse. Clojure's immutable nature is better suited to parallelize the parts of a program. Clojure also has some constructs that are suited for parallelism by the virtue of Clojure's consideration of available hardware resources. The result is, the operations execute optimized for certain use case scenarios.</p><div class="section" title="pmap"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec21"/>pmap</h3></div></div></div><p>The <code class="literal">pmap</code> function (similar to <code class="literal">map</code>) accepts as arguments a function and one, or more collections<a class="indexterm" id="id387"/> of data elements. The function is applied to each of the data elements in such a way that some of the elements are processed by the function in parallel. The parallelism factor is chosen at runtime by the <code class="literal">pmap</code> implementation, as two greater than the total number of available processors. It still processes the elements lazily, but the realization factor is same as the parallelism factor. </p><p>Check out the following code:</p><div class="informalexample"><pre class="programlisting">(pmap (partial reduce +)
        [(range 1000000)
         (range 1000001 2000000)
         (range 2000001 3000000)])</pre></div><p>To use <code class="literal">pmap</code> effectively, it is imperative that we understand what it is meant for. As the documentation says, it is meant for computationally intensive functions. It is optimized for CPU-bound and cache-bound jobs. High latency and low CPU tasks, such as blocking I/O, are a gross misfit for <code class="literal">pmap</code>. Another pitfall to be aware of is whether the function used in <code class="literal">pmap</code> performs a lot of memory operations or not. Since the same function will be applied across all the threads, all the processors (or cores) may compete for the memory interconnect and the sub-system bandwidth. If the parallel memory access becomes a bottleneck, <code class="literal">pmap</code> cannot make the operation truly parallel, due to the contention on the memory access.</p><p>Another concern is what happens when several <code class="literal">pmap</code> operations run concurrently? Clojure does not attempt to detect multiple <code class="literal">pmap</code>s running concurrently. The same number of threads will be launched afresh for every new <code class="literal">pmap</code> operation. The developer is responsible to ensure the performance characteristics, and the response time of the program resulting from the concurrent pmap executions. Usually, when the latency reasons are paramount, it is advisable to limit the concurrent instances of <code class="literal">pmap</code> running in the program.</p></div><div class="section" title="pcalls"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec22"/>pcalls</h3></div></div></div><p>The <code class="literal">pcalls</code> function <a class="indexterm" id="id388"/>is built using <code class="literal">pmap</code>, so it borrows properties from the latter. However, the <code class="literal">pcalls</code> function accepts zero or more functions as arguments and executes them in parallel, returning the result values<a class="indexterm" id="id389"/> of the calls as a list.</p></div><div class="section" title="pvalues"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec23"/>pvalues</h3></div></div></div><p>The <code class="literal">pvalues</code> macro is built using <code class="literal">pcalls</code>, so it transitively shares the properties of <code class="literal">pmap</code>. It's<a class="indexterm" id="id390"/> behavior is similar to <code class="literal">pcalls</code>, but instead of functions, it accepts zero or more S-expressions that are evaluated in the parallel using <code class="literal">pmap</code>.</p></div></div><div class="section" title="Java 7's fork/join framework"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec78"/>Java 7's fork/join framework</h2></div></div></div><p>Java 7 introduced <a class="indexterm" id="id391"/>a new framework for parallelism called "fork/join," based on divide-and-conquer and the work-stealing scheduler algorithms. The basic idea of how to use the fork/join framework is fairly simple—if the work is small enough, then do it directly in the same thread; otherwise, split the work into two pieces, invoke them in a fork/join thread pool, and wait for the results to combine. </p><p>This way, the job gets recursively split into smaller parts such as an inverted tree, until the smallest part can be carried out in just a single thread. When the leaf/subtree jobs return, the parent combines the result of all children, and returns the results.</p><p>The fork/join framework is implemented in Java 7 in terms of a special kind of thread pool; check out <code class="literal">java.util.concurrent.ForkJoinPool</code>. The specialty of this thread pool is that it accepts the jobs of <code class="literal">java.util.concurrent.ForkJoinTask</code> type, and whenever these jobs block, waiting for the child jobs to finish, the threads used by the waiting jobs are allocated to the child jobs. When the child finishes its work, the thread is allocated back to the blocked parent jobs in order to continue. This style of dynamic thread allocation is described as "work-stealing". The fork/join framework can be used from within Clojure. The <code class="literal">ForkJoinTask</code> interface has two implementations: <code class="literal">RecursiveAction</code> and <code class="literal">RecursiveTask</code> in the <code class="literal">java.util.concurrent</code> package. Concretely, <code class="literal">RecursiveTask</code> maybe more useful with Clojure, as <code class="literal">RecursiveAction</code> is designed to work with mutable data, and does not return any value from its operation.</p><p>Using the fork-join framework entails choosing the batch size to split a job into, which is a crucial factor in parallelizing a long job. Too large a batch size may not utilize all the CPU cores enough; on the other hand, a small batch size may lead to a longer overhead, coordinating across the parent/child batches. As we will see in the next section, Clojure integrates with the Fork/join framework to parallelize the reducers implementation.</p></div></div>
<div class="section" title="Parallelism with reducers"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Parallelism with reducers</h1></div></div></div><p>Reducers <a class="indexterm" id="id392"/>are a new abstraction introduced in Clojure 1.5, and are likely to have a wider impact on the rest of the Clojure implementation in the future versions. They depict a different way of thinking about processing collections in Clojure—the key concept is to break down the notion that collections can be processed only sequentially, lazily, or producing a seq, and more. Moving away from such a behavior guarantee raises the potential for eager and parallel operations on one hand, whereas incurring constraints on the other. Reducers are compatible with the existing collections.</p><p>For an <a class="indexterm" id="id393"/>example, a keen observation of the regular <code class="literal">map</code> function reveals that its classic definition is tied to the mechanism (recursion), order (sequential), laziness (often), and representation (list/seq/other) aspects of producing the result. Most of this actually defines "how" the operation is performed, rather than "what" needs to be done. In the case of <code class="literal">map</code>, the "what" is all about applying a function to each element of its collection arguments. But since the collection types can be of various types (tree-structured, sequence, iterator, and more), the operating function cannot know how to navigate the collection. Reducers decouple the "what" and "how" parts of the operation.</p><div class="section" title="Reducible, reducer function, reduction transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec79"/>Reducible, reducer function, reduction transformation</h2></div></div></div><p>Collections <a class="indexterm" id="id394"/>are of various kinds, hence only a <a class="indexterm" id="id395"/>collection knows how to navigate itself. In the reducers model at a fundamental level, an internal "reduce" operation in<a class="indexterm" id="id396"/> each collection type has access to its properties and behavior, and access to what it returns. This makes all the collection types essentially "reducible". All the operations that work with collections can be modeled in terms of the internal "reduce" operation. The new modeled form of such operations is a "reducing function", which is typically a function of two arguments, the first argument being the accumulator, and the second being the new input.</p><p>How does it work when we need to layer several functions upon another, over the elements of a collection? For an example, let's say first we need to "filter", "map," and then "reduce". In such cases, a "transformation function" is used to model a reducer function (for example, for "filter") as another reducer function (for "map") in such a way that it adds the functionality during the transformation. This is called "reduction transformation".</p></div><div class="section" title="Realizing reducible collections"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec80"/>Realizing reducible collections</h2></div></div></div><p>While <a class="indexterm" id="id397"/>the reducer functions retain the purity of the abstraction, they are not useful all by themselves. The reducer operations in the namespace called as <code class="literal">clojure.core.reducers</code> similar to <code class="literal">map</code>, <code class="literal">filter</code>, and more, basically return a reducible collection that embed the reducer functions within themselves. A reducible collection is not realized, not even lazily realized—rather, it is just a recipe that is ready to be realized. In order to realize a reducible collection, we must use one of the <code class="literal">reduce</code> or <code class="literal">fold</code> operations.</p><p>The <code class="literal">reduce</code> operation that realizes a reducible collection is strictly sequential, albeit with the performance gains compared to <code class="literal">clojure.core/reduce</code>, due to reduced object allocations on the heap. The <code class="literal">fold</code> operation, which realizes a reducible collection, is potentially <a class="indexterm" id="id398"/>parallel, and uses a "reduce-combine" approach over the fork-join framework. Unlike the traditional "map-reduce" style, the use of fork/join the reduce-combine approach reduces at the bottom, and subsequently combines by the means of reduction again. This makes the <code class="literal">fold</code> implementation less wasteful and better performing.</p></div><div class="section" title="Foldable collections and parallelism"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec81"/>Foldable collections and parallelism</h2></div></div></div><p>Parallel reduction by <code class="literal">fold</code> puts certain constraints on the collections and operations. The tree-based <a class="indexterm" id="id399"/>collection types (persistent map, persistent vector, and persistent set) are amenable to parallelization. At the same time, the sequences may not be parallelized by <code class="literal">fold</code>. Secondly, <code class="literal">fold</code> requires that the<a class="indexterm" id="id400"/> individual reducer functions should be "associative", that is, the order of the input arguments applied to the reducer function should not matter. The reason being, <code class="literal">fold</code> can segment the elements of the collection to process in parallel, and the order in which they may be combined is not known in advance.</p><p>The <code class="literal">fold</code> function accepts few extra arguments, such as the "combine function," and the partition batch size (default being 512) for the parallel processing. Choosing the optimum partition size depends on the jobs, host capabilities, and the performance benchmarking. There are certain functions that are foldable (that is, parallelizable by <code class="literal">fold</code>), and there are others that are not, as shown here. They live in the <code class="literal">clojure.core.reducers</code> namespace:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Foldable</strong></span>: <code class="literal">map</code>, <code class="literal">mapcat</code>, <code class="literal">filter</code>, <code class="literal">remove</code>, and <code class="literal">flatten</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Non-foldable</strong></span>: <code class="literal">take-while</code>, <code class="literal">take</code>, and <code class="literal">drop</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Combine functions</strong></span>: <code class="literal">cat</code>, <code class="literal">foldcat</code>, and <code class="literal">monoid</code></li></ul></div><p>A notable aspect of reducers is that it is foldable in parallel only when the collection is a tree type. This implies that the entire data set must be loaded in the heap memory when folding over them. This has the downside of memory consumption during the high load on a system. On the other hand, a lazy sequence is a perfectly reasonable solution for such scenarios. When processing large amount of data, it may make sense to use a combination of lazy sequences and reducers for performance.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Summary</h1></div></div></div><p>Concurrency and parallelism are extremely important for performance in this multi-core age. Effective use of concurrency requires substantial understanding of the underlying principles and details. Fortunately, Clojure provides safe and elegant ways to deal with concurrency and state. Clojure's new feature called "reducers" provides a way to achieve granular parallelism. In the coming years, we are likely to see more and more processor cores, and an increasing demand to write code that takes advantage of these. Clojure places us in the right spot to meet such challenges.</p><p>In the next chapter, we will look at the performance measurement, analysis, and monitoring.</p></div></body></html>