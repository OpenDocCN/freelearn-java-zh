<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Build a Fraud Detection System</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we are going to develop an algorithm based on the Gaussian Distribution function using Spark ML. We will apply the algorithm to detect fraud in transactions data. This kind of algorithm can be applied toward building robust fraud detection solutions for financial institutions, such as banks, which handle great quantities of online transactions.</span></p>
<p class="mce-root">At the heart of the Gaussian Distribution, the function is the notion of an <strong>anomaly</strong>. The fraud detection problem is only a classification task but in a very narrow sense. It is a balanced supervised learning problem. The term <em>balanced</em> refers to the fact that the positives in the dataset are of a small number in relation to the negatives. On the other hand, an anomaly detection problem is typically not balanced. The dataset contains a significantly small number of anomalies (positives) in relation to the negatives. The fraud detection problem is a prime example of an anomaly detection problem. This is a problem where the dataset has a small number of outliers or data points whose values depart considerably from normal, to-be-expected values.</p>
<p class="mce-root">The overarching learning objective of this chapter is to implement a Scala solution that will predict fraud in financial transactions. We will lean on the Spark ML library's APIs and its supporting libraries in order to build a fraud detection prediction application.</p>
<p>In this chapter, we will cover the following topics: </p>
<ul>
<li>Fraud detection problem</li>
<li>Project overview—problem formulation</li>
<li>Getting started </li>
<li>Implementation steps</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fraud detection problem</h1>
                </header>
            
            <article>
                
<p>The fraud detection problem is not a supervised learning problem. <span>We have an unbalanced class situation in our fraud detection scenario. What do we have to say about</span> <span>the</span> <span>importance of the F1 score</span> <span>in relation to the target variable? </span><span>First, the target variable is a binary label. </span><span>The F1 score is relevant to our fraud detection proble</span>m because we have an <span>u</span><span>nbalanced class, where one class is practically more </span><span>important than the other. What do we mean by that?</span> <span>The bottom line of the fraud detection classification process concerns whether a certain instance is </span><span>fraudulent, and getting the classifier to classify or label this instance correctly as fraudulent.</span> <span>The emphasis is not on labeling an instance as </span><span>non-fraudulent.</span></p>
<p><span> </span><span>To reiterate, there are two classes in our fraud detection problem:</span></p>
<ul>
<li><span>Fraudulent </span></li>
<li><span>Non-fraudulent</span></li>
</ul>
<p><span>That said, we will now look at the dataset that this implementation depends upon </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fraud detection dataset at a glance</h1>
                </header>
            
            <article>
                
<p>Download the dataset from the <kbd>ModernScalaProjects_Code</kbd> download folder.</p>
<p>Here is what the dataset looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e314c67-5cea-472f-aa67-d37a7e5da734.jpg" style="width:44.08em;height:43.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The dataset that our fraud detection system is built on</div>
<p>The Gaussian Distribution function is the basis for our algorithm.<span><br/></span><span>So, is the F1 score important? Yes. The F1 score cannot be ignored (in the case of a balanced class situation, the F1 score is not necessarily important). </span><span>It is the measure of an ML binary classification's process accuracy.</span></p>
<div class="packt_infobox"><span>There is an F1 score for each class (one for fraudulent and the other for non-fraudulent). </span><span>Therefore, if we want to compute the F1 score, we need to ensure that the F1 score is associated with the fraudulent class. </span></div>
<p>Fraud detection in the context of ML is a classification technique method that allows us to build models that attempt to detect outliers. Flagging outliers leads us to do what is needed to address fraud. For example, if I swiped my card in Portland, Maine, on a whale watching vacation in a location more than 1,000 miles away from where I live, it is possible that an underlying fraud detection algorithm associated with my credit card will flag <strong>fraud</strong>. The distance, in this case, led the algorithm to claim that the said transaction at a certain seafood establishment place on Maine's waterfront was fake. This is a simple use case. There are other financial transactions that this algorithm is trained to monitor and flag fraud.</p>
<p>For example, imagine a situation where Kate lost her card, and some random person picked that card up on the street (let us assume that Kate wasn't aware that she had lost her card until a day later) and tried to fill up his truck's gas tank with about $50 worth of gas. Even though this transaction was carried out, assuming that this person that attempted to use her card somehow got past the zip code check, Kate's credit card's fraud detection ML algorithm will flag as a suspicious transaction. Most likely, the algorithm will simply kick in and cause the transaction to fail or, even if that does not happen, she would get a call from the credit card company asking her where she used the card recently. In this case scenario, she got that call from the credit card company because the fraud detection algorithm flagged that particular transaction as suspicious, a fraudulent incident that requires the credit card company to take action. </p>
<p>A fraud detection system deals with massive amounts of data. A fraud detection classifier, as described in this chapter, will sift through a dataset of transaction data and process it. Spark's streaming capabilities allow us to detect outliers, samples in our dataset whose values do not fall within a normal, anticipated<span> range of values. Detecting such values and generating a set of predictions flagging fraud is the emphasis of the fraud detection problem described in this chapter.</span></p>
<p>We will evaluate the performance of the algorithm and look at whether it flags or tags non-fraudulent samples as fraudulent or fraudulent samples as fraudulent, by computing metrics, such as precision, recall, and a harmonic mean of the precision and recall, known as the F1 score.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Precision, recall, and the F1 score</h1>
                </header>
            
            <article>
                
<p>The following are important:</p>
<ul>
<li>The F1 measure</li>
<li>Error term</li>
</ul>
<p><span>In terms of a mathematical function, an F1 score can be defined mathematically as:</span></p>
<pre><span>2 * precision recall / precision + recall</span></pre>
<p><span>We will talk a little about the F1 score or measure. The error term is denoted by the Epsilon symbol <strong>(ε)</strong>. Central to all this is the </span><span>labeled input points in our unbalanced dataset. We are going to optimize the Epsilon parameter. How exactly do we do this? </span><span>Let's first find the best possible F1 score. </span><span>What is an Epsilon? In statistics, it is an <strong>error term</strong>. A </span><span>measurement may deviate from its expected value. </span><span>For example, it be the mean height of all males in a certain population. </span><span>What is its purpose? We may denote an arbitrarily small, positive number as <strong>ε</strong>. </span><span>Before computing the Epsilon, let's persist the testing dataframe. </span><span>We have the following tasks cut out for us:</span></p>
<ul>
<li><span> Write a function to help us calculate the best Epsilon and the best F1 score</span></li>
<li>Understand what the F1 score is</li>
</ul>
<p><span>The maximum possible value that the F1 measure can take is 1. </span><span>It denotes the level of correctness of the classification process by the classifier, that is, the proportion of samples or instances </span><span>that are classified with a high degree of correctness or precision. </span><span>It </span><span>also tells us how robust (or not) the classifier is—whether the classifier missed classifying only a small number of samples or more.</span><span> The F1 score is like a balanced mean between the precision and recall.</span></p>
<p><span>F1 becomes more important in an unbalanced class situation. It is a more practical measure than <strong>accuracy</strong>. </span><span>Even though accuracy is more intuitive, and because of the fact </span><span>that both false positives and false negatives are considered, a weighted score such as F1 becomes more meaningful in understanding the degree of </span><span>correctness of the classifier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection</h1>
                </header>
            
            <article>
                
<p class="mce-root">Carefully selecting features is a crucial step in the formulation of a fraud detection program. Selecting many features or features, that does not contribute in a meaningful way, may impact the performance or skew predictions.</p>
<p class="mce-root">So, if we wish to flag fraudulent transactions, we should start small and build a system with two features only that we deem to meaningfully contribute to the classification. We choose two features that we represent in the dataset as columns of double values in a comma-separated file. These features are as follows:</p>
<ul>
<li class="mce-root"><strong>Transaction</strong>: Money spent on buying a certain commodity or service</li>
<li class="mce-root"><strong>Distance</strong>: A geographical distance from the address of the cardholder on file, or a general distance outside of the perimeter defined by the zip code of the cardholder</li>
</ul>
<p class="mce-root">That said, the goal for our fraud detection algorithm is that, with the feature selection process in place, we want to process the data, crunch all of the data points in our dataset, and flag potential fraud. This is a good place to bring in the Gaussian Distribution function, the basis for how we implement our fraud detection model. We need to talk a little more about this equation. It will help us understand exactly what our algorithm does and why it does what it does. In the next section, we will talk about the Gaussian Distribution function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Gaussian Distribution function</h1>
                </header>
            
            <article>
                
<p>The Gaussian Distribution function is also known as a <strong>bell curve</strong> or a <strong>normal distribution curve</strong>. The bell curve has the following characteristics:</p>
<ul>
<li>This kind of distribution (of data) is said to be a continuous distribution.</li>
<li>Data is spread out in this curve so that it converges around the bell portion (the highest point) of the curve, rather than to the left or the right. This center at the highest point is also the mean of the curve.</li>
<li><span>The highest point on the bell curve corresponds to the highest probability of an occurrence and, as the curve tapers off, the probability of occurrences slides down to positions on either side of the curve on its slopes.</span></li>
<li>Because of this property, the bell curve is also known as a normal distribution curve. All it needs are the standard deviation and the (population) mean.</li>
<li>In a normal distribution, the three statistics, mean, mode and median, all bear the same value.</li>
<li>The normal distribution curve is plotted with values of probability densities (a.k.a. normal frequencies). Referring to the figure present in <em>Project overview—problem formulation</em> section, the following are the meanings of the symbols in the equation:</li>
<li style="padding-left: 30px">µ = Mean of the population</li>
<li style="padding-left: 30px">σ = Standard deviation</li>
<li style="padding-left: 30px">x = Plotted on the <em>x</em> axis, this represents a continuous random variable</li>
<li style="padding-left: 30px">e = Natural logarithmic base, with a value of 2.71</li>
<li style="padding-left: 30px">π = 3.1415</li>
</ul>
<ul>
<li>The mean is simply a net value that is equal to the sum of all values of the data points, divided by the number of data points.</li>
<li>It is to be noted that <em>y</em> is nothing but <kbd>f(x)</kbd> or <kbd>p(x)</kbd>, the values of which are plotted on the <em>y</em> axis of the bell curve.</li>
</ul>
<p>The following diagram illustrates a bell curve:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/42d2d95d-7717-4897-862c-42ab143de09f.jpg" style="width:34.75em;height:27.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Bell curve</div>
<p><span>Non-fraudulent data comprises the bulk of our data. Such data is clustered in or close to the peak of the bell curve. In general terms, the top of the bell curve represents an event or a data point with the highest probability of occurrence. The tapering edges of the curve are where anomalies or outliers indicating fraud are found.</span></p>
<p class="mce-root"/>
<p>We mentioned fraud detection as a classification problem in a sense. It falls under the banner of anomaly detection. The table that follows describes the fundamental differences between a classification task and an anomaly detection task: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/24625414-7a53-44f6-95dc-15fd02b34a56.jpg" style="width:40.33em;height:34.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Classification versus anomaly identification task</div>
<p>Looking at the preceding table, it is clear that the reasons that stand out to justify the use of an anomaly identification system are as follows:</p>
<ul>
<li>Samples that may be anomalous in one dataset may not be anomalous when they are new, incoming samples in another dataset of financial transactions.</li>
<li>On the other hand, consider a breast cancer sample in an experimental unit of breast cancer samples that are classified as <strong>malignant</strong>. If the same sample is an incoming sample to experimental unit 2, the result of the classification will be the same.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Where does Spark fit in all this?</h1>
                </header>
            
            <article>
                
<p>Whether you run Spark locally or in an environment where you have a cluster operating several distributed nodes, Spark will ramp up. In a local Spark environment, Spark will treat CPU cores as resources in a cluster. </p>
<p>The Gaussian Distribution algorithm is worth looking into. Let's see what our approach should be in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fraud detection approach</h1>
                </header>
            
            <article>
                
<p><span>The following diagram illustrates the high-level architecture for our fraud detection pipeline:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/065ef341-edbd-48f7-83fd-8348c1fdd32a.png" style="width:37.58em;height:31.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">H<span>igh-level architecture for the fraud detection pipeline</span></div>
<p>The following is a quick overview of the fraud detection process:</p>
<ol>
<li>First, we compute statistics on the<span> </span><span>training set, which serves as a cross-validation dataset. We are interested in the m</span>ean and standard deviation from the statistical numbers.</li>
<li>Next, we want to compute the net <strong>probability density function</strong> (<strong>PDF</strong>) for each sample in our cross-validation set.</li>
<li>We derive the net probability density as a product of the individual probability densities. </li>
<li>Inside the algorithm, we compare a PDF value with an <strong>Error Term</strong> value to determine whether that sample represents an outlier, a potential <strong>Fraud</strong> transaction.</li>
<li>We optimize our classification process by having the algorithm executed on a cross-validation dataset.</li>
<li>As part of the optimization process, our algorithm computes the best possible value for the <strong>Error Term</strong>, an optimal value for the <strong>Error Term</strong> (denoted by an Epsilon) corresponding to a computed value of the <strong>F1</strong> score that is the highest. The algorithm, after repeated iterations, will come up with this highest score of the Epsilon.</li>
<li>A look at the dataset tells us that the most transaction data points fall within the range of 55-105 dollars. These transactions occurred within a radius of two to seven miles.</li>
<li>Spark will run this fraud detection program and extract a certain number of potential <strong>Fraud</strong> data points, for example, a good split dataset to use.</li>
<li> A breakup of the dataset could be as follows:
<ul>
<li>65% as training examples to train the model on</li>
<li>35% as a cross-validation set with instances of potential fraud in it</li>
</ul>
</li>
<li>Evaluating the performance of our fraud detection algorithm is not done by the accuracy metric. The reason lies in the fact that if only a handful of samples exist that should be flagged as fraudulent, the algorithm that successfully flagged non-fraudulent samples may fail to flag the ones that are indeed potentially fraudulent.</li>
<li>Instead, we will compute the precision and recall metrics and, consequently, the F1 measure as a way to evaluate the performance of the fraud detection classifier.</li>
</ol>
<p>Now we will look at an overview of our project, where we formulate the problem at hand, mostly in mathematical terms.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project overview – problem formulation</h1>
                </header>
            
            <article>
                
<p>Here is a helpful flowchart formulates the fraud detection problem at hand:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/36e52fd5-e8f2-4634-aac4-6c074b1f698a.jpg" style="width:37.33em;height:43.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Fraud detection flowchart</div>
<p><br/>
That said, let's get started. We do so by setting up an implementation infrastructure first.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p>In this section, we will talk about setting up an implementation infrastructure or using the existing infrastructure from previous chapters. The following upgrades to your infrastructure are optional but recommended. </p>
<p>Starting in <a href="dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml">Chapter 3</a>, <em>Stock Price Predictions</em>, we set up the <strong>Hortonworks Development Platform</strong> <span>(<strong>HDP</strong>)</span> Sandbox as a virtual machine. That said, three kinds of (isolated) HDP Sandbox deployments are possible. Of the three, we will only talk about two of them and those are:</p>
<ul>
<li><strong>Virtual machine environment (with Hypervisor) for Sandbox deployment:</strong> HDP Sandbox running in an Oracle VirtualBox virtual machine.</li>
<li><strong>A cloud-based environment for Sandbox deployment:</strong> This option is attractive for users that have host machine memory limitations. The Sandbox runs in the cloud as opposed to a virtual machine that runs on your host machine.</li>
</ul>
<p>With that opening point made, you can always run the fraud detection system code on the Spark shell. You have two options here:</p>
<ul>
<li>Use <strong>Simple Build Tool</strong> (<strong>SBT</strong>) to build and deploy your application in your Spark environment</li>
<li>Open a Spark shell, develop interactively, and run it inside the shell</li>
</ul>
<p>Last, but not least, you need the following software to simply launch the Spark shell and develop locally:</p>
<ul>
<li>Spark 2.3</li>
<li>Scala 2.11.12</li>
<li>SBT 1.0.4</li>
<li>IntelliJ IDEA 2018.1.5 Community Edition</li>
<li>At least 16 GB of RAM; 32 GB is even better.</li>
</ul>
<p><span>Please refer back to the <em>Setting up prerequisite software</em> section in <a href="4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml">Chapter 1</a>, <em>Predict the Class of a Flower from the Iris Dataset</em>. That sets us up with Java, Scala, and Spark, allowing us to use the Spark shell for interactive development.</span></p>
<p>In the next section, we will explain how to set up the Hortonworks Sandbox deployment on the Microsoft Azure Cloud, thereby proceeding to the implementation part.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up Hortonworks Sandbox in the cloud</h1>
                </header>
            
            <article>
                
<p>You might ask, why Microsoft Azure? Like any popular cloud services provider out there (and Google Compute Cloud is another solid offering), Azure prides itself as a set of robust cloud services that lets individual users and organizations develop and provision their applications on the cloud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating your Azure free account, and signing in</h1>
                </header>
            
            <article>
                
<p>The following are the steps to create an account:</p>
<ol>
<li>To get started, head over to the <a href="https://azure.microsoft.com/en-us/">https://azure.microsoft.com/en-us/</a> web address. Click on the <span class="packt_screen">Start free</span> <span>button. Doing so will take you to the account login screen. If you do not have an account, set one up. This process only gives you a new Microsoft Azure account, not an actual cloud account; at least not yet.</span></li>
</ol>
<ol start="2">
<li>Enter a password that you would like to choose with your new Microsoft Azure Cloud account.</li>
</ol>
<ol start="3">
<li><span>Next, head over to your email account, and verify your email address by entering the security code.</span></li>
</ol>
<ol start="4">
<li>If everything went well, your new Azure account is ready to use. </li>
</ol>
<p>The next step is accessing the Azure Marketplace. In this marketplace, we will proceed with further steps, such as deployment. For now, let's locate the marketplace.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Azure Marketplace</h1>
                </header>
            
            <article>
                
<p>The following are the steps involved:</p>
<ol>
<li>Head over to <span class="packt_screen">Azure</span> <span class="packt_screen">Marketplace</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a105cc3d-59a2-4601-a9ea-a0bc27b68e72.jpg" style="width:44.17em;height:19.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Looking for Azure Marketplace</div>
<ol start="2">
<li>After clicking on <span class="packt_screen">Azure Marketplace</span>, type in <kbd>Hortwonworks</kbd> in the search box on the right, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f4c527e1-8eb2-4aca-b6b9-35a61dc39dc1.jpg" style="width:38.17em;height:17.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Searching for the Hortonworks Data Platform link</div>
<p style="padding-left: 60px">Click on the <span class="packt_screen">Hortonworks Data Platform</span> link, as shown in the preceding screenshot. This takes you to the HDP Sandbox page. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The HDP Sandbox home page</h1>
                </header>
            
            <article>
                
<p>On the HDP page, here is what you can expect to find:</p>
<ol>
<li>Sign in to the Microsoft Azure Marketplace portal.</li>
<li>Kick off the Sandbox creation process, and the follow-through steps. </li>
<li>After the Sandbox creation has finished, deployment should follow. All the subsequent steps will be ex<span>plained as we work through this.</span></li>
</ol>
<p>Please perform the following steps to meet the preceding expectations:</p>
<ol>
<li>For now, click on the <span><span class="packt_screen">GET IT NOW</span> </span><span>button, as shown in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8111ec04-79d3-4679-9858-ec1503014bd9.jpg" style="width:44.17em;height:25.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The HDP GET IT NOW page</div>
<p class="mce-root"/>
<ol start="2">
<li>After clicking on the <span class="packt_screen">GET IT NOW</span> blue button, the next thing likely to happen is a dialog asking you to sign in to Microsoft Azure:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c16dbbda-e67d-4fa4-bab4-d14a3e341625.jpg" style="width:48.17em;height:26.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"> Sign-in page</div>
<ol start="3">
<li>The sign-in process takes you to another page, a page that lists a form where you need to enter details, such as your name, work email, job role, country/region, and phone number. You will be redirected to the Azure portal, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d5bc5154-adc2-45f5-9d8e-16d021ac4bad.jpg"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The Welcome to Microsoft Azure portal page</div>
<p style="padding-left: 60px">On the welcome screen of your portal, you can take the tour if you like, or simply click <span class="packt_screen">Maybe later</span> and get down to business. Let's get on with the business of getting the Sandbox deployed on Azure.</p>
<p class="mce-root"/>
<ol start="4">
<li>The next step is to locate the blue <span class="packt_screen">Create</span> button, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/54a09a37-be44-4144-807f-2e63895701b3.jpg" style="width:47.75em;height:28.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Create button screenshot</div>
<ol start="5">
<li>Now, we will finally get started on the Sandbox deployment process. Note that clicking on the <span class="packt_screen">Create</span> button does not actually start the deployment process right away. The first order of business is creating a virtual machine:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f7451809-8065-46bc-9533-2cd30bef2244.jpg"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The creation of virtual machines screenshot</div>
<p>Let's get started on a virtual machine now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation objectives</h1>
                </header>
            
            <article>
                
<p>The following implementation objectives cover the steps required to implement the Gaussian Distribution algorithm. We will perform the preliminary steps, such as <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>) once, and develop the implementation code. The breakdown is as follows:</p>
<ol>
<li>Get the breast cancer dataset from the <strong>UCI Machine Learning Repository</strong>.</li>
<li>Carry out the preliminary EDA in the Sandbox Zeppelin Notebook environment (or Spark shell), and run a statistical analysis.</li>
<li>Develop the pipeline incrementally in your local Spark shell, or in a Zeppelin Notebook on your host machine, managed virtual machine, or your virtual machine on the Azure Cloud. Or simply run your Spark fraud detection application as an SBT application and deploy it in Spark by creating an Uber JAR using <kbd>spark-submit</kbd>.</li>
</ol>
<ol start="4">
<li>Flesh out your code in IntelliJ. What this means is:
<ul>
<li> Do not forget to wire up all the necessary dependencies in the <kbd>build.sbt</kbd> file.</li>
<li> Interpret the classification process, because you want to know how well the classifier performed, how close the predicted values are to those in the original dataset, and so on.</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation steps</h1>
                </header>
            
            <article>
                
<p>A good way to start is to download the skeleton SBT project archive file from the <kbd>ModernScalaProjects_Code</kbd> folder. </p>
<p>The step-by-step instructions are as follows:</p>
<ol>
<li>EDA on the testing (cross-validation) dataset.</li>
<li>Calculate the probability densities.</li>
<li>Generate a fraud detection model.</li>
<li>Generate scores that measure the accuracy of the model:
<ul>
<li>Compute the best F1 score</li>
<li>Compute the best error term</li>
</ul>
</li>
<li>Calculate outliers by repeatedly having the model generate predictions over each value of error term in a range. </li>
</ol>
<p>We will create a <kbd>FraudDetection</kbd> trait now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create the FraudDetection trait</h1>
                </header>
            
            <article>
                
<p class="mce-root">In an empty <kbd>FraudDetectionPipeline.scala</kbd> file, add in the following imports. These are imports that we need for <kbd>Logging</kbd>, <kbd>Feature Vector</kbd> creation, <kbd>DataFrame</kbd> and <kbd>SparkSession</kbd> respectively:</p>
<pre class="mce-root">import org.apache.log4j.{Level, Logger}<br/>import org.apache.spark.ml.linalg.Vectors<br/>import org.apache.spark.sql.{DataFrame, SparkSession}</pre>
<p class="mce-root">This is an all-important trait, holding a method for <kbd>SparkSession</kbd> creation and other code. The code from classes that extend from this trait can share one instance of a <kbd>SparkSession</kbd>:</p>
<pre class="mce-root"><span>trait FraudDetectionWrapper {</span></pre>
<p class="mce-root"/>
<p class="mce-root">Next, we need the path to the testing dataset, meant for cross-validation, which is crucial to our classification:</p>
<pre class="mce-root"><span>val trainSetFileName = "training.csv</span>"<br/></pre>
<p class="mce-root">The entry point to programming Spark with the <kbd>Dataset</kbd> and <kbd>DataFrame</kbd> API is the <kbd>SparkSession</kbd>, which creates <kbd>SparkSession</kbd> for our fraud detection pipeline, as shown in the following code:</p>
<pre class="mce-root">lazy val session: SparkSession = {<br/> SparkSession<br/> .builder()<br/> .master("local")<br/> .appName("fraud-detection-pipeline")<br/> .getOrCreate()<br/> }<br/> </pre>
<p class="mce-root">These two statements turn off <kbd>INFO</kbd> statements. Feel free to turn them on, as follows:</p>
<pre class="mce-root">Logger.getLogger("org").setLevel(Level.OFF)<br/>Logger.getLogger("akka").setLevel(Level.OFF)<br/> </pre>
<p class="mce-root">The path to the dataset file is as follows:</p>
<pre class="mce-root">val dataSetPath = "C:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter5A\\"<br/> </pre>
<p class="mce-root">Create convenience tuples for holding the name of the <kbd>features</kbd> vector column and the <kbd>label</kbd> column names <kbd><span>val fdTrainSet_EDA = ("summary","fdEdaFeaturesVectors")</span></kbd> are as follows:</p>
<pre class="mce-root">val fdFeatures_IndexedLabel_Train = ("fd-features-vectors","label")<br/>val fdFeatures_IndexedLabel_CV = ("fd-features-vectors","label")<br/> </pre>
<p class="mce-root">This method allows us to transform our cross-validation dataset into a <kbd>DataFrame</kbd>. It takes in the training <kbd>Dataset</kbd> and outputs <kbd>DataFrame</kbd>:</p>
<pre class="mce-root">def buildTestVectors(trainPath: String): DataFrame= {<br/> def analyzeFeatureMeasurements: Array[(org.apache.spark.ml.linalg.Vector,String)] = {<br/> val featureVectors = session.sparkContext.textFile(trainPath, 2)<br/> .flatMap { featureLine =&gt; featureLine.split("\n").toList }<br/> .map(_.split(",")).collect.map(featureLine =&gt; ( Vectors.dense( featureLine(0).toDouble,featureLine(1).toDouble),featureLine(2)))<br/> featureVectors<br/> }<br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root">Create <kbd>DataFrame</kbd> by transforming an array of a tuple of <kbd>Feature Vectors</kbd> and <kbd>Label</kbd>:</p>
<pre class="mce-root">val fdDataFrame = session.createDataFrame(analyzeFeatureMeasurements).toDF(fdFeatures_IndexedLabel_CV._1, fdFeatures_IndexedLabel_CV._2)<br/><br/></pre>
<p>This <kbd>package</kbd> statement is required. Place this file in a package of your choice:</p>
<pre class="mce-root"><span>package com.packt.modern.chapter5</span></pre>
<p class="mce-root">The following imports are required as we are going to pass around features as serializable vectors; therefore, we need <kbd>DenseVector</kbd>. The other imports are self-explanatory. For example, we cannot do without <kbd>DataFrame</kbd>, our fundamental unit of abstraction of data in Spark:</p>
<pre class="mce-root">import org.apache.spark.ml.linalg.DenseVector<br/>import org.apache.spark.sql.{DataFrame, Dataset, Row}<br/>import org.apache.spark.rdd.RDD</pre>
<p class="mce-root">The purpose of this program is to develop a data pipeline for detecting outliers, so-called data anomalies representing data points that point to fraud:</p>
<pre class="mce-root"><span>object FraudDetectionPipeline extends App with FraudDetectionWrapper</span> {</pre>
<p class="mce-root">The pipeline program entry point is as follows:</p>
<pre class="mce-root">def main(args: Array[String]): Unit = {</pre>
<p class="mce-root">Now, convert raw data from a training dataset to <kbd>DataFrame</kbd>. The test data resides in the <kbd>testing.csv</kbd> file that is right under the root of our SBT project folder. The training data contains two columns holding double values. The first column contains <kbd>Cost Data</kbd> and the second column contains <kbd>Distance</kbd>:</p>
<pre class="mce-root"><span>val trainSetForEda: DataFrame = session.read .format("com.databricks.spark.csv") .option("header", false).option("inferSchema", "true") .load(dataSetPath + trainSetFileName</span>)</pre>
<p class="mce-root">The raw training set <kbd>DataFrame</kbd> (<kbd>rawTrainsSetForEda</kbd>) we just obtained is meant for EDA. For example, we could inspect it for missing values or characters that don't belong. That said, we will inspect all the rows of the dataset by running the <kbd>show()</kbd> command:</p>
<pre class="mce-root"><span>cachedTrainSet.show()</span></pre>
<p class="mce-root">This builds a <kbd>DataFrame</kbd> for a testing dataset. Its main purpose is cross-validation, an important ML technique that is explained back in the theory section:</p>
<pre class="mce-root">val testingSet: DataFrame = buildTestVectors(dataSetPath + crossValidFileName)</pre>
<p class="mce-root">Display the new <kbd>DataFrame</kbd> test set:</p>
<pre class="mce-root"><span> trainSetEdaStats.show()</span></pre>
<p class="mce-root">Next, the <kbd>summary</kbd> method is used to display the results of the EDA, including standard deviation, mean, and variances. These are required in our fraud detection classification task: </p>
<pre class="mce-root"><span> val trainSetEdaStats: DataFrame = cachedTrainSet.summary(</span>)</pre>
<p>Next, persist the training set dataframe with the default storage level <kbd>(`MEMORY_AND_DISK`)</kbd>.</p>
<p class="mce-root">Now, let's display the summary of that Spark extracted for us:</p>
<pre class="mce-root">trainSetEdaStats.show()</pre>
<p class="mce-root">Extract the <kbd>summary</kbd> row containing <kbd>"mean"</kbd> for both columns:</p>
<pre class="mce-root">val meanDf: DataFrame = trainSetEdaStats.where("summary == 'mean'")</pre>
<p class="mce-root">Display the new <kbd>DataFrame</kbd>—a one-row <kbd>DataFrame</kbd>:</p>
<pre class="mce-root">meanDf.show()</pre>
<p class="mce-root">Next, convert <kbd>DataFrame</kbd> to an array of rows. Issuing a <kbd>map</kbd> function call on this array extracts the <kbd>"mean"</kbd> row into this array containing a single tuple containing the mean values of both the <kbd>Cost</kbd> and <kbd>Distance</kbd> values. The result is a <kbd>"Mean Pairs"</kbd> array containing a tuple of strings:</p>
<pre class="mce-root">val meanDfPairs: Array[(String, String)] = meanDf.collect().map(row =&gt; (row.getString(1), row.getString(2)))<br/> </pre>
<p class="mce-root">Pull out both values of <kbd>mean</kbd> from the <kbd>"Mean Pairs"</kbd> tuple:</p>
<pre class="mce-root">val transactionMean = meanDfPairs(0)._1.toDouble<br/>val distanceMean = meanDfPairs(0)._2.toDouble</pre>
<p class="mce-root">Now, we want to issue the <kbd>where</kbd> query to extract just the values of standard deviation<br/>
from the training dataset EDA statistics <kbd>DataFrame</kbd>:</p>
<pre class="mce-root"><span>val trainSetSdDf: DataFrame = trainSetEdaStats.where("summary == 'stddev' ")</span><br/> </pre>
<p class="mce-root">Display the content of this standard <kbd>DataFrame</kbd> deviations:</p>
<pre class="mce-root"><span>trainSetSdDf.show(</span>)</pre>
<p class="mce-root">We have <kbd>DataFrame</kbd> containing two standard deviation values. Extract these values into an array of tuples. This array contains just one tuple holding two string values of standard deviation:</p>
<pre class="mce-root"><span>val sdDfPairs: Array[(String, String)] = trainSetSdDf.collect().map(row =&gt; (row.getString(1), row.getString(2)))</span></pre>
<p class="mce-root">Extract the standard deviation values from the enclosing tuple. First, we need the standard deviation value for the transaction feature:</p>
<pre class="mce-root">val transactionSD = sdDfPairs(0)._1.toDouble</pre>
<p class="mce-root">Next, we want to extract the standard deviation of the distance feature:</p>
<pre><span>val </span>distanceSD = sdDfPairs(<span>0</span>)._2.toDouble</pre>
<p class="mce-root">Let's build the following tuple pair for making a <kbd>broadcast</kbd> variable:</p>
<pre class="mce-root">val meanSdTupleOfTuples = ( (transactionMean,distanceMean),(transactionSD, distanceSD) )</pre>
<p>Now, let's wrap the preceding tuple pair in <kbd>DenseVector</kbd>. Why are we doing this? Simple. We need a vector to send down into the cluster as a <kbd>broadcast</kbd> variable. Create an array of transaction vectors containing both mean and standard deviation values. What we want inside the transaction vector looks like this <kbd>[Transaction Mean, Transaction SD]</kbd>:</p>
<pre class="mce-root">val meansVector = new DenseVector(Array(meanSdTupleOfTuples._1._1, meanSdTupleOfTuples._1._2))</pre>
<p class="mce-root">Let's display the vector. Scala offers an elegant way to display the content of a collection structure:</p>
<pre class="mce-root">println("Transaction Mean and Distance Mean Vector looks like this: " + meansVector.toArray.mkString(" "))</pre>
<p class="mce-root">Create an array of distance vectors containing mean and standard deviation. And since we need a second vector, it looks like this <kbd>[Distance Mean, Distance SD)</kbd>:</p>
<pre class="mce-root"><span>val sdVector: DenseVector = new DenseVector(Array<br/>(meanSdTupleOfTuples._2._1, meanSdTupleOfTuples._2._2))</span></pre>
<p class="mce-root">Display the standard deviation vector:</p>
<pre class="mce-root">println("Distance Mean and Distance SD Vector looks like this: " + sdVector.toArray.mkString(" "))</pre>
<p class="mce-root">It is now time to broadcast the following into all the nodes in your Spark cluster:</p>
<ul>
<li class="mce-root">Mean vectors</li>
<li class="mce-root">Standard deviation vectors</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Broadcasting mean and standard deviation vectors</h1>
                </header>
            
            <article>
                
<p>The <kbd>sparkContext</kbd> variable provides a <kbd>broadcast</kbd> method:</p>
<pre class="mce-root">val broadcastVariable = session.sparkContext.broadcast((meansVector, sdVector))</pre>
<p class="mce-root">Everything we did up until now was in preparation to calculate the PDF, a value that denotes the probability of fraud. That said, we will see how to calculate a PDF in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating PDFs</h1>
                </header>
            
            <article>
                
<p class="mce-root">For each sample in the testing dataset, a PDF value is to be computed. Therefore, we will iterate over the entire dataset, and pass each feature vector into the <kbd>probabilityDensity</kbd> function. That function should, for each sample, compute the <kbd>probabilityDensity</kbd> value of type <kbd>Double</kbd>. Ultimately, we build an entire dataset containing PDF values, all of type <kbd>Double</kbd>.</p>
<p class="mce-root">The <kbd>testingDF</kbd> dataframe contains two columns:</p>
<ul>
<li class="mce-root"><kbd>feature</kbd> vector column</li>
<li class="mce-root"><kbd>label</kbd></li>
</ul>
<p class="mce-root">Therefore, inside <kbd>map</kbd>, each <kbd>labelledFeatureVectorRow.getAs()</kbd> returns the feature vector.</p>
<p class="mce-root">Next, extract the features vector out of the testing dataframe. For each sample in the testing dataset, a PDF value is to be computed. Therefore, we will iterate over the entire dataset, and pass each feature vector into the <kbd>probabilityDensity</kbd> function. That function should, for each sample, compute the <kbd>probabilityDensity</kbd> value of type <kbd>Double</kbd>.<br/>
Ultimately, we build an entire dataset containing probability density function values, all of type <kbd>Double</kbd>. The following are two datasets that are extracted from the EDA dataset. </p>
<p class="mce-root">The first dataset shows <kbd>mean</kbd>, while the second one shows standard deviation:</p>
<pre class="mce-root"><strong> +-------+-----------------+-----------------+</strong><br/><strong> |summary| _c0| _c1|</strong><br/><strong> +-------+-----------------+-----------------+</strong><br/><strong> | mean|97.37915046250084|6.127270023033664|</strong><br/><strong>+-------+-----------------+------------------+</strong><br/><strong> |summary| _c0| _c1|</strong><br/><strong> +-------+-----------------+------------------+</strong><br/><strong> | stddev|10.83679761471887|3.2438494882693900|</strong><br/><strong> +-------+-----------------+------------------+</strong></pre>
<p class="mce-root">We need <kbd>implicits</kbd> here <span>to account for the fact that an implicit encoder is needed to convert <kbd>DataFrame</kbd> </span><span>into <kbd>Dataset</kbd> holding doubles. These <kbd>implicits</kbd> are provided to us by our <kbd>SparkSession</kbd> instance <kbd>session</kbd></span>, as shown here:</p>
<pre class="mce-root">import session.implicits._</pre>
<p class="mce-root">Iterate over the testing dataset, and for each feature vector row inside it. Apply a method to calculate a probability density value. A dataset of doubles is returned that has with in it double values of computed probabilities:</p>
<pre><span>val fdProbabilityDensities: DataFrame = testingDframe.map(labelledFeatureVectorRow =&gt; probabilityDensity( labelledFeatureVectorRow.getAs(0) /* Vector containing 2 Doubles*/ , broadcastVariable.value) ).toDF("PDF")</span></pre>
<p class="mce-root">Display the dataset of probability values, as shown here:</p>
<pre>fdProbabilityDensities.show()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">F1 score</h1>
                </header>
            
            <article>
                
<p><span>Since our fraudulent class is the important one, we are going to need the following to help us choose a classifier that has the best F1 score as follows</span></p>
<ul>
<li class="mce-root">Labeled data is the test <kbd>DataFrame</kbd>—<kbd><kbd>testingDf</kbd></kbd></li>
<li class="mce-root">PDF—a product of probabilities computed in the <kbd>probabilityDensity</kbd> function</li>
</ul>
<p class="mce-root">Keeping in mind that we need labeled data (points) to arrive at the best F1 score, the following background information is helpful as follows:</p>
<ul>
<li class="mce-root">What is the role of cross-validation? To understand cross-validation, we revisit the validation process, where a subset of the samples from the training set is used to train the model. Cross-validation is an improvement over validation, because of the fact that there are more observations available for the model to be fitted. Cross-validation becomes an attractive option because we are now able to pick from several models.</li>
<li class="mce-root">What are the prerequisites before calculating the Epsilon and the F1 score? Those prerequisites are:</li>
<li style="padding-left: 30px">The step size = <em>the maximum of probability density - the minimum probability density / 500</em></li>
<li style="padding-left: 30px">To arrive at the maximum value of probability, we need calculate the probability density value for each sample in the testing dataset and then derive the maximum</li>
<li style="padding-left: 30px">Labeled data points and features in the form of vectors:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5a4bcc95-97b3-4a21-abec-4d3ccce167ec.png" style="width:20.17em;height:18.58em;"/> </p>
<div class="packt_figref CDPAlignCenter CDPAlign">Labeled data points and features in the form of vectors</div>
<p>For each labeled data point, one PDF value needs to be computed. This requires prior knowledge of the following statistics:</p>
<ul>
<li>Mean cost, mean distance</li>
<li>The standard cost deviation and, the standard deviation of the instance</li>
<li>The data point itself</li>
</ul>
<p>Each labeled data point (or feature) has two mean values and two standard deviations value. One labeled data point (feature) has one cost value and one distance value. We will take both mean and standard deviation into account and calculate the PDF for that labeled data point.</p>
<p>It turns out that each feature pair returns a probability density value pair, of course. We take the product of both probabilities in this pair and return a combined probability value as the PDF for that particular feature row.<br/>
<br/>
We have enough information to calculate the combined probability value:</p>
<pre><span>def </span>probabilityDensity(labelledFeaturesVector: Vector, broadcastVariableStatsVectorPair: (Vector <span>/ Transactions /</span>, Vector <span>/ Distance / </span>)): Double = {<br/>}</pre>
<p><span>The strategy is to convert the labeled feature passed in into an array and then invoke a <kbd>map</kbd> operation on it. </span><span>We want the probability density for each data point. Each data point is like this, <kbd>{ feature 1, feature 2}</kbd></span><span>, where <kbd>feature 1</kbd> has one mean, and one standard deviation, and <kbd>feature 2</kbd> has the same. T</span><span>o do this, we need to apply the mean and standard deviation of the entire dataset.</span></p>
<p>Inside the PDF, write the following code. Define an inner function inside called <kbd>featureDoubles</kbd>:</p>
<pre>def featureDoubles(features: Array[Double],<br/>                    transactionSdMeanStats: Array[Double],                  <br/>distanceSdMeanStats: Array[Double]): List[(Double, Double, Double)] = {    }</pre>
<p>Inside the inner function, place the following code. The idea is to assemble a list of tuples that looks like this:</p>
<pre><strong>(93.47397393,79.98437516250003,18.879)   (6.075334279,5.13..,1.9488924384002693)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the first tuple, <kbd>93.47397393</kbd> is the transaction feature value of the first row, <kbd>79.98437516250003</kbd> is the mean of all the transactions, and <kbd>18.879</kbd> is the standard deviation of all the transactions.</p>
<p>In the second tuple, <kbd>6.075334279</kbd> is the distance feature value of the first row, <kbd>5.13..</kbd> is the mean of all the distances, and <kbd>1.9488924384002693</kbd> is the standard deviation of all the distances. </p>
<p>The goal then is to calculate the PDF of each data point. Since there are two features, there are two PDFs per data point. The combined probability then is the product of the two.</p>
<p>What we first want is a tuple containing our testing <kbd>DataFrame features</kbd> in the form of an <kbd>Array[Double]</kbd>, a transaction <kbd>DataFrame</kbd> containing mean and standard deviation in the form of an <kbd>Array[Double]</kbd>, and a distance <kbd>DataFrame</kbd> containing mean and standard deviation in the form of <kbd>Array[Double]</kbd>:</p>
<pre>(features, transactionSdMeanStats, distanceSdMeanStats).zipped.toList</pre>
<p>The inner function, <kbd>featureDoubles</kbd>, is done. Let's define a variable called <kbd>pdF</kbd>, representing a list of probability density tuples.</p>
<p>The finished function, <kbd><span>featureDoubles</span></kbd>, looks like this:</p>
<pre><span>def </span>featureDoubles(features: ...., <br/>                  transactionSdMeanStats: ..., <br/>                  distanceSdMeanStats: ...): <span>List</span>[(Double, Double, Double)] = {<br/>  (features, transactionSdMeanStats, distanceSdMeanStats).zipped.toList)<br/><br/>}</pre>
<p>Next, we need a PDF calculator that we call <kbd>pDfCalculator</kbd>. <kbd>pDfCalculator</kbd> <span>is a name that represents a <kbd>List</kbd> of three tuples per row in the test dataset; each tuple contains three doubles. </span><span>What we want inside each tuple looks like this: a transaction value, a transaction mean, and a transaction sd. Since there is a second tuple, </span><span>the second tuple looks like this: (distance value, distance mean, and distance sd). </span><span>When <kbd>map</kbd> is invoked, each tuple in turn inside the list (of tuples) is operated upon. Three</span><span> values inside the tuple are there for a reason. All three are needed to calculate the probability density of </span><span>one feature, as follows:</span></p>
<pre>val pDfCalculator: List[(Double, Double, Double)] = featureDoubles( <br/> labelledFeaturesVector.toArray, <br/> broadcastVariableStatsVectorPair._1.toArray, <br/> broadcastVariableStatsVectorPair._2.toArray)</pre>
<p class="mce-root"/>
<p>In the next line of code, we will carry out a <kbd>map</kbd> operation. Inside the <kbd>map</kbd> operation, we will apply a function that returns <kbd>probabilityDensityValue</kbd>. To do this, we will fall back on the <kbd>NormalDistribution</kbd> class in the Apache Commons Math library. The constructor to the <kbd>NormalDistribution</kbd> class requires mean and standard deviation and the data point itself. There are two features in a single feature row. That feature row contains two columns—<kbd>Transaction</kbd> and <kbd>Distance</kbd>. Therefore, <kbd>map</kbd> will successively calculate the probability density value for both data points, a <kbd>Transaction</kbd> data point, and a <kbd>Distance</kbd> data point, respectively:</p>
<pre>val probabilityDensityValue: Double = pDfCalculator.map(pDf =&gt; new NormalDistribution(pDf._2,pDf._3).density(pDf._1)).product</pre>
<p>The <kbd>probabilityDensity</kbd> function in its final form looks something like this:</p>
<pre>def probabilityDensity2(labelledFeaturesVector: ----,            broadcastVariableStatsVectorPair: (----,----)): Double = {<br/><br/>  def featureDoubles(features: -----, <br/>                    transactionSdMeanStats: ----, <br/>                    distanceSdMeanStats: -----): List[(Double, Double, Double)] = {<br/><br/> A tuple converted to a  List[(Double, Double, Double)]<br/>(Feature Vector, Mean and Standard Deviation of Transaction, Mean and Standard Deviation of Distance)  <br/><br/>}</pre>
<p>Finally, we want the <kbd>probabilityDensity</kbd> function to return the probability density value computed by <kbd>val probabilityDensityValue</kbd>.</p>
<p>With the probability density calculation behind us, we will now shift our attention to calculating the best error term. The error term is denoted by the Greek letter, Epsilon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the best error term and best F1 score</h1>
                </header>
            
            <article>
                
<p>In this section, we will write a function to calculate:</p>
<ul>
<li>The best error term (also known as the Epsilon)</li>
<li>The best F1 score</li>
</ul>
<p class="mce-root"/>
<p>We start by defining a function called <kbd>errorTermCalc</kbd>. What parameters does it require? It is apparent that we need two parameters: </p>
<ul>
<li>Our cross-validation dataset—<kbd>DataFrame</kbd></li>
<li><kbd>DataFrame</kbd>, containing probability densities</li>
</ul>
<p>There you go. We now have a function called <kbd>errorTermCalc</kbd> that takes two parameters and returns both the best error term and the best F1.</p>
<p>Why are these numbers important? To answer that question, we want to detect the outliers first. These are labeled data points that indicate fraud. Generating a new dataframe that classifies labeled data points as either fraudulent or not is the first step before we get down to calculating the best error term and best F1. </p>
<p>These are:</p>
<ul>
<li>Smallest of all the PDFs—<kbd>pDfMin</kbd></li>
<li>Largest of all the PDFs—<kbd>pDFMax</kbd></li>
</ul>
<p>The algorithm inside the code starts by assigning a baseline value, <kbd>pdFMin</kbd>, to the best error term. It then loops through to <kbd>pdfMax</kbd> in terms of a carefully selected step size. Remember, we want the best F1 score and the best way to do that is to assign <kbd>0</kbd>, the worst value that any F1 score can potentially take.</p>
<p>The algorithm then works its way through the range of PDF values and arrives at the final values of both the b<span>est error term and the best F1 score, respectively</span>. Basically, these final values are obtained starting with the following primary checks:</p>
<ol>
<li>Is an intermediate calculated value of the best F1 score greater than 0?</li>
<li>Is the value of any error term less than the probability density value for a certain labeled data point?</li>
</ol>
<div class="packt_infobox">Remember, there is a probability density for every data point; so we are cycling through the entire (cross-) validation dataset.</div>
<p>If the test in primary check 1 passes, the best F1 at that point is updated to that <span>intermediate calculated value of the best F1 score. </span>PDF step-wise values are compared to the predefined error term. If the PDF is less than the predefined Epsilon, that data point becomes a predicted fraudulent value.</p>
<p class="mce-root"/>
<p>The definition for the <kbd>errorTermCalc</kbd> function looks like this:</p>
<pre>private def errorTermCalc(testingDframe: DataFrame, probabilityDensities: DataFrame/*Dataset[Double] */) = { }</pre>
<p>We will start fleshing out of details inside the curly braces of the new function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximum and minimum values of a probability density</h1>
                </header>
            
            <article>
                
<p>Here is how we extract the smallest and largest values of the probability density:</p>
<pre>val maxMinArray: Array[Double] = probabilityDensities.collect().map(pbRow =&gt; pbRow.getDouble(0) )</pre>
<p><span>We need a reasonable, carefully selected step size for the error term. That is what we will be doing in the next step.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step size for best error term calculation</h1>
                </header>
            
            <article>
                
<p>Now, let's define a <kbd>step</kbd> size to calculate the best Epsilon:</p>
<pre><span>val stepsize = (maxMinPair._1 - maxMinPair._2) / 1000.0</span></pre>
<p>We need a loop for the algorithm to step through and calculate the <kbd>labelAndPredictions</kbd> dataframe at each <kbd>step</kbd> size value of the error term. This will also help us to find the best F1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A loop to generate the best F1 and the best error term</h1>
                </header>
            
            <article>
                
<p>Let's find the best F1 for different Epsilon values:</p>
<pre>for (errorTerm &lt;- maxMinPair._2 to maxMinPair._1 by stepsize) {</pre>
<p>Broadcast the error term into Spark. Create the <kbd>broadcast</kbd> variable first:</p>
<pre class="mce-root">val broadCastedErrorTerm:Broadcast[Double] = session.sparkContext.broadcast(errorTerm)<br/><br/>val broadcastTerm: Double = broadCastedErrorTerm.value</pre>
<p class="mce-root">Generate predictions here. If the <kbd>Double</kbd> value in the probability densities dataframe happens to be less than the <kbd>broadCastedErrorTerm</kbd>, that value is flagged as <kbd>fraud</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root">It is possible you may run into the following error:<br/>
<kbd>Unable to find encoder for type stored in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._ Support for serializing other types will be added in future releases</kbd>.</p>
<p class="mce-root">To get around this problem, we add the following <kbd>import</kbd> statement:</p>
<pre class="mce-root"> import session.implicits._</pre>
<p><span>For data of a certain datatype to be put away in a new <kbd>DataFrame</kbd>, Spark wants you to pass in appropriate <kbd>Encoders</kbd>. With that out of the way, </span>let's get down to generating predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating predictions – outliers that represent fraud</h1>
                </header>
            
            <article>
                
<p>We start by transforming the <kbd>probabilityDensities</kbd> dataframe from before:</p>
<pre class="mce-root"> val finalPreds: DataFrame= probabilityDensities.map { probRow =&gt;<br/> if (probRow.getDouble(0) &lt; broadcastTerm) {<br/> 1.0 /* Fraud is flagged here */<br/> } else 0.0<br/> }.toDF("PDF")</pre>
<p>Now, let's create a new dataframe with two dataframes—the testing dataframe, and the final predictions dataframe. Drop the <kbd>"label"</kbd> column in the testing dataframe and do a cross-join with the <kbd>finalpreds</kbd> dataframe. Do not forget to persist the new dataframe with the default storage level <kbd>(MEMORY_AND_DISK)</kbd>:</p>
<pre class="mce-root">val labelAndPredictions: DataFrame = testingDframe.drop("label").crossJoin(finalPreds).cache()<br/> println("Label And Predictions: " )<br/> labelAndPredictions.show()</pre>
<p>Next, we want to generate the best error term and the best F1 measure. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating the best error term and best F1 measure</h1>
                </header>
            
            <article>
                
<p>In this section, we want to come up with the number of false positives, the number of true positives, and the number of false negatives. First, we want to know how many false positives there are: </p>
<pre class="mce-root">val fPs = positivesNegatives(labelAndPredictions, 0.0, 1.0)<br/>println("No of false negatives is: " + fPs)</pre>
<p class="mce-root">Now, we want to know how many true positives there are:</p>
<pre class="mce-root">val tPs = positivesNegatives(labelAndPredictions, 1.0, 1.0)</pre>
<p class="mce-root">We also want to know how many false negatives exist:</p>
<pre class="mce-root">val fNs = positivesNegatives(labelAndPredictions, 1.0, 0.0)</pre>
<p class="mce-root">Now that we have <kbd>fNs</kbd>, <kbd>tPs</kbd>, and <kbd>fPs</kbd>, we can calculate the <kbd>precision</kbd> and <kbd>recall</kbd> metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing to compute precision and recall</h1>
                </header>
            
            <article>
                
<p class="mce-root">Here are the lines of code that implement a simple mathematical equation to come up with the precision and recall scores.</p>
<p class="mce-root">Let's calculate <kbd>precision</kbd> now:</p>
<pre class="mce-root"> val precision = tPs / Math.max(1.0, tPs + fPs) </pre>
<p class="mce-root">Followed by a calculation of <kbd>recall</kbd>:</p>
<pre class="mce-root">val recall = tPs / Math.max(1.0, tPs + fNs)</pre>
<p class="mce-root">We have both <kbd>precision</kbd> and <kbd>recall</kbd>. This gives us what we need to calculate the F1 score or the <kbd>f1Measure</kbd>, as follows:</p>
<pre class="mce-root">val f1Measure = 2.0 * precision * recall / (precision + recall)</pre>
<p class="mce-root">Next, let's determine <kbd>bestErrorTermValue</kbd> and <kbd>bestF1measure</kbd>:</p>
<pre class="mce-root"><span>if (f1Measure &gt; bestF1Measure){ bestF1Measure = f1Measure bestErrorTermValue = errorTerm //println("f1Measure &gt; bestF1Measure") scores +( (1, bestErrorTermValue), (2, bestF1Measure) ) } }</span></pre>
<p class="mce-root"/>
<p>We are almost done with the calculation of the best error term (Epsilon) and the best F1 measure.</p>
<p>In the next step, we will summarize what we just did to generate the best Epsilon and the best error term.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A recap of how we looped through a ranger of Epsilons, the best error term, and the best F1 measure</h1>
                </header>
            
            <article>
                
<p>We implemented a loop just prior to arriving here. Here is a breakdown of those steps in pseudocode:</p>
<pre><span>for </span>(errorTerm &lt;- maxMinPair._2 to maxMinPair._1 by stepsize) {<br/><br/>//Step 1: We broadcast the error term (epsilon) into Spark<br/><br/>//Step 2: We generate predictions <br/><br/>//Step 3: We will crossjoin the final predictions dataframe with our initial Testing Dataframe<br/><br/>//Step 4: We calculate False Negatives, True Negatives, False Negatives and True Positives<br/><br/>//Step 5: Calculate Precision and Recall<br/><br/>//Step 6: Calculate F1<br/><br/>Step 7: Return Best Error Term and Best F1 Measure<br/><br/>}</pre>
<p>In the preceding <kbd>Step 3</kbd>, we derived the<kbd>labelsAndPredictions</kbd> dataframe. In <kbd>Step 4</kbd>, we set out to calculate the following:</p>
<ul>
<li>False positives</li>
<li>False negatives</li>
<li>True positives</li>
</ul>
<p class="mce-root"/>
<p><span>In the next section, we will implement the method called <kbd>positivesNegatives</kbd> to calculate false positives, false negatives, and true positives. </span><span>H</span>ere is a representation of the <kbd>evalScores</kbd> method function, where the algorithm does a lot of processing: </p>
<pre><span>def </span>evalScores(testingDframe: <span>DataFrame</span>,probabilityDensities: <span>DataFrame</span>): ListMap[ Int, Double] = {<br/><br/>/*<br/>  Extract the smallest value of probability density and the largest.  */<br/><span>val </span>maxMinArray: Array[Double] = probabilityDensities.collect().map(pbRow =&gt; pbRow.getDouble(<span>0</span>) )<br/> <br/><span>/*<br/>  A sensible step size<br/>*/    <br/>val </span>stepsize = (maxMinPair._1 - maxMinPair._2) / 750<span>.0<br/><br/>/*<br/>   Write the loop to calculate the best Epsilon and the best F1 at that Best Epsilon<br/>*/<br/></span><span>for </span>(errorTerm &lt;- maxMinPair._2 to maxMinPair._1 by stepsize) {<br/>   //Step 1: We broadcast the error term (epsilon) into Spark<br/><span>     val </span>broadCastedErrorTerm:Broadcast[Double] = ----<br/>    <br/>     //Step 2: We generate predictions <br/><span>   import </span><span>session</span>.implicits._<br/><span>   val </span>finalPreds: <span>DataFrame</span>= probabilityDensities.map { ...... }<br/><br/>    //Step 3: We will crossjoin the final predictions dataframe with our initial Testing Dataframe<br/><span>     val </span>labelAndPredictions: <span>DataFrame </span>= testingDframe.drop(<span>"label"</span>).crossJoin(finalPreds).cache()<br/><br/>    //Step 4: We calculate False Negatives, True Negatives, False Negatives and True Positives<br/><br/>    //Step 5: Calculate Precision and Recall<br/><span>   val </span>fPs = &lt;&lt;Invoke the positivesNegatives here &gt;&gt;<br/><span>   val </span>tPs =  &lt;&lt;Invoke the positivesNegatives here &gt;&gt;<br/>   val tPs =  &lt;&lt;Invoke the positivesNegatives here &gt;&gt;<br/><br/>    //The Precision and recall based on Step 5<br/><span>     val </span>precision = tPs / Math.<span>max</span>(<span>1.0</span>, tPs + fPs)<br/><span>     val </span>recall = tPs / Math.<span>max</span>(<span>1.0</span>, tPs + fNs)<br/><br/>   //Step 6: Calculate F1 based on results from Step 5<br/><span>    val </span>f1Measure = <span>2.0 </span>* precision * recall / (precision + recall)<br/>   <br/>    //Step 7: Return Best Error Term and Best F1 Measure<br/>     /*<br/>      //The logic to get at the Best Error Term (epsilon) and the F1 is this:<br/>      // At any point of time, in the looping process, if the F1 measure value from Step 6 is <br/>      // greater than 0,  then that F1 value is assigned to the Scala val representing the Best F1<br/>     // Both these value are added into a Scala ListMap<br/>     //When the loop is done executing we have an updated ListMap that contains two values: The Best F1     //and the Best Error Term <br/><br/>     </pre>
<p><span>Up until this point, we talked about wanting to calculate the best error term and the best F1 measure. Both of those metrics need computed values of precision and recall, which, in turn, depend on computed values of fPs, fNs, and tPs. That brings us to the next task of creating a function that calculates these numbers. That is the focus of the next step.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Function to calculate false positives</h1>
                </header>
            
            <article>
                
<p>In this <span>section, we write a <kbd>positivesNegatives</kbd> function that takes in the <kbd>labelsAndPredictions</kbd> </span><span>dataframe from <kbd>Step 3</kbd> and spits out either false positives, false negatives, or true positives, depending on what we want.</span></p>
<p><span>It also takes in two other parameters:</span></p>
<ul>
<li>A target label that can take the following values:</li>
<li style="padding-left: 30px">A value of <kbd>1.0</kbd> for true positives</li>
<li style="padding-left: 30px">A value of <kbd>0.0</kbd> for false positives</li>
<li style="padding-left: 30px">A value of <kbd>1.0</kbd> for false negatives</li>
<li>A final predicted value that can take the following values:</li>
<li style="padding-left: 30px">A value of <kbd>1.0</kbd> for true positives</li>
<li style="padding-left: 30px">A value of <kbd>1.0</kbd> for false positives</li>
<li style="padding-left: 30px">A value of <kbd>0.0</kbd> for false negatives</li>
</ul>
<p>Accordingly, here is one method that calculates all three: true positives, false positives, and false negatives:</p>
<pre><span>def </span>positivesNegatives(labelAndPredictions: <span>DataFrame </span><span>/* Dataset[(Double, Double)] */</span>, <br/> targetLabel: Double, <br/> finalPrediction: Double): Double = {<br/><br/>}</pre>
<p>The body of the method is a single line of code that calculates a <kbd>Double</kbd> value, of course: </p>
<pre>labelAndPredictions.filter( labelAndPrediction =&gt; <br/>                                                        labelAndPrediction.getAs(<span>"PDF"</span>) == targetLabel &amp;&amp; <br/>                                                        labelAndPrediction.get(<span>1</span>) == finalPrediction ).count().toDouble</pre>
<p>The finished method looks like this:</p>
<pre><span>def positivesNegatives(labelAndPredictions: DataFrame /* Dataset[(Double, Double)] */, targetLabel: Double, finalPrediction: Double): Double = {</span><br/><br/>   //We do a filter operation on our labelsAndPredictions DataFrame. The filter condition is as follows:<br/>   // if the value under the label column matches the incoming targetLabel AND the value in the predictions column matches the finalPrediction value then count the number of datapoints that satisfy this condition. This will be your count of False Positives, for example.<br/><br/>        labelAndPredictions.filter( &lt;&lt;the filter condition&gt;&gt;).count().toDouble <br/><br/>}</pre>
<p>This completes the implementation of the fraud detection system. In the next section, we will summarize what this chapter has accomplished.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Fraud detection is not a supervised learning problem. We did not use the random forests algorithm, decision trees, or <strong>logistic regression</strong> (<strong>LR</strong>). Instead, we leveraged what is known as a Gaussian Distribution equation to build an algorithm that performed classification, which is really an anomaly detection or identification task. The importance of picking an appropriate Epsilon (error term) to enable the algorithm to find the anomalous samples cannot be overestimated. Otherwise, the algorithm could go off the mark and label non-fraudulent examples as anomalies or outliers that indicate a fraudulent transaction. The point is, tweaking the Epsilon parameter does help with a better fraud detection process.</p>
<p>A good part of the computational power required was devoted to finding the so-called best Epsilon. <span>Computing the best Epsilon was one key part. The other part, of course, was the algorithm itself. </span>This is where Spark helped out a lot. The Spark ecosystem provided us with a powerful environment, letting us write code that parallelizes and orchestrates our data analytics efficiently in a distributed manner.</p>
<p><span>In the next chapter, we will carry out </span><span> data analysis tasks on flight performance data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>The following are questions that will consolidate and deepen your knowledge of fraud detection:</p>
<ol>
<li>What is a Gaussian Distribution?</li>
<li>The algorithm in our fraud detection system requires something really important to be fed into it, before generating probabilities? What is that?</li>
<li>Why is the selection of an error term (Epsilon) such a big deal in detecting outliers and identifying the correct false positives and false negatives?</li>
<li>Why is fraud detection not exactly a classification problem? </li>
<li>Fraud detection is essentially an anomaly identification problem. Can you name two properties that define anomaly identification?</li>
<li>Can you think of other applications that can leverage anomaly identification or outlier detection?</li>
<li>Why is cross-validation so important?</li>
</ol>
<ol start="8">
<li>Why is our fraud detection problem not a supervised learning problem?</li>
<li>Can you name a couple of ways to optimize the Gaussian Distribution algorithm?</li>
<li>Sometimes, our results may not be satisfactory because the algorithm failed to identify certain samples as a fraud. What could we do better?</li>
</ol>
<p>It is time to move to the last section, where we invite readers to further enrich their learning journey by referring to the resources indicated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>PayPal's data platforms carry out real-time decision making that prevents fraud. Their systems crunch several petabytes every single day. Check out <a href="https://qcon.ai/">https://qcon.ai/</a> for a recent conference on AI and ML. Study their use cases and learn how companies such as PayPal leverage the latest advances in AI and ML to help combat fraud.</p>
<p>Explore how Kafka can work with Spark to bring near real-time fraud detection to your fraud detection procedures. </p>
<p>We are all familiar with Airbnb (<a href="https://www.airbnb.com/trust">https://www.airbnb.com/trust</a>). Find out how Airbnb's trust and safety team is combating fraud, while they protect and grow their business model that is so critically based on trust.</p>


            </article>

            
        </section>
    </body></html>