- en: Monitoring, Performance, and Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now seen how to craft modern, scalable, and resilient microservices
    with Java EE. In particular, the part about adding resilience as well as technical
    cross-cutting to microservices is a topic that we want to pursue further.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise applications run on server environments remote from the users. In
    order to provide insights into the system, we need to add visibility. There are
    multiple ways to achieve this aspect of telemetry that includes monitoring, health
    checks, tracing, or logging. This chapter covers the reasoning behind each of
    these approaches and what makes sense for enterprise applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Business and technical metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to meet performance needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java Performance Diagnostic Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and sampling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why traditional logging is harmful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring, logging, and tracing in a modern world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suitability of performance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visibility in the business processes is crucial to business-related persons
    in order to see and interpret what is happening inside an enterprise system. Business-relevant
    metrics allow to evaluate the effectiveness of processes. Without visibility into
    the processes, the enterprise application acts as a black box.
  prefs: []
  type: TYPE_NORMAL
- en: Business-related metrics are an invaluable asset to business experts. They provide
    domain-specific information as to how the use cases perform. Which metrics are
    of interested obviously depends on the domain.
  prefs: []
  type: TYPE_NORMAL
- en: How many cars are created per hour? How many articles are purchased and for
    what amount? What is the conversion rate? How many users followed the email marketing
    campaign? These are examples of domain-specific key performance indicators. The
    business experts have to define these indicators for the specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: The enterprise application has to emit this information which originate from
    various points in the business processes. The nature of this information depends
    on the actual domain. In many cases, business metrics arise from domain events
    that occur during performing the business processes.
  prefs: []
  type: TYPE_NORMAL
- en: Take the number of cars that are created per hour as an example. The car creation
    use case emits a corresponding `CarCreated` domain event, which is collected for
    future statistics. Whereas calculating the conversion rate involves much more
    information.
  prefs: []
  type: TYPE_NORMAL
- en: The business experts have to define the semantics and origin behind key performance
    indicators. Defining and collecting these indicators becomes part of the use case.
    Emitting this information is a responsibility of the application as well.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to distinguish between business-motivated and technically-motivated
    metrics. Although business metrics provide insights of high value, they are directly
    impacted by technical metrics. An example of a technical metric is the service
    response time which is, in turn, affected by other technical metrics. The sub-chapter
    *Technical metrics* will examine this topic further.
  prefs: []
  type: TYPE_NORMAL
- en: Business experts, therefore, must not only care about the business aspects of
    monitoring but also the technical impact of an application's responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting business metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Business-relevant metrics allow business experts to evaluate the effectiveness
    of the enterprise system. The metrics provide helpful insights into specific parts
    of the business domain. The application is responsible for gathering business-relevant
    metrics as part of it's use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The *car manufacture* package, for example, performs business logic that can
    emit certain metrics, such as the number of cars created per hour.
  prefs: []
  type: TYPE_NORMAL
- en: From a business perspective, the relevant metrics usually originate from domain
    events. It's advisable to define and emit domain events, such as `CarCreated`,
    as part of the use case, as soon as a car has been successfully manufactured.
    These events are collected and being used to derive further information in the
    form of specific business metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CarCreated` event is fired in the boundary as a CDI event and can be observed
    in a separate statistics collector. The following code snippet shows a domain
    event fired as part of a use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The boundary fires the CDI event that notifies about a successful car creation.
    The corresponding handling is decoupled from the business process and no further
    logic is involved in this place. The event will be observed in a separate application
    scoped bean. Synchronous CDI events can define to be handled during specific transaction
    phases. The following transactional observer therefore ensures that only successful
    database transactions are measured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The event information is collected and processed further in order to provide
    the business metrics. Depending on the situation, more business-relevant data
    could be required.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the relevant information as domain events matches the business definition
    and decouples the use case from the statistics calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Besides defining domain events, the information can also be collected via cross-cutting
    components, such as interceptors, depending on the situation and requirements. In
    the simplest case, the metrics are instrumented and collected in primitives. Application
    developers have to consider bean scopes in order not to throw away collected data
    with incorrect scopes.
  prefs: []
  type: TYPE_NORMAL
- en: Emitting metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics are usually not persisted in the application but in another system that
    is part of the environment, such as external monitoring solutions. This simplifies
    the implementation of metrics; the enterprise application keeps the information
    in memory and emits the specified metrics. External monitoring solutions scrape
    and process these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: There are several techniques that can be used to emit and collect metrics. For
    example, measures can be formatted into custom JSON strings and exposed via HTTP
    endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: A monitoring solution that is part of the Cloud Native Computing Foundation,
    and, as of today, has huge momentum, is **Prometheus**. Prometheus is a monitoring
    and alerting technology that scrapes, efficiently stores, and queries time series
    data. It gathers data that is emitted by some service over HTTP in a specific
    format. Prometheus is powerful in scraping and storing data.
  prefs: []
  type: TYPE_NORMAL
- en: For graphs and dashboards for business-related information, other solutions
    can be built on top of this. A technology that works well with Prometheus and
    provides many possibilities for appealing graphs is **Grafana**. Grafana doesn't
    store time series itself but uses sources such as Prometheus to query and display
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of a Grafana dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82f83925-226d-4da5-9af3-49ca9a817d8c.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea of dashboards provides visibility for business experts and combines
    relevant information. Depending on the requirements and motivations, coherent
    information is combined into graphs that provide overviews and insights. Dashboards
    provide the ability to query and customize time series representations based on
    the target group.
  prefs: []
  type: TYPE_NORMAL
- en: Enter Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following examples show how to integrate Prometheus into Java EE. This is
    one possible monitoring solution and aims to give the readers an idea of how to
    slimly integrate business-related metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application will emit the gathered metrics in the Prometheus output format.
    The Prometheus instances scrape and store this information, as demonstrated in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bd00845-3cc1-4de5-b5b8-584a218eb27a.png)'
  prefs: []
  type: TYPE_IMG
- en: Developers can implement custom functionality to collect and emit information,
    or use Prometheus' Client API which already ships with several metric types.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple Prometheus metric types as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The one mostly used is a **counter** which represents an increasing numeric
    value. It counts the occurred events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **gauge** is a numeric value that goes up and down. It can be used for measuring
    values such as conversion rates, temperatures, or turnover.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histograms** and **summaries** are more complex metric types used to sample
    observations in buckets. They typically observe metrics distribution. For example,
    how long does it take to create a car, how much do these values vary, and how
    are they distributed?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Prometheus metric has a name and labels, which are sets of key-value pairs.
    A time series is identified by the metric's name and a set of labels. The label
    can be seen as parameters, sharding the overall amount of information.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a counter metric representation using labels is `cars_manufactured_total{color="RED",
    engine="DIESEL"}`. The `cars_manufactured_total` counter includes the total number
    of manufactured cars that are specified by their color and engine type. The collected
    metrics can be queried for the provided label information later on.
  prefs: []
  type: TYPE_NORMAL
- en: Realization with Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following statistics implementation observes the domain event specified
    earlier and stores the information in the Prometheus counter metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The counter metric is created and registered to the Prometheus Client API. Measured
    values are qualified by the car `color` and `engine` type, which are taken into
    account when scraping the values.
  prefs: []
  type: TYPE_NORMAL
- en: In order to emit this information, the Prometheus servlet library can be included.
    This outputs all the registered metrics in the correct format. The monitoring
    servlet is configured via `web.xml`. It's also possible to include a JAX-RS resource
    to emit the data by accessing `CollectorRegistry.defaultRegistry`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The emitted output will look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Java EE components, such as CDI events, support developers in integrating domain
    event metrics in a lean way. In the preceding example, the `ManufacturingStatistics`
    class is the only point that depends on the Prometheus API.
  prefs: []
  type: TYPE_NORMAL
- en: It's highly advisable to include the Prometheus Client API as a separate container
    image layer and not in the application artifact.
  prefs: []
  type: TYPE_NORMAL
- en: The monitoring solution scrapes and further processes the provided information,
    in order to gather the required business metrics. Scraping the counter of manufactured
    cars over time leads to the number of created cars per hour. This metric can be
    queried for the total number of cars as well as for specific color and engine
    combinations. The queries that define the business metrics can also be adapted
    and refined due to the requirements. The application ideally emits the needed
    atomic business-relevant metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application emits the business-relevant metrics via HTTP. The Prometheus
    instance scrapes and store this data and makes it available via queries, graphs,
    and external solutions, such as Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: In a container orchestration, the Prometheus instance runs inside the cluster.
    This removes the necessity to configure externally accessible monitoring endpoints.
    Prometheus integrates with Kubernetes in order to discover the application instances.
    Prometheus needs to access every application pod individually, since every application
    instance emits its monitoring metrics separately. Prometheus accumulates the information
    of all instances.
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus configuration is either stored in a config map or part of a base
    image. The instance is configured to access the applications and exporters every
    *n* seconds in order to scrape the time series. For configuring Prometheus, refer
    to its current documentation.
  prefs: []
  type: TYPE_NORMAL
- en: This is one possible solution for integrating business monitoring into a cloud
    native application.
  prefs: []
  type: TYPE_NORMAL
- en: Business-related metrics are advisably represented by domain events that emerge
    as part of the business use case. Integrating the chosen monitoring solutions
    should happen transparently from the domain logic, without much vendor lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: Meeting performance requirements in distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Responsiveness is an important non-technical requirement of an enterprise application.
    The system only provides business value if client requests can be served within
    a reasonable amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Meeting performance requirements in distributed systems requires to take all
    participating applications into account.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise application are often required to meet a **service level agreement**
    (**SLA**). SLAs usually define thresholds for availability or response times,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Service level agreements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to calculate and meet SLAs, it's important to consider which processes
    and applications are included in business use cases, especially in regard to synchronous
    communication. The performance of applications that synchronously call external
    systems directly depend on the performance of these calls. As mentioned before,
    distributed transactions should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: As per its nature, SLAs can only be met if all applications perform and work
    well together. Every application affects the SLAs of dependent systems. This not
    only concerns the slowest application in a system but all participating services.
  prefs: []
  type: TYPE_NORMAL
- en: For example, meeting an uptime of 99.995% per definition is not possible if
    it includes synchronous calls to two applications with each of them guaranteeing
    99.995%. The resulting SLA is 99.99%, the values of each participating system
    multiplied.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true for guaranteed response times. Every involved system slows
    down the overall response, resulting in a total response time that is the sum
    of all SLA times.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving SLAs in distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see an example how to achieve SLAs in distributed systems, assuming the
    enterprise application resides in a high performance scenario where it's crucial
    to meet guaranteed response times. The application synchronously communicates
    with one or more backend systems that provide necessary information. The overall
    system needs to meet an SLA response time of 200 milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario the backend applications support in meeting the SLA time by
    applying backpressure and preventively rejecting requests that won't meet the
    guaranteed SLA. By doing so the originating application has the chance to use
    another backend service that may respond in time.
  prefs: []
  type: TYPE_NORMAL
- en: In order to appropriately configure pooling, the engineers need to know the
    average response time of the backend system, here 20 milliseconds. The corresponding
    business functionality defines a dedicated thread pool by using a dedicated managed
    executor service. The thread pool can be configured individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration is achieved by following some steps: The engineers configure
    the maximum limit of the thread pool size plus the maximum queue size, so that
    the SLA time is *n* times the average response time. This *n*, here `10`, is the
    maximum number of requests the system will handle at a time, consisting of the
    maximum pool size and maximum queue size limit. Any request that exceeds this
    number is immediately rejected by a service temporarily unavailable response.
    This is based on the calculation that the new request will likely exceed the calculated
    SLA time of 200 milliseconds, if the current number of handled requests exceeds
    *n*.'
  prefs: []
  type: TYPE_NORMAL
- en: Immediately rejecting requests sounds like a harsh response, but by doing so,
    the client application is given the opportunity to retry a different backend without
    consuming the whole SLA time in vain in a single invocation. It's a case example
    for high performance scenarios with multiple backends where meeting SLAs has a
    high priority.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of this scenario is similar to the backpressure example in
    the previous chapter. The client uses different backends as a fallback if the
    first invocation failes with an unavailable service. This implicitly makes the
    client resilient since it uses multiple backends as fallback. The backend service
    implicitly applies the bulkhead pattern. A single functionality that is unavailable
    doesn't affect the rest of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling performance issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Technical metrics, such as response time, throughput, error rates or uptime indicate
    the responsiveness of the system. As long as the application''s responsiveness
    is in acceptable ranges, there is arguably no other metric to consider. Insufficient
    performance means that the system''s SLAs are not being met, that is, the response
    time is too high or client requests fail. Then the question arises: what needs
    to be changed to improve the situation?'
  prefs: []
  type: TYPE_NORMAL
- en: Theory of constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the desired load on the system increases, the throughput ideally increases
    as well. The theory of constraints is based on the assumption that there will
    be at least one constraint that will throttle the system's throughput. The constraints
    or bottlenecks therefore cause a performance regression.
  prefs: []
  type: TYPE_NORMAL
- en: Like a chain that is only as strong as its weakest link, the constraining resource
    limits the overall performance of the system or certain functionality thereof.
    It prevents the application from handling more load while other resources are
    not fully utilized. Only by increasing the flow through the constraining resource,
    that is, removing the bottleneck, will the throughput be increased. If the system
    is optimized *around the bottleneck* rather than removing it, the responsiveness
    of the overall system won't improve and, ultimately, it may even decrease.
  prefs: []
  type: TYPE_NORMAL
- en: It's therefore crucial to identify what the bottleneck is. The overall performance
    won't improve, before the limiting bottleneck gets targeted.
  prefs: []
  type: TYPE_NORMAL
- en: For example, throwing more CPU power at an application with high CPU utilization
    probably won't help to achieve better performance. Maybe the application isn't
    performing well because of other root causes than insufficient CPU.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to mention here that the limiting constraint likely is external
    to the application. In a single, monolithic application, this includes the hardware
    and the operating system, with all running processes. If other software running
    on the same hardware heavily utilizes the network adapter, the application's network
    I/O and overall performance will be affected as well, even if the root cause,
    the limiting constraint, isn't the in responsibility of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting performance issues therefore needs to take more into account than
    just the application itself. The whole set of processes running on the same hardware
    can have an impact on the application's performance, depending on how the other
    processes utilize the system's resources.
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed environment, performance analytics also involves all interdependent
    applications, that interact with the application, and the communication in between.
    In order to identify the constraining resource, the overall situation of the system
    has to be taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: Since the applications are interconnected, improving the responsiveness of a
    single system will affect others and can potentially even decrease the overall
    responsiveness. Again, trying to improve the wrong aspect, such as optimizing
    around the bottleneck, will not improve rather than most likely even downgrade
    the overall performance. An application that connects to an external system that
    represents the bottleneck , puts certain pressure on the external system. If the
    application's performance is tuned, rather than the external application, the
    load and pressure on the latter is increased which ultimately leads to overall
    worse responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: In distributed systems, the situation with all interdependent applications involved
    vastly complicates solving performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying performance regression with jPDM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Java Performance Diagnostic Model** (**jPDM**) is a performance diagnostic
    model that abstracts the complexity of systems. It helps interpreting the *performance
    counters* of the system and thus understanding the root cause of why we experience
    performance regression.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with identifying performance regression is that a specific scenario
    is the result of innumerable influences, many of them external to the application.
    jPDM and the resulting methodologies helps dealing with that complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of responsiveness, there is an infinite number of things that can go
    wrong, but they will go wrong in a finite number of ways. Performance regression
    can therefore be categorized into different manifestations. There will be a few
    typical forms of issues, emerging in innumerable, varying scenarios, and root
    causes. In order to identify the different categories, we will make use of the
    diagnostic model.
  prefs: []
  type: TYPE_NORMAL
- en: jPDM identifies important subsystems of our system, their roles, functions,
    and attributes. The subsystems interact with each other. The model helps to identify
    tools to measure levels of activity and interactions between the subsystems. Methodologies
    and processes that help to study and analyze systems and situations in regard
    to performance, fall out of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Subsystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The subsystems in a Java application environment are: the operating system,
    including hardware, the Java Virtual Machine, the application, and the actors.
    Subsystems utilize their corresponding, underlying subsystem to perform their
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the jPDM subsystems interact with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea6405ce-e28f-4b85-83b7-034a53748208.png)'
  prefs: []
  type: TYPE_IMG
- en: Actors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The actors are the users of the system in a broadest sense. This includes end
    users, batch processes, or external systems, depending on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: By using the system, the actors will generate work load. The properties of the
    actors include the load factor, that is how many users are involved, as well as
    the velocity, that is how fast user requests are processed. These properties influence
    the overall situation similar to all other subsystem's properties.
  prefs: []
  type: TYPE_NORMAL
- en: The actors themselves don't represent a performance issue, they simply use the
    application. That said, if the system's performance isn't met, the limiting constraint
    is not to be searched for within the actors; the actors and the load they generate
    are part of the circumstances the system has to deal with.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The enterprise application contains the business logic algorithms. Part of the
    business use cases is to allocate memory, schedule threads, and use external resources.
  prefs: []
  type: TYPE_NORMAL
- en: The application will use framework and Java language functionalities to fulfill
    this. It ultimately makes use of JVM code and configuration, directly or indirectly.
    By doing so, the application puts a certain load on the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: JVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Java Virtual Machine** (**JVM**) interprets and executes the application
    byte code. It takes care of memory management--allocation as well as garbage collection.
    There are vast optimization techniques in place to increase the performance of
    the program, such as **Just-In-Time** (**JIT**) compilation of the Java HotSpot
    Performance Engine.
  prefs: []
  type: TYPE_NORMAL
- en: The JVM utilizes operating system resources to allocate memory, run threads,
    or use network or disk I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Operating system and hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A computer's hardware components such as CPU, memory, disk and network I/O,
    define the resources of a system. They contain certain attributes, such as capacities
    or speed.
  prefs: []
  type: TYPE_NORMAL
- en: Since the hardware components represent non-shareable resources, the operating
    system provisions hardware between the processes. The operating system provides
    system-wide resources and schedule threads for the CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the model considers the overall system, including hardware.
    The enterprise application potentially doesn't run alone on the system's hardware.
    Other processes utilize hardware components and thus influence the application's
    performance. Multiple processes that simultaneously try to access the network
    will result in poorer responsiveness than running each of them independently.
  prefs: []
  type: TYPE_NORMAL
- en: jPDM instances - production situations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Specific situations in a production system, are instances of the jPDM model.
    They contain all their properties, characteristics, and specific bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Any change in one of the subsystems would result in a different situation with
    different properties and characteristics, thus in a different instance of the
    model. For example, changing the load on the system could result in a totally
    different bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: This is also the reason why performance tests in environments other than production
    will result in potentially different bottlenecks. The different environment has
    at least a different OS and hardware situation, not necessarily in the hardware
    and configuration being used, but in the whole condition of OS processes. Simulated
    scenarios such as performance tests therefore don't allow conclusions about bottlenecks
    or performance optimizations. They represent a different jPDM instance.
  prefs: []
  type: TYPE_NORMAL
- en: Since we use the model to ultimately analyze performance issues, the following
    approaches only make sense when there are actual performance issues. If there
    is no issue, that is, the defined SLAs are met, there is nothing to investigate
    or act upon.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the jPDM instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: jPDM is used to assist in investigating performance regression. Methodologies,
    processes and tools that fall out of the model help to identify limiting constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Each subsystem with its distinct set of attributes and resources plays a specific
    role in the system. We use specific tools to both expose specific performance
    metrics and monitor interactions between subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the theory of constraints, we want to investigate the limiting
    constraint of a production situation, an instance of the jPDM. The tooling helps
    with investigating. It's important for the investigation to take the overall system
    into account. The hardware is shared by all operating system processes. The dominance,
    therefore, may be caused by a single process or the sum of all processes running
    on that hardware.
  prefs: []
  type: TYPE_NORMAL
- en: First, we investigate the dominating consumer of the CPU and how the CPU is
    utilized. The CPU consumption pattern will lead us to the subsystem that contains
    the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to investigate the dominating consumer, we make use of a decision
    tree. It indicates where the CPU time is spent - in kernel space, user space,
    or idling. The following diagram shows the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9c3ebf8-92cb-4a85-b606-62beb88093dc.png)'
  prefs: []
  type: TYPE_IMG
- en: The round nodes in the graph represent the dominant consumers of the CPU. The
    colors represent the jPDM subsystems. Following the decision tree leads us to
    the subsystem that contains the bottleneck. Identifying the subsystem narrows
    down the performance regression to a specific category. We then use further tooling
    to analyze the instance of jPDM, the actual situation.
  prefs: []
  type: TYPE_NORMAL
- en: Since performance issues can originate from an infinite number of things, we
    need to follow a process to narrow down the causes. If we would not follow a process
    but blindly *peek and poke* or guess, we would not only waste time and effort
    but potentially wrongly identify symptoms as actual dominating constraints.
  prefs: []
  type: TYPE_NORMAL
- en: The dominant consumers of the CPU represent where the CPU time is spent. This
    is an important information to investigate the situation. It's not sufficient
    to solely look at the overall amount of CPU utilization. This information alone
    neither gives us much evidence of the existence of a bottleneck or how much headroom
    there is, nor does it lead to the dominating consumer. A CPU usage of 60% doesn't
    tell us whether the CPU is the constraining resource, that is whether adding more
    CPU would improve the overall responsiveness. The CPU time needs to be analyzed
    in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: First, we look at the ratio between CPU user and system time. This indicates
    whether the CPU time is spent in the kernel for longer than expected and thus
    whether the operating system is the dominating consumer of the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Dominating consumer - OS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The operating system dominates the CPU consumption when it's asked to work harder
    than it usually should. This means that too much CPU time is spent on resource
    and device management. This includes network and disk I/O, locks, memory management,
    or context switches.
  prefs: []
  type: TYPE_NORMAL
- en: If the CPU system time is more than a certain percentage value of the user time,
    the operating system is the dominating consumer. The jPDM identified 10% as a
    threshold value, based on the experience of analyzing innumerable production situations.
    That means if the CPU system time is more than 10% of the user time, the bottlenecks
    are contained in the OS subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we investigate the issue further using operating system tools,
    such as `vmstat`, `perf`, `netstat`, and others.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an enterprise application that retrieves database entries with
    a huge number of individually executed queries puts lots of pressure on the operating
    system in managing all these database connections. The overhead spent on establishing
    each and every network connection will eventually dominate the overall system
    and represent the constraining resource in the system. Investigating this situation
    thus shows a big share of CPU time spent in the kernel where the network connections
    are established.
  prefs: []
  type: TYPE_NORMAL
- en: Dominating consumer - none
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the CPU time didn't identify the OS to be the dominating consumer, we follow
    the decision tree further and analyze whether the CPU is idling. If that is the
    case, it means there is still CPU time available that cannot be consumed.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are analyzing a situation where the SLA is not met, that is, the overall
    system is in a situation where it doesn't perform well enough for the given load,
    a well-saturated situation would fully utilize the CPU. Idling CPU times thus
    indicates a liveliness issue.
  prefs: []
  type: TYPE_NORMAL
- en: What needs to be investigated is why the threads are not scheduled by the operating
    system. This can have multiple causes, such as empty connection or thread pools,
    deadlock situations, or slow responding external systems. The state of the threads
    will indicate the cause of the constraint. We again use operating system tooling
    to investigate the situation.
  prefs: []
  type: TYPE_NORMAL
- en: An example for this category of issues is when an external system that responds
    slowly is accessed synchronously. It will lead to threads that are waiting for
    network I/O and can't run. This is the difference to dominating OS consumption,
    that the thread is not actively executing work but waiting to get scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: Dominating consumer - JVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dominating consumers so far weren't contained in the application or JVM
    subsystems. If the CPU time is not overly spent in the kernel or idling, we start
    investigations in the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: Since the JVM is responsible for memory management, its performance will indicate
    potential memory issues. Mainly **Garbage Collection** (**GC**) logs, together
    with **JMX** tooling help investigate scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Memory leaks will lead to increasing memory usage and excessive garbage collector
    runs that occupy the CPU. Inefficient memory usage will equally lead to excessive
    garbage collections. The GC executions ultimately cause the JVM being the dominating
    consumer of the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: This is another example of why it's important to follow the process of the jPDM
    decision tree. The performance issue arises in high CPU usage, although the actual
    bottleneck in this case is the memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: As of today, the main cause of performance issues are related to memory, mostly
    from application logging that results in extensive string object creation.
  prefs: []
  type: TYPE_NORMAL
- en: Dominating consumer - application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the JVM analysis didn't indicate a memory issue, finally the application
    is the dominating consumer of the CPU. This means that the application code itself
    is responsible for the bottleneck. Especially applications that run sophisticated
    algorithms excessively utilize the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Application-related profiling will lead to conclusions where in the application
    the issue originates and how the issue might be resolved. This means that the
    application either contains suboptimal code or reached the possible limit with
    the given resources, and ultimately needs to be scaled horizontally or vertically.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach of solving performance issues is to try to characterize the regression
    first by investigating the situation by following a specific process. After the
    constraining resource has been identified, further steps to resolve the situation
    are applied. After potentially fixing the situation, the measurement in production
    needs to be repeated. It's important to not change behavior or configuration without
    the verification that the changes in fact provide the expected results.
  prefs: []
  type: TYPE_NORMAL
- en: The jPDM approach investigates performance regression without considering the
    application's code, by applying a uniform solving process.
  prefs: []
  type: TYPE_NORMAL
- en: What tools and metrics are needed to apply this approach?
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the system in production, tools that ship with the operating system,
    as well as Java Runtime-related tools, are useful. Since all aspects consider
    the overall system at the operating system level rather than just the application
    alone, operating system tools and lower-level metrics are more helpful than application-specific
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: However, the technical metrics of the application, such as response time or
    throughput, are the first place of focus that indicate the application's quality
    of service. If these metrics indicate a performance issue, then it makes sense
    to investigate using lower-level metrics and tools.
  prefs: []
  type: TYPE_NORMAL
- en: The next section examines how to gather the application's technical metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Technical metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technical metrics indicate the load and responsiveness of the system. Prime
    examples for these metrics are the response time, as well as the throughput, often
    gathered as requests or transactions per second, respectively. They provide information
    about how the overall system currently performs.
  prefs: []
  type: TYPE_NORMAL
- en: These metrics will ultimately have an impact on other, business-related metrics.
    At the same time, as we have seen in the previous section, these metrics are just
    indicators and themselves affected by a lot of other technical aspects, namely
    all properties of jPDM subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, an application's performance is impacted by a lot of technical influences.
    Thus, the question is, which technical metrics besides response time, throughput,
    error rates, and uptime should reasonably be collected?
  prefs: []
  type: TYPE_NORMAL
- en: Types of technical metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technical metrics are primarily concerned with the quality of the application's
    service, such as response times or throughput. They are the indicators that represent
    the application's responsiveness and may point out potential performance issues.
    The information can be used to create statistics about trends and application
    peaks.
  prefs: []
  type: TYPE_NORMAL
- en: This insight increases the likelihood of foreseeing potential outages and performance
    issues in a timely manner. It is the technical equivalent of business insights
    into the otherwise black box system. These metrics alone allow no sound conclusions
    about the root cause or constraining resources in the case of performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: Lower-level technical information includes resource consumption, threading,
    pooling, transactions, or sessions. It's again important to mention that this
    information alone does not direct the engineers to potential bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: As shown previously, it's necessary to inspect the overall situation with everything
    running on specific hardware. The operating system information provides the best
    source of information. In order to solve performance issues, the operating system,
    as well as application tools, are required to take this into account.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn't mean that the technical information emitted by the application
    or the JVM runtime has no value at all. The application-specific metrics can assist
    in solving performance issues. It's important to keep in mind that these metrics
    alone will lead to potentially wrong assumptions about what the constraining resources
    are when a system needs to be performance-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: High frequency monitoring versus sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, monitoring aims to collect technical metrics with a high frequency of
    many collections per second. The problem with this high frequency collection is
    that it heavily impacts the performance of the system. Metrics often get collected
    even if there is no performance regression.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, application-level metrics, such as resource consumption, alone
    don't help much in identifying potential performance constraints. In the same
    way, the collection disrupts the responsiveness of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of monitoring with a high frequency it's advisable to sample metrics
    with lower frequency, such as for only a few times per minute. The theory behind
    statistical populations shows that these few samples represent the population
    of data well enough.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling the information should have as little impact on the application's performance
    as possible. All subsequent investigations, or metrics querying or calculations
    should happen out-of-band; that is, outsourced to a system that does not impact
    the running application. The concerns for sampling the information from storing,
    querying, and displaying it, are thus separated.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting technical metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application is a good place to gather the technical metrics, ideally at
    the system boundaries. It's equally possible to collect these in a potential proxy
    server.
  prefs: []
  type: TYPE_NORMAL
- en: The application server already emits technically-relevant metrics such as information
    about resource consumption, threading, pooling, transactions, or sessions. Some
    solutions also provide Java agents that sample and emit technically-relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, application servers are required to make technically relevant
    metrics available via JMX. This functionality is part of the Management API, but
    has never been used much in projects. One of the reasons for this is that the
    model and API are quite cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: However, it's helpful to mention that Java EE application servers are required
    to gather and provide data about its resources. The container emits this information
    via JMX. There are several ways to scrape this information.
  prefs: []
  type: TYPE_NORMAL
- en: There are so-called exporters available, applications that either run standalone
    or as **Java agents**, that access the JMX information and emit it via HTTP. The
    Prometheus JMX exporter, which exports the information in a similar format as
    shown previously, is an example of this. The benefit of this approach is that
    it doesn't add dependencies into the application.
  prefs: []
  type: TYPE_NORMAL
- en: The installation and configuration of Java agents is done in the application
    server, in a base container image layer. This once again emphasizes the principle
    that containers should not couple the application's artifact with implementation
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Boundary metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technical metrics that are application-specific, such as response times, throughput,
    uptime, or error rates can be gathered at the system boundaries. This can happen
    via the interceptors or filters, depending on the situation. HTTP-relevant monitoring
    can be collected via a servlet filter for any technology that builds upon servlets,
    such as JAX-RS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows a servlet filter that gathers the response
    time and throughput in a Prometheus histogram metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This metric is registered similarly to the business-related example previously,
    and emitted via the Prometheus output format. The histogram buckets collect the
    time in four buckets, with the specified times from 0.1, 0.4, or 1.0 seconds,
    and everything above. These bucket configurations need to be adapted to the SLAs.
  prefs: []
  type: TYPE_NORMAL
- en: The servlet filter is active on all resource paths and will collect the statistics,
    qualified by each path.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historically, logging had quite high importance in enterprise applications.
    We have seen a lot of logging framework implementations and supposedly best practices
    on how to implement reasonable logs.
  prefs: []
  type: TYPE_NORMAL
- en: Logging is typically used for debugging, tracing, journaling, monitoring, and
    outputting errors. In general, all information that developers consider somewhat
    important, but not made apparent to the users, is been placed into logs. In almost
    all cases, this includes logging to files.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of traditional logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach, which is way too common in enterprise projects, comes with a
    few problems.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional logging, especially extensively used logging invocations, creates
    a lot of string objects. Even APIs such as **Slf4J** that aim to reduce unnecessary
    string concatenation will result in high memory rates. All these objects need
    to be garbage collected after their use, which utilizes the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Storing log events as string messages is a verbose way of storing information. Choosing
    different formats, mainly binary formats would drastically reduce the message
    size and result in more efficient memory consumption and higher throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Log messages that are stored in a buffer or directly on disk need to be synchronized
    with other log invocations. Synchronous loggers ultimately cause a file to be
    written within a single invocation. All simultaneous log invocations need to be
    synchronized in order to ensure that logged events appear in the right order. This
    presents the issue that synchronization indirectly couples functionality that
    otherwise is completely unrelated. It decreases the parallelism of intrinsically
    independent functionality and has a negative overall performance impact. With
    a high number of log messages being written, the probability of blocking threads
    due to synchronization increases.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is that logging frameworks usually don't write the log messages
    to disk directly; rather, they use several layers of buffering. This optimization
    technique comes with certain management overhead involved that does not improve
    the situation either. Synchronous file operations advisably work with the least
    overhead layers as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Log files that reside on NFS storage decrease the overall performance even more,
    since the write operation hits the operation system I/O twice, with both file
    system and network calls involved. In order to manage and persist log files, network
    storage is an often chosen solution, especially for container orchestration that
    needs persisted volumes.
  prefs: []
  type: TYPE_NORMAL
- en: In general, experience shows that logging has the biggest impact in an application's
    performance. This is mostly due to the memory impact on string log messages.
  prefs: []
  type: TYPE_NORMAL
- en: Log levels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging solutions include the ability to specify the importance of a log entry
    via log levels, such as *debug*, *info*, *warning,* or *error*. Developers might
    ask themselves which log level to choose for specific invocations.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of having several layers certainly sounds reasonable, since production
    systems can specify a higher log level than development runs, so as not to produce
    too much data in production.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with this situation is that in production there is usually no
    debug log information available when it's needed. Potential error situations that
    could need additional insights don't have this information available. Debug or
    trace log levels that include tracing information are switched off.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing log levels is always a trade-off regarding what information should
    be included. Debugging in development is done best using actual debug tools which
    connect against running applications, potentially remotely. Debug or trace logs
    are usually not available in production and therefore provide little benefit.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas defining multiple log levels may have emerged from a good intention,
    the practical use in production systems adds little value.
  prefs: []
  type: TYPE_NORMAL
- en: Log format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional logging solutions specify particular log layouts that format the
    log messages in the resulting log file. The application needs to manage the creation,
    rolling, and formatting of log files that are not relevant for the business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Quite a few enterprise applications ship with third-party logging dependencies
    that implement this functionality, but provide no business value.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing particular plain text log formats is another decision that needs to
    be made by the application developers. There is a trade-off between a log entry
    format that is readable by both humans and machines. The result is usually the
    worst compromise for both parties; string log formats that are both hardly readable
    and have a tremendous impact on the system's performance.
  prefs: []
  type: TYPE_NORMAL
- en: It would be more reasonable to choose binary formats that store information
    with the highest density. Humans then could use tooling to make the messages visible.
  prefs: []
  type: TYPE_NORMAL
- en: Amounts of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extensive logging introduces a huge amount of data that is contained in log
    files. In particular, logs that are used for debugging and tracing purposes result
    in big files that are cumbersome and expensive to parse.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing log formats in general introduces an avoidable overhead. Information
    that is potentially technically-relevant is serialized in a specific format first,
    just to be parsed again later when inspecting the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this sub-section, we will see what other solutions there are.
  prefs: []
  type: TYPE_NORMAL
- en: Obfuscation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equal to unreasonably checked exception handling, logging obfuscates business
    logic in the source code. This is especially the case for boilerplate log patterns
    that are common in many projects.
  prefs: []
  type: TYPE_NORMAL
- en: Log statements take up too much space in the code and especially draw the developer's
    attention.
  prefs: []
  type: TYPE_NORMAL
- en: Some logging technology, such as Slf4j, provides functionality to format strings
    in readable ways while avoiding immediate string concatenation. But still, log
    statements add obfuscating invocations that are unrelated to the business problem.
  prefs: []
  type: TYPE_NORMAL
- en: This is obviously less the case if the debug log statements are added in a cross-cutting
    component, such as an interceptor. However, these cases mostly add logging for
    tracing purposes. We will see in the next sub-section that there are more suitable
    solutions for this.
  prefs: []
  type: TYPE_NORMAL
- en: The concerns of applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in 12-factor applications, it is not an application's concern
    to choose log files and message formats.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, logging frameworks that promise simpler logging solutions add
    technically-motivated third-party dependencies to the application; dependencies
    that have no direct business value.
  prefs: []
  type: TYPE_NORMAL
- en: If there is business value in events or messages, then the use of another solution
    should be favored. The following shows how traditional logging is misused for
    these other applications' concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Wrong choice of technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional logging, and how it is used in the majority of enterprise projects,
    is a suboptimal choice for concerns that are better handled using different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question is: what do developers want to log, anyway? What about metrics,
    such as the current resource consumption? Or business-related information, such
    as *car manufactured*? Should we log debugging and tracing information such as
    *request with UUID xy originated from application A, and called subsequent application
    B*? What about occurring exceptions?'
  prefs: []
  type: TYPE_NORMAL
- en: Attentive readers will see that most of the use cases for traditional logging
    are far better handled using other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: If logging is used for debugging or debug tracing applications, the approach
    with using trace or debug levels doesn't help much. Information that will not
    be available in production can't reproduce a potential bug. Logging a huge amount
    of debug or trace events in production, however, will affect the application's
    responsiveness due to disk I/O, synchronization, and memory consumption. Debugging
    concurrency-related errors may even lead to a different outcome, due to the modified
    order of execution.
  prefs: []
  type: TYPE_NORMAL
- en: For debugging functionality, it's much more advisable to use actual debugger
    features during development, such as IDEs that connect against a running application.
    Logging that is used for business-motivated journaling is better accomplished
    via a proper journaling solution, as we will see later in this chapter. The plain
    text log messages are certainly not the ideal solution. The chosen technology
    should minimize the performance impact on the application.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to realize the same motivations behind journaling is to introduce
    event sourcing. This makes the domain events part of the application's core model.
  prefs: []
  type: TYPE_NORMAL
- en: Business-motivated tracing, this should be part of the business use case as
    well, implemented using an adequate solution. As we will see in the next sub-section,
    there are more suitable tracing solutions that require less parsing and have a
    smaller performance impact. Tracing solutions also support the consolidation of
    information and requests across microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring information that is stored in log messages is better managed via
    the use of proper monitoring solutions. This approach is not just much more performant,
    it is also a more effective way of emitting the information in proper data structures.
    The examples we have seen earlier in this chapter illustrate monitored data and
    possible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Logging is also traditionally being used to output exceptions and errors that
    cannot properly be handled in the application otherwise. This is arguably the
    only reasonable use of logging. Together with other potential metrics that may
    capture the error, such as error rate counters at the system boundary, the logged
    exception may support developers in investigating errors.
  prefs: []
  type: TYPE_NORMAL
- en: However, errors and exceptions should only be logged if they in fact concern
    the application and represent an error that can be resolved by developers. With
    monitoring and alerting solutions in place, the need to look into logs should
    indicate a serious problem with the application.
  prefs: []
  type: TYPE_NORMAL
- en: Logging in a containerized world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the 12-factor principles is to treat logging as a stream of events. This
    includes the idea that handling log files should not be a concern of the enterprise
    application. Log events should simply output to the process' standard output.
  prefs: []
  type: TYPE_NORMAL
- en: The application's runtime environment consolidates and processes the log streams.
    There are solutions for unified access over all participating applications that
    can be deployed into the environment. The runtime environment where the application
    is deployed takes care of processing the log streams. **fluentd**, which is part
    of the Cloud Native Computing Foundation unifies the access to log events in a
    distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: Application developers should treat the used logging technology as simply as
    possible. The application container is configured to output all server and application
    log events to standard output. This approach simplifies matters for enterprise
    developers and enables them to focus more on solving actual business problems.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, there is not much information left that application developers
    reasonably can log in a traditional way. Monitoring, journaling, or tracing solutions,
    as well as event sourcing, can solve the requirements in more suitable ways.
  prefs: []
  type: TYPE_NORMAL
- en: Together with logging to standard output without the need for sophisticated
    log file handling, there is no need for sophisticated logging framework. This
    supports zero-dependency applications and enables developers to be able to focus
    on business concerns instead.
  prefs: []
  type: TYPE_NORMAL
- en: It's therefore advisable to avoid third-party logging frameworks, as well as
    writing to traditional log files. The need to manage log rotations, log entry
    formats, levels, and framework dependencies, as well as configuration, becomes
    no longer necessary.
  prefs: []
  type: TYPE_NORMAL
- en: However, the following might seem antithetical to enterprise developers.
  prefs: []
  type: TYPE_NORMAL
- en: The straightforward, 12-factor way to log the output is using the standard output
    capabilities of Java via `System.out` and `System.err`. This directly writes the
    synchronous output without needless layers of buffering.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to mention that outputting data via this approach will not perform.
    The introduced synchronization, again, ties otherwise independent parts of the
    application together. If the output of the process is grabbed and emitted by a
    video card, the performance will further decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Logging to console is only meant to emit errors that are, as the name of the
    Java type indicates - an exception. In all other cases, engineers must ask themselves
    why they want to output an information in the first place, or whether other solutions
    are more suitable. Therefore, logged errors should indicate a fatal problem that
    requires engineering action. It should not be expected to receive this log output
    in production; in this fatal error case, performance can be disrespected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to output fatal error information, Java EE applications can use CDI
    features as well as Java SE 8 functional interfaces to provide a uniform logging
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Consumer<Throwable>` logger is then injectable in other beans, and it
    logs using the `accept()` method of the consumer type. If a more readable interface
    is desired, a thin logger facade type which is injected via `@Inject` can be defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This approach will seem antithetical to enterprise developers, especially logging
    without using a logging framework. Using a sophisticated logging framework, which
    is used to direct the output to standard out again, introduces overhead, which
    ultimately ends up in the same result. Some developers may prefer to use JDK logging
    at this point.
  prefs: []
  type: TYPE_NORMAL
- en: However, providing sophisticated log interfaces and thus giving application
    developers the opportunity to output all kinds of information, especially human-readable
    strings, is counterproductive. This is why the code examples only allow to output
    throwable types in fatal error situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to notice the following few aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional logging should be avoided and substituted with more-suited solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only fatal error cases that are the exception, and are expected to ideally never
    happen, should be logged
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerized applications are advised to output log events to standard out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application logging and interfaces should be as simple as possible, preventing
    developers from excessive use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Journaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If journaling is needed as part of the business logic, there are better ways
    than using logging frameworks. The requirements for journaling could be auditing
    regulations, such as is the case for trading systems.
  prefs: []
  type: TYPE_NORMAL
- en: If the business logic requires journaling, it should accordingly be treated
    as such - a business requirement. There is journaling technology available that
    synchronously persists the required information with higher density and lower
    latency than traditional logging. An example of these solutions is **Chronicle
    Queue**, which allows us to store messages with high throughput and low latency.
  prefs: []
  type: TYPE_NORMAL
- en: The application domain could model the information as a domain event and directly
    persist it into a journaling solution. As mentioned previously, another way is
    to base the application on an event sourcing model. The auditing information is
    then already part of the application's model.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing is used to reproduce specific scenarios and request flows. It's already
    helpful in retracing complex application processes, but it's especially helpful
    when multiple applications and instances are involved.
  prefs: []
  type: TYPE_NORMAL
- en: However, what's important to be pointed out is that there needs to be a business,
    not technical, requirement for tracing systems, similar to journaling.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing is a poor technique for debugging or performance tracing systems. It
    will have a certain impact on performance and doesn't help much in resolving performance
    regression. Interdependent, distributed applications that need to be optimized
    in their performance advisably solely emit information about their quality of
    service, such as response times. Sampling techniques can sufficiently gather information
    that indicate performance issues in the applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, let's have a look at business-motivated tracing to track the components
    and systems involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a trace of a specific request involving multiple
    application instances and components thereof:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8a210b2-6eba-4d32-abf1-059aa19fb534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The trace can also be displayed in a timeline to show the synchronous invocations
    as demonstrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76967c55-0e8a-4e2a-91db-0c10f6107986.png)'
  prefs: []
  type: TYPE_IMG
- en: Tracing includes information about which applications or application components
    have been involved and how long the individual invocations took.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, log files have been used for this, by logging the start and end
    of each method or component invocation including a correlation ID, such as a thread
    identifier. There is the possibility of including correlation IDs into logs that
    are used from a single originating request and are reused and logged in subsequent
    applications. This results in traces that also span multiple applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of logging, the tracing information was accumulated from multiple
    log files; for example, using solutions such as the **ELK** stack. Trace logs
    are usually implemented in cross-cutting ways; for example, using logging filters
    and interceptors so as not to obfuscate the code.
  prefs: []
  type: TYPE_NORMAL
- en: However, using log files for tracing is not advisable. Even enterprise applications
    that experience a moderate load introduce a lot of log entries that are written
    to files. Many log entries are needed for each and every request.
  prefs: []
  type: TYPE_NORMAL
- en: File-based I/O and the needed log format serialization generally is too heavy
    for this approach and greatly affects the performance. Tracing to log file formats
    introduces a lot of data that needs to be parsed again afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: There are tracing solutions that provide a much better fit.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing in a modern world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past months and years, multiple tracing solutions have originated that
    aim to minimize the performance impact on the system.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenTracing** is standard, vendor-neutral tracing technology that is part
    of the Cloud Native Computing Foundation. It defines the concepts and semantics
    of traces and supports tracing in distributed applications. It is implemented
    by multiple tracing technologies such as **Zipkin**, **Jaeger**, or **Hawkular**.'
  prefs: []
  type: TYPE_NORMAL
- en: A hierarchical trace consists of several spans, similar to the ones shown in
    the previous figures. A span can be a child of, or follow, another span.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the car manufacture component span is a child of the
    load balancer span. The persistence span follows the client span since their invocations
    happen sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: An OpenTracing API span includes a time span, an operation name, context information,
    as well as optional sets of tags and logs. The operation names and tags are somewhat
    similar to Prometheus metric names and labels described earlier in the *Enter Prometheus*
    section. Logs describe information such as span messages.
  prefs: []
  type: TYPE_NORMAL
- en: An example for a single span is `createCar` with the tags `color=RED` and `engine=DIESEL,`
    as well as a log `message` field `Car successfully created`.
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet shows an example of using the OpenTracing Java API
    in the *car manufacture* application. It supports Java's try-with-resource feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The created span starts actively and is added as a child to a potentially existing
    parent span. The `Tracer` is produced by a CDI producer that depends on the specific
    OpenTracing implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this approach obfuscates the code a lot and should be moved to cross-cutting
    components, such as interceptors. Tracing interceptor bindings can decorate methods
    and extract information about method names and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the desired information included in tracing spans, the interceptor
    binding can be enhanced to provide further information, such as the operation
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows a business method decorated with an interceptor
    binding that adds tracing in a lean way. Implementing the interceptor is left
    as an exercise for the reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The traced information is carried into subsequent applications via span contexts
    and carriers. They enable participating applications to add their tracing information
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: The gathered data can be extracted via the used OpenTracing implementation.
    There are filter and interceptor implementations available for technology such
    as JAX-RS resources and clients that transparently add the required debug information
    to the invocations, for example, using HTTP headers.
  prefs: []
  type: TYPE_NORMAL
- en: This way of tracing impacts the system's performance way less than traditional
    logging. It defines the exact steps and systems that instrument the business logic
    flow. However, as mentioned before, there needs to be a business requirement to
    implement a tracing solution.
  prefs: []
  type: TYPE_NORMAL
- en: Typical performance issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance issues come with typical symptoms, such as response times, that
    are slow or become slower over time, timeouts, or even completely unavailable
    services. The error rates indicate the latter.
  prefs: []
  type: TYPE_NORMAL
- en: When performance issues arise, the question to be asked is what the actual constraining
    resource, the bottleneck, is. Where does the issue originate? As shown earlier,
    engineers are advised to follow an investigative process that considers the overall
    situation, including hardware and operating systems, in order to find the constraint.
    There should be no guessing and premature decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Performance problems can have a huge number of root causes. Most of them originate
    in coding errors or misconfiguration rather than actual workload exceeding the
    available resources. Modern application servers can handle a lot of load until
    the performance becomes an issue.
  prefs: []
  type: TYPE_NORMAL
- en: However, experience shows that there are typical performance issue root causes.
    The following will show you the most serious ones.
  prefs: []
  type: TYPE_NORMAL
- en: Engineers are instructed to investigate issues properly, without following supposedly
    best practices and premature optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and memory consumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional logging, such as writing string-formatted log messages to files,
    is the most common root cause for poor performance. This chapter has already described
    the issues and advisable solutions for them.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest reason for poor performance is the extensive string object creation
    and resulting memory consumption. High memory consumption, in general, represents
    a major performance issue. This is not only caused by logging but by high memory
    rates in caching, memory leaks, or extensive object creation.
  prefs: []
  type: TYPE_NORMAL
- en: Since the JVM manages the garbage collection of memory, these high memory rates
    result in garbage collector runs, trying to free unused memory. The garbage collection
    utilizes the CPU. The situation is not resolved by a single collection run, what
    results in subsequent GC executions and thus high CPU usage. This happens if not
    sufficient memory can be freed either because of memory leaks or a high workload
    with high consumption. Even if the system doesn't crash with `OutOfMemoryError`,
    the CPU usage can effectively stall the application.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection logs, heap dumps, and measurements can help with investigating
    these issues. JMX tools provide insights about the memory distribution and potential
    hot spots.
  prefs: []
  type: TYPE_NORMAL
- en: If business logic is implemented in a lean, straightforward way using short-lived
    objects, memory issues are far less likely.
  prefs: []
  type: TYPE_NORMAL
- en: Premature optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It regularly happens in enterprise projects that developers try to prematurely
    optimize applications without proper verification. Examples for this are the usage
    of caching, configuring pools, and application server behavior, without sampling
    sufficient measurements before and after tweaking.
  prefs: []
  type: TYPE_NORMAL
- en: It's highly advisable to not consider to use these optimizations before there
    is an identified performance problem. Proper performance sampling and measurements
    in production, as well as investigating the constraining resource, are a necessity
    before changing the setup.
  prefs: []
  type: TYPE_NORMAL
- en: In the vast majority of cases, it's sufficient to go with convention over configuration.
    This is true for both the JVM runtime as well as the application server. If developers
    take a plain Java EE approach with the default application server configuration,
    they won't likely run into issues with premature optimization.
  prefs: []
  type: TYPE_NORMAL
- en: If technical metrics indicate that the current approach is not sufficient for
    the production workload, only then is there a need to introduce change. Also,
    engineers should validate the necessity of the change over time. Technology changes
    and an optimization that provided remedy in previous runtime versions might not
    be the best solution anymore.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of convention over configuration and taking the default configuration
    first also requires the least amount of initial effort.
  prefs: []
  type: TYPE_NORMAL
- en: Again, experience shows that a lot of issues originated from prematurely introducing
    change without proper verification beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typical scapegoats for insufficient performance are relational databases. Usually,
    application servers are deployed in multiple instances that all connect against
    a single database instance. This is a necessity to ensure consistency due to the
    CAP theorem.
  prefs: []
  type: TYPE_NORMAL
- en: The database as a single point of responsibility, or failure, is predestined
    to become a bottleneck. Still, engineers must consider proper measurements to
    verify this assumption.
  prefs: []
  type: TYPE_NORMAL
- en: If metrics indicate that the database response is slower than acceptable, again
    the first approach is to investigate the root cause. If the database query is
    responsible for causing the slow response, engineers are advised to take a look
    at the performed queries. Is a lot of data being loaded? If yes, is all this data
    necessary or will it be filtered and reduced by the application later on? In some
    cases, the database queries load more data than required.
  prefs: []
  type: TYPE_NORMAL
- en: This is also a question relevant to the business, especially for retrieving
    data, whether everything is required. More specific database queries that pre-filter
    results or size limits, such as pagination, can help in these cases.
  prefs: []
  type: TYPE_NORMAL
- en: Databases perform exceptionally well when joining and filtering data. Performing
    more complex queries directly in the database instance usually outperforms loading
    all required data into the application's memory and executing the query there.
    It's possible to define complex, nested SQL queries in Java and to execute them
    in the database. However, what enterprise applications should avoid is to define
    business logic queries directly in the database, using stored procedures. Business-related
    logic should reside in the application.
  prefs: []
  type: TYPE_NORMAL
- en: A typical configuration mistake is also neglecting to index relevant database
    columns that are used in queries. There were many cases in projects where the
    overall performance could be improved by several factors just by defining proper
    indexes.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the insight measurements of specific use cases usually provide good
    insights on where the issue might originate from.
  prefs: []
  type: TYPE_NORMAL
- en: In some scenarios, queries that update data often result in optimistic locking
    errors. This originates from domain entities simultaneously being updated. Optimistic
    locking is rather a business issue than a technical one. The service error rate
    will indicate such issues.
  prefs: []
  type: TYPE_NORMAL
- en: If the business use case requires that entities are often changed simultaneously,
    development teams can consider changing the functionality to an event-based model.
    Similarly, as shown previously, event sourcing and event-driven architectures
    get rid of this situation by introducing eventual consistency.
  prefs: []
  type: TYPE_NORMAL
- en: If the performance issues purely originates from workload and concurrent accesses,
    then ultimately a different data model is required, such as event-driven architectures
    realized with CQRS. However, usually the situation is solvable in another way.
    The vast majority of enterprise applications scale well enough using relational
    databases.
  prefs: []
  type: TYPE_NORMAL
- en: Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of communication-related performance issues are due to synchronous
    communication. Most issues in this area emerge from missing timeouts that lead
    client calls to block infinitely and cause deadlock situations. This happens if
    no client-side timeouts are configured and the invoked system is unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: A less critical but similarly imperfect situation occurs if the configured timeouts
    are too large. This causes systems to wait for too long, slowing down processes
    and blocking threads.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring timeouts for client invocations, as described earlier, provides
    simple but effective relief from this issue.
  prefs: []
  type: TYPE_NORMAL
- en: High response time and low throughput can have multiple origins. Performance
    analysis provides insights into where the time is spent.
  prefs: []
  type: TYPE_NORMAL
- en: There are some other potential bottlenecks, such as payload sizes. Whether data
    is sent as plain text or binary data can make quite some difference in payload
    sizes. Serialization that uses imperfect algorithms or technology can also decrease
    the responsiveness. Still, these concerns are usually negligible unless the application
    resides in high performance situations.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple, synchronous invocations are required, they should happen in parallel
    if possible, using container-managed threads; for example, provided by a managed
    executor service. This avoids unnecessarily making the application wait.
  prefs: []
  type: TYPE_NORMAL
- en: In general, use cases that span multiple transactional systems, such as databases
    using distributed transactions, should be avoided. As described previously, distributed
    transactions won't scale. The business use case should be considered to effectively
    process asynchronously instead.
  prefs: []
  type: TYPE_NORMAL
- en: Threading and pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to reuse threads as well as connections, application containers manage
    pools. Requested threads don't necessarily have to be created but are reused from
    a pool.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling is used to control the load on specific parts of the system. Choosing
    appropriate pool sizes allows the system to be saturated well, but prevents it
    from overloading. This is due to the fact that empty pools will lead to suspended
    or rejected requests. All threads and connections of that pool are then being
    utilized already.
  prefs: []
  type: TYPE_NORMAL
- en: The bulkhead pattern prevents different parts of the system from affecting each
    other by defining dedicated thread pools. This limits the resource shortage to
    a potentially problematic functionality. In some cases, functionality such as
    a legacy system might be known to cause issues. Bulkheads, implemented as dedicated
    thread pools, and timeout configuration help preserve the application's health.
  prefs: []
  type: TYPE_NORMAL
- en: Empty pools either originate from the current load on that pool being exceptionally
    high, or resources that are acquired for much longer than expected. In any case,
    it's advisable not to simply increase the corresponding pool size but to investigate
    where the issue originates from. The described investigation techniques as well
    as JMX insights and thread dumps will supports you in finding bottlenecks, as
    well as potential programming errors, such as deadlocks, misconfigured timeouts,
    or resource leaks. In the minority of cases will a shortage in pooling actually
    originate from a high workload.
  prefs: []
  type: TYPE_NORMAL
- en: Pool sizes and configuration is made in the application container. Engineers
    must perform proper performance sampling in production before and after reconfiguring
    the server.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The challenge with performance testing is that the tests run in a simulated
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Simulated environments are fine for other kinds of tests, such as system tests,
    since certain aspects are abstracted. Mock servers, for example, can simulate
    behavior similarly to production.
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike in functional tests, validating the system's responsiveness
    requires to take everything in the environment into account. At the end of the
    day, applications are running on actual hardware, thus the hardware, as well as
    the overall situation, impacts the application's performance. The system's performance
    in simulated environments will never behave equally in production. Therefore,
    performance tests are not a reliable way of finding performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: There are many scenarios where an application can perform much better in production
    compared to performance tests, depending on all the immediate and imminent influences.
    The HotSpot JVM, for example, performs better under high load.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating performance constraints therefore can only happen in production.
    As shown earlier, the jPDM investigation processes, together with sampling techniques
    and tools applied to the production system, will identify the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and stress tests help in finding obvious code or configuration errors,
    such as resource leaks, serious misconfiguration, missing timeouts, or deadlocks.
    These bugs will be found before deploying to production. Performance tests can
    also capture performance trends over time and warn engineers if the overall responsiveness
    decreases. Still, this may only indicate potential issues but should not lead
    the engineers to premature conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and stress tests only make sense in the whole network of interdependent
    applications. This is because of dependencies and performance influences of all
    the systems and databases involved. The setup needs to be as similar to production
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Even then, the outcome will not be the same as in production. It's highly important
    that engineers are aware of this. Performance optimizations that follow performance
    tests are therefore never fully representative.
  prefs: []
  type: TYPE_NORMAL
- en: For performance tuning, it's important to use investigative processes together
    with sampling on production instead. Continuous Delivery techniques support in
    quickly bringing configuration changes to production. Then engineers can use the
    sampling and performance insights to see whether changing the setup has improved
    the overall solution. And again, the overall system needs to be taken into account.
    Simply tuning a single application without considering the whole system can have
    negative effects on the overall scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Business-related metrics can provide helpful insights into the enterprise application.
    These metrics are a part of the business use case and therefore should be treated
    as such. Business metrics are ultimately impacted by other, technical metrics.
    It's therefore advisable to monitor these metrics as well.
  prefs: []
  type: TYPE_NORMAL
- en: The theory of constraints describes that there will be one ore more limiting
    constraints that prevent the system from infinitely increasing its throughput.
    In order to improve the application's performance the limiting constraint therefore
    needs to be eradicated. jPDM helps identifying the limiting constraints by finding
    the dominating consumer of the CPU first and using appropriate tooling to further
    investigate performance issues. It's advisable to investigate potential bottlenecks
    by following this process, which takes the overall situation into account, rather
    than to blindly *peek and poke*.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using high-frequency monitoring, engineers are advised to sample
    technical metrics with low frequency and to query, calculate, and investigate
    out-of-band. This has tremendously less impact on the application's performance.
    Distributed applications will need to meet SLAs. The backpressure approach as
    well as the bulkhead pattern can help achieve highly responsive, resilient enterprise
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional logging should be avoided for a number of reasons, especially the
    negative performance impact. Enterprise applications are advised to only output
    log events in case of fatal, unexpected errors, which are written to standard
    output in a preferably straightforward way. For all other motivations, such as
    debugging, tracing, journaling, or monitoring, there are more suitable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and stress tests running in simulated environments can be used to
    find obvious errors in the application. The environments should be as close to
    production, including all applications and databases involved. For any other reasoning,
    especially statements about an application's expected performance, bottlenecks,
    or optimizations, performance tests are not helpful and might even lead to wrong
    assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover the topic of application security.
  prefs: []
  type: TYPE_NORMAL
