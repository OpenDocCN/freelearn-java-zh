- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding Logging and Tracing to Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about logging and tracing tools. We will use
    Spring Micrometer, Brave, the **Elasticsearch, Logstash, and Kibana** (**ELK**)
    stack, and Zipkin. ELK and Zipkin will be used to implement the distributed logging
    and tracing of the request/response of API calls. **Spring Micrometer** with **Actuator**
    will be used to inject tracing information into API calls. You will learn how
    to publish and analyze the logging and tracing of different requests and logs
    related to responses.
  prefs: []
  type: TYPE_NORMAL
- en: These aggregated logs will help you to troubleshoot web services. You will call
    one service (such as the gRPC client), which will then call another service (such
    as the gRPC server), and link them with a trace identifier. Then, using this trace
    identifier, you can search the centralized logs and debug the request flows. In
    this chapter, we will use this sample flow. However, the same tracing can be used
    when service calls require more internal calls. You will also use **Zipkin** to
    ascertain the performance of each API call.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will explore logging and monitoring tools, including the ELK stack
    and Zipkin, using Spring Micrometer. These tools (ELK and Zipkin) will then be
    used to implement the distributed logging and tracing of API requests and responses.
    Spring Micrometer will be used to inject the tracing information into API calls.
    You will learn how to publish and analyze the logging and tracing of different
    requests and logs related to responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will explore the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging and tracing using the ELK stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing logging and tracing in the gRPC code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing with Zipkin and Micrometer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following to develop and execute the code in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Any Java IDE, such as NetBeans, IntelliJ, or Eclipse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Java Development Kit** (**JDK**) **17**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An internet connection to clone the code and download the dependencies and Gradle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insomnia/cURL (for API testing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker and Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code used in this chapter at [https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12).
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Logging and tracing using the ELK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, products and services are divided into multiple small parts and executed
    as separate processes or deployed as separate services, rather than as a monolithic
    system. An API call may make several other internal API calls. Therefore, you
    need distributed and centralized logging to trace a request that spans multiple
    web services. This tracing can be done using the trace identifier (`traceId`),
    which can also be referred to as a correlation identifier (`correlationId`). This
    identifier is a collection of characters that forms a unique string, which is
    populated and assigned to an API call that requires multiple inter-service calls.
    Then, the same trace identifier is propagated to subsequent API calls for tracking
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Errors and issues are imminent in the production system. You need to carry out
    debugging to ascertain the root cause. One of the key tools associated with debugging
    is logs. Logs can also give you warnings related to the system if the system is
    designed to do so. Logs also offer throughput, capacity, and monitoring of the
    health of the system. Therefore, you need an excellent logging platform and strategy
    that enables effective debugging.
  prefs: []
  type: TYPE_NORMAL
- en: There are different open source and enterprise tools available on the market
    for logging, including Splunk, Graylog, and the ELK stack. The ELK stack is the
    most popular of these, and you can use it for free if you are not going to provide
    ELK-based service as SaaS. We are going to use the ELK stack for logging throughout
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand the ELK stack in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ELK stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ELK stack comprises three components – Elasticsearch, Logstash, and Kibana.
    All three products are part of Elasticsearch B.V. ([https://www.elastic.co/](https://www.elastic.co/)).
    The ELK stack performs the aggregation, analysis, visualization, and monitoring
    of logs. The ELK stack provides a complete logging platform that allows you to
    analyze, visualize, and monitor all types of logs, including product and system
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are going to use the following workflow to publish logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Log flows in the ELK stack](img/Figure_12.01_B19349.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Log flows in the ELK stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: Services/system logs are pushed to Logstash on the TCP port
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash pushes the logs to Elasticsearch for indexing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana then uses the Elasticsearch index to query and visualize the logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an ideal production system, you should use one more layer. A broker layer
    such as Redis, Kafka, or RabbitMQ should be placed between the service logs and
    Logstash. This prevents data loss and can handle the sudden spike in input load.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for ELK stack configuration
  prefs: []
  type: TYPE_NORMAL
- en: The ELK stack is fully customizable and comes with a default configuration.
    However, if you are using an Elasticsearch cluster (more than one instance of
    Elasticsearch is deployed), it is better to use an odd number of Elasticsearch
    nodes (instances) to avoid the split-brain problem.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use the appropriate data type for all the fields (input
    in JSON format for the logs). This will allow you to perform logical checks and
    comparisons while querying the log data. For example, the `http_status < 400`
    check would work only if the `http_status` field type is a number and may fail
    if the `http_status` field type is a string.
  prefs: []
  type: TYPE_NORMAL
- en: If you are already familiar with the ELK stack, you can skip this introduction
    and move on to the next section. Here, you’ll find a brief introduction to each
    of the tools in the ELK stack.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Elasticsearch is one of the most popular enterprise full-text search engines.
    It is based on Apache Lucene and developed using Java. Elasticsearch is also a
    high-performance, full-featured text search engine library. Recent changes in
    the licensing terms have made it restricted open source software, which prevents
    you from offering Elasticsearch or the ELK stack as SaaS. It is distributable
    and supports multi-tenancy. A single Elasticsearch server stores multiple indexes
    (each index represents a database), and a single query can search the data of
    multiple indexes. It is a distributed search engine and supports clustering.
  prefs: []
  type: TYPE_NORMAL
- en: It is readily scalable and can provide near-real-time searches with a latency
    of one second. Elasticsearch APIs are extensive and very elaborate. Elasticsearch
    provides JSON-based schema-less storage and represents data models in JSON. Elasticsearch
    APIs use JSON documents for HTTP requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logstash is an open source data collection engine with real-time pipeline capabilities.
    It performs three major operations – it collects the data, filters the information,
    and outputs the processed information to data storage, in the same way as Elasticsearch
    does. It allows you to process any event data, such as logs from a variety of
    systems, because of its data pipeline capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash runs as an agent that collects the data, parses it, filters it, and
    sends the output to a designated data store, such as Elasticsearch, or as simple
    standard output on a console.
  prefs: []
  type: TYPE_NORMAL
- en: On top of that, it has a rich set of plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kibana is an open source web application that is used for visualizing and performing
    information analytics. It interacts with Elasticsearch and provides easy integration
    with it. You can perform searching and display and interact with the information
    stored in Elasticsearch indices.
  prefs: []
  type: TYPE_NORMAL
- en: It is a browser-based web application that lets you perform advanced data analysis
    and visualize your data in a variety of charts, tables, and maps. Moreover, it
    is a zero-configuration application. Therefore, it does not require any coding
    or additional infrastructure following installation.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s learn how to install the ELK stack.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the ELK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use various methods to install the ELK stack, such as installing individual
    components as per the operating system, downloading the Docker images and running
    them individually, or executing the Docker images using Docker Compose, Docker
    Swarm, or Kubernetes. You are going to use Docker Compose in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the grammar of a Docker Compose file before we create the
    ELK stack Docker Compose file. A Docker Compose file is defined using YAML. The
    file contains four important top-level keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`version`: This denotes the version of the Docker Compose file format. You
    can use the appropriate version based on the installed Docker Engine. You can
    check [https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/)
    to ascertain the mapping between the Docker Compose file version and the Docker
    Engine version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`services`: This contains one or more service definitions. The service definition
    represents the service executed by the container and contains the container name
    (`container_name`), Docker image (`image`), environment variables (`environment`),
    external and internal ports (`port`), the command to be executed when running
    the container (`command`), the network to be used for communicating with other
    services (`networks`), mapping of the host filesystem with a running container
    (`volume`), and the container to be executed once the dependent service has started
    (`depends_on`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`networks`: This represents the (top-level) named network that needs to be
    created to establish a communication channel among the defined services. Then,
    this network is used by the service to communicate based on the `networks` key
    of the defined service. The top-level network key contains the driver field, which
    can be `bridge` for a single host and `overlay` when used in Docker Swarm. We
    will use `bridge`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`volumes`: A top-level `volumes` key is used to create the named volume that
    mounts the host path. Make sure to use it only if required by the multiple services;
    otherwise, you can use the `volumes` key inside the service definition, which
    would be service-specific.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s create the Docker Compose file, `docker-compose.yaml`, in the `Chapter12`
    directory to define the ELK stack. Then, you can add the following code to this
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/docker-compose.yaml](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/docker-compose.yaml)'
  prefs: []
  type: TYPE_NORMAL
- en: First, we defined the version of the Docker Compose file. Then, we created the
    `services` key section, which contains the `elasticsearch` service. The service
    contains the container name, Docker image, environment variables, and network
    (because you want ELK components to communicate with one another). Finally, ports
    are defined in `external:internal` format. You are going to use port `19200` from
    the browser to access it. However, other services will use port `9200` for communicating
    with Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, you can define the `logstash` service next, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Logstash configuration contains two extra service keys:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a command key that contains the `logstash` command with a given configuration
    (using `-e`). The Logstash configuration normally contains three important parts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input`: Logstash input channels, such as `tcp` or `File`. We are going to
    use a TCP input channel. This means that gRPC server and client services will
    push the logs in JSON format (a JSON-coded plugin is used) to `logstash` on port
    `5001`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter`: The `filter` key contains various filter expressions using different
    means, such as `grok`. You don’t want to filter anything from logs, so you should
    opt out of using this key.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output`: Where to send the input data after filtering out the information.
    Here, we are using Elasticsearch. Logstash pushes the received log information
    to Elasticsearch on port `9200` and uses the `modern-api` Elasticsearch index.
    This index is then used on Kibana for querying, analyzing, and visualizing the
    logs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A second key, `depends_on`, tells Docker Compose to start Elasticsearch before
    executing the `logstash` service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s add the final service, `kibana`, as shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The service’s `kibana` definition is in line with other defined services. It
    uses the `ELASTICSEARCH_HOSTS` environment variable to connect to Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the Docker Compose file, you define the `elk-net` network, which
    uses the `bridge` driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are done with configuring the ELK stack Docker Compose file. Let’s now
    start Docker Compose using the following command. If you run this for the very
    first time, local images of Elasticsearch, Logstash, and Kibana will also be fetched.
    You can use the `-f` flag if you’ve used other filenames apart from `docker-compose.yaml`
    or `docker-compose.yml` in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `-d` option is used, which will start Docker Compose in the background.
    It starts the `es-container` Elasticsearch container first based on dependencies
    (the `depends_on` key).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch uses 2 GB of heap size by default. Docker also uses 2 GB of memory
    by default in some systems, such as Mac.
  prefs: []
  type: TYPE_NORMAL
- en: This may cause an error such as `error-137`. Therefore, you should increase
    the default Docker memory to at least 8 GB (more the better) and swap memory to
    at least 2 GB to avoid such issues.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [https://docs.docker.com/config/containers/resource_constraints/#memory](https://docs.docker.com/config/containers/resource_constraints/#memory)
    for Docker memory configurations.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that Docker Compose first creates the network, and then creates
    the service containers and starts them. Once all the containers are up, you can
    hit the URL `http://localhost:19200/` (which contains the external port defined
    for the `Elasticsearch` service) in the browser to check whether the Elasticsearch
    instance is up or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you hit the URL, you may find the following type of JSON response if the
    Elasticsearch service is up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s check the Kibana dashboard by hitting the URL `http://localhost:5600`
    (which contains the external port defined for the `kibana` service) in the browser.
    This should load the home page of Kibana, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Kibana home page](img/Figure_12.02_B19349.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Kibana home page
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering how the logs can be viewed since you have used the `-d`
    option. You can use the `docker-compose logs [service name]` command. If you don’t
    provide the service name, then it will show the logs of all the services. You
    can use the `--tail` flag to filter the number of lines. The `--tail="all"` flag
    will show all the lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the following command to stop Docker Compose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output may vary a bit. However, it should stop and remove all the running
    containers. The command stops the containers according to the dependencies provided
    in the `docker-compose.yaml` file based on the `depends_on` property and then
    removes them. Finally, it removes the network.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s make the code changes to integrate the application with the ELK
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logging and tracing in the gRPC code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging and tracing go hand in hand. Logging in the application code is already
    taken care of by default. You use Logback for logging. Logs are either configured
    to display on the console or pushed to the filesystem. However, you also need
    to push the logs to the ELK stack for indexing and analysis. For this purpose,
    you make certain changes to the Logback configuration file, `logback-spring.xml`,
    to push the logs to Logstash. On top of that, these logs should also contain tracking
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation/trace identifiers should be populated and propagated in distributed
    transactions for tracing purposes. A distributed transaction refers to the main
    API call that internally calls other services to serve the request. Before Spring
    Boot 3, Spring provided distributed tracing support through the **Spring Cloud
    Sleuth** library; now, tracing support is provided by Spring Micrometer. It generates
    the trace ID along with the span identifier. The trace ID gets propagated to all
    the participant services during the distributed transaction. The span ID also
    participates in a distributed transaction. However, the scope of the span identifier
    belongs to its service (the one it populates).
  prefs: []
  type: TYPE_NORMAL
- en: As communicated earlier, you are going to use Zipkin; therefore, you’ll use
    Brave, a distributed tracing instrumentation library by Zipkin.
  prefs: []
  type: TYPE_NORMAL
- en: You can copy and enhance the code from [*Chapter 11*](B19349_11.xhtml#_idTextAnchor250),
    *gRPC API Development and Testing*, found at [https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter11](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter11),
    to implement logging and tracing, or refer to this chapter’s code at [https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12)
    for changes.
  prefs: []
  type: TYPE_NORMAL
- en: First, you’ll make the changes to the gRPC server code in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the gRPC server code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To enable tracing and the publishing of logs to the ELK stack, you need to
    make the following code changes, as demonstrated in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following dependencies to the `build.gradle` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/build.gradle](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/build.gradle%20)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you are adding the following four dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`spring-logback.xml` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**micrometer-tracing-bridge-brave**: This dependency takes care of managing
    the trace and span IDs. Micrometer Tracing Bridge Brave is an abstraction library
    for Zipkin Brave that publishes the collected tracing information to Zipkin using
    Brave.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**spring-boot-starter-actuator**: You used this library earlier in [*Chapter
    9*](B19349_09.xhtml#_idTextAnchor215), *Deployment of Web Services*, to provide
    the health endpoint. It also provides metric endpoints. On top of that, it performs
    the metrics and tracing autoconfiguration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, add/modify the `spring-logback.xml` file with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/logback-spring.xml](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/resources/logback-spring.xml)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you have defined the properties. The values of two of them are taken from
    the Spring configuration file (`application.properties` or `application.yaml`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now add the Logstash encoder, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `STASH` appender is defined and uses the TCP socket to push the logs
    to Logstash. It contains the `destination` element, which is used to assign Logstash’s
    `<HOST>:<TCP Port>` value. Another element encoder contains the fully qualified
    class name, `LogstashEncoder`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you will add the `STASH` appender to the root element, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The root level is set as `INFO` because you want to simply print the information
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: Test configuration for Logstash
  prefs: []
  type: TYPE_NORMAL
- en: 'To disable the LOGSTASH (STASH) appender sending the logs to the Logstash instance,
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. You may copy `logback-spring.xml` into the `test/resources` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Rename it `test/resources/logback-test.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Remove the LOGSTASH (STASH) appender and its entry from `<ROOT>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s add the Spring properties used in this `logback-spring.xml` file
    to `application.properties`, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/application.properties](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/application.properties)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the Logstash destination host is set to `localhost`. If you are running
    on a remote machine, change the host accordingly. The Logstash TCP port is set
    to the same as the Logstash external port set in the Docker Composer file.
  prefs: []
  type: TYPE_NORMAL
- en: Default probability sampling for tracing is only 10% of the actual requests.
    Therefore, we are setting the `management.tracing.sampling.probability` property
    to `1.0`. Now, it collects tracing information for 100% of the requests. Therefore,
    every request will be traced.
  prefs: []
  type: TYPE_NORMAL
- en: 'The required dependencies and configurations are now set. You can add the tracing
    server interceptor to the gRPC server. (Note: you don’t need a tracing interceptor
    if you are using the RESTful web service as Spring’s autoconfiguration mechanism
    takes care of this.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First of all, let’s define a new bean in a configuration file, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/Config.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/Config.java)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you are creating an `ObservationGrpcServerInterceptor` bean, which is
    required for creating the tracing server interceptor. Before Spring Boot 3, the
    `RpcTracing` bean was provided by Spring Sleuth. Now, autoconfiguration of the
    `RpcTracing` bean is not available because Spring Boot 3 supports Spring Micrometer
    over Sleuth. You’ll add this bean in the gRPC server as an interceptor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s modify the gRPC server Java file (`GrpcServer.java`) to add the tracing
    server interceptor to the gRPC server, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/GrpcServer.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/GrpcServer.java%20)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see that the bean created in the configuration file in the previous
    step is injected using the constructor. Later, the `oInterceptor` bean has been
    used to create the gRPC server interceptor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes required to enable log publishing to the ELK stack and tracing
    are made to the gRPC server. You can rebuild the gRPC server and run its `JAR`
    file to see the changes in effect. Check the following command-line output for
    the reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the logs are following the pattern configured in `logback-spring.xml`.
    The log block printed after `INFO` contains the application/service name, as well
    as the trace and span IDs. The highlighted line is showing a *blank* trace ID
    and span ID because no external call is made that involves the distributed transaction.
    The trace and span IDs only get added to logs if the distributed transaction (service-to-service
    communication) is called.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can add the logging and tracing implementation in the gRPC client
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the gRPC client code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To enable tracing and the publishing of logs to the ELK stack, you need to
    make code changes in the gRPC client as well, which are very similar to the changes
    implemented in the gRPC server code. Refer to the following steps for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following dependencies to the `build.gradle` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/build.gradle](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/build.gradle)'
  prefs: []
  type: TYPE_NORMAL
- en: These are the same dependencies you added to the gRPC server.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can add the `logback-spring.xml` file in the same way you added it
    to the gRPC server code to configure the logging. Make sure to use `chapter12-grpc-client`
    in place of `chapter12-grpc-server` in the XML file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, let’s add the following Spring properties to `application.properties`.
    A few of these properties are referred to in the `logback-spring.xml` file as
    well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/resources/application.properties](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/resources/application.properties)'
  prefs: []
  type: TYPE_NORMAL
- en: The required dependencies and configurations are now set. Now, you can add tracing
    to the gRPC client. `ObservationGrpcClientInterceptor`, provided by the Micrometer
    library, provides the interceptor for the gRPC client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You don’t need additional tracing changes if you are using the RESTful web service;
    Spring autoconfiguration takes care of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s define a new bean, `ObservationGrpcClientInterceptor`,
    in a configuration file, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/Config.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/Config.java)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can modify the gRPC client Java file to add the `ObservationGrpcClientInterceptor`
    interceptor to the gRPC client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/client/GrpcClient.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/client/GrpcClient.java)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see that the bean created in the configuration file in the previous
    step is autowired. Later, the `observationGrpcClientInterceptor` interceptor is
    added to the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes required to facilitate log publishing to the ELK stack and tracing
    are also done for the gRPC client. You can now rebuild the gRPC client and run
    its JAR file to see the changes in effect. Check the following command-line output
    for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the logs follow the pattern configured in `logback-spring.xml`.
    The log block printed after `INFO` contains the application/service name, trace
    ID, and span ID. The trace and span IDs are blank because they only get added
    to logs if distributed transactions (service-to-service communication) are called.
  prefs: []
  type: TYPE_NORMAL
- en: The changes required to enable log aggregation and distributed tracing in both
    the gRPC server and client services are now complete.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you’ll test the changes and view the logs in Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the logging and tracing changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before beginning testing, make sure that the ELK stack is up and running. Also,
    make sure that you first start the gRPC server and then the gRPC client service.
  prefs: []
  type: TYPE_NORMAL
- en: You can add the appropriate log statements to your services for verbose logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the following command in the new terminal window. This will call
    the `/charges` REST endpoint in the gRPC client service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It should respond with the following JSON output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous `curl` command should generate logs like the following one in
    the gRPC client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, the blocks highlighted the application name (`grpc-client`), trace ID
    (`64456d940c51e3e2baec07f7448beee6`), and span ID (`baec07f7448beee6`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The command should also generate the following logs in the gRPC server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'implementation ''net.logstash.logback:logstash-logback-                encoder:7.3''implementation
    ''io.micrometer:micrometer-tracing-    bridge-brave''implementation ''application.properties
    file (for both the gRPC server and client):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: $ java -jar zipkin-server-2.24.0-exec.jar
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: $ curl http://localhost:8081/charges
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '2023-04-24 11:35:10.313      INFO [grpc-client,64461c16391707ee95478f957f3ccb1d,95478f957f3ccb1d]     62484
    --- [nio-8081-exec-2] c.p.m.api.controller.ChargeController    : CustomerId :
    ab1ab2ab3ab4ab5'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Part 4 – GraphQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn about GraphQL-based API development. After completing
    this section, you will know the in-depth fundamentals of GraphQL, be able to differentiate
    between REST, reactive, and gRPC APIs, in the context of GraphQL thoroughly, and
    understand when to use which API style. You will also learn how to design the
    GraphQL schema, which will be used to generate Java code. Finally, you will learn
    how to write data fetchers and loaders to resolve query fields to serve the GraphQL
    API requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19349_13.xhtml#_idTextAnchor288), *Getting Started with GraphQL*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B19349_14.xhtml#_idTextAnchor313), *GraphQL API Development
    and Testing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
