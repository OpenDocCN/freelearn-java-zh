- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Adding Logging and Tracing to Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将日志和跟踪添加到服务中
- en: In this chapter, you will learn about logging and tracing tools. We will use
    Spring Micrometer, Brave, the **Elasticsearch, Logstash, and Kibana** (**ELK**)
    stack, and Zipkin. ELK and Zipkin will be used to implement the distributed logging
    and tracing of the request/response of API calls. **Spring Micrometer** with **Actuator**
    will be used to inject tracing information into API calls. You will learn how
    to publish and analyze the logging and tracing of different requests and logs
    related to responses.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习关于日志和跟踪工具的内容。我们将使用 Spring Micrometer、Brave、**Elasticsearch、Logstash
    和 Kibana**（**ELK**）堆栈以及 Zipkin。ELK 和 Zipkin 将用于实现 API 调用的请求/响应的分布式日志和跟踪。**Spring
    Micrometer** 与 **Actuator** 将用于将跟踪信息注入 API 调用中。你将学习如何发布和分析不同请求的日志以及与响应相关的日志。
- en: These aggregated logs will help you to troubleshoot web services. You will call
    one service (such as the gRPC client), which will then call another service (such
    as the gRPC server), and link them with a trace identifier. Then, using this trace
    identifier, you can search the centralized logs and debug the request flows. In
    this chapter, we will use this sample flow. However, the same tracing can be used
    when service calls require more internal calls. You will also use **Zipkin** to
    ascertain the performance of each API call.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些聚合日志将帮助你排查网络服务问题。你将调用一个服务（例如 gRPC 客户端），然后该服务将调用另一个服务（例如 gRPC 服务器），并通过跟踪标识符将它们链接起来。然后，使用这个跟踪标识符，你可以搜索集中式日志并调试请求流程。在本章中，我们将使用这个示例流程。然而，当服务调用需要更多内部调用时，也可以使用相同的跟踪。你还将使用
    **Zipkin** 来确认每个 API 调用的性能。
- en: Then, we will explore logging and monitoring tools, including the ELK stack
    and Zipkin, using Spring Micrometer. These tools (ELK and Zipkin) will then be
    used to implement the distributed logging and tracing of API requests and responses.
    Spring Micrometer will be used to inject the tracing information into API calls.
    You will learn how to publish and analyze the logging and tracing of different
    requests and logs related to responses.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用 Spring Micrometer 探索日志和监控工具，包括 ELK 堆栈和 Zipkin。这些工具（ELK 和 Zipkin）随后将用于实现
    API 请求和响应的分布式日志和跟踪。Spring Micrometer 将用于将跟踪信息注入 API 调用中。你将学习如何发布和分析不同请求的日志以及与响应相关的日志。
- en: 'You will explore the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本章中探索以下主题：
- en: Logging and tracing using the ELK stack
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ELK 堆栈进行日志和跟踪
- en: Implementing logging and tracing in the gRPC code
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 gRPC 代码中实现日志和跟踪
- en: Distributed tracing with Zipkin and Micrometer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Zipkin 和 Micrometer 进行分布式跟踪
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You will need the following to develop and execute the code in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在本章中开发和执行代码，你需要以下内容：
- en: Any Java IDE, such as NetBeans, IntelliJ, or Eclipse
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何 Java 集成开发环境（IDE），例如 NetBeans、IntelliJ 或 Eclipse
- en: '**Java Development Kit** (**JDK**) **17**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java 开发工具包**（**JDK**）**17**'
- en: An internet connection to clone the code and download the dependencies and Gradle
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个互联网连接来克隆代码和下载依赖项以及 Gradle
- en: Insomnia/cURL (for API testing)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Insomnia/cURL（用于 API 测试）
- en: Docker and Docker Compose
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 和 Docker Compose
- en: You can find the code used in this chapter at [https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12)找到本章中使用的代码。
- en: So, let’s begin!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Logging and tracing using the ELK stack
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ELK 堆栈进行日志和跟踪
- en: Today, products and services are divided into multiple small parts and executed
    as separate processes or deployed as separate services, rather than as a monolithic
    system. An API call may make several other internal API calls. Therefore, you
    need distributed and centralized logging to trace a request that spans multiple
    web services. This tracing can be done using the trace identifier (`traceId`),
    which can also be referred to as a correlation identifier (`correlationId`). This
    identifier is a collection of characters that forms a unique string, which is
    populated and assigned to an API call that requires multiple inter-service calls.
    Then, the same trace identifier is propagated to subsequent API calls for tracking
    purposes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，产品和服务的划分已经变成了多个小部分，作为单独的过程执行或作为独立的服务部署，而不是作为一个单体系统。一个API调用可能会触发其他几个内部API调用。因此，您需要分布式和集中式日志来追踪跨越多个Web服务的请求。这种追踪可以使用跟踪标识符（`traceId`）来完成，这也可以被称为关联标识符（`correlationId`）。这个标识符是一组字符，形成一个唯一的字符串，它被填充并分配给需要多个服务间调用的API调用。然后，相同的跟踪标识符被传播到后续的API调用以进行跟踪。
- en: Errors and issues are imminent in the production system. You need to carry out
    debugging to ascertain the root cause. One of the key tools associated with debugging
    is logs. Logs can also give you warnings related to the system if the system is
    designed to do so. Logs also offer throughput, capacity, and monitoring of the
    health of the system. Therefore, you need an excellent logging platform and strategy
    that enables effective debugging.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产系统中，错误和问题随时可能发生。您需要执行调试以确定根本原因。与调试相关的一个关键工具是日志。如果系统设计为这样做，日志还可以提供与系统相关的警告。日志还提供吞吐量、容量和系统健康状况的监控。因此，您需要一个优秀的日志平台和策略，以实现有效的调试。
- en: There are different open source and enterprise tools available on the market
    for logging, including Splunk, Graylog, and the ELK stack. The ELK stack is the
    most popular of these, and you can use it for free if you are not going to provide
    ELK-based service as SaaS. We are going to use the ELK stack for logging throughout
    this chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上提供了不同的开源和商业日志工具，包括Splunk、Graylog和ELK堆栈。ELK堆栈是其中最受欢迎的，如果您不打算提供基于ELK的服务作为SaaS，您可以使用它。我们将在本章中使用ELK堆栈进行日志记录。
- en: Let’s understand the ELK stack in the next subsection.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一小节中了解ELK堆栈。
- en: Understanding the ELK stack
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解ELK堆栈
- en: The ELK stack comprises three components – Elasticsearch, Logstash, and Kibana.
    All three products are part of Elasticsearch B.V. ([https://www.elastic.co/](https://www.elastic.co/)).
    The ELK stack performs the aggregation, analysis, visualization, and monitoring
    of logs. The ELK stack provides a complete logging platform that allows you to
    analyze, visualize, and monitor all types of logs, including product and system
    logs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ELK堆栈由三个组件组成 – Elasticsearch、Logstash和Kibana。这三个产品都是Elasticsearch B.V.的一部分([https://www.elastic.co/](https://www.elastic.co/))。ELK堆栈执行日志的聚合、分析、可视化和监控。ELK堆栈提供了一个完整的日志平台，允许您分析、可视化和监控所有类型的日志，包括产品和系统日志。
- en: 'You are going to use the following workflow to publish logs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用以下工作流程来发布日志：
- en: '![Figure 12.1 – Log flows in the ELK stack](img/Figure_12.01_B19349.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1 – ELK堆栈中的日志流程](img/Figure_12.01_B19349.jpg)'
- en: Figure 12.1 – Log flows in the ELK stack
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – ELK堆栈中的日志流程
- en: 'Let’s understand the diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这个图：
- en: Services/system logs are pushed to Logstash on the TCP port
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务/系统日志被推送到Logstash的TCP端口
- en: Logstash pushes the logs to Elasticsearch for indexing
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logstash将日志推送到Elasticsearch进行索引
- en: Kibana then uses the Elasticsearch index to query and visualize the logs
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana随后使用Elasticsearch索引来查询和可视化日志
- en: In an ideal production system, you should use one more layer. A broker layer
    such as Redis, Kafka, or RabbitMQ should be placed between the service logs and
    Logstash. This prevents data loss and can handle the sudden spike in input load.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想的生产系统中，您应该使用一个额外的层。例如Redis、Kafka或RabbitMQ这样的代理层应该放在服务日志和Logstash之间。这可以防止数据丢失，并能够处理输入负载的突然增加。
- en: Tips for ELK stack configuration
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ELK堆栈配置的技巧
- en: The ELK stack is fully customizable and comes with a default configuration.
    However, if you are using an Elasticsearch cluster (more than one instance of
    Elasticsearch is deployed), it is better to use an odd number of Elasticsearch
    nodes (instances) to avoid the split-brain problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ELK堆栈是完全可定制的，并附带默认配置。然而，如果您正在使用Elasticsearch集群（部署了多个Elasticsearch实例），最好使用奇数个Elasticsearch节点（实例）以避免脑裂问题。
- en: It is recommended to use the appropriate data type for all the fields (input
    in JSON format for the logs). This will allow you to perform logical checks and
    comparisons while querying the log data. For example, the `http_status < 400`
    check would work only if the `http_status` field type is a number and may fail
    if the `http_status` field type is a string.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 建议为所有字段使用适当的数据类型（日志的输入以 JSON 格式）。这将允许你在查询日志数据时执行逻辑检查和比较。例如，`http_status < 400`
    检查只有在 `http_status` 字段类型是数字时才会工作，如果 `http_status` 字段类型是字符串，可能会失败。
- en: If you are already familiar with the ELK stack, you can skip this introduction
    and move on to the next section. Here, you’ll find a brief introduction to each
    of the tools in the ELK stack.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉 ELK 堆栈，你可以跳过这个介绍，直接进入下一节。在这里，你可以找到 ELK 堆栈中每个工具的简要介绍。
- en: Elasticsearch
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Elasticsearch is one of the most popular enterprise full-text search engines.
    It is based on Apache Lucene and developed using Java. Elasticsearch is also a
    high-performance, full-featured text search engine library. Recent changes in
    the licensing terms have made it restricted open source software, which prevents
    you from offering Elasticsearch or the ELK stack as SaaS. It is distributable
    and supports multi-tenancy. A single Elasticsearch server stores multiple indexes
    (each index represents a database), and a single query can search the data of
    multiple indexes. It is a distributed search engine and supports clustering.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是最受欢迎的企业级全文搜索引擎之一。它基于 Apache Lucene，并使用 Java 开发。Elasticsearch
    还是一个高性能、功能齐全的文本搜索引擎库。最近在许可条款中的变化使其成为受限制的开源软件，这阻止了你将 Elasticsearch 或 ELK 堆栈作为 SaaS
    提供。它是可分发的，并支持多租户。单个 Elasticsearch 服务器可以存储多个索引（每个索引代表一个数据库），单个查询可以搜索多个索引中的数据。它是一个分布式搜索引擎，并支持集群。
- en: It is readily scalable and can provide near-real-time searches with a latency
    of one second. Elasticsearch APIs are extensive and very elaborate. Elasticsearch
    provides JSON-based schema-less storage and represents data models in JSON. Elasticsearch
    APIs use JSON documents for HTTP requests and responses.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它易于扩展，可以提供近实时的搜索，延迟为一秒。Elasticsearch API 非常广泛且非常详尽。Elasticsearch 提供基于 JSON 的无模式存储，并以
    JSON 表示数据模型。Elasticsearch API 使用 JSON 文档进行 HTTP 请求和响应。
- en: Logstash
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Logstash
- en: Logstash is an open source data collection engine with real-time pipeline capabilities.
    It performs three major operations – it collects the data, filters the information,
    and outputs the processed information to data storage, in the same way as Elasticsearch
    does. It allows you to process any event data, such as logs from a variety of
    systems, because of its data pipeline capabilities.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 是一个开源的数据收集引擎，具有实时管道功能。它执行三个主要操作——收集数据、过滤信息，并将处理后的信息输出到数据存储，就像 Elasticsearch
    所做的那样。由于其数据管道功能，它允许你处理任何事件数据，例如来自各种系统的日志。
- en: Logstash runs as an agent that collects the data, parses it, filters it, and
    sends the output to a designated data store, such as Elasticsearch, or as simple
    standard output on a console.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 以代理的形式运行，收集数据、解析数据、过滤数据，并将输出发送到指定的数据存储，如 Elasticsearch，或作为控制台上的简单标准输出。
- en: On top of that, it has a rich set of plugins.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还拥有丰富的插件集。
- en: Kibana
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is an open source web application that is used for visualizing and performing
    information analytics. It interacts with Elasticsearch and provides easy integration
    with it. You can perform searching and display and interact with the information
    stored in Elasticsearch indices.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 是一个开源的 Web 应用程序，用于可视化执行信息分析。它与 Elasticsearch 交互，并提供与它的简单集成。你可以执行搜索、显示和与存储在
    Elasticsearch 索引中的信息进行交互。
- en: It is a browser-based web application that lets you perform advanced data analysis
    and visualize your data in a variety of charts, tables, and maps. Moreover, it
    is a zero-configuration application. Therefore, it does not require any coding
    or additional infrastructure following installation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个基于浏览器的 Web 应用程序，让你能够执行高级数据分析，并以各种图表、表格和地图的形式可视化你的数据。此外，它是一个零配置应用程序。因此，安装后不需要任何编码或额外的基础设施。
- en: Next, let’s learn how to install the ELK stack.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习如何安装 ELK 堆栈。
- en: Installing the ELK stack
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 ELK 堆栈
- en: You can use various methods to install the ELK stack, such as installing individual
    components as per the operating system, downloading the Docker images and running
    them individually, or executing the Docker images using Docker Compose, Docker
    Swarm, or Kubernetes. You are going to use Docker Compose in this chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用各种方法来安装ELK堆栈，例如根据操作系统安装单个组件，下载Docker镜像并单独运行它们，或者使用Docker Compose、Docker
    Swarm或Kubernetes执行Docker镜像。你将在本章中使用Docker Compose。
- en: 'Let’s understand the grammar of a Docker Compose file before we create the
    ELK stack Docker Compose file. A Docker Compose file is defined using YAML. The
    file contains four important top-level keys:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建ELK堆栈Docker Compose文件之前，让我们先了解Docker Compose文件的语法。Docker Compose文件使用YAML定义。该文件包含四个重要的顶级键：
- en: '`version`: This denotes the version of the Docker Compose file format. You
    can use the appropriate version based on the installed Docker Engine. You can
    check [https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/)
    to ascertain the mapping between the Docker Compose file version and the Docker
    Engine version.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`version`：这表示Docker Compose文件格式的版本。你可以根据安装的Docker Engine使用适当的版本。你可以检查[https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/)以确定Docker
    Compose文件版本和Docker Engine版本之间的映射关系。'
- en: '`services`: This contains one or more service definitions. The service definition
    represents the service executed by the container and contains the container name
    (`container_name`), Docker image (`image`), environment variables (`environment`),
    external and internal ports (`port`), the command to be executed when running
    the container (`command`), the network to be used for communicating with other
    services (`networks`), mapping of the host filesystem with a running container
    (`volume`), and the container to be executed once the dependent service has started
    (`depends_on`).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`services`：这包含一个或多个服务定义。服务定义表示由容器执行的服务，并包含容器名称（`container_name`）、Docker镜像（`image`）、环境变量（`environment`）、外部和内部端口（`port`）、运行容器时要执行的命令（`command`）、用于与其他服务通信的网络（`networks`）、将主机文件系统映射到运行容器的映射（`volume`），以及一旦依赖服务启动后要执行容器（`depends_on`）。'
- en: '`networks`: This represents the (top-level) named network that needs to be
    created to establish a communication channel among the defined services. Then,
    this network is used by the service to communicate based on the `networks` key
    of the defined service. The top-level network key contains the driver field, which
    can be `bridge` for a single host and `overlay` when used in Docker Swarm. We
    will use `bridge`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`networks`：这代表需要创建的（顶级）命名网络，用于在定义的服务之间建立通信通道。然后，该网络根据定义服务的`networks`键由服务用于通信。顶级网络键包含驱动器字段，对于单个主机可以是`bridge`，当在Docker
    Swarm中使用时可以是`overlay`。我们将使用`bridge`。'
- en: '`volumes`: A top-level `volumes` key is used to create the named volume that
    mounts the host path. Make sure to use it only if required by the multiple services;
    otherwise, you can use the `volumes` key inside the service definition, which
    would be service-specific.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`volumes`：顶级`volumes`键用于创建挂载主机路径的命名卷。确保仅在需要时使用它；否则，你可以在服务定义内部使用`volumes`键，这将使卷特定于服务。'
- en: 'Now, let’s create the Docker Compose file, `docker-compose.yaml`, in the `Chapter12`
    directory to define the ELK stack. Then, you can add the following code to this
    file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在`Chapter12`目录中创建Docker Compose文件，名为`docker-compose.yaml`，以定义ELK堆栈。然后，你可以将以下代码添加到该文件中：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/docker-compose.yaml](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/docker-compose.yaml)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/docker-compose.yaml](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/docker-compose.yaml)'
- en: First, we defined the version of the Docker Compose file. Then, we created the
    `services` key section, which contains the `elasticsearch` service. The service
    contains the container name, Docker image, environment variables, and network
    (because you want ELK components to communicate with one another). Finally, ports
    are defined in `external:internal` format. You are going to use port `19200` from
    the browser to access it. However, other services will use port `9200` for communicating
    with Elasticsearch.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了Docker Compose文件的版本。然后，我们创建了`services`键部分，其中包含`elasticsearch`服务。该服务包含容器名称、Docker镜像、环境变量和网络（因为你希望ELK组件相互通信）。最后，以`external:internal`格式定义端口。你将使用浏览器中的端口`19200`来访问它。然而，其他服务将使用端口`9200`与Elasticsearch通信。
- en: 'Similarly, you can define the `logstash` service next, as shown in the following
    code block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你可以定义下一个`logstash`服务，如下面的代码块所示：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Logstash configuration contains two extra service keys:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash配置包含两个额外的服务键：
- en: 'First, a command key that contains the `logstash` command with a given configuration
    (using `-e`). The Logstash configuration normally contains three important parts:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，一个包含给定配置（使用`-e`）的`logstash`命令的命令键。Logstash配置通常包含三个重要部分：
- en: '`input`: Logstash input channels, such as `tcp` or `File`. We are going to
    use a TCP input channel. This means that gRPC server and client services will
    push the logs in JSON format (a JSON-coded plugin is used) to `logstash` on port
    `5001`.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`：Logstash输入通道，如`tcp`或`File`。我们将使用TCP输入通道。这意味着gRPC服务器和客户端服务将以JSON格式（使用JSON编码的插件）将日志推送到端口`5001`上的`logstash`。'
- en: '`filter`: The `filter` key contains various filter expressions using different
    means, such as `grok`. You don’t want to filter anything from logs, so you should
    opt out of using this key.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`：`filter`键包含使用不同方式（如`grok`）的各种过滤器表达式。你不想从日志中过滤任何内容，因此应该选择不使用此键。'
- en: '`output`: Where to send the input data after filtering out the information.
    Here, we are using Elasticsearch. Logstash pushes the received log information
    to Elasticsearch on port `9200` and uses the `modern-api` Elasticsearch index.
    This index is then used on Kibana for querying, analyzing, and visualizing the
    logs.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output`：在过滤信息后，将输入数据发送到何处。在这里，我们使用Elasticsearch。Logstash将接收到的日志信息推送到端口`9200`上的Elasticsearch，并使用`modern-api`
    Elasticsearch索引。此索引随后在Kibana上用于查询、分析和可视化日志。'
- en: A second key, `depends_on`, tells Docker Compose to start Elasticsearch before
    executing the `logstash` service.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个键`depends_on`告诉Docker Compose在执行`logstash`服务之前启动Elasticsearch。
- en: 'Next, let’s add the final service, `kibana`, as shown in the following code
    block:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加最后一个服务`kibana`，如下面的代码块所示：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The service’s `kibana` definition is in line with other defined services. It
    uses the `ELASTICSEARCH_HOSTS` environment variable to connect to Elasticsearch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务的`kibana`定义与其他定义的服务一致。它使用`ELASTICSEARCH_HOSTS`环境变量连接到Elasticsearch。
- en: At the end of the Docker Compose file, you define the `elk-net` network, which
    uses the `bridge` driver.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker Compose文件的末尾，你定义了`elk-net`网络，该网络使用`bridge`驱动程序。
- en: 'You are done with configuring the ELK stack Docker Compose file. Let’s now
    start Docker Compose using the following command. If you run this for the very
    first time, local images of Elasticsearch, Logstash, and Kibana will also be fetched.
    You can use the `-f` flag if you’ve used other filenames apart from `docker-compose.yaml`
    or `docker-compose.yml` in the following command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经完成了ELK堆栈Docker Compose文件的配置。现在，让我们使用以下命令启动Docker Compose。如果你是第一次运行此命令，Elasticsearch、Logstash和Kibana的本地镜像也将被检索。如果你在以下命令中使用了除`docker-compose.yaml`或`docker-compose.yml`之外的其他文件名，可以使用`-f`标志：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, the `-d` option is used, which will start Docker Compose in the background.
    It starts the `es-container` Elasticsearch container first based on dependencies
    (the `depends_on` key).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用`-d`选项，这将使Docker Compose在后台运行。它首先根据依赖关系（`depends_on`键）启动`es-container` Elasticsearch容器。
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Elasticsearch uses 2 GB of heap size by default. Docker also uses 2 GB of memory
    by default in some systems, such as Mac.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch默认使用2 GB的堆大小。在某些系统（如Mac）中，Docker也默认使用2 GB的内存。
- en: This may cause an error such as `error-137`. Therefore, you should increase
    the default Docker memory to at least 8 GB (more the better) and swap memory to
    at least 2 GB to avoid such issues.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会导致如`error-137`之类的错误。因此，你应该将默认的Docker内存增加到至少8 GB（越多越好），并将交换内存增加到至少2 GB以避免此类问题。
- en: Please refer to [https://docs.docker.com/config/containers/resource_constraints/#memory](https://docs.docker.com/config/containers/resource_constraints/#memory)
    for Docker memory configurations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 [https://docs.docker.com/config/containers/resource_constraints/#memory](https://docs.docker.com/config/containers/resource_constraints/#memory)
    了解 Docker 内存配置。
- en: You can see that Docker Compose first creates the network, and then creates
    the service containers and starts them. Once all the containers are up, you can
    hit the URL `http://localhost:19200/` (which contains the external port defined
    for the `Elasticsearch` service) in the browser to check whether the Elasticsearch
    instance is up or not.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到 Docker Compose 首先创建网络，然后创建服务容器并启动它们。一旦所有容器都启动，你可以在浏览器中访问 URL `http://localhost:19200/`（其中包含为
    `Elasticsearch` 服务定义的外部端口）来检查 Elasticsearch 实例是否已启动。
- en: 'Once you hit the URL, you may find the following type of JSON response if the
    Elasticsearch service is up:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你访问了 URL，如果 Elasticsearch 服务已启动，你可能会看到以下类型的 JSON 响应：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, let’s check the Kibana dashboard by hitting the URL `http://localhost:5600`
    (which contains the external port defined for the `kibana` service) in the browser.
    This should load the home page of Kibana, as shown in the following screenshot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过在浏览器中访问 URL `http://localhost:5600`（其中包含为 `kibana` 服务定义的外部端口）来检查 Kibana
    仪表板。这应该会加载 Kibana 的主页，如下面的截图所示：
- en: '![Figure 12.2 – Kibana home page](img/Figure_12.02_B19349.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – Kibana 主页](img/Figure_12.02_B19349.jpg)'
- en: Figure 12.2 – Kibana home page
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – Kibana 主页
- en: 'You may be wondering how the logs can be viewed since you have used the `-d`
    option. You can use the `docker-compose logs [service name]` command. If you don’t
    provide the service name, then it will show the logs of all the services. You
    can use the `--tail` flag to filter the number of lines. The `--tail="all"` flag
    will show all the lines:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道如何查看日志，因为你使用了 `-d` 选项。你可以使用 `docker-compose logs [服务名称]` 命令。如果你不提供服务名称，它将显示所有服务的日志。你可以使用
    `--tail` 标志来过滤行数。`--tail="all"` 标志将显示所有行：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can use the following command to stop Docker Compose:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令停止 Docker Compose：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output may vary a bit. However, it should stop and remove all the running
    containers. The command stops the containers according to the dependencies provided
    in the `docker-compose.yaml` file based on the `depends_on` property and then
    removes them. Finally, it removes the network.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能略有不同。然而，它应该停止并删除所有正在运行的容器。该命令根据 `docker-compose.yaml` 文件中提供的依赖关系，基于 `depends_on`
    属性停止容器，然后删除它们。最后，它删除网络。
- en: Next, let’s make the code changes to integrate the application with the ELK
    stack.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们对代码进行更改，以将应用程序集成到 ELK 堆栈中。
- en: Implementing logging and tracing in the gRPC code
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 gRPC 代码中实现记录和跟踪
- en: Logging and tracing go hand in hand. Logging in the application code is already
    taken care of by default. You use Logback for logging. Logs are either configured
    to display on the console or pushed to the filesystem. However, you also need
    to push the logs to the ELK stack for indexing and analysis. For this purpose,
    you make certain changes to the Logback configuration file, `logback-spring.xml`,
    to push the logs to Logstash. On top of that, these logs should also contain tracking
    information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 记录和跟踪是相辅相成的。在应用程序代码中，记录默认已经处理。你使用 Logback 进行记录。日志要么配置为在控制台上显示，要么推送到文件系统。然而，你还需要将日志推送到
    ELK 堆栈以进行索引和分析。为此，你将对 Logback 配置文件 `logback-spring.xml` 进行一些更改，以将日志推送到 Logstash。此外，这些日志还应包含跟踪信息。
- en: Correlation/trace identifiers should be populated and propagated in distributed
    transactions for tracing purposes. A distributed transaction refers to the main
    API call that internally calls other services to serve the request. Before Spring
    Boot 3, Spring provided distributed tracing support through the **Spring Cloud
    Sleuth** library; now, tracing support is provided by Spring Micrometer. It generates
    the trace ID along with the span identifier. The trace ID gets propagated to all
    the participant services during the distributed transaction. The span ID also
    participates in a distributed transaction. However, the scope of the span identifier
    belongs to its service (the one it populates).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪目的，相关/跟踪标识符应在分布式事务中填充和传播。分布式事务指的是内部调用其他服务以处理请求的主要 API 调用。在 Spring Boot 3
    之前，Spring 通过 **Spring Cloud Sleuth** 库提供分布式跟踪支持；现在，跟踪支持由 Spring Micrometer 提供。它生成跟踪
    ID 和跨度标识符。跟踪 ID 在分布式事务期间传播到所有参与服务。跨度 ID 也参与分布式事务。然而，跨度标识符的作用域属于其服务（即它填充的那个服务）。
- en: As communicated earlier, you are going to use Zipkin; therefore, you’ll use
    Brave, a distributed tracing instrumentation library by Zipkin.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您将使用 Zipkin；因此，您将使用 Zipkin 的分布式跟踪仪表库 Brave。
- en: You can copy and enhance the code from [*Chapter 11*](B19349_11.xhtml#_idTextAnchor250),
    *gRPC API Development and Testing*, found at [https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter11](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter11),
    to implement logging and tracing, or refer to this chapter’s code at [https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12)
    for changes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以复制并增强来自[*第11章*](B19349_11.xhtml#_idTextAnchor250)的代码，即*gRPC API 开发和测试*，该章节位于[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter11](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter11)，以实现日志记录和跟踪，或者参考该章节的代码在[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12)中的变更。
- en: First, you’ll make the changes to the gRPC server code in the following subsection.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将在以下子节中对 gRPC 服务器代码进行修改。
- en: Changing the gRPC server code
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改 gRPC 服务器代码
- en: 'To enable tracing and the publishing of logs to the ELK stack, you need to
    make the following code changes, as demonstrated in the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用跟踪并将日志发布到 ELK 堆栈，您需要按照以下步骤进行以下代码更改：
- en: 'Add the following dependencies to the `build.gradle` file:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下依赖项添加到 `build.gradle` 文件中：
- en: '[PRE7]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/build.gradle](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/build.gradle%20)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/build.gradle](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/build.gradle%20)'
- en: 'Here, you are adding the following four dependencies:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您正在添加以下四个依赖项：
- en: '`spring-logback.xml` file.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spring-logback.xml` 文件。'
- en: '**micrometer-tracing-bridge-brave**: This dependency takes care of managing
    the trace and span IDs. Micrometer Tracing Bridge Brave is an abstraction library
    for Zipkin Brave that publishes the collected tracing information to Zipkin using
    Brave.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**micrometer-tracing-bridge-brave**：这个依赖项负责管理跟踪和跨度 ID。Micrometer Tracing Bridge
    Brave 是一个抽象库，用于 Zipkin Brave，它使用 Brave 将收集到的跟踪信息发布到 Zipkin。'
- en: '**spring-boot-starter-actuator**: You used this library earlier in [*Chapter
    9*](B19349_09.xhtml#_idTextAnchor215), *Deployment of Web Services*, to provide
    the health endpoint. It also provides metric endpoints. On top of that, it performs
    the metrics and tracing autoconfiguration.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spring-boot-starter-actuator**：您在[*第9章*](B19349_09.xhtml#_idTextAnchor215)的*Web
    服务部署*中使用了这个库来提供健康端点。它还提供了度量端点。除此之外，它还执行了度量跟踪的自动配置。'
- en: 'Next, add/modify the `spring-logback.xml` file with the following content:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，添加/修改 `spring-logback.xml` 文件，内容如下：
- en: '[PRE8]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/logback-spring.xml](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/resources/logback-spring.xml)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/logback-spring.xml](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/resources/logback-spring.xml)'
- en: Here, you have defined the properties. The values of two of them are taken from
    the Spring configuration file (`application.properties` or `application.yaml`).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您已经定义了属性。其中两个属性的值取自 Spring 配置文件（`application.properties` 或 `application.yaml`）。
- en: 'Let’s now add the Logstash encoder, as shown in the following code block:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们添加 Logstash 编码器，如下所示代码块：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, the `STASH` appender is defined and uses the TCP socket to push the logs
    to Logstash. It contains the `destination` element, which is used to assign Logstash’s
    `<HOST>:<TCP Port>` value. Another element encoder contains the fully qualified
    class name, `LogstashEncoder`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`STASH` 追加器被定义并使用 TCP 套接字将日志推送到 Logstash。它包含一个 `destination` 元素，用于分配 Logstash
    的 `<HOST>:<TCP Port>` 值。另一个元素编码器包含完全限定的类名，`LogstashEncoder`。
- en: 'Finally, you will add the `STASH` appender to the root element, as shown next:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您将向根元素添加 `STASH` 追加器，如下所示：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The root level is set as `INFO` because you want to simply print the information
    logs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 根级别设置为 `INFO`，因为您只想简单地打印信息日志。
- en: Test configuration for Logstash
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 配置测试
- en: 'To disable the LOGSTASH (STASH) appender sending the logs to the Logstash instance,
    do the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用将日志发送到 Logstash 实例的 LOGSTASH (STASH) 追加器，请执行以下操作：
- en: 1\. You may copy `logback-spring.xml` into the `test/resources` directory.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 您可以将 `logback-spring.xml` 复制到 `test/resources` 目录。
- en: 2\. Rename it `test/resources/logback-test.xml`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 将其重命名为 `test/resources/logback-test.xml`。
- en: 3\. Remove the LOGSTASH (STASH) appender and its entry from `<ROOT>`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 从 `<ROOT>` 中移除 LOGSTASH (STASH) 追加器和其条目。
- en: 'Next, let’s add the Spring properties used in this `logback-spring.xml` file
    to `application.properties`, as shown in the following code block:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们将在此 `logback-spring.xml` 文件中使用的 Spring 属性添加到 `application.properties`
    中，如下所示代码块：
- en: '[PRE11]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/application.properties](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/application.properties)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/application.properties](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/resources/application.properties)'
- en: Here, the Logstash destination host is set to `localhost`. If you are running
    on a remote machine, change the host accordingly. The Logstash TCP port is set
    to the same as the Logstash external port set in the Docker Composer file.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Logstash 目标主机设置为 `localhost`。如果您在远程机器上运行，请相应地更改主机。Logstash TCP 端口设置为与 Docker
    Composer 文件中设置的 Logstash 外部端口相同。
- en: Default probability sampling for tracing is only 10% of the actual requests.
    Therefore, we are setting the `management.tracing.sampling.probability` property
    to `1.0`. Now, it collects tracing information for 100% of the requests. Therefore,
    every request will be traced.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪的默认概率采样仅为实际请求的 10%。因此，我们将 `management.tracing.sampling.probability` 属性设置为
    `1.0`。现在，它收集 100% 请求的跟踪信息。因此，每个请求都将被跟踪。
- en: 'The required dependencies and configurations are now set. You can add the tracing
    server interceptor to the gRPC server. (Note: you don’t need a tracing interceptor
    if you are using the RESTful web service as Spring’s autoconfiguration mechanism
    takes care of this.)'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所需的依赖项和配置现已设置。您可以将跟踪服务器拦截器添加到 gRPC 服务器。（注意：如果您使用的是 RESTful Web 服务，则不需要跟踪拦截器，因为
    Spring 的自动配置机制会处理这一点。）
- en: 'First of all, let’s define a new bean in a configuration file, as shown here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在一个配置文件中定义一个新的 bean，如下所示：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/Config.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/Config.java)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/Config.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/blob/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/Config.java)'
- en: Here, you are creating an `ObservationGrpcServerInterceptor` bean, which is
    required for creating the tracing server interceptor. Before Spring Boot 3, the
    `RpcTracing` bean was provided by Spring Sleuth. Now, autoconfiguration of the
    `RpcTracing` bean is not available because Spring Boot 3 supports Spring Micrometer
    over Sleuth. You’ll add this bean in the gRPC server as an interceptor.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您正在创建一个 `ObservationGrpcServerInterceptor` bean，这是创建跟踪服务器拦截器所必需的。在 Spring
    Boot 3 之前，`RpcTracing` bean 由 Spring Sleuth 提供。现在，由于 Spring Boot 3 支持 Spring Micrometer
    而不是 Sleuth，因此 `RpcTracing` bean 的自动配置不可用。您将在 gRPC 服务器中添加此 bean 作为拦截器。
- en: 'Let’s modify the gRPC server Java file (`GrpcServer.java`) to add the tracing
    server interceptor to the gRPC server, as shown in the following code block:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们修改 gRPC 服务器 Java 文件（`GrpcServer.java`），向 gRPC 服务器添加跟踪服务器拦截器，如下代码块所示：
- en: '[PRE13]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/GrpcServer.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/GrpcServer.java%20)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/GrpcServer.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/server/src/main/java/com/packt/modern/api/server/GrpcServer.java)'
- en: Here, you can see that the bean created in the configuration file in the previous
    step is injected using the constructor. Later, the `oInterceptor` bean has been
    used to create the gRPC server interceptor.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到在上一步骤中配置文件中创建的 bean 是通过构造函数注入的。之后，`oInterceptor` bean 已被用于创建 gRPC 服务器拦截器。
- en: 'The changes required to enable log publishing to the ELK stack and tracing
    are made to the gRPC server. You can rebuild the gRPC server and run its `JAR`
    file to see the changes in effect. Check the following command-line output for
    the reference:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用将日志发布到 ELK 堆栈和跟踪，所需的更改是对 gRPC 服务器的修改。您可以重新构建 gRPC 服务器并运行其 `JAR` 文件以查看更改效果。以下命令行输出仅供参考：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can see that the logs are following the pattern configured in `logback-spring.xml`.
    The log block printed after `INFO` contains the application/service name, as well
    as the trace and span IDs. The highlighted line is showing a *blank* trace ID
    and span ID because no external call is made that involves the distributed transaction.
    The trace and span IDs only get added to logs if the distributed transaction (service-to-service
    communication) is called.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到日志遵循 `logback-spring.xml` 中配置的模式。在 `INFO` 之后打印的日志块包含应用程序/服务名称，以及跟踪和跨度 ID。高亮显示的行显示一个
    *空白* 的跟踪 ID 和跨度 ID，因为没有进行涉及分布式事务的外部调用。只有在调用分布式事务（服务间通信）时，跟踪和跨度 ID 才会添加到日志中。
- en: Similarly, you can add the logging and tracing implementation in the gRPC client
    next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以在 gRPC 客户端中添加日志和跟踪实现。
- en: Changing the gRPC client code
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改 gRPC 客户端代码
- en: 'To enable tracing and the publishing of logs to the ELK stack, you need to
    make code changes in the gRPC client as well, which are very similar to the changes
    implemented in the gRPC server code. Refer to the following steps for more information:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用跟踪并将日志发布到 ELK 堆栈，您还需要在 gRPC 客户端进行代码更改，这些更改与 gRPC 服务器代码中实现的更改非常相似。有关更多信息，请参考以下步骤：
- en: 'Add the following dependencies to the `build.gradle` file:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下依赖项添加到 `build.gradle` 文件中：
- en: '[PRE15]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/build.gradle](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/build.gradle)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/build.gradle](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/build.gradle)'
- en: These are the same dependencies you added to the gRPC server.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是与您添加到 gRPC 服务器的相同依赖项。
- en: Next, you can add the `logback-spring.xml` file in the same way you added it
    to the gRPC server code to configure the logging. Make sure to use `chapter12-grpc-client`
    in place of `chapter12-grpc-server` in the XML file.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以像添加到 gRPC 服务器代码中一样添加 `logback-spring.xml` 文件以配置日志。请确保在 XML 文件中使用 `chapter12-grpc-client`
    替换 `chapter12-grpc-server`。
- en: 'Next, let’s add the following Spring properties to `application.properties`.
    A few of these properties are referred to in the `logback-spring.xml` file as
    well:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们将以下 Spring 属性添加到 `application.properties` 中。其中一些属性也在 `logback-spring.xml`
    文件中引用：
- en: '[PRE16]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/resources/application.properties](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/resources/application.properties)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/resources/application.properties](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/resources/application.properties)'
- en: The required dependencies and configurations are now set. Now, you can add tracing
    to the gRPC client. `ObservationGrpcClientInterceptor`, provided by the Micrometer
    library, provides the interceptor for the gRPC client.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所需的依赖项和配置现已设置。现在，你可以向gRPC客户端添加跟踪。由Micrometer库提供的`ObservationGrpcClientInterceptor`为gRPC客户端提供了拦截器。
- en: Note
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You don’t need additional tracing changes if you are using the RESTful web service;
    Spring autoconfiguration takes care of this.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是RESTful Web服务，则不需要额外的跟踪更改；Spring自动配置会处理这一点。
- en: 'First of all, let’s define a new bean, `ObservationGrpcClientInterceptor`,
    in a configuration file, as shown next:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在一个配置文件中定义一个新的bean，`ObservationGrpcClientInterceptor`，如下所示：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/Config.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/Config.java)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/Config.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/Config.java)'
- en: 'Now, you can modify the gRPC client Java file to add the `ObservationGrpcClientInterceptor`
    interceptor to the gRPC client:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以修改gRPC客户端Java文件，向gRPC客户端添加`ObservationGrpcClientInterceptor`拦截器：
- en: '[PRE18]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/client/GrpcClient.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/client/GrpcClient.java)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/client/GrpcClient.java](https://github.com/PacktPublishing/Modern-API-Development-with-Spring-6-and-Spring-Boot-3/tree/dev/Chapter12/client/src/main/java/com/packt/modern/api/client/GrpcClient.java)'
- en: Here, you can see that the bean created in the configuration file in the previous
    step is autowired. Later, the `observationGrpcClientInterceptor` interceptor is
    added to the client.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到在前面步骤中配置文件中创建的bean已自动装配。稍后，将`observationGrpcClientInterceptor`拦截器添加到客户端。
- en: 'The changes required to facilitate log publishing to the ELK stack and tracing
    are also done for the gRPC client. You can now rebuild the gRPC client and run
    its JAR file to see the changes in effect. Check the following command-line output
    for reference:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于将日志发布到ELK堆栈和跟踪所做的更改也已应用于gRPC客户端。你现在可以重新构建gRPC客户端并运行其JAR文件以查看更改的效果。以下命令行输出仅供参考：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see that the logs follow the pattern configured in `logback-spring.xml`.
    The log block printed after `INFO` contains the application/service name, trace
    ID, and span ID. The trace and span IDs are blank because they only get added
    to logs if distributed transactions (service-to-service communication) are called.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到日志遵循在`logback-spring.xml`中配置的模式。在`INFO`之后打印的日志块包含应用程序/服务名称、跟踪ID和跨度ID。跟踪和跨度ID为空，因为只有在调用分布式事务（服务间通信）时才会将它们添加到日志中。
- en: The changes required to enable log aggregation and distributed tracing in both
    the gRPC server and client services are now complete.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 启用gRPC服务器和客户端服务中的日志聚合和分布式跟踪所需的更改现已完成。
- en: Next, you’ll test the changes and view the logs in Kibana.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将测试更改并在Kibana中查看日志。
- en: Testing the logging and tracing changes
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试日志和跟踪更改
- en: Before beginning testing, make sure that the ELK stack is up and running. Also,
    make sure that you first start the gRPC server and then the gRPC client service.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始测试之前，请确保ELK堆栈正在运行。此外，请确保首先启动gRPC服务器，然后启动gRPC客户端服务。
- en: You can add the appropriate log statements to your services for verbose logs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为你的服务添加适当的日志语句以生成详细日志。
- en: 'Let’s run the following command in the new terminal window. This will call
    the `/charges` REST endpoint in the gRPC client service:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的终端窗口中运行以下命令。这将调用gRPC客户端服务中的`/charges` REST端点：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It should respond with the following JSON output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 应该返回以下JSON输出：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The previous `curl` command should generate logs like the following one in
    the gRPC client:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的`curl`命令应该在gRPC客户端生成如下日志：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, the blocks highlighted the application name (`grpc-client`), trace ID
    (`64456d940c51e3e2baec07f7448beee6`), and span ID (`baec07f7448beee6`).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，高亮显示的块包括了应用名称（`grpc-client`）、跟踪ID（`64456d940c51e3e2baec07f7448beee6`）和跨度ID（`baec07f7448beee6`）。
- en: 'The command should also generate the following logs in the gRPC server:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 命令还应在gRPC服务器生成以下日志：
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'implementation ''net.logstash.logback:logstash-logback-                encoder:7.3''implementation
    ''io.micrometer:micrometer-tracing-    bridge-brave''implementation ''application.properties
    file (for both the gRPC server and client):'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: implementation 'net.logstash.logback:logstash-logback-encoder:7.3'
- en: '[PRE24]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: $ java -jar zipkin-server-2.24.0-exec.jar
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: $ java -jar zipkin-server-2.24.0-exec.jar
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: $ curl http://localhost:8081/charges
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: $ curl http://localhost:8081/charges
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '2023-04-24 11:35:10.313      INFO [grpc-client,64461c16391707ee95478f957f3ccb1d,95478f957f3ccb1d]     62484
    --- [nio-8081-exec-2] c.p.m.api.controller.ChargeController    : CustomerId :
    ab1ab2ab3ab4ab5'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '2023-04-24 11:35:10.313      INFO [grpc-client,64461c16391707ee95478f957f3ccb1d,95478f957f3ccb1d]     62484
    --- [nio-8081-exec-2] c.p.m.api.controller.ChargeController    : 客户端ID : ab1ab2ab3ab4ab5'
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Part 4 – GraphQL
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分 – GraphQL
- en: In this part, you will learn about GraphQL-based API development. After completing
    this section, you will know the in-depth fundamentals of GraphQL, be able to differentiate
    between REST, reactive, and gRPC APIs, in the context of GraphQL thoroughly, and
    understand when to use which API style. You will also learn how to design the
    GraphQL schema, which will be used to generate Java code. Finally, you will learn
    how to write data fetchers and loaders to resolve query fields to serve the GraphQL
    API requests.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，你将学习基于GraphQL的API开发。完成本节后，你将了解GraphQL的深入基础知识，能够区分REST、响应式和gRPC API，在GraphQL的背景下进行深入理解，并了解何时使用哪种API风格。你还将学习如何设计GraphQL模式，该模式将用于生成Java代码。最后，你将学习如何编写数据检索器和加载器，以解析查询字段以服务GraphQL
    API请求。
- en: 'This part contains the following chapters:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分包含以下章节：
- en: '[*Chapter 13*](B19349_13.xhtml#_idTextAnchor288), *Getting Started with GraphQL*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B19349_13.xhtml#_idTextAnchor288)，*开始使用GraphQL*'
- en: '[*Chapter 14*](B19349_14.xhtml#_idTextAnchor313), *GraphQL API Development
    and Testing*'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第14章*](B19349_14.xhtml#_idTextAnchor313)，*GraphQL API开发和测试*'
