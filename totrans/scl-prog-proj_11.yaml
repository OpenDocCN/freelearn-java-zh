- en: Batch and Streaming Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced Spark and obtained BTC/USD transaction
    data from [www.bitstamp.net](http://www.bitstamp.net). Using that data, we can
    now perform some analysis on it.
  prefs: []
  type: TYPE_NORMAL
- en: First, we are going to query this data using a notebook tool named Apache Zeppelin.
    After that, we will write a program that receives the live transactions from [https://www.bitstamp.net/](https://www.bitstamp.net/) and
    sends them to a Kafka topic as they arrive.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will use Zeppelin again to run some streaming analytics queries
    on the data coming to the Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing transactions with Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming transactions to Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing Streaming transactions with Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Zeppelin is an open source software offering a web interface to create
    notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: In a notebook, you can inject some data, execute snippets of code to perform
    analysis on the data, and then visualize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zeppelin is a collaborative tool; several users can use it simultaneously.
    You can share notebooks and define the role of each user. You would typically
    define two different roles:'
  prefs: []
  type: TYPE_NORMAL
- en: The writer, usually a developer, can edit all the paragraphs and create forms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The end user does not know much about the technical implementation details.
    He will just want to change some values in a form and then look at the effect
    on the results. The results can be a table or a graph, and can be exported to
    CSV.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before installing Zeppelin, you need to have Java and Spark installed on your
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to install Zeppelin:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the binary from: [http://zeppelin.apache.org/download.html](http://zeppelin.apache.org/download.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explode the `.tgz` file using your favorite program
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it. Zeppelin is installed and configured with default settings. The next
    step is to start it.
  prefs: []
  type: TYPE_NORMAL
- en: Starting Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start the Zeppelin daemon process, run the following command in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linux and macOS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Windows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Zeppelin is now running and ready to accept requests on [http://localhost:8080](http://localhost:8080).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the URL in your favorite browser. You should see this page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/765efbd0-b62f-4e64-9acf-7d7b0ac45600.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's create a new notebook to test if our installation is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the [http://localhost:8080](http://localhost:8080) home page, click on Create
    New Note and set `Demo` as a name in the popup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d5686f6-7f3e-4bab-a385-af87777a33c2.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned in the window, the default interpreter will be Spark, and this
    is what we want. Click on Create now.
  prefs: []
  type: TYPE_NORMAL
- en: 'You just created your first notebook. You should see the following in your
    browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd21ed57-f511-4700-8c70-aabea16b0e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of a notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A notebook is a document in which you can add **paragraphs**. Each paragraph
    can use a different **interpreter**, which interacts with a specific framework
    or language.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each paragraph, there are two sections:'
  prefs: []
  type: TYPE_NORMAL
- en: The top one is an editor, into which you can type some source code and run it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom one displays the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If, for example, you choose to use a Spark interpreter, all the lines of code
    you write in the paragraph will be interpreted (like the REPL seen in [Chapter
    1](db4d7854-92ff-43c8-a87b-2d605bf88d1b.xhtml), *Writing Your First Program*).
    All variables defined will be kept in memory and shared with all the other paragraphs
    of the notebook. Similarly to the Scala REPL, when you execute it, the output
    of the execution along with the types of the variables defined will be printed
    in the result section.
  prefs: []
  type: TYPE_NORMAL
- en: Zeppelin is installed by default with interpreters like Spark, Python, Cassandra,
    Angular, HDFS, Groovy, and JDBC.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a paragraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Well, our first notebook is completely empty! We can reuse the example used
    in [Chapter 10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml), *Fetching and Persisting
    Bitcoin Market Data*, in the section *Exploring Spark''s API with the Scala console*.
    If you remember, we created `Dataset` from a sequence of strings. Enter the following
    in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then hit *Shift* + *Enter* (or click on the play triangle of the UI).
  prefs: []
  type: TYPE_NORMAL
- en: 'The interpreter runs and you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67ae70c1-f3e9-42f9-9385-087a1b9c22d8.png)'
  prefs: []
  type: TYPE_IMG
- en: The notebook created the dataset from the sequence and printed in the result
    section of the paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in the previous chapter, when we executed this code from the Scala
    console, we had to create `SparkSession` and had to add some imports. When we
    use the Spark interpreter in Zeppelin, all the implicits and imports are done
    automatically and `SparkSession` is created for us.
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` is exposed with the variable name `spark`. You can, for instance,
    get the Spark version with the following code in a paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After having run the paragraph, you should see the version printed in the result
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/573f1cad-9491-45e0-8cee-bd1ae4a8b863.png)'
  prefs: []
  type: TYPE_IMG
- en: At that point, we tested the installation and it is working properly.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing charts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to create a basic `Dataset` and draw a chart of
    its data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new paragraph, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We define a `Demo` class with `id` and a `data` property, then we create a list
    of different `Demo` objects and convert it into `Dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `dataDS` dataset ,we call the `.createOrReplaceTempView("demoView")` method.
    This function is going to register the dataset as a temporary view. With this
    view defined, we can use SQL to query this dataset. We can try it by adding in
    a new paragraph, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The newly created paragraph starts with `%sql`. This defines the interpreter
    that we use. In our case, this is the Spark SQL interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The query selects all the columns from `demoView`. After you hit *Shift* + *Enter*,
    the following table will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45161a31-1db6-4730-bd04-f17c97cc92f3.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the SQL interpreter shows the result of the query by displaying
    a table in the result section of the paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Notice the menu at the top of the table. There are multiple ways to represent
    the data—bar chart, pie chart, area chart, line chart, and scatter chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the bar chart. The notebook now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cbf9992-902a-46de-841d-d804651de9e8.png)'
  prefs: []
  type: TYPE_IMG
- en: It might seem odd that we don't have data available. Actually, we need to configure
    the chart to get it working. Click on settings, then define which column is the
    value and on which one you want to aggregate the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag the `id` tag to the groups box and `data` to the values box. As we choose
    to group the IDs, `SUM` of `data` is performed. The configuration and the updated
    chart should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bad433c-ef81-4b33-8833-a42750cf47e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The chart is now correct. The sum of all the `id` `a` is `3`, `8` for the `b` `id` and `4` for
    the `c` `id`.
  prefs: []
  type: TYPE_NORMAL
- en: We are now familiar enough with Zeppelin to perform analytics on the Bitcoin
    transactions data that we produced in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing transactions with Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we wrote a program that saves BTC/USD transactions
    to Parquet files. In this section, we are going to use Zeppelin and Spark to read
    those files and draw some charts.
  prefs: []
  type: TYPE_NORMAL
- en: If you came directly to this chapter, you first need to set up the `bitcoin-analyser` project,
    as explained in [Chapter 10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml), *Fetching
    and Persisting Bitcoin Market Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you can either:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `BatchProducerAppIntelliJ`. This will save the last 24 hours of transactions
    in the `data` folder of the project directory, then save new transactions every
    hour.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the sample transaction data that is committed in GitHub. You will have to
    check out this project: [https://github.com/PacktPublishing/Scala-Programming-Projects](https://github.com/PacktPublishing/Scala-Programming-Projects).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing our first chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With these Parquet files ready, create a new notebook in Zeppelin and name
    it `Batch analytics`. Then, in the first cell, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first line creates `DataFrame` from the transactions files. You need to
    replace the absolute path in the `parquet` function with the path to your Parquet
    files. The second line uses the special `z` variable to show the content of `DataFrame` in
    a table. This `z` variable is automatically provided in all notebooks. Its type
    is `ZeppelinContext`, and it allows you to interact with the Zeppelin renderer
    and interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the cell with *Shift* + *Enter*. You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65ccb66a-9c04-49f3-88a0-7a9815fedc81.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a warning message saying that the output is truncated. This is because
    we retrieved too much data. If we then attempt to draw a chart, some data will
    be missing.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this issue could be to change Zeppelin's settings and increase
    the limit. But if we were to do that, the browser would have to keep a lot of
    data in memory, and the notebook's file when saved to disk would be large as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better solution is to aggregate the data. We cannot show all the transactions
    that happened in a day on a chart, but if we can aggregate them with a window
    of `20 minutes`, that will reduce the number of data points to display. Create
    a new cell with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this new cell. You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a further description for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we group our transactions with a window of `20 minutes`. In the output,
    you can see that the type of `group` is `RelationalGroupedDataset`. This is an
    intermediate type on which we must call an aggregation method to produce another `DataFrame`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we call the `agg` method on `group` to compute several aggregations in
    one go. The `agg` method takes several `Column` arguments as `vararg`. The `Column` objects
    we pass are obtained by calling various aggregation functions from the `org.apache.spark.sql.functions` object.
    Each column is renamed so that we can refer to it easily later on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting variable `tmpAgg` is `DataFrame` which has a column `window` of
    type `struct`. `struct` nests several columns. In our case, it has a column `start` and
    a column `end` of type `timestamp`. `tmpAgg` also has all the columns containing
    the aggregations—`count`, `avgPrice`, and `sumAmount`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, we select only the columns that we are interested in and then assign
    the resulting `DataFrame` in a variable `aggregate`. Notice that we can refer
    to the nested columns of the window column with the `“.”` notation. Here we select
    all rows apart from `window.end`. We then `sort` the `DataFrame` by ascending `start` time
    so that we can use `start` as the *x* axis of our future chart. Finally, we `cache` the `DataFrame` so
    that Spark will not have to reprocess it when we create other cells with different
    charts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Underneath the output, Zeppelin displays a table, but this time it does not
    give us any warning. It is able to load all the data without truncation. We can,
    therefore, draw a chart:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the line chart button
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drag and drop the `start` column in the section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drag and drop `avgPrice` and `lastPrice` in the values section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7f27a93-4652-4491-9064-297ecfcf2506.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see the evolution of the average price and of the last price in 20-minute
    increments.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you hover the mouse on the chart, Zeppelin displays the information of the
    corresponding data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eb66130-e99b-49a2-8ccd-93e824f6a5b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Feel free to try different types of charts, with different *values* for the *y *axis.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing more charts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since our `aggregate DataFrame` is available in scope, we can create new cells
    to plot different charts. Create two new cells with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, on each new cell, click on the line chart button and drag the `start` column
    to the keys section. After that:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first chart, drag and drop `sumAmount` in the values section. The chart
    shows the evolution of the volume exchanged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second chart, drag and drop `stddevPrice` in the values section. The
    chart shows the evolution of the standard deviation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on settings on both cells to hide the settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Hide editor on both cells.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should obtain something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9121c11-a18f-456a-b412-cde98bbeba92.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe spikes at around the same time—when there is a spike in volume
    exchanged, the standard deviation spikes as well. This is because many large transactions
    move the price significantly, which increases the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: You now have the basic building blocks to run your own analysis. With the power
    of the `Dataset` API, you could drill down to a specific period using `filter` and
    then plot the evolution of a moving average over different time spans.
  prefs: []
  type: TYPE_NORMAL
- en: The only problem with our notebook is that we would only get new transactions
    every hour. The `BatchProducerApp` that we wrote in the previous chapter does
    not produce transactions more frequently, and if you try to call the REST API
    every few seconds, you will get blacklisted by the Bitstamp server. The preferred
    way of getting live transactions is to use a WebSocket API.
  prefs: []
  type: TYPE_NORMAL
- en: For solving this, in the next section, we are going to build an application
    called `StreaminProducerApp` that will push live transactions to a Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, *Introducing Lambda Architecture*, we mentioned that
    Kafka is used for stream processing. Apache Kafka is a high throughput distributed
    messaging system. It allows decoupling the data coming in with the data going
    out.
  prefs: []
  type: TYPE_NORMAL
- en: It means that multiple systems (**producers**) can send messages to Kafka. Kafka
    will then deliver these messages out to the **consumers** registered.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka is distributed, resilient, and fault-tolerant, and has a very low latency.
    Kafka can scale horizontally by adding more machines to the system. It is written
    in Scala and Java.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka is broadly used; Airbnb, Netflix, Uber, and LinkedIn use this technology.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this chapter is not to become an expert in Kafka, but rather
    to familiarize you with the fundamentals of this technology. By the end of the
    chapter, you will be able to understand the use case developed in this chapter—streaming
    bitcoin transactions in a Lambda architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Topics, partitions, and offsets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to process the messages exchanged between the producers and the consumer,
    Kafka defines three main components—topics, partitions, and offsets.
  prefs: []
  type: TYPE_NORMAL
- en: A **topic** groups messages of the same type for streaming. It has a name and
    a number of partitions. You can have as many topics as you want. As Kafka can
    be distributed on multiple nodes, it needs a way to materialize the stream of
    messages on these different nodes. This is why the message stream (topic) is split into
    multiple **partitions**. Each partition contains a portion of the messages sent
    to a topic.
  prefs: []
  type: TYPE_NORMAL
- en: Each node of the Kafka cluster manages several partitions. A given partition
    is assigned to several nodes. This avoids losing data if a node is lost, and allows
    a higher throughput. By default, Kafka uses the hash code of the message to assign
    it to a partition. You can define a key for a message to control this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In a partition, the order of the message is guaranteed, and once a message is
    written on it, it cannot be changed. The messages are **immutable**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A topic can be consumed by zero to many **consumer** processes. A key feature
    of Kafka is that each consumer can consume the stream at his own pace: a producer
    can be sending message 120, while one consumer is processing message 40, and another
    one is processing message 100.'
  prefs: []
  type: TYPE_NORMAL
- en: This asymmetry is made possible by storing the messages on disk. Kafka keeps
    the messages for a limited amount of time; the default setting is one week.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, Kafka uses IDs to keep track of the messages and a sequence number
    to generate these IDs. It maintains one unique sequence per partition. This sequence
    number is called an **offset**. An offset only has a meaning for a specific partition.
  prefs: []
  type: TYPE_NORMAL
- en: Each node of the Kafka cluster runs a process called a **broker. **Each broker
    manages several topics with one or more partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize everything with an example. We can define a topic named `shapes` with
    a number of partitions equal to two. This topic receives messages, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ac8b7b-69e6-47fd-9d1c-62480007107d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say we have three nodes in the cluster. The representation of the brokers,
    partitions, offsets, and messages would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58ec2d48-b82b-4421-9885-563af723c6e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that as we defined only two partitions and we have three machines, one
    of the machines is not going to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Another option available when you define a partition is the number of replicas.
    To be resilient, Kafka replicates the data in multiple brokers, so if one broker
    is failing, the data can be retrieved from another one.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should now be more familiar with the fundamentals of the Kafka architecture.
    We will now spend a little bit of time on two other components: the producer and
    the consumer.'
  prefs: []
  type: TYPE_NORMAL
- en: Producing data into Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The component to send messages to a topic is called a **producer** in Kafka.
    The responsibility of a producer is to automatically select a partition through
    a broker to write messages. In case of failure, the producer should automatically
    recover.
  prefs: []
  type: TYPE_NORMAL
- en: The partition selection is based on a key. The producer will take care of sending
    all messages with the same key to the same partition. If there is no key provided
    with the message, the producer load balances the messages using a round-robin
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can configure the producer with the level of acknowledgment you want to
    receive. There are three levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acks=0`: The producer sends the data and forgets it; no acknowledgment is
    done. There is no guarantee, and messages could be lost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acks=1`: The producer waits for an acknowledgment of the first replicas. You
    can be sure that no data will be lost as long as the broker that acknowledged
    does not crash.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acks=all`: The producer waits for an acknowledgment of all the replicas. You
    can be sure that no data will be lost even if one broker crashes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, if you want an acknowledgment of all the replicas, you might expect
    longer latencies. Acknowledgment of the first replica (`ack=1`) is a good compromise
    between safety and latency.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming data from Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To read messages from a topic, you need to run a **consumer**. As with the producer,
    the consumer will automatically select the broker to read from and will recover
    in case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: The consumer reads the messages from all partitions. Within a partition, it
    is guaranteed to receive messages in the same order as they were produced.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer group
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can have multiple consumers for the same topic in a **consumer group**.
    If you have the same number of consumers and partitions in a group, each consumer
    will read only one partition. This allows parallelizing the consumption. If you
    have more consumers than partitions, the first consumer takes the partition and
    the rest of the consumers are in waiting mode. They will consume only if a consumer
    reading on a partition is failing.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we only have one group of consumers reading from partitions. In a
    typical system, you would have many consumer groups. For example, in the case
    of a Bitcoin transaction, we could have a consumer group reading the messages
    to perform analytics on it, and another group for a user interface that shows
    a feed of all the transactions. The latency between the two cases are not the
    same, and we don't want to have a dependency between each use case. For that purpose,
    Kafka uses the notion of groups.
  prefs: []
  type: TYPE_NORMAL
- en: Offset management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important concept is that when a consumer reads a message, it will
    automatically inform Kafka of the offset that was read. This way, if a consumer
    dies, Kafka knows the offset of the last message read. When the consumer restarts,
    it can send the next message to it. As for the producer, we can decide when to
    commit offsets. There are three options :'
  prefs: []
  type: TYPE_NORMAL
- en: At most once; as soon as the message is received, the offset is committed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least once; the offset is committed after the message has been processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exactly once; the offset is committed after the message has been processed,
    and there are additional constraints on the producer—it must not resend messages
    in case of network failures. The producer must have idempotent and transactional
    capabilities, which were introduced in Kafka 0.11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most commonly used is the *at least once* option. If the processing of the
    message is failing, you can reprocess it, but you might occasionally receive the
    same message multiple times. In the case of *at most once*, if anything goes wrong
    during the process, the message will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: OK, enough theor. We have learned about topics, partitions, offsets, consumers,
    and producers. The last piece of missing knowledge is a simple question—how do
    I connect my producer or consumer to Kafka?
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced in the section *Topics, partitions, and offsets* the notion of
    brokers. Brokers are deployed on multiple machines, and all the brokers form what
    we call a Kafka cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to connect to a Kafka cluster, all you need to know is the address
    of one of the brokers. All the brokers know about all the metadata of the cluster—brokers,
    partitions, and topics. Internally, Kafka uses a product named **Zookeeper**.
    This allows the sharing of all this metadata between brokers.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming transactions to Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to write a program that produces a stream of
    the BTC/USD transactions that happen in real time. Our program will:'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to the WebSocket API of Bitstamp to get a stream of transactions in
    JSON format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each transaction coming in the stream, it will:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deserialize it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert it to the same `Transaction` case class that we used in `BatchProducer` in
    the previous chapter
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Serialize it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Send it to a Kafka topic
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will use Zeppelin again with Spark Streaming to query
    the data streamed to the Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: Subscribing with Pusher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to Bitstamp's WebSocket API for live transactions: [https://www.bitstamp.net/websocket/](https://www.bitstamp.net/websocket/).
  prefs: []
  type: TYPE_NORMAL
- en: You will see that this API uses a tool called Pusher channels for real-time WebSocket
    streaming. The API documentation provides a Pusher Key that we need to use to
    receive live transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pusher** channels is a hosted solution for delivering a stream of messages
    using a publish/subscribe pattern. You can find out more on their website: [https://pusher.com/features](https://pusher.com/features).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to use Pusher to receive some live BTC/USD transactions. Open the
    project `bitcoin-analyser`, start a new Scala console, and type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first line, we create the Pusher client with the key that was specified
    in Bitstamp's documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we connect to the remote Pusher server and subscribe to the `"live_trades"` channel.
    We obtain an object of type `channel`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use the `channel` to register (`bind`) a callback function that
    will be called every time the `channel` receives a new event with the name `trade`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a few seconds, you should see some trades being printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The data is in JSON format, with a schema that conforms to what was defined
    in Bitstamp’s WebSocket documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these few lines, we can write the first building block of our application.
    Create a new object, `StreamingProducerApp`, in the package `coinyser` in `src/main/scala` with
    the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Our function `subscribe` takes a `Pusher` instance (of type `Client`) and a
    callback function, `onTradeReceived`, and returns `IO[Unit]`. When `IO` is run,
    it will call `onTradeReceived` each time a new trade is received. The implementation
    is similar to the few lines that we typed into the console. It basically wraps
    every side-effecting function in an `IO`.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of conciseness and readability, we have not exposed the details
    of this function's unit test. You can check it out in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: For writing the test, we had to create a `FakePusher` class that implements
    a few methods of the `Client` interface.
  prefs: []
  type: TYPE_NORMAL
- en: Deserializing live transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The JSON payload that we receive when we subscribe to live transactions is slightly
    different from the one we had in the REST endpoint for fetching batch transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to need to deserialize it to a case class before we can transform
    it to the same `Transaction` case class that we used in the previous chapter.
    For this, first create a new case class, `coinyser.WebsocketTransaction` in `src/main/scala`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The names and types of the attributes correspond to the JSON attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we can write a unit test for a new function, `deserializeWebsocketTransaction`.
    Create a new class, `coinyser.StreamingProducerSpec`, in `src/test/scala`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The test is straightforward—we define a sample JSON string, call the function
    under `test`, and make sure the deserialized object `SampleWebsocketTransaction` contains
    the same values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to implement the function. Add a new `val mapper: ObjectMapper` and
    a new function `deserializeWebsocketTransaction` to the `StreamingProducer` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For this part of the project, we use the **Jackson** Java library to deserialize/serialize
    JSON objects. It is the library that is used under the hood by Spark when it reads/writes
    dataframe from/to JSON. Hence, it is available without adding any more dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a constant, `mapper: ObjectMapper`, which is the entry point of Jackson
    for serializing/deserializing classes. We configure it to write timestamps in
    a format that is compatible with what Spark can parse. This will be necessary
    later on when we read the Kafka topic using Spark. Then the function’s implementation
    calls `readValue` to deserialize the JSON into `WebsocketTransaction`.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting to transaction and serializing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are able to listen to live transactions and deserialize them to objects
    of type `WebsocketTransaction`. The next steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert these `WebsocketTransaction` objects into the same case class, `Transaction`,
    that we defined in the previous chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send these `Transaction` objects to a Kafka topic. But for this, they need to
    be serialized first. The simplest way is to serialize to JSON.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As usual, we start by writing tests. Add the following tests to `StreamingProducerSpec`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The test for `convertWsTransaction` checks that, once converted, `SampleWebsocketTransaction` is
    the same as `SampleTransaction`.
  prefs: []
  type: TYPE_NORMAL
- en: The test for `serializeTransaction` checks that, once serialized, `SampleTransaction` is
    the same as `SampleJsonTransaction`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of these two functions is straightforward. Add the following
    definitions in `StreamingProducer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In `convertWsTransaction`, we have to multiply the timestamp by 1,000 to get
    the time in milliseconds. The other attributes are just copied.
  prefs: []
  type: TYPE_NORMAL
- en: In `serializeTransaction`, we reuse the `mapper` object to serialize a `Transaction` object
    to JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have all the building blocks to create our application. Create a new
    object, `coinyser.StreamingProducerApp`, and type the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Our object extends `cats.IOApp`, and as such we have to implement a `run` function
    that returns `IO[ExitCode]`.
  prefs: []
  type: TYPE_NORMAL
- en: In the `run` function, we first create `KafkaProducer[Int, String]`. This Java
    class from the Kafka client library will allow us to send messages to a topic.
    The first type parameter is the type of the message's key. The key of our message
    will be the `tid` attribute in the `Transaction` case class, which is of type `Int`.
    The second type parameter is the type of the message itself. In our case, we use `String`,
    because we are going to serialize our messages to JSON. If storage space was a
    concern, we could have used `Array[Byte]` and a binary serialization format such
    as Avro.
  prefs: []
  type: TYPE_NORMAL
- en: The `props Map` passed to construct `KafkaProducer` contains various configuration
    options for interacting with the Kafka cluster. In our program, we pass the minimum
    set of properties and leave the others with default values, but there are many
    more fine-tuning options. You can find out more here: [http://kafka.apache.org/documentation.html#producerconfigs](http://kafka.apache.org/documentation.html#producerconfigs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we call the `StreamingProducer.subscribe` function that we implemented
    earlier, and pass a callback function that will be called each time we receive
    a new transaction. This anonymous function will:'
  prefs: []
  type: TYPE_NORMAL
- en: Deserialize the JSON into `WebsocketTransaction`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert `WebsocketTransaction` into `Transaction`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serialize `Transaction` to JSON.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send the JSON transaction to the Kafka topic using `kafkaProducer.send`. For
    this, we have to create `ProducerRecord`, which contains the `topic` name, the
    key, and the content of the message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `subscribe` function returns `IO[Unit]`. This will start a background thread
    and complete immediately when we run it. But we do not want to stop the main thread
    immediately; we need to keep our program running forever. This is why we `flatMap` it
    and return `IO.never`, which will keep the main thread running until we kill the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Running StreamingProducerApp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before running our application, we need to start a Kafka cluster. For the sake
    of simplicity, we are just going to start a single broker on your workstation.
    If you wish to set up a multinode cluster, please refer to the Kafka documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Kafka 0.10.2.2 from this URL: [https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz](https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz).
    We use the version 1.1.1 because we have tested it with the`spark-sql-kafka-0.10` library
    at the time of writing. You could use a later version of Kafka, but it is not
    guaranteed that an old Kafka client can always communicate with a more recent
    broker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a console and then decompress the package in your favorite directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tar xfvz kafka*.tgz`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Go to the installation directory and start Zookeeper:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You should see a lot of log output. The last line should contain `INFO binding
    to port 0.0.0.0/0.0.0.0:2181`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new console, and start the Kafka server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You should see some logs present in the last line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Kafka has started, open a new console and run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This command starts a console consumer that listens to the `transactions` topic.
    Any message sent to this topic will be printed on the console. This will allow
    us to test that our program works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now run `StreamingProducerApp` in IntelliJ. After a few seconds, you should
    get an output similar to this in IntelliJ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our application received some messages from the WebSocket API and sent them
    to the Kafka topic transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you then go back to the console on which you started the console consumer,
    you should see new `Transaction` serialized objects being printed in real time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that our streaming application works as expected. It listens
    to the Pusher channel to receive BTC/USD transactions and sends everything it
    receives to the Kafka topic `transactions`. Now that we have some data going to
    a Kafka topic, we can use Spark Streaming to run some analytics queries.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml) [10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml),* Fetching
    and Persisting Bitcoin Market Data*, we used Spark to save transactions in a batch
    mode. The batch mode is fine when you have to perform an analysis on a bunch of
    data all at once.
  prefs: []
  type: TYPE_NORMAL
- en: But in some cases, you might need to process data as it is entering into the
    system. For example, in a trading system, you might want to analyze all the transactions
    done by the broker to detect fraudulent transactions. You could perform this analysis
    in batch mode after the market is closed; but in this case, you can only act after
    the fact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark Streaming allows you to consume a streaming source (file, socket, and
    Kafka topic) by dividing the input data into many micro-batches. Each micro-batch
    is an RDD that can then be processed by the **Spark Engine**. Spark divides the
    input data using a time window. So if you define a time window of 10 seconds,
    then Spark Streaming will create and process a new RDD every 10 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f316f1f7-bd8f-4a80-852f-85472c21d2ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Going back to our fraud detection system, by using Spark Streaming we could
    detect a pattern of a fraudulent transaction as it arises and immediately act
    on the broker to limit the damage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, you learned that Spark offers two APIs for processing
    batch data—RDD and `Dataset`. RDD is the original and core API, and `Dataset` is
    the more recent one that allows it to perform SQL queries and optimize the execution
    plan automatically. Similarly, Spark Streaming offers two APIs to process streams
    of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discretized Stream** (**DStream**) is basically a continuous series of RDDs.
    Each RDD in a DStream contains data from a certain time interval. You can get
    more information about it here: [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured streaming** is more recent. It allows you to use the same methods
    as the Dataset API. The difference is that in `Dataset` you manipulate an unbound
    table that grows as new input data arrives. You can get more information about
    it here: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to use Zeppelin to run a Spark-structured
    streaming query on the BTC/USD transaction data that we previously produced in
    a Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing streaming transactions with Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, if you have not done so yet, you should start the following
    processes on your machine. Please refer to the previous sections if you are not
    sure how to start them:'
  prefs: []
  type: TYPE_NORMAL
- en: Zookeeper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka broker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `StreamingProducerApp` consuming live BTC/USD transactions and pushing them
    to a Kafka topic named `transactions`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading transactions from Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using your browser, create a new Zeppelin Notebook named `Streaming`, and 
    then type the following code in the first cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Execute the cell to define the `Transaction` class and the variable `schema`.
    We have to redefine the `Transaction` case class in Zeppelin because Zeppelin
    does not have access to the classes of our IntelliJ project, `bitcoin-analyser`.
    We could have instead packaged a `.jar` file and added it into Zeppelin's dependency
    settings, but as this is the only class we need, we found it easier to redefine
    it. The `schema` variable is of type `DataType`. We will use it in the following
    paragraph to deserialize the JSON transactions that we will consume from the Kafka
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then create a new cell underneath with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a new variable `dfStream` of type `DataFrame`. For creating this,
    we called:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The method `readStream` on the `spark: SparkSession` object. It returns an
    object of the `DataStreamReader` type that we can configure further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The methods `format("kafka")` and `option("kafka.bootstrap.servers", "localhost:9092")` specify
    that we want to read data from Kafka and point to the broker localhost on port `9092`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option("startingoffsets", "latest")` indicates that we only want to consume
    the data from the latest offset. Other options are `"earliest"`, or a JSON string
    specifying a starting offset for each `TopicPartition`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`option("subscribe", "transaction")` specifies that we want to listen to the
    topic `transactions`. This is the topic name that we used in our `StreamingProducerApp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The call to `load()` returns `DataFrame`. But at this stage, it only contains
    one `value` column with the raw JSON.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then deserialize the JSON using the `from_json` function and the `schema` object
    that we created in the previous paragraph. This returns a single column of type `struct`.
    In order to have all the columns at the root level, we rename the column using `alias("v")`, and
    select all columns inside it using `select("v.*")`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When you run the paragraph, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have `DataFrame` with all the columns that we need to run
    further analysis on right? Let''s try to display it. Create a new paragraph with
    this code and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The problem here is that there is a missing writing step. Even though the type `DataFrame` is
    exactly the same as the one we obtained in the `Batch` notebook, we cannot use
    it in exactly the same way. We have created a streaming `DataFrame` that can consume
    and transform messages from Kafka, but what is it supposed to do with these messages?
  prefs: []
  type: TYPE_NORMAL
- en: In the Spark-structured streaming world, the action methods such as `show`, `collect`,
    or `take` cannot be used. You have to tell Spark where to write the data it consumes.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to an in-memory sink
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Spark structured streaming process has three types of components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **input source** is specified with the `format(source: String)` method
    on `DataStreamReader`. This source can be a file, a Kafka topic, a network socket,
    or a constant rate. Once configured with `option`, the call to `load()` returns
    a `DataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations** are the classic `DataFrame`/`Dataset` transformations, such
    as `map`, `filter`, `flatMap`, and `reduce`. They take `Dataset` as input and
    return another transformed `Dataset` with the recorded transformation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **output sink** writes the transformed data. For specifying the sink, we
    must first obtain `DataStreamWriter` by calling the `writeStream` method on `Dataset`, and
    then configure it. For this, we have to call the `format(source: String)` method
    on `DataStreamWriter`. The output sink can be a file, a Kafka topic, or `foreach` that
    takes a callback. For debugging purposes, there is also a console sink and a memory
    sink.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Going back to our streaming transactions, the error that we obtained after
    calling `z.show(dfStream)` indicated that we were missing a sink for our `DataFrame`.
    To remediate this, add a new paragraph with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This code configures a memory sink for our `DataFrame`. This sink creates an
    in-memory Spark table whose name is given by `queryName("transactionsStream")`.
    A table named `transactionsStream` will be updated every time a new micro-batch
    of transactions is processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several strategies for writing to a streaming sink, specified by `outputMode(outputMode:
    String)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"append"` means that only the new rows will be written to the sink.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"complete"` will write all the rows every time there is an update.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"update"` will write all the rows that were updated. This is useful when you
    perform some aggregation; for instance, counting the number of messages. You would
    want the new count to be updated every time there is a new row coming in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once our `DataStreamWriter` is configured, we call the `start()` method. This
    will start the whole workflow in the background. It is only from this point that
    the data starts to be consumed from the Kafka topic and gets written to the in-memory
    table. All the previous operations were lazy and were just configuring the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the paragraph now, and you will see an output which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This `StreamingQuery` object is a handle to the streaming query running in the
    background. You can use it to monitor the progress of the streaming workflow,
    get some information about its execution plan, or `stop` it altogether. Feel free
    to explore the API of `StreamingQuery` and try to call its methods.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing a scatter chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we have a running `StreamingQuery` that writes to an in-memory table
    called `transactionsStream`, we can display the data contained in this table with
    the following paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this paragraph, and if you have some transactions coming into your Kafka
    topic, you should see a table which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8da5a58-b286-4807-baad-ebf627065be1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can then click on the scatter chart button. If you want to draw the evolution
    of the price, you can drag and drop the `timestamp` in the *x* axis and the `price` in
    the *y* axis. You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10b668f8-f57c-4304-9815-eaf7bb93a653.png)'
  prefs: []
  type: TYPE_IMG
- en: If you want to refresh the chart with the latest transactions, you can just
    re-rerun the paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty good, but if you let the query run for some time, you might end
    up with quite a lot of transactions. Ultimately, you will reach the limit of what
    Zeppelin can handle and you will get the same error, `OUTPUT IS TRUNCATED`, that
    we had in the section *Drawing our first chart*.
  prefs: []
  type: TYPE_NORMAL
- en: To remediate this, we have to group the transactions using a time window, in
    the same way that we did for the batch transactions that we read from `parquet`.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating streaming transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add a new paragraph with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This code is very similar to the one we wrote in the section *Drawing our first
    chart*. The only difference is the call to `withWatermark`, but the rest of the
    code is the same. This is one of the main benefits of using Spark-structured streaming—we
    can reuse the same code for transforming batch datasets and streaming datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Watermarking is mandatory when we aggregate streaming datasets. In a few words,
    we need to tell Spark how long it will wait to receive late data before discarding
    it, and what timestamp column it should use to measure the time between two rows.
  prefs: []
  type: TYPE_NORMAL
- en: As Spark is distributed, it can potentially consume the Kafka topic using several
    consumers, each of them consuming a different partition of the topic. This means
    that Spark Streaming could potentially process transactions out of order. In our
    case, we have just one Kafka broker and do not expect a high volume of transactions;
    hence, we use a low watermark of one second. Watermarking is a quite complex subject.
    You can find out more on the Spark website: [https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time](https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have run this new paragraph, you will have a new `Dataframe`, `aggDfStream`,
    which will aggregate transactions in 10-second windows. But before we can draw
    a chart of this aggregated data, we need to create a query to connect an in-memory
    sink. Create a new paragraph with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: It is nearly the same as the one we wrote in the section *Drawing a scatter
    chart*. We just used `aggDfStream` instead of `df`, and the output table name
    is now called `aggregateStream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, add a new paragraph to display the data contained in the `aggregateStream` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to wait at least 30 seconds after having run `aggQuery` to get
    some aggregated transaction data. Wait a bit, then run the paragraph. After that,
    click on the line chart button, then drag and drop the `start` column in the keys section
    and `avgPrice` in the values section. You should see a chart that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99fb5115-e696-40e8-b68f-b12278399905.png)'
  prefs: []
  type: TYPE_IMG
- en: If you re-run the paragraph after 10 seconds or more, you should see it being
    updated with new transactions.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that this `aggregateStream` `DataFrame` has exactly the same columns
    as the `aggregate` `DataFrame` that we created in the section *Drawing our first
    chart*. The difference between them is that `aggregate` is built using the historical
    batch data coming from Parquet files, and `aggregateStream` is built using the
    live data coming from Kafka. They are actually complementary—`aggregate` has got
    all the transactions from the last hours or days, while `aggregateStream` has
    got all the transactions from the point in time in which we started the `aggQuery`.
    If you want to draw a chart containing all the data up to the latest live transaction,
    you can simply use `union` of both dataframes, adequately filtered so that time
    windows are not duplicated.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned how to use Zeppelin to query parquet files
    and display some charts. Then, you developed a small program to stream transaction
    data from a WebSocket to a Kafka topic. Finally, you used Spark Streaming inside
    Zeppelin to query the data arriving in the Kafka topic in real time.
  prefs: []
  type: TYPE_NORMAL
- en: With all these building blocks in place, you have all the tools to analyze the
    Bitcoin transaction data in more detail. You could let the `BatchProducerApp` run
    for several days or weeks to get some historical data. With the help of Zeppelin
    and Spark, you could then try to detect patterns and come up with a trading strategy.
    Finally, you could then use a Spark Streaming flow to detect in real time when
    some trading signal arises and perform a transaction automatically.
  prefs: []
  type: TYPE_NORMAL
- en: We have produced streaming data on only one topic, but it would be quite straightforward
    to add other topics covering other currency pairs, such as BTC/EUR or BTC/ETH.
    You could also create another program that fetches data from another cryptocurrency
    exchange. This would enable you to create Spark Streaming queries that detect
    arbitrage opportunities (when the price of a product is cheaper in one market
    than in another).
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks we implemented in this chapter can be also used in a Lambda
    architecture. A Lambda architecture is a data processing architecture designed
    to handle large volumes of data by using both batch processing and streaming methods.
    Many Lambda architectures involve having different code bases for the batch and
    streaming layers, but with Spark, this negative point can be greatly reduced.
    You can find out more about Lambda architectures on this website: [http://lambda-architecture.net/](http://lambda-architecture.net/).
  prefs: []
  type: TYPE_NORMAL
- en: This completes the final chapter in our book. We hope you enjoyed reading it
    as much as we enjoyed writing it. You will be empowered by the ins and outs of
    various concepts of Scala. Scala is a language that is well designed and is not
    an end by himself, this is the basement to build more interesting concepts, like
    the type theory and for sure the category theory, I encourage your curiosity to
    look for more concepts to continually improve the readability and the quality
    of your code. You will also be able to apply it to solve a variety of real-world
    problems.
  prefs: []
  type: TYPE_NORMAL
