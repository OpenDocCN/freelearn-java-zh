- en: Extending the Game - Run Parallel, Run Faster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will extend the Mastermind game. As it is now, it can guess
    the secret that was hidden and also hide the pegs. The test code can even do both
    at the same time. It can play against itself leaving us only with the fun of programming.
    What it cannot do is make use of all the processors that we have in today's notebooks
    and servers. The code runs synchronous and utilizes only a single processor core.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: We will alter the code extending the guessing algorithm to slice up the guessing
    into subtasks and execute the code in parallel. During this, we will get acquainted
    with Java concurrent programming. This will be a huge topic with many subtle corners
    and caveats lurking in the dark. We will get into those details that are the most
    important and will form a firm base for further studies whenever you need concurrent
    programs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'As the outcome of the game is the same as it was, only faster, we have to assess
    what faster is. To do that, we will utilize a new feature introduced in Java 9:
    microbenchmarking harness.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of processes, threads and fibers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithreading in Java
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues with multithread programming and how to avoid them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locking, synchronization, and blocking queues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microbenchmarking
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make Mastermind parallel
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The old algorithm was to go through all the variations and try to find a guess
    that matches the current state of the table. Assuming that the currently examined
    guess is the secret, will we get the same answers for the guesses that are already
    on the table as the answers are actually on the table? If yes, then the current
    guess can be the secret, and it is just as good a guess as any other guesses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A more complex approach can implement the min-max algorithm ([https://en.wikipedia.org/wiki/Minimax](https://en.wikipedia.org/wiki/Minimax)).
    This algorithm does not simply get the next possible guess but also looks at all
    the possible guesses and selects the one that shortens the outcome of the game
    the most. If there is a guess that can be followed by three more guesses in the
    worst case, and there is another for which this number is only two, then min-max
    will choose the latter. It is a good exercise for the interested readers. In the
    case of the six colors and four columns for the pegs, the min-max algorithm solves
    the game in no more than 5 steps. The simple algorithm we implemented also solves
    the game in 5 steps. However, we do not go in that direction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we want to have a version of the game that utilizes more than one processor.
    How can you transform the algorithm into a parallel one? There is no simple answer
    to this. When you have an algorithm, you can analyze the calculations and parts
    of the algorithm, and you can try to find dependencies. If there is some calculation
    *B* that needs the data, which is the result of another calculation *A*, then
    it is obvious that *A* can only be performed when *B* is ready. If there are parts
    of the algorithm that do not depend on the outcome of the others, then they can
    be executed in parallel.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的不是只用一个处理器的游戏版本。你该如何将算法转换成并行算法呢？这个问题没有简单的答案。当你有一个算法时，你可以分析算法的计算部分，并尝试找出依赖关系。如果有一些计算
    *B* 需要另一计算 *A* 的数据，而 *A* 是 *B* 的结果，那么很明显，*A* 只能在 *B* 准备好的时候执行。如果算法中有不依赖于其他部分结果的计算部分，那么它们可以并行执行。
- en: 'For example, the quick-sort has two major tasks: partitioning and then sorting
    of the two parts. It is fairly obvious that the partitioning has to finish before
    we start sorting the two partitioned parts. However, the sorting tasks of the
    two parts do not depend on each other, they can be done independently. You can
    give them to two different processors. One will be happy sorting the part containing
    the smaller elements; the other one will carry the heavier, larger ones.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，快速排序有两个主要任务：分区和排序两个部分。很明显，分区必须在开始排序两个已分区的部分之前完成。然而，两个部分的排序任务并不相互依赖，它们可以独立完成。你可以将它们分配给两个不同的处理器。一个会高兴地排序包含较小元素的分区；另一个则会处理较重、较大的元素。
- en: If you turn the pages back to [Chapter 3](part0076.html), *Optimizing the Sort
    - Making Code Professional* where we implemented quick-sort in a non-recursive
    way, you can see that we scheduled sorting tasks into a stack and then performed
    the sorting by fetching the elements from the stack in a `while` loop. Instead
    of executing the sort right there in the core of the loop, we could pass the task
    to an asynchronous thread to perform it and go back for the next waiting task.
    We just do not know how. Yet. That is why we are here in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你翻到[第3章](part0076.html)，*优化排序 - 使代码专业化*，我们在这里以非递归的方式实现了快速排序，你可以看到我们将排序任务安排到栈中，然后通过在
    `while` 循环中从栈中获取元素来执行排序。我们本可以在循环的核心处直接执行排序，但我们可以将任务传递给异步线程来执行，然后返回去处理下一个等待的任务。我们只是不知道如何做。目前还不知道。这就是我们为什么在这里的原因。
- en: Processors, threads, and processes are complex and abstract things and they
    are hard to imagine. Different programmers have different techniques to imagine
    parallel processing and algorithms. I can tell you how I do it but it is not a
    guarantee that this will work for you. Others may have different techniques in
    their mind. As a matter of fact, I just realized that as I write this, I have
    actually never told this to anyone before. It may seem childish, but anyway, here
    it goes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器、线程和进程是复杂且抽象的概念，它们很难想象。不同的程序员有不同的技巧来想象并行处理和算法。我可以告诉你我是如何做的，但这并不能保证这对你也有效。其他人可能在心中有不同的技巧。实际上，当我写这段话的时候，我实际上从未告诉过任何人。这听起来可能有些幼稚，但无论如何，就这样吧。
- en: When I imagine algorithms, I imagine people. One processor is one person. This
    helps me overcome the freaking fact that a processor can make billions of calculations
    in a second. I actually imagine a bureaucrat wearing a brown suit and doing the
    calculations. When I create a code for a parallel algorithm, I imagine many of
    them working behind their desks. They work alone and they do not talk. It is important
    that they do not talk to each other. They are very formal. When there is a need
    for information exchange, they stand up with a piece of paper they have written
    something on, and they bring it to each other. Sometimes, they need a piece of
    paper for their work. Then they stand up, go to the place where the paper is,
    take it, bring it back to their desk, and go on working. When they are ready,
    they go back and bring the paper back. If the paper is not there when they need
    it, they queue up and wait until someone who has the paper brings it there.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: How does it help with Mastermind?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: I imagine a boss who is responsible for the guesses. There is a table on the
    wall in the office with the previous guesses and the results for each row. The
    boss is too lazy to come up with new guesses so he gives this task to subordinates.
    When a subordinate comes up with a guess, the boss checks whether the guess is
    valid or not. He does not trust the subordinates, and if the guess is good, he
    makes it as an official guess, putting it on the table along with the result.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The subordinates deliver the guesses written on small post-it notes, and they
    put them in a box on the table of the boss. The boss looks at the box from time
    to time, and if there is a note, the boss takes it. If the box is full and a subordinate
    wants to put a paper there, the subordinate stops and waits until the boss takes
    at least one note so that there is some room in the box for a new note. If the
    subordinates queue up to deposit guesses in the box, they all wait for their time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The subordinates should be coordinated; otherwise, they will just come up with
    the same guesses. Each of them should have an interval of guesses. For example,
    the first one should check the guesses from 1234 up until 2134, the second should
    check from 2134 up until 3124, and so on, if we denote the colors with numbers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Will this structure work? Common sense says that it will. However, bureaucrats,
    in this case, are metaphors and metaphors are not exact. Bureaucrats are human,
    even when they do not seem like it much more than threads or processors. They
    sometimes behave extremely strangely, doing things that normal humans don't really
    do often. However, we can still use this metaphor if it helps us imagine how parallel
    algorithms work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: We can imagine that the boss goes on holiday and does not touch the heap of
    paper piling up on the table. We can imagine that some of the workers are producing
    results much faster than the others. As this is only imagination, the speedup
    can be 1000 times (think of a time-lapse video). Imagining these situations may
    help us discover special behavior that rarely happens, but may cause problems.
    As the threads work in parallel, many times subtle differences may influence the
    general behavior greatly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: In some early version, as I coded the parallel Mastermind algorithm, the bureaucrats
    started working and filled the box of the boss with guesses before the boss could
    put any of them on the table. As there were no guesses on the table, the bureaucrats
    simply found all possible variations in their interval being a possibly good guess.
    The boss gained nothing by the help of the parallel helpers; they had to select
    the correct ones from all possible guesses, while the guessers were just idle.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Another time, the bureaucrats were checking guesses against the table while
    the boss was putting a guess on one of them created beforehand. And some of the
    bureaucrats freaked out saying that it is not possible to check a guess against
    a table if someone is changing it. More precisely, the code executing in one thread,
    threw `ConcurrentModificationException` when the `List` of the table was modified.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Another time, I tried to avoid the too fast work of bureaucrats, and I limited
    the size of the box where they could put their papers containing the guesses.
    When the boss finally found the secret, and the game finished, the boss told the
    bureaucrats that they could go home. The boss did that by creating a small paper
    with the instruction: you can go home, and put it on the tables of the bureaucrats.
    What did the bureaucrats do? Kept waiting for the box to have space for the paper!
    (Until the process was killed. This is kind of equivalent on Mac OS and on Linux
    as ending the process from the task manager on Windows.)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Such coding errors happen and, to avoid as many as possible, we have to do at
    least two things. Firstly, we have to understand how Java multithreading works
    and secondly, have a code as clean as possible. For the second, we will clean
    up the code even more and then we will look at how the parallel algorithm described
    earlier can be implemented in Java, running on the JVM instead of utilizing bureaucrats.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we finished the previous chapter, we had the classes of the Mastermind
    game designed and coded in a nice and perfectly object oriented way that did not
    break any of the *OO* principles. Did we? Absurd. There is no code, except some
    trivial examples, that cannot be made to look nicer or better. Usually, when we
    develop code and finish the coding, it looks great. It works, the tests all run,
    and documentation is ready. From the professional point of view, it really is
    perfect. Well, it is good enough. The big question that we have not tested yet
    is maintainability. What is the cost to alter the code?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: That is not an easy question, especially because it is not a definite one. Alter
    to what? What is the modification that is to be made to the code? We do not know
    that when we create the code in the first place. If the modification is to fix
    a bug, then it is obvious that we did not know that beforehand. If we knew, we
    would not have introduced the bug in the first place. If this is a new feature,
    then there is a possibility that the function was foreseen. However, usually it
    is not the case. When a developer tries to predict the future, and what features
    the program will need in the future, they usually fail. It is the task of the
    customer to know the business. Features needed are driven by the business in case
    of professional software development. After all, that is what it means to be professional.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we do not exactly know what needs to be altered later in the code,
    there are certain things that may give hints to experienced software developers.
    Usually, the OO code is easier to maintain than the ad-hoc code, and there are
    code smells that one can spot. For example, take a look at the following code
    lines:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We may sense the odor of something strange. (Each of these lines is in the code
    of the application as we finished it in [Chapter 4](part0111.html), *Mastermind
    - Creating a Game*.) The return value of the `guess` method is compared to `Row.none`,
    which is a `Row`. Then, we compare the return value of `nextGuess` to `Guesser.none`,
    which should be a `Guesser`. When we add a new guess to something, we actually
    add a `Row`. Finally, we can realize that `nextGuess` returns a guess that is
    not an object with its own declared class. A `guess` is just an array of colors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Should we introduce another layer of abstraction creating a `Guess` class? Will
    it make the code more maintainable? Or will it only make the code more complex?
    It is usually true that the less code lines we have, the less possibility we have
    for bugs. However, sometimes, the lack of abstraction will make the code complex
    and tangled. What is the case in this situation? How can we decide that generally?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The more experience you have, the easier you will tell by looking at the code
    and acutely knowing what modifications you want to make. Many times, you will
    not bother making the code more abstract, and many other times, you will create
    new classes without hesitation. When in doubt, do create the new classes and see
    what comes out. The important thing is not to ruin the already existing functionality.
    You can do that only if you have sufficient unit tests.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你的经验越多，你通过查看代码和敏锐地知道你想要进行的修改就越容易。很多时候，你不会费心使代码更加抽象，而在许多其他时候，你会毫不犹豫地创建新的类。当有疑问时，创建新的类并看看结果。重要的是不要破坏已经存在的功能。你只能在你有足够的单元测试的情况下做到这一点。
- en: When you want to introduce some new functionality or fix a bug, but the code
    is not appropriate, you will have to modify it first. When you modify the code
    so that the functionality does not change, the process is named **refactoring**.
    You change a small part of the code in a limited time, and then you build it.
    If it compiles and all unit tests run, then you can go on. The hint is to run
    the build frequently. It is like building a new road near an existing one. Once
    in every few miles, you should meet the old line. Failing to do so, you will end
    up somewhere in the middle of the desert in a totally wrong direction, and all
    you can do is return to the starting point—your old to-be-refactored code. Effort
    wasted.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想引入一些新的功能或修复一个错误，但代码不合适时，你将不得不先修改它。当你修改代码而不改变功能的过程时，这个过程被称为**重构**。你在一个有限的时间内修改代码的一小部分，然后构建它。如果它能编译并且所有单元测试都运行，那么你可以继续。提示是经常运行构建。这就像在现有道路附近修建一条新道路。每隔几英里，你应该遇到旧线路。如果没有这样做，你最终会在沙漠的中间某个地方，完全错误的方向，而你唯一能做的就是回到起点——你旧的重构代码。这是徒劳的。
- en: It is not only the safety that advises us to run the build frequently, it is
    also time limitation. Refactoring does not directly deliver revenue. The functionality
    of the program is tied directly to income. Nobody will pay us for infinite refactoring
    work. Refactoring has to stop some time and it is usually not the time when there
    is nothing to be refactored any more. The code will never be perfect, but you
    may stop when it is good enough. And, many times, programmers are never satisfied
    with the quality of the code, and when they are stopped by some external factor
    (usually called project manager), the code should compile and tests should run
    so that the new feature and bug fixing can be performed on the actual code base.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅安全性建议我们经常运行构建，还有时间限制。重构不会直接带来收入。程序的功能直接与收入挂钩。没有人会为我们无限期的重构工作付费。重构必须在某时停止，这通常不是没有更多重构要做的时候。代码永远不会完美，但你可以在它足够好的时候停止。而且，很多时候，程序员永远不会对代码的质量感到满意，当被某些外部因素（通常称为项目经理）阻止时，代码应该能编译，测试应该能运行，以便在实际代码库上执行新功能和错误修复。
- en: Refactoring is a huge topic and there are many techniques that can be followed
    during such an activity. It is so complex that there is a whole book about it
    by Martin Fowler ([http://martinfowler.com/books/refactoring.html](http://martinfowler.com/books/refactoring.html)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重构是一个非常大的主题，在进行这样的活动时可以遵循许多技术。它如此复杂，以至于有一整本书是关于它的，由马丁·福勒所著([http://martinfowler.com/books/refactoring.html](http://martinfowler.com/books/refactoring.html))。
- en: 'In our case, the modification we want to apply to our code is to implement
    a parallel algorithm. The first thing we will modify is the `ColorManager`. When
    we wanted to print guesses and rows on the terminal, we had to implement some
    bad tricks. Why not have color implementations that can be printed? We can have
    a class that extends the original `Color` class and has a method that returns
    something that represents that color. Do you have any candidate name for that
    method? It is the `toString` method. It is implemented in the `Object` class and
    any class can freely override it. When you concatenate an object to a string,
    automatic type conversion will call this method to convert the object to `String`.
    By the way, it is an old trick to use `""+object` instead of `object.toString()`
    to avoid `null` pointer exception. Needless to say, we do not use tricks. The
    `toString` method is also invoked by the IDEs when the debugger wants to display
    the value of some object, so it is generally recommended to implement `toString`
    if for nothing else, then to ease development. If we have a `Color` class that
    implements `toString`, then the `PrettyPrintRow` class becomes fairly straightforward
    and tricks less:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We removed the problem from the printing class, but you may argue that the issue
    is still there, and you are right. Many times, when there is a problem in a class
    design, the way to the solution to move the problem from the class to another.
    If it is still a problem there, then you may split the design more and more and,
    at the last stage, you will realize that what you have is an issue and not a problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a `LetteredColor` class is also straightforward:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Again, the problem was pushed forward. But, in reality, this is not a problem.
    It is an OO design. Printing is not responsible for assigning `String` to colors
    for their representation. And the color implementation itself is also not responsible
    for that. The assignment has to be performed where the color is made, and then
    the `String` has to be passed to the constructor of the `LetteredColor` class.
    The `color` instances are created in `ColorManager` so we have to implement this
    in the `ColorManager` class. Or not? What does `ColorManager` do? It creates the
    colors and...
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: When you come to an explanation or description of a class that lists the functionalities,
    you may immediately see that the **single responsibility principle** was ignored.
    `ColorManager` should manage the colors. Managing is providing a way to get the
    colors in a definite order and getting the first and the next when we know one
    color. We should implement the other responsibility—the creation of a color in
    a separate class.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'A class that has the sole functionality to create an instance of another class
    is called factory. That is almost the same as using the `new` operator but unlike
    `new`, the factories can be used more flexibly. We will see that immediately.
    The `ColorFactory` interface contains a single method, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Interfaces that define only one method are named functional interfaces because
    their implementation can be provided as a lambda expression at the place where
    you would use an object that is an instance of a class which implements the functional
    interface. The `SimpleColorFactory` implementation creates the following `Color`
    objects:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It is very much like how we create an interface, and then an implementation,
    instead of just writing `new Color()` in the code in `ColorManager`. `LetteredColorFactory`
    is a bit more interesting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, here we have the functionality that assigns `String`s to the `Color` objects
    when they are created. It is very important that the `counter` variable that keeps
    track of the already created colors is not `static`. The similar variable in the
    previous chapter was `static` and it meant that it could run out of characters
    as ever-newer `ColorManager`s created too many colors. It actually did happen
    to me during the unit test execution when the tests each created `ColorManager`s
    and new `Color` instances, and the printing code tried to assign new letters to
    the new colors. The tests were running in the same JVM under the same classloader
    and the unfortunate `static` variable had no clue when it could just start counting
    from zero for the new tests. The drawback is that somebody, somewhere, has to
    instantiate the factory and it is not the `ColorManager`. `ColorManager` already
    has a responsibility and it is not to create a color factory. The `ColorManager`
    has to get the `ColorFactory` in its constructor:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You may also notice that I could not resist refactoring the `createColors` method
    into two methods to follow the single responsibility principle.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the code that creates `ColorManager` has to create a factory and pass
    it to the constructor. For example, the unit test''s `ColorManagerTest` class
    will contain the following method:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the simplest way ever to implement a factory defined by a functional
    interface. Just name the class and reference the `new` operator like it was a
    method by creating a method reference.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The next thing we will refactor is the `Guess` class, which, actually, we did
    not have so far. A `Guess` class contains the pegs of the guess and can calculate
    the number of full (color as well as position) and partial (color present but
    in wrong position) matches, and can also calculate the next `Guess` that comes
    after this guess. This functionality was implemented in the `Guesser` class so
    far, but this is not really the functionality for how we select the guesses when
    checking the already made guesses on the table. If we follow the pattern we set
    up for the colors, we may implement this functionality in a separate class named
    `GuessManager`, but as for now, it is not needed. Again, this is not black and
    white.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that a `Guess` object can only be made at once. If
    it is on the table, the player is not allowed to change it. If we have a `Guess`
    that is not yet on the table, it is still just a `Guess` identified by the colors
    and orders of the pegs. A `Guess` object never changes after it was created. Such
    objects are easy to use in multithread programs and are called immutable objects:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The constructor is creating a copy of the array of colors that were passed.
    As a `Guess` is immutable, this is extremely important. If we just keep the original
    array, any code outside of the `Guess` class could alter the elements of the array,
    essentially changing the content of `Guess` that is not supposed to be changing:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this method, we start to calculate the next `Guess` starting with the color
    array that is contained in the actual object. We need a work array that is modified,
    so we will copy the original. The final new object can, this time, use the array
    we use during the calculation, so that will need a separate constructor that does
    not create a copy. It is possible extra code, but we should consider making that
    only if we see that that is the bottleneck in the code and we are not satisfied
    with the actual performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method just checks if the passed `Guess` has the same number of colors
    as the actual one. This is just a safety check used by the next two methods that
    calculate the matches:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `isUnique` method checks if there is any color more than once in the `Guess`.
    As the `Guess` is immutable, it may not happen that a `Guess` is unique one time
    and not unique at another time. This method should return the same result whenever
    it is called on a specific object. Because of that, it is possible to cache the
    result. This method does this, saving the return value to an instance variable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'You may say that this is premature optimization. Yes, it is. I decided to do
    it for one reason. It is demonstration, and based on that, you can try to modify
    the `nextGuess` method to do the same:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Methods that return the same result for the same arguments are called idempotent.
    Caching the return value for such a method can be very important if the method
    is called many times and the calculation is using a lot of resources. When the
    method has arguments, the result caching is not simple. The object method has
    to remember the result for all arguments that were already calculated, and this
    storage has to be effective. If it takes more resources to find the stored result
    than the calculation of it, then the use of cache not only uses more memory but
    also slows down the program. If the method is called for several arguments during
    the lifetime of the object, then the storage memory may just grow too large. Some
    of the elements have to be purged—those that will not be needed anymore in the
    future. However, we cannot know which elements of the cache are not needed, so
    we will have to guess.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, caching can get complex very fast and, to do that professionally,
    it is almost always better to use some readily available cache implementation.
    The caching we use here is only the tip of the iceberg. Or, it is even only the
    sunshine glimpsing on it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the class is fairly standard and something we have talked about
    in detail—a good check of your knowledge is to understand how the `equals`, `hashCode`,
    and `toString` methods are implemented this way. I implemented the `toString`
    method to help me during debugging, but it is also used in the following example
    output:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is mainly the modification that I needed while I developed the parallel
    algorithm. Now, the code is fairly up-to-date and described to focus on the main
    topic of this chapter: how to execute code in Java in parallel.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The parallel execution of the code in Java is done in threads. You may know
    that there is a `Thread` object in Java runtime, but without understanding what
    a thread in the computer is, it makes no sense. In the following subsections,
    we will learn what these threads are, how to start a new thread, how to synchronize
    data exchange between threads, and finally put all this together and implement
    the Mastermind parallel guessing algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Processes
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start your computer, the program that starts is the **operating system**
    (**OS**). The OS controls the machine hardware and the programs that you can run
    on the machine. When you start a program, the OS creates a new process. It means
    that the OS allocates a new entry in a table (array) where it administers the
    processes and fills in the parameters that it knows, and needs to know, about
    the process. For example, it registers what memory segment the process is allowed
    to use, what the ID of the process is, and which user started from which other
    process. You cannot start a process just out of thin air. When you double-click
    on an EXE file, you actually tell the file explorer, which is a program running
    as a process, to start the EXE file as a separate process. The explorer calls
    the system via some API and kindly asks the OS to do that. The OS will register
    the explorer process as the parent of the new process. The OS does not actually
    start the process, but creates all the data that it needs to start it and, when
    there is some free CPU resource, then the process gets started, and then it gets
    paused very soon. You will not notice it because the OS will start it again and
    again and is always pausing the process repeatedly. It needs to do it to provide
    run possibilities to all processes. That way, we experience all processes running
    at the same time. In reality, processes do not run at the same time on a single
    processor, but they get time slots to run often.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: If you have more than one CPU in the machine, then processes can actually run
    at the same time, as many CPUs as there are. As the integration gets more advanced
    today, desktop computers have CPUs that contain multiple cores that function almost
    like separate CPUs. On my machine, I have four cores, each capable of executing
    two threads simultaneously; so, my Mac is almost like an 8 CPU machine.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Processes have separate memories. They are allowed to use one part of the memory
    and if a process tries to use another part that does not belong to it, the processor
    will stop doing so. The OS will kill the process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Just imagine how frustrated the developers of the original UNIX could have been
    that they named the program to stop a process to kill, and stopping a process
    is called killing it. It is like medieval ages when they cut off the hand of a
    felon. You touch the wrong part of the memory and get killed. I would not like
    to be a process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The memory handling by the operating system is very complex in addition to separating
    the processes from each other. When there is not enough memory, the OS writes
    part of the memory to disk freeing up the memory and reloading that part when
    it is needed again. This is a very complex, low-level implemented and highly optimized
    algorithm that is the responsibility of the OS.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I said that the OS executes the processes in time slots, I was not absolutely
    precise. Every process has one or more threads, and threads are executed. A thread
    is the smallest execution managed by an external scheduler. Older operating systems
    did not have the notion of a thread and were executing processes. As a matter
    of fact, the first thread implementations were simply duplications of processes
    that were sharing the memory.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: You may hear the terminology, lightweight process—it means a thread.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The important thing is that the threads do not have their own memory. They use
    the memory of the process. In other words, the threads that run in the same process
    have undistinguished access to the same memory segment. It is an extremely powerful
    possibility to implement parallel algorithms that make use of the multiple cores
    in the machine, but at the same time, it may lead to bugs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Imagine that two threads increment the same long variable. The increment first
    calculates the incremented value of the lower 32 bits and then the upper, if there
    were any overflow bits. These are two or more steps that may be interrupted by
    the OS. It may happen that one thread increments the lower 32 bits, remembers
    that there is something to do to the upper 32 bits, starts the calculation, but
    has no time to store the result before it gets interrupted. Then, another thread
    increments the lower 32 bits, the upper 32 bits, and then the first thread just
    saves the upper 32 bits that it calculated. The result gets garbled. On an older
    32-bit Java implementation, it was extremely easy to demonstrate this effect.
    On a 64-bit Java implementation, all the 64 bits are loaded into registers and
    saved back to the memory in one step so it is not that easy to demonstrate multithread
    issues, but it does not mean that there are none.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: When a thread is paused and another thread is started, the operating system
    has to perform a context switch. It means that, among other things, the CPU registers
    have to be saved and then set to the value that they should have for the other
    thread. A context switch is always saving the state of the thread and loading
    the previously saved state of the thread to be started. This is on a CPU register
    level. This context switch is time consuming; therefore, the more context switches
    that are done, the more CPU resource is used for the thread administration instead
    of letting them run. On the other hand, if there are not enough switches, some
    threads may not get enough time slots to execute, and the program hangs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Fibers
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java does not have fibers, but as there are some libraries that support fiber
    handlings, it is worth mentioning. A fiber is a finer unit than a thread. A program
    code executing in a thread may decide to give up the execution and tell the fiber
    manager to just execute some other fiber. What is the point and why is it better
    than using another thread? The reason is that this way, fibers can avoid part
    of the context switch.  A context switch cannot be avoided totally because a different
    part of the code that starts to execute it may use the CPU registers in a totally
    different way. As it is the same thread, the context switching is not the task
    of the OS, but the application.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The OS does not know if the value of a register is used or not. There are bits
    in the registers, and no one can tell seeing only the processor state whether
    those bits are relevant for the current code execution or just happen to be there
    in that way. The program generated by a compiler does know which registers are
    important and which are those that can just be ignored. This information changes
    from place to place in the code, but when there is a need for a switch, the fiber
    passes the information of what is needed to be switched at that point to the code
    that does the switching.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The compiler calculates this information, but Java does not support fibers in
    the current version. The tools that implement fibers in Java analyze and modify
    the byte code of the classes to do this after the compilation phase.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Golang's goroutines are fibers and that is why you can easily start many thousand
    goroutines in Go, but you better limit the number of threads in Java to a lower
    number. They are not the same things.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: As the terminology lightweight process is fading out and used by less and less
    fibers, many times are referred to as lightweight threads.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: java.lang.Thread
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As everything in Java (well, almost) is object, if we want to start a new thread,
    we will need a class that represents the thread. This class is `java.lang.Thread`
    built into the JDK. When you start a Java code, the JVM automatically creates
    a few `Thread` objects and uses them to run different tasks that are needed by
    it. If you start up **VisualVM**, you can select the Threads tab of any JVM process
    and see the actual threads that are in the JVM. For example, the VisualVM as I
    started it has 29 live threads. One of them is the thread named `main`. This is
    the one that starts to execute the `main` method (surprise!). The `main` thread
    started most of the other threads. When we want to write a multithread application,
    we will have to create new `Thread` objects and start them. The simplest way to
    do that is `new Thread()`, and then calling the `start` method on the thread.
    It will start a new Thread that will just finish immediately as we did not give
    it anything to do. The `Thread` class, as it is in the JDK, does not do our business
    logic. The following are the two ways to specify the business logic:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Creating a class that implements the `Runnable` interface
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a class that extends the `Thread` class and overrides the `run` method
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following block of code is a very simple demonstration program:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code creates two threads and starts them one after the other.
    When the start method is called, it schedules the thread object to be executed
    and then returns. As a result, the new thread will soon start executing asynchronously
    while the calling thread continues its execution. The two threads, and the `main`
    thread, run parallel in the following example and create an output that looks
    something like this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The actual output changes from run to run. There is no definite order of the
    execution or how the threads get access to the single screen output. There is
    not even guarantee that in each and every execution, the message `started` is
    printed before any of the thread messages.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of this, we will have to look at the state diagram
    of threads. A Java Thread can be in one of the following states:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '`NEW`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUNNABLE`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BLOCKED`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WAITING`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TIMED_WAITING`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TERMINATED`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These states are defined in the `enumThread.State`. When you create a new thread
    object, it is in the `NEW` state. At this moment, the thread is nothing special,
    it is just an object but the operating system execution-scheduling does not know
    about it. In some sense, it is only a piece of memory allocated by the JVM.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: When the start method is invoked, the information about the thread is passed
    to the operating system and the OS schedules the thread so it can be executed
    by it when there is an appropriate time slot. Doing this is a resourceful action
    and that is the reason why we do not create and, especially, do not start new
    Thread objects only when it is needed. Instead of creating new Threads, we will
    keep the existing threads for a while, even if they are not needed at the moment,
    and reuse an existing one if there is one suitable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread in the OS can also be in a running state as well as runnable when
    the OS schedules and executes it at the moment. Java JDK API does not distinguish
    between the two for good reason. It would be useless. When a thread is in the
    `RUNNABLE` state asking if it is actually running from the thread itself, it will
    result in an obvious answer: if the code just returned from the `getState` method
    implemented in the `Thread` class, then it runs. If it were not running, it would
    not have returned from the call in the first place. If the `getState` method was
    called from another thread, then the result about the other thread by the time
    the method returns would be meaningless. The OS may have stopped, or started,
    the queried thread several times until then.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: A thread is in a `BLOCKED` state when the code executing in the thread tries
    to access some resource that is not currently available. To avoid constant polling
    of resources, the operating system provides effective notification mechanism so
    the threads get back to the `RUNNABLE` state when the resource they need becomes
    available.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: A thread is in the `WAIT` or `TIMED_WAITING` state when it waits for some other
    thread or lock. `TIMED_WAITING` is the state when the waiting started calling
    a version of a method that has timeout.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `TERMINATED` state is reached when the thread finishes its execution.
    If you append the following lines to the end of our previous example, then you
    will get a `TERMINATED` printout and also an exception thrown up to the screen
    complaining about illegal thread state, which is because you cannot start an already
    terminated thread:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Instead of extending the `Thread` class to define what to execute asynchronously,
    we can create a class that implements `Runnable`. Doing that is more in line with
    the OO programming approach. The something that we implement in the class is not
    a functionality of a thread. It is more of a something that can be executed. It
    is something that can just run.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: If this execution is asynchronous in a different thread, or it is executed in
    the same thread that was calling the run method, is a different concern that has
    to be separated. If we do it that way, we can pass the class to a `Thread` object
    as a constructor argument. Calling `start` on the `Thread` object will start the
    run method of the object we passed. This is not the gain. The gain is that we
    can also pass the `Runnable` object to an `Executor` (dreadful name, huhh!). `Executor`
    is an interface, and implementations execute `Runnable` (and also `Callable`,
    see later) objects in `Thread`s in an efficient way. Executors usually have a
    pool of `Thread` objects that are prepared, and in the `BLOCKED` state. When the
    `Executor` has a new task to execute, it gives it to one of the `Thread` objects
    and releases the lock that is blocking the thread. The `Thread` gets into the
    `RUNNABLE` state, executes the `Runnable`, and gets blocked again. It does not
    terminate and thus can be reused to execute another `Runnable` later. That way,
    `Executor`s avoid the resource consuming process of thread registration into the
    operating system.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Professional application code never creates a new `Thread`. Application code
    uses some framework to handle the parallel execution of the code or uses `Executor`s
    provided by some `ExecutorService` to start `Runnable` or `Callable` objects.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Pitfalls
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed many of the problems that we may face when developing
    parallel program. In this section, we will summarize them with the usual terminology
    used for the problems. Terminology is not only interesting, but it is also important
    when you talk with colleagues to easily understand each other.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Deadlocks
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deadlock is the most infamous parallel programming pitfall, and for this reason,
    we will start with this one. To describe the situation, we will follow the metaphor
    of bureaucrats.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The bureaucrat has to stamp a paper he has in his hand. To do that, he needs
    the stamp, and he also needs the inkpad. First, he goes to the drawer where the
    stamp is and takes it. Then, he walks to the drawer where the inkpad is and takes
    the inkpad. He inks the stamp, pushes on the paper. Then, he puts the stamp back
    to its place and then the inkpad back in its place. Everything is nice, we are
    on cloud 9.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: What happens if another bureaucrat takes the inkpad first and then the stamp
    second? They may soon end up as one bureaucrat with the stamp in hand waiting
    for the inkpad and another one with the inkpad in hand waiting for the stamps.
    And, they may just stay there, frozen forever, and then more and more start to
    wait for these locks, the papers never get stamped, and the whole system sinks
    into anarchy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: To avoid such situations, the locks have to be ordered and the locks should
    always be acquired in the order. In the preceding example, the simple agreement
    that the inkpad is acquired first and the stamp second solves the problem. Whoever
    acquired the stamp can be sure that the inkpad is free or will soon be free.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We talk about race conditions when the result of a calculation may be different
    based on the speed and CPU access of the different parallel running threads. Let''s
    take a look at the following two code lines:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If the value of `b` at the start of the execution is 0, and two different threads
    execute the two methods, then the order of the lines can be 1234, 1324, 1342,
    3412, 3142, or 3142\. Any execution order of the four lines may happen which assures
    that 1 runs before 2 and 3 runs before 4, but no other restrictions. The outcome,
    the value of `b`, is either 1 or 2 at the end of the execution of the segments,
    which may not be good and what we wanted when coding.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Note that the implementation of the parallel Mastermind game also has something
    like this. The actual guesses very much depend on the speed of the different threads,
    but this is irrelevant from the final result point of view. We may have different
    guesses in different runs and that way the algorithm is not deterministic, but
    we are guaranteed to find the final solution.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Overused locks
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many situations, it may happen that the threads are waiting on a lock, which
    protects a resource from concurrent access. If the resource cannot be used by
    multiple threads simultaneously, and there are more threads than can be served,
    then the threads are starving. However, in many cases, the resource can be organized
    in a way so that the threads can get access to some of the services that the resource
    provides, and the locking structure can be less restrictive. In that case, the
    lock is overused and the situation can be mended without allocating more resource
    for the threads. It may be possible to use several locks that control the access
    to the different functionality of the resource.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Starving
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starving is the situation when several threads are waiting for a resource trying
    to acquire a lock and some threads get access to the lock only after extremely
    long time or never. When the lock is released and there are threads waiting for
    it, then one of the threads can get the lock. There is usually no guarantee that
    a thread gets the lock if it waits long enough. Such a mechanism would require
    intensive administration of the threads, sorting them in the waiting queue. As
    locking should be a low latency and high performance action, even a few CPU clock
    cycles are significant; therefore, the locks do not provide this type of fair
    access by default. Not wasting time with fairness in thread scheduling is a good
    approach, in case the locks have one thread waiting. The main goal of locks is
    not scheduling the waiting threads, but rather preventing parallel access to resources.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: It is like in a shop. If there is somebody at the cashier, you wait. It is a
    lock built in implicitly. It is not a problem if people do not queue up for the
    cashier, so long as long there is almost always one free. However, when there
    are several queues built up in front of the cashiers, then having no queue and
    waiting order will certainly lead to some very long waiting order for someone
    who is slow to get access to the cashier. Generally, the solution of fairness
    and creating queue of waiting threads (customers) is not a good solution. The
    good solution is to eliminate the situation that leads to waiting queues. You
    can employ more cashiers, or you can do something totally different that makes
    the peak load smaller. In a shop, you can give discount to drive customers who
    come in at off-peak hours. In programming, several techniques can be applied,
    usually, depending on the actual business we code and fair scheduling of locks
    is usually a workaround.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: ExecutorService
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ExecutorService` is an interface in the JDK. An implementation of the interface
    can execute a `Runnable` or `Callable` class in an asynchronous way. The interface
    only defines the API for the implementation and does not require that the invocation
    is asynchronous but, in reality, that is the main point implementing such a service.
    Invoking the `run` method of a `Runnable` interface in a synchronous way is simply
    calling a method. We do not need a special class for that.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: The `Runnable` interface defines one `run` method. It has no arguments returns
    no value and does not throw any exception. The `Callable` interface is parameterized
    and the only method it defines, `call`, has no argument but returns a generic
    value and may also throw `Exception`. In our code, we will implement `Runnable`
    if we just want to run something, and `Callable` when we want to return something.
    Both of these interfaces are functional interfaces, therefore, they are good candidates
    to be implemented using lambda.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'To have an instance of an implementation of an `ExecutorService`, we can use
    the utility class `Executors`. Many times when there is an `XYZ` interface in
    the JDK, there can be an `XYZs` (plural) utility class that provides factory for
    the implementations of the interface. If we want to start the `t1` task many times,
    we can do so without creating a new `Thread`. We should use the following executor
    service:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This time, we do not get any exception. Instead, the `t1` task runs second time.
    In this example, we are using a fixed size thread pool that has two `Thread`s.
    As we want to start only two threads simultaneously, it is enough. There are implementations
    that grow and shrink the size of the pool dynamically. Fixed size pool should
    be used when we want to limit the number of the threads or we know from some other
    information source the number of the a-priory threads. In this case, it is a good
    experiment to change the size of the pool to one and see that the second task
    will not start in this case until the first one finishes. The service will not
    have another thread for `t2` and will have to wait until the one and only `Thread`
    in the pool is freed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: When we submit the task to the service, it returns even if the task cannot currently
    be executed. The tasks are put in a queue and will start execution as soon as
    there is enough resource to start them. The submit method returns a `Future` object,
    as we can see in the preceding sample.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: It is like a service ticket. You bring your car to the repair mechanic, and
    you get a ticket. You are not required to stay there until the car is fixed, but
    at any time, you can ask if the car is ready. All you need is the ticket. You
    can also decide to wait until the car is ready. A `Future` object is also something
    like that. You do not get the value that you need. It will be calculated asynchronously.
    However, there is a `Future` promise that it will be there and your ticket to
    access the object you need is the `Future` object.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: When you have a `Future` object, you can call the `isDone` method to see if
    it is ready. You can start waiting for it to call `get` with, or without, some
    timeout. You can also cancel the task executing it, but in that case, the outcome
    may be questionable. Just like, in case of your car, if you decide to cancel the
    task, you may get back your car with the motor disassembled. Similarly, cancelling
    a task that is not prepared for it may lead to resource loss, opened and inaccessible
    database connection (this is a painful memory for me, even after 10 years), or
    just a garbled unusable object. Prepare your tasks to be cancelled or do not cancel
    them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, there is no return value for `Future` because we submitted
    a `Runnable` object and not a `Callable` one. In that case the value passed to
    the `Future` is not to be used. It is usually `null`, but that is nothing to lean
    on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The final and most important thing that many developers miss, even me, after
    not writing multithread Java API using code for years, is shutting down the `ExecutorService`.
    The `ExecutorService` is created and it has `Thread` elements. The JVM stops when
    all non-daemon threads are stopped. It ain't over till the fat lady sings.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: A thread is a daemon thread if it was set to be daemon (invoking `setDaemon(true)`)
    before it was started. A thread is automatically daemon of the starting thread
    is a daemon thread. Daemon threads are stopped by the JVM when all other threads
    are finished and the JVM wants to finish. Some of the threads the JVM executes
    itself are daemon threads, but it is likely that there is no practical use of
    creating daemon threads in an application program.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Not shutting down the service simply prevents the JVM from stopping. The code
    will hang after the `main` method finishes. To tell the `ExecutorService` that
    there is no need for the threads it has, we will have to `shutdown` the service.
    The call will only start the shutdown and return immediately. In this case, we
    do not want to wait. The JVM does anyway. If we need to wait, we will have to
    call `awaitTermination`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: ForkJoinPool
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ForkJoinPool` is a special `ExecutorService` that has methods to execute
    `ForkJoinTask` objects. These classes are very handy when the task that we want
    to perform can be split into many small tasks and then the results, when they
    are available, aggregated. Using this executor, we need not care about the size
    of the thread pool and shutting down the executor. The size of the thread pool
    is adjusted to the number of processors on the given machine to have optimal performance.
    As the `ForkJoinPool` is a special `ExecutorService` that is designed for short
    running tasks, it does not expect any task to be there longer or being needed
    when there are no more tasks to run. Therefore, it is executed as a daemon thread;
    when the JVM shuts down, the `ForkJoinPool` automatically stops and the lady does
    not sing any more.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: To create a task, the programmer should extend either `RecursiveTask` or `RecursiveAction`.
    The first one is to be used when there is some return value from the task, the
    second when there is no computed value returned. They are called recursive because
    many times, these tasks split the problem they have to solve smaller problems
    and invoke these tasks asynchronously through the fork-join API.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: A typical problem to be solved using this API is the quick-sort. In the [Chapter
    3](part0076.html), *Optimizing the Sort - Making Code Professional* we created
    two versions of the quick-sort algorithm. One using recursive calls and one without
    using it. We can also create a new one, which, instead of calling itself recursively,
    schedule the task to be executed, perhaps by another processor. The scheduling
    is the task of the `ForkJoinPool` implementation of `ExecutorService`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'You may revisit the code of `Qsort.java` in [Chapter 3](part0076.html), *Optimizing
    the Sort - Making Code Professional*. Here is the version that is using `ForkJoinPool`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Whenever you can split your tasks into subtasks similar to the way it was done
    in the preceding quick-sort example, I recommend that you use `ForkJoinPool` as
    an `ExecutorService`. You can find good documentation on the API and the use on
    the JavaDoc documentation of Oracle.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Variable access
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we can start threads and create code that runs parallel, it is time
    to talk a little bit about how these threads can exchange data between each other.
    At first glimpse, it seems fairly simple. The threads use the same shared memory;
    therefore, they all can read and write all the variables that the Java access
    protection allows them. This is true, except that some threads may just decide
    not to read the memory. After all, if they have just recently read the value of
    some variable, why read it again from the memory to the registers if it was not
    modified? Who would have modified them? Let''s see the following short example:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: What will happen? You may expect that the code starts up, starts the new thread,
    and one minute, when the `main` thread sets the object to something not `null`,
    will it stop? It will not.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: It may stop on some Java implementations, but in most of them, it will just
    keep spinning. The reason for that is that the JIT compiler optimizes the code.
    It sees that the loop does nothing and also that the variable will just never
    be non-null. It is allowed to assume that because the variables not declared `volatile`
    are not supposed to be modified by any other thread, the JIT is eligible to optimize.
    If we declare the `Object o` variable to be `volatile` (with the `volatile` keyword),
    then the code will stop.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In case you try to remove the call to sleep, the code will also stop. This,
    however, does not fix the issue. The reason is that JIT optimization kicks in
    only after about 5000 loops of the code execution. Before that, the code runs
    naive and stops before the optimization will eliminate the extra and regularly
    not needed access to the non-volatile variable.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: If this is so gruesome, then why don't we declare all variables to be volatile?
    Why does Java not do that for us? The answer is speed, and to understand it deeper,
    we will use our metaphor, the office, and the bureaucrat.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The CPU heartbeat
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days CPUs run on 2 to 4 GHz frequency processors. It means that a processor
    gets 2 to 4 times 10⁹ clock signals to do something every second. A processor
    cannot do any atomic operation faster than this, and also there is no reason to
    create a clock that is faster than what a processor can follow. It means that
    a CPU performs a simple operation, such as incrementing a register in half or
    quarter of a nanosecond. This is the heartbeat of the processor, and if we think
    of the bureaucrat as humans, who they are, then it is equivalent to one second,
    approximately, if and as their heartbeat.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Processors have registers and caches on the chip on different levels, L1, L2,
    and sometimes L3; there is memory, SSD, disk, network, and tapes that may be needed
    to retrieve data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data that is in the L1 cache is approximately 0.5ns. You can grab
    a paper that is on your desk—half of a second. L2 cache is 7ns. This is a paper
    in the drawer. You have to push the chair a bit back, bend it in a sitting position,
    pull out the drawer, take the paper, push the drawer back, and raise and put the
    paper on the desk; it takes 10 seconds, give or take.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Main memory read is 100ns. The bureaucrat stands up, goes to the shared file
    at the wall, he waits while other bureaucrats are pulling their papers or putting
    theirs back, selects the drawer, pulls it out, takes the paper, and walks back
    to the desk. This is two minutes. This is volatile variable access every time
    you write a single word on a document and it has to be done twice. Once to read,
    and once to write, even if you happen to know that the next thing you will do
    is just fill another field of the form on the same paper.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Modern architectures, where there are no multiple CPUs but rather single CPUs
    with multiple cores, are a bit faster. One core may check the other core's caches
    to see if there was any modification on the same variable, but this speeds the
    volatile access to 20ns or so, which is still a magnitude slower than nonvolatile.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Although the rest is less focused on multithread programming, it is worth mentioning
    here, because it gives good understanding on the different time magnitudes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Reading a block from an SSD (4K block usually) is 150,000ns. In human speed,
    that is a little bit more than 5 days. Reading or sending something to a server
    over the network on the Gb local Ethernet is 0.5ms, which is like waiting for
    almost a month for the metaphoric bureaucrat. If the data over the network is
    on a spinning magnetic disk, then seek time adds up (the time until the disk rotates
    so that the part of the magnetic surface gets under the reading head) to 20ms.
    It is, approximately, a year in human terms.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: If we send a network packet over the Atlantic on the Internet, it is approximately
    is 150ms. It is like 14 years, and this was only one single package; if we want
    to send data over the ocean, it may be seconds that count up to historic times,
    thousands of years. If we count one minute for a machine to boot, it is equivalent
    to the time span of our whole civilization.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We should consider these numbers when we want to understand what the CPU is
    doing most of the time: it waits. Additionally, it also helps cool your nerves
    when you think about the speed of a real-life bureaucrat. They are not that slow
    after all, if we consider their heartbeat, which implies the assumption that they
    have a heart. However, let''s go back to real life, CPUs, and L1, L2 caches and
    volatile variables.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Volatile variables
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s modify the declaration of the `o` variable in our sample code as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code runs fine and stops after a second or so. Any Java implementation
    has to guarantee that multiple threads can access `volatile` fields and the value
    of the field is consistently updated. This does not mean that volatile declaration
    will solve all synchronization issues, but guarantees that the different variables
    and their value change relations are consistent. For example, let''s consider
    we have the following two fields incremented in a method:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, reading `i` and `j` from another thread will never result
    an `i>j`. Without the volatile declaration, the compiler is free to reorganize
    the execution of the increment operations if it needs and thus, it will not guarantee
    that an asynchronous thread reads consistent values.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Synchronized block
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Declaring variables are not the only tool to ensure the consistency between
    threads. There are other tools in the Java language and one of them is the synchronized
    block. The `synchronized` keyword is part of the language and it can be used in
    front of a method or a program block inside a method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Every object in the Java program has a monitor that can be locked and unlocked
    by any running thread. When a thread locks a monitor, it is said that that thread
    holds the lock, and no two threads can hold the lock of a monitor at a time. If
    a thread tries to lock a monitor that is already locked, it gets `BLOCKED` until
    the monitor is released. A synchronized block starts with the `synchronized` keyword,
    and then an object instance specified between parentheses and the block comes.
    The following small program demonstrates the `synchronized` block:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The code starts two different threads. One of the threads appends `aa` to the
    `StringBuffer`. The other one appends `bb`. This appending is done in two separate
    steps with a sleep in between. The sleep is needed to avoid JIT that optimizes
    the two separate steps into one. Each thread executes the `append` 1000 times
    each time appending `a` or `b` two times. As the two `append`s one after the other
    are inside a `synchronized` block it cannot happen that an `aba` or `bab` sequence
    gets into the `StringBuffer`. While one thread executes the synchronized block,
    the other thread cannot execute it.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: If I remove the synchronized block, then the JVM I used to test Java HotSpot
    (TM) 64-Bit Server VM (build 9-ea+121, mixed mode) prints out the failure with
    a try-count around a few hundreds.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: It clearly demonstrates what the synchronization means, but it draws our attention
    to another important phenomena. The error occurs only around every few hundred
    thousand executions only. It is extremely rare, even though this example was furnished
    to demonstrate such a mishap. If a bug appears so rare, it is extremely hard to
    reproduce and, even more, to debug and fix. Most of the synchronization errors
    manifest in mysterious ways and their fixing usually is the result of meticulous
    code review rather than debugging. Therefore, it is extremely important to clearly
    understand the true nature of Java multithread behavior before starting commercial
    multithread application.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The `synchronized` keyword can also be used in front of a method. In this case,
    the object to acquire the lock of is the object. In case of a `static` method,
    the synchronization is done on the whole class.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Wait and notify
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are five methods implemented in the class `Object` that can be used to
    get further synchronization functionality: `wait` with three different timeout
    argument signature, `notify`, and `notifyAll`. To call `wait`, the calling thread
    should have the lock of the `Object` on which `wait` is invoked. It means that
    you can only invoke `wait` from inside a synchronized block, and when it is called,
    the thread gets `BLOCKED` and releases the lock. When another thread calls `notify`
    all on the same `Object`, the thread gets into the `RUNNABLE` state. It cannot
    continue execution immediately as it cannot get the lock on the object. The lock
    is held at that moment by the thread that just called `notifyAll`. However, sometime
    after the other thread releases, the lock gets out of the `synchronized` block,
    and the waiting thread continues the execution.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: If there are more threads waiting on an object, all of them get out of the `BLOCKED`
    state. The `notify` method wakes only one of the waiting threads. There is no
    guarantee which thread is awakened.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The typical use of `wait`, `notify`, and `notifyAll` is when one or more threads
    are creating `Object`s that are consumed by other thread, or threads. The storage
    where the objects travel between the threads is some kind of queue. The consumer
    waits until there is something to read from the queue, and the producer puts the
    objects into the queue one after the other. The producer notifies the consumers
    when it stores something into the queue. If there is no room left in the queue,
    the producer has to stop and wait until the queue has some space. In this case,
    the producer calls the `wait` method. To wake the producer up, the consumer calls
    `notifyAll` when it reads something.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The consumer consumes the objects from the queue in a loop and calls `wait`
    only if there is nothing to be read from the queue. When the producer calls `notifyAll`,
    and there is no consumer waiting, the notification is just ignored. It flies away,
    but this is not a problem; consumers are not waiting. When the consumer consumes
    an object and calls `notifyAll` and there is no producer waiting, the situation
    is the same. It is not a problem.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: It cannot happen that the consumer consumes, calls `notifyAll`, and after the
    notification was flying in the air not finding any waiting producer, a producer
    starts to wait. This cannot happen because the whole code is in a `synchronized`
    block and it ensures that no producer is in the critical section. This is the
    reason why `wait`, `notify`, and `notifyAll` can only be invoked when the lock
    of the `Object` class is acquired.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: If there are many consumers, which are executing the same code and are equivalently
    good in consuming the objects, then it is an optimization to call `notify` instead
    of `notifyAll`. In that case, `notifyAll` will just awake all consumer threads
    and all, but the lucky one will recognize that they were woken up but somebody
    else already got away with the bait.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'I recommend that you practice at least once to implement a blocking queue that
    can be used to pass `Object`s between threads. However, never use that code in
    production: starting with Java 1.5, there are implementations of the `BlockingQueue`
    interface. Use one that fits your needs. We will too, in our example code.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Feel lucky that you can code in Java 9\. I started using Java professionally
    when it was 1.4 and once I had to implement a blocking queue. Life gets just better
    and easier all the time with Java.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: In professional code, we usually avoid using `synchronized` methods or blocks
    and `volatile` fields as well as the `wait` and `notify` methods, `notifyAll`
    too, if possible. We can use asynchronous communication between threads, or pass
    the whole multithreading to the framework for handling. `Synchronized` and `volatile`
    cannot be avoided in some special cases when the performance of the code is important,
    or we cannot find a better construct. Sometimes, the direct synchronization on
    specific code and data structures is more efficient than the approach delivered
    by JDK classes. It is to note, however, that those classes also use these low-level
    synchronization constructs, so it is not magic how they work; and to develop yourself,
    you can look into the code of the JDK classes before you want to implement your
    own version. You will realize that it is not that simple to implement these queues;
    the code of the classes is not complex and compound without reason. If you find
    the code simple, it means that you are senior enough to know what not to reimplement.
    Or, perhaps, you do not even realize what code you read.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Lock
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Lock`s are built in Java; every `Object` has a lock that a thread may acquire
    when it enters a `synchronized` block. We discussed that already. In some programming
    code, there are situations when this kind of structure is not optimal.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In some situations, the structure of locks may be lined up to avoid deadlock.
    It may be needed to acquire lock *A* before *B* and to acquire *B* before C*.*
    However, *A* should be released as soon as possible, not to prevent access to
    resource protected by lock *D*, but also needing lock *A* before it. In complex
    and highly parallel structures, the locks are structured many times into trees
    where accessing a resource a thread should climb down along the tree to a leaf
    representing the resource. In this climbing, the thread gets hold of a lock on
    a node, then a lock on a node below it, and then releases the lock above, just
    like a real climber descending (or climbing up if you imagine the tree with the
    leafs at the top, which is more realistic, nevertheless graphs usually show trees
    upside down).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'You cannot leave a `synchronized` block remaining in another that is inside
    the first one. Synchronized blocks are nested. The `java.util.concurrent.Lock`
    interface defines methods to handle that situation and the implementations are
    also there in the JDK to be used in our code. When you have a lock, you can call
    the methods lock and unlock. The actual order is in your hand and you can write
    the following line of code to get the locking sequence:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The freedom, however, comes with responsibility as well. The locks and unlocks
    are not tied to the execution sequence of the code, like in case of synchronized
    block, and it may be very easy to create code that in some case just loses a lock
    not unlocking it rendering some resource unusable. The situation is similar to
    a memory leak: you will allocate (lock) something and forget to release (unlock)
    it. After a while, the program will run out of resource.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: My personal recommendation is to avoid using locks if possible and use higher-level
    constructs and asynchronous communications between threads, such as blocking queues.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Condition
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `java.util.concurrent.Condition` interface in functionality is similar to
    the built-in `wait, notify`, and `notifyAll`. Any implementation of `Lock` should
    create new `Condition` objects and return as a result to the invocation of the
    `newCondition` method. When the thread has a `Condition`, it can call `await`,
    `signal`, and `signalAll` when the thread has the lock that created the condition
    object.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The functionality is very similar to the methods of `Object` mentioned. However,
    the big difference is that you can create many `Condition`  for a single `Lock`
    and they will work independent of each other, but not independent of the `Lock`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: ReentrantLock
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ReentrantLock` is the simplest implementation of the interface lock in the
    JDK. There are two ways to create this type of lock: with and without fairness
    policy. If the `ReentrantLock(Boolean fair)` constructor is called with the true
    argument, then the lock will be assigned to the thread that is waiting for the
    lock the longest time in case there are many threads waiting. This will avoid
    a thread made to wait for infinite time and starving.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: ReentrantReadWriteLock
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This class is an implementation of `ReadWriteLock`. `ReadWriteLock` is a lock
    that can be used for parallel read access and exclusive write access. It means
    that several threads can read the resource protected by the lock, but when a thread
    writes the resource, no other thread can get access to it, not even read during
    that period. A `ReadWriteLock` is simply two `Lock` objects returned by the `readLock`
    and `writeLock` methods. To get read access on `ReadWriteLock`, the code has to
    invoke `myLock.readLock().lock()`, and to get access to write lock, `myLock.writeLock().lock()`.
    Acquiring one of the locks and releasing it in the implementation is coupled with
    the other lock. To acquire a write lock, no thread should have an active read
    lock, for example.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: There are several intricacies in the use of the different lock. For example,
    you can acquire a read lock, but you cannot get a write lock so long as you have
    the read lock. You have to release the read lock first to acquire a write lock.
    This is just one of the simple details, but this is the one that novice programmers
    have trouble with many times. Why is it implemented this way? Why should the program
    get a write lock, which is more expensive—in sense of higher probability locking
    other threads—when it still is not sure that it wants to write the resource? The
    code wants to read it and. based on the content. it may later decide that it wants
    to write it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The issue is not with the implementation. The developers of the library decided
    this rule, not because they just liked it that way or because they were aware
    of parallel algorithms and deadlock possibilities. When two threads have read
    lock and each decides to upgrade the lock to write lock, then they would intrinsically
    create a deadlock. Each would hold the read lock waiting for the write and none
    of them would get it ever.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: On the other end, you can downgrade a write lock to a read lock without risking
    that in the meantime somebody acquires a write lock and modifies the resource.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Atomic classes
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Atomic classes enclose primitive values into objects and provide atomic operations
    on them. We discussed race conditions and volatile variables. For example, if
    we have an `int` variable to be used as a counter and we want to assign a unique
    value to objects that we work with, we can increment the value and use the result
    as a unique ID. However, when multiple threads use the same code, we cannot be
    sure about the value we read after the increment. It may happen that another thread
    also incremented the value in the meantime. To avoid that, we will have to enclose
    the increment and the assignment of the incremented value to an object into a
    `synchronized` block. This can also be done using `AtomicInteger`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: If we have a variable of `AtomicInteger`, then calling `incrementAndGet` increments
    the value of `int` enclosed in the class and returns the incremented value. Why
    do it instead of using synchronized block? The first answer is that if the functionality
    is there in the JDK, then using it is less line than implementing it again. Developers
    maintaining the code you create are expected to know the JDK libraries but have
    to study your code, and this takes time and time is money.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The other reason is that these classes are highly optimized and, many times,
    they implement the features using platform specific native code that greatly over
    performs the version we can implement using synchronized blocks. Worrying about
    performance too early is not good, but parallel algorithms and synchronization
    between threads are usually used when performance is crucial; thus, there is a
    good chance that the performance of the code using the atomic classes is important.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: In the `java.util.concurrent.atomic` package, there are several classes, `AtomicInteger`,
    `AtomicBoolean`, `AtomicLong`, and `AtomicReference` among them. They all provide
    methods that are specific to the encapsulated value.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'The method, which is implemented by every atomic class, is `compareAndSet`.
    This is a conditional value-setting operation that has the following format:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When it is applied on an atomic class, it compares the actual value with the
    one `expectedValue`, and if they are the same, then it sets the value to `updateValue`.
    If the value was updated, the method returns `true` and it does all this in an
    atomic action.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: You may ask the question that if this method is in all of these classes, why
    is there no `Interface` defining this method? The reason for this is that the
    argument types are different based on the encapsulated type, and these types are
    primitives. As primitives cannot be used as generic types, not even a generic
    interface can be defined. In case of `AtomicXXXArray`, the method has an extra
    first argument, which is the index of the array element handled in the call.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The variables encapsulated are handled the same way as volatile, as far as the
    reordering is concerned, but there are special methods that loosen the conditions
    a bit to be used when possible, and performance is key.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The general advice is to consider using atomic classes, if there is one usable,
    and you will find yourself creating a synchronized block for check-and-set, atomic
    increment, or addition operations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: BlockingQueue
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`BlockingQueue` is an interface that extends the standard `Queue` interface
    with methods that are suitable to be used by multithread applications. Any implementation
    of this interface provides methods that allow different threads to put element
    into the queue, pull elements off the queue, and wait for elements that are in
    the queue.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: When there is a new element that is to be stored in the queue, you can `add`
    it, `offer` it, or `put` it. These are the name of the methods that store elements
    and they do the same thing, but a bit differently. The `add` element throws an
    exception if the queue is full and there is no room for the element. The `offer`
    element does not throw exception but returns either `true` or `false`, based on
    the success. If it can store the element in the queue, it returns `true`. There
    is also a version of `offer` that specifies a timeout. That version of the method
    waits, and returns only `false` if it cannot store the value into the queue during
    the period. The `put` element is the dumbest version; it waits until it can do
    its job.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: When talking about available room in a queue, do not get puzzled and mix it
    with general Java memory management. If there is no more memory, and the garbage
    collector can also not release any, you will certainly get `OutOfMemoryError`.
    Exception is thrown by `add`, and `false` is returned by `offer`, when the queue
    limits are reached. Some of the `BlockingQueue` implementations can limit the
    number of elements that can be stored at a time in a queue. If that limit is reached,
    then the queue is full and cannot accept more elements.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Fetching elements from a `BlockingQueue` implementation also has four different
    ways. In this direction, the special case is when the queue is empty. In that
    case, `remove` throws an exception instead of returning the element, `poll` returns
    `null` if there is no element, and `take` just waits until it can return an element.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there are two methods inherited from the interface `Queues` that do
    not consume the element from the queue only *look at*. The `element` return the
    head of the queue and throws an exception if the queue is empty, and `peek` returns
    `null` if there is no element in the queue. The following table summarizes the
    operations borrowed from the documentation of the interface:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Throws exception** | **Special value** | **Blocks** | **Times out**
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| **Insert** | `add(e)` | `offer(e)` | `put(e)` | `offer(e, time, unit)` |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| **Remove** | `remove()` | `poll()` | `take()` | `poll(time, unit)` |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| **Examine** | `element()` | `peek()` | `not applicable` | `not applicable`
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: LinkedBlockingQueue
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is an implementation of the `BlockingQueue` interface, which is backed
    up by a linked list. The size of the queue is not limited by default (to be precise,
    it is `Integer.MAX_VALUE`) but it can optionally be limited in a constructor argument.
    The reason to limit the size in this implementation is to aid the use when the
    parallel algorithm performs better with limited size queue, but the implementation
    does not have any restriction on the size.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: LinkedBlockingDeque
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the simplest implementation of the `BlockingQueue` and also its subinterface
    `BlockingDeque`. As we discussed in the previous chapter, a `Deque` is a double-ended
    queue that has `add`, `remove`, `offer`, and so on, type of methods in the form
    of `xxxFirst` and `xxxLast` to do the act with one or the other end of the queue.
    The `Deque` interface defines `getFirst` and `getLast` instead of consistently
    naming `elementFirst` and `elementLast`, so this is something you should get used
    to. After all, the IDEs help with automatic code completion so this should not
    be a really big problem.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: ArrayBlockingQueue
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ArrayBlockingQueue` implements the `BlockingQueue` interface, hence the Queue
    interface. This implementation manages a queue with fixed size elements. The storage
    in the implementation is an array and the elements are handled in a *FIFO* manner:
    first-in first-out. This is the class that we will also use in the parallel implementation
    of Mastermind for the communication between the boss and the subordinated bureaucrats.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: LinkedTransferQueue
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `TransferQueue` interface is extending `BlockingQueue` and the only implementation
    of it in the JDK is `LinkedTransferQueue`. A `TransferQueue` comes handy when
    a thread wants to hand over some data to another thread and needs to be sure that
    some other thread takes the element. This `TransferQueue` has a method transfer
    that puts an element on the queue but does not return until some other thread
    `remove`s (or `poll`s) it. That way the producing thread can be sure that the
    object put on the queue is in the hands of another processing thread and does
    not wait in the queue. The method `transfer` also has a format `tryTransfer` in
    which you can specify some timeout value. If the method times out the element
    is not put into the queue.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: IntervalGuesser
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed the different Java language elements and JDK classes that are all
    available to implement parallel algorithms. Now, we will see how to use these
    approaches to implement the parallel guesser for the Masterrmind game.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The class that performs the creation of the guesses is named `IntervalGuesser`.
    It creates the guesses between a start and an end guess and sends them to a `BlockingQueue`.
    The class implements `Runnable` so it can run in a separate `Thread`. The purist
    implementation will separate the `Runnable` functionality from the interval guessing,
    but as the whole class is hardly more than 50 lines, it is forgivable sin implementing
    the two functionalities in a single class.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The implementation is very simple as most of the functionality is already implemented
    in the abstract `Guesser` class. The more interesting code is the one that invoked
    the `IntervalGuesser`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: ParallelGamePlayer
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ParallelGamePlayer` class implements the `Player` interface that defines
    the `play` method:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This method creates a Table, a `RandomSecret` that creates the guess used as
    a secret in a random way, a `Game` object, `IntervalGuesser`s, and a `UniqueGuesser`.
    The `IntervalGuesser`s are the bureaucrats; the `UniqueGuesser` is the boss who
    crosschecks the guesses that the `IntervalGuesser`s create. The method starts
    off the asynchronous guessers and then reads the guesses in a loop from them and
    puts them on the table if they are OK until the game finishes. At the end of the
    method, in the `finally` block, the asynchronous guessers are stopped.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The start and the stop method for the asynchronous guessers use `ExecutorService`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The code is quite straightforward. The only thing that may need mention is that
    the queue of the guesses is drained into a collection that we do not use afterward.
    This is needed to help any `IntervalGuesser` that is waiting with a suggested
    guess in hand, trying to put it into the queue. When we drain the queue, the guesser
    thread returns from the method put in the `guessQueue.put(guess);` line in `IntervalGuesser`
    and can catch the interrupt. The rest of the code does not contain anything that
    would be radically different from what we have already seen and you can find it
    on GitHub.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The last question that we still want to discuss in this chapter is how much
    speed did we gain making the code parallel?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Microbenchmarking
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microbenchmarking is measuring the performance of a small code fragment. When
    we want to optimize our code, we will have to measure it. Without measurement,
    code optimization is like shooting blindfolded. You will not hit the target, but
    you likely will shoot somebody else.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Shooting is a good metaphor because you should usually not do it, but when you
    really have to then you have no choice. If there is no performance issue and the
    software meets the requirements, then any optimization, including speed measurement,
    is a waste of money. This does not mean that you are encouraged to write slow
    and sloppy code. When we measure performance, we will compare it against a requirement,
    and the requirement is usually on the user level. Something like, the response
    time of the application should be less than 2 seconds. To do such a measurement,
    we usually create load tests in a test environment and use different profiling
    tools that tell us what is consuming the most time and where we should optimize.
    Many times, it is not only Java code, but configuration optimization, using larger
    database connection pool, more memory, and similar things.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Microbenchmarking is a different story. It is about the performance of a small
    Java code fragment and, as such, closer to the Java programming.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: It is rarely used, and before starting to do a microbenchmark for real commercial
    environment, we will have to think twice. Microbenchmark is a luring tool to optimize
    something small without knowing if it is worth optimizing that code. When we have
    a huge application that has several modules run on several servers, how can we
    be sure that improving some special part of the application drastically improves
    the performance? Will it pay back in increased revenue that generates so much
    profit that will cover the cost we burned into the performance testing and development?
    Statistically, almost sure that such an optimization including microbenchmarking
    will not pay off.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Once I was maintaining the code of a senior's colleague. He created a highly
    optimized code to recognize configuration keywords that were present in a file.
    He created a program structure that represented a decision tree based on the characters
    in the key string. If there was a keyword in the configuration file that was misspelled,
    the code threw an exception at the very first character where it could decide
    that the keyword could not be correct.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: To insert a new keyword, it needed to get through the code structure to find
    the occasion in the code where the new keyword was first different from already
    existing ones and extend the deeply nested if/else structures. To read the list
    of the handled keywords was possible from the comments that listed all the keywords
    that he did not forget to document. The code was working blazingly fast, probably
    saving a few milliseconds of the servlet application startup time. The application
    was started up only after system maintenance every few month.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: You feel the irony, don't you? Seniority is not always the number of years.
    Lucky ones can save their inner child.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'So when to use microbenchmarking? I can see two areas:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: You identified the code segment that eats most of the resources in your application
    and the improvement can be tested by microbenchmarks
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot identify the code segment that will eat most of the resources in
    an application but you suspect it
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first is the usual case. The second is when you develop a library, and you
    just do not know all the applications that will use it. In this case, you will
    try to optimize the part that you think is the most crucial for most of the imagined,
    suspected applications. Even in that case, it is better to take some sample applications
    that are created by users of your library and collect some statistics about the
    use.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Why should we talk about microbenchmarking in details? What are the pitfalls?
    Benchmarking is an experiment. The first programs I wrote was a TI calculator
    code and I can just count the number of steps the program made to factor two large
    (10 digits those days) prime numbers. Even at that time, I was using an old Russian
    stopwatch to measure the time, being lazy to calculate the number of steps. Experiment
    and measurement was easier.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Today, you cannot calculate the number of steps the CPU makes even if you wanted.
    There are so many small factors that may change the performance of the applications
    that are out of control of the programmer, which makes it impossible to calculate
    the steps. We have the measurement left for us, and we will gain all the problems
    of measurements.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: What is the biggest problem? We are interested in something, say *X*, and we
    usually cannot measure that. So, we will measure *Y* instead and hope that the
    values of *Y* and *X* are coupled together. We want to measure the length of the
    room, but instead we measure the time it takes for the laser beam to travel from
    one end to the other. In this case, the length *X* and the time *Y* are strongly
    coupled. Many times, *X* and *Y* only correlate more or less. Most of the times,
    when people do measurement, the *X* and *Y* values have no relation to each other
    at all. Still, people put their money and more on decisions backed by such measurements.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Microbenchmarking is no different. The first question is how to measure the
    execution time? Small code runs short times and `System.currentTimeMillis()` may
    just return the same value when the measurement starts and when it ends, because
    we are still in the same millisecond. Even if the execution is 10ms, the error
    of the measurement is still at least 10% purely because of the quantization of
    the time as we measure. Luckily, there is `System.nanoTime()`. But is there? Just
    because the name says it returns the number of nanoseconds from a specific start
    time, it does not necessarily mean it really can.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: It very much depends on the hardware and the implementation of the method in
    the JDK. It is called nano because this is the precision that we cannot certainly
    reach. If it was microseconds, then some implementation may be limited by the
    definition, even if on the specific hardware, there is a more precise clock. However,
    this is not only the precision of an available hardware clock; it is about the
    precision of the hardware.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Let's remember the heartbeat of the bureaucrats, and the time needed to read
    something from memory. Calling a method, such as `System.nanoTime(),` is like
    asking the bellboy in a hotel to run down from the second floor to the lobby and
    peek out to look at the clock on the tower on the other side of the road, come
    back, and tell seconds precision what the time was it when we asked. Nonsense.
    We should know the precision of the clock on the tower and the speed of the bellboy
    running from the floor to the lobby and back. This is a bit more than just calling
    `nanoTime`. This is what a microbenchmarking harness does for us.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The **Java Microbenchmarking Harness** (**JMH**) is available for some time
    as a library. It is developed by Oracle and used to tune the performance of some
    core JDK classes, and with Java 9, these performance measurements and results
    become part of the distributed JDK. This is good news for those who develop Java
    platform for new hardware, but also for developers, because it means that the
    JMH is and will be supported by Oracle.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '"*JMH is a Java harness to build, run, and analyze nano/micro/milli/macro benchmarks
    written in Java and other languages targeting the JVM.*" (quote from the official
    site of JMH, [http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/)).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: You can run `jmh` as a separate project independent from the actual project
    you measure, or you can just store the measurement code in a separate directory.
    The harness will compile against the production class files and will execute the
    benchmark. The easiest way, as I see, is to use the Gradle plugin to execute JMH.
    You can store the benchmark code in a directory called `jmh` (the same level as
    `main` and `test`) and create a `main` that can start the benchmark.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gradle build script is extended with the following lines:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And the microbenchmark class is as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`ParallelGamePlayer` is created to play the game with -1, 1, 4, and 8 `IntervalGuesser`
    threads, and in each case, there is a test running with a queue of length 1, 10,
    100, and 1 million. These are 16 test executions. When the number of threads is
    negative, then the constructor uses `LinkedBlockingDeque`. There is another separate
    measurement that measures the nonparallel player. The test was executed with unique
    guesses and secrets (no color used more than once) and ten colors and six columns.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: When the harness starts, it does all the calibrations automatically and runs
    the tests for many iterations to let the JVM start up. You may recall the code
    that just never stopped unless we used the `volatile` modifier in for the variable
    that was used to signal the code to stop. That happened because the JIT compiler
    optimized the code. This is done only when the code was already run a few thousand
    times. The harness makes these executions to warm up the code and ensure that
    the measurement is done when JVM is at full speed.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Running this benchmark takes approximately 15 minutes on my machine. During
    the execution, it is recommended to stop all other processes and let the benchmark
    use all available resources. If there is anything using resources during the measurement,
    then it will be reflected in the result.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The actual output of the program is a bit more verbose; it was edited for printing
    purposes. The `Score` column shows how many times the benchmark can run in a second.
    The `Error` shows that the measurement shows less than 10% scattering.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: The fastest performance we have is when the algorithm runs on eight threads,
    which is the number of threads the processor can independently handle on my machine.
    It is interesting that limiting the size of the queue did not help the performance.
    I actually expected it to be different. Using a one million length array as a
    blocking queue has a huge overhead and this is not a surprise that, in this case,
    the execution is slower than when we have only 100 elements in the queue. The
    unlimited linked list-based queue handling, on the other hand, fairly fast and
    clearly shows that the extra speed at the limited queue for 100 elements does
    not come from the fact that the limit does not allow the `IntervalThreads` to
    run too far.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: When we start one thread, then we expect similar results, as when we run the
    serial algorithm. The fact that the serial algorithm beats the parallel algorithm
    running on one thread is not a surprise. The thread creation and the communication
    between the main thread and the extra one thread have overhead. The overhead is
    significant, especially when the queue is unnecessarily large.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned a lot of things. First of all, we refactored the
    code to be ready for further development that uses parallel guessing. We got acquainted
    with processes and threads, and we even mentioned fibers. After that, we looked
    at how Java implements threads and how to create code that runs on multiple threads.
    Additionally, we saw the different means that Java provides to programmers needing
    parallel programs, starting threads, or just starting some tasks in already existing
    threads.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most important part of this chapter that you should remember is
    the metaphor of bureaucrats and the different speeds. This is extremely important
    when you want to understand the performance of concurrent applications. And I
    hope that this is a catchy picture, which is easy to remember.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: There was a huge topic about the different synchronization means that Java provides,
    and you have also learned about the pitfalls that programmers can fall into when
    programming concurrent applications.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we created the concurrent version of the Mastermind guesser
    and also measured that it is indeed faster than the version that uses only one
    processor (at least on my machine). We used the Java Microbenchmark Harness with
    the Gradle build tool and discussed, a bit, how to perform microbenchmarking.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: This was a long chapter and not an easy one. I may tend to think that this is
    the most complex and most theoretical one. If you understood half of it at first
    read, you can be proud. On the other hand, be aware that this is only a good base
    to start experimenting with concurrent programming and there is a long way to
    being senior and professional in this area. And, it is not an easy one. But first
    of all, be proud of yourself at the end of this chapter.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters we will learn more about web and web programming.
    In the very next chapter we will develop our little game so that it can run in
    a server and the player can play with it using a web browser. This will establish
    the basic knowledge for web programming. Later we will build on this developing
    web based service applications, reactive programming and all the tools and areas
    that will make a professional Java developer.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
