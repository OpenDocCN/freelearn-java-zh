<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Monitoring, Performance, and Logging</h1>
                </header>
            
            <article>
                
<p>We have now seen how to craft modern, scalable, and resilient microservices with Java EE. In particular, the part about adding resilience as well as technical cross-cutting to microservices is a topic that we want to pursue further.</p>
<p>Enterprise applications run on server environments remote from the users. In order to provide insights into the system, we need to add visibility. There are multiple ways to achieve this aspect of telemetry that includes monitoring, health checks, tracing, or logging. This chapter covers the reasoning behind each of these approaches and what makes sense for enterprise applications.</p>
<p>In this chapter we will cover the following topics:</p>
<ul>
<li>Business and technical metrics</li>
<li>Integrating Prometheus</li>
<li>How to meet performance needs</li>
<li>Java Performance Diagnostic Model</li>
<li>Monitoring and sampling techniques</li>
<li>Why traditional logging is harmful</li>
<li>Monitoring, logging, and tracing in a modern world</li>
<li>Suitability of performance tests</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Business metrics</h1>
                </header>
            
            <article>
                
<p>Visibility in the business processes is crucial to business-related persons in order to see and interpret what is happening inside an enterprise system. B<span>usiness-relevant metrics allow to evaluate the effectiveness of processes. </span>Without visibility into the processes, the enterprise application acts as a black box.</p>
<p>Business-related metrics are an invaluable asset to business experts. They provide domain-specific information as to how the use cases perform. Which metrics are of interested obviously depends on the domain.</p>
<p>How many cars are created per hour? How many articles are purchased and for what amount? What is the conversion rate? How many users followed the email marketing campaign? These are examples of domain-specific key performance indicators. The business experts have to define these indicators for the specific domain.</p>
<p>The enterprise application has to emit this information which originate from various points in the business processes. The nature of this information depends on the actual domain. In many cases, business metrics arise from domain events that occur during performing the business processes.</p>
<p>Take the number of cars that are created per hour as an example. The car creation use case emits a corresponding <kbd>CarCreated</kbd> domain event, which is collected for future statistics. Whereas calculating the conversion rate involves much more information.</p>
<p>The business experts have to define the semantics and origin behind key performance indicators. Defining and collecting these indicators becomes part of the use case. Emitting this information is a responsibility of the application as well.</p>
<p>It's important to distinguish between business-motivated and technically-motivated metrics. Although business metrics provide insights of high value, they are directly impacted by technical metrics. An example of a technical metric is the service response time which is, in turn, affected by other technical metrics. The sub-chapter <em>Technical metrics</em> will examine this topic further.</p>
<p>Business experts, therefore, must not only care about the business aspects of monitoring but also the technical impact of an application's responsiveness.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting business metrics</h1>
                </header>
            
            <article>
                
<p>Business-relevant metrics allow business experts to evaluate the effectiveness of the enterprise system. <span>The metrics provide helpful insights</span> into specific parts of the business domain. The application is responsible for gathering business-relevant metrics as part of it's use cases.</p>
<p>The <em>car manufacture</em> package, for example, performs business logic that can emit certain metrics, such as the number of cars created per hour.</p>
<p>From a business perspective, the relevant metrics usually originate from domain events. It's advisable to define and emit domain events, such as <kbd>CarCreated</kbd>, as part of the use case, as soon as a car has been successfully manufactured. These events are collected and being used to derive further information in the form of specific business metrics.</p>
<p>The <kbd>CarCreated</kbd> event is fired in the boundary as a CDI event and can be observed in a separate statistics collector. The following code snippet shows a domain event fired as part of a use case:</p>
<pre>@Stateless
public class CarManufacturer {

    @Inject
    CarFactory carFactory;

    <strong>@Inject
    Event&lt;CarCreated&gt; carCreated;</strong>

    @PersistenceContext
    EntityManager entityManager;

    public Car manufactureCar(Specification spec) {
        Car car = carFactory.createCar(spec);
        entityManager.merge(car);
        <strong>carCreated.fire(new CarCreated(spec));</strong>
        return car;
    }

}</pre>
<p>The boundary fires the CDI event that notifies about a successful car creation. The corresponding handling is decoupled from the business process and no further logic is involved in this place. The event will be observed in a separate application scoped bean. Synchronous CDI events can define to be handled during specific transaction phases. The following transactional observer therefore ensures that only successful database transactions are measured:</p>
<pre>import javax.enterprise.event.TransactionPhase;

@ApplicationScoped
public class ManufacturingStatistics {

    public void carCreated(<strong>@Observes(during =</strong><br/><strong>            TransactionPhase.AFTER_SUCCESS) Specification spec</strong>) {<br/>        // gather statistics about car creation with <br/>        // provided specification
        // e.g. increase counters
    }
}</pre>
<p>The event information is collected and processed further in order to provide the business metrics. Depending on the situation, more business-relevant data could be required.</p>
<p>Modeling the relevant information as domain events matches the business definition and decouples the use case from the statistics calculation.</p>
<p>Besides defining domain events, the information can also be collected via cross-cutting components, such as interceptors, depending on the situation and requirements. In the simplest case, the metrics are instrumented and collected in primitives. Application developers have to consider bean scopes in order not to throw away collected data with incorrect scopes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Emitting metrics</h1>
                </header>
            
            <article>
                
<p>Metrics are usually not persisted in the application but in another system that is part of the environment, such as external monitoring solutions. This simplifies the implementation of metrics; the enterprise application keeps the information in memory and emits the specified metrics. External monitoring solutions scrape and process these metrics.</p>
<p>There are several techniques that can be used to emit and collect metrics. For example, measures can be formatted into custom JSON strings and exposed via HTTP endpoints.</p>
<p>A monitoring solution that is part of the Cloud Native Computing Foundation, and, as of today, has huge momentum, is <strong>Prometheus</strong>. Prometheus is a monitoring and alerting technology that scrapes, efficiently stores, and queries time series data. It gathers data that is emitted by some service over HTTP in a specific format. Prometheus is powerful in scraping and storing data.</p>
<p>For graphs and dashboards for business-related information, other solutions can be built on top of this. A technology that works well with Prometheus and provides many possibilities for appealing graphs is <strong>Grafana</strong>. Grafana doesn't store time series itself but uses sources such as Prometheus to query and display time series.</p>
<p>The following screenshot shows an example of a Grafana dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/82f83925-226d-4da5-9af3-49ca9a817d8c.png"/></div>
<p>The idea of dashboards provides visibility for business experts and combines relevant information. Depending on the requirements and motivations, coherent information is combined into graphs that provide overviews and insights. Dashboards provide the ability to query and customize time series representations based on the target group.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enter Prometheus</h1>
                </header>
            
            <article>
                
<p>The following examples show how to integrate Prometheus into Java EE. This is one possible monitoring solution and aims to give the readers an idea of how to slimly integrate business-related metrics.</p>
<p>The application will emit the gathered metrics in the Prometheus output format. The Prometheus instances scrape and store this information, as demonstrated in the following diagram:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/0bd00845-3cc1-4de5-b5b8-584a218eb27a.png"/></div>
<p>Developers can implement custom functionality to collect and emit information, or use Prometheus' Client API which already ships with several metric types.</p>
<p>There are multiple Prometheus metric types as follows:</p>
<ul>
<li>The one mostly used is a <strong>counter</strong> which represents an increasing numeric value. It counts the occurred events.</li>
<li>A <strong>gauge</strong> is a numeric value that goes up and down. It can be used for measuring values such as conversion rates, temperatures, or turnover.</li>
<li><strong>Histograms</strong> and <strong>summaries</strong> are more complex metric types used to sample observations in buckets. They typically observe metrics distribution. For example, how long does it take to create a car, how much do these values vary, and how are they distributed?</li>
</ul>
<p>A Prometheus metric has a name and labels, which are sets of key-value pairs. A time series is identified by the metric's name and a set of labels. The label can be seen as parameters, sharding the overall amount of information.</p>
<p>An example of a counter metric representation using labels is <kbd>cars_manufactured_total{color="RED", engine="DIESEL"}</kbd>. The <kbd>cars_manufactured_total</kbd> counter includes the total number of manufactured cars that are specified by their color and engine type. The collected metrics can be queried for the provided label information later on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Realization with Java EE</h1>
                </header>
            
            <article>
                
<p>The following statistics implementation observes the domain event specified earlier and stores the information in the Prometheus counter metric:</p>
<pre>import io.prometheus.client.Counter;

@ApplicationScoped
public class ManufacturingStatistics {

    <strong>private Counter createdCars;</strong>

    @PostConstruct
    private void initMetrics() {
        <strong>createdCars = Counter.build("cars_manufactured_total",</strong><br/><strong>                "Total number of manufactured cars")
                .labelNames("color", "engine")
                .register();</strong>
    }

    public void carCreated(@Observes(during =<br/>            TransactionPhase.AFTER_SUCCESS) Specification spec) {<br/><br/>        <strong>createdCars.labels(spec.getColor().name(),</strong><br/><strong>                spec.getEngine().name()).inc();</strong>
    }

}</pre>
<p>The counter metric is created and registered to the Prometheus Client API. Measured values are qualified by the car <kbd>color</kbd> and <kbd>engine</kbd> type, which are taken into account when scraping the values.</p>
<p>In order to emit this information, the Prometheus servlet library can be included. This outputs all the registered metrics in the correct format. The monitoring servlet is configured via <kbd>web.xml</kbd>. It's also possible to include a JAX-RS resource to emit the data by accessing <kbd>CollectorRegistry.defaultRegistry</kbd>.</p>
<p>The emitted output will look similar to the following:</p>
<pre>...
cars_manufactured_total{color="RED", engine="DIESEL"} 4.0
cars_manufactured_total{color="BLACK", engine="DIESEL"} 1.0</pre>
<p>Java EE components, such as CDI events, support developers in integrating domain event metrics in a lean way. In the preceding example, the <kbd>ManufacturingStatistics</kbd> class is the only point that depends on the Prometheus API.</p>
<p>It's highly advisable to include the Prometheus Client API as a separate container image layer and not in the application artifact.</p>
<p>The monitoring solution scrapes and further processes the provided information, in order to gather the required business metrics. Scraping the counter of manufactured cars over time leads to the number of created cars per hour. This metric can be queried for the total number of cars as well as for specific color and engine combinations. The queries that define the business metrics can also be adapted and refined due to the requirements. The application ideally emits the needed atomic business-relevant metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating the environment</h1>
                </header>
            
            <article>
                
<p>The application emits the business-relevant metrics via HTTP. The Prometheus instance scrapes and store this data and makes it available via queries, graphs, and external solutions, such as Grafana.</p>
<p>In a container orchestration, the Prometheus instance runs inside the cluster. This removes the necessity to configure externally accessible monitoring endpoints. Prometheus integrates with Kubernetes in order to discover the application instances. Prometheus needs to access every application pod individually, s<span>ince every application instance emits its monitoring metrics separately. Prometheus accumulates the </span>information of all instances.</p>
<p>The Prometheus configuration is either stored in a config map or part of a base image. The instance is configured to access the applications and exporters every <em>n</em> seconds in order to s<span>crape the time series</span>. For configuring Prometheus, refer to its current documentation.</p>
<p>This is one possible solution for integrating business monitoring into a cloud native application.</p>
<p>Business-related metrics are advisably represented by domain events that emerge as part of the business use case. Integrating the chosen monitoring solutions should happen transparently from the domain logic, without much vendor lock-in.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Meeting performance requirements in distributed systems</h1>
                </header>
            
            <article>
                
<p>Responsiveness is an important non-technical requirement of an enterprise application. The system only provides business value if client requests can be served within a reasonable amount of time.</p>
<p class="mce-root"><span>Meeting performance requirements in distributed systems requires to take all participating applications into account.</span></p>
<p>Enterprise application are often required to meet a <strong>service level agreement</strong> (<strong>SLA</strong>). SLAs usually define thresholds for availability or response times, respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service level agreements</h1>
                </header>
            
            <article>
                
<p>In order to calculate and meet SLAs, it's important to consider which processes and applications are included in business use cases, especially in regard to synchronous communication. The performance of applications that synchronously call external systems directly depend on the performance of these calls. As mentioned before, distributed transactions should be avoided.</p>
<p>As per its nature, SLAs can only be met if all applications perform and work well together. Every application affects the SLAs of dependent systems. This not only concerns the slowest application in a system but all participating services.</p>
<p>For example, meeting an uptime of 99.995% per definition is not possible if it includes synchronous calls to two applications with each of them guaranteeing 99.995%. The resulting SLA is 99.99%, the values of each participating system multiplied.</p>
<p>The same is true for guaranteed response times. Every involved system slows down the overall response, resulting in a total response time that is the sum of all SLA times.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Achieving SLAs in distributed systems</h1>
                </header>
            
            <article>
                
<p>Let's see an example how to achieve SLAs in distributed systems, assuming the enterprise application resides in a high performance scenario where it's crucial to meet guaranteed response times. The application synchronously communicates with one or more backend systems that provide necessary information. The overall system needs to meet an SLA response time of 200 milliseconds.</p>
<p>In this scenario the backend applications support in meeting the SLA time by applying backpressure and preventively rejecting requests that won't meet the guaranteed SLA. By doing so the originating application has the chance to use another backend service that may respond in time.</p>
<p>In order to appropriately configure pooling, the engineers need to know the average response time of the backend system, here 20 milliseconds. The corresponding business functionality defines a dedicated thread pool by using a dedicated managed executor service. The thread pool can be configured individually.</p>
<p>The configuration is achieved by following some steps: The engineers configure the maximum limit of the thread pool size plus the maximum queue size, so that the SLA time is <em>n</em> times the average response time. This <em>n</em>, here <kbd>10</kbd>, is the maximum number of requests the system will handle at a time, consisting of the maximum pool size and maximum queue size limit. Any request that exceeds this number is immediately rejected by a service temporarily unavailable response. This is based on the calculation that the new request will likely exceed the calculated SLA time of 200 milliseconds, if the current number of handled requests exceeds <em>n</em>.</p>
<p>Immediately rejecting requests sounds like a harsh response, but by doing so, the client application is given the opportunity to retry a different backend without consuming the whole SLA time in vain in a single invocation. It's a case example for high performance scenarios with multiple backends where meeting SLAs has a high priority.</p>
<p>The implementation of this scenario is similar to the backpressure example in the previous chapter. The client uses different backends as a fallback if the first invocation failes with an unavailable service. This implicitly makes the client resilient since it uses multiple backends as fallback. The backend service implicitly applies the bulkhead pattern. A single functionality that is unavailable doesn't affect the rest of the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tackling performance issues</h1>
                </header>
            
            <article>
                
<p>Technical metrics, such as response time, throughput, error rates or uptime indicate the responsiveness of the system. As long as the application's responsiveness is in acceptable ranges, there is arguably no other metric to consider. Insufficient performance means that the system's SLAs are not being met, that is, the response time is too high or client requests fail. Then the question arises: what needs to be changed to improve the situation?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Theory of constraints</h1>
                </header>
            
            <article>
                
<p><span>If the desired load on the system increases, the throughput ideally increases as well. The theory of constraints is based on the assumption that there will be at least one constraint that will throttle the system's throughput. The constraints or bottlenecks therefore cause a performance regression.</span></p>
<p>Like a chain that is only as strong as its weakest link, the constraining resource limits the overall performance of the system or certain functionality thereof. It prevents the application from handling more load while other resources are not fully utilized. Only by increasing the flow through the constraining resource, that is, removing the bottleneck, will the throughput be increased. If the system is optimized <em>around the bottleneck</em> rather than removing it, the responsiveness of the overall system won't improve and, ultimately, it may even decrease.</p>
<p>It's therefore crucial to identify what the bottleneck is. The overall performance won't improve, before the limiting bottleneck gets targeted.</p>
<p>For example, throwing more CPU power at an application with high CPU utilization probably won't help to achieve better performance. Maybe the application isn't performing well because of other root causes than insufficient CPU.</p>
<p>It's important to mention here that the limiting constraint likely is external to the application. In a single, monolithic application, this includes the hardware and the operating system, with all running processes. If other software running on the same hardware heavily utilizes the network adapter, the application's network I/O and overall performance will be affected as well, even if the root cause, the limiting constraint, isn't the in responsibility of the application.</p>
<p>Inspecting performance issues therefore needs to take more into account than just the application itself. The whole set of processes running on the same hardware can have an impact on the application's performance, depending on how the other processes utilize the system's resources.</p>
<p>In a distributed environment, performance analytics also involves all interdependent applications, that interact with the application, and the communication in between. In order to identify the constraining resource, the overall situation of the system has to be taken into account.</p>
<p>Since the applications are interconnected, improving the responsiveness of a single system will affect others and can potentially even decrease the overall responsiveness. Again, trying to improve the wrong aspect, such as optimizing around the bottleneck, will not improve rather than most likely even downgrade the overall performance. An application that connects to an external system that represents the bottleneck , puts certain pressure on the external system. If the application's performance is tuned, rather than the external application, the load and pressure on the latter is increased which ultimately leads to overall worse responsiveness.</p>
<p>In distributed systems, the situation with all interdependent applications involved vastly complicates solving performance issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying performance regression with jPDM</h1>
                </header>
            
            <article>
                
<p><span>The </span><strong>Java Performance Diagnostic Model</strong><span> (</span><strong>jPDM</strong><span>) is a performance diagnostic model that abstracts the complexity of systems. It helps interpreting the </span><em>performance counters</em><span> of the system and thus understanding the root cause of why we experience performance regression.</span></p>
<p><span>The challenge with identifying performance regression is that a specific scenario is the result of innumerable influences, many of them external to the application. jPDM and the resulting methodologies helps dealing with that complexity.</span></p>
<p><span>In terms of responsiveness, there is an infinite number of things that can go wrong, but they will go wrong in a finite number of ways. Performance regression can therefore be categorized into different manifestations. There will be a few typical forms of issues, emerging in innumerable, varying scenarios, and root causes. In order to identify the different categories, we will make use of the diagnostic model.</span></p>
<p><span>jPDM identifies important subsystems of our system, their roles, functions, and attributes. The subsystems interact with each other. The model helps to identify tools to measure levels of activity and interactions between the subsystems. Methodologies and processes that help to study and analyze systems and situations in regard to performance, fall out of this model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subsystems</h1>
                </header>
            
            <article>
                
<p>The subsystems in a Java application environment are: the operating system, including hardware, the Java Virtual Machine, the application, and the actors. Subsystems utilize their corresponding, underlying subsystem to perform their tasks.</p>
<p>The following diagram shows how the jPDM subsystems interact with each other:</p>
<div style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/ea6405ce-e28f-4b85-83b7-034a53748208.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Actors</h1>
                </header>
            
            <article>
                
<p>The actors are the users of the system in a broadest sense. This includes end users, batch processes, or external systems, d<span>epending on the use case</span>.</p>
<p>By using the system, the actors will generate work load. The properties of the actors include the load factor, that is how many users are involved, as well as the velocity, that is how fast user requests are processed. These properties influence the overall situation similar to all other subsystem's properties.</p>
<p>The actors themselves don't represent a performance issue, they simply use the application. That said, if the system's performance isn't met, the limiting constraint is not to be searched for within the actors; the actors and the load they generate are part of the circumstances the system has to deal with.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application</h1>
                </header>
            
            <article>
                
<p>The enterprise application contains the business logic algorithms. Part of the business use cases is to allocate memory, schedule threads, and use external resources.</p>
<p>The application will use framework and Java language functionalities to fulfill this. It ultimately makes use of JVM code and configuration, directly or indirectly. By doing so, the application puts a certain load on the JVM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JVM</h1>
                </header>
            
            <article>
                
<p>The <strong>Java Virtual Machine</strong> (<strong>JVM</strong>) interprets and executes the application byte code. It takes care of memory management--allocation as well as garbage collection. There are vast optimization techniques in place to increase the performance of the program, such as <strong>Just-In-Time</strong> (<strong>JIT</strong>) compilation of the Java HotSpot Performance Engine.</p>
<p>The JVM utilizes operating system resources to allocate memory, run threads, or use network or disk I/O.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating system and hardware</h1>
                </header>
            
            <article>
                
<p>A computer's hardware components such as CPU, memory, disk and network I/O, define the resources of a system. They contain certain attributes, such as capacities or speed.</p>
<p>Since the hardware components represent non-shareable resources, the operating system provisions hardware between the processes. The operating system provides system-wide resources and schedule threads for the CPUs.</p>
<p>For this reason, the model considers the overall system, including hardware. The enterprise application potentially doesn't run alone on the system's hardware. Other processes utilize hardware components and thus influence the application's performance. Multiple processes that simultaneously try to access the network will result in poorer responsiveness than running each of them independently.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">jPDM instances - production situations</h1>
                </header>
            
            <article>
                
<p>Specific situations in a production system, are instances of the jPDM model. They contain all their properties, characteristics, and specific bottlenecks.</p>
<p>Any change in one of the subsystems would result in a different situation with different properties and characteristics, thus in a different instance of the model. For example, changing the load on the system could result in a totally different bottleneck.</p>
<p>This is also the reason why performance tests in environments other than production will result in potentially different bottlenecks. The different environment has at least a different OS and hardware situation, not necessarily in the hardware and configuration being used, but in the whole condition of OS processes. Simulated scenarios such as performance tests therefore don't allow conclusions about bottlenecks or performance optimizations. They represent a different jPDM instance.</p>
<p>Since we use the model to ultimately analyze performance issues, the following approaches only make sense when there are actual performance issues. If there is no issue, that is, the defined SLAs are met, there is nothing to investigate or act upon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the jPDM instances</h1>
                </header>
            
            <article>
                
<p>jPDM is used to assist in investigating performance regression. Methodologies, processes and tools that fall out of the model help to identify limiting constraints.</p>
<p>Each subsystem with its distinct set of attributes and resources plays a specific role in the system. We use specific tools to both expose specific performance metrics and monitor interactions between subsystems.</p>
<p>Looking back at the theory of constraints, we want to investigate the limiting constraint of a production situation, an instance of the jPDM. The tooling helps with investigating. <span>It's important for the investigation to take the overall system into account. The hardware is shared by all operating system processes. The dominance, therefore, may be caused by a single process or the sum of all processes running on that hardware.</span></p>
<p>First, we investigate the dominating consumer of the CPU and how the CPU is utilized. The CPU consumption pattern will lead us to the subsystem that contains the bottleneck.</p>
<p>In order to investigate the dominating consumer, we make use of a decision tree. It indicates where the CPU time is spent - in kernel space, user space, or idling. The following diagram shows the decision tree:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d9c3ebf8-92cb-4a85-b606-62beb88093dc.png"/></div>
<p>The round nodes in the graph represent the dominant consumers of the CPU. The colors represent the jPDM subsystems. Following the decision tree leads us to the subsystem that contains the bottleneck. Identifying the subsystem narrows down the performance regression to a specific category. We then use further tooling to analyze the instance of jPDM, the actual situation.</p>
<p>Since performance issues can originate from an infinite number of things, we need to follow a process to narrow down the causes. If we would not follow a process but blindly <em>peek and poke</em> or guess, we would not only waste time and effort but potentially wrongly identify symptoms as actual dominating constraints.</p>
<p>The dominant consumers of the CPU represent where the CPU time is spent. This is an important information to investigate the situation. It's not sufficient to solely look at the overall amount of CPU utilization. This information alone neither gives us much evidence of the existence of a bottleneck or how much headroom there is, nor does it lead to the dominating consumer. A CPU usage of 60% doesn't tell us whether the CPU is the constraining resource, that is whether adding more CPU would improve the overall responsiveness. The CPU time needs to be analyzed in greater detail.</p>
<p>First, we look at the ratio between CPU user and system time. This indicates whether the CPU time is spent in the kernel for longer than expected and thus whether the operating system is the dominating consumer of the CPU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dominating consumer - OS</h1>
                </header>
            
            <article>
                
<p>The operating system dominates the CPU consumption when it's asked to work harder than it usually should. This means that too much CPU time is spent on resource and device management. This includes network and disk I/O, locks, memory management, or context switches.</p>
<p>If the CPU system time is more than a certain percentage value of the user time, the operating system is the dominating consumer. The jPDM identified 10% as a threshold value, based on the experience of analyzing innumerable production situations. That means if the CPU system time is more than 10% of the user time, the bottlenecks are contained in the OS subsystem.</p>
<p>In this case, we investigate the issue further using operating system tools, such as <kbd>vmstat</kbd>, <kbd>perf</kbd>, <kbd>netstat</kbd>, and others.</p>
<p>For example, an enterprise application that retrieves database entries with a huge number of individually executed queries puts lots of pressure on the operating system in managing all these database connections. The overhead spent on establishing each and every network connection will eventually dominate the overall system and represent the constraining resource in the system. Investigating this situation thus shows a big share of CPU time spent in the kernel where the network connections are established.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dominating consumer - none</h1>
                </header>
            
            <article>
                
<p>If the CPU time didn't identify the OS to be the dominating consumer, we follow the decision tree further and analyze whether the CPU is idling. If that is the case, it means there is still CPU time available that cannot be consumed.</p>
<p>Since we are analyzing a situation where the SLA is not met, that is, the overall system is in a situation where it doesn't perform well enough for the given load, a well-saturated situation would fully utilize the CPU. Idling CPU times thus indicates a liveliness issue.</p>
<p>What needs to be investigated is why the threads are not scheduled by the operating system. This can have multiple causes, such as empty connection or thread pools, deadlock situations, or slow responding external systems. The state of the threads will indicate the cause of the constraint. We again use operating system tooling to investigate the situation.</p>
<p>An example for this category of issues is when an external system that responds slowly is accessed synchronously. It will lead to threads that are waiting for network I/O and can't run. This is the difference to dominating OS consumption, that the thread is not actively executing work but waiting to get scheduled.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dominating consumer - JVM</h1>
                </header>
            
            <article>
                
<p>The dominating consumers so far weren't contained in the application or JVM subsystems. If the CPU time is not overly spent in the kernel or idling, we start investigations in the JVM.</p>
<p>Since the JVM is responsible for memory management, its performance will indicate potential memory issues. Mainly <strong>Garbage Collection</strong> (<strong>GC</strong>) logs, together with <strong>JMX</strong> tooling help investigate scenarios.</p>
<p>Memory leaks will lead to increasing memory usage and excessive garbage collector runs that occupy the CPU. Inefficient memory usage will equally lead to excessive garbage collections. The GC executions ultimately cause the JVM being the dominating consumer of the CPU.</p>
<p>This is another example of why it's important to follow the process of the jPDM decision tree. The performance issue arises in high CPU usage, although the actual bottleneck in this case is the memory leak.</p>
<p>As of today, the main cause of performance issues are related to memory, mostly from application logging that results in extensive string object creation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dominating consumer - application</h1>
                </header>
            
            <article>
                
<p>If the JVM analysis didn't indicate a memory issue, finally the application is the dominating consumer of the CPU. This means that the application code itself is responsible for the bottleneck. Especially applications that run sophisticated algorithms excessively utilize the CPU.</p>
<p>Application-related profiling will lead to conclusions where in the application the issue originates and how the issue might be resolved. This means that the application either contains suboptimal code or reached the possible limit with the given resources, and ultimately needs to be scaled horizontally or vertically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conclusion</h1>
                </header>
            
            <article>
                
<p>The approach of solving performance issues is to try to characterize the regression first by investigating the situation by following a specific process. After the constraining resource has been identified, further steps to resolve the situation are applied. After potentially fixing the situation, the measurement in production needs to be repeated. It's important to not change behavior or configuration without the verification that the changes in fact provide the expected results.</p>
<p>The jPDM approach investigates performance regression without considering the application's code, by applying a uniform solving process.</p>
<p>What tools and metrics are needed to apply this approach?</p>
<p>Depending on the system in production, tools that ship with the operating system, as well as Java Runtime-related tools, are useful. Since all aspects consider the overall system at the operating system level rather than just the application alone, operating system tools and lower-level metrics are more helpful than application-specific ones.</p>
<p>However, the technical metrics of the application, such as response time or throughput, are the first place of focus that indicate the application's quality of service. If these metrics indicate a performance issue, then it makes sense to investigate using lower-level metrics and tools.</p>
<p>The next section examines how to gather the application's technical metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical metrics</h1>
                </header>
            
            <article>
                
<p>Technical metrics indicate the load and responsiveness of the system. Prime examples for these metrics are the response time, as well as the throughput, often gathered as requests or transactions per second, respectively. They provide information about how the overall system currently performs.</p>
<p>These metrics will ultimately have an impact on other, business-related metrics. At the same time, as we have seen in the previous section, these metrics are just indicators and themselves affected by a lot of other technical aspects, namely all properties of jPDM subsystems.</p>
<p>Therefore, an application's performance is impacted by a lot of technical influences. Thus, the question is, which technical metrics besides response time, throughput, error rates, and uptime should reasonably be collected?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of technical metrics</h1>
                </header>
            
            <article>
                
<p>Technical metrics are primarily concerned with the quality of the application's service, such as response times or throughput. They are the indicators that represent the application's responsiveness and may point out potential performance issues. The information can be used to create statistics about trends and application peaks.</p>
<p>This insight increases the likelihood of foreseeing potential outages and performance issues in a timely manner. It is the technical equivalent of business insights into the otherwise black box system. These metrics alone allow no sound conclusions about the root cause or constraining resources in the case of performance issues.</p>
<p>Lower-level technical information includes resource consumption, threading, pooling, transactions, or sessions. It's again important to mention that this information alone does not direct the engineers to potential bottlenecks.</p>
<p>As shown previously, it's necessary to inspect the overall situation with everything running on specific hardware. The operating system information provides the best source of information. In order to solve performance issues, the operating system, as well as application tools, are required to take this into account.</p>
<p>This doesn't mean that the technical information emitted by the application or the JVM runtime has no value at all. The application-specific metrics can assist in solving performance issues. It's important to keep in mind that these metrics alone will lead to potentially wrong assumptions about what the constraining resources are when a system needs to be performance-tuned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High frequency monitoring versus sampling</h1>
                </header>
            
            <article>
                
<p>Often, monitoring aims to collect technical metrics with a high frequency of many collections per second. The problem with this high frequency collection is that it heavily impacts the performance of the system. Metrics often get collected even if there is no performance regression.</p>
<p>As mentioned, application-level metrics, such as resource consumption, alone don't help much in identifying potential performance constraints. In the same way, the collection disrupts the responsiveness of the system.</p>
<p>Instead of monitoring with a high frequency it's advisable to sample metrics with lower frequency, such as for only a few times per minute. The theory behind statistical populations shows that these few samples represent the population of data well enough.</p>
<p>Sampling the information should have as little impact on the application's performance as possible. All subsequent investigations, or metrics querying or calculations should happen out-of-band; that is, outsourced to a system that does not impact the running application. The concerns for sampling the information from storing, querying, and displaying it, are thus separated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting technical metrics</h1>
                </header>
            
            <article>
                
<p>The application is a good place to gather the technical metrics, ideally at the system boundaries. It's equally possible to collect these in a potential proxy server.</p>
<p>The application server already emits technically-relevant metrics such as information about resource consumption, threading, pooling, transactions, or sessions. Some solutions also provide Java agents that sample and emit technically-relevant information.</p>
<p>Traditionally, application servers are required to make technically relevant metrics available via JMX. This functionality is part of the Management API, but has never been used much in projects. One of the reasons for this is that the model and API are quite cumbersome.</p>
<p>However, it's helpful to mention that Java EE application servers are required to gather and provide data about its resources. The container emits this information via JMX. There are several ways to scrape this information.</p>
<p>There are so-called exporters available, applications that either run standalone or as <strong>Java agents</strong>, that access the JMX information and emit it via HTTP. The Prometheus JMX exporter, which exports the information in a similar format as shown previously, is an example of this. The benefit of this approach is that it doesn't add dependencies into the application.</p>
<p>The installation and configuration of Java agents is done in the application server, in a base container image layer. This once again emphasizes the principle that containers should not couple the application's artifact with implementation details.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boundary metrics</h1>
                </header>
            
            <article>
                
<p>Technical metrics that are application-specific, such as response times, throughput, uptime, or error rates can be gathered at the system boundaries. This can happen via the interceptors or filters, depending on the situation. HTTP-relevant monitoring can be collected via a servlet filter for any technology that builds upon servlets, such as JAX-RS.</p>
<p>The following code snippet shows a servlet filter that gathers the response time and throughput in a Prometheus histogram metrics:</p>
<pre style="padding-left: 30px">import javax.servlet.*;
import javax.servlet.annotation.WebFilter;
import javax.servlet.http.HttpServletRequest;

<strong>@WebFilter(urlPatterns = "/*")</strong>
public class MetricsCollectorFilter <strong>implements Filter</strong> {

    <strong>private Histogram requestDuration;</strong>

    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
        <strong>requestDuration = Histogram.build("request_duration_seconds",</strong><br/><strong>                "Duration of HTTP requests in seconds")
                .buckets(0.1, 0.4, 1.0)
                .labelNames("request_uri")
                .register();</strong>
    }

    public <strong>void doFilter(ServletRequest req, ServletResponse res,</strong><br/><strong>            FilterChain chain)</strong> throws IOException, ServletException {<br/>        if (!(req instanceof HttpServletRequest)) {
            chain.doFilter(req, res);
            return;
        }

        String url = ((HttpServletRequest) req).getRequestURI();
        try (<strong>Histogram.Timer ignored = requestDuration</strong><br/><strong>                .labels(url).startTimer()</strong>) {
            chain.doFilter(req, res);
        }
    }

    @Override
    public void destroy() {
        // nothing to do
    }
}</pre>
<p>This metric is registered similarly to the business-related example previously, and emitted via the Prometheus output format. The histogram buckets collect the time in four buckets, with the specified times from 0.1, 0.4, or 1.0 seconds, and everything above. These bucket configurations need to be adapted to the SLAs.</p>
<p>The servlet filter is active on all resource paths and will collect the statistics, qualified by each path.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging and tracing</h1>
                </header>
            
            <article>
                
<p>Historically, logging had quite high importance in enterprise applications. We have seen a lot of logging framework implementations and supposedly best practices on how to implement reasonable logs.</p>
<p>Logging is typically used for debugging, tracing, journaling, monitoring, and outputting errors. In general, all information that developers consider somewhat important, but not made apparent to the users, is been placed into logs. In almost all cases, this includes logging to files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shortcomings of traditional logging</h1>
                </header>
            
            <article>
                
<p>This approach, which is way too common in enterprise projects, comes with a few problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance</h1>
                </header>
            
            <article>
                
<p>Traditional logging, especially extensively used logging invocations, creates a lot of string objects. Even APIs such as <strong>Slf4J</strong> that aim to reduce unnecessary string concatenation will result in high memory rates. All these objects need to be garbage collected after their use, which utilizes the CPU.</p>
<p>Storing log events as string messages is a verbose way of storing information. Choosing different formats, mainly binary formats would drastically reduce the message size and result in more efficient memory consumption and higher throughput.</p>
<p>Log messages that are stored in a buffer or directly on disk need to be synchronized with other log invocations. Synchronous loggers ultimately cause a file to be written within a single invocation. All simultaneous log invocations need to be synchronized in order to ensure that logged events appear in the right order. This presents the issue that synchronization indirectly couples functionality that otherwise is completely unrelated. It decreases the parallelism of intrinsically independent functionality and has a negative overall performance impact. With a high number of log messages being written, the probability of blocking threads due to synchronization increases.</p>
<p>Another issue is that logging frameworks usually don't write the log messages to disk directly; rather, they use several layers of buffering. This optimization technique comes with certain management overhead involved that does not improve the situation either. Synchronous file operations advisably work with the least overhead layers as possible.</p>
<p>Log files that reside on NFS storage decrease the overall performance even more, since the write operation hits the operation system I/O twice, with both file system and network calls involved. In order to manage and persist log files, network storage is an often chosen solution, especially for container orchestration that needs persisted volumes.</p>
<p>In general, experience shows that logging has the biggest impact in an application's performance. This is mostly due to the memory impact on string log messages.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Log levels</h1>
                </header>
            
            <article>
                
<p>Logging solutions include the ability to specify the importance of a log entry via log levels, such as <em>debug</em>, <em>info</em>, <em>warning,</em> or <em>error</em>. Developers might ask themselves which log level to choose for specific invocations.</p>
<p>The approach of having several layers certainly sounds reasonable, since production systems can specify a higher log level than development runs, so as not to produce too much data in production.</p>
<p>The challenge with this situation is that in production there is usually no debug log information available when it's needed. Potential error situations that could need additional insights don't have this information available. Debug or trace log levels that include tracing information are switched off.</p>
<p>Choosing log levels is always a trade-off regarding what information should be included. Debugging in development is done best using actual debug tools which connect against running applications, potentially remotely. Debug or trace logs are usually not available in production and therefore provide little benefit.</p>
<p>Whereas defining multiple log levels may have emerged from a good intention, the practical use in production systems adds little value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Log format</h1>
                </header>
            
            <article>
                
<p>Traditional logging solutions specify particular log layouts that format the log messages in the resulting log file. The application needs to manage the creation, rolling, and formatting of log files that are not relevant for the business logic.</p>
<p>Quite a few enterprise applications ship with third-party logging dependencies that implement this functionality, but provide no business value.</p>
<p>Choosing particular plain text log formats is another decision that needs to be made by the application developers. There is a trade-off between a log entry format that is readable by both humans and machines. The result is usually the worst compromise for both parties; string log formats that are both hardly readable and have a tremendous impact on the system's performance.</p>
<p>It would be more reasonable to choose binary formats that store information with the highest density. Humans then could use tooling to make the messages visible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amounts of data</h1>
                </header>
            
            <article>
                
<p>Extensive logging introduces a huge amount of data that is contained in log files. In particular, logs that are used for debugging and tracing purposes result in big files that are cumbersome and expensive to parse.</p>
<p>Parsing log formats in general introduces an avoidable overhead. Information that is potentially technically-relevant is serialized in a specific format first, just to be parsed again later when inspecting the logs.</p>
<p>Later in this sub-section, we will see what other solutions there are.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Obfuscation</h1>
                </header>
            
            <article>
                
<p>Equal to unreasonably checked exception handling, logging obfuscates business logic in the source code. This is especially the case for boilerplate log patterns that are common in many projects.</p>
<p>Log statements take up too much space in the code and especially draw the developer's attention.</p>
<p>Some logging technology, such as Slf4j, provides functionality to format strings in readable ways while avoiding immediate string concatenation. But still, log statements add obfuscating invocations that are unrelated to the business problem.</p>
<p>This is obviously less the case if the debug log statements are added in a cross-cutting component, such as an interceptor. However, these cases mostly add logging for tracing purposes. We will see in the next sub-section that there are more suitable solutions for this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The concerns of applications</h1>
                </header>
            
            <article>
                
<p>As we have seen in 12-factor applications, it is not an application's concern to choose log files and message formats.</p>
<p>In particular, logging frameworks that promise simpler logging solutions add technically-motivated third-party dependencies to the application; dependencies that have no direct business value.</p>
<p>If there is business value in events or messages, then the use of another solution should be favored. The following shows how traditional logging is misused for these other applications' concerns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrong choice of technology</h1>
                </header>
            
            <article>
                
<p>Traditional logging, and how it is used in the majority of enterprise projects, is a suboptimal choice for concerns that are better handled using different approaches.</p>
<p>The question is: what do developers want to log, anyway? What about metrics, such as the current resource consumption? Or business-related information, such as <em>car manufactured</em>? Should we log debugging and tracing information such as <em>request with UUID xy originated from application A, and called subsequent application B</em>? What about occurring exceptions?</p>
<p>Attentive readers will see that most of the use cases for traditional logging are far better handled using other approaches.</p>
<p>If logging is used for debugging or debug tracing applications, the approach with using trace or debug levels doesn't help much. Information that will not be available in production can't reproduce a potential bug. Logging a huge amount of debug or trace events in production, however, will affect the application's responsiveness due to disk I/O, synchronization, and memory consumption. Debugging concurrency-related errors may even lead to a different outcome, due to the modified order of execution.</p>
<p>For debugging functionality, it's much more advisable to use actual debugger features during development, such as IDEs that connect against a running application. Logging that is used for business-motivated journaling is better accomplished via a proper journaling solution, as we will see later in this chapter. The plain text log messages are certainly not the ideal solution. The chosen technology should minimize the performance impact on the application.</p>
<p>Another approach to realize the same motivations behind journaling is to introduce event sourcing. This makes the domain events part of the application's core model.</p>
<p>Business-motivated tracing, this should be part of the business use case as well, implemented using an adequate solution. As we will see in the next sub-section, there are more suitable tracing solutions that require less parsing and have a smaller performance impact. Tracing solutions also support the consolidation of information and requests across microservices.</p>
<p>Monitoring information that is stored in log messages is better managed via the use of proper monitoring solutions. This approach is not just much more performant, it is also a more effective way of emitting the information in proper data structures. The examples we have seen earlier in this chapter illustrate monitored data and possible solutions.</p>
<p>Logging is also traditionally being used to output exceptions and errors that cannot properly be handled in the application otherwise. This is arguably the only reasonable use of logging. Together with other potential metrics that may capture the error, such as error rate counters at the system boundary, the logged exception may support developers in investigating errors.</p>
<p>However, errors and exceptions should only be logged if they in fact concern the application and represent an error that can be resolved by developers. With monitoring and alerting solutions in place, the need to look into logs should indicate a serious problem with the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging in a containerized world</h1>
                </header>
            
            <article>
                
<p>One of the 12-factor principles is to treat logging as a stream of events. This includes the idea that handling log files should not be a concern of the enterprise application. Log events should simply output to the process' standard output.</p>
<p>The application's runtime environment consolidates and processes the log streams. There are solutions for unified access over all participating applications that can be deployed into the environment. The runtime environment where the application is deployed takes care of processing the log streams.<span> </span><strong>fluentd</strong>, which is part of the Cloud Native Computing Foundation unifies the access to log events in a distributed environment.</p>
<p>Application developers should treat the used logging technology as simply as possible. The application container is configured to output all server and application log events to standard output. This approach <span>simplifies matters for enterprise developers and enables them to focus more on solving actual business problems.</span></p>
<p>As we have seen, there is not much information left that application developers reasonably can log in a traditional way. Monitoring, journaling, or tracing solutions, as well as event sourcing, can solve the requirements in more suitable ways.</p>
<p>Together with logging to standard output without the need for sophisticated log file handling, there is no need for sophisticated logging framework. This supports zero-dependency applications and enables developers to be able to focus on business concerns instead.</p>
<p>It's therefore advisable to avoid third-party logging frameworks, as well as writing to traditional log files. The need to manage log rotations, log entry formats, levels, and framework dependencies, as well as configuration, becomes no longer necessary.</p>
<p>However, the following might seem antithetical to enterprise developers.</p>
<p>The straightforward, 12-factor way to log the output is using the standard output capabilities of Java via <kbd>System.out</kbd> and <kbd>System.err</kbd>. This directly writes the synchronous output without needless layers of buffering.</p>
<p>It's important to mention that outputting data via this approach will not perform. The introduced synchronization, again, ties otherwise independent parts of the application together. If the output of the process is grabbed and emitted by a video card, the performance will further decrease.</p>
<p>Logging to console is only meant to emit errors that are, as the name of the Java type indicates - an exception. In all other cases, engineers must ask themselves why they want to output an information in the first place, or whether other solutions are more suitable. Therefore, logged errors should indicate a fatal problem that requires engineering action. It should not be expected to receive this log output in production; in this fatal error case, performance can be disrespected.</p>
<p>In order to output fatal error information, Java EE applications can use CDI features as well as Java SE 8 functional interfaces to provide a uniform logging functionality:</p>
<pre>public class LoggerExposer {

    <strong>@Produces</strong>
    public <strong>Consumer&lt;Throwable&gt;</strong> fatalErrorConsumer() {
        return <strong>Throwable::printStackTrace</strong>;
    }
}</pre>
<p>The <kbd>Consumer&lt;Throwable&gt;</kbd> logger is then injectable in other beans, and it logs using the <kbd>accept()</kbd> method of the consumer type. If a more readable interface is desired, a thin logger facade type which is injected via <kbd>@Inject</kbd> can be defined as follows:</p>
<pre>public class ErrorLogger {

    public void fatal(Throwable throwable) {
        throwable.printStackTrace();
    }
}</pre>
<p>This approach will seem antithetical to enterprise developers, especially logging without using a logging framework. Using a sophisticated logging framework, which is used to direct the output to standard out again, introduces overhead, which ultimately ends up in the same result. Some developers may prefer to use JDK logging at this point.</p>
<p>However, providing sophisticated log interfaces and thus giving application developers the opportunity to output all kinds of information, especially human-readable strings, is counterproductive. This is why the code examples only allow to output throwable types in fatal error situations.</p>
<p>It's important to notice the following few aspects:</p>
<ul>
<li>Traditional logging should be avoided and substituted with more-suited solutions</li>
<li>Only fatal error cases that are the exception, and are expected to ideally never happen, should be logged</li>
<li>Containerized applications are advised to output log events to standard out</li>
<li>Application logging and interfaces should be as simple as possible, preventing developers from excessive use</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Journaling</h1>
                </header>
            
            <article>
                
<p>If journaling is needed as part of the business logic, there are better ways than using logging frameworks. The requirements for journaling could be auditing regulations, such as is the case for trading systems.</p>
<p>If the business logic requires journaling, it should accordingly be treated as such - a business requirement. There is journaling technology available that synchronously persists the required information with higher density and lower latency than traditional logging. An example of these solutions is <strong>Chronicle Queue</strong>, which allows us to store messages with high throughput and low latency.</p>
<p><span>The application domain could model the information as a domain event and directly persist it into a journaling solution. </span>As mentioned previously, another way is to base the application on an event sourcing model. The auditing information is then already part of the application's model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tracing</h1>
                </header>
            
            <article>
                
<p>Tracing is used to reproduce specific scenarios and request flows. It's already helpful in retracing complex application processes, but it's especially helpful when multiple applications and instances are involved.</p>
<p>However, what's important to be pointed out is that there needs to be a business, not technical, requirement for tracing systems, similar to journaling.</p>
<p>Tracing is a poor technique for debugging or performance tracing systems. It will have a certain impact on performance and doesn't help much in resolving performance regression. Interdependent, distributed applications that need to be optimized in their performance advisably solely emit information about their quality of service, such as response times. Sampling techniques can sufficiently gather information that indicate performance issues in the applications.</p>
<p>However, let's have a look at business-motivated tracing to track the components and systems involved.</p>
<p>The following diagram shows a trace of a specific request involving multiple application instances and components thereof:</p>
<div style="padding-left: 60px" class="mce-root CDPAlignLeft CDPAlign"><img height="301" width="503" src="assets/e8a210b2-6eba-4d32-abf1-059aa19fb534.png"/></div>
<p>The trace can also be displayed in a timeline to show the synchronous invocations as demonstrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/76967c55-0e8a-4e2a-91db-0c10f6107986.png"/></div>
<p>Tracing includes information about which applications or application components have been involved and how long the individual invocations took.</p>
<p>Traditionally, log files have been used for this, by logging the start and end of each method or component invocation including a correlation ID, such as a thread identifier. There is the possibility of including correlation IDs into logs that are used from a single originating request and are reused and logged in subsequent applications. This results in traces that also span multiple applications.</p>
<p>In the case of logging, the tracing information was accumulated from multiple log files; for example, using solutions such as the <strong>ELK</strong> stack. Trace logs are usually implemented in cross-cutting ways; for example, using logging filters and interceptors so as not to obfuscate the code.</p>
<p>However, using log files for tracing is not advisable. Even enterprise applications that experience a moderate load introduce a lot of log entries that are written to files. Many log entries are needed for each and every request.</p>
<p>File-based I/O and the needed log format serialization generally is too heavy for this approach and greatly affects the performance. Tracing to log file formats introduces a lot of data that needs to be parsed again afterwards.</p>
<p>There are tracing solutions that provide a much better fit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tracing in a modern world</h1>
                </header>
            
            <article>
                
<p>In the past months and years, multiple tracing solutions have originated that aim to minimize the performance impact on the system.</p>
<p><strong>OpenTracing</strong> is standard, vendor-neutral tracing technology that is part of the Cloud Native Computing Foundation. It defines the concepts and semantics of traces and supports tracing in distributed applications. It is implemented by multiple tracing technologies such as <strong>Zipkin</strong>, <strong>Jaeger</strong>, or <strong>Hawkular</strong>.</p>
<p>A hierarchical trace consists of several spans, similar to the ones shown in the previous figures. A span can be a child of, or follow, another span.</p>
<p>In the previous example, the car manufacture component span is a child of the load balancer span. The persistence span follows the client span since their invocations happen sequentially.</p>
<p>An OpenTracing API span includes a time span, an operation name, context information, as well as optional sets of tags and logs. The operation names and tags are somewhat similar to Prometheus metric names and labels described earlier in the <em>Enter Prometheus</em> section. Logs describe information such as span messages.</p>
<p>An example for a single span is <kbd>createCar</kbd> with the tags <kbd>color=RED</kbd> and <kbd>engine=DIESEL,</kbd> as well as a log <kbd>message</kbd> field <kbd>Car successfully created</kbd>.</p>
<p>The following code snippet shows an example of using the OpenTracing Java API in the <em>car manufacture</em> application. It supports Java's try-with-resource feature.</p>
<pre>import io.opentracing.ActiveSpan;
import io.opentracing.Tracer;

@Stateless
public class CarManufacturer {

    <strong>@Inject
    Tracer tracer;</strong>

    public Car manufactureCar(Specification spec) {
        try (<strong>ActiveSpan span = tracer.buildSpan("createCar")
                .withTag("color", spec.getColor().name())
                .withTag("engine", spec.getEngine().name())
                .startActive()</strong>) {

            // perform business logic

            <strong>span.log("Car successfully created");</strong>
        }
    }
}</pre>
<p>The created span starts actively and is added as a child to a potentially existing parent span. The <kbd>Tracer</kbd> is produced by a CDI producer that depends on the specific OpenTracing implementation.</p>
<p>Obviously, this approach obfuscates the code a lot and should be moved to cross-cutting components, such as interceptors. Tracing interceptor bindings can decorate methods and extract information about method names and parameters.</p>
<p>Depending on the desired information included in tracing spans, the interceptor binding can be enhanced to provide further information, such as the operation name.</p>
<p>The following code snippet shows a business method decorated with an interceptor binding that adds tracing in a lean way. Implementing the interceptor is left as an exercise for the reader:</p>
<pre>@Stateless
public class CarManufacturer {

    ...

    <strong>@Traced(operation = "createCar")</strong>
    public Car manufactureCar(Specification spec) {
        // perform business logic
    }
}</pre>
<p>The traced information is carried into subsequent applications via span contexts and carriers. They enable participating applications to add their tracing information as well.</p>
<p>The gathered data can be extracted via the used OpenTracing implementation. There are filter and interceptor implementations available for technology such as JAX-RS resources and clients that transparently add the required debug information to the invocations, for example, using HTTP headers.</p>
<p>This way of tracing impacts the system's performance way less than traditional logging. It defines the exact steps and systems that instrument the business logic flow. However, as mentioned before, there needs to be a business requirement to implement a tracing solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Typical performance issues</h1>
                </header>
            
            <article>
                
<p>Performance issues come with typical symptoms, such as response times, that are slow or become slower over time, timeouts, or even completely unavailable services. The error rates indicate the latter.</p>
<p>When performance issues arise, the question to be asked is what the actual constraining resource, the bottleneck, is. Where does the issue originate? As shown earlier, engineers are advised to follow an investigative process that considers the overall situation, including hardware and operating systems, <span>in order to find the constraint</span>. There should be no guessing and premature decisions.</p>
<p>Performance problems can have a huge number of root causes. Most of them originate in coding errors or misconfiguration rather than actual workload exceeding the available resources. Modern application servers can handle a lot of load until the performance becomes an issue.</p>
<p>However, experience shows that there are typical performance issue root causes. The following will show you the most serious ones.</p>
<p>Engineers are instructed to investigate issues properly, without following supposedly best practices and premature optimizations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging and memory consumption</h1>
                </header>
            
            <article>
                
<p>Traditional logging, such as writing string-formatted log messages to files, is the most common root cause for poor performance. This chapter has already described the issues and advisable solutions for them.</p>
<p>The biggest reason for poor performance is the extensive string object creation and resulting memory consumption. High memory consumption, in general, represents a major performance issue. This is not only caused by logging but by high memory rates in caching, memory leaks, or extensive object creation.</p>
<p>Since the JVM manages the garbage collection of memory, these high memory rates result in garbage collector runs, trying to free unused memory. The garbage collection utilizes the CPU. The situation is not resolved by a single collection run, what results in subsequent GC executions and thus <span>high CPU usage</span>. This happens if not sufficient memory can be freed either because of memory leaks or a high workload with high consumption. Even if the system doesn't crash with <kbd>OutOfMemoryError</kbd>, the CPU usage can effectively stall the application.</p>
<p>Garbage collection logs, heap dumps, and measurements can help with investigating these issues. JMX tools provide insights about the memory distribution and potential hot spots.</p>
<p>If business logic is implemented in a lean, straightforward way using short-lived objects, memory issues are far less likely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Premature optimization</h1>
                </header>
            
            <article>
                
<p>It regularly happens in enterprise projects that developers try to prematurely optimize applications without proper verification. Examples for this are the usage of caching, configuring pools, and application server behavior, without sampling sufficient measurements before and after tweaking.</p>
<p>It's highly advisable to not consider to use these optimizations before there is an identified performance problem. Proper performance sampling and measurements in production, as well as investigating the constraining resource, are a necessity before changing the setup.</p>
<p>In the vast majority of cases, it's sufficient to go with convention over configuration. This is true for both the JVM runtime as well as the application server. If developers take a plain Java EE approach with the default application server configuration, they won't likely run into issues with premature optimization.</p>
<p>If technical metrics indicate that the current approach is not sufficient for the production workload, only then is there a need to introduce change. Also, engineers should validate the necessity of the change over time. Technology changes and an optimization that provided remedy in previous runtime versions might not be the best solution anymore.</p>
<p>The approach of convention over configuration and taking the default configuration first also requires the least amount of initial effort.</p>
<p>Again, experience shows that a lot of issues originated from prematurely introducing change without proper verification beforehand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Relational databases</h1>
                </header>
            
            <article>
                
<p>Typical scapegoats for insufficient performance are relational databases. Usually, application servers are deployed in multiple instances that all connect against a single database instance. This is a necessity to ensure consistency due to the CAP theorem.</p>
<p>The database as a single point of responsibility, or failure, is predestined to become a bottleneck. Still, engineers must consider proper measurements to verify this assumption.</p>
<p>If metrics indicate that the database response is slower than acceptable, again the first approach is to investigate the root cause. If the database query is responsible for causing the slow response, engineers are advised to take a look at the performed queries. Is a lot of data being loaded? If yes, is all this data necessary or will it be filtered and reduced by the application later on? In some cases, the database queries load more data than required.</p>
<p>This is also a question relevant to the business, especially for retrieving data, whether everything is required. More specific database queries that pre-filter results or size limits, such as pagination, can help in these cases.</p>
<p>Databases perform exceptionally well when joining and filtering data. Performing more complex queries directly in the database instance usually outperforms loading all required data into the application's memory and executing the query there. It's possible to define complex, nested SQL queries in Java and to execute them in the database. However, what enterprise applications should avoid is to define business logic queries directly in the database, using stored procedures. Business-related logic should reside in the application.</p>
<p>A typical configuration mistake is also neglecting to index relevant database columns that are used in queries. There were many cases in projects where the overall performance could be improved by several factors just by defining proper indexes.</p>
<p>In general, the insight measurements of specific use cases usually provide good insights on where the issue might originate from.</p>
<p>In some scenarios, queries that update data often result in optimistic locking errors. This originates from domain entities simultaneously being updated. Optimistic locking is rather a business issue than a technical one. The service error rate will indicate such issues.</p>
<p>If the business use case requires that entities are often changed simultaneously, development teams can consider changing the functionality to an event-based model. Similarly, as shown previously, event sourcing and event-driven architectures get rid of this situation by introducing eventual consistency.</p>
<p>If the performance issues purely originates from workload and concurrent accesses, then ultimately a different data model is required, such as event-driven architectures realized with CQRS. However, usually the situation is solvable in another way. The vast majority of enterprise applications scale well enough using relational databases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Communication</h1>
                </header>
            
            <article>
                
<p>The majority of c<span>ommunication-related performance issues are </span>due to synchronous communication. Most issues in this area emerge from missing timeouts that lead client calls to block infinitely and cause deadlock situations. This happens if no client-side timeouts are configured and the invoked system is unavailable.</p>
<p>A less critical but similarly imperfect situation occurs if the configured timeouts are too large. This causes systems to wait for too long, slowing down processes and blocking threads.</p>
<p>Configuring timeouts for client invocations, as described earlier, provides simple but effective relief from this issue.</p>
<p>High response time and low throughput can have multiple origins. Performance analysis provides insights into where the time is spent.</p>
<p>There are some other potential bottlenecks, such as payload sizes. Whether data is sent as plain text or binary data can make quite some difference in payload sizes. Serialization that uses imperfect algorithms or technology can also decrease the responsiveness. Still, these concerns are usually negligible unless the application resides in high performance situations.</p>
<p>If multiple, synchronous invocations are required, they should happen in parallel if possible, using container-managed threads; for example, provided by a managed executor service. This avoids unnecessarily making the application wait.</p>
<p>In general, use cases that span multiple transactional systems, such as databases using distributed transactions, should be avoided. As described previously, distributed transactions won't scale. The business use case should be considered to effectively process asynchronously instead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Threading and pooling</h1>
                </header>
            
            <article>
                
<p>In order to reuse threads as well as connections, application containers manage pools. Requested threads don't necessarily have to be created but are reused from a pool.</p>
<p>Pooling is used to control the load on specific parts of the system. Choosing appropriate pool sizes allows the system to be saturated well, but prevents it from overloading. This is due to the fact that empty pools will lead to suspended or rejected requests. All threads and connections of that pool are then being utilized already.</p>
<p>The bulkhead pattern prevents different parts of the system from affecting each other by defining dedicated thread pools. This limits the resource shortage to a potentially problematic functionality. In some cases, functionality such as a legacy system might be known to cause issues. Bulkheads, implemented as dedicated thread pools, and timeout configuration help preserve the application's health.</p>
<p>Empty pools either originate from the current load on that pool being exceptionally high, or resources that are acquired for much longer than expected. In any case, it's advisable not to simply increase the corresponding pool size but to investigate where the issue originates from. The described investigation techniques as well as JMX insights and thread dumps will supports you in finding bottlenecks, as well as potential programming errors, such as deadlocks, misconfigured timeouts, or resource leaks. In the minority of cases will a shortage in pooling actually originate from a high workload.</p>
<p>Pool sizes and configuration is made in the application container. Engineers must perform proper performance sampling in production before and after reconfiguring the server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance testing</h1>
                </header>
            
            <article>
                
<p>The challenge with performance testing is that the tests run in a simulated environment.</p>
<p>Simulated environments are fine for other kinds of tests, such as system tests, since certain aspects are abstracted. Mock servers, for example, can simulate behavior similarly to production.</p>
<p>However, unlike in functional tests, validating the system's responsiveness requires to take everything in the environment into account. At the end of the day, applications are running on actual hardware, thus the hardware, as well as the overall situation, impacts the application's performance. The system's performance in simulated environments will never behave equally in production. Therefore, performance tests are not a reliable way of finding performance bottlenecks.</p>
<p>There are many scenarios where an application can perform much better in production compared to performance tests, depending on all the immediate and imminent influences. The HotSpot JVM, for example, performs better under high load.</p>
<p>Investigating performance constraints therefore can only happen in production. As shown earlier, the jPDM investigation processes, together with sampling techniques and tools applied to the production system, will identify the bottleneck.</p>
<p>Performance and stress tests help in finding obvious code or configuration errors, such as resource leaks, serious misconfiguration, missing timeouts, or deadlocks. These bugs will be found before deploying to production. Performance tests can also capture performance trends over time and warn engineers if the overall responsiveness decreases. Still, this may only indicate potential issues but should not lead the engineers to premature conclusions.</p>
<p>Performance and stress tests only make sense in the whole network of interdependent applications. This is because of dependencies and performance influences of all the systems and databases involved. The setup needs to be as similar to production as possible.</p>
<p>Even then, the outcome will not be the same as in production. It's highly important that engineers are aware of this. Performance optimizations that follow performance tests are therefore never fully representative.</p>
<p>For performance tuning, it's important to use investigative processes together with sampling on production instead. Continuous Delivery techniques support in quickly bringing configuration changes to production. Then engineers can use the sampling and performance insights to see whether changing the setup has improved the overall solution. And again, the overall system needs to be taken into account. Simply tuning a single application without considering the whole system can have negative effects on the overall scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Business-related metrics can provide helpful insights into the enterprise application. These metrics are a part of the business use case and therefore should be treated as such. Business metrics are ultimately impacted by other, technical metrics. It's therefore advisable to monitor these metrics as well.</p>
<p class="mce-root"><span>The theory of constraints describes that there will be one ore more limiting constraints that prevent the system from infinitely increasing its throughput. In order to improve the application's performance the limiting constraint therefore needs to be eradicated. jPDM helps identifying the limiting constraints by finding the dominating consumer of the CPU first and using appropriate tooling to further investigate performance issues. It's advisable to investigate potential bottlenecks by following this process, which takes the overall situation into account, rather than to blindly</span> <em>peek and poke</em><span>.</span></p>
<p>Rather than using high-frequency monitoring, engineers are advised to sample technical metrics with low frequency and to query, calculate, and investigate out-of-band. This has tremendously less impact on the application's performance. Distributed applications will need to meet SLAs. The backpressure approach as well as the bulkhead pattern can help achieve highly responsive, resilient enterprise systems.</p>
<p>Traditional logging should be avoided for a number of reasons, especially the negative performance impact. Enterprise applications are advised to only output log events in case of fatal, unexpected errors, which are written to standard output in a preferably straightforward way. For all other motivations, such as debugging, tracing, journaling, or monitoring, there are more suitable solutions.</p>
<p>Performance and stress tests running in simulated environments can be used to find obvious errors in the application. The environments should be as close to production, including all applications and databases involved. For any other reasoning, especially statements about an application's expected performance, bottlenecks, or optimizations, performance tests are not helpful and might even lead to wrong assumptions.</p>
<p>The next chapter will cover the topic of application security.</p>


            </article>

            
        </section>
    </body></html>