- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Using a Service Mesh to Improve Observability and Management
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用服务网格提高可观察性和管理
- en: In this chapter, you will be introduced to the concept of a service mesh and
    see how its capabilities can be used to handle challenges in a system landscape
    of microservices, in areas including security, policy enforcement, resilience,
    and traffic management. A service mesh can also be used to provide observability,
    the capability to visualize how traffic flows between microservices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解服务网格的概念，并了解其功能如何用于处理微服务系统景观中的挑战，包括安全性、策略执行、弹性和流量管理。服务网格还可以用于提供可观察性，即可视化微服务之间流量流向的能力。
- en: A service mesh overlaps partly with the capabilities of Spring Cloud and Kubernetes
    that we learned about earlier in this book. But most of the functionality in a
    service mesh complements Spring Cloud and Kubernetes, as we will see in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格部分重叠了我们在这本书中之前学到的 Spring Cloud 和 Kubernetes 的功能。但正如本章所示，服务网格中的大多数功能都是与 Spring
    Cloud 和 Kubernetes 相补充的。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An introduction to the service mesh concept and Istio, a popular open source
    implementation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务网格概念和 Istio 介绍，Istio 是一个流行的开源实现
- en: Deploying Istio in Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 Istio
- en: Creating, observing, and securing a service mesh
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建、观察和保障服务网格
- en: Ensuring that a service mesh is resilient
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保服务网格具有弹性
- en: Performing zero-downtime updates
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行零停机更新
- en: Testing the microservice landscape using Docker Compose to ensure that the source
    code in the microservices is not locked into either Kubernetes or Istio
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker Compose 测试微服务景观，以确保微服务中的源代码不会被锁定在 Kubernetes 或 Istio 中
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For instructions on how to install the tools used in this book and how to access
    the source code for this book, see:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何安装本书中使用的工具以及如何访问本书源代码的说明，请参阅：
- en: '*Chapter 21*, *Installation Instructions for macOS*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第21章*，*macOS 安装说明*'
- en: '*Chapter 22*, *Installation Instructions for Microsoft Windows with WSL 2 and
    Ubuntu*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第22章*，*使用 WSL 2 和 Ubuntu 的 Microsoft Windows 安装说明*'
- en: The code examples in this chapter all come from the source code in `$BOOK_HOME/Chapter18`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码示例全部来自 `$BOOK_HOME/Chapter18` 的源代码。
- en: If you want to view the changes applied to the source code in this chapter,
    that is, see what it took to create a service mesh using Istio, you can compare
    it with the source code for *Chapter 17*, *Implementing Kubernetes Features to
    Simplify the System Landscape*. You can use your favorite `diff` tool and compare
    the two folders, `$BOOK_HOME/Chapter17` and `$BOOK_HOME/Chapter18`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想查看对本章源代码所做的更改，即查看使用 Istio 创建服务网格所需的步骤，您可以将其与 *第17章*，*实现 Kubernetes 功能以简化系统景观*
    的源代码进行比较。您可以使用您喜欢的 `diff` 工具比较两个文件夹，`$BOOK_HOME/Chapter17` 和 `$BOOK_HOME/Chapter18`。
- en: Introducing service meshes using Istio
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Istio 介绍服务网格
- en: A service mesh is an infrastructure layer that controls and observes the communication
    between services, for example, microservices. The capabilities in a service mesh,
    for example, observability, security, policy enforcement, resilience, and traffic
    management, are implemented by controlling and monitoring all internal communication
    inside the service mesh, that is, between the microservices in the service mesh.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是一个基础设施层，它控制和观察服务之间的通信，例如微服务。服务网格中的功能，例如可观察性、安全性、策略执行、弹性和流量管理，是通过控制和监控服务网格内部的所有内部通信来实现的，即服务网格中的微服务之间的通信。
- en: One of the core components in a service mesh is a lightweight **proxy** component,
    which is injected into each microservice that will be part of the service mesh.
    All traffic in and out of a microservice is configured to go through its proxy
    component. The proxy components are configured at runtime by a **control plane**
    in the service mesh, using APIs exposed by the proxy. The control plane also collects
    telemetry data through these APIs from the proxies to visualize how the traffic
    flows in the service mesh.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格中的一个核心组件是一个轻量级的 **代理** 组件，它被注入到将成为服务网格一部分的每个微服务中。所有进出微服务的流量都被配置为通过其代理组件。代理组件由服务网格中的
    **控制平面** 在运行时通过代理公开的 API 进行配置。控制平面还通过这些 API 从代理收集遥测数据，以可视化服务网格中的流量流向。
- en: 'A service mesh also contains a **data plane**, consisting of the proxy components
    together with separate components for handling external traffic to and from the
    service mesh, known as an **ingress gateway** and an **egress gateway**, respectively.
    The gateway components also communicate with the control plane using a proxy component.
    This is illustrated in the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格还包含一个**数据平面**，由代理组件以及处理服务网格外部流量（分别称为**入口网关**和**出口网关**）的独立组件组成。网关组件也通过代理组件与控制平面通信。这将在以下图中说明：
- en: '![A picture containing text, screenshot, rectangle, diagram  Description automatically
    generated](img/B19825_18_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、矩形、图表的图片，自动生成描述](img/B19825_18_01.png)'
- en: 'Figure 18.1: Service mesh with a control plane and a data plane'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1：具有控制平面和数据平面的服务网格
- en: The first publicly available implementation of a service mesh was the open source
    project **Linkerd**, managed by Buoyant ([https://linkerd.io](https://linkerd.io)),
    having its origins in Twitter’s Finagle project ([http://twitter.github.io/finagle](http://twitter.github.io/finagle)).
    It was launched in 2016 and, one year later, in 2017, IBM, Google, and Lyft launched
    the open source project **Istio** ([https://istio.io](https://istio.io)). Since
    then, several service mesh projects have been launched.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格的第一个公开实施是开源项目**Linkerd**，由Buoyant管理([https://linkerd.io](https://linkerd.io))，其起源可以追溯到Twitter的Finagle项目([http://twitter.github.io/finagle](http://twitter.github.io/finagle))。它于2016年推出，一年后，即2017年，IBM、Google和Lyft推出了开源项目**Istio**([https://istio.io](https://istio.io))。从那时起，已经启动了几个服务网格项目。
- en: 'For an overview of available implementations, see the **service mesh** category
    in CNCF’s cloud-native landscape map: [https://landscape.cncf.io/card-mode?category=service-mesh&grouping=category](https://landscape.cncf.io/card-mode?category=service-mesh&grouping=category).
    In this book, we will use Istio.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解可用的实现概述，请参阅CNCF云原生景观图中的**服务网格**类别：[https://landscape.cncf.io/card-mode?category=service-mesh&grouping=category](https://landscape.cncf.io/card-mode?category=service-mesh&grouping=category)。在本书中，我们将使用Istio。
- en: Introducing Istio
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Istio
- en: Istio can be deployed on a number of Kubernetes distributions and platforms
    using various installation tools as described in [https://istio.io/docs/setup](https://istio.io/docs/setup).
    We will use Istio’s CLI tool, `istioctl`, to install Istio in our minikube-based,
    single-node Kubernetes cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用各种安装工具在多个Kubernetes发行版和平台上部署Istio，具体请参阅[https://istio.io/docs/setup](https://istio.io/docs/setup)。我们将使用Istio的CLI工具`istioctl`在我们的基于minikube的单节点Kubernetes集群中安装Istio。
- en: Istio is, as explained previously, divided into a control plane and a data plane.
    As an operator, we will define the desired state by creating Istio objects in
    the Kubernetes API server, for example, declaring routing rules. The control plane
    will read these objects and send commands to the proxies in the data plane to
    take actions according to the desired state, for example, configuring routing
    rules. The proxies handle the actual communication between the microservices and
    report back telemetry data to the control plane. The telemetry data is used in
    the control plane to visualize what’s going on in the service mesh.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Istio分为控制平面和数据平面。作为操作员，我们将通过在Kubernetes API服务器中创建Istio对象来定义所需状态，例如，声明路由规则。控制平面将读取这些对象，并向数据平面中的代理发送命令以根据所需状态采取行动，例如，配置路由规则。代理处理微服务之间的实际通信，并将遥测数据报告给控制平面。遥测数据用于控制平面以可视化服务网格中的情况。
- en: 'When deploying Istio on Kubernetes, most of its runtime components are deployed
    in a separate Kubernetes namespace, `istio-system`. For the configuration we will
    use in this book, we will find the following Deployments in this Namespace:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Kubernetes上部署Istio时，其大多数运行时组件都部署在单独的Kubernetes命名空间`istio-system`中。对于本书中将要使用的配置，我们将在该命名空间中找到以下部署：
- en: '`istiod`, Istio’s daemon that runs the whole control plane.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`istiod`，运行整个控制平面的Istio守护进程。'
- en: '**Fun fact**: Up until Istio v1.4, the control plane was divided into a set
    of cooperating microservices. Starting with v1.5, they were consolidated into
    a single binary run by `istiod`, simplifying the installation and configuration
    of the control plane at runtime. Also, runtime characteristics such as startup
    time, resource usage, and responsiveness improved. This evolution of Istio’s control
    plane is, to me, an interesting lesson learned when it comes to the use of fine-grained
    microservices.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**有趣的事实**：直到 Istio v1.4，控制平面被分成一组协作的微服务。从 v1.5 开始，它们被合并为一个由 `istiod` 运行的单一二进制文件，简化了运行时控制平面的安装和配置。此外，启动时间、资源使用和响应性等运行时特性也得到了改善。我认为，Istio
    控制平面的这种演变是关于使用细粒度微服务时学到的有趣经验教训。'
- en: '`istio-ingressgateway` and `istio-egressgateway`, Istio’s ingress and egress
    gateway components, are part of the data plane.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`istio-ingressgateway` 和 `istio-egressgateway`，Istio 的入口和出口网关组件，是数据平面的一部分。'
- en: 'A number of integrations with other popular open source projects are supported
    by Istio to bring in extra functionality to the control plane. In this book, we
    will integrate the following components:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio 支持与其他流行的开源项目进行多种集成，以将额外的功能引入控制平面。在这本书中，我们将集成以下组件：
- en: '**Kiali**: Provides observability to the service mesh, visualizing what is
    going on in the mesh. For more information, see [https://www.kiali.io](https://www.kiali.io).'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kiali**：为服务网格提供可观察性，可视化网格中的情况。更多信息，请参阅[https://www.kiali.io](https://www.kiali.io)。'
- en: '**Tracing**: Handles and visualizes distributed tracing information, based
    on either Jaeger or Zipkin. We will use Jaeger. For more information, see [https://www.jaegertracing.io](https://www.jaegertracing.io).'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪**：基于 Jaeger 或 Zipkin 处理和可视化分布式跟踪信息。我们将使用 Jaeger。更多信息，请参阅[https://www.jaegertracing.io](https://www.jaegertracing.io)。'
- en: '**Prometheus**: Performs data ingestion and storage for time-series-based data,
    for example, performance metrics. For more information, see [https://prometheus.io](https://prometheus.io).'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus**：执行基于时间序列数据（例如性能指标）的数据摄取和存储。更多信息，请参阅[https://prometheus.io](https://prometheus.io)。'
- en: '**Grafana**: Visualizes performance metrics and other time-series-related data
    collected by Prometheus. For more information, see [https://grafana.com](https://grafana.com).'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grafana**：可视化 Prometheus 收集的性能指标和其他时间序列相关数据。更多信息，请参阅[https://grafana.com](https://grafana.com)。'
- en: In *Chapter 20*, *Monitoring Microservices*, we will explore performance monitoring
    capabilities using Prometheus and Grafana.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第 20 章“监控微服务”中，我们将探讨使用 Prometheus 和 Grafana 的性能监控能力。
- en: For more information on the integration available in Istio, see [https://istio.io/latest/docs/ops/integrations/](https://istio.io/latest/docs/ops/integrations/).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 Istio 中可用的集成更多信息，请参阅[https://istio.io/latest/docs/ops/integrations/](https://istio.io/latest/docs/ops/integrations/)。
- en: The only Istio components that are deployed outside of the `istio-system` Namespace
    are the proxy components, which are injected into the microservices that are part
    of the service mesh. The proxy component is based on Lyft’s Envoy proxy ([https://www.envoyproxy.io](https://www.envoyproxy.io)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `istio-system` 命名空间之外，唯一部署的 Istio 组件是代理组件，这些组件被注入到服务网格中的微服务中。代理组件基于 Lyft
    的 Envoy 代理([https://www.envoyproxy.io](https://www.envoyproxy.io))。
- en: 'The runtime components in Istio’s control plane and data plane are summarized
    in the following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 控制平面和数据平面中的运行时组件总结如下图所示：
- en: '![A picture containing text, screenshot, rectangle  Description automatically
    generated](img/B19825_18_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、矩形 描述自动生成](img/B19825_18_02.png)'
- en: 'Figure 18.2: Istio runtime components'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2：Istio 运行时组件
- en: Now that we’ve had an introduction, we will look into how these proxy objects
    can be injected into the microservices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了这些内容，我们将探讨如何将这些代理对象注入到微服务中。
- en: Injecting Istio proxies into microservices
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Istio 代理注入到微服务中
- en: The microservices we have deployed in Kubernetes in the previous chapters run
    as a single container in a Kubernetes Pod (refer to the *Introducing Kubernetes
    API objects* section in *Chapter 15*, *Introduction to Kubernetes*, for a recap).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们在 Kubernetes 中部署的微服务作为一个单独的容器在 Kubernetes Pod 中运行（回顾一下，请参考第 15 章“Kubernetes
    简介”中的“介绍 Kubernetes API 对象”部分）。
- en: To make a microservice join an Istio-based service mesh, an Istio proxy is injected
    into each microservice. This is done by adding an extra container to the Pod that
    runs the Istio proxy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使微服务加入基于 Istio 的服务网格，需要在每个微服务中注入 Istio 代理。这是通过向运行 Istio 代理的 Pod 中添加一个额外的容器来实现的。
- en: A container added to a Pod with the aim of supporting the main container, such
    as an Istio proxy, is referred to as a **sidecar**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到 Pod 中，旨在支持主容器（如 Istio 代理）的容器被称为 **sidecar**。
- en: 'The following diagram shows how an Istio proxy has been injected into a sample
    Pod, **Pod A**, as a sidecar:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了 Istio 代理如何作为侧边容器注入到示例 Pod，**Pod A**：
- en: '![A picture containing text, screenshot, number, font  Description automatically
    generated](img/B19825_18_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、数字、字体描述的图片，自动生成](img/B19825_18_03.png)'
- en: 'Figure 18.3: Istio proxy injected into Pod A'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3：注入到 Pod A 的 Istio 代理
- en: The main container in the Pod, **Container A**, is configured to route all its
    traffic through the Istio proxy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 中的主容器，**Container A**，被配置为将所有流量路由通过 Istio 代理。
- en: 'Istio proxies can be injected either automatically when a Pod object is created
    or manually using the `istioctl` tool. To tell Istio to automatically inject an
    Istio proxy into new Pods in a Namespace, the Namespace can be labeled with `istio-injection:
    enabled`. If some Pods in the Namespace are to be excluded from the auto-injection,
    they can be annotated with `sidecar.istio.io/inject: "false"`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'Istio 代理可以在创建 Pod 对象时自动注入，或者使用 `istioctl` 工具手动注入。要告诉 Istio 自动将 Istio 代理注入到命名空间中的新
    Pod，可以将命名空间标记为 `istio-injection: enabled`。如果命名空间中的某些 Pod 要排除在自动注入之外，它们可以注解为 `sidecar.istio.io/inject:
    "false"`。'
- en: 'To inject an Istio proxy manually into the Pods of an existing Deployment object,
    the following command can be used:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Istio 代理手动注入到现有 Deployment 对象的 Pod 中，可以使用以下命令：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This command may, at first glance, appear somewhat daunting, but it is actually
    just three separate commands. The previous command sends its output to the next
    command using pipes, that is, the `|` character. Let’s go through each command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令乍一看可能有些令人畏惧，但实际上它只是三个独立的命令。前面的命令通过管道将输出发送到下一个命令，即 `|` 字符。让我们逐一分析每个命令：
- en: The `kubectl get deployment` command gets the current definition of a Deployment
    named `sample-deployment` from the Kubernetes API server and returns its definition
    in the YAML format.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl get deployment` 命令从 Kubernetes API 服务器获取名为 `sample-deployment` 的 Deployment
    的当前定义，并以 YAML 格式返回其定义。'
- en: The `istioctl kube-inject` command reads the definition from the `kubectl get
    deployment` command and adds an extra container for an Istio proxy in Pods that
    the Deployment handles. The configuration for the existing container in the `Deployment`
    object is updated so that incoming and outgoing traffic goes through the Istio
    proxy.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`istioctl kube-inject` 命令从 `kubectl get deployment` 命令读取定义，并在 Deployment 对象中添加一个用于
    Istio 代理的额外容器。`Deployment` 对象中现有容器的配置被更新，以便进出流量通过 Istio 代理。'
- en: The `istioctl` command returns the new definition of the Deployment object,
    including a container for the Istio proxy.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`istioctl` 命令返回包含 Istio 代理容器的 Deployment 对象的新定义。'
- en: The `kubectl apply` command reads the updated configuration from the `istioctl
    kube-inject` command and applies the updated configuration. An upgrade of the
    Pods belonging to the Deployment will start up in the same way as we have seen
    before (refer to the *Trying out a sample deployment* section in *Chapter 15*,
    *Introduction to Kubernetes*).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl apply` 命令从 `istioctl kube-inject` 命令读取更新的配置，并应用更新后的配置。属于 Deployment
    的 Pod 的升级将以我们之前看到的方式启动（参考第 15 章，*Kubernetes 简介*中的*尝试一个示例部署*部分）。'
- en: 'In this book, we will inject the Istio proxies automatically by applying the
    following definition of the `hands-on` Namespace:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将通过应用以下 `hands-on` 命名空间的定义来自动注入 Istio 代理。
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From the preceding definition, we can see how the Namespace is given the label
    `istio-injection` with the value `enabled`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的定义中，我们可以看到命名空间被赋予 `istio-injection` 标签，其值为 `enabled`。
- en: 'At the time of writing, Istio is not fully capable of acting as a proxy for
    MySQL, MongoDB, and RabbitMQ, so they will be excluded from the service mesh by
    adding the following annotation to their Helm charts’ `values.yaml` file:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Istio 还不能完全胜任作为 MySQL、MongoDB 和 RabbitMQ 的代理，因此它们将通过在它们的 Helm 图表的 `values.yaml`
    文件中添加以下注解而被排除在服务网格之外：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After this introduction to how Istio proxies can be injected into Pods, we can
    now learn about the Istio API objects used in this book.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍如何将 Istio 代理注入到 Pod 之后，我们现在可以学习本书中使用的 Istio API 对象。
- en: Introducing Istio API objects
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Istio API 对象
- en: Istio also comes with a set of Kubernetes **Custom Resource Definitions** (**CRDs**).
    CRDs are used in Kubernetes to extend its API, that is, to add new objects to
    its API. Refer to the *Introducing Kubernetes API objects* section in *Chapter
    15*, *Introduction to Kubernetes*, for a recap of the Kubernetes API.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 还附带了一套 Kubernetes **自定义资源定义**（**CRDs**）。CRDs 用于 Kubernetes 以扩展其 API，即向其
    API 添加新对象。请参阅 *Chapter 15*，*Introduction to Kubernetes* 中的 *Introducing Kubernetes
    API objects* 部分，以回顾 Kubernetes API。
- en: 'In this book, we will use the following Istio objects:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用以下 Istio 对象：
- en: '**Gateway** is used to configure how to handle incoming traffic to, and outgoing
    traffic from, the service mesh. A gateway depends on a virtual service routing
    the incoming traffic to Kubernetes Services. We will use a `gateway` object to
    accept incoming traffic to DNS names ending with `minikube.me`, using HTTPS. The
    Istio gateway objects will replace the **Ingress** objects used in the previous
    chapter. Refer to the *Replacing Kubernetes Ingress Controller with Istio ingress
    gateway* section for details.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gateway** 用于配置如何处理进入和离开服务网格的流量。网关依赖于将入站流量路由到 Kubernetes 服务的虚拟服务。我们将使用 `gateway`
    对象来接受以 `minikube.me` 结尾的 DNS 名称的入站流量，并使用 HTTPS。Istio 网关对象将替换前一章中使用的 **Ingress**
    对象。有关详细信息，请参阅 *Replacing Kubernetes Ingress Controller with Istio ingress gateway*
    部分。'
- en: '**VirtualService** is used to define routing rules in the service mesh. We
    will use virtual Services to describe how to route incoming traffic from an Istio
    gateway to the Kubernetes Services and between Services. We will also use virtual
    Services to inject faults and delays to test the reliability and resilience capabilities
    of the service mesh.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VirtualService** 用于在服务网格中定义路由规则。我们将使用虚拟服务来描述如何将来自 Istio 网关的入站流量路由到 Kubernetes
    服务，以及服务之间的路由。我们还将使用虚拟服务来注入故障和延迟，以测试服务网格的可靠性和弹性能力。'
- en: '**DestinationRule** is used to define policies and rules for traffic that is
    routed (using a virtual service) to a specific service (that is, a destination).
    We will use destination rules to set up encryption policies to encrypt internal
    HTTP traffic and define service subsets that describe available versions of the
    services. We will use service subsets when performing zero-downtime (blue-green)
    deployments from an existing version of a microservice to a new version.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DestinationRule** 用于定义路由到特定服务（即目的地）的流量策略和规则。我们将使用目的地规则来设置加密策略以加密内部 HTTP 流量，并定义描述服务可用版本的子集。在从现有版本的微服务到新版本的零停机（蓝绿）部署时，我们将使用服务子集。'
- en: The division of responsibility between `VirtualService` and `DestinationRule`
    might seem a bit unclear in the beginning. A `VirtualService` object is used to
    configure routing **to** a service and `DestinationRule` is used to configure
    how to handle traffic **for** a selected service. So, first are `VirtualService`
    objects, used to determine where to send a request. Once that is decided, the
    receiving service’s `DestinationRule` is applied.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`VirtualService` 和 `DestinationRule` 之间的责任划分在开始时可能显得有些不清楚。`VirtualService`
    对象用于配置路由**到**服务，而 `DestinationRule` 用于配置如何处理选定服务的流量。因此，首先是 `VirtualService` 对象，用于确定请求发送到哪里。一旦确定，接收服务的
    `DestinationRule` 就会被应用。'
- en: '**PeerAuthentication** is used to control service-to-service authentication
    inside the service mesh. Istio can protect communication between services in a
    service mesh by automatically provisioning **mutual TLS** (**mTLS**) for transport
    authentication, where client services are authenticated by using a client certificate
    that is provided by Istio. To allow Kubernetes to call liveness and readiness
    probes using plain HTTP, we will configure Istio to allow a mix of mTLS and plain
    HTTP, called `PERMISSIVE` mode.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PeerAuthentication** 用于控制服务网格内部的服务间认证。Istio 可以通过自动配置传输认证的**双向 TLS**（**mTLS**）来保护服务网格中服务之间的通信，其中客户端服务通过
    Istio 提供的客户端证书进行认证。为了允许 Kubernetes 使用纯 HTTP 调用存活性和就绪性探针，我们将配置 Istio 允许 mTLS 和纯
    HTTP 的混合模式，称为 `PERMISSIVE` 模式。'
- en: '**RequestAuthentication** is used to authenticate end users based on the credentials
    provided in a request. Istio supports using **JSON Web Tokens** (**JWTs**) in
    general and specifically when used according to the **OpenID Connect** (**OIDC**)
    specification. Istio supports the use of the standard discovery endpoint in OIDC
    to specify where Istio can fetch the public key set **JSON Web Key Set** (JWKS)
    to validate the signatures of the JWTs. We will configure Istio to authenticate
    external requests using the auth server by specifying its JWKS discovery endpoint.
    For a recap, see *Chapter 11*, *Securing Access to APIs*.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RequestAuthentication** 用于根据请求中提供的凭据对最终用户进行身份验证。Istio 支持在一般情况下使用 **JSON Web
    Tokens**（**JWTs**），特别是当按照 **OpenID Connect**（**OIDC**）规范使用时。Istio 支持使用 OIDC 的标准发现端点来指定
    Istio 可以从中获取用于验证 JWT 签名的公钥集 **JSON Web Key Set**（JWKS）。我们将配置 Istio 使用 auth 服务器通过指定其
    JWKS 发现端点来对外部请求进行身份验证。为了回顾，请参阅第 11 章 *保护 API 访问*。'
- en: '**AuthorizationPolicy** is used to provide access control in Istio. We will
    not use Istio’s access control in this book. Instead, we will reuse the existing
    access control implemented in the `product-composite` microservice. We will therefore
    configure an `AuthorizationPolicy` object that allows access to the `product-composite`
    microservice for any authenticated user, that is, for requests that contain a
    valid JWT in the form of an OIDC access token.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AuthorizationPolicy** 用于在 Istio 中提供访问控制。在本书中，我们不会使用 Istio 的访问控制。相反，我们将重用
    `product-composite` 微服务中实现的现有访问控制。因此，我们将配置一个允许任何经过身份验证的用户（即包含有效 JWT 的 OIDC 访问令牌的请求）访问
    `product-composite` 微服务的 `AuthorizationPolicy` 对象。'
- en: For more information on these API objects, see [https://istio.io/v1.17/docs/reference/config/networking/](https://istio.io/v1.17/docs/reference/config/networking/)
    and [https://istio.io/v1.17/docs/reference/config/security/](https://istio.io/v1.17/docs/reference/config/security/).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于这些 API 对象的信息，请参阅 [https://istio.io/v1.17/docs/reference/config/networking/](https://istio.io/v1.17/docs/reference/config/networking/)
    和 [https://istio.io/v1.17/docs/reference/config/security/](https://istio.io/v1.17/docs/reference/config/security/)。
- en: Now that we have introduced the API objects we will use, we will go through
    the changes applied to the microservice landscape arising from the introduction
    of Istio.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了我们将使用的 API 对象，我们将通过 Istio 的引入来查看微服务景观产生的变更。
- en: Simplifying the microservice landscape
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化微服务景观
- en: 'As we have seen in the preceding section, Istio comes with components that
    overlap with components currently used in the microservice landscape in terms
    of functionality:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Istio 包含与微服务景观中当前使用的组件在功能上重叠的组件：
- en: The Istio ingress gateway can act as an edge server, an alternative to a Kubernetes
    Ingress controller
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio 网关可以作为边缘服务器，作为 Kubernetes Ingress 控制器的替代方案
- en: The Jaeger component that comes bundled with Istio can be used for distributed
    tracing instead of the Zipkin server that we deploy together with the microservices
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随 Istio 一起捆绑的 Jaeger 组件可以用作分布式跟踪，而不是我们与微服务一起部署的 Zipkin 服务器
- en: In the following two subsections, we will get an overview of why and how the
    Kubernetes Ingress controller is replaced with an Istio ingress gateway, and our
    Zipkin server is replaced with the Jaeger component that comes integrated with
    Istio.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个小节中，我们将概述为什么以及如何用 Istio 网关替换 Kubernetes Ingress 控制器，以及我们的 Zipkin 服务器被
    Istio 集成的 Jaeger 组件所替换的原因。
- en: Replacing the Kubernetes Ingress controller with an Istio ingress gateway
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用 Istio 网关替换 Kubernetes Ingress 控制器
- en: 'In the previous chapter, we introduced the Kubernetes Ingress controller as
    an edge server (refer to the *Replacing Spring Cloud Gateway* section in *Chapter
    17*, *Implementing Kubernetes Features to Simplify the System Landscape*). An
    Istio ingress gateway has a number of advantages over a Kubernetes Ingress controller:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 Kubernetes Ingress 控制器作为一个边缘服务器（参考第 17 章 *实现 Kubernetes 功能以简化系统景观*
    中的 *替换 Spring Cloud Gateway* 部分）。与 Kubernetes Ingress 控制器相比，Istio ingress 网关具有许多优势：
- en: It can report telemetry data to the control plane for the traffic that flows
    through it
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以向控制平面报告通过它的流量遥测数据
- en: It can be used for more fine-grained routing
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用于更细粒度的路由
- en: It can both authenticate and authorize requests before routing them into the
    service mesh
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以在将请求路由到服务网格之前对请求进行身份验证和授权
- en: To benefit from these advantages, we will replace the Kubernetes Ingress controller
    with the Istio ingress gateway. The Istio ingress gateway is used by creating
    `Gateway` and `VisualService` objects, as described previously in the *Introducing
    Istio API objects* section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用这些优势，我们将用 Istio 入口网关替换 Kubernetes Ingress 控制器。Istio 入口网关是通过创建 `Gateway`
    和 `VisualService` 对象来使用的，如之前在 *介绍 Istio API 对象* 部分中所述。
- en: The definition of the previously used Ingress objects has been removed from
    the `dev-env` and `prod-env` Helm charts in `kubernetes/helm/environments`. The
    definition files for Istio `Gateway` and `VirtualService` objects will be explained
    in the *Creating the service mesh* section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `kubernetes/helm/environments` 中的 `dev-env` 和 `prod-env` Helm 图表中删除了之前使用的
    Ingress 对象的定义。Istio `Gateway` 和 `VirtualService` 对象的定义文件将在 *创建服务网格* 部分中解释。
- en: The Istio ingress gateway is reached using a different IP address from the IP
    address used to access the Kubernetes Ingress controller, so we also need to update
    the IP address mapped to the hostname, `minikube.me`, which we use when running
    tests. This is handled in the *Setting up access to Istio services* section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与访问 Kubernetes Ingress 控制器所用的不同 IP 地址来访问 Istio 入口网关，因此我们还需要更新映射到主机名 `minikube.me`
    的 IP 地址，这是我们运行测试时使用的。这已在 *设置访问 Istio 服务* 部分中处理。
- en: Replacing the Zipkin server with Istio’s Jaeger component
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用 Istio 的 Jaeger 组件替换 Zipkin 服务器
- en: As mentioned in the *Introducing Istio* section, Istio comes with built-in support
    for distributed tracing using Jaeger. Using Jaeger, we can offload and simplify
    the microservice landscape in Kubernetes by removing the Zipkin server we introduced
    in *Chapter 14*, *Understanding Distributed Tracing*. We will also change the
    way trace and span IDs are propagated between the microservices, from using the
    default W3C trace context headers to using OpenZipkin’s `B3` headers. See the
    *Introducing distributed tracing with Micrometer Tracing and Zipkin* section in
    *Chapter 14*, *Understanding Distributed Tracing,* for more information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *介绍 Istio* 部分所述，Istio 内置了对使用 Jaeger 的分布式跟踪的支持。使用 Jaeger，我们可以通过移除我们在 *第 14
    章*，*理解分布式跟踪* 中引入的 Zipkin 服务器，来减轻并简化 Kubernetes 中的微服务景观。我们还将更改微服务之间传播跟踪和跨度 ID 的方式，从使用默认的
    W3C 跟踪上下文头信息更改为使用 OpenZipkin 的 `B3` 头信息。有关更多信息，请参阅 *第 14 章*，*理解分布式跟踪* 中的 *使用 Micrometer
    跟踪和 Zipkin 介绍分布式跟踪* 部分。
- en: 'The following changes have been applied to the source code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下更改已应用于源代码：
- en: 'The following dependencies have been replaced in all microservice build files,
    `build.gradle`:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有微服务构建文件 `build.gradle` 中已替换以下依赖项：
- en: '[PRE3]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The dependencies have been replaced with the following:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 依赖项已替换为以下内容：
- en: '[PRE4]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `management.zipkin.tracing.endpoint` property in the common configuration
    file `config-repo/application.yml` points to the Jaeger component in Istio. It
    has the hostname `jaeger-collector.istio-system`.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共配置文件 `config-repo/application.yml` 中的 `management.zipkin.tracing.endpoint`
    属性指向 Istio 中的 Jaeger 组件。它具有主机名 `jaeger-collector.istio-system`。
- en: The definition of the Zipkin server in the three Docker Compose files, `docker-compose.yml`,
    `docker-compose-partitions.yml`, and `docker-compose-kafka.yml`, has been retained
    to be able to use distributed tracing outside of Kubernetes and Istio, but the
    Zipkin server has been given the same hostname as the Jaeger component in Istio,
    `jaeger-collector.istio-system`.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在三个 Docker Compose 文件 `docker-compose.yml`，`docker-compose-partitions.yml` 和
    `docker-compose-kafka.yml` 中保留了对 Zipkin 服务器的定义，以便能够在 Kubernetes 和 Istio 之外使用分布式跟踪，但
    Zipkin 服务器已被赋予与 Istio 中 Jaeger 组件相同的名称，即 `jaeger-collector.istio-system`。
- en: The Helm chart for the Zipkin server has been removed.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已删除 Zipkin 服务器的 Helm 图表。
- en: Jaeger will be installed in the *Deploying Istio in a Kubernetes cluster* section
    coming up.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 将在接下来的 *在 Kubernetes 集群中部署 Istio* 部分中安装。
- en: With these simplifications of the microservice landscape explained, we are ready
    to deploy Istio in the Kubernetes cluster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了微服务景观的简化之后，我们已准备好在 Kubernetes 集群中部署 Istio。
- en: Deploying Istio in a Kubernetes cluster
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中部署 Istio
- en: In this section, we will learn how to deploy Istio in a Kubernetes cluster and
    how to access the Istio services in it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 Kubernetes 集群中部署 Istio 以及如何访问其中的 Istio 服务。
- en: We will use Istio’s CLI tool, `istioctl`, to install Istio using a `demo` configuration
    of Istio that is suitable for testing Istio in a development environment, that
    is, with most features enabled but configured for minimalistic resource usage.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Istio 的 CLI 工具 `istioctl`，使用适合在开发环境中测试 Istio 的 `demo` 配置安装 Istio，即，启用大多数功能但配置为最小化资源使用。
- en: This configuration is unsuitable for production usage and for performance testing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置不适合生产使用和性能测试。
- en: For other installation options, see [https://istio.io/latest/docs/setup/install/](https://istio.io/latest/docs/setup/install/).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他安装选项，请参阅 [https://istio.io/latest/docs/setup/install/](https://istio.io/latest/docs/setup/install/)。
- en: 'To deploy Istio, perform the following steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署 Istio，请执行以下步骤：
- en: 'Ensure that your Minikube instance from the previous chapter is up and running
    with the following command:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保上一章中的 Minikube 实例已通过以下命令启动并运行：
- en: '[PRE5]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Expect a response along the lines of the following, provided it is up and running:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期以下类似的响应，前提是它已启动并运行：
- en: '![A screen shot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_04.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度中等](img/B19825_18_04.png)'
- en: 'Figure 18.4: Minikube status OK'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.4：Minikube 状态正常
- en: 'Run a precheck to verify that the Kubernetes cluster is ready for Istio to
    be installed in it:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行预检查以验证 Kubernetes 集群是否已准备好在其中安装 Istio：
- en: '[PRE6]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Expect a response like the following:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期以下类似的响应：
- en: '![A screen shot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_05.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度中等](img/B19825_18_05.png)'
- en: 'Figure 18.5: Istio precheck OK'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5：Istio 预检查正常
- en: 'Install Istio using the `demo` profile with the following command:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令使用 `demo` 配置文件安装 Istio：
- en: '[PRE7]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The command parameters do the following:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 命令参数执行以下操作：
- en: The `accessLog` parameters are used to enable the Istio proxies to log requests
    that are processed. Once Pods are up and running with Istio proxies installed,
    the access logs can be inspected with the command `kubectl logs <MY-POD> -c istio-proxy`.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accessLog` 参数用于启用 Istio 代理记录处理请求。一旦 Pods 配置了 Istio 代理并启动运行，可以使用命令 `kubectl
    logs <MY-POD> -c istio-proxy` 检查访问日志。'
- en: The `PILOT_JWT_PUB_KEY_REFRESH_INTERVAL` parameter configures Istio’s daemon,
    `istiod`, to refresh the fetched JWKS public keys every 15 seconds. The usage
    of this parameter will be explained in the *Deploying v1 and v2 versions of the
    microservices with routing to the v1 version* section.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PILOT_JWT_PUB_KEY_REFRESH_INTERVAL` 参数配置 Istio 的守护进程 `istiod` 每 15 秒刷新获取的
    JWKS 公钥。此参数的用法将在 *部署 v1 和 v2 版本的微服务并路由到 v1 版本* 部分中解释。'
- en: 'The configuration file `kubernetes/istio-tracing.yml` enables the creation
    of trace spans used for distributed tracing. It also configures Istio to create
    trace spans for all requests. It looks like this:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置文件 `kubernetes/istio-tracing.yml` 启用创建用于分布式跟踪的跟踪跨度。它还配置 Istio 为所有请求创建跟踪跨度。它看起来像这样：
- en: '[PRE8]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Wait for the Deployment objects and their Pods to be available with the following
    command:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令等待 Deployment 对象及其 Pods 可用：
- en: '[PRE9]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, install the extra components described in the *Introducing Istio* section
    – Kiali, Jaeger, Prometheus, and Grafana – with these commands:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令安装 *介绍 Istio* 部分中描述的额外组件——Kiali、Jaeger、Prometheus 和 Grafana：
- en: '[PRE10]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If any of these commands fail, try rerunning the failing command. Errors can
    occur due to timing issues, which can be resolved by running commands again. Specifically,
    the installation of Kiali can result in error messages starting with `unable to
    recognize`. Rerunning the command makes these error messages go away.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些命令中的任何一个失败，请尝试重新运行失败的命令。错误可能由于时间问题引起，可以通过再次运行命令来解决。特别是，Kiali 的安装可能会导致以 `unable
    to recognize` 开头的错误消息。重新运行命令会使这些错误消息消失。
- en: 'Wait a second time for the extra components to be available with the following
    command:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次等待以下命令以使额外组件可用：
- en: '[PRE11]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, run the following command to see what we got installed:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行以下命令以查看我们安装了什么：
- en: '[PRE12]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Expect an output similar to this:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期以下类似的输出：
- en: '![A screen shot of a black screen  Description automatically generated with
    low confidence](img/B19825_18_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![黑色屏幕的屏幕截图  描述自动生成，置信度低](img/B19825_18_06.png)'
- en: 'Figure 18.6: Deployments in the Istio Namespace'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.6：Istio 命名空间中的部署
- en: Istio is now deployed in Kubernetes, but before we move on and create the service
    mesh, we need to learn a bit about how to access the Istio services in a Minikube
    environment.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Istio现在已在Kubernetes中部署，但在我们继续创建服务网格之前，我们需要了解如何在Minikube环境中访问Istio服务。
- en: Setting up access to Istio services
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置访问Istio服务的权限
- en: The `demo` configuration used in the previous section to install Istio comes
    with a few connectivity-related issues that we need to resolve. The Istio ingress
    gateway is configured as a load-balanced Kubernetes service; that is, its type
    is `LoadBalancer`. To be able to access the gateway, we need to run a load balancer
    in front of the Kubernetes cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个部分中用于安装Istio的`demo`配置包含一些与连接相关的问题，我们需要解决。Istio入口网关被配置为一个负载均衡的Kubernetes服务；也就是说，其类型是`LoadBalancer`。为了能够访问网关，我们需要在Kubernetes集群前面运行一个负载均衡器。
- en: Minikube contains a command that can be used to simulate a local load balancer,
    `minikube tunnel`. This command assigns an external IP address to each load-balanced
    Kubernetes service, including the Istio ingress gateway. The hostname, `minikube.me`,
    that we use in our tests needs to be translated into the external IP address of
    the Istio ingress gateway. To simplify access to the web UIs of components like
    Kiali and Jaeger, we will also add hostnames dedicated to these services, for
    example, `kiali.minikube.me`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube包含一个可以用来模拟本地负载均衡器的命令，`minikube tunnel`。此命令为每个负载均衡的Kubernetes服务分配一个外部IP地址，包括Istio入口网关。我们在测试中使用的`minikube.me`主机名需要被转换成Istio入口网关的外部IP地址。为了简化对Kiali和Jaeger等组件的Web
    UI的访问，我们还将添加专门用于这些服务的域名，例如，`kiali.minikube.me`。
- en: We will also register a hostname to the external `health` endpoint as described
    in the *Observing the service mesh* section. Finally, a few hostnames for services
    installed and used in subsequent chapters will also be registered so we don’t
    need to add new hostnames in the following chapters. The services that we will
    install in the next chapters are Kibana, Elasticsearch, and a mail server.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将注册一个主机名到外部`健康`端点，如*观察服务网格*部分所述。最后，还会注册后续章节中安装和使用的服务的几个主机名，这样我们就不需要在以下章节中添加新的主机名。下一章我们将安装的服务包括Kibana、Elasticsearch和一个邮件服务器。
- en: To enable external access using these hostnames to the Istio services, a Helm
    chart has been created; see `kubernetes/helm/environments/istio-system`. The chart
    contains a `Gateway`, `VirtualService`, and `DestinationRule` object for each
    Istio component. To protect requests to these hostnames from eavesdropping, only
    HTTPS requests are allowed. The `cert-manager`, which was introduced in the previous
    chapter, is used by the chart to automatically provision a TLS certificate for
    the hostnames and store it in a Secret named `hands-on-certificate`. All gateway
    objects are configured to use this Secret in their configuration of the HTTPS
    protocol. All definition files can be found in the Helm charts `templates` folder.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些主机名通过外部访问Istio服务，已创建了一个Helm图表；请参阅`kubernetes/helm/environments/istio-system`。该图表包含每个Istio组件的`Gateway`、`VirtualService`和`DestinationRule`对象。为了保护对这些主机名的请求免受窃听，只允许HTTPS请求。在前一章中引入的`cert-manager`由图表用于自动为这些主机名提供TLS证书并将其存储在名为`hands-on-certificate`的Secret中。所有网关对象都配置为在HTTPS协议的配置中使用此Secret。所有定义文件都可以在Helm图表的`templates`文件夹中找到。
- en: The use of these API objects will be described in more detail in the *Creating
    the service mesh* and *Protecting external endpoints with HTTPS and certificates*
    sections below.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这些API对象的使用将在下面的*创建服务网格*和*使用HTTPS和证书保护外部端点*部分中更详细地描述。
- en: 'Run the following command to apply the Helm chart:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以应用Helm图表：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will result in the gateway being able to route requests for the following
    hostnames to the corresponding Kubernetes Service:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使网关能够将以下主机名的请求路由到相应的Kubernetes服务：
- en: '`kiali.minikube.me` requests are routed to `kiali:20001`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kiali.minikube.me`请求被路由到`kiali:20001`'
- en: '`tracing.minikube.me` requests are routed to `tracing:80`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tracing.minikube.me`请求被路由到`tracing:80`'
- en: '`prometheus.minikube.me` requests are routed to `prometheus:9000`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus.minikube.me`请求被路由到`prometheus:9000`'
- en: '`grafana.minikube.me` requests are routed to `grafana:3000`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grafana.minikube.me`请求被路由到`grafana:3000`'
- en: 'To verify that the `certificate` and `secret` objects have been created, run
    the following commands:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证`证书`和`密钥`对象是否已创建，请运行以下命令：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Expect an output like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_07.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的截图，中等置信度自动生成描述](img/B19825_18_07.png)'
- en: 'Figure 18.7: The cert-manager has delivered both a TLS Secret and a certificate'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.7：cert-manager 已交付 TLS Secret 和证书
- en: 'The following diagram summarizes how the components can be accessed:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了如何访问组件：
- en: '![A picture containing text, screenshot, font, line  Description automatically
    generated](img/B19825_18_08.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、字体、行描述的图片，自动生成](img/B19825_18_08.png)'
- en: 'Figure 18.8: Hostnames to be used for accessing components through the Minikube
    tunnel'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.8：通过 Minikube 隧道访问组件时要使用的主机名
- en: 'Perform the following steps to set up the Minikube tunnel and register the
    hostnames:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以设置 Minikube 隧道并注册主机名：
- en: 'Run the following command in a separate terminal window (the command locks
    the terminal window when the tunnel is up and running):'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端窗口中运行以下命令（当隧道启动并运行时，该命令会锁定终端窗口）：
- en: '[PRE15]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that this command requires that your user has `sudo` privileges and that
    you enter your password during startup. It can take a couple of seconds before
    the command asks for the password, so it is easy to miss!
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，此命令要求你的用户具有 `sudo` 权限，并且在启动时输入你的密码。在命令请求密码之前可能需要几秒钟，所以很容易错过！
- en: Once the tunnel is up and running, it will list the `istio-ingressgateway` as
    one of the services it exposes (the only one in our case).
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦隧道启动并运行，它将列出 `istio-ingressgateway` 作为其公开的服务之一（在我们的案例中是唯一的）。
- en: 'Configure the hostnames to be resolved to the IP address of the Istio ingress
    gateway. Start by getting the IP address exposed by the `minikube tunnel` command
    for the Istio ingress gateway and save it in an environment variable named `INGRESS_IP`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置主机名，使其解析到 Istio 入口网关的 IP 地址。首先，获取 `minikube tunnel` 命令为 Istio 入口网关暴露的 IP 地址，并将其保存到名为
    `INGRESS_IP` 的环境变量中：
- en: '[PRE16]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `echo` command will print an IP address. Since we use Minikube’s Docker
    driver, it will always be `127.0.0.1`.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`echo` 命令将打印一个 IP 地址。由于我们使用 Minikube 的 Docker 驱动程序，它始终是 `127.0.0.1`。'
- en: 'Update `/etc/hosts` so that all `minikube.me` hostnames will use the IP address
    of the Istio ingress gateway:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 `/etc/hosts`，以便所有 `minikube.me` 主机名都将使用 Istio 入口网关的 IP 地址：
- en: '[PRE17]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On Windows, we also need to update the Windows `hosts` file:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Windows 上，我们还需要更新 Windows 的 `hosts` 文件：
- en: In Windows, open a `PowerShell` terminal.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Windows 中，打开一个 `PowerShell` 终端。
- en: 'Open the Windows `hosts` file in Visual Code Studio with the command:'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在 Visual Code Studio 中打开 Windows 的 `hosts` 文件：
- en: '[PRE18]'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Add a similar line to the Window `hosts` file:'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将类似的行添加到 Windows 的 `hosts` 文件中：
- en: '[PRE19]'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When you try to save it, you will get an error regarding `Insufficient permissions`.
    Click on the **Retry as Admin...** button to update the `hosts` file as an administrator.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你尝试保存时，你会得到一个关于 `Insufficient permissions` 的错误。点击 **Retry as Admin...** 按钮以管理员身份更新
    `hosts` 文件。
- en: 'Verify the update:'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证更新：
- en: '[PRE20]'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By default, the `/etc/hosts` file is overwritten by the content in the Windows
    `hosts` file when WSL is restarted. Restarting WSL takes a long time as it also
    restarts Docker. Restarting Docker, in turn, results in the Minikube instance
    being stopped, so it needs to be restarted manually. To avoid this slow and tedious
    restart process, we simply updated both files.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当 WSL 重新启动时，`/etc/hosts` 文件会被 Windows 的 `hosts` 文件内容覆盖。重启 WSL 需要很长时间，因为它也会重启
    Docker。重启 Docker 又会导致 Minikube 实例停止，因此需要手动重启。为了避免这个缓慢且繁琐的重启过程，我们简单地更新了这两个文件。
- en: 'Remove the line in `/etc/hosts` where `minikube.me` points to only the IP address
    of the Minikube instance (`127.0.0.1`). Verify that `/etc/hosts` only contains
    one line that translates `minikube.me` and that it points to the IP address of
    the Istio ingress gateway, `127.0.0.1`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除 `/etc/hosts` 中指向 Minikube 实例 IP 地址（`127.0.0.1`）的 `minikube.me` 行。验证 `/etc/hosts`
    只包含一行，将 `minikube.me` 转换为指向 Istio 入口网关的 IP 地址，即 `127.0.0.1`：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_09.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的截图，中等置信度自动生成描述](img/B19825_18_09.png)'
- en: 'Figure 18.9: /etc/hosts file updated'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.9：已更新的 `/etc/hosts` 文件
- en: 'Verify that Kiali, Jaeger, Grafana, and Prometheus can be reached through the
    tunnel with the following commands:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证 Kiali、Jaeger、Grafana 和 Prometheus 是否可以通过隧道访问：
- en: '[PRE21]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Each command should return `200` (`OK`). If the request sent to Kiali doesn’t
    return `200`, it often means that its internal initialization is not complete.
    Wait a minute and try again in that case.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 每个命令都应该返回 `200`（`OK`）。如果发送给 Kiali 的请求没有返回 `200`，通常意味着其内部初始化尚未完成。在这种情况下，请等待一分钟，然后重试。
- en: The `minikube tunnel` command will stop running if, for example, your computer
    or the Minikube instance are paused or restarted. It needs to be restarted manually
    in these cases. So, if you fail to call APIs on any of the `minikube.me` hostnames,
    always check whether the Minikube tunnel is running and restart it if required.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果例如您的计算机或 Minikube 实例被暂停或重启，`minikube tunnel` 命令将停止运行。在这些情况下，需要手动重启。因此，如果您在
    `minikube.me` 任何主机名上无法调用 API，请始终检查 Minikube 隧道是否正在运行，并在需要时重启它。
- en: With the Minikube tunnel in place, we are now ready to create the service mesh.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Minikube 隧道就绪后，我们现在可以创建服务网格了。
- en: Creating the service mesh
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建服务网格
- en: With Istio deployed, we are ready to create the service mesh. The steps required
    to create the service mesh are basically the same as those we used in *Chapter
    17*, *Implementing Kubernetes Features to Simplify the System Landscape* (refer
    to the *Testing with Kubernetes ConfigMaps, Secrets, Ingress, and cert-manager*
    section). Let’s first see what additions have been made to the Helm templates
    to set up the service mesh before we run the commands to create the service mesh.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了 Istio 后，我们就可以创建服务网格了。创建服务网格所需的步骤基本上与我们用于 *第 17 章*，*通过实现 Kubernetes 功能简化系统景观*（参考
    *使用 Kubernetes ConfigMaps、Secrets、Ingress 和 cert-manager 进行测试* 部分）中使用的步骤相同。在运行创建服务网格的命令之前，让我们先看看
    Helm 模板中为设置服务网格所做的添加。
- en: Source code changes
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源代码更改
- en: To be able to run the microservices in a service mesh managed by Istio, the
    `dev-env` Helm chart brings in two new named templates from the `common` chart,
    `_istio_base.yaml` and `_istio_dr_mutual_tls.yaml`. Let’s go through them one
    by one.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够在由 Istio 管理的服务网格中运行微服务，`dev-env` Helm 图表从 `common` 图表引入了两个新的命名模板 `_istio_base.yaml`
    和 `_istio_dr_mutual_tls.yaml`。让我们逐一查看它们。
- en: Content in the _istio_base.yaml template
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`_istio_base.yaml` 模板中的内容'
- en: '`_istio_base.yaml` defines a number of Kubernetes manifests that will be used
    by both environment charts, `dev-env` and `prod-env`. First, it defines three
    Istio-specific security-related manifests:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`_istio_base.yaml` 定义了将被 `dev-env` 和 `prod-env` 环境图表共同使用的多个 Kubernetes 清单。首先，它定义了三个与
    Istio 相关的安全相关清单：'
- en: An `AuthorizationPolicy` manifest named `product-composite-require-jwt`
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为 `product-composite-require-jwt` 的 `AuthorizationPolicy` 清单
- en: A `PeerAuthentication` manifest named `default`
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为 `default` 的 `PeerAuthentication` 清单
- en: A `RequestAuthentication` manifest named `product-composite-request-authentication`
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为 `product-composite-request-authentication` 的 `RequestAuthentication` 清单
- en: These three manifests will be explained in the *Securing a service mesh* section
    below.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 *Securing a service mesh* 部分将解释这三个清单。
- en: The remaining four manifests will be discussed here. They are two pairs of `Gateway`
    and `VirtualService` manifests that are used to configure access to, and routing
    from, the hostnames `minikube.me` and `health.minikube.me`. Gateway objects will
    be used to define how to receive external traffic and `VirtualService` objects
    are used to describe how to route the incoming traffic inside the service mesh.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的四个清单将在这里讨论。它们是两对用于配置访问和从主机名 `minikube.me` 和 `health.minikube.me` 路由的 `Gateway`
    和 `VirtualService` 清单。将使用 Gateway 对象来定义如何接收外部流量，而 `VirtualService` 对象用于描述如何在服务网格内部路由传入的流量。
- en: 'The `Gateway` manifest for controlling access to `minikube.me` looks like this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 控制访问 `minikube.me` 的 `Gateway` 清单如下所示：
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here are some explanations for the source code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于源代码的解释：
- en: The gateway is named `hands-on-gw`; this name is used by the virtual services
    underneath.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网关命名为 `hands-on-gw`；这个名称被下方的虚拟服务所使用。
- en: The `selector` field specifies that the gateway object will be handled by the
    default Istio ingress gateway, named `ingressgateway`.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector` 字段指定网关对象将由默认的 Istio 入口网关，名为 `ingressgateway`，处理。'
- en: The `hosts` and `port` fields specify that the gateway will handle incoming
    requests for the `minikube.me` hostname using HTTPS over port `443`.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hosts` 和 `port` 字段指定网关将使用 HTTPS 通过端口 `443` 处理对 `minikube.me` 主机名的传入请求。'
- en: The `tls` field specifies that the Istio ingress gateway can find the certificate
    and private key used for HTTPS communication in a TLS Secret named `hands-on-certificate`.
    Refer to the *Protecting external endpoints with HTTPS and certificates* section
    below for details on how these certificate files are created. The `SIMPLE` mode
    denotes that normal TLS semantics will be applied.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tls`字段指定Istio入口网关可以在名为`hands-on-certificate`的TLS Secret中找到用于HTTPS通信的证书和私钥。有关如何创建这些证书文件的详细信息，请参阅下面的`Protecting
    external endpoints with HTTPS and certificates`部分。`SIMPLE`模式表示将应用正常的TLS语义。'
- en: 'The `VirtualService` manifest for routing requests sent to `minikube.me` appears
    as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 路由发送到`minikube.me`的请求的`VirtualService`清单如下所示：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Explanations for the preceding manifest are as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前面清单的解释如下：
- en: The `gateways` and `hosts` fields specify that the virtual service will route
    requests that are sent to the `minikube.me` hostname through the `hands-on-gw`
    gateway.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gateways`和`hosts`字段指定虚拟服务将通过`hands-on-gw`网关路由发送到`minikube.me`主机名的请求。'
- en: 'Under the `http` element follows an array of `match` and `route` blocks that
    specify how URL paths will be forwarded to the associated Kubernetes service.
    In the manifest above, only the first pair of `match` and `route` elements is
    shown. They map requests sent to `minikube.me` using the path `/oauth2` to the
    `auth-server` service. This mapping should be familiar from how we specified routing
    rules in both Spring Cloud Gateway and Ingress objects in the previous chapters.
    The remaining pairs of `match` and `route` elements configure the same routing
    rules as we have seen for Spring Cloud Gateway and Ingress objects:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`http`元素之后是一个`match`和`route`块的数组，指定了URL路径将如何转发到相关的Kubernetes服务。在上面的清单中，只显示了第一对`match`和`route`元素。它们将使用路径`/oauth2`发送到`auth-server`服务的请求映射。这种映射应该与我们在前几章中指定路由规则的方式相似。其余的`match`和`route`元素配置了与我们在Spring
    Cloud Gateway和Ingress对象中看到的相同的路由规则：
- en: '`/login → auth-server`'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/login → auth-server`'
- en: '`/error → auth-server`'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/error → auth-server`'
- en: '`/product-composite → product-composite`'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/product-composite → product-composite`'
- en: '`/openapi → product-composite`'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/openapi → product-composite`'
- en: '`/webjars → product-composite`'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/webjars → product-composite`'
- en: For details, see `kubernetes/helm/common/templates/_istio_base.yaml`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅`kubernetes/helm/common/templates/_istio_base.yaml`。
- en: In the preceding source code, the destination host is specified using its short
    name, in other words, `product-composite`. This works, since the example is based
    on Kubernetes definitions from the same Namespace, `hands-on`. If that is not
    the case, it is recommended in the Istio documentation to use the host’s **fully
    qualified domain name** (**FQDN**) instead. In this case, it is `product-composite.hands-on.svc.cluster.local`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的源代码中，使用其短名称指定了目标主机，换句话说，是`product-composite`。这有效，因为示例基于同一命名空间`hands-on`的Kubernetes定义。如果不是这种情况，建议在Istio文档中使用主机的**完全限定域名**（**FQDN**）代替。在这种情况下，它是`product-composite.hands-on.svc.cluster.local`。
- en: Content in the _istio_dr_mutual_tls.yaml template
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`_istio_dr_mutual_tls.yaml`模板中的内容'
- en: '`_istio_dr_mutual_tls.yaml` defines a template for specifying a number of `DestinationRule`
    objects. It is used to specify that mTLS should be used when routing a request
    to its corresponding service. It can also be used optionally to specify `subsets`,
    something that we will use in the `prod-env` chart in the *Performing zero-downtime
    updates* section below. The template looks like this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`_istio_dr_mutual_tls.yaml`定义了一个模板，用于指定多个`DestinationRule`对象。它用于指定在路由请求到其对应服务时应使用mTLS。它还可以选择性地用于指定`subsets`，我们将在下面的`Performing
    zero-downtime updates`部分中的`prod-env`图表中使用它。模板看起来如下所示：'
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here are some comments about the preceding template:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些关于前面模板的注释：
- en: The `range` directive loops over the elements defined in the `destinationRules`
    variable
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`range`指令遍历在`destinationRules`变量中定义的元素'
- en: The `host` field in the `spec` part of the manifest is used to specify the name
    of the Kubernetes Service that this `DestinationRule` applies to
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清单的`spec`部分的`host`字段用于指定此`DestinationRule`应用到的Kubernetes Service的名称
- en: A `subsets` section is only defined if a corresponding element is found in the
    current element, `$dr`, in the `destinationRules` list
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在`destinationRules`列表中的当前元素`$dr`中找到相应的元素，则仅定义`subsets`部分
- en: A `trafficPolicy` is always used to require mTLS
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是使用`trafficPolicy`来强制要求mTLS
- en: 'The template is used in the `dev-end` Helm chart by specifying the `destinationRules`
    variable in the `values.yaml` file as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 模板在 `dev-end` Helm 图表中使用，通过在 `values.yaml` 文件中指定 `destinationRules` 变量如下：
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The files can be found at `kubernetes/helm/common/templates/_istio_dr_mutual_tls.yaml`
    and `kubernetes/helm/environments/dev-env/values.yaml`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 文件位于 `kubernetes/helm/common/templates/_istio_dr_mutual_tls.yaml` 和 `kubernetes/helm/environments/dev-env/values.yaml`。
- en: With these changes in the source code in place, we are now ready to create the
    service mesh.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在源代码中进行这些更改后，我们现在可以创建服务网格。
- en: Running commands to create the service mesh
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行命令以创建服务网格
- en: 'Create the service mesh by running the following commands:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令创建服务网格：
- en: 'Build Docker images from the source code with the following commands:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从源代码构建 Docker 镜像：
- en: '[PRE26]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `eval $(minikube docker-env -u)` command ensures that the `./gradlew build`
    command uses the host’s Docker engine and not the Docker engine in the Minikube
    instance. The `build` command uses Docker to run test containers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`eval $(minikube docker-env -u)` 命令确保 `./gradlew build` 命令使用主机的 Docker 引擎，而不是
    Minikube 实例中的 Docker 引擎。`build` 命令使用 Docker 运行测试容器。'
- en: 'Recreate the `hands-on` Namespace, and set it as the default Namespace:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建 `hands-on` 命名空间，并将其设置为默认命名空间：
- en: '[PRE27]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that the `hands-on-namespace.yml` file creates the `hands-on` Namespace
    labeled with `istio-injection: enabled`. This means that Pods created in this
    Namespace will get `istio-proxy` containers injected as sidecars automatically.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '注意，`hands-on-namespace.yml` 文件创建了一个带有 `istio-injection: enabled` 标签的 `hands-on`
    命名空间。这意味着在此命名空间中创建的 Pod 将自动注入 `istio-proxy` 容器作为边车。'
- en: 'Resolve the Helm chart dependencies with the following commands:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令解决 Helm 图表依赖项：
- en: 'First, we update the dependencies in the `components` folder:'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们更新 `components` 文件夹中的依赖项：
- en: '[PRE28]'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we update the dependencies in the `environments` folder:'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们更新 `environments` 文件夹中的依赖项：
- en: '[PRE29]'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Deploy the system landscape using Helm and wait for all Deployments to complete:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Helm 部署系统架构，并等待所有 Deployment 完成：
- en: '[PRE30]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once the Deployment is complete, verify that we have two containers in each
    of the microservice Pods:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 Deployment 完成，验证每个微服务 Pod 中都有两个容器：
- en: '[PRE31]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Expect a response along the lines of the following:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期响应将与以下内容类似：
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B19825_18_10.png)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机程序的截图  描述由中等置信度自动生成](img/B19825_18_10.png)'
- en: 'Figure 18.10: Pods up and running'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 18.10：Pod 启动并运行
- en: Note that the Pods that run our microservices report two containers per Pod;
    that is, they have the Istio proxy injected as a sidecar!
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，运行我们的微服务的 Pod 报告每个 Pod 有两个容器；也就是说，它们注入了 Istio 代理作为边车！
- en: 'Run the usual tests with the following command:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行常规测试：
- en: '[PRE32]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The default values for the `test-em-all.bash` script have been updated from
    previous chapters to accommodate Kubernetes running in Minikube.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`test-em-all.bash` 脚本的默认值已从前面的章节更新，以适应在 Minikube 中运行的 Kubernetes。'
- en: 'Expect the output to be similar to what we have seen in previous chapters:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将与我们在前面的章节中看到的内容相似：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_11.png)'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机的截图  描述由中等置信度自动生成](img/B19825_18_11.png)'
- en: 'Figure 18.11: Tests running successfully'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 18.11：测试运行成功
- en: Before we start to try out Istio and its various components, let’s see how we
    can log the propagation of trace and span IDs using the B3 headers mentioned in
    the *Replacing the Zipkin server with Istio’s Jaeger component* section above.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始尝试 Istio 及其各种组件之前，让我们看看如何使用上面 *用 Istio 的 Jaeger 组件替换 Zipkin 服务器* 部分中提到的
    B3 标头来记录跟踪和跨度 ID 的传播。
- en: Logging propagation of trace and span IDs
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪和跨度 ID 的日志传播
- en: 'We can see the trace and span IDs in the outgoing requests from the `product-composite`
    microservice, as we did in the *Sending a successful API request* section in *Chapter
    14*, *Understanding Distributed Tracing*. Since we now run the microservices in
    Kubernetes, we need to change the log configuration in a ConfigMap and then delete
    the running Pod to make it affect the microservice:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 `product-composite` 微服务的出站请求中看到跟踪和跨度 ID，就像我们在 *第 14 章，理解分布式跟踪* 中的 *发送成功的
    API 请求* 部分所做的那样。由于我们现在在 Kubernetes 中运行微服务，我们需要更改 ConfigMap 中的日志配置，然后删除正在运行的 Pod
    以使其影响微服务：
- en: 'Edit the ConfigMap with the following command:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令编辑 ConfigMap：
- en: '[PRE33]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Look for the following lines:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查找以下行：
- en: '[PRE34]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Uncomment the two last of these lines and exit the editor.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取消注释这两行最后的注释并退出编辑器。
- en: 'Restart the `product-composite` microservice by deleting its Pod with this
    command:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令重启 `product-composite` 微服务，通过删除其 Pod：
- en: '[PRE35]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Print the log output to a terminal window with the following command:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将日志输出打印到终端窗口：
- en: '[PRE36]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Acquire an access token and make a request using the access token:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个访问令牌并使用访问令牌进行请求：
- en: '[PRE37]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Verify that the command returns the HTTP status code for success, `200`.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证命令返回的 HTTP 状态码是否为成功，即 `200`。
- en: 'In the log output, lines like the following should be seen:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在日志输出中，应该可以看到如下类似的行：
- en: '[PRE38]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the sample log output above, we can see the standard `B3` headers like `X-B3-TraceId`
    and `X-B3-SpanId`.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上面的示例日志输出中，我们可以看到标准的 `B3` 头部，如 `X-B3-TraceId` 和 `X-B3-SpanId`。
- en: Revert to not logging trace and span IDs by adding back the comments in the
    ConfigMap and restart the microservice by deleting its Pod.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在 ConfigMap 中重新添加注释并删除其 Pod 来重启微服务，以停止记录跟踪和跨度 ID。
- en: With the service mesh up and running, let’s see how we can observe what’s going
    on in it using Kiali!
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务网格运行起来之后，让我们看看如何使用 Kiali 来观察其中的情况！
- en: Observing the service mesh
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察服务网格
- en: In this section, we will use Kiali together with Jaeger to observe what’s going
    on in the service mesh.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Kiali 与 Jaeger 一起观察服务网格中的情况。
- en: Before we do that, we need to understand how to get rid of some noise created
    by the health checks performed by Kubernetes’ liveness and readiness probes. In
    the previous chapters, they used the same port as the API requests. This means
    that Istio will collect metrics for the usage of both health checks and requests
    sent to the API. This will cause the graphs shown by Kiali to become unnecessarily
    cluttered. Kiali can filter out traffic that we are not interested in, but a simpler
    solution is to use a different port for the health checks.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这样做之前，我们需要了解如何消除 Kubernetes 的存活和就绪探针执行的健康检查产生的噪音。在之前的章节中，它们使用了与 API 请求相同的端口。这意味着
    Istio 将收集健康检查和发送到 API 的请求的使用情况指标。这将导致 Kiali 显示的图表变得不必要地杂乱。Kiali 可以过滤掉我们不感兴趣的流量，但一个更简单的解决方案是为健康检查使用不同的端口。
- en: 'Microservices can be configured to use a separate port for requests sent to
    the actuator endpoints, for example, health checks sent to the `/actuator/health`
    endpoint. The following line has been added to the common configuration file for
    all microservices, `config-repo/application.yml`:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务可以被配置为使用单独的端口来处理发送到 actuator 端点的请求，例如发送到 `/actuator/health` 端点的健康检查。以下行已被添加到所有微服务的通用配置文件
    `config-repo/application.yml` 中：
- en: '[PRE39]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will make all microservices use port `4004` to expose the `health` endpoints.
    The `values.yaml` file in the `common` Helm chart has been updated to use port
    `4004` in the default liveness and readiness probes. See `kubernetes/helm/common/values.yaml`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使所有微服务使用端口 `4004` 来暴露 `health` 端点。`common` Helm 图表的 `values.yaml` 文件已被更新，以在默认的存活和就绪探针中使用端口
    `4004`。请参阅 `kubernetes/helm/common/values.yaml`。
- en: The `product-composite` microservice exposes its management port not only to
    the Kubernetes probes but also externally for health checks, for example, performed
    by `test-em-all.bash`. This is done through Istio’s ingress gateway, and therefore
    port `4004` is added to the `product-composite` microservice Deployment and Service
    manifests. See the `ports` and `service.ports` definitions in `kubernetes/helm/components/product-composite/values.yaml`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`product-composite` 微服务不仅将其管理端口暴露给 Kubernetes 探针，还对外部健康检查进行暴露，例如由 `test-em-all.bash`
    执行的健康检查。这是通过 Istio 的入口网关完成的，因此端口 `4004` 被添加到 `product-composite` 微服务的 Deployment
    和 Service 清单中。请参阅 `kubernetes/helm/components/product-composite/values.yaml` 中的
    `ports` 和 `service.ports` 定义。'
- en: 'Spring Cloud Gateway (which is retained so we can run tests in Docker Compose)
    will continue to use the same port for requests to the API and the `health` endpoint.
    In the `config-repo/gateway.yml` configuration file, the `management` port is
    reverted to the port used for the API:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Gateway（保留以在 Docker Compose 中运行测试）将继续使用相同的端口来处理 API 请求和 `health`
    端点的请求。在 `config-repo/gateway.yml` 配置文件中，`management` 端口被恢复为 API 所使用的端口：
- en: '[PRE40]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: To simplify external access to the health check exposed by the `product-composite`
    microservice, a route is configured for the `health.minikube.me` hostname to the
    `management` port on the `product-composite` microservice. Refer to the explanation
    of the `_istio_base.yaml` template above.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化对由 `product-composite` 微服务暴露的健康检查的外部访问，为 `health.minikube.me` 主机名配置了一个路由到
    `product-composite` 微服务的 `management` 端口。请参阅上面 `_istio_base.yaml` 模板的说明。
- en: With the requests sent to the `health` endpoint out of the way, we can start
    to send some requests through the service mesh.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在发送到 `health` 端点的请求处理完毕后，我们可以开始通过服务网格发送一些请求。
- en: We will start a low-volume load test using `siege`, which we learned about in
    *Chapter 16*, *Deploying Our Microservices to Kubernetes*. After that, we will
    go through some of the most important parts of Kiali to see how it can be used
    to observe a service mesh in a web browser. We will also see how Jaeger is used
    for distributed tracing.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `siege` 启动低流量负载测试，我们在第 16 章 *将我们的微服务部署到 Kubernetes* 中了解到它。之后，我们将查看 Kiali
    的几个重要部分，以了解它如何用于在 Web 浏览器中观察服务网格。我们还将了解 Jaeger 如何用于分布式跟踪。
- en: Since the certificate we use is self-signed, web browsers will not rely on it
    automatically. Most web browsers let you visit the web page if you assure them
    that you understand the security risks. If the web browser refuses, opening a
    private window helps in some cases.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的证书是自签名的，因此网络浏览器不会自动依赖它。大多数网络浏览器允许您访问网页，如果您向它们保证您理解安全风险。如果网络浏览器拒绝，打开私密窗口在某些情况下可能会有帮助。
- en: Specifically, regarding Chrome, if it does not let you visit the web page, saying
    **Your connection is not private**, you can click the **Advanced** button and
    then click on the link **Proceed to … (unsafe)**.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是关于 Chrome，如果它不允许您访问网页，并说 **您的连接不安全**，您可以点击 **高级** 按钮，然后点击链接 **继续访问 …（不安全）**。
- en: 'Start the test client with the following commands:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启动测试客户端：
- en: '[PRE41]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The first command will get an OAuth 2.0/OIDC access token that will be used
    in the next command, where `siege` is used to submit one HTTP request per second
    to the `product-composite` API.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令将获取一个 OAuth 2.0/OIDC 访问令牌，该令牌将在下一个命令中使用，其中 `siege` 用于每秒提交一个 HTTP 请求到 `product-composite`
    API。
- en: 'Expect output from the `siege` command as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 预期 `siege` 命令的输出如下：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_12.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由中等置信度自动生成](img/B19825_18_12.png)'
- en: 'Figure 18.12: System landscape under siege'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.12：系统在攻击下
- en: 'Use a web browser of your choice that accepts self-signed certificates and
    proceed with the following steps:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您选择的接受自签名证书的 Web 浏览器，并按照以下步骤操作：
- en: 'Open Kiali’s web UI using the [https://kiali.minikube.me](https://kiali.minikube.me)
    URL. By default, you will be logged in as an anonymous user. Expect a web page
    similar to the following:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[https://kiali.minikube.me](https://kiali.minikube.me) URL 打开 Kiali 的 Web
    UI。默认情况下，您将以匿名用户身份登录。预期网页将与以下类似：
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_18_13.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/B19825_18_13.png)'
- en: 'Figure 18.13: Kiali web UI'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.13：Kiali Web UI
- en: Click on the **Overview** tab, if it is not already active.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 **概览** 选项卡尚未激活，请点击它。
- en: Click on the menu in the box named **hands-on** (three vertical dots in the
    top-right corner) and select **Graph**. Expect a graph to be shown, representing
    the current traffic flowing through the service mesh.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击名为 **动手实践**（右上角三个垂直点）的菜单，并选择 **图形**。预期会显示一个图形，表示当前通过服务网格流动的流量。
- en: Click on the **Display** button, and deselect all options except for **Response
    Time,** **Median**, and **Traffic Animation**.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **显示** 按钮，取消选择除 **响应时间**、**中位数** 和 **流量动画** 之外的所有选项。
- en: In the **Hide…** field, specify `name = jaeger` to avoid cluttering the view
    with traces sent to Jaeger.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **隐藏…** 字段中，指定 `name = jaeger` 以避免将发送到 Jaeger 的跟踪视图搞乱。
- en: 'Kiali now displays a graph representing requests that are currently sent through
    the service mesh, where active requests are represented by small moving circles
    along the arrows, as follows:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kiali 现在显示一个图形，表示当前通过服务网格发送的请求，其中活动请求由箭头旁的小移动圆圈表示，如下所示：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_14.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由中等置信度自动生成](img/B19825_18_14.png)'
- en: 'Figure 18.14: Kiali graph showing the hands-on Namespace'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.14：显示动手实践命名空间的 Kiali 图表
- en: The traffic from **unknown** to the **auth-server** represents calls to the
    authorization server to get the JWKS public keys.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**未知**到**auth-server**的流量代表了对授权服务器进行调用以获取JWKS公钥的请求。
- en: This gives a pretty good initial overview of what’s going on in the service
    mesh!
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这为服务网格中正在发生的事情提供了一个相当好的初步概述！
- en: 'Let’s now look at some distributed tracing using Jaeger. Open the web UI using
    the [https://tracing.minikube.me](https://tracing.minikube.me) URL. Click on the
    **Service** dropdown in the menu to the left and select the **istio-ingressgateway.istio-system**service.
    Click on the **Find Trace** button and you should see a result like this:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看看使用Jaeger的一些分布式追踪。使用[https://tracing.minikube.me](https://tracing.minikube.me)
    URL打开Web UI。在左侧菜单中点击**服务**下拉菜单并选择**istio-ingressgateway.istio-system**服务。点击**查找追踪**按钮，你应该会看到一个如下所示的结果：
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_18_15.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由自动生成](img/B19825_18_15.png)'
- en: 'Figure 18.15: Distributed traces visualized by Jaeger'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.15：Jaeger可视化的分布式追踪
- en: 'Click on one of the traces that is reported to contain **23 Spans** to examine
    it. Expect a web page such as the following:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击报告包含**23个跨度**的其中一个追踪来检查它。预期会看到一个如下所示的网页：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_16.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由中等置信度自动生成](img/B19825_18_16.png)'
- en: 'Figure 18.16: View of a full trace call tree in Jaeger'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.16：Jaeger中完整追踪调用树的视图
- en: This is basically the same tracing information as Zipkin made available in *Chapter
    14*, *Understanding Distributed Tracing*. Note that we can see trace information
    from both the Istio proxies and the microservices themselves. The spans reported
    by Istio proxies are suffixed with the Kubernetes Namespace, that is,`.istio-system`
    and `.hands-on`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上与Zipkin在*第14章*，*理解分布式追踪*中提供的追踪信息相同。请注意，我们可以看到来自Istio代理和微服务本身的追踪信息。由Istio代理报告的跨度后面跟随着Kubernetes命名空间，即，`.istio-system`和`.hands-on`。
- en: There is much more to explore, but this is enough by way of an introduction.
    Feel free to explore the web UI in Kiali and Jaeger on your own.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多内容可以探索，但作为介绍已经足够了。请随意探索Kiali和Jaeger上的Web UI。
- en: Be aware that the access token acquired for the test client, `siege`, is only
    valid for an hour. If the traffic drops unexpectedly, check the output from `siege`;
    if it reports `4XX` instead of `200`, it’s time to renew the access token!
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为测试客户端`siege`获取的访问令牌仅在一小时内有效。如果流量意外下降，检查`siege`的输出；如果它报告`4XX`而不是`200`，那么是时候更新访问令牌了！
- en: Let’s move on and learn how Istio can be used to improve security in the service
    mesh!
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续学习如何使用Istio在服务网格中提高安全性！
- en: Securing a service mesh
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护服务网格
- en: 'In this section, we will learn how to use Istio to improve the security of
    a service mesh. We will cover the following topics:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用Istio来提高服务网格的安全性。我们将涵盖以下主题：
- en: How to protect external endpoints with HTTPS and certificates
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用HTTPS和证书保护外部端点
- en: How to require that external requests are authenticated using OAuth 2.0/OIDC
    access tokens
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何要求外部请求使用OAuth 2.0/OIDC访问令牌进行认证
- en: How to protect internal communication using **mutual authentication** (**mTLS**)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用**相互认证**（**mTLS**）保护内部通信
- en: Let’s now understand each of these in the following sections.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在以下章节中了解这些内容的每一个。
- en: Protecting external endpoints with HTTPS and certificates
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HTTPS和证书保护外部端点
- en: From the *Setting up access to Istio services* and *Content in the _istio_base.yaml
    template* sections, we learned that the gateway objects use a TLS certificate
    stored in a Secret named `hands-on-certificate` for its HTTPS endpoints.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 从*设置对Istio服务的访问*和*`_istio_base.yaml`模板中的内容*部分，我们了解到网关对象使用存储在名为`hands-on-certificate`的Secret中的TLS证书为其HTTPS端点。
- en: 'The Secret is created by the cert-manager based on the configuration in the
    `istio-system` Helm chart. The chart’s template, `selfsigned-issuer.yaml`, is
    used to define an internal self-signed CA and has the following content:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 该Secret是由cert-manager根据`istio-system` Helm图中的配置创建的。图表的模板`selfsigned-issuer.yaml`用于定义一个内部自签名CA，并具有以下内容：
- en: '[PRE42]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'From the preceding manifests, we can see the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的清单中，我们可以看到以下内容：
- en: A self-signed issuer named `selfsigned-issuer`.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为`selfsigned-issuer`的自签名发行者。
- en: This issuer is used to create a self-signed certificate, named `ca-cert`.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此发行者用于创建一个名为`ca-cert`的自签名证书。
- en: The certificate is given the common name `hands-on-ca`.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证书被赋予通配名`hands-on-ca`。
- en: Finally, a self-signed CA, `ca-issuer`, is defined using the certificate, `ca-cert`,
    as its root certificate. This CA will be used to issue the certificate used by
    the gateway objects.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，使用证书 `ca-cert` 作为其根证书定义了一个自签名 CA，`ca-issuer`。此 CA 将用于签发网关对象使用的证书。
- en: 'The chart’s template, `hands-on-certificate.yaml`, defines this certificate
    as:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的模板 `hands-on-certificate.yaml` 将此证书定义为：
- en: '[PRE43]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'From this manifest, we can learn that:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 从此清单中，我们可以了解到：
- en: The certificate is named `hands-on-certificate`
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证书命名为 `hands-on-certificate`
- en: Its common name is set to `minikube.me`
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其通用名称设置为 `minikube.me`
- en: It specifies a few optional extra details about its `subject` (left out for
    clarity)
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它指定了一些关于其 `subject` 的可选额外详细信息（为了清晰起见省略）
- en: All other hostnames are declared as **Subject Alternative Names** in the certificate
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有其他主机名都在证书中声明为 **Subject Alternative Names**
- en: It will use the issuer named `ca-issuer` declared above
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将使用上面声明的名为 `ca-issuer` 的发行者
- en: The `cert-manager` will store the TLS certificate in a Secret named `hands-on-certificate`
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cert-manager` 将 TLS 证书存储在名为 `hands-on-certificate` 的 Secret 中'
- en: When the `istio-system` Helm chart was installed, these templates were used
    to create the corresponding API objects in Kubernetes. This triggered the `cert-manager`
    to create the certificates and Secrets.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `istio-system` Helm 图表安装时，这些模板被用来在 Kubernetes 中创建相应的 API 对象。这触发了 `cert-manager`
    创建证书和 Secrets。
- en: The template files can be found in the `kubernetes/helm/environments/istio-system/templates`
    folder.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 模板文件可以在 `kubernetes/helm/environments/istio-system/templates` 文件夹中找到。
- en: 'To verify that it is these certificates that are used by the Istio ingress
    gateway, we can run the following command:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证 Istio 入口网关使用的是这些证书，我们可以运行以下命令：
- en: '[PRE44]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Expect the following output:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 预期以下输出：
- en: '![A screen shot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_17.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由中等置信度自动生成](img/B19825_18_17.png)'
- en: 'Figure 18.17: Inspecting the certificate for minikube.me'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.17：检查 minikube.me 的证书
- en: The output shows that the certificate is issued for the common name `minikube.se`
    and that it is issued by our own CA, using its root certificate with the common
    name `hands-on-ca`.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，该证书是为通用名称 `minikube.se` 签发的，并且是由我们自己的 CA 签发的，使用其通用名称为 `hands-on-ca` 的根证书。
- en: As mentioned in *Chapter 17*, *Implementing Kubernetes Features to Simplify
    the System Landscape* (refer to the *Automating certificate provisioning* section),
    this self-signed CA needs to be replaced for production use cases with, for example,
    Let’s Encrypt or another CA that the `cert-manager` can use to provision trusted
    certificates.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *第 17 章*，*实现 Kubernetes 功能以简化系统景观*（参见 *自动化证书供应* 部分）所述，此自签名 CA 需要替换为生产用例中的
    Let’s Encrypt 或其他 `cert-manager` 可以用来提供信任证书的 CA。
- en: With the certificate configuration verified, let’s move on to see how the Istio
    ingress gateway can protect microservices from unauthenticated requests.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证证书配置后，让我们继续看看 Istio 入口网关如何保护微服务免受未认证请求。
- en: Authenticating external requests using OAuth 2.0/OIDC access tokens
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 OAuth 2.0/OIDC 访问令牌验证外部请求
- en: An Istio ingress gateway can require and validate JWT-based OAuth 2.0/OIDC access
    tokens, in other words, protect the microservices in the service mesh from external
    unauthenticated requests. For a recap on JWT, OAuth 2.0, and OIDC, refer to *Chapter
    11*, *Securing Access to APIs* (see the *Protecting APIs using OAuth 2.0 and OpenID
    Connect* section). Istio can also be configured to perform authorization but,
    as mentioned in the *Introducing Istio API objects* section, we will not use it.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 入口网关可以要求并验证基于 JWT 的 OAuth 2.0/OIDC 访问令牌，换句话说，保护服务网格中的微服务免受外部未认证请求。关于 JWT、OAuth
    2.0 和 OIDC 的概述，请参阅 *第 11 章*，*保护 API 访问*（见 *使用 OAuth 2.0 和 OpenID Connect 保护 API*
    部分）。Istio 还可以配置为执行授权，但如 *介绍 Istio API 对象* 部分所述，我们不会使用它。
- en: 'This is configured in the `common` Helm chart’s template, `_istio_base.yaml`.
    The two manifests look like this:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在 `common` Helm 图表的模板 `_istio_base.yaml` 中配置的。两个清单看起来如下：
- en: '[PRE45]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'From the manifests, we can see the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 从清单中，我们可以看到以下内容：
- en: The `RequestAuthentication` named `product-composite-request-authentication`
    requires a valid JWT-encoded access token for requests sent to the `product-composite`
    service.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名为 `product-composite-request-authentication` 的 `RequestAuthentication` 要求对发送到
    `product-composite` 服务的请求使用有效的 JWT 编码访问令牌。
- en: 'It selects services that it performs request authentication for based on a
    label selector, `app.kubernetes.io/name: product-composite`.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '它根据标签选择器选择它执行请求认证的服务，`app.kubernetes.io/name: product-composite`。'
- en: It allows tokens from the issuer, `http://auth-server`.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许来自提供者`http://auth-server`的令牌。
- en: It will use the `http://auth-server.hands-on.svc.cluster.local/oauth2/jwks`
    URL to fetch a JWKS. The key set is used to validate the digital signature of
    the access tokens.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将使用`http://auth-server.hands-on.svc.cluster.local/oauth2/jwks` URL来获取JWKS。密钥集用于验证访问令牌的数字签名。
- en: It will forward the access token to the underlying services, in our case, the
    `product-composite` microservice.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将访问令牌转发到底层服务，在我们的例子中，是`product-composite`微服务。
- en: The `AuthorizationPolicy` named `product-composite-require-jwt` is configured
    to allow all requests to the `product-composite` service; it will not apply any
    authorization rules.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名为`product-composite-require-jwt`的`AuthorizationPolicy`配置为允许对`product-composite`服务的所有请求；它不会应用任何授权规则。
- en: It can be a bit hard to understand whether Istio’s `RequestAuthentication` is
    validating the access tokens or whether it is only the `product-composite` service
    that is performing the validation. One way to ensure that Istio is doing its job
    is to change the configuration of `RequestAuthentication` so that it always rejects
    access tokens.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解Istio的`RequestAuthentication`是否正在验证访问令牌，或者只是`product-composite`服务正在执行验证可能有点困难。确保Istio正在执行其工作的一种方法是将`RequestAuthentication`的配置更改为始终拒绝访问令牌。
- en: 'To verify that `RequestAuthentication` is in action, apply the following commands:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证`RequestAuthentication`正在生效，应用以下命令：
- en: 'Make a normal request:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送一个正常请求：
- en: '[PRE46]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Verify that it returns an HTTP response status code `200` (`OK`).
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证它返回的HTTP响应状态码为`200`（`OK`）。
- en: 'Edit the `RequestAuthentication` object and temporarily change the `issuer`,
    for example, to `http://auth-server-x`:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`RequestAuthentication`对象，并暂时更改`issuer`，例如，更改为`http://auth-server-x`：
- en: '[PRE47]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Verify the change:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证更改：
- en: '[PRE48]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Verify that the `issuer` has been updated, in my case, to `http://auth-server-x`.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证`issuer`是否已更新，在我的情况下，更新为`http://auth-server-x`。
- en: 'Make the request again. It should fail with the HTTP response status code `401`
    (Unauthorized) and the error message `Jwt issuer is not configured`:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次发送请求。它应该以HTTP响应状态码`401`（未授权）和错误消息`Jwt issuer is not configured`失败：
- en: '[PRE49]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Since it takes a few seconds for Istio to propagate the change, the new name
    of the `issuer`, you might need to repeat the command a couple of times before
    it fails.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于Istio传播更改需要几秒钟，您可能需要重复命令几次，直到它失败。
- en: This proves that Istio is validating the access tokens!
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这证明了Istio正在验证访问令牌！
- en: 'Revert the changed name of the issuer to `http://auth-server`:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`issuer`的更改名称恢复为`http://auth-server`：
- en: '[PRE50]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Verify that the request works again. First, wait a few seconds for the change
    to be propagated. Then, run the command:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证请求是否再次工作。首先，等待几秒钟以使更改传播。然后，运行以下命令：
- en: '[PRE51]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**Suggested additional exercise**: Try out the Auth0 OIDC provider, as described
    in *Chapter 11*, *Securing Access to APIs* (refer to the *Testing with an external
    OpenID Connect provider* section). Add your Auth0 provider to `jwt-authentication-policy.yml`.
    In my case, it appears as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**建议的附加练习**：尝试Auth0 OIDC提供者，如第11章中所述的*保护API访问*（参考*使用外部OpenID Connect提供者进行测试*部分）。将您的Auth0提供者添加到`jwt-authentication-policy.yml`中。在我的情况下，它如下所示：'
- en: '[PRE52]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, let’s move on to the last security mechanism that we will cover in Istio:
    the automatic protection of internal communication in the service mesh using mutual
    authentication, mTLS.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讨论在Istio中将要覆盖的最后一个安全机制：使用双向认证，mTLS自动保护服务网格中的内部通信。
- en: Protecting internal communication using mutual authentication (mTLS)
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用双向认证（mTLS）保护内部通信
- en: In this section, we will learn how Istio can be configured to automatically
    protect internal communication within the service mesh using **mTLS**. When using
    mutual authentication, not only does the service prove its identity by exposing
    a certificate but the clients also prove their identity to the service by exposing
    a client-side certificate. This provides a higher level of security compared to
    normal TLS/HTTPS usage, where only the identity of the service is proven. Setting
    up and maintaining mutual authentication, that is, provisioning new certificates
    and rotating outdated certificates for the clients, is known to be complex and
    is therefore seldom used. Istio fully automates the provisioning and rotation
    of certificates for the mutual authentication used for internal communication
    inside the service mesh.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何配置Istio以自动使用**mTLS**保护服务网格内部的通信。在使用双向认证时，不仅服务通过暴露证书来证明其身份，客户端也通过暴露客户端证书来向服务证明其身份。这比仅证明服务身份的正常TLS/HTTPS使用提供了更高的安全性。设置和维护双向认证，即为客户分配新证书和轮换过期的证书，被认为是复杂的，因此很少使用。Istio完全自动化了服务网格内部通信所使用的双向认证证书的分配和轮换。
- en: This makes it much easier to use mutual authentication compared to setting it
    up manually.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动设置相比，这使得使用双向认证变得容易得多。
- en: So, why should we use mutual authentication? Isn’t it sufficient to protect
    external APIs with HTTPS and OAuth 2.0/OIDC access tokens?
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们应该使用双向认证呢？使用HTTPS和OAuth 2.0/OIDC访问令牌保护外部API不是足够了吗？
- en: As long as the attacks come through the external API, it might be sufficient.
    But what if a Pod inside the Kubernetes cluster becomes compromised? For example,
    if an attacker gains control over a Pod, they can start listening to traffic between
    other Pods in the Kubernetes cluster. If the internal communication is sent as
    plaintext, it will be very easy for the attacker to gain access to sensitive information
    sent between Pods in the cluster. To minimize the damage caused by such an intrusion,
    mutual authentication can be used to prevent an attacker from eavesdropping on
    internal network traffic.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 只要攻击是通过外部API发起的，这可能就足够了。但如果Kubernetes集群内的Pod被攻陷呢？例如，如果攻击者控制了一个Pod，他们可以开始监听Kubernetes集群中其他Pod之间的流量。如果内部通信以明文形式发送，攻击者将很容易获取集群中Pod之间发送的敏感信息。为了最小化此类入侵造成的损害，可以使用双向认证来防止攻击者窃听内部网络流量。
- en: To enable the use of mutual authentication managed by Istio, Istio needs to
    be configured both on the server side, using a policy called `PeerAuthentication`,
    and on the client side, using a `DestinationRule`.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用由Istio管理的双向认证的使用，Istio需要在服务器端配置，使用名为`PeerAuthentication`的策略，并在客户端使用`DestinationRule`。
- en: 'The policy is configured in the `common` Helm chart’s template, `_istio_base.yaml`.
    The manifest looks like this:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略配置在`common` Helm图表的模板`_istio_base.yaml`中。其配置文件看起来像这样：
- en: '[PRE53]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As mentioned in the *Introducing Istio API objects* section, the `PeerAuthentication`
    policy is configured to allow both mTLS and plain HTTP requests using the `PERMISSIVE`
    mode. This enables Kubernetes to call liveness and readiness probes using plain
    HTTP.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*介绍Istio API对象*部分中提到的，`PeerAuthentication`策略配置为使用`PERMISSIVE`模式允许同时进行mTLS和平文HTTP请求。这使Kubernetes能够使用平文HTTP调用存活和就绪探测。
- en: 'We have also already met the `DestinationRule` manifests in the *Content in
    the _istio_dr_mutual_tls.yaml template* section. The central part of the `DestinationRule`
    manifests for requiring mTLS looks like this:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*`_istio_dr_mutual_tls.yaml`模板中的内容*部分遇到了`DestinationRule`配置文件。要求mTLS的`DestinationRule`配置文件的核心部分如下所示：
- en: '[PRE54]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'To verify that the internal communication is protected by mTLS, perform the
    following steps:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证内部通信是否由mTLS保护，请执行以下步骤：
- en: Ensure that the load tests started in the preceding *Observing the service mesh*
    section are still running and report `200` (`OK`).
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保在前面的*观察服务网格*部分启动的负载测试仍在运行并报告`200`（`OK`）。
- en: Go to the Kiali graph in a web browser ([https://kiali.minikube.me](https://kiali.minikube.me)).
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页浏览器中访问Kiali图([https://kiali.minikube.me](https://kiali.minikube.me))。
- en: 'Click on the **Display** button and enable the **Security** label. The graph
    will show a padlock on all communication links that are protected by Istio’s automated
    mutual authentication, as follows:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**显示**按钮并启用**安全**标签。图表将显示所有由Istio的自动双向认证保护的数据通信链接上的锁状图标，如下所示：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_18.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图，描述自动生成，置信度中等](img/B19825_18_18.png)'
- en: 'Figure 18.18: Inspecting mTLS settings in Kiali'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.18：在 Kiali 中检查 mTLS 设置
- en: Expect a padlock on all links.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 预期所有链接都将显示一个锁形图标。
- en: Calls to RabbitMQ, MySQL, and MongoDB are not handled by Istio proxies, and
    therefore require manual configuration to be protected using TLS, if required.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 对 RabbitMQ、MySQL 和 MongoDB 的调用不由 Istio 代理处理，因此如果需要，需要手动配置以使用 TLS 进行保护。
- en: With this, we have seen all three security mechanisms in Istio in action, and
    it is now time to see how Istio can help us to verify that a service mesh is resilient.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经看到了 Istio 中所有三种安全机制的实际应用，现在是时候看看 Istio 如何帮助我们验证服务网格的弹性了。
- en: Ensuring that a service mesh is resilient
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保服务网格具有弹性
- en: In this section, we will learn how to use Istio to ensure that a service mesh
    is resilient, that is, that it can handle temporary faults in a service mesh.
    Istio comes with mechanisms similar to what the Spring Framework offers in terms
    of timeouts, retries, and a type of circuit breaker called **outlier detection**
    to handle temporary faults.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 Istio 确保服务网格具有弹性，也就是说，它能够处理服务网格中的临时故障。Istio 提供了类似于 Spring 框架提供的超时、重试以及一种称为**异常检测**的断路器类型来处理临时故障。
- en: When it comes to deciding whether language-native mechanisms should be used
    to handle temporary faults or whether this should be delegated to a service mesh
    such as Istio, I tend to favor using language-native mechanisms, as in the examples
    in *Chapter 13*, *Improving Resilience Using Resilience4j*. In many cases, it
    is important to keep the logic for handling errors, for example, handling fallback
    alternatives for a circuit breaker, together with other business logic for a microservice.
    Keeping the logic for handling temporary faults in the source code also makes
    it easier to test it using, for example, JUnit and test containers, something
    that becomes much more complex if handling temporary faults is delegated to a
    service mesh like Istio.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到决定是否应该使用语言原生机制来处理临时故障，或者是否应该将其委托给像 Istio 这样的服务网格时，我倾向于更喜欢使用语言原生机制，就像 *第
    13 章*，*使用 Resilience4j 提高弹性* 中的例子一样。在许多情况下，保持错误处理的逻辑，例如，处理断路器的回退选项，与其他微服务的业务逻辑一起是很重要的。将处理临时故障的逻辑保留在源代码中也使得使用
    JUnit 和测试容器等工具进行测试变得更加容易，如果将处理临时故障的任务委托给像 Istio 这样的服务网格，这个过程会变得更加复杂。
- en: There are cases when the corresponding mechanisms in Istio could be of great
    help. For example, if a microservice is deployed and it is determined that it
    can’t handle temporary faults that occur in production from time to time, then
    it can be very convenient to add a timeout or a retry mechanism using Istio instead
    of waiting for a new release of the microservice with corresponding error handling
    features put in place.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，Istio 中的相应机制可能非常有帮助。例如，如果一个微服务已经部署，并且确定它无法处理生产中偶尔发生的临时故障，那么使用 Istio 添加超时或重试机制可能非常方便，而不是等待带有相应错误处理功能的微服务新版本发布。
- en: Another capability in the area of resilience that comes with Istio is the capability
    to inject faults and delays into an existing service mesh. Why might we want to
    do that?
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 在弹性领域提供的另一个功能是向现有的服务网格中注入故障和延迟的能力。我们为什么想要这样做呢？
- en: Injecting faults and delays in a controlled way is very useful for verifying
    that the resilient capabilities in the microservices work as expected! We will
    try them out in this section, verifying that the retry, timeout, and circuit breaker
    in the `product-composite` microservice work as expected.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 以受控的方式注入故障和延迟对于验证微服务的弹性功能是否按预期工作非常有用！我们将在本节中尝试这些功能，验证 `product-composite` 微服务中的重试、超时和断路器是否按预期工作。
- en: In *Chapter 13*, *Improving Resilience Using Resilience4j* (refer to the *Adding
    programmable delays and random errors* section), we added support for injecting
    faults and delays into the microservices source code. That source code should
    preferably be replaced by using Istio’s capabilities for injecting faults and
    delays at runtime, as demonstrated in the following subsections.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 13 章*，*使用 Resilience4j 提高弹性*（参考 *添加可编程延迟和随机错误* 部分）中，我们添加了对将故障和延迟注入微服务源代码的支持。最好用
    Istio 的功能来替换这部分源代码，以便在运行时注入故障和延迟，如下面的子节所示。
- en: We will begin by injecting faults to see whether the retry mechanisms in the
    `product-composite` microservice work as expected. After that, we will delay the
    responses from the `product` service and verify that the circuit breaker handles
    the delay as expected.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先注入故障以查看 `product-composite` 微服务中的重试机制是否按预期工作。之后，我们将延迟 `product` 服务的响应并验证断路器是否按预期处理延迟。
- en: Testing resilience by injecting faults
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过注入故障测试弹性
- en: Let’s make the `product` service throw random errors and verify that the microservice
    landscape handles this correctly. We expect the retry mechanism in the `product-composite`
    microservice to kick in and retry the request until it succeeds or its limit of
    the maximum numbers of retries is reached. This will ensure that a short-lived
    fault does not affect the end user more than the delay introduced by the retry
    attempts. Refer to the *Adding a retry mechanism* section in *Chapter 13*, *Improving
    Resilience Using Resilience4j*, for a recap on the retry mechanism in the `product-composite`
    microservice.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让 `product` 服务抛出随机错误，并验证微服务景观是否正确处理这种情况。我们预计 `product-composite` 微服务中的重试机制将启动并重试请求，直到成功或达到最大重试次数限制。这将确保短暂的故障不会比重试尝试引入的延迟更影响最终用户。请参阅
    *第 13 章*，*使用 Resilience4j 提高弹性* 中的 *添加重试机制* 部分，以回顾 `product-composite` 微服务中的重试机制。
- en: 'Faults can be injected using a virtual service like `kubernetes/resilience-tests/product-virtual-service-with-faults.yml`.
    This appears as follows:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似 `kubernetes/resilience-tests/product-virtual-service-with-faults.yml`
    的虚拟服务注入故障。如下所示：
- en: '[PRE55]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The definition says that `20`% of the requests sent to the `product` service
    will be aborted with the HTTP status code `500` (`Internal Server Error`).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 定义说明，发送到 `product` 服务的 `20`% 请求将使用 HTTP 状态码 `500` (`Internal Server Error`)
    被终止。
- en: 'Perform the following steps to test this:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以测试此功能：
- en: Ensure that the load tests using `siege`, as started in the *Observing the service
    mesh* section, are running.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保使用 `siege` 进行的负载测试，如 *观察服务网格* 部分中启动的，正在运行。
- en: 'Apply the fault injection with the following command:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令应用故障注入：
- en: '[PRE56]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Monitor the output from the `siege` load tests tool. Expect output similar to
    the following:![A screenshot of a computer  Description automatically generated](img/B19825_18_19.png)
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控 `siege` 负载测试工具的输出。期望输出类似于以下内容：![计算机屏幕截图  自动生成的描述](img/B19825_18_19.png)
- en: 'Figure 18.19: Observing the retry mechanism in action'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 18.19：观察重试机制在行动
- en: From the sample output, we can see that all requests are still successful, in
    other words, status `200` (`OK`) is returned; however, some of them (20%) take
    an extra second to complete. This indicates that the retry mechanism in the `product-composite`
    microservice has kicked in and has retried a failed request to the `product` service.
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从样本输出中，我们可以看到所有请求仍然成功，换句话说，返回了状态 `200` (`OK`)；然而，其中一些（20%）需要额外一秒钟来完成。这表明 `product-composite`
    微服务中的重试机制已经启动，并重试了对 `product` 服务的失败请求。
- en: 'Conclude the tests by removing the fault injection with the following command:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令移除故障注入以结束测试：
- en: '[PRE57]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Let’s now move on to the next section, where we will inject delays to trigger
    the circuit breaker.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一节，我们将注入延迟以触发断路器。
- en: Testing resilience by injecting delays
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过注入延迟测试弹性
- en: From *Chapter 13*, *Improving Resilience Using Resilience4j*, we know that a
    circuit breaker can be used to prevent problems due to the slow or complete lack
    of response of services after accepting requests.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *第 13 章*，*使用 Resilience4j 提高弹性* 我们知道，断路器可以用来防止在接收请求后，由于服务响应缓慢或完全无响应而产生的问题。
- en: Let’s verify that the circuit breaker in the `product-composite` service works
    as expected by injecting a delay into the `product` service using Istio. A delay
    can be injected using a virtual service.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用 Istio 向 `product` 服务注入延迟来验证 `product-composite` 服务中的断路器是否按预期工作。可以使用虚拟服务注入延迟。
- en: 'Refer to `kubernetes/resilience-tests/product-virtual-service-with-delay.yml`.
    Its code appears as follows:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 `kubernetes/resilience-tests/product-virtual-service-with-delay.yml`。其代码如下：
- en: '[PRE58]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This definition says that all requests sent to the `product` service will be
    delayed by `3` seconds.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 此定义说明，发送到 `product` 服务的所有请求都将延迟 `3` 秒。
- en: Requests sent to the `product` service from the `product-composite` service
    are configured to time out after 2 seconds. The circuit breaker is configured
    to open its circuit if 3 consecutive requests fail. When the circuit is open,
    it will fast-fail; in other words, it will immediately throw an exception, not
    attempting to call the underlying service. The business logic in the `product-composite`
    microservice will catch this exception and apply fallback logic. For a recap,
    see *Chapter 13*, *Improving Resilience Using Resilience4j* (refer to the *Adding
    a circuit breaker and a time limiter* section).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 从`product-composite`服务发送到`product`服务的请求被配置为在2秒后超时。断路器被配置为在连续三次请求失败时打开电路。当电路打开时，它将快速失败；换句话说，它将立即抛出异常，不会尝试调用底层服务。`product-composite`微服务中的业务逻辑将捕获此异常并应用回退逻辑。为了回顾，请参阅*第13章*，*使用Resilience4j提高弹性*（参考*添加断路器和时间限制器*部分）。
- en: 'Perform the following steps to test the circuit breaker by injecting a delay:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤通过注入延迟来测试断路器：
- en: Stop the load test by pressing *Ctrl* + *C* in the terminal window where `siege`
    is running.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行`siege`的终端窗口中按*Ctrl* + *C*停止负载测试。
- en: 'Create a temporary delay in the `product` service with the following command:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在`product`服务中创建临时延迟：
- en: '[PRE59]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Acquire an access token as follows:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式获取访问令牌：
- en: '[PRE60]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Send six requests in a row:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续发送六个请求：
- en: '[PRE61]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Expect the following:'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期以下情况：
- en: The circuit opens up after the first three failed calls
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 断路器在第一次三次失败调用后打开
- en: The circuit breaker applies fast-fail logic for the last three calls
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 断路器对最后三次调用应用快速失败逻辑
- en: A fallback response is returned for the last three calls
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后三次调用返回回退响应
- en: 'The responses from the first 3 calls are expected to be a timeout-related error
    message, with a response time of 2 seconds (in other words, the timeout time).
    Expect responses for the first 3 calls along the lines of the following:'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期前三次调用的响应将是一个与超时相关的错误消息，响应时间为2秒（换句话说，超时时间）。预期前三次调用的响应如下：
- en: '![A picture containing text, screenshot, font  Description automatically generated](img/B19825_18_20.png)'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![包含文本、屏幕截图、字体的图片，描述自动生成](img/B19825_18_20.png)'
- en: 'Figure 18.20: Observing timeouts'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图18.20：观察超时
- en: 'The responses from the last 3 calls are expected to come from the fallback
    logic with a short response time. Expect responses for the last 3 calls as follows:'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期最后三次调用的响应将来自回退逻辑，响应时间较短。预期最后三次调用的响应如下：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_21.png)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![一张包含计算机屏幕截图的图片，描述自动生成，置信度中等](img/B19825_18_21.png)'
- en: 'Figure 18.21: Fallback method in action'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图18.21：回退方法在行动
- en: 'Simulate the delay problem being fixed by removing the temporary delay with
    the following command:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令通过移除临时延迟来模拟延迟问题已被解决：
- en: '[PRE62]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Verify that correct answers are returned again, and without any delay, by sending
    a new request using the `for` loop command in *step 4*.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用*步骤 4*中的`for`循环命令发送新的请求，验证是否能够再次返回正确答案，并且没有任何延迟。
- en: 'If you want to check the state of the circuit breaker, you can do it with the
    following command:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想检查断路器的状态，可以使用以下命令：
- en: '[PRE63]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: It should report `CLOSED`, `OPEN`, or `HALF_OPEN`, depending on its state.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该报告`CLOSED`、`OPEN`或`HALF_OPEN`，具体取决于其状态。
- en: This proves that the circuit breaker reacts as expected when we inject a delay
    using Istio. This concludes testing the features in Istio that can be used to
    verify that the microservice landscape is resilient. The final feature we will
    explore in Istio is its support for traffic management; we will see how it can
    be used to enable deployments with zero downtime.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了当使用Istio注入延迟时，断路器反应如预期。这标志着测试了可用于验证微服务景观弹性的Istio功能。我们将探索Istio的下一个功能是它对流量管理的支持；我们将看到它如何被用来实现零停机部署。
- en: Performing zero-downtime updates
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行零停机更新
- en: As mentioned in *Chapter 16*, *Deploying Our Microservices to Kubernetes*, being
    able to deploy an update without downtime becomes crucial with a growing number
    of autonomous microservices that are updated independently of one another.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第16章*中所述，*将我们的微服务部署到Kubernetes*，随着越来越多的独立更新的自主微服务数量的增长，能够在不停机的情况下部署更新变得至关重要。
- en: In this section, we will learn about Istio’s traffic management and routing
    capabilities and how they can be used to perform deployments of new versions of
    microservices without requiring any downtime. In *Chapter 15*, *Introduction to
    Kubernetes*, we learned that Kubernetes can be used to perform a rolling upgrade
    without requiring any downtime. Using the Kubernetes rolling upgrade mechanism
    automates the entire process but, unfortunately, provides no option to test the
    new version before all users are routed to it.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解Istio的流量管理和路由能力，以及如何使用它们来部署微服务的新版本，而无需停机。在*第15章*，*Kubernetes简介*中，我们了解到Kubernetes可以用于进行滚动升级，而无需停机。使用Kubernetes的滚动升级机制可以自动化整个过程，但遗憾的是，它没有提供在所有用户都被路由到新版本之前测试新版本的选择。
- en: 'Using Istio, we can deploy the new version but initially route all users to
    the existing version (called the **old** version in this chapter). After that,
    we can use Istio’s fine-grained routing mechanism to control how users are routed
    to the new and the old versions. We will see how two popular upgrade strategies
    can be implemented using Istio:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Istio，我们可以部署新版本，但最初将所有用户路由到现有版本（在本章中称为**旧**版本）。之后，我们可以使用Istio的细粒度路由机制来控制用户如何被路由到新版本和旧版本。我们将看到如何使用Istio实现两种流行的升级策略：
- en: '**Canary deployments**: When using canary deployments, all users are routed
    to the old version, except for a group of selected test users who are routed to
    the new version. When the test users have approved the new version, regular users
    can be routed to the new version using a blue-green deploy.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金丝雀部署**：在金丝雀部署中，所有用户都被路由到旧版本，除了被选中的测试用户组，他们被路由到新版本。当测试用户批准新版本后，可以使用蓝绿部署将常规用户路由到新版本。'
- en: '**Blue-green deployments**: Traditionally, a blue-green deploy means that all
    users are switched to either the blue or the green version, one being the new
    version and the other being the old version. If something goes wrong when switching
    over to the new version, it is very simple to switch back to the old version.
    Using Istio, this strategy can be refined by gradually shifting users over to
    the new version, for example, starting with 20% of the users and then slowly increasing
    the percentage. At all times, it is very easy to route all users back to the old
    version if a fatal error is revealed in the new version.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝绿部署**：传统上，蓝绿部署意味着所有用户都会切换到蓝色或绿色版本，一个版本是新的，另一个版本是旧的。如果在切换到新版本时出现问题，切换回旧版本非常简单。使用Istio，可以通过逐渐将用户转移到新版本来细化这种策略，例如，从20%的用户开始，然后逐渐增加百分比。在任何时候，如果在新版本中发现了致命错误，都可以非常容易地将所有用户路由回旧版本。'
- en: As already stated in *Chapter 16*, it is important to remember that a prerequisite
    for these types of upgrade strategies is that the upgrade is **backward-compatible**.
    Such an upgrade is compatible both in terms of APIs and message formats, which
    are used to communicate with other services and database structures. If the new
    version of the microservice requires changes to external APIs, message formats,
    or database structures that the old version can’t handle, these upgrade strategies
    can’t be applied.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在*第16章*中已经提到的，我们需要记住，这些升级策略的一个先决条件是升级必须是**向后兼容**的。这种升级在API和消息格式方面都是兼容的，这些API和消息格式用于与其他服务和数据库结构进行通信。如果新版本的微服务需要对旧版本无法处理的外部API、消息格式或数据库结构进行更改，那么这些升级策略就不能应用。
- en: 'We will go through the following deployment scenario:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下部署场景：
- en: We will start by deploying the `v1` and `v2` versions of the microservices,
    with routing configured to send all requests to the `v1` version of the microservices.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先部署微服务的`v1`和`v2`版本，并将路由配置为将所有请求发送到微服务的`v1`版本。
- en: Next, we will allow a test group to run canary tests; that is, we’ll verify
    the new `v2` versions of the microservices. To simplify the tests somewhat, we
    will only deploy new versions of the core microservices, that is, the `product`,
    `recommendation`, and `review` microservices.
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将允许一个测试组运行金丝雀测试；也就是说，我们将验证微服务的`v2`新版本。为了简化测试，我们只部署核心微服务的新版本，即`product`、`recommendation`和`review`微服务。
- en: Finally, we will start to move regular users over to the new versions using
    a blue-green deploy; initially, a small percentage of users and then, over time,
    more and more users until, eventually, they are all routed to the new version.
    We will also see how we can quickly switch back to the `v1` version if a fatal
    error is detected in the new `v2` version.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将开始使用蓝绿部署将常规用户迁移到新版本；最初是少量用户，然后随着时间的推移，越来越多的用户，直到最终所有用户都被路由到新版本。如果在新 `v2`
    版本中检测到致命错误，我们还将看到如何快速切换回 `v1` 版本。
- en: Let’s first see what changes need to be applied to the source code to be able
    to deploy and route traffic to two concurrent versions, `v1` and `v2`, of the
    core microservices.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看需要应用到源代码中的哪些更改，以便能够部署并将流量路由到核心微服务的两个并发版本，`v1` 和 `v2`。
- en: Source code changes
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 源代码更改
- en: To be able to run multiple versions of a microservice concurrently, the Deployment
    objects and their corresponding Pods must have different names, for example, `product-v1`
    and `product-v2`. There must, however, be only one Kubernetes Service object per
    microservice. All traffic to a specific microservice always goes through the same
    Service object, irrespective of what version of the Pod the request will be routed
    to in the end. To configure the actual routing rules for canary tests and blue-green
    deployments, Istio’s `VirtualService` and `DestinationRule` objects are used.
    Finally, the `values.yaml` file in the `prod-env` Helm chart is used to specify
    the versions of each microservice that will be used in the production environment.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够同时运行微服务的多个版本，部署对象及其对应的 Pod 必须有不同的名称，例如，`product-v1` 和 `product-v2`。然而，每个微服务必须只有一个
    Kubernetes 服务对象。所有流量都通过同一个服务对象到达特定的微服务，无论请求最终将被路由到哪个版本的 Pod。为了配置金丝雀测试和蓝绿部署的实际路由规则，使用
    Istio 的 `VirtualService` 和 `DestinationRule` 对象。最后，`prod-env` Helm 图表中的 `values.yaml`
    文件用于指定生产环境中将使用的每个微服务的版本。
- en: 'Let’s go through the details for each definition in the following subsections:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以下一节中的每个定义的详细信息进行说明：
- en: Virtual services and destination rules
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟服务和目标规则
- en: Deployments and services
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和服务
- en: Tying things together in the `prod-env` Helm chart
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `prod-env` Helm 图表中整合事物
- en: Virtual services and destination rules
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟服务和目标规则
- en: To split the traffic between two versions of a microservice, we need to specify
    the weight distribution between the two versions in a virtual service, on the
    sender side. The virtual service will spread the traffic between two subsets,
    called `old` and `new`. The exact meaning of the `new` and `old` subset is defined
    in a corresponding `DestinationRule`, on the receiver side. It uses `labels` to
    determine which Pods run the old and new versions of the microservice.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在微服务的两个版本之间分割流量，我们需要在虚拟服务中指定两个版本之间的权重分布，在发送方。虚拟服务将在两个子集之间分配流量，称为 `old` 和 `new`。`new`
    和 `old` 子集的确切含义在接收方的相应 `DestinationRule` 中定义。它使用 `labels` 来确定哪些 Pod 运行微服务的旧版本和新版本。
- en: To support canary tests, a routing rule is required in the virtual services
    that always routes the canary testers to the `new` subset. To identify canary
    testers, we will assume that requests from a canary tester contain an HTTP header
    named `X-group` with the value `test`.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持金丝雀测试，虚拟服务中需要一个始终将金丝雀测试者路由到 `new` 子集的路由规则。为了识别金丝雀测试者，我们将假设来自金丝雀测试者的请求包含一个名为
    `X-group` 的 HTTP 头部，其值为 `test`。
- en: 'A template has been added to the `common` Helm chart for creating a set of
    virtual services that can split the traffic between two versions of a microservice.
    The template is named `_istio_vs_green_blue_deploy.yaml` and looks like this:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 已将一个模板添加到 `common` Helm 图表，用于创建一组虚拟服务，这些服务可以在微服务的两个版本之间分割流量。该模板名为 `_istio_vs_green_blue_deploy.yaml`，其内容如下：
- en: '[PRE64]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'From the template, we can see the following:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 从模板中，我们可以看到以下内容：
- en: The `range` directive loops over the elements defined in the `virtualServices`
    variable
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`range` 指令遍历 `virtualServices` 变量中定义的元素'
- en: The `hosts` field in the `spec` part of the manifest is used to specify the
    names of the Kubernetes service that this `VirtualService` will apply to
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清单 `spec` 部分的 `hosts` 字段用于指定此 `VirtualService` 将应用到的 Kubernetes 服务的名称
- en: 'In the `http` section, three routing destinations are declared:'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `http` 部分中，声明了三个路由目标：
- en: One route matching the canary testers’ HTTP header, `X-group`, set to `test`.
    This route always sends the requests to the `new` subset.
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与金丝雀测试者的 HTTP 头部 `X-group` 匹配的路由，设置为 `test`。此路由始终将请求发送到 `new` 子集。
- en: One route destination for the `old` subset and one for the `new` subset.
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`old`子集和一个`new`子集的一个路由目标。'
- en: The `weight` is specified as a percentage and the sum of the weights will always
    be 100.
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`被指定为百分比，权重的总和始终为100。'
- en: All traffic is initially routed to the `old` subset
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有流量最初都路由到`old`子集
- en: To be able to route canary testers to the new versions based on header-based
    routing, the `product-composite` microservice has been updated to forward the
    HTTP header, `X-group`. Refer to the `getCompositeProduct()` method in the `se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl`
    class for details.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够根据基于头部的路由将金丝雀测试者路由到新版本，`product-composite`微服务已被更新为转发HTTP头`X-group`。有关详细信息，请参阅`se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl`类中的`getCompositeProduct()`方法。
- en: For the destination rules, we will reuse the template introduced in the *Content
    in the _istio_dr_mutual_tls.yaml template* section above. This template will be
    used by the `prod-env` Helm chart to specify the versions of the microservices
    to be used. This is described in the *Tying things together in the prod-env Helm
    chart* section below.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标规则，我们将重用上面*在_istio_dr_mutual_tls.yaml模板中的内容*部分中引入的模板。此模板将由`prod-env` Helm图表用于指定要使用的微服务版本。这将在下面的*在prod-env
    Helm图表中整合各项*部分中描述。
- en: Deployments and services
  id: totrans-494
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署和服务
- en: 'To make it possible for a destination rule to identify the version of a Pod
    based on its labels, a `version` label has been added in the template for deployments
    in the `common` Helm chart, `_deployment.yaml`. Its value is set to the `tag`
    of the Pod’s Docker image. We will use the Docker image tags `v1` and `v2`, so
    that will also be the value of the `version` label. The added line looks like
    this:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使目标规则能够根据Pod的标签识别其版本，`common` Helm图表中的部署模板`_deployment.yaml`已添加了一个`version`标签。其值设置为Pod的Docker镜像的`tag`。我们将使用`v1`和`v2`的Docker镜像标签，因此这也将是`version`标签的值。添加的行看起来像这样：
- en: '[PRE65]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To give the Pods and their Deployment objects names that contain their version,
    their default names have been overridden in the `prod-env` chart. In their `values.yaml`
    files, the `fullnameOverride` field is used to specify a name that includes version
    info. This is done for the three core microservices and looks like this:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给Pod及其Deployment对象命名，包含它们的版本信息，`prod-env`图表中已覆盖了它们的默认名称。在它们的`values.yaml`文件中，使用`fullnameOverride`字段指定包含版本信息的名称。这是为三个核心微服务所做的，看起来像这样：
- en: '[PRE66]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: An undesired side effect of this is that the corresponding Service objects will
    also get a name that includes the version info. As explained above, we need to
    have one service that can route requests to the different versions of the Pods.
    To avoid this naming problem, the Service template, `_service.yaml`, in the `common`
    Helm chart is updated to use the `common.name` template instead of the `common.fullname`
    template used previously in *Chapter 17*.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 这的不希望产生的副作用是，相应的Service对象也将获得一个包含版本信息的名称。如上所述，我们需要有一个服务可以将请求路由到Pod的不同版本。为了避免这种命名问题，`common`
    Helm图表中的服务模板`_service.yaml`已更新为使用`common.name`模板，而不是之前在*第17章*中使用的`common.fullname`模板。
- en: Finally, to be able to deploy multiple versions of the three core microservices,
    their Helm charts have been duplicated in the `kubernetes/helm/components` folder.
    The name of the new charts is suffixed with `-green`. The only difference compared
    to the existing charts is that they don’t include the Service template from the
    `common` chart, avoiding the creation of two Service objects per core microservice.
    The new charts are named `product-green`, `recommendation-green`, and `review-green`.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了能够部署三个核心微服务的多个版本，它们的Helm图表已在`kubernetes/helm/components`文件夹中进行了复制。新图表的名称后缀为`-green`。与现有图表相比，唯一的区别是它们不包括来自`common`图表的服务模板，从而避免了每个核心微服务创建两个Service对象。新图表的名称为`product-green`、`recommendation-green`和`review-green`。
- en: Tying things together in the prod-env Helm chart
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在prod-env Helm图表中整合各项
- en: The `prod-env` Helm chart includes the `_istio_vs_green_blue_deploy.yaml` template
    from the `common` Helm chart, as well as the templates included by the `dev-env`
    chart; see the *Creating the service mesh* section.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '`prod-env` Helm图表包括来自`common` Helm图表的`_istio_vs_green_blue_deploy.yaml`模板，以及`dev-env`图表中包含的模板；请参阅*创建服务网格*部分。'
- en: The three new `*-green` Helm charts for the core microservices are added as
    dependencies to the `Chart.yaml` file.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 将三个新的`*-green` Helm图表作为依赖项添加到`Chart.yaml`文件中。
- en: In its `values.yaml` file, everything is tied together. From the previous section,
    we have seen how the `v1` versions of the core microservices are defined with
    names that include version info.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在其`values.yaml`文件中，所有内容都紧密相连。从前一节中，我们看到了如何使用包含版本信息的名称定义核心微服务的`v1`版本。
- en: 'For the `v2` versions, the three new `*-green` Helm charts are used. The values
    are the same as for the `v1` versions except for the name and Docker image tag.
    For example, the configuration of the `v2` version of the `product` microservice
    looks like this:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`v2`版本，使用三个新的`*-green` Helm图表。值与`v1`版本相同，除了名称和Docker镜像标签。例如，`product`微服务的`v2`版本配置如下：
- en: '[PRE67]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'To declare virtual services for the three core microservices, the following
    declaration is used:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 为了声明三个核心微服务的虚拟服务，使用以下声明：
- en: '[PRE68]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, the destination rules are declared in a similar way as in the `dev-env`
    Helm chart. The main difference is that we now use subsets to declare the actual
    versions that should be used when traffic is routed by the virtual services to
    either the `old` or the `new` subset. For example, the destination rule for the
    `product` microservice is declared like this:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以与`dev-env` Helm图表类似的方式声明目标规则。主要区别在于我们现在使用子集来声明当虚拟服务将流量路由到`old`或`new`子集时应使用的实际版本。例如，`product`微服务的目标规则声明如下：
- en: '[PRE69]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: From the declaration above, we can see that traffic sent to the `old` subset
    is directed to `v1` Pods of the `product` microservice and to `v2` Pods for the
    `new` subset.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的声明中，我们可以看到发送到`old`子集的流量被导向`product`微服务的`v1` Pods，而对于`new`子集，则导向`v2` Pods。
- en: For details, see the file in the `prod-env` chart available in the `kubernetes/helm/environments/prod-env`
    folder.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅`kubernetes/helm/environments/prod-env`文件夹中`prod-env`图表中的文件。
- en: Note that this is where we declare for the production environment what the existing
    (`old`) and the coming (`new`) versions are, `v1` and `v2` in this scenario. In
    a future scenario, where it is time to upgrade `v2` to `v3`, the `old` subset
    should be updated to use `v2` and the `new` subset should use `v3`.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是我们声明生产环境中的现有（旧版）和即将到来（新版）版本的地方，在这个场景中是`v1`和`v2`。在未来的场景中，当升级`v2`到`v3`时，`old`子集应更新为使用`v2`，而`new`子集应使用`v3`。
- en: Now, we have seen all the changes to the source code and we are ready to deploy
    `v1` and `v2` versions of the microservices.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看到了源代码的所有更改，我们准备部署微服务的`v1`和`v2`版本。
- en: Deploying v1 and v2 versions of the microservices with routing to the v1 version
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署将路由到`v1`版本的微服务的`v1`和`v2`版本
- en: To be able to test the `v1` and `v2` versions of the microservices, we need
    to remove the development environment we used earlier in this chapter and create
    a production environment where we can deploy the `v1` and `v2` versions of the
    microservices.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够测试微服务的`v1`和`v2`版本，我们需要移除本章前面使用的开发环境，并创建一个可以部署微服务的`v1`和`v2`版本的生产环境。
- en: 'To achieve this, run the following commands:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，运行以下命令：
- en: 'Uninstall the development environment:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卸载开发环境：
- en: '[PRE70]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To monitor the termination of Pods in the development environment, run the
    following command until it reports `No resources found in hands-on namespace`:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要监控开发环境中Pod的终止，运行以下命令直到报告“在hands-on命名空间中未找到资源”：
- en: '[PRE71]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Start MySQL, MongoDB, and RabbitMQ outside of Kubernetes:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kubernetes外部启动MySQL、MongoDB和RabbitMQ：
- en: '[PRE72]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Tag the Docker images with `v1` and `v2` versions:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`v1`和`v2`版本标记Docker镜像：
- en: '[PRE73]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The `v1` and `v2` versions of the microservices will be the same versions of
    the microservices in this test. But it doesn’t matter to Istio, so we can use
    this simplified approach to test Istio’s routing capabilities.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务的`v1`和`v2`版本将是这个测试中微服务的相同版本。但对于Istio来说，这并不重要，因此我们可以使用这种简化的方法来测试Istio的路由功能。
- en: 'Deploy the system landscape using Helm and wait for all deployments to complete:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Helm部署系统架构并等待所有部署完成：
- en: '[PRE74]'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Once the deployment is complete, verify that we have `v1` and `v2` Pods up
    and running for the three core microservices with the following command:'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦部署完成，使用以下命令验证我们为三个核心微服务创建了`v1`和`v2` Pods并正在运行：
- en: '[PRE75]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Expect a response like this:'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期收到如下响应：
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B19825_18_22.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  描述由中等置信度自动生成](img/B19825_18_22.png)'
- en: 'Figure 18.22: v1 and v2 Pods deployed at the same time'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.22：同时部署的v1和v2 Pods
- en: 'Run the usual tests to verify that everything works:'
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行常规测试以验证一切是否正常工作：
- en: '[PRE76]'
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Unfortunately, the tests will fail initially with an error message like:'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不幸的是，测试最初会失败，并显示如下错误信息：
- en: '[PRE77]'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'This error is caused by the Istio daemon, `istiod`, caching the JWKS public
    keys from the auth server in the development environment. The auth server in the
    production environment will have new JWKS keys but the same identity as `istiod`,
    so it tries to reuse the old JWKS public keys, causing this failure. Istio caches
    JWKS public keys for 20 minutes by default, but when installing Istio, we lowered
    the refresh interval to 15 seconds; see the *Deploying Istio in a Kubernetes cluster*
    section. So, after waiting a short while, up to a minute depending on how quickly
    the refreshed keys are propagated, you should be able to run the tests successfully.
    The tests might still fail once the issue with cached JWKS has disappeared, with
    errors such as this:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 此错误是由 Istio 守护程序 `istiod` 在开发环境中缓存来自身份验证服务器的 JWKS 公钥引起的。生产环境中的身份验证服务器将具有新的 JWKS
    密钥，但与 `istiod` 具有相同的身份，因此它试图重用旧的 JWKS 公钥，导致此失败。Istio 默认情况下将 JWKS 公钥缓存 20 分钟，但在安装
    Istio 时，我们将刷新间隔降低到 15 秒；请参阅*在 Kubernetes 集群中部署 Istio*部分。因此，等待一段时间后，最长可达一分钟，具体取决于刷新密钥传播的速度，你应该能够成功运行测试。一旦缓存的
    JWKS 问题消失，测试可能会失败，出现如下错误：
- en: '[PRE78]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Then, simply rerun the command and it should run fine! These errors are secondary
    failures caused by the original error caused by the JWKS cache.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需重新运行命令，它应该运行良好！这些错误是由 JWKS 缓存引起的原始错误导致的次级故障。
- en: 'Expect output that is similar to what we have seen from the previous chapters:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出与之前章节中看到的内容类似：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_23.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  描述自动生成，置信度中等](img/B19825_18_23.png)'
- en: 'Figure 18.23: Tests run successfully'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.23：测试运行成功
- en: We are now ready to run some **zero-downtime deployment** tests. Let’s begin
    by verifying that all traffic goes to the `v1` version of the microservices!
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行一些**零停机部署**测试了。让我们首先验证所有流量都路由到微服务的 `v1` 版本！
- en: Verifying that all traffic initially goes to the v1 version of the microservices
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证所有流量最初都路由到微服务的 v1 版本
- en: To verify that all requests are routed to the `v1` version of the microservices,
    we will start up the load test tool, `siege`, and then observe the traffic that
    flows through the service mesh using Kiali.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证所有请求都路由到微服务的 `v1` 版本，我们将启动负载测试工具 `siege`，然后使用 Kiali 观察通过服务网格的流量。
- en: 'Perform the following steps:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Get a new access token and start the `siege` load test tool, with the following
    commands:'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取新的访问令牌并启动 `siege` 负载测试工具，使用以下命令：
- en: '[PRE79]'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Go to the **Graph** view in Kiali’s web UI ([https://kiali.minikube.me](https://kiali.minikube.me)):'
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 Kiali 的 Web UI 中的**图**视图([https://kiali.minikube.me](https://kiali.minikube.me))：
- en: Click on the **Display** menu button and select **Namespace Boxes**.
  id: totrans-551
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**显示**菜单按钮并选择**命名空间框**。
- en: Click on the **App graph** menu button and select **Versioned app graph**.
  id: totrans-552
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**应用图**菜单按钮并选择**版本化应用图**。
- en: 'Expect only traffic to the **v1** version of the microservices, as follows:'
  id: totrans-553
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预期只有流量路由到微服务的**v1**版本，如下所示：
- en: '![A screenshot of a diagram  Description automatically generated with low confidence](img/B19825_18_24.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![图表截图  描述自动生成，置信度低](img/B19825_18_24.png)'
- en: 'Figure 18.24: All requests go to the v1 Pods'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.24：所有请求都发送到 v1 Pods
- en: This means that, even though the `v2` versions of the microservices are deployed,
    they do not get any traffic routed to them. Let’s now try out canary tests where
    selected test users are allowed to try out the `v2` versions of the microservices!
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，尽管微服务的 `v2` 版本已部署，但它们没有收到任何流量路由到它们。现在让我们尝试金丝雀测试，允许选定的测试用户尝试微服务的 `v2` 版本！
- en: Running canary tests
  id: totrans-557
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行金丝雀测试
- en: To run a canary test so that some users are routed to the new versions while
    all other users are still routed to the old versions of the deployed microservices,
    we need to add the `X-group` HTTP header set to the value `test` in our requests
    sent to the external API.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行金丝雀测试，使一些用户被路由到新版本，而所有其他用户仍然被路由到已部署微服务的旧版本，我们需要在我们的发送到外部 API 的请求中添加 `X-group`
    HTTP 标头，并将其设置为 `test` 值。
- en: To see which version of a microservice served a request, the `serviceAddresses`
    field in the response can be inspected. The `serviceAddresses` field contains
    the hostname of each service that took part in creating the response. The hostname
    is equal to the name of the Pod, so we can find the version in the hostname; for
    example, `product-v1-...` for a `product` service of version `v1`, and `product-v2-...`
    for a `product` service of version `v2`.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看哪个版本的微服务处理了请求，可以在响应中的`serviceAddresses`字段进行检查。`serviceAddresses`字段包含参与创建响应的每个服务的主机名。主机名等于Pod的名称，因此我们可以在主机名中找到版本；例如，对于版本为`v1`的`product`服务，是`product-v1-...`，对于版本为`v2`的`product`服务，是`product-v2-...`。
- en: Let’s begin by sending a normal request and verifying that it is the `v1` versions
    of the microservices that respond to our request. Next, we’ll send a request with
    the `X-group` HTTP header set to the value `test`, and verify that the new `v2`
    versions are responding.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从发送一个正常请求并验证它是否是响应我们请求的微服务的`v1`版本开始。接下来，我们将发送一个带有`X-group` HTTP头设置为值`test`的请求，并验证新的`v2`版本是否在响应。
- en: 'To do this, perform the following steps:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，请执行以下步骤：
- en: 'Perform a normal request to verify that the request is routed to the `v1` version
    of the microservices by using `jq` to filter out the `serviceAddresses` field
    in the response:'
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用`jq`过滤响应中的`serviceAddresses`字段来执行正常请求以验证请求是否被路由到微服务的`v1`版本：
- en: '[PRE80]'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Expect a response along the lines of the following:'
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期响应将与以下类似：
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_18_25.png)'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成描述](img/B19825_18_25.png)'
- en: 'Figure 18.25: All requests go to the v1 Pods'
  id: totrans-566
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图18.25：所有请求都进入v1 Pods
- en: As expected, all three core services are `v1` versions of the microservices.
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如预期，所有三个核心服务都是微服务的`v1`版本。
- en: 'If we add the `X-group=test` header, we expect the request to be served by
    `v2` versions of the core microservices. Run the following command:'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们添加`X-group=test`头，我们期望请求由核心微服务的`v2`版本提供服务。运行以下命令：
- en: '[PRE81]'
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Expect a response similar to the following:'
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期响应将与以下类似：
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_18_26.png)'
  id: totrans-571
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成描述](img/B19825_18_26.png)'
- en: 'Figure 18.26: Setting the HTTP header to X-group=test makes the requests go
    to the v2 Pods'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.26：将HTTP头设置为X-group=test使请求进入v2 Pods
- en: As expected, all three core microservices that respond are now `v2` versions;
    as a canary tester, we are routed to the new `v2` versions!
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，所有响应的三个核心微服务现在都是`v2`版本；作为金丝雀测试员，我们被路由到新的`v2`版本！
- en: Given that the canary tests returned the expected results, we are ready to allow
    normal users to be routed to the new `v2` versions using a blue-green deployment.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 由于金丝雀测试返回了预期的结果，我们现在准备允许正常用户通过蓝绿部署路由到新的`v2`版本。
- en: Running a blue-green deployment
  id: totrans-575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行蓝绿部署
- en: To route a portion of the normal users to the new `v2` versions of the microservices,
    we need to modify the weight distribution in the virtual services. They are currently
    100/0; in other words, all traffic is routed to the old `v1` versions. We can
    achieve this, as we did before, by editing the manifest files of the virtual services
    and executing a `kubectl apply` command to make the changes take effect. As an
    alternative, we can use the `kubectl patch` command to change the weight distribution
    directly on the `virtual Service` objects in the Kubernetes API server.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 要将部分正常用户路由到微服务的新的`v2`版本，我们需要修改虚拟服务中的权重分布。它们目前是100/0；换句话说，所有流量都被路由到旧的`v1`版本。我们可以通过编辑虚拟服务的清单文件并执行一个`kubectl
    apply`命令来实现这一点，以使更改生效。作为替代方案，我们可以在Kubernetes API服务器中的`virtual Service`对象上直接使用`kubectl
    patch`命令来更改权重分布。
- en: 'I find the `patch` command useful when making a number of changes to the same
    objects to try something out, for example, to change the weight distribution in
    the routing rules. In this section, we will use the `kubectl patch` command to
    quickly change the weight distribution in the routing rules between the `v1` and
    `v2` versions of the microservices. To get the state of a virtual service after
    a few `kubectl patch` commands have been executed, a command such as `kubectl
    get vs NNN -o yaml` can be issued. For example, to get the state of the virtual
    service of the `product` microservice, issue the following command: `kubectl get
    vs product -o yaml`.'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要对同一对象进行多项更改以尝试新事物时，我发现`patch`命令很有用，例如，更改路由规则中的权重分配。在本节中，我们将使用`kubectl patch`命令快速更改微服务`v1`和`v2`版本之间路由规则中的权重分配。要获取执行几个`kubectl
    patch`命令后虚拟服务的状态，可以发出类似`kubectl get vs NNN -o yaml`的命令。例如，要获取`product`微服务虚拟服务的状态，可以发出以下命令：`kubectl
    get vs product -o yaml`。
- en: Since we haven’t used the `kubectl patch` command before and it can be a bit
    involved to start with, let’s undertake a short introduction to see how it works
    before we perform the blue-green deployment.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们之前没有使用过`kubectl patch`命令，并且一开始可能会有些复杂，所以在进行蓝绿部署之前，让我们简要介绍一下它是如何工作的。
- en: A short introduction to the kubectl patch command
  id: totrans-579
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: kubectl patch命令简介
- en: 'The `kubectl patch` command can be used to update specific fields in an existing
    object in the Kubernetes API server. We will try the patch command on the virtual
    service for the review microservice, named `review`. The relevant parts of the
    definition of the virtual service, `review`, appear as follows:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl patch`命令可用于更新Kubernetes API服务器中现有对象的特定字段。我们将尝试在名为`review`的虚拟服务上使用补丁命令，该虚拟服务是用于`review`微服务的。虚拟服务`review`定义的相关部分如下：'
- en: '[PRE82]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'A sample `patch` command that changes the weight distribution of the routing
    to the `v1` and `v2` Pods in the `review` microservice appears as follows:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例`patch`命令，该命令更改了`review`微服务中`v1`和`v2` Pods的路由权重分配，如下所示：
- en: '[PRE83]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The command will configure the routing rules of the `review` microservice to
    route 80% of the requests to the old version, and 20% of the requests to the new
    version.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将配置`review`微服务的路由规则，将80%的请求路由到旧版本，20%的请求路由到新版本。
- en: To specify that the `weight` value should be changed in the `review` virtual
    service, the `/spec/http/1/route/0/weight` path is given for the old version,
    and `/spec/http/1/route/1/weight` for the new version.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定在`review`虚拟服务中更改`weight`值，为旧版本提供`/spec/http/1/route/0/weight`路径，为新版本提供`/spec/http/1/route/1/weight`路径。
- en: The `0` and `1` in the path are used to specify the index of array elements
    in the definition of the virtual service. For example, `http/1` means the second
    element in the array under the `http` element. See the definition of the preceding
    `review` virtual service.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 路径中的`0`和`1`用于指定虚拟服务定义中数组元素的索引。例如，`http/1`表示`http`元素下的数组中的第二个元素。参见前面的`review`虚拟服务定义。
- en: From the definition, we can see that the first element with index `0` is the
    `match` element, which we will not change. The second element is the `route` element,
    which we want to change.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 从定义中我们可以看到，索引为`0`的第一个元素是`match`元素，我们不会更改它。第二个元素是`route`元素，我们想要更改它。
- en: Now that we know a bit more about the `kubectl patch` command, we are ready
    to test a blue-green deployment.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对`kubectl patch`命令有了更多了解，我们就准备好测试蓝绿部署了。
- en: Performing the blue-green deployment
  id: totrans-589
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行蓝绿部署
- en: 'It is time to gradually move more and more users to the new versions using
    a blue-green deployment. To perform the deployment, run the following steps:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候逐渐使用蓝绿部署将越来越多的用户迁移到新版本了。要执行部署，请按照以下步骤操作：
- en: Ensure that the load test tool, `siege`, is still running. Note that it was
    started in the preceding *Verifying that all traffic initially goes to the v1
    version of the microservices* section.
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保负载测试工具`siege`仍在运行。注意，它是在前面的*验证所有初始流量都流向微服务的v1版本*部分启动的。
- en: 'To allow 20% of users to be routed to the new `v2` version of the review microservice,
    we can patch the virtual service and change the weights with the following command:'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要允许20%的用户被路由到新的`v2`版本的`review`微服务，我们可以使用以下命令修补虚拟服务并更改权重：
- en: '[PRE84]'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: To observe the change in the routing rule, go to the Kiali web UI ([https://kiali.minikube.me](https://kiali.minikube.me))
    and select the **Graph** view.
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要观察路由规则的变化，请访问Kiali Web UI（[https://kiali.minikube.me](https://kiali.minikube.me)）并选择**图形**视图。
- en: Click on the **Display** menu and change the edge labels to **Traffic Distribution**.
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**显示**菜单，将边标签更改为**流量分布**。
- en: 'Wait for a minute before the metrics are updated in Kiali so that we can observe
    the change. Expect the graph in Kiali to show something like the following:'
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kiali更新指标之前等待一分钟，以便我们可以观察变化。预期Kiali中的图表将显示如下：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_27.png)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度中等](img/B19825_18_27.png)'
- en: 'Figure 18.27: 80% goes to v1 services and 20% goes to v2 services'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.27：80%流向v1服务，20%流向v2服务
- en: Depending on how long you have waited, the graph might look a bit different!
    In the screenshot, we can see that Istio now routes traffic to both the `v1` and
    `v2` versions of the `review` microservice.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您等待的时间长短，图表可能看起来略有不同！在屏幕截图中，我们可以看到Istio现在将流量路由到`review`微服务的`v1`和`v2`版本。
- en: Of the traffic that is sent to the `review` microservice from the `product-composite`
    microservice, 6.4% is routed to the new `v2` Pod, and 22.3% to the old `v1` Pod.
    This means that 6.4/(6.4 + 22.3) = 22% of the requests are routed to the `v2`
    Pod, and 78% to the `v1` Pod. This is in line with the 20/80 distribution we have
    requested.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 从`product-composite`微服务发送到`review`微服务的流量中，6.4%被路由到新的`v2` Pod，22.3%被路由到旧的`v1`
    Pod。这意味着6.4/(6.4 + 22.3) = 22%的请求被路由到`v2` Pod，78%被路由到`v1` Pod。这与我们请求的20/80分布相一致。
- en: Please feel free to try out the preceding `kubectl patch` command to affect
    the routing rules for the other core microservices, `product` and `recommendation`.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意尝试前面的`kubectl patch`命令，以影响其他核心微服务（`product`和`recommendation`）的路由规则。
- en: 'To simplify changing the weight distribution for all three core microservices,
    the`./kubernetes/routing-tests/split-traffic-between-old-and-new-services.bash`
    script can be used. For example, to route all traffic to the `v2` version of all
    microservices, run the following script, feeding it with the weight distribution
    `0 100`:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化对所有三个核心微服务的权重分布的更改，可以使用`./kubernetes/routing-tests/split-traffic-between-old-and-new-services.bash`脚本。例如，要将所有流量路由到所有微服务的`v2`版本，运行以下脚本，并传入权重分布`0
    100`：
- en: '[PRE85]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: You have to give Kiali a minute or two to collect metrics before it can visualize
    the changes in routing, but remember that the change in the actual routing is
    immediate!
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kiali能够可视化路由变化之前，您需要给它一分钟左右的时间来收集指标，但请记住，实际路由的变化是立即发生的！
- en: 'Expect that requests are routed only to the `v2` versions of the microservices
    in the graph after a while:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 预期一段时间后，图表中将只显示将请求路由到微服务`v2`版本的请求：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_18_28.png)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度中等](img/B19825_18_28.png)'
- en: 'Figure 18.28: All traffic goes to v2 services'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.28：所有流量都流向v2服务
- en: Depending on how long you have waited, the graph might look a bit different!
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您等待的时间长短，图表可能看起来略有不同！
- en: 'If something goes terribly wrong following the upgrade to `v2`, the following
    command can be executed to revert all traffic to the `v1` version of all microservices:'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 如果升级到`v2`版本后出现严重错误，可以使用以下命令将所有微服务的流量回滚到`v1`版本：
- en: '[PRE86]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: After a short while, the graph in Kiali should look like the screenshot in the
    previous *Verifying that all traffic initially goes to the v1 version of the microservices*
    section, showing all requests going to the `v1` version of all microservices again.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，Kiali中的图表应该看起来像之前*验证所有流量最初都流向微服务的v1版本*部分中的屏幕截图，再次显示所有请求都流向所有微服务的`v1`版本。
- en: This concludes the introduction to the service mesh concept and Istio as an
    implementation of it.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了对服务网格概念及其实现者Istio的介绍。
- en: Before we wrap up the chapter, let’s recap how we can run tests in Docker Compose
    to ensure that the source code of our microservices does not rely on either the
    deployment in Kubernetes or the presence of Istio.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，让我们回顾一下如何使用Docker Compose运行测试，以确保我们的微服务源代码不依赖于Kubernetes的部署或Istio的存在。
- en: Running tests with Docker Compose
  id: totrans-614
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Compose运行测试
- en: As mentioned a few times now, it is important to ensure that the source code
    of the microservices doesn’t become dependent on a platform such as Kubernetes
    or Istio from a functional perspective.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前多次提到的，确保微服务的源代码从功能角度来看不依赖于像Kubernetes或Istio这样的平台是很重要的。
- en: 'To verify that the microservices work as expected without the presence of Kubernetes
    and Istio, run the tests as described in *Chapter 17* (refer to the *Testing with
    Docker Compose* section). Since the default values of the test script, `test-em-all.bash`,
    have been changed, as described previously in the *Running commands to create
    the service mesh* section, the following parameters must be set when using Docker
    Compose: `USE_K8S=false HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443`.
    For example, to run the tests using the default Docker Compose file, `docker-compose.yml`,
    run the following command:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证微服务在没有Kubernetes和Istio的情况下按预期工作，请运行*第17章*中描述的测试（参考*使用Docker Compose进行测试*部分）。由于测试脚本`test-em-all.bash`的默认值已经改变，如*运行创建服务网格的命令*部分之前所述，使用Docker
    Compose时必须设置以下参数：`USE_K8S=false HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443`。例如，要使用默认的Docker
    Compose文件`docker-compose.yml`运行测试，请执行以下命令：
- en: '[PRE87]'
  id: totrans-617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The test script should, as before, begin by starting all containers; it should
    then run the tests and, finally, stop all containers. For details of the expected
    output, see *Chapter 17* (refer to the *Verifying that the microservices work
    without Kubernetes* section).
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 测试脚本应该像以前一样，首先启动所有容器；然后运行测试，最后停止所有容器。有关预期输出的详细信息，请参阅*第17章*（参考*验证微服务在没有Kubernetes的情况下工作*部分）。
- en: After successfully executing the tests using Docker Compose, we have verified
    that the microservices are dependent on neither Kubernetes nor Istio from a functional
    perspective. These tests conclude the chapter on using Istio as a service mesh.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Docker Compose成功执行测试后，我们已经验证了从功能角度来看，微服务既不依赖于Kubernetes也不依赖于Istio。这些测试完成了关于使用Istio作为服务网格的章节。
- en: Summary
  id: totrans-620
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the service mesh concept and Istio, an open
    source implementation of the concept. A service mesh provides capabilities for
    handling challenges in a system landscape of microservices in areas such as security,
    policy enforcement, resilience, and traffic management. A service mesh can also
    be used to make a system landscape of microservices observable by visualizing
    the traffic that flows through the microservices.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了服务网格的概念和Istio，这是该概念的开放源代码实现。服务网格为处理微服务系统景观中的挑战提供了能力，例如安全性、策略执行、弹性和流量管理。服务网格还可以用来使微服务系统景观可观察，通过可视化通过微服务的流量。
- en: For observability, Istio can be integrated with Kiali, Jaeger, and Grafana (more
    on Grafana and Prometheus in *Chapter 20*, *Monitoring Microservices*). When it
    comes to security, Istio can be configured to use a certificate to protect external
    APIs with HTTPS and require that external requests contain valid JWT-based OAuth
    2.0/OIDC access tokens. Finally, Istio can be configured to automatically protect
    internal communication using mutual authentication (mTLS).
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可观察性，Istio可以与Kiali、Jaeger和Grafana（更多关于Grafana和Prometheus的内容请参阅*第20章*，*监控微服务*）集成。当涉及到安全性时，Istio可以被配置为使用证书来保护外部API使用HTTPS，并要求外部请求包含有效的基于JWT的OAuth
    2.0/OIDC访问令牌。最后，Istio可以被配置为使用相互认证（mTLS）自动保护内部通信。
- en: For resilience and robustness, Istio comes with mechanisms for handling retries,
    timeouts, and an outlier detection mechanism similar to a circuit breaker. In
    many cases, it is preferable to implement these resilience capabilities in the
    source code of the microservices, if possible. The ability in Istio to inject
    faults and delays is very useful for verifying that the microservices in the service
    mesh work together as a resilient and robust system landscape. Istio can also
    be used to handle zero-downtime deployments. Using its fine-grained routing rules,
    both canary and blue-green deployments can be performed.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弹性和健壮性，Istio提供了处理重试、超时和类似于熔断器的异常检测机制的机制。在许多情况下，如果可能的话，最好在微服务的源代码中实现这些弹性能力。在Istio中注入故障和延迟的能力对于验证服务网格中的微服务作为一个弹性和健壮的系统景观协同工作非常有用。Istio还可以用来处理零停机时间部署。使用其细粒度的路由规则，既可以进行金丝雀部署，也可以进行蓝绿部署。
- en: One important area that we haven’t covered yet is how to collect and analyze
    log files created by all microservice instances. In the next chapter, we will
    see how this can be done using a popular stack of tools, known as the EFK stack,
    based on Elasticsearch, Fluentd, and Kibana.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有涉及的一个重要领域是如何收集和分析所有微服务实例创建的日志文件。在下一章中，我们将看到如何使用一个流行的工具栈，称为 EFK 工具栈，基于 Elasticsearch、Fluentd
    和 Kibana 来实现这一点。
- en: Questions
  id: totrans-625
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the purpose of a proxy component in a service mesh?
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务网格中代理组件的目的是什么？
- en: What’s the difference between a control plane and a data plane in a service
    mesh?
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务网格中的控制平面和数据平面有什么区别？
- en: What is the `istioctl kube-inject` command used for?
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`istioctl kube-inject` 命令用于什么？'
- en: What is the `minikube tunnel` command used for?
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`minikube tunnel` 命令用于什么？'
- en: What tools are Istio integrated with for observability?
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Istio 集成了哪些工具来实现可观察性？
- en: What configuration is required to make Istio protect communication within the
    service mesh using mutual authentication?
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使 Istio 使用相互认证保护服务网格内的通信，需要哪些配置？
- en: What can the `abort` and `delay` elements in a virtual service be used for?
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虚拟服务中的 `abort` 和 `delay` 元素可以用作什么？
- en: What configuration is required to set up a blue-green deployment scenario?
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置蓝绿部署场景需要哪些配置？
