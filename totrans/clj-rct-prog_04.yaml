- en: Chapter 4. Introduction to core.async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Long gone are the days when programs were required to do only one thing at a
    time. Being able to perform several tasks concurrently is at the core of the vast
    majority of modern business applications. This is where asynchronous programming
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming—and, more generally, concurrency—is about doing more
    with your hardware resources than you previously could. It means fetching data
    from the network or a database connection without having to wait for the result.
    Or, perhaps, reading an Excel spreadsheet into memory while the user can still
    operate the graphical interface. In general, it improves a system's responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at how different platforms handle this style
    of programming. More specifically, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Be introduced to core.async's background and API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solidify our understanding of core.async by re-implementing the stock market
    application in terms of its abstractions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how core.async deals with error handling and backpressure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a brief tour on transducers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous programming and concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different platforms have different programming models. For instance, JavaScript
    applications are single-threaded and have an event loop. When making a network
    call, it is common to register a callback that will be invoked at a later stage,
    when that network call completes either successfully or with an error.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, when we're on the JVM, we can take full advantage of multithreading
    to achieve concurrency. It is simple to spawn new threads via one of the many
    concurrency primitives provided by Clojure, such as futures.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, asynchronous programming becomes cumbersome. Clojure futures don''t
    provide a native way for us to be notified of their completion at a later stage.
    In addition, retrieving values from a not-yet-completed future is a blocking operation.
    This can be seen clearly in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The second call to print dereferences the future, causing the main thread to
    block since it hasn't finished yet. This is why you only see the last print after
    the thread in which the future is running has finished. Callbacks can, of course,
    be simulated by spawning a separate thread to monitor the first one, but this
    solution is clunky at best.
  prefs: []
  type: TYPE_NORMAL
- en: An exception to the lack of callbacks is GUI programming in Clojure. Much like
    JavaScript, Clojure Swing applications also possess an event loop and can respond
    to user input and invoke listeners (callbacks) to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option is rewriting the previous example with a custom callback that
    is passed into the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This time the order of the outputs should make more sense. However, if we return
    the future from this function, we have no way to give it another callback. We
    have lost the ability to perform an action when the future ends and are back to
    having to dereference it, thus blocking the main thread again—exactly what we
    wanted to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Java 8 introduces a new class, `CompletableFuture`, that allows registering
    a callback to be invoked once the future completes. If that's an option for you,
    you can use interop to make Clojure leverage the new class.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might have realized, CES is closely related to asynchronous programming:
    the stock market application we built in the previous chapter is an example of
    such a program. The main—or UI—thread is never blocked by the Observables fetching
    data from the network. Additionally, we were also able to register callbacks when
    subscribing to them.'
  prefs: []
  type: TYPE_NORMAL
- en: In many asynchronous applications, however, callbacks are not the best way to
    go. Heavy use of callbacks can lead to what is known as callback hell. Clojure
    provides a more powerful and elegant solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will explore `core.async`, a Clojure library for
    asynchronous programming, and how it relates to Reactive Programming.
  prefs: []
  type: TYPE_NORMAL
- en: core.async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''ve ever done any amount of JavaScript programming, you have probably
    experienced callback hell. If you haven''t, the following code should give you
    a good idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This style of programming can easily get out of hand—instead of writing more
    natural, sequential steps to achieving a task, that logic is instead scattered
    across multiple callbacks, increasing the developer's cognitive load.
  prefs: []
  type: TYPE_NORMAL
- en: In response to this issue, the JavaScript community released several promises
    libraries that are meant to solve the issue. We can think of promises as empty
    boxes we can pass into and return from our functions. At some point in the future,
    another process might put a value inside this box.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the preceding snippet can be written with promises like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we tend to think in sequences of steps, however, we would like to write
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Even though the code looks synchronous, the behavior should be no different
    from the previous examples. This is exactly what `core.async` lets us do in both
    Clojure and ClojureScript.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating sequential processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `core.async` library is built on an old idea. The foundation upon which
    it lies was first described by Tony Hoare—of Quicksort fame—in his 1978 paper
    *Communicating Sequential Processes* (*CSP*; see [http://www.cs.ucf.edu/courses/cop4020/sum2009/CSP-hoare.pdf](http://www.cs.ucf.edu/courses/cop4020/sum2009/CSP-hoare.pdf)).
    CSP has since been extended and implemented in several languages, the latest of
    which being Google's **Go** programming language.
  prefs: []
  type: TYPE_NORMAL
- en: It is beyond the scope of this book to go into the details of this seminal paper,
    so what follows is a simplified description of the main ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'In CSP, work is modeled using two main abstractions: channels and processes.
    CSP is also message-driven and, as such, it completely decouples the producer
    from the consumer of the message. It is useful to think of channels as blocking
    queues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simplistic approach demonstrating these basic abstractions is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code in the REPL should show us output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In order not to block our program, we start both the consumer and the producer
    in their own threads using a future. Since the consumer was started first, we
    most likely will see its output immediately. However, as soon as it attempts to
    take a value from the channel—or queue—it will block. It will wait for a value
    to become available and will only proceed after the producer is done taking its
    nap—clearly a very important task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compare it with a solution using core.async. First, create a new
    leiningen project and add a dependency on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, type this in the REPL or in your core namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This time we are using a helper function, `prn-with-thread-id`, which appends
    the current thread ID to the output string. I will explain why shortly, but apart
    from that, the output will have been equivalent to the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Structurally, both solutions look fairly similar, but since we are using quite
    a few new functions here, let''s break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chan` is a function that creates a `core.async` channel. As mentioned previously,
    it can be thought of as a concurrent blocking queue and is the main abstraction
    in the library. By default `chan` creates an unbounded channel, but `core.async`
    provides many more useful channel constructors, a few of which we''ll be using
    later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` is another such channel constructor. It gives us a *channel* that
    will *wait* for a given amount of time before returning nil to the taking process,
    closing itself immediately afterward. This is the `core.async` equivalent of **Thread/sleep**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The functions `>!` and `<!` are used to put and take values from a channel,
    respectively. The caveat is that they have to be used inside a `go` block, as
    we will explain later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`go` is a macro that takes a body of expressions—which form a `go` block—and
    creates lightweight processes. This is where the magic happens. Inside a `go`
    block, any calls to `>!` and `<!` that would ordinarily block waiting for values
    to be available in channels are instead parked. Parking is a special type of blocking
    used internally in the state machine of `core.async`. The blog post by Huey Petersen
    covers this state machine in depth (see [http://hueypetersen.com/posts/2013/08/02/the-state-machines-of-core-async/](http://hueypetersen.com/posts/2013/08/02/the-state-machines-of-core-async/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go blocks are the very reason for which I chose to print the thread IDs in our
    example. If we look closely, we'll realize that the last two statements were executed
    in the same thread—this isn't true 100 percent of the time as concurrency is inherently
    non-deterministic. This is a fundamental difference between `core.async` and solutions
    using threads/futures.
  prefs: []
  type: TYPE_NORMAL
- en: Threads can be expensive. On the JVM, their default stack size is 512 kilobytes—configurable
    via the `-Xss` JVM startup option. When developing a highly concurrent system,
    creating thousands of threads can quickly drain the resources of the machine the
    application is running on.
  prefs: []
  type: TYPE_NORMAL
- en: '`core.async` acknowledges this limitation and gives us lightweight processes.
    Internally, they do share a thread pool, but instead of wastefully creating a
    thread per go block, threads are recycled and reused when a put/take operation
    is waiting for a value to become available.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, the thread pool used by `core.async` defaults to the
    number of available processors x 2, + 42\. So, a machine with eight processors
    will have a pool with 58 threads.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is common for `core.async` applications to have dozens of thousands
    of lightweight processes. They are extremely cheap to create.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is a book on Reactive Programming, the question that might be in
    your head now is: can we build reactive applications using `core.async`? The short
    answer is yes, we can! To prove it, we will revisit our stock market application
    and rewrite it using `core.async`.'
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting the stock market application with core.async
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using an example we are familiar with, we are able to focus on the differences
    between all approaches discussed so far, without getting side tracked with new,
    specific domain rules.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the implementation, let's quickly do an overview of how
    our solution should work.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in our previous implementations, we have a service from which we can
    query share prices. Where our approach differs, however, is a direct consequence
    of how `core.async` channels work.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a given schedule, we would like to write the current price to a `core.async`
    channel. This might look like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewriting the stock market application with core.async](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This process will continuously put prices in the `out` channel. We need to
    do two things with each price: display it and display the calculated sliding window.
    Since we like our functions decoupled, we will use two `go` blocks, one for each
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewriting the stock market application with core.async](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Hold on. There seems to be something off with our approach. Once we take a price
    from the output channel, it is not available any longer to be taken by other go
    blocks, so, instead of calculating the sliding window starting with 10, our function
    ends up getting the second value, 20\. With this approach, we will end up with
    a sliding window that calculates a sliding window with roughly every other item,
    depending on how consistent the interleaving between the go blocks is.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, this is not what we want, but it helps us think about the problem a
    little more. The semantics of `core.async` prevent us from reading a value from
    a channel more than once. Most of the time, this behavior is just fine—especially
    if you think of them as queues. So how can we provide the same value to both functions?
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, we will take advantage of another channel constructor
    provided by `core.async` called `broadcast`. As the name implies, `broadcast`
    returns a channel, which, when written to, writes its value into the channels
    passed to it as arguments. Effectively, this changes our high-level picture to
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewriting the stock market application with core.async](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In summary, we will have a go loop writing prices to this broadcast channel,
    which will then forward its values to the two channels from which we will be operating:
    prices and the sliding window.'
  prefs: []
  type: TYPE_NORMAL
- en: With the general idea in place, we are ready to dive into the code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the application code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We already have a project depending on `core.async` that we created in the
    previous section, so we''ll be working off that. Let''s start by adding an extra
    dependency on seesaw to your `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a file called `stock_market.clj` in the `src` directory and add
    this namespace declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This might be a good point to restart your REPL if you haven't done so. Don't
    worry about any functions we haven't seen yet. We'll get a feel for them in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GUI code remains largely unchanged, so no explanation should be necessary
    for the next snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The only difference is that now we have a `sliding-buffer` function that returns
    a window of data. This is in contrast with our original application, where the
    `rolling-avg` function was responsible for both creating the window and calculating
    the average. This new design is more general as it makes this function easier
    to reuse. The sliding logic is the same, however.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have our main application logic using `core.async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let's walk through the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function, `broadcast-at-interval`, is responsible for creating the
    broadcasting channel. It receives a variable number of arguments: a number of
    milliseconds describing the interval, the function representing the task to be
    executed, and a sequence of one of more output channels. These channels are used
    to create the broadcasting channel to which the go loop will be writing prices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have our main function. The `let` block is where the interesting bits
    are. As we discussed in our high-level diagrams, we need two output channels:
    one for prices and one for the sliding window. They are both created in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`prices-ch` should be self-explanatory; however, `sliding-buffer-ch` is using
    a function we haven''t encountered before: `map>`. This is yet another useful
    channel constructor in `core.async`. It takes two arguments: a function and a
    target channel. It returns a channel that applies this function to each value
    before writing it to the target channel. An example will help illustrate how it
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That is, we write a price to the channel and get a sliding window on the other
    end. Finally, we create the two go blocks containing the side effects. They loop
    indefinitely, getting values from both channels and updating the user interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see it in action by running the program from the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Error handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in [Chapter 2](part0021_split_000.html#page "Chapter 2. A Look at Reactive
    Extensions"), *A Look at Reactive Extensions*, we learned how Reactive Extensions
    treats errors and exceptions. It provides a rich set of combinators to deal with
    exceptional cases and are straightforward to use.
  prefs: []
  type: TYPE_NORMAL
- en: Despite being a pleasure to work with, `core.async` doesn't ship with much support
    for exception handling. In fact, if we write our code with only the happy path
    in mind we don't even know an error occurred!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s watch what happens when we put this together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Nothing happens. Zero, zip, zilch, nada.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is precisely the problem with error handling in `core.async`: by default,
    our exceptions are swallowed by the go block as it runs on a separate thread.
    We are left in this state where we don''t really know what happened.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all is lost, however. David Nolen outlined on his blog a pattern for dealing
    with such asynchronous exceptions. It only requires a few extra lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining a helper function and macro—this would probably live in
    a utility namespace we require anywhere we use `core.async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `throw-err` function receives a value and, if it's a subclass of `Throwable`,
    it is thrown. Otherwise, it is simply returned.
  prefs: []
  type: TYPE_NORMAL
- en: The macro `<?` is essentially a drop-in replacement for `<!`. In fact, it uses
    `<!` to get the value out of the channel but passes it to `throw-err` first.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these utilities in place, we need to make a couple of changes, first to
    our `process` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The only change is that we wrapped `get-data` in a `try`/`catch` block. Look
    closely at the `catch` block: it simply returns the exception.'
  prefs: []
  type: TYPE_NORMAL
- en: This is important as we need to ensure the exception gets put into the channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we update our consumer code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This time we use `<?` in place of `<!`. This makes sense as it will rethrow
    any exceptions found in the channel. As a result we can now use a simple `try`/`catch`
    to regain control over our exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Backpressure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main mechanism by which `core.async` allows for coordinating backpressure
    is buffering. `core.async` doesn't allow unbounded buffers as this can be a source
    of bugs and a resource hog.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we are required to think hard about our application's unique needs
    and choose an appropriate buffering strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the simplest form of buffering. It is fixed to a chosen number `n`,
    allowing producers to put items in the channel without having to wait for consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we created a buffer of size `5` and started a `go`
    loop to consume values from it. The `go` loop uses a `timeout` channel to delay
    its start.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we start another go block that puts numbers from 0 to 4 into the result
    channel and prints to the console once it's done.
  prefs: []
  type: TYPE_NORMAL
- en: By then, the first timeout will have expired and we will see the values printed
    to the REPL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s watch what happens if the buffer isn''t large enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This time our buffer size is `2` but everything else is the same. As you can
    see the `go` loop finishes much later as it attempted to put another value in
    the result channel and was blocked/parked since its buffer was full.
  prefs: []
  type: TYPE_NORMAL
- en: As with most things, this might be OK but if we are not willing to block a fast
    producer just because we can't consume its items fast enough, we must look for
    another option.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A dropping buffer also has a fixed size. However, instead of blocking producers
    when it is full, it simply ignores any new items as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As before, we still have a buffer of size two, but this time the producer ends
    quickly without ever getting blocked. The `dropping-buffer` simply ignored all
    items over its limit.
  prefs: []
  type: TYPE_NORMAL
- en: Sliding buffer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A drawback of dropping buffers is that we might not be processing the latest
    items at a given time. For the times where processing the latest information is
    a must, we can use a sliding buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As before, we only get two values but they are the latest ones produced by the
    `go` loop.
  prefs: []
  type: TYPE_NORMAL
- en: When the limit of the sliding buffer is overrun, `core.async` drops the oldest
    items to make room for the newest ones. I end up using this buffering strategy
    most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Transducers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we finish up with our `core.async` portion of the book, it would be unwise
    of me not to mention what is coming up in Clojure 1.7 as well as how this affects
    `core.async`.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, Clojure's latest release is `1.7.0-alpha5`—and
    even though it is an alpha release, a lot of people—myself included—are already
    using it in production.
  prefs: []
  type: TYPE_NORMAL
- en: As such, a final version could be just around the corner and perhaps by the
    time you read this, 1.7 final will be out already.
  prefs: []
  type: TYPE_NORMAL
- en: One of the big changes in this upcoming release is the introduction of `transducers`.
    We will not cover the nuts and bolts of it here but rather focus on what it means
    at a high-level with examples using both Clojure sequences and `core.async` channels.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to know more I recommend Carin Meier's *Green Eggs and Transducers*
    blog post ([http://gigasquidsoftware.com/blog/2014/09/06/green-eggs-and-transducers/](http://gigasquidsoftware.com/blog/2014/09/06/green-eggs-and-transducers/)).
    It's a great place to start.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the official Clojure documentation site on the subject is another
    useful resource ([http://clojure.org/transducers](http://clojure.org/transducers)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started by creating a new leiningen project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open your `project.clj` file and make sure you have the right dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, fire up a REPL session in the project root and require `core.async`,
    which we will be using shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will start with a familiar example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting in Clojure 1.7, the previous example can be written like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The Clojure documentation describes transducers as composable algorithmic transformations.
    Let's see why that is.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the new version, a whole range of the core sequence combinators, such as
    `map` and `filter`, have gained an extra arity: if you don''t pass it a collection,
    it instead returns a transducer.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, `(map inc)` returns a transducer that knows how to
    apply the function `inc` to elements of a sequence. Similarly, `(filter even?)`
    returns a transducer that will eventually filter elements of a sequence. Neither
    of them do anything yet, they simply return functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is interesting because transducers are composable. We build larger and
    more complex transducers by using simple function composition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our transducer ready, we can apply it to a collection in a few
    different ways. For this example, we chose `sequence` as it will return a lazy
    sequence of the applications of the given transducer to the input sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As previously highlighted, this code does not create intermediate sequences;
    transducers extract the very core of the algorithmic transformation at hand and
    abstracts it away from having to deal with sequences directly.
  prefs: []
  type: TYPE_NORMAL
- en: Transducers and core.async
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We might now be asking ourselves "What do transducers have to do with `core.async`?"
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that once we're able to extract the core of these transformations
    and put them together using simple function composition, there is nothing stopping
    us from using transducers with data structures other than sequences!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit our first example using standard `core.async` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should look familiar by now: it''s the `core.async` equivalent of
    the sequence-only version shown earlier. As before, we have unnecessary allocations
    here as well, except that this time we''re allocating channels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the new support for transducers, `core.async` can take advantage of the
    same transformation defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The code remains largely unchanged except we now use the same `xform` transformation
    defined earlier when creating a new channel. It's important to note that we did
    not have to use `core.async` combinators—in fact a lot of these combinators have
    been deprecated and will be removed in future versions of `core.async`.
  prefs: []
  type: TYPE_NORMAL
- en: The functions `map` and `filter` used to define `xform` are the same ones we
    used previously, that is, they are core Clojure functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the next big advantage of using transducers: by removing the underlying
    data structure from the equation via transducers, libraries such as `core.async`
    can reuse Clojure''s core combinators to prevent unnecessary allocation and code
    duplication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s not too far fetched to imagine other frameworks like RxClojure could
    take advantage of transducers as well. All of them would be able to use the same
    core function across substantially different data structures and contexts: sequences,
    channels, and Obervables.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of extracting the essence of computations disregarding their underlying
    data structures is an exciting topic and has been seen before in the Haskell community,
    although they deal with lists specifically.
  prefs: []
  type: TYPE_NORMAL
- en: Two papers worth mentioning on the subject are *Stream Fusion* [11] by Duncan
    Coutts, Roman Leshchinskiy and Don Stewart and *Transforming programs to eliminate
    trees* [12] by Philip Wadler. There are some overlaps so the reader might find
    these interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, I hope to have proved that you can write reactive applications using
    `core.async`. It's an extremely powerful and flexible concurrency model with a
    rich API. If you can design your solution in terms of queues, most likely `core.async`
    is the tool you want to reach for.
  prefs: []
  type: TYPE_NORMAL
- en: This version of the stock market application is shorter and simpler than the
    version using only the standard Java API we developed earlier in this book—for
    instance, we didn't have to worry about thread pools. On the other hand, it feels
    like it is a little more complex than the version implemented using Reactive Extensions
    in [Chapter 3](part0028_split_000.html#page "Chapter 3. Asynchronous Programming
    and Networking"), *Asynchronous Programming and Networking*.
  prefs: []
  type: TYPE_NORMAL
- en: This is because `core.async` operates at a lower level of abstraction when compared
    to other frameworks. This becomes especially obvious in our application as we
    had to worry about creating broadcasting channels, go loops, and so on—all of
    which can be considered incidental complexity, not directly related to the problem
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '`core.async` does, however, provide an excellent foundation for building our
    own CES abstractions. This is what we will be exploring next.'
  prefs: []
  type: TYPE_NORMAL
