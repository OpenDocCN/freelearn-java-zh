<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Node Failure and Post-Mortem Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Node Failure and Post-Mortem Analysis</h1></div></div></div><p>In the previous chapter, we learned how to troubleshoot common performance and reliability issues that come up when using Elasticsearch using case studies with real-world examples. This chapter explores some common causes of node and cluster failures. Specific topics covered are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to determine the root cause of a failure</li><li class="listitem" style="list-style-type: disc">How to take corrective action for node failures</li><li class="listitem" style="list-style-type: disc">Case studies with real-world examples of diagnosing system failures</li></ul></div><div class="section" title="Diagnosing problems"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Diagnosing problems</h1></div></div></div><p>Elasticsearch node failures can manifest in many different ways. Some of the symptoms of node failures <a id="id314" class="indexterm"/>are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A node crashes during heavy data indexing</li><li class="listitem" style="list-style-type: disc">Elasticsearch process stops running for an unknown reason</li><li class="listitem" style="list-style-type: disc">A cluster won't recover from a yellow or red state</li><li class="listitem" style="list-style-type: disc">Query requests time out</li><li class="listitem" style="list-style-type: disc">Index requests time out</li></ul></div><p>When a node in your cluster experiences problems such as these, it can be tempting to just restart Elasticsearch or the node itself and move on like nothing happened. However, without addressing the underlying issue, the problem is likely to resurface in the future. If you encounter scenarios such as the ones just listed, check the health of your cluster in the following manner:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Check the cluster health with Elasticsearch-head or Kopf</li><li class="listitem" style="list-style-type: disc">Check the historical health with Marvel</li><li class="listitem" style="list-style-type: disc">Check for Nagios alerts</li><li class="listitem" style="list-style-type: disc">Check Elasticsearch log files</li><li class="listitem" style="list-style-type: disc">Check system log files</li><li class="listitem" style="list-style-type: disc">Check the system health using command-line tools</li></ul></div><p>These steps will<a id="id315" class="indexterm"/> help diagnose the root cause of problems in your cluster. In this section, we'll look at some underlying causes that lead to node failure, including the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Out-of-memory errors</li><li class="listitem" style="list-style-type: disc">Not enough system memory available</li><li class="listitem" style="list-style-type: disc">Resource contention</li><li class="listitem" style="list-style-type: disc">Running out of disk space</li></ul></div><div class="section" title="OutOfMemoryError exceptions"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec71"/>OutOfMemoryError exceptions</h2></div></div></div><p>If a node <a id="id316" class="indexterm"/>throws an <code class="literal">OutOfMemoryError</code>, the<a id="id317" class="indexterm"/> immediate fix is to restart it. However, it's not always obvious when or why a node encounters this error. Symptoms include the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Shard failures</li><li class="listitem" style="list-style-type: disc">Search query failures</li><li class="listitem" style="list-style-type: disc">Indexing failures</li></ul></div><p>Often, there will be no immediate symptoms at all. Unfortunately, checking Elasticsearch-head, Marvel, and Bigdesk won't tell you outright that an <code class="literal">OutOfMemoryError</code> exception has occurred, but they can give us some warning signs that one <span class="emphasis"><em>may</em></span> have occurred. To be sure that an <code class="literal">OutOfMemoryError</code> exception has occurred, check the Elasticsearch logs.</p><div class="section" title="Shard failures"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec18"/>Shard failures</h3></div></div></div><p>One sign that <a id="id318" class="indexterm"/>an <code class="literal">OutOfMemoryError</code> exception has occurred is the appearance of shard failures in query responses. This <a id="id319" class="indexterm"/>response indicates shard failures in the <code class="literal">_shards.failed</code> key, and it describes the failure in <code class="literal">_shards.failures</code>.</p><p>The following <a id="id320" class="indexterm"/>example query shows what a shard failure looks like in a query response:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XGET http://elasticsearch-node-01:9200/twitter/_search?size=0</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    "_shards": {</strong></span>
<span class="strong"><strong>        "failed": 1,</strong></span>
<span class="strong"><strong>        "failures": [</strong></span>
<span class="strong"><strong>            {</strong></span>
<span class="strong"><strong>                "index": "twitter",</strong></span>
<span class="strong"><strong>                "reason": "NodeDisconnectedException[[elasticsearch-node-03][inet[elasticsearch-node-03/192.168.56.113:9300]][indices:data/read/search[phase/query]] disconnected]",</strong></span>
<span class="strong"><strong>                "shard": 1,</strong></span>
<span class="strong"><strong>                "status": 500</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        ],</strong></span>
<span class="strong"><strong>        "successful": 2,</strong></span>
<span class="strong"><strong>        "total": 3</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "hits": {</strong></span>
<span class="strong"><strong>        ...</strong></span>
<span class="strong"><strong>        "total": 10803</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Note that even<a id="id321" class="indexterm"/> though a shard failed in this query, it <a id="id322" class="indexterm"/>still returned results. However, because there are three total shards and only two returned data successfully, the query results are not representative of all data in the index.</p><p>Sometimes, if the cluster is in a red state, for example, the <code class="literal">_shards</code> object will indicate fewer successful shards than the total available shards but won't report an error. Take a look a the following code where <code class="literal">_shards.successful</code> is less than <code class="literal">_shards.total</code>, but <code class="literal">_shards.failed</code> is set to <code class="literal">0</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    "_shards": {</strong></span>
<span class="strong"><strong>        "failed": 0,</strong></span>
<span class="strong"><strong>        "successful": 2,</strong></span>
<span class="strong"><strong>        "total": 3</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "hits": {</strong></span>
<span class="strong"><strong>        ...</strong></span>
<span class="strong"><strong>        "total": 10803</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>In both of these cases, the <code class="literal">hits.total</code> value is only representative of approximately two thirds of our actual total data count.</p><p>When we encounter shard failures or shards that don't return data successfully, it's a good idea to use Elasticsearch-head to check the state of our cluster. Elasticsearch-head may look something<a id="id323" class="indexterm"/> like the following:</p><div class="mediaobject"><img src="graphics/B03798_07_01.jpg" alt="Shard failures"/><div class="caption"><p>Shards relocating in Elasticsearch-head</p></div></div><p>In this screenshot, we <a id="id324" class="indexterm"/>can see that all shards are now available, but the cluster is still recovering and there aren't any shards assigned to <code class="literal">elasticsearch-node-01</code>. At this point, we may also notice that the cluster takes a very long time to return to a green state, or possibly never returns to a green state. This problem may be due to a node that is out of heap space failing to relocate one of its shards to another node with more memory.</p><p>Next, open Elasticsearch-kopf to get a more detailed view of our nodes:</p><div class="mediaobject"><img src="graphics/B03798_07_02.jpg" alt="Shard failures"/><div class="caption"><p>Shards relocating in Elasticsearch-kopf</p></div></div><p>In<a id="id325" class="indexterm"/> Elasticsearch-kopf, we see high heap usage <a id="id326" class="indexterm"/>on <code class="literal">elasticsearch-node-01</code> and <code class="literal">elasticsearch-node-02</code>, which is a good indicator that an <code class="literal">OutOfMemoryError</code> exception has occurred. Checking the logs, we confirm that an <code class="literal">OutOfMemoryError</code> was thrown on <code class="literal">elasticsearch-node-01</code>:</p><div class="mediaobject"><img src="graphics/B03798_07_03.jpg" alt="Shard failures"/><div class="caption"><p>Examining the Elasticsearch logs shows an OutOfMemoryError</p></div></div><p>Additionally, we<a id="id327" class="indexterm"/> see several other Exceptions<a id="id328" class="indexterm"/> recorded in the log file that start appearing after the <code class="literal">OutOfMemoryError</code>, like in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03798_07_04.jpg" alt="Shard failures"/><div class="caption"><p>Additional errors in the Elasticsearch log related to OutOfMemoryError</p></div></div><p>Continuing to <a id="id329" class="indexterm"/>examine the log file, we see an <a id="id330" class="indexterm"/>error indicating shard failure due to the node running out of memory:</p><div class="mediaobject"><img src="graphics/B03798_07_05.jpg" alt="Shard failures"/><div class="caption"><p>Shard failure errors in the Elasticsearch log</p></div></div></div><div class="section" title="Slow queries"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec19"/>Slow queries</h3></div></div></div><p>Slow queries <a id="id331" class="indexterm"/>are another sign that an <code class="literal">OutOfMemoryError</code> has<a id="id332" class="indexterm"/> occurred. In the previous example, checking the slow-log file with the Unix <code class="literal">less</code> command on <code class="literal">elasticsearch-node-02 </code>shows<a id="id333" class="indexterm"/> the following:</p><div class="informalexample"><pre class="programlisting">less my_elasticsearch_cluster_index_search_slowlog.log.2016-04-28</pre></div><div class="mediaobject"><img src="graphics/B03798_07_06.jpg" alt="Slow queries"/><div class="caption"><p>Slow queries may indicate that an error has occurred</p></div></div><p>Checking <a id="id334" class="indexterm"/>the Elasticsearch log on <code class="literal">elasticsearch-node-02</code>, we <a id="id335" class="indexterm"/>can verify that an <code class="literal">OutOfMemoryError</code> was captured:</p><div class="mediaobject"><img src="graphics/B03798_07_07.jpg" alt="Slow queries"/><div class="caption"><p>Verifying that Elasticsearch threw an exception</p></div></div></div><div class="section" title="Resolving OutOfMemoryError exceptions"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec20"/>Resolving OutOfMemoryError exceptions</h3></div></div></div><p>As mentioned<a id="id336" class="indexterm"/> earlier, when you see an <code class="literal">OutOfMemoryError</code>, it's best to restart the node to prevent further exceptions. However, this is only a temporary fix. It's important to also fix the underlying issue that is causing the error. Refer to  <a class="link" href="ch06.html" title="Chapter 6. Troubleshooting Performance and Reliability Issues">Chapter 6</a>, <span class="emphasis"><em>Troubleshooting Performance and Reliability Issues</em></span>, for more strategies about solving <code class="literal">OutOfMemoryError</code> exceptions. A few strategies are listed, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Limit the size of the field data cache</li><li class="listitem" style="list-style-type: disc">Enable circuit breakers for the field data cache</li><li class="listitem" style="list-style-type: disc">Adjust the size and frequency of bulk data inserts</li><li class="listitem" style="list-style-type: disc">Reduce the number of total shards</li><li class="listitem" style="list-style-type: disc">Ensure that <code class="literal">ES_HEAP_SIZE</code> is properly set</li><li class="listitem" style="list-style-type: disc">Ensure that there is enough physical memory available to the machine</li></ul></div><p>After seeing errors in the log, we can correlate their timestamps with Marvel to see what kind of activity <a id="id337" class="indexterm"/>was going on when the error occurred. For example, let's suppose that we see the following <code class="literal">OutOfMemoryError</code>:</p><div class="mediaobject"><img src="graphics/B03798_07_08.jpg" alt="Resolving OutOfMemoryError exceptions"/><div class="caption"><p>An OutOfMemoryError exception occurred at 4/29/2016 15:26:39</p></div></div><p>We can check Marvel's activity around the <code class="literal">4/29/2016 15:26:39</code> timeframe. In this case, we'll set it from <code class="literal">2016-04-29 15:25:00</code> to <code class="literal">2016-04-29 15:27:30</code>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03798_07_09.jpg" alt="Resolving OutOfMemoryError exceptions"/><div class="caption"><p>Change the date range in Marvel</p></div></div><p>Although<a id="id338" class="indexterm"/> we see no search activities taking place in the index at the time of the collapse, a modest <span class="strong"><strong>Indexing Rate</strong></span> is followed by a drop-off in indexing activity:</p><div class="mediaobject"><img src="graphics/B03798_07_10.jpg" alt="Resolving OutOfMemoryError exceptions"/><div class="caption"><p>Investigating with Marvel</p></div></div><p>The <a id="id339" class="indexterm"/>drop-off probably occurred after the <code class="literal">OutOfMemoryError</code>, and a heavy indexing load may have caused the error.</p><p>As <code class="literal">OutOfMemoryError</code> exceptions may not occur very frequently, it can be difficult to know for sure whether applying the fixes that we implemented successfully resolved the issue. To ensure that the issue is completely solved, it's best to find a way to reliably recreate the error. Then, adjust Elasticsearch configuration settings and load until you don't see the issue any more. It's often possible to recreate the issue by standing up a simple single-node cluster with a similar configuration and load as a node in the primary cluster. In the previous example, we might try to verify that data bulk loading caused the exception by indexing documents at a similar rate into a single-node test cluster in a controlled environment.</p></div></div><div class="section" title="Elasticsearch process crashes"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec72"/>Elasticsearch process crashes</h2></div></div></div><p>If the Elasticsearch process unexpectedly stops running, it may be because the operating system <a id="id340" class="indexterm"/>killed it. In these cases, the Elasticsearch log <a id="id341" class="indexterm"/>file may not have any useful information about the error, and we instead have to check the <code class="literal">syslog</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo tail -n200 /var/log/syslog</strong></span>
<span class="strong"><strong>View the last 200 lines of /var/log/syslog</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Apr 29 14:56:00 elasticsearch-node-01 kernel: [39935.321257] Out of memory: Kill process 5969 (java) score 446 or sacrifice child</strong></span>
<span class="strong"><strong>Apr 29 14:56:00 elasticsearch-node-01 kernel: [39935.321343] Killed process 5969 (java) total-vm:2361960kB, anon-rss:441676kB, file-rss:14392kB</strong></span>
</pre></div><p>This can happen if Elasticsearch tries to claim more system memory than the memory available, and this is often a result of an improperly set <code class="literal">ES_HEAP_SIZE</code> or resource contention with other processes. If your cluster experiences this issue and there are other memory-heavy processes running on the cluster, it may be a good idea to move these processes off the Elasticsearch cluster. To verify that Elasticsearch was forcibly stopped by the operating system, check the <code class="literal">syslog</code> file at <code class="literal">/var/log/syslog</code>:</p><div class="mediaobject"><img src="graphics/B03798_07_11.jpg" alt="Elasticsearch process crashes"/><div class="caption"><p>Operating-system killed Elasticsearch process due to running out of memory</p></div></div><p>Take the<a id="id342" class="indexterm"/> following line:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>Apr 29 14:55:34 elasticsearch-node-01 kernel: [39909.742047] Out of memory: Kill process 5878 (java) score 446 or sacrifice child</strong></span>
</pre></div><p>This line indicates<a id="id343" class="indexterm"/> that the operating system killed the Elasticsearch process. In this case, we won't see any corresponding log entries in the Elasticsearch log files.</p></div><div class="section" title="Disk space"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec73"/>Disk space</h2></div></div></div><p>When a<a id="id344" class="indexterm"/> node runs out of disk space, it will stay in the cluster and can<a id="id345" class="indexterm"/> still handle index and search requests, but it offloads its shards to other nodes in the cluster. As the cluster reallocates shards, queries may run slow or timeout. Once shards are reallocated to other nodes in the cluster, you'll likely see some performance degradation as the cluster is operating with one less data node.</p><p>A node running out of disk space can be dangerous if all nodes are configured with the same amount of space. If one node runs out of space, it's likely that other nodes in the cluster are running low on disk space too. Once the cluster finishes reallocating shards to other nodes in the cluster, it can cause these nodes to run out of space as well. This causes a chain reaction<a id="id346" class="indexterm"/> that ultimately results in the entire cluster going <a id="id347" class="indexterm"/>down.</p><p>We can check whether a node is running low on disk space using Kopf or Marvel or by configuring a Nagios alert. Additionally, we'll see an error in the Elasticsearch log related to low disk space, as seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03798_07_12.jpg" alt="Disk space"/><div class="caption"><p>Kopf shows that <code class="literal">elasticsearch-node-01</code> is low on disk space</p></div></div><div class="section" title="Resolving the issue"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec21"/>Resolving the issue</h3></div></div></div><p>Disk space<a id="id348" class="indexterm"/> issues can be grouped into two categories:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Elasticsearch has too much data loaded into it and is filling up the disk.</li><li class="listitem">Something other than Elasticsearch data is filling up the disk, for example, a large log file.</li></ol></div><p>To resolve the first category issues, one solution is to increase the node's storage capacity by adding another drive or volume to the node and configuring Elasticsearch to use the space. For example, if we mount additional storage at <code class="literal">/data</code>, update the <code class="literal">elasticsearch.yml</code> configuration file to use it as follows:</p><div class="informalexample"><pre class="programlisting">path.data: /var/lib/elasticsearch,/data</pre></div><p>Restart Elasticsearch on the node. The node will then distribute its data across the two data directories.</p><p>For the second category, if the cause is external to Elasticsearch, removing the offending files to clear up disk space will be enough to get Elasticsearch going again. There's no need to restart the node.</p><p>Some additional measures we can take to reduce disk space usage are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Add additional nodes to the cluster.</li><li class="listitem" style="list-style-type: disc">Decrease shard replication; for example, from two replicas to one replica.</li><li class="listitem" style="list-style-type: disc">Ensure that individual shards don't grow too big by breaking large indices into smaller indices. For example, instead of storing all Twitter data in one index, create a new index every month to store new data.</li><li class="listitem" style="list-style-type: disc">Enable data compression (refer to <a class="link" href="ch06.html" title="Chapter 6. Troubleshooting Performance and Reliability Issues">Chapter 6</a>, <span class="emphasis"><em>Troubleshooting Performance and Reliability Issues</em></span> ).</li></ul></div></div></div></div></div>
<div class="section" title="Reviewing some case studies"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec42"/>Reviewing some case studies</h1></div></div></div><p>This section<a id="id349" class="indexterm"/> discusses some real-world scenarios of Elasticsearch node failure and how to address them.</p><div class="section" title="The ES process quits unexpectedly"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec74"/>The ES process quits unexpectedly</h2></div></div></div><p>A few weeks<a id="id350" class="indexterm"/> ago we noticed in Marvel that the Elasticsearch process was down on one of our nodes. We restarted Elasticsearch on this node, and everything seemed to return to normal. However, checking Marvel later on in the week, we notice that the node is down again. We decide to look at the Elasticsearch log files, but don't notice any exceptions. As we don't see anything in the Elasticsearch log, we suspect that the operating system may have killed Elasticsearch. Checking <code class="literal">syslog</code> at <code class="literal">/var/log/syslog</code>, we see the error:</p><div class="informalexample"><pre class="programlisting">Out of memory: Kill process 5969 (java) score 446 or sacrifice child</pre></div><p>This verifies<a id="id351" class="indexterm"/> that the operating system killed Elasticsearch because the system was running out of memory. We check the Elasticsearch configuration and don't see any issues. This node is configured in the same way as the other nodes in the cluster. Next, we check for resource contention with other processes by running the <code class="literal">top</code> command and get the following results:</p><p> </p><div class="mediaobject"><img src="graphics/B03798_07_13.jpg" alt="The ES process quits unexpectedly"/><div class="caption"><p>top showing resource contention</p></div></div><p>
</p><p>It looks like a MySQL server is also running on this node and is taking up a lot of system memory. We suspect that resource contention with MySQL is probably what's causing the operating <a id="id352" class="indexterm"/>system to kill Elasticsearch. We are able to move the MySQL database to its own dedicated host, and after a few weeks with no more memory issues, we can conclude that this resolved the problem.</p></div><div class="section" title="Query requests slow and timing out"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec75"/>Query requests slow and timing out</h2></div></div></div><p>Users<a id="id353" class="indexterm"/> of our corporate Elasticsearch-backed <a id="id354" class="indexterm"/>web application have started reporting that the search functionality is slow and sometimes doesn't return at all. We are able to verify this by running a few searches on the web application, and we decide to use Kopf to investigate the issue. In Kopf, we notice that the disk indicator is red for one of our nodes, <code class="literal">elasticsearch-node-01</code>, as seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03798_07_14.jpg" alt="Query requests slow and timing out"/><div class="caption"><p>Kopf shows that <code class="literal">elasticsearch-node-01</code> is low on disk space</p></div></div><p>The red <span class="strong"><strong>disk</strong></span> indicator for <code class="literal">elasticsearch-node-01</code> means that we are low on disk space. Also, all of the node's shards have been reallocated to other nodes. Checking the log of <code class="literal">elasticsearch-node-01</code> at <code class="literal">/var/log/elasticsearch/my_elasticsearch_cluster.log</code>, we confirm that the disk is full by <a id="id355" class="indexterm"/>seeing the <span class="strong"><strong>no space left on device</strong></span> message. We are able to resolve the issue by adding an additional hard drive to all nodes<a id="id356" class="indexterm"/> in the cluster and configuring Elasticsearch to use the new space in the <code class="literal">elasticsearch.yml</code> file.</p><p>To prevent this issue in the future, we decide to install Nagios using the instructions found in 
<a class="link" href="ch05.html" title="Chapter 5. System Monitoring">Chapter 5</a>, <span class="emphasis"><em>System Monitoring</em></span>, in order to send out e-mail alerts the next time the disk space gets low.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec43"/>Summary</h1></div></div></div><p>This chapter looked into how to diagnose node failures, determine the root cause of the problem, and apply corrective action. Some key things we learned are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Many errors, from shard failures to slow query performance, are caused by <code class="literal">OutOfMemoryError</code> exceptions</li><li class="listitem" style="list-style-type: disc">Running out of disk space on one node can cause other nodes to run out of disk space as well when shards are reallocated</li><li class="listitem" style="list-style-type: disc">Running Elasticsearch alongside other services that require a lot of memory can result in the operating system killing Elasticsearch to free up memory</li></ul></div><p>The next chapter will talk about Elasticsearch 5.0, the next major release of the platform, and it will give you an overview of the various new monitoring tools that will accompany the Elasticsearch 5.0 release.</p></div></body></html>