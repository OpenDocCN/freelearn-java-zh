<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Image Segmentation and Feature Extraction with ImageJ"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Image Segmentation and Feature Extraction with ImageJ</h1></div></div></div><p>The previous chapter looked at processing images to view and correct imperfections in acquisition. This chapter will introduce techniques for segmenting images and extracting features that are relevant for processing and analysis. The following topics will be covered in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Image segmentation</li><li class="listitem" style="list-style-type: disc">Morphological processing</li><li class="listitem" style="list-style-type: disc">Image filtering and convolution</li><li class="listitem" style="list-style-type: disc">Feature extraction</li></ul></div><div class="section" title="Image segmentation"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec21"/>Image segmentation</h1></div></div></div><p>For many steps <a id="id137" class="indexterm"/>in image analysis, it is important to split the image into two separate (non-overlapping) components. These components are usually labeled as background and foreground. Generally speaking, the background is the part of the image we are not directly interested in when we analyze the image. We normally restrict our analysis to parts of the image that are deemed as the foreground. This splitting into two components is <a id="id138" class="indexterm"/>called segmentation and is primarily based on pixel intensity. This is important if you wish to count and measure a number of unique objects of a specific type or measure the intensity of a single complex object while excluding the background from the measurement.</p><div class="section" title="Image thresholding"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Image thresholding</h2></div></div></div><p>To achieve the <a id="id139" class="indexterm"/>split of an image into background and <a id="id140" class="indexterm"/>foreground, we will set a threshold value. Values below this threshold will be classified as one group, while pixels with higher or equal values will be classified as another group. In general, the background in fluorescent images contains values close to black (that is, a dark background), while brightfield images have background values closer to white (a light background). The output of thresholding is an image called a mask in ImageJ, which is a binary image. Its pixels have only two values (0 and 255).</p><p>We will look at how to perform basic thresholding on a grayscale image first. After that, we will look at the possibilities for thresholding a color image. The difference between these two image types <a id="id141" class="indexterm"/>stems from the fact that a color image <a id="id142" class="indexterm"/>does not have an easy way of setting a threshold. Each pixel contains three values (red, green, and blue), and a single threshold value does not segment the image in useful ways generally.</p><div class="section" title="Thresholding grayscale images"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec13"/>Thresholding grayscale images</h3></div></div></div><p>We will start by <a id="id143" class="indexterm"/>taking a grayscale image <a id="id144" class="indexterm"/>from the sample images and <a id="id145" class="indexterm"/>segmenting it. For this example, we will use the <span class="strong"><strong>Blobs</strong></span> image. Thresholding would be the first step if you wanted to measure the size of each individual blob as well as get a count of the number of blobs in the image. Note that for small images such as this example, counting could be done by hand. However, if you need to do this for a large number of images, this method of counting by hand would be very tedious.</p><p>To set a threshold, go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Adjust</strong></span> | <span class="strong"><strong>Threshold…</strong></span> or press <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>T</em></span>. The threshold dialog will open with a few options:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_01.jpg" alt="Thresholding grayscale images"/></div><p>For fluorescent images, the <span class="strong"><strong>Dark background</strong></span> checkbox needs to be selected, while for brightfield images, it needs to be deselected (unless you use darkfield illumination methods). The methods available can be set in the drop-down list on the left-hand side. The default method is based on the <span class="strong"><strong>IsoData</strong></span> method. The <span class="strong"><strong>IsoData</strong></span> method determines the value of the threshold based on the following procedure:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Take an <a id="id146" class="indexterm"/>initial value for the <a id="id147" class="indexterm"/>threshold <span class="emphasis"><em>T</em></span></li><li class="listitem" style="list-style-type: disc">Calculate the <a id="id148" class="indexterm"/>average intensity of the background (<span class="emphasis"><em>BG</em></span>) and the foreground (<span class="emphasis"><em>FG</em></span>) pixels based on the value of <span class="emphasis"><em>T</em></span></li><li class="listitem" style="list-style-type: disc">If the average of <span class="emphasis"><em>BG</em></span> and <span class="emphasis"><em>FG</em></span> in step <span class="emphasis"><em>ii</em></span> is not equal to <span class="emphasis"><em>T</em></span>, increment the threshold value <span class="emphasis"><em>T</em></span> and repeat step <span class="emphasis"><em>ii</em></span></li></ul></div><p>For more <a id="id149" class="indexterm"/>information and references on thresholding methods, refer to the Fiji website at <a class="ulink" href="http://fiji.sc/Auto_Threshold#Available_methods">http://fiji.sc/Auto_Threshold#Available_methods</a> for an overview. The drop-down list on the right-hand side gives the option to show the effect of the thresholding. When <span class="strong"><strong>Red</strong></span> is selected, the foreground selection is displayed as red while the background stays in grayscale.</p><p>Once you have the threshold set, you can create a binary image by pressing <span class="strong"><strong>Apply</strong></span> in the threshold window or by going to <span class="strong"><strong>Edit</strong></span> | <span class="strong"><strong>Selection</strong></span> | <span class="strong"><strong>Create Mask</strong></span>. The former method will modify your original image, while the latter method will open a new window with the thresholded image. The red parts (that is, the above-threshold values) in the original image are now white, while the non-red parts (that is, the below-threshold values) in the original image are now black. Sometimes, the threshold is not perfect and has gaps or holes in places where the signal was not even. You will learn how to deal with these issues in the <a id="id150" class="indexterm"/>
<span class="emphasis"><em>Morphological processing</em></span> section. The <a id="id151" class="indexterm"/>three stages of this process are shown in the following image:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_02.jpg" alt="Thresholding grayscale images"/></div><p>The image in the left panel is the original image. The middle panel shows the auto threshold with the <a id="id152" class="indexterm"/>foreground areas in red. The right panel shows the resulting mask that was created based on the threshold.</p></div><div class="section" title="Thresholding color images"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec14"/>Thresholding color images</h3></div></div></div><p>As mentioned <a id="id153" class="indexterm"/>earlier, color images are more complicated to segment. When talking about color images, it is important to distinguish <a id="id154" class="indexterm"/>between RGB images and multichannel stacks. The latter can be thresholded just fine using the techniques described in the previous section. Multichannel stacks can be seen as individual grayscale images that were given a specific LUT to appear colored. RGB images, on the other hand, are a little more complex. If the image only contains pixels that are red, green, or blue, you could convert the image into a multichannel image.</p><p>To segment an RGB image with more colors, you need to transform the image to a different color space. To select the foreground based on color, the HSB color space is more convenient. As we saw in the <a class="link" href="ch02.html" title="Chapter 2. Basic Image Processing with ImageJ">Chapter 2</a>, <span class="emphasis"><em>Basic Image Processing with ImageJ</em></span>, the color information in HSB images is a separate channel encoded in grayscale values. When you want to set a threshold on an <a id="id155" class="indexterm"/>RGB color image in ImageJ and Fiji, the <a id="id156" class="indexterm"/>
<span class="strong"><strong>Threshold Color</strong></span> dialog opens automatically:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_03.jpg" alt="Thresholding color images"/></div><p>By default, it opens in the HSB color space, where the top chart shows the distribution of the hue channel. The two <a id="id157" class="indexterm"/>sliders underneath indicate which colors you wish to select. In this case, orange is selected. The second panel shows the controls for saturation. As the sliders are far to the right, we only select bright orange colors. Finally, the third panel at the bottom shows the controls for brightness, which is set for a wide range of brightness values starting at the mid-level. This example shows you how to select a specific range of <a id="id158" class="indexterm"/>colors. In this case, the threshold was set to select the hair of the clown in the <span class="strong"><strong>Clown</strong></span> sample image:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_04.jpg" alt="Thresholding color images"/></div><p>As you can see, the threshold is not perfect. There are small areas on the cheek and near the nose that are also within the threshold. Furthermore, there are also gaps in the area that are part of the hair, especially around the right eye and in the top-right corner of the image.</p><p>The threshold method list has the same methods as the standard ImageJ threshold dialog, and it works only on the <a id="id159" class="indexterm"/>brightness channel. The <span class="strong"><strong>Original</strong></span> button is similar to the <span class="strong"><strong>Reset</strong></span> option in the grayscale threshold dialog. The <span class="strong"><strong>Select</strong></span> button will convert the thresholded region into a selection. The <span class="strong"><strong>Sample</strong></span> button will use a selected portion of the image to generate a threshold based on the hue, saturation, and brightness channels in that area.</p></div></div></div></div>
<div class="section" title="Morphological processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Morphological processing</h1></div></div></div><p>After segmenting <a id="id160" class="indexterm"/>the image into the two components, you are left with a mask or binary image. As was clear from the examples, these masks are not always suitable for direct measurement. Imperfections in the image may result in gaps in objects or small discontinuities in structures. Also, some areas might be detected as foreground when they are actually not really objects of interest. You could manually correct this by converting the missing pixels to white or black in order to include or exclude them, respectively. In some cases, this might be the only possible recourse. However, in many cases, there are a few processing steps available that can fix these problems in a systematic way. These steps are called morphological processing, which we will examine in <a id="id161" class="indexterm"/>greater detail in the next section.</p><div class="section" title="Morphological operators"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"/>Morphological operators</h2></div></div></div><p>ImageJ <a id="id162" class="indexterm"/>supports the two main principal operators for morphological processing: <span class="strong"><strong>erode</strong></span> and <span class="strong"><strong>dilate</strong></span>. It also has functions for filling holes, skeletonizing, and watershedding binary images, which will be discussed in a later section. These functions will be explained in the upcoming sections using a few basic examples.</p><div class="section" title="Erode and dilate"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec15"/>Erode and dilate</h3></div></div></div><p>To start with, we <a id="id163" class="indexterm"/>will look at the basic operators <span class="strong"><strong>erode</strong></span> and <span class="strong"><strong>dilate</strong></span>. The <a id="id164" class="indexterm"/>erode operator takes a foreground pixel (<span class="strong"><strong>FG</strong></span>) and looks at the surrounding pixels in a 3 x 3 neighborhood. Based on the number of FG pixels, the pixel will be changed to a background pixel (<span class="strong"><strong>BG</strong></span>), or it stays as an FG pixel. The dilate operator functions in the opposite way. ImageJ determines whether a pixel will be changed or not based on the binary options, which can be set by going to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Binary</strong></span> | <span class="strong"><strong>Options…</strong></span>:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_05.jpg" alt="Erode and dilate"/></div><p>
<span class="strong"><strong>Iterations</strong></span> determines how many times the operator is repeated, and <span class="strong"><strong>Count</strong></span> determines the number of pixels used for the threshold that determines whether a pixel is switched or not. <span class="strong"><strong>EDM output</strong></span> determines where the results from distance mapping functions are written. When <span class="strong"><strong>Overwrite</strong></span> is selected, the distance mapping overwrites the pixels in your mask image. <span class="strong"><strong>Pad edges when eroding</strong></span> determines whether pixels will be eroded when they are located on the edge of the image. When selected, there will be no erosion at the edges of the image.</p><p>For the following example, I will assume that the number of iterations is set to <code class="literal">1</code>, the count to <code class="literal">1</code>, and the black background is unchecked.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the <code class="literal">4909_03b_binary.tif</code> image in ImageJ. It is available on the Packt website.</li><li class="listitem">Set a threshold for <a id="id165" class="indexterm"/>the image using <code class="literal">0</code> for the top slider <a id="id166" class="indexterm"/>and <code class="literal">75</code> for the bottom slider, using the default method. Leave the dark background unchecked.</li><li class="listitem">Select <span class="strong"><strong>Edit</strong></span> | <span class="strong"><strong>Selection</strong></span> | <span class="strong"><strong>Create Mask</strong></span> to generate a new image or press <span class="strong"><strong>Apply</strong></span> in the threshold dialog to overwrite the original image.</li><li class="listitem">Finally, select the masked image and press <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>I</em></span> to invert the image so that it has a white background. You should now have the following result (the original is on the left-hand side and the mask is on the right-hand side):<div class="mediaobject"><img src="graphics/Insert_image_4909_03b_06.jpg" alt="Erode and dilate"/></div></li></ol></div><p>When you look closely at the masked image, you will appreciate that there are a few small problems. Most notably, the letter <span class="strong"><strong>a</strong></span> in <span class="strong"><strong>Binary</strong></span> and <span class="strong"><strong>ImageJ</strong></span> is broken in three disconnected parts. Also, the letters <span class="strong"><strong>p</strong></span> and <span class="strong"><strong>g</strong></span> are not entirely complete and have a break but are not completely disconnected. For humans, this is not a large problem. We can easily fill in the gaps in our minds and read the text. Computers, on the other hand, may have a more difficult time trying to decipher the text. We will now look at the effect of binary operators on this mask. You will also see how this may solve our problem of fragmented letters.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select the masked image and go to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Binary</strong></span> | <span class="strong"><strong>Options…</strong></span> to open the options dialog. This will show a few more options now that we have a masked image, most notably the <span class="strong"><strong>Do</strong></span> drop-down menu and the <span class="strong"><strong>Preview</strong></span> checkbox.</li><li class="listitem">Zoom in on one of the letters a using the magnification tool or the <span class="emphasis"><em>+</em></span> key on the keyboard.</li><li class="listitem">Select <span class="strong"><strong>Erode</strong></span> from the <span class="strong"><strong>Do</strong></span> drop-down menu and check the <span class="strong"><strong>Preview</strong></span> checkbox, but don't press OK!</li></ol></div><p>In preview mode, you will notice that the entire mask went white, and the text completely disappeared when you selected the erode operator. When you increase the value in the <span class="strong"><strong>Count</strong></span> field, you will start to notice that parts of the text will start to come back. With a value of <code class="literal">3</code>, some pixels are visible, while a value of <code class="literal">7</code> or <code class="literal">8</code> gives you most of the text unscathed. When the value is set to <code class="literal">8</code>, the only victim of the erode operation is the isolated pixel of the letter <span class="strong"><strong>a</strong></span>. All the other pixels remain intact, but this isolated pixel is removed. This is one of the most used applications of the erode operator—removing isolated single pixels caused by noise in your image.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>When using <span class="strong"><strong>Erode</strong></span>, isolated pixels can be removed, but the entire mask becomes smaller, reducing the area that we would like to measure. Using <span class="strong"><strong>Dilate</strong></span> directly after an <span class="strong"><strong>Erode</strong></span> operation (or using <span class="strong"><strong>Open</strong></span>), we can remove isolated pixels while still preserving the area we would like to measure. Once an FG pixel is lost because of <span class="strong"><strong>Erode</strong></span>, it can never return, no matter how many times you use <span class="strong"><strong>Dilate</strong></span>!</p></div></div><p>Now, select <span class="strong"><strong>Dilate</strong></span> from the <span class="strong"><strong>Do</strong></span> drop-down menu, set <span class="strong"><strong>Count</strong></span> to <code class="literal">1</code> again, and see what the effect is. When you use the dilate operator, the text will become thicker, but it also fills in the gaps in the letters. This outcome is much more useful. However, there are a few problems here. The bottom <a id="id167" class="indexterm"/>tail of the letter <span class="strong"><strong>g</strong></span> as well as the opening of the <a id="id168" class="indexterm"/>letter <span class="strong"><strong>e</strong></span> are now filled in. By increasing <span class="strong"><strong>Count</strong></span> to <code class="literal">2</code>, this problem is ameliorated, and the letter <span class="strong"><strong>e</strong></span> as well as the tail of the letter <span class="strong"><strong>g</strong></span> are open again. When <span class="strong"><strong>Count</strong></span> is <code class="literal">2</code>, <span class="strong"><strong>Dilate</strong></span> fixes the problem of the fragments. However, our letters are now much thicker, and some letters have merged. Take a look at <span class="strong"><strong>eJ</strong></span> in the word <span class="strong"><strong>ImageJ</strong></span>. The tail of the letter <span class="strong"><strong>e</strong></span> is directly connected to the tail of the letter <span class="strong"><strong>J</strong></span>. We would now like to take two steps. First, we want to dilate the mask to fill the gaps, and then, we want to erode the mask to get rid of the connected letters. Executing the operators in succession on the mask can perform this combination. First, we will dilate the mask, and then, we will erode the result. However, there is also a special function that performs both steps in this order called <span class="strong"><strong>Close</strong></span>. If you want to perform the steps in the opposite order (first erode and then dilate), you can use the <span class="strong"><strong>Open</strong></span> function.</p><p>When you select the <span class="strong"><strong>Close</strong></span> option in the drop-down menu, you can see the result of this action. The result is OK, but not that great. The fact that the result in this case is not that great is caused by the fact that we used different values of <span class="strong"><strong>Count</strong></span> for each step. The <span class="strong"><strong>Dilate</strong></span> operator worked best when we used <code class="literal">2</code>, while the <span class="strong"><strong>Erode</strong></span> operator worked best when we used a value of <code class="literal">7</code> or <code class="literal">8</code>. For this example, it is better to perform the <span class="strong"><strong>Dilate</strong></span> and <span class="strong"><strong>Erode</strong></span> operators in succession with specific values for count in each step. In the following images, the <span class="strong"><strong>Close</strong></span> operation was used with a value of <code class="literal">2</code> for count, while the succession of <span class="strong"><strong>Dilate</strong></span> and <span class="strong"><strong>Erode</strong></span> were performed using <code class="literal">2</code> and <code class="literal">5</code>, respectively (the left-hand-side image is the original mask):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_07.jpg" alt="Erode and dilate"/></div><p>As can be seen in the middle and right panels, both methods have their advantages and drawbacks. The <span class="strong"><strong>Close</strong></span> <a id="id169" class="indexterm"/>operation (middle panel) filled in the letter <span class="strong"><strong>e</strong></span>, and <a id="id170" class="indexterm"/>there is still an isolated pixel that is part of the letter <span class="strong"><strong>a</strong></span>. However, the letters themselves still have good details. The manual successive dilate/erode steps (right panel) preserved the hole in the letter e as well as the details of the letter <span class="strong"><strong>g</strong></span>. However, the details of the letter <span class="strong"><strong>a</strong></span> are less pronounced. Specifically, the <span class="emphasis"><em>serif</em></span> (the small hook at the bottom-right corner of the letter <span class="strong"><strong>a</strong></span>) is completely lost.</p></div><div class="section" title="Skeletonize and watershed"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec16"/>Skeletonize and watershed</h3></div></div></div><p>After processing the <a id="id171" class="indexterm"/>mask using <span class="strong"><strong>Dilate</strong></span>, <span class="strong"><strong>Erode</strong></span>, <span class="strong"><strong>Open</strong></span>, and <a id="id172" class="indexterm"/>
<span class="strong"><strong>Close</strong></span>, we may want to reduce the mask to the most basic features. The core of the letters that we segmented earlier is formed by the strokes. Each character consists of a set of strokes in different directions, which together define the character. In ImageJ, we can recreate these strokes using the skeletonize function, which can be found by going to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Binary</strong></span> | <span class="strong"><strong>Skeletonize</strong></span> in the menu or by selecting it from the <span class="strong"><strong>Do</strong></span> drop-down menu in the binary options dialog. <span class="strong"><strong>Skeletonize</strong></span> looks at each pixel's neighbors and removes a pixel if it is flanked by other FG pixels. This leads to reducing the mask to a single pixel width mask.</p><p>When applied to the result of the masks after our close (left panel) and successive dilate/erode (right panel) operations, the results are as follows:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_08.jpg" alt="Skeletonize and watershed"/></div><p>The result from the <span class="strong"><strong>Close</strong></span> operation (left panel) is not very satisfactory. The letter <span class="strong"><strong>e</strong></span> is unrecognizable and looks more like a letter <span class="strong"><strong>c</strong></span>. The successive dilate/erode operation (right panel) has a slightly better result due to the skeletonize operations. Although the letters look a bit funny and wobbly, all the important strokes are present.</p><p>The watershed function <a id="id173" class="indexterm"/>separates objects that are touching. We will <a id="id174" class="indexterm"/>look at the effect of this operation using the blobs sample image. You could apply it to the text example. However, the problem in the text example was the fact that objects needed to be joined not separated.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the <span class="strong"><strong>Blobs</strong></span> image from the sample images.</li><li class="listitem">Set a threshold using the <span class="strong"><strong>Default</strong></span> method, leave the <span class="strong"><strong>Dark background</strong></span> box unchecked, and click on <span class="strong"><strong>Apply</strong></span> to create the mask.</li><li class="listitem">Now, go to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Binary</strong></span> | <span class="strong"><strong>Watershed</strong></span> from the menu.</li></ol></div><p>The result will look as follows, with the original mask to the left and the result of the watershed operation to the right:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_09.jpg" alt="Skeletonize and watershed"/></div><p>As can be seen, four blobs were split into two separate objects each. This operation looks for areas that are pinched. When an object has a narrow part between two thicker parts (similar to the middle of an 8), it will be separated along the narrow part. Notice, however, that this does not work for some of the blobs (indicated by blue rectangles). When there is no pinch in the outline, the watershed algorithm will not split the object. This would be very useful <a id="id175" class="indexterm"/>if you wish to quantify the number of <a id="id176" class="indexterm"/>objects when you know that objects can overlap. However, you may run into problems if you wish to measure object size or area. As the overlapping area cannot be measured accurately, the measurements for overlapping objects will underestimate the actual size. This problem can be solved by assuming that the objects have a regular shape, such as an oval, but this might not hold in many cases. In ImageJ, this latter assumption can be used using the particle analyzer, which will be discussed in <a class="link" href="ch05.html" title="Chapter 5. Basic Measurements with ImageJ">Chapter 5</a>, <span class="emphasis"><em>Basic Measurements with ImageJ</em></span>. The best way to solve this problem is by making sure that the amount of overlap is reduced, which might require changes in your sample preparation or acquisition.</p></div></div></div>
<div class="section" title="Image filtering"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Image filtering</h1></div></div></div><p>The previous section <a id="id177" class="indexterm"/>looked at ways to segment the image in the foreground and background using a threshold. It also looked at ways to derive a result suitable for analysis with the use of morphological operators. The morphological operators were used to clean the results of the threshold by removing isolated pixels. In most real-life applications, these isolated pixels are due to the effect of noise in your image-acquisition system. Some of the noise can be removed using the techniques described in the previous chapter, but this may not remove all the noise. In this section, we will look at ways to use filters to remove noise and prepare images to create masks. Filtering can be a step that is inserted before thresholding and morphological processing. If your images are high contrast and have extremely low levels of noise, this might not be required. However, this is relatively rare.</p><p>There are two categories of filtering, depending on the type of domain that is used for filtering. Images can be seen in two different domains: the <span class="strong"><strong>spatial</strong></span> and the <span class="strong"><strong>frequency</strong></span> domain. The most recognizable to humans is the spatial domain. This is an image as we recognize it from our cameras. Each location in space has a value, and the combination of an area filled with closely spaced locations with differing values forms an image. If all the values were identical, the image would appear uniform as a single color or gray. In the case of digital images, locations are specified by the pixels that form the image, and the value is represented as a gray value or an RGB value.</p><p>The frequency domain is less recognizable to humans. An image in the frequency domain is represented by the rate of change of values or frequency. Humans recognize frequency in terms of wavelengths of light. Light with a higher frequency will appear blue/violet, while light with a lower frequency will appear orange/red. However, in image processing, the frequency of an image is <a id="id178" class="indexterm"/>determined by the way pixel intensities change within an image, and not necessarily the color of the pixels. I will start with filtering in the frequency domain, as this is more complex. Note that most of the filtering for image processing is done in the spatial domain with excellent results.</p><div class="section" title="Filtering in the frequency domain"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec33"/>Filtering in the frequency domain</h2></div></div></div><p>Filtering of <a id="id179" class="indexterm"/>images is based on a transformation technique described by Joseph Fourier in 1822. This transformation takes data in one domain and transforms it to a different domain and back again. For image processing, the transformation goes from the <span class="emphasis"><em>spatial domain</em></span> to the <span class="emphasis"><em>frequency domain</em></span>. The spatial domain considers points to be in a space, either a plane (2D) or a volume (3D). Each location of a point has an intensity value, which changes over different locations for most images. The rate at which the intensities change along a dimension determines the frequency. Take a look at this artificial image:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_10.jpg" alt="Filtering in the frequency domain"/></div><p>If we look at the profile of intensity along the width of the image as well as along the height of the image in the middle, we would get the following results (horizontal profile to the left and vertical profile to the right):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_11.jpg" alt="Filtering in the frequency domain"/></div><p>As is obvious from these plots, there is a clear difference in the rate of intensity changes. The horizontal profile (left) shows fast changes in intensity over distance, while the vertical profile (right) shows no change whatsoever. Another way of describing this is that the frequency <a id="id180" class="indexterm"/>along the horizontal profile is large, while it is low on the vertical axis.</p><p>The Fourier transform will calculate the frequencies in the spatial domain and plot them as frequencies in the <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> direction. The idea of the transform is based on the fact that any signal can be described as the (infinite) sum of harmonic functions (that is, sines and cosines) with different frequencies. These frequencies are represented by the coefficients for the sines and cosines, which are displayed as gray values by ImageJ in an image. We will obtain the Fourier transform that is, <span class="strong"><strong>Fast Fourier Transform</strong></span> (<span class="strong"><strong>FFT</strong></span>) of the artificial image by going to <a id="id181" class="indexterm"/>
<span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>FFT</strong></span> | <span class="strong"><strong>FFT</strong></span> from the menu:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_12.jpg" alt="Filtering in the frequency domain"/></div><p>The center (that is, origin) of the image has a frequency of 0, while the horizontal line through the origin represents the frequencies along the <span class="emphasis"><em>x</em></span> axis of the image. The values in the quadrants determine the frequencies along the diagonals of the image. Values close to the center of the image represent low frequencies, while values close to the edge represent higher frequencies. As <a id="id182" class="indexterm"/>there is only a change in frequency along the <span class="emphasis"><em>X</em></span> coordinates of the image, the transformed image shows only vertical lines. If the pattern had been diagonal, the lines in the transformed image would also be diagonal.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>The dashed appearance of the lines in the transformed image is caused by the fact that the input image was not square. The width is 512, but the height only 128 pixels. If the image were a 512 x 512 square, the transformed image would only show a row of dots along the <span class="emphasis"><em>x</em></span> axis through the origin. If you halved the height of the sample image, the dashes become roughly twice as long.</p></div></div><p>When we use the <a id="id183" class="indexterm"/>FFT image as input, we can create the original image when we select <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>FFT</strong></span> | <span class="strong"><strong>Inverse FFT</strong></span> from the menu:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_13.jpg" alt="Filtering in the frequency domain"/></div><p>Note that since we used the FFT and immediately the inversed FFT, we actually did not apply any filtering. The image before and after the transform is identical. This is a very desirable feature of the transform, because this means that the transform is lossless. No information was lost during the process. To actually filter the image, we need to modify the transformed image by modifying the pixel values in the transformed image.</p><p>To apply some (crude) filtering, we will take the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select the transformed image.</li><li class="listitem">Go to <span class="strong"><strong>Edit</strong></span> | <span class="strong"><strong>Selection</strong></span> | <span class="strong"><strong>Specify…</strong></span> from the menu and enter the following values: <span class="strong"><strong>Width</strong></span> as <code class="literal">255</code>, <span class="strong"><strong>Height</strong></span> as <code class="literal">255</code>, <span class="strong"><strong>X coordinate</strong></span> as <code class="literal">0</code>, and <span class="strong"><strong>Y coordinate</strong></span> as <code class="literal">0</code>. Then, press <span class="strong"><strong>OK</strong></span>.</li><li class="listitem">Open the color picker by going to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Color</strong></span> | <span class="strong"><strong>Color Picker</strong></span> from the menu or pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>K</em></span> on the keyboard.</li><li class="listitem">Make sure that the foreground is set to black by clicking on the little icon of a black-and-white square in the bottom-right corner of the color picker.</li><li class="listitem">Now, fill the selection that we specified with black by going to <span class="strong"><strong>Edit</strong></span> | <span class="strong"><strong>Fill</strong></span> or by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>F</em></span>.</li><li class="listitem">Repeat steps 2 and 5, but now, specify the selection to have the <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> coordinates of <code class="literal">257</code>.</li><li class="listitem">Finally, go to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>FFT</strong></span> | <span class="strong"><strong>Inverse FFT</strong></span> from the menu to generate the filtered image.</li></ol></div><p>If you followed the <a id="id184" class="indexterm"/>instructions, your FFT image would look as follows:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_14.jpg" alt="Filtering in the frequency domain"/></div><p>The inverse FFT <a id="id185" class="indexterm"/>image will look as follows:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_15.jpg" alt="Filtering in the frequency domain"/></div><p>As can be seen in the inverse FFT image, there are now significant differences before and after the manipulations. For instance, the frequency in the vertical direction is different. Each bar now changes intensity as you go from top to bottom. Try the same routine, but this time, specify the <a id="id186" class="indexterm"/>selection using the following parameters in step 2, and skip step 6:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Width</strong></span>: <code class="literal">64</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Height</strong></span>: <code class="literal">512</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>X coordinate</strong></span>: <code class="literal">272</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Y coordinate</strong></span>: <code class="literal">0</code></li></ul></div><p>After filling in the selection with black and calculating the inverse FFT, the image will appear as shown here in the right panel. You have specifically removed a small subset of the frequencies from the frequency domain. After calculating the inverse FFT, you will get the following result (zoomed area in the top-left corner):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_16.jpg" alt="Filtering in the frequency domain"/></div><p>On the left-hand side, you see the original image, and on the right-hand side is the filtered image. As the region selected and removed entailed the lower frequencies, the higher frequencies remain resulting in a greater change in the intensity values along the horizontal axis.</p><p>As this example is very artificial, the results here are not necessarily practical for analysis. However, if you have an image that is corrupted by high-frequency intensity changes (for example, imaging noise), you know you have to remove the frequencies at the edge of the FFT <a id="id187" class="indexterm"/>transform. On the other hand, if you have a slow gradient of intensity changes (for example, uneven illumination), you need to remove the low frequencies in the FFT transform. Using black to remove the frequencies, you're creating a filter that excludes the frequencies covered by your selection. If you filled the selection with white, you would include all the selected frequencies covered by your selection. In the next section, we will look at filtering in the spatial domain, which is slightly more intuitive to apply.</p></div><div class="section" title="Image filtering in the spatial domain"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec34"/>Image filtering in the spatial domain</h2></div></div></div><p>Filtering in the <a id="id188" class="indexterm"/>spatial domain involves using a filter, usually referred to as a <a id="id189" class="indexterm"/>
<span class="strong"><strong>kernel</strong></span>. This filter transforms every pixel using a method called <span class="strong"><strong>convolution</strong></span>. Convolution involves taking a center pixel with a small array of neighboring pixels (usually 3 x 3) and multiplying the intensities with a set of weights as defined in the kernel. The sum of the multiplications will become the new pixel intensity for the center pixel. In the following example, there is a part of an image (left), the kernel (middle), and the outcome of the convolution (right):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_17.jpg" alt="Image filtering in the spatial domain"/></div><p>The center pixel (highlighted in orange) and the surrounding pixels in a 3 x 3 neighborhood are multiplied with the kernel (middle). The result of the convolution is shown on the right-hand side. The value of the center pixel used to be 128, but is now 78 after convolution. The <a id="id190" class="indexterm"/>kernel shown in this example is a simple smoothing filter (also called a <span class="strong"><strong>box</strong></span> filter). The main effect of this filter is that it averages pixels, resulting in a blurring of the image. The following image is a detail from the <span class="strong"><strong>Boats</strong></span> sample image, before (left) and after (right) convolving with the box filter:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_18.jpg" alt="Image filtering in the spatial domain"/></div><p>When you change the filter size to 7 x 7, the effect of the smoothing will be much stronger, as more pixels in the neighborhood will influence the value of the center pixel. When using the box filter with a size of 7 x 7, each weight will be equal to 1/49. The result for the same image will look as follows:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_19.jpg" alt="Image filtering in the spatial domain"/></div><p>Notice that the filtering has almost completely smoothed the letters, making them unrecognizable. The box filter functions as a low-pass filter—only low frequencies in the image will remain. This is caused by the fact that fast changes in intensity will be smoothed out more aggressively by the box filter than the low frequencies. Even though this filtering happens in the spatial domain, the effects are also reflected in the frequency domain.</p><p>To recreate the preceding <a id="id191" class="indexterm"/>images, follow these steps on the <span class="strong"><strong>Boats</strong></span> image from the sample images:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Filters</strong></span> | <span class="strong"><strong>Convolve…</strong></span> from the menu and remove everything in the text field in the dialog that opens.</li><li class="listitem">Type three 1s separated by a space and press enter. Repeat this twice.</li><li class="listitem">Make sure that the <span class="strong"><strong>Normalize kernel</strong></span> checkbox is selected and press <span class="strong"><strong>OK</strong></span>.</li><li class="listitem">The image now looks a little less sharp as it has been convolved with a 3 x 3 box filter.</li></ol></div><p>If you want to convolve with a 7 x 7 box filter, just type seven rows of seven 1s separated by spaces, and repeat the steps on a newly opened Boats image to see the effect of kernel size.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>When applying the kernel on an image that was already convolved, the effect will be larger than when the image wasn't convolved yet. When using the 3 x 3 box filter twice in succession, the effect will be the same as running a 3 x 3 box filter with weights of 1/81 per pixel (<span class="emphasis"><em>1/9 * 1/9</em></span>).</p></div></div><p>The result of filtering using a kernel depends on the values of the weights that you specify and the kernel's size. Typically, there are two types of kernel that can be separated based on the sum of their weights. When the sum of the weights in a kernel adds up to 1, the kernel is called normalized. The advantage of a normalized kernel is that the result of the convolution will not exceed the maximum pixel value allowed by the bit depth of the image. When the <span class="strong"><strong>Normalized kernel</strong></span> checkbox is checked in the <span class="strong"><strong>Convolve</strong></span> dialog, ImageJ will automatically take care of the normalization. Non-normalized kernels can exhibit clamping artifacts. When the sum of the kernel exceeds 1, the result of convolution may exceed the maximum allowable value (that is, 255 for 8-bit images). When this happens, the value after the transformation will be clamped at the maximum value. This clamping may result in artifacts such as blocks of white pixels.</p><p>The box filter is a very simple filter, but it does not discriminate any features in the image. It averages evenly in all directions. Other filters exist that actually enhance certain features in your image. An example of such a filter is the <span class="strong"><strong>Mexican hat</strong></span> filter. This filter emphasizes the center pixel over the surrounding pixels:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_20.jpg" alt="Image filtering in the spatial domain"/></div><p>The Mexican hat filter is shaped a little like a sombrero, hence the name. It makes areas with high contrast become bright, while areas of uniform intensity become darker. Applied on the <span class="strong"><strong>Boats</strong></span> <a id="id192" class="indexterm"/>image, the result looks as follows:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_21.jpg" alt="Image filtering in the spatial domain"/></div><p>What stands out immediately is the fact that the edges of the letters are greatly emphasized. This makes sense because the contrast is relatively strong. These are black letters on a mostly even light-gray background. The only edges that are not clearly recognizable are the points where the letters touch each other and the places where the ropes hide parts of the letters. You can imagine that this filter might also work well for the text example and the segmentation of the blobs mentioned earlier. It basically functions as a high-pass filter. Only regions with fast changes in intensity are emphasized, while regions with slow changes in intensity (that is, low frequencies) are reduced.</p><p>Besides entering kernel weights manually, ImageJ and Fiji also have some common filter kernels that can be accessed by going to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Filters</strong></span>. Two of the most often used filter kernels include the <span class="strong"><strong>Gaussian Blur…</strong></span> and the <span class="strong"><strong>Mean…</strong></span> filter. The latter is identical to the box filter. The former is similar to the Mexican hat filter. However, it does not use negative values in the kernel. The <span class="strong"><strong>Gaussian Blur</strong></span> filter smoothens the image just like a box filter does, but it does it in a more gradual way. The advantage of <span class="strong"><strong>Gaussian Blur</strong></span> is that it can have fewer artifacts when you apply it. The response of the filter in the frequency domain is also better, making it possible to combine spatial and frequency domain filtering.</p><p>Next, we will look at some operators that can be used to detect specific features in an image that may be relevant <a id="id193" class="indexterm"/>for processing. These operators also use convolution, but they have different properties compared to the filters described earlier.</p></div></div>
<div class="section" title="Feature extraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Feature extraction</h1></div></div></div><p>As we saw in the <a id="id194" class="indexterm"/>earlier sections, filters can be used to isolate different frequencies using filters. By convolving an image with a Mexican hat filter, high frequencies are preserved, while using the box filter has the opposite effect. The difference between the filters in this section and the filters in the previous section is in specificity. The Mexican hat filter had no preference for direction. When there was an edge with sharp contrast (quick change in intensity), the filter had a strong effect. However, sometimes, you are only interested in a specific type of edge. Let's assume that we only want to detect vertical edges. The Mexican hat filter will give us all the edges in all directions, not just the vertical ones. This will be the topic of the following section.</p><div class="section" title="Edge detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec35"/>Edge detection</h2></div></div></div><p>To detect only <a id="id195" class="indexterm"/>vertical edges, we need to create a kernel that emphasizes pixels that are in a vertical orientation. The following kernels can detect different orientations of edges:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03b_22.jpg" alt="Edge detection"/></div><p>To perform the <span class="strong"><strong>Sobel edge</strong></span> detection, you can use the <span class="strong"><strong>Find Edges</strong></span> command from the <span class="strong"><strong>Process</strong></span> menu. This command will run both the horizontal and the vertical Sobel kernel over the image.</p><p>Finally, there is also the <span class="strong"><strong>Canny procedure</strong></span> for edge detection that involves five steps. This procedure was developed by John F. Canny and consists of the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Apply Gaussian <a id="id196" class="indexterm"/>smoothing to remove noise.</li><li class="listitem">Detect gradients in the image using edge detection.</li><li class="listitem">Thin edges using convolution with a kernel such as the Mexican hat.</li><li class="listitem">Apply two different thresholds to determine weak and strong edges.</li><li class="listitem">Remove weak edges that are not connected to strong edges.</li></ol></div><p>The first three steps involve using different kernels for smoothing, edge detection, and edge thinning in succession. Note that the first step is only required if the image is degraded by noise. If the contrast is high and noise is absent, this step can be skipped. This step is also the weakest point of the procedure. Both noise and edges are forms of high-frequency signals, and the Gaussian filter smoothens both equally. If noise is present, techniques that <a id="id197" class="indexterm"/>reduce the noise specifically while leaving the edges intact should show great improvement.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/>Summary</h1></div></div></div><p>In this chapter, we looked at ways to separate an image into foreground and background. We saw different methods to set the threshold in grayscale and color images. We applied filtering in the spatial and frequency domains to aid in cleaning the image and extracting edges for further processing. All these steps will help us when we wish to measure objects in the image, which is the topic of the next chapter.</p></div></body></html>