- en: Kafka Streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, instead of using the Kafka Java API for producers and consumers
    as in previous chapters, we are going to use Kafka Streams, the Kafka module for
    stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Streams in a nutshell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Streams project setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding and running the Java `PlainStreamsProcessor`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling out with Kafka Streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding and running the Java `CustomStreamsProcessor`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding and running the Java `AvroStreamsProcessor`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding and running the Late `EventProducer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding and running the Kafka Streams processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Streams in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka Streams is a library and part of Apache Kafka, used to process streams
    into and from Kafka. In functional programming, there are several operations over
    collections, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupBy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`join`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The success of streaming platforms such as Apache Spark, Apache Flink, Apache
    Storm, and Akka Streams is to incorporate these stateless functions to process
    data streams. Kafka Streams provides a DSL to incorporate these functions to manipulate
    data streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka Streams also has stateful transformations; these are operations related
    to the aggregation that depend on the state of the messages as a group, for example,
    the windowing functions and support for late arrival data. Kafka Streams is a
    library, and this means that Kafka Streams applications can be deployed by executing
    your application jar. There is no need to deploy the application on a server,
    which means you can use any application to run a Kafka Streams application: Docker,
    Kubernetes, servers on premises, and so on. Something wonderful about Kafka Streams
    is that it allows horizontal scaling. That is, if it runs in the same JVM, it
    executes multiple threads, but if several instances of the application are started,
    it can run several JVMs to scale out.'
  prefs: []
  type: TYPE_NORMAL
- en: The Apache Kafka core is built in Scala; however, Kafka Streams and KSQL are
    built in Java 8\. Kafka Streams is packaged in the open source distribution of
    Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Project setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to modify the `kioto` project. We have to add the dependencies
    to `build.gradle`, as shown in *Listing 6.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6.1: Kioto Gradle build file for Kafka Streams'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the examples in this chapter, we also need the dependencies for Jackson.
    To use Kafka Streams, we just need one dependency, which is given in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To use Apache Avro with Kafka Streams, we add the serializers and deserializers
    as given in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following lines are needed to run a Kafka Streams application as a jar.
    The build generates a fat jar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The directory tree structure of the project should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Java PlainStreamsProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/plain` directory, create a file called `PlainStreamsProcessor.java`
    with the contents of *Listing 6.2,* shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6.2: PlainStreamsProcessor.java'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the magic happens inside the `process()` method. The first step in a Kafka
    Streams application is to get a `StreamsBuilder` instance, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `StreamsBuilder` is an object that allows building a topology. A topology
    in Kafka Streams is a structural description of a data pipeline. The topology
    is a succession of steps that involve transformations between streams. A topology
    is a very important concept in streams; it is also used in other technologies
    such as Apache Storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `StreamsBuilder` is used to consume data from a topic. There are other
    two important concepts in the context of Kafka Streams: a `KStream`, a representation
    of a stream of records, and a `KTable`, a log of the changes in a stream (we will
    see KTables in detail in [Chapter 7](cafc7e52-647d-4f97-8a42-965ff94e678c.xhtml),
    *KSQL*). To obtain a `KStream` from a topic, we use the `stream()` method of the
    `StreamsBuilder`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There is an implementation of the `stream()` method that just receives the topic
    name as a parameter. But, it is good practice to use the implementation where
    we can also specify the serializers, as in this example we have to specify the
    Serializer for the key and the Serializer for the value for the `Consumed` class;
    in this case, both are strings.
  prefs: []
  type: TYPE_NORMAL
- en: Don't let the serializers be specified through application-wide properties,
    because the same Kafka Streams application might read from several data sources
    with different data formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have obtained a JSON stream. The next step in the topology is to obtain
    the `HealthCheck` object stream, and we do so by building the following Stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: First, note that we are using the `mapValues()` method, so as in Java 8, the
    method receives a lambda expression. There are other implementations for the `mapValues()`
    method, but here we are using the lambda with just one argument (`v->`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mapValues()` here could be read as follows: for each element in the input
    Stream, we are applying a transformation from the JSON object to the `HealthCheck`
    object, and this transformation could raise an `IOException`, so we are catching
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Recapitulating until the moment, in the first transformation, we read from the
    topic a stream with (`String, String`) pairs. In the second transformation, we
    go from the value in JSON to the value in `HealthCheck` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third step, we are going to calculate the `uptime` and send it to the
    `uptimeStream`, as shown in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using the `map()` method, also as in Java 8, the method receives
    a lambda expression. There are other implementations for the `map()` method; here,
    we are using a lambda with two arguments (`(k, v)->`)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `map()` here could be read as follows: for each element in the input stream,
    we extract the tuples (key, value). We are using just the value (anyway, the key
    is `null`), cast it to `HealthCheck`, extract two attributes (the start time and
    the `SerialNumber`), calculate the `uptime`, and return a new `KeyValue` pair
    with (`SerialNumber`, `uptime`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to write these values into the `uptimes` topic, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, I will emphasize it until I get tired: it is widely recommended to declare
    the data types of our Streams. Always stating, in this case for example, that
    key value pairs are of type (`String, String`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Read from the input topic key value pairs of type (`String, String`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deserialize each JSON object to `HealthCheck`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the `uptimes`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the `uptimes` to the output topic in key value pairs of type (`String,
    String`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it is time to start the Kafka Streams engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting it, we need to specify the topology and two properties, the
    broker and the application ID, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that the serializers and deserializers are just explicitly defined when
    reading from and writing to topics. So, we are not tied application-wide to a
    single data type, and we can read from and write to topics with different data
    types, as happens continuously in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Also with this good practice, between different topics, there is no ambiguity
    about which Serde to use.
  prefs: []
  type: TYPE_NORMAL
- en: Running the PlainStreamsProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the project, run this command from the `kioto` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is correct, the output is something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to run a console consumer for the `uptimes` topic, shown
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From the IDE, run the main method of the `PlainStreamsProcessor`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the IDE, run the main method of the `PlainProducer` (built in previous
    chapters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Scaling out with Kafka Streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To scale out the architecture as promised, we must follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run a console consumer for the `uptimes` topic, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the application jar from the command line, shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is when we verify that our application really scales out.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a new command-line window, we execute the same command, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If we remember the theory of [Chapter 1](0bde3875-cd30-435c-9b32-b96fccb2e6d0.xhtml),
    *Configuring Kafka*, when we created our topic, we specified that it had four
    partitions. This nice message from Kafka Streams is telling us that the application
    was assigned to partitions two and three of our topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read that the first instance was using the four partitions, then when
    we ran the second instance, it entered a state where the partitions were reassigned
    to consumers; to the first instance was assigned two partitions: `healthchecks-0`
    and `healthchecks-1`.'
  prefs: []
  type: TYPE_NORMAL
- en: And this is how Kafka Streams smoothly scale out. Remember that all this works
    because the consumers are part of the same consumer group and are controlled from
    Kafka Streams through the `application.id` property.
  prefs: []
  type: TYPE_NORMAL
- en: We must also remember that the number of threads assigned to each instance of
    our application can also be modified by setting the `num.stream.threads` property.
    Thus, each thread would be independent, with its own producer and consumer. This
    ensures that the resources of our servers are used in a more efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Java CustomStreamsProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summing up what has happened so far, in previous chapters we saw how to make
    a producer, a consumer, and a simple processor in Kafka. We also saw how to do
    the same with a custom SerDe, how to use Avro, and the Schema Registry. So far
    in this chapter, we have seen how to make a simple processor with Kafka Streams.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use all our knowledge so far to build a `CustomStreamsProcessor`
    with Kafka Streams to use our own SerDe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/custom` directory, create a file called `CustomStreamsProcessor.java`
    with the contents of *Listing 6.3*, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6.3: CustomStreamsProcessor.java'
  prefs: []
  type: TYPE_NORMAL
- en: All the magic happens inside the `process()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in a Kafka Streams application is to get a `StreamsBuilder`
    instance, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can reuse the `Serdes` built in the previous chapters. The following code
    creates a `KStream` that deserializes the values of the messages as `HealthCheck`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `serdeFrom()` method of the `Serde` class dynamically wraps our `HealthCheckSerializer`
    and `HealthCheckDeserializer` into a single `HealthCheck Serde`.
  prefs: []
  type: TYPE_NORMAL
- en: We can reuse the `Serdes` built on the previous chapters. The following code
    creates a `KStream` that deserializes the values of the messages as `HealthCheck`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `StreamsBuilder` is used to consume data from a topic. The same as in previous
    sections, to obtain a `KStream` from a topic, we use the `stream()` method of
    the `StreamsBuilder`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We use the implementation where we can also specify the serializers, as in this
    example, we have to specify the serializer for the key, and the serializer for
    the value for the `Consumed` class, in this case the key is a String (always `null`),
    and the serializer for the value is our new `customSerde`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The magic here is that the rest of the code of the `process()` method remains
    the same as in the previous section; it is also shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Running the CustomStreamsProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the project, run this command from the `kioto` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is correct, the output is something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to run a console consumer for the `uptimes` topic, shown
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: From our IDE, run the main method of the `CustomStreamsProcessor`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From our IDE, run the main method of the `CustomProducer` (built in previous
    chapters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Java AvroStreamsProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section we will see how to use all this power gathered together: Apache
    Avro, Schema Registry, and Kafka Streams.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to use Avro format in our messages, as we did in previous
    chapters. We consumed this data by configuring the Schema Registry URL and using
    the Kafka Avro deserializer. For Kafka Streams, we need to use a Serde, so we
    added the dependency in the Gradle build file, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This dependency has the `GenericAvroSerde` and specific `avroSerde` explained
    in previous chapters. Both Serde implementations allow us to work with Avro records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/avro` directory, create a file called `AvroStreamsProcessor.java`
    with the contents of *Listing 6.4,* shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6.4: AvroStreamsProcessor.java'
  prefs: []
  type: TYPE_NORMAL
- en: One main difference with the previous code listings is the specification of
    the Schema Registry URL. The same as before, the magic happens inside the `process()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in a Kafka Streams Application is to get a `StreamsBuilder`
    instance, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The seconds step is to get an instance of the `GenericAvroSerde` object, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As we are using the `GenericAvroSerde`, we need to configure it with the Schema
    Registry URL (as in previous chapters); it is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `configure()` method of `GenericAvroSerde` receives a map as a parameter;
    as we just need a map with a single entry, we used the singleton map method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create a `KStream` using this Serde. The following code generates
    an Avro Stream that contains `GenericRecord` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note how we request the name of the `AvroTopic`, and that we have to specify
    the serializer for the key and the serializer for the value for the `Consumed`
    class; in this case, the key is a `String` (always `null`), and the serializer
    for the value is our new `avroSerde`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deserealize the values for the `HealthCheck` Stream, we apply the same methods
    used in previous chapters inside the lambda of the `mapValues()` method with one
    argument (`v->`), shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And again, the rest of the code of the `process()` method remains the same
    as in previous sections, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the code could be cleaner: we could create our own Serde that includes
    the deserialization code, so we can directly deserialize Avro Objects into `HealthCheck`
    Objects. To achieve this, this class has to extend the generic Avro deserializer.
    We leave this as an exercise for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Running the AvroStreamsProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the project, run this command from the `kioto` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is correct, the output is something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to run a console consumer for the `uptimes` topic, shown
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: From our IDE, run the main method of the `AvroStreamsProcessor`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From our IDE, run the main method of the `AvroProducer` (built in previous chapters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Late event processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we talked about message processing, but now we will talk about events.
    An event in this context is something that happens at a particular time. An event
    is a message that happens at a point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand events, we have to know the timestamp semantics. An
    event always has two timestamps, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event time**: The point in time when the event happened at the data source'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing time**: The point in time when the event is processed in the data
    processor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Due to limitations imposed by the laws of physics, the processing time will
    always be subsequent to and necessarily different from the event time, for the
    following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**There is always network latency**: The time to travel from the data source
    to the Kafka broker cannot be zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The client could have a cache**: If the client cached some events before,
    send them to the data processor. As an example, think about a mobile device that
    is not always connected to the network because there are zones without network
    access, and the device holds some data before sending it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The existence of back pressure**: Sometimes, the broker will not process
    the events as they arrive, because it is busy and there are too many.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having said the previous points, it is always important that our messages have
    a timestamp. Since version 0.10 of Kafka, the messages stored in Kafka always
    have an associated timestamp. The timestamp is normally assigned by the producer;
    if the producer sends a message without a timestamp, the broker assigns it one.
  prefs: []
  type: TYPE_NORMAL
- en: As a professional tip, when generating messages, always assign a timestamp from
    the producer.
  prefs: []
  type: TYPE_NORMAL
- en: Basic scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To explain late events, we need a system where the events arrive periodically
    and we want to know how many events are produced by unit of time. In *Figure 6.1*,
    we show this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e64ca6a4-d30b-4ff2-8794-c0c1222dcb37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: The events as they were produced'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, each marble represents an event. They are not supposed
    to have dimensions as they are at a specific point in time. Events are punctual,
    but for demonstration purposes, we represent them as balls. As we can see in **t1**
    and **t2,** two different events can happen at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: In our figure, tn represents the n^(th) time unit. Each marble represents a
    single event. To differentiate between them, the events on **t1** have one stripe,
    the events on **t2** have two stripes, and the events on **t3** have three stripes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to count the events per unit of time, so we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**t1** has six events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t2** has four events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t3** has three events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As systems have failures (such as network latency, shutdown of servers, network
    partitioning, power failures, voltage variations, and so on), suppose that an
    event that happened during **t2** has a delay and reached our system at **t3,**
    shown as follows in *Figure 6.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4402dcaa-bf85-4335-9fda-37b380982e3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: The events as they were processed'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we count our events using the **processing time**, we have the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**t1** has six events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t2** has three events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t3** has four events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have to calculate how many events were produced per time unit, our results
    would be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The event that arrived on **t3** instead of **t2** is called a late event.
    We just have two alternatives, they are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When **t2** ends, produce a preliminary result that the count for **t2** is
    three events. And then, during processing, when we find in another time an event
    belonging to **t2**, we update the result for **t2**: **t2** has four events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When each window ends, we wait a little after the end before we produce a result.
    For example, we could wait another time unit. In this case, the results for tn
    are obtained when t(n+1) ends. Remember, the time to wait to produce results might
    not be related to the time unit size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can guess, these scenarios are quite common in practice, and there are
    currently many interesting proposals. One of the most complete and advanced suites
    for handling late events is the Apache Beam proposal. However, Apache Spark, Apache
    Flink, and Akka Streams are also very powerful and attractive.
  prefs: []
  type: TYPE_NORMAL
- en: As we want to see how it is solved with Kafka Streams here, let's see that.
  prefs: []
  type: TYPE_NORMAL
- en: Late event generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test the Kafka Streams solution for late events, the first thing we need
    is a late event generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify things, our generator will constantly send events at a fixed rate.
    And from time to time, it will generate a late event. The generator generates
    events with the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: Each window is 10 seconds long
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It produces one event every second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The event should be generated in 54^(th) second of each minute, and will be
    delayed by 12 seconds; that is, it will arrive in the sixth second of the next
    minute (in the next window)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we say that the window is of 10 seconds, we mean that we will make aggregations
    every 10 seconds. Remember that the objective of the test is that the late events
    are counted in the correct window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `src/main/java/kioto/events` directory and, inside it, create a
    file called `EventProducer.java` with the contents of *Listing 6.5*, shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6.5: EventProducer.java'
  prefs: []
  type: TYPE_NORMAL
- en: The event generator is a Java `KafkaProducer`, so declare the same properties
    as all the Kafka Producers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator code is very simple, and the first thing that is required is
    a timer that generates an event every second. The timer triggers 0.3 seconds after
    every second to avoid messages sent at 0.998 seconds, for example. The `produce()`
    method is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: When the timer is triggered, the run method is executed. We send one event each
    second except on second 54, where we delay this event by 12 seconds. Then, we
    send this late event in the sixth second of the next minute, modifying the timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `sendMessage()` method, we just assign the timestamp of the event, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Running the EventProducer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the `EventProducer`, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the events topic, as shown in the following block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a console consumer for the events topic using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: From the IDE, run the main method of the `EventProducer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer for the events topic should be similar to
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Note that each event window changes every 10 seconds. Also, note how the 54^(th)
    event is not sent between the 53^(rd) and 55^(th) events. The 54^(th) event, belonging
    to a previous window, arrives in the next minute between the sixth and seventh
    seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Streams processor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's solve the problem of counting how many events are in each window.
    For this, we will use Kafka Streams. When we do this type of analysis, it is said
    that we are doing **streaming aggregation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `src/main/java/kioto/events` directory, create a file called `EventProcessor.java`
    with the contents of *Listing 6.6*, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6.6: EventProcessor.java'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the processing logic is contained in the `process()` method. The first
    step is to create a `StreamsBuilder` to create the `KStream`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As we know, we specify from topic we are reading the events in this case is
    called **events**, and then we always specify the `Serdes`, both keys and values
    of type `String`.
  prefs: []
  type: TYPE_NORMAL
- en: If you remember, we have each step as a transformation from one stream to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to build a `KTable`. To do so, we first use the `groupBy()`
    function, which receives a key-value pair, and we assign a key called `"foo"`,
    because it is not relevant but we need to specify one. Then, we apply the `windowedBy()`
    function, specifying that the window will be 10 seconds long. Finally, we use
    the `count()` function, so we are producing key-value pairs with `String` as keys
    and `long` as values. This number is the count of the events for each window (the
    key is the window start time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have problems with the conceptual visualization of the `KTable`, which
    keys are of type `KTable<Windowed<String>>` and values are of type `long`, and
    printing it (in the KSQL chapter we will see how to do it), would be something
    like the one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The key has the window ID and the utility aggregation key with value `"foo"`.
    The value is the number of elements counted in the window at a specific point
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, as we need to output the `KTable` to a topic, we need to convert it to
    a `KStream` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The `toStream()` method of the `KTable` returns a `KStream`. We use a `map()`
    function that receives two values, the window and the count, then we extract the
    window start time as the key and the count as the value. The `to()` method specifies
    to which topic we want to output (always specifying the serdes as a good practice).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as in previous sections, we need to start the topology and the application,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the `commit.interval.ms` property indicates how many milliseconds
    we will wait to write the results to the `aggregates` topic.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Streams processor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the `EventProcessor`, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `aggregates` topic as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a console consumer for the aggregates topic, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: From the IDE, run the main method of the `EventProducer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the IDE, run the main method of the `EventProcessor`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remember that it writes to the topic every 30 seconds. The output on the console
    consumer for the aggregates topic should be similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'After the second window, we can see that the values in the `KTable` are updated
    with fresh (and correct) data, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Note how in the first print, the value for the last window is 3, and the window
    started in `1532529070000` has a value of `9`. Then in the second print, the values
    are correct. This behavior is because in the first print, the delayed event had
    not arrived yet. When this finally arrived, the count values were corrected for
    all the windows.
  prefs: []
  type: TYPE_NORMAL
- en: Stream processor analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a lot of questions here, it is normal.
  prefs: []
  type: TYPE_NORMAL
- en: The first thought to consider is that in streaming aggregation, and in streaming
    in general, the Streams are unbounded. It is never clear when we will take the
    final results, that is, we as programmers have to decide when to consider a partial
    value of an aggregation as a final result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the print of the Stream is an instant photo of the `KTable` at
    a certain time. Therefore, the results of a `KTable` are only valid at the time
    of the output. It is important to remember that in the future, the values of the
    `KTable` may be different. Now, to see results more frequently, change the value
    of the commit interval to zero, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This line says that the results of the `KTable` will be printed when they are
    modified, that is, it will print new values every second. If you run the program,
    the value of the `KTable` will be printed with each update (every second), shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep a note of two effects:'
  prefs: []
  type: TYPE_NORMAL
- en: The aggregate result (the count) for the window stops at 9 when the window ends
    and the next window events begin to arrive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the late event finally arrives, it produces an update in the window's count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, Kafka Streams apply event time semantics in order to do the aggregation.
    It is important to remember that in order to visualize the data, we had to modify
    the commit interval. Leaving this value at zero would have negative repercussions
    on a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: As you may guess, processing an event stream is much more complex than processing
    a fixed dataset. The events usually arrive late, in disorder, and it is practically
    impossible to know when the totality of the data has arrived. How do you know
    when there are late events? If there is, how much should we expect for them? When
    should we discard a late event?
  prefs: []
  type: TYPE_NORMAL
- en: The quality of a programmer is determined by the quality of their tools. The
    capabilities of the processing tool make a big difference when processing data.
    In this context, we have to reflect when the results are produced and when they
    arrived late.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of discarding events has a special name: watermarking. In Kafka
    Streams, this is achieved through setting the aggregation windows'' retention
    period.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Streams is a powerful library, and is the only option when building data
    pipelines with Apache Kafka. Kafka Streams removes much of the boilerplate work
    needed when implementing plain Java clients. Compared to Apache Spark or Apache
    Flink, the Kafka Streams applications are much simpler to build and manage.
  prefs: []
  type: TYPE_NORMAL
- en: We also have seen how to improve a Kafka Streams application to deserialize
    data in JSON and Avro formats. The serialization part (writing to a topic) is
    very similar since we are using SerDes that are capable of both data serialization
    and deserialization.
  prefs: []
  type: TYPE_NORMAL
- en: For those who work with Scala, there is a library for Kafka Streams called circe
    that offers SerDes to manipulate JSON data. The circe library is the equivalent
    in Scala of the Jackson library.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, Apache Beam has a more complex suite of tools, but is
    totally focused on Stream management. Its model is based on triggers and semantics
    between events. It also has a powerful model for watermark handling.
  prefs: []
  type: TYPE_NORMAL
- en: One notable advantage of Kafka Streams over Apache Beam is that its deployment
    model is simpler. This leads many developers to adopt it. However, for more complex
    problems, Apache Beam may be a better tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chapters, we will talk about how to get the best of two worlds:
    Apache Spark and Kafka Streams.'
  prefs: []
  type: TYPE_NORMAL
