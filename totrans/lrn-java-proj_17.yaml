- en: '17'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we explored the nuances of streamlined data manipulation
    and parallelized operations that utilize the power of modern multi-core processors.
    This was already a little introduction to this chapter’s topic: concurrency!'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency allows applications to perform multiple tasks at the same time.
    This makes the system more efficient. Any available resources can be utilized
    more efficiently, and this leads to overall improved performance. In order to
    do multiple things at the same in Java, we need to know quite a bit. That’s what
    this chapter is for!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what we’ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: A definition of concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atomic classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synchronized keyword
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using locks for exclusive thread access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `ExecutorService`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common threading problems and how to avoid them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is often a dreaded (or threaded?) topic, especially for new developers,
    so don’t despair if you need to go over parts of this chapter twice. I’m going
    to try my best to carefully walk you through all the concepts you need to know.
    Unlike your applications, focus solely on this chapter and don’t do other things
    simultaneously. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-Java-with-Projects/tree/main/ch17](https://github.com/PacktPublishing/Learn-Java-with-Projects/tree/main/ch17).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wondered how many tasks a computer can truly run simultaneously?
    It’s tempting to say *several*, yet, in reality, a single-core computer can only
    execute one process at a given instant. This might appear as simultaneous due
    to the impressive speed at which CPUs switch between processes, thus creating
    the illusion of simultaneous multitasking.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is the concept of executing multiple tasks or threads at the same
    time, rather than sequentially. In a sequential system, tasks are executed one
    after the other, with each task waiting for its predecessor to complete before
    starting.
  prefs: []
  type: TYPE_NORMAL
- en: For our Java applications, concurrency refers to executing different segments
    of a program, simultaneously. The term *simultaneously* might be a little ambiguous
    here, as it could mean multiple things – and that is because concurrency can occur
    at the hardware level, such as in multi-core processors, or at the software level.
    An OS could schedule threads to run on different cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Which one we mean exactly depends on the type of concurrency being employed.
    An overview of them can be found in *Figure 17**.1*. These can be any of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multitasking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithreading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First off, let’s discuss multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of **multiprocessing**, the simultaneous execution of diverse
    processes is facilitated by the presence of multiple CPUs. Each CPU independently
    executes its own process. To draw a parallel from our daily life, consider two
    individuals managing a household where one person is occupied with childcare,
    while the other is out for grocery shopping. They are both a “CPU,” each taking
    care of a unique task concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Multitasking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next concept is **multitasking**, where the term “simultaneous” obtains
    a slightly different connotation. It implies rapid alternating execution rather
    than literal simultaneous execution. Imagine a scenario where a person is cooking
    and intermittently stepping out to hang laundry while the pot is cooking (safely
    away from the kids, of course). They are the “CPU,” continuously switching between
    two (or more) tasks, giving the illusion of simultaneous progression. This, however,
    doesn’t exactly constitute parallel execution, but it is a very efficient use
    of resources for sure.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but not least, we have **multithreading** – and that happens to be our
    primary focus. Multithreading involves different sections of the program running
    on different threads of execution. This can take place in both single- and multi-CPU
    environments. Both previously mentioned everyday scenarios can exemplify multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.1 - Schematic overview of multiprocessing, multitasking, and multithreading](img/B19793_17_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17.1 - Schematic overview of multiprocessing, multitasking, and multithreading
  prefs: []
  type: TYPE_NORMAL
- en: We will delve deeper into the concept of threads shortly. Let’s first talk about
    why we need to have concurrency in our applications (or lives actually!).
  prefs: []
  type: TYPE_NORMAL
- en: Importance of concurrency in modern applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To help you visualize concurrency in computing, consider the way your computer
    runs multiple programs at the same time. You might have a browser, an email client,
    a text editor, a code editor, and Slack running concurrently. This kind of operation
    demands the ability to manage multiple processes concurrently. It’s also seen
    within applications, such as an IDE processing your input while executing code.
    Without some sort of concurrency, stopping a script with an infinite loop would
    be impossible, as the IDE would be too consumed with the execution of the infinite
    loop to deal with your click on the button.
  prefs: []
  type: TYPE_NORMAL
- en: And let’s think of web services for a second; imagine a web server processing
    hundreds, even thousands, of requests concurrently. Such an operation would be
    unfeasible without concurrency, so it’s safe to say that concurrency is an essential
    aspect of our day-to-day computer use and even daily life!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s sum up the advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved performance**: Applications can complete operations faster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsiveness**: Applications remain responsive even when performing resource-intensive
    tasks (as background threads can handle these tasks without blocking the main
    thread)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource utilization**: More efficient use of system resources by taking
    advantage of multi-core processors and other hardware resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages like this, make real-time execution use cases possible. At this point,
    you might be very enthusiastic about concurrency. And you should be! However,
    employing concurrency in our Java applications does come with its own set of costs
    and complexities. Let’s talk about it.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in concurrent programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I’ve said it before, and I’ll say it again: every magic trick comes with a
    price. While concurrency offers many benefits, it also introduces challenges that
    can make concurrent programming complex and even error-prone. We even have some
    errors that are unique to concurrent environments. We’ll mention them in more
    detail later, but it’s good to keep these in mind before diving in:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data race**: When multiple threads access the same memory location in a non-synchronized
    manner and at least one of these threads performs a write. For example, one thread
    wants to read the value and concludes the value is 5, but the other thread increments
    to 6\. This way, the former thread doesn’t have the latest value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Race condition**: A problem that occurs due to the timing and order of events.
    This problematic order of events can influence the correctness of the outcome.
    A race condition typically requires external input, from the OS, hardware, or
    even user. It can, for example, happen when two users try to sign up with the
    same username at the same. When not handled well, this can lead to unpredictable
    and undesirable results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deadlocks**: When two or more threads are waiting for each other to release
    a resource, a deadlock can occur, causing the application to become unresponsive.
    For example, when you think your friend will call you and you wait until they
    do, and your friend thinks you’ll call them and they wait until you do, nothing
    happens, and the friendship is stuck.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Livelocks**: Similar to deadlocks, livelocks occur when two or more threads
    are stuck in a loop, unable to progress due to constantly changing conditions.
    Let’s say you and your friend said you’d meet up at a church in the city center.
    You are at church *a*, and your friend is at church *b*. You wonder whether your
    friend is at church *b* and you walk there. Your friend wonders if you are at
    church *a* and walks there. (And you take a different path and don’t bump into
    each other.) You don’t find each other at the church and keep on walking from
    church *a* to church *b*. Not a very effective use of resources (but all the walking
    is probably great for your health!).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starvation: When a thread is unable to obtain the resources it needs to progress,
    it can experience starvation, leading to poor application performance and inefficient
    use of resources. A real-life example could be a busy bar where multiple people
    are trying to acquire a drink from the bartender. There are a lot of people at
    the bar; the people represent threads. The bartender is serving the people who
    shout the loudest (comparable to threads with higher priority). The shy person
    that doesn’t stand out experiences “starvation” (or thirst) because he doesn’t
    get access to the shared resource (the bartender).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenges are there to be overcome! Java provides various concurrency constructs
    and tools, which we will explore throughout this chapter. I will refer to these
    aforementioned problems every now and then. At the end of the chapter, you’ll
    even see some examples of how to break things! But first, let’s talk about a key
    concept of concurrency: threads!'
  prefs: []
  type: TYPE_NORMAL
- en: Working with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s finally get to explaining *threads*. Threads are sequences of executed
    instructions, representing the most fundamental units of execution. Each thread
    follows a certain path through the code. The threads perform specific tasks within
    processes. A process is typically composed of multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: To give you an example, the programs we’ve created so far had one user-created
    thread (and the user in this case is the developer). The thread went through the
    lines of code in a certain order; for example, when a method was called, the thread
    would execute that method before continuing with the code that was directly on
    the next line after the method call. This is the path of execution of the thread.
  prefs: []
  type: TYPE_NORMAL
- en: When multiple threads are running, multiple paths of execution are being walked
    through your code, and that’s why multiple things are happening at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make this possible, we’ll need duplicates of certain Java constructs.
    For example, we cannot have two threads using the same stack. That’s why every
    thread has its own stack. We will not dive into the details of the Java memory
    model here. However, it helps to at least realize that while each thread has its
    own stack, they share the heap with other threads.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make this digestible for your brain, we’ll explain the theory with
    some not-too-interesting but easy-to-follow examples.. We’ll start with threads.
    There are multiple ways to create and start a thread. Let’s see how we can create
    a thread using the `Thread` class.
  prefs: []
  type: TYPE_NORMAL
- en: The Thread class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Possibly the simplest way to create a thread is by extending the `Thread` class.
    The `Thread` class provides an entry point for your thread’s execution through
    the `run()` method. To create a custom thread, you need to define a subclass of
    `Thread` and override the `run()` method with the code that the thread should
    execute. Here’s a silly example to demonstrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And then in some other class (or even in the same but that might be confusing),
    we can create a new `MyThread` and kick off the thread execution with the `start()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following two lines, but we cannot be sure about the order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `start()` method is part of the `Thread` class that we inherited from and
    it is used to start a new thread. You could also execute the content of the `run()`
    method by calling `myThead.run()`, but that would not start a new thread! That
    would be the same thread as the one executing the `main` method, which would be
    executing the content of the `run()` method.
  prefs: []
  type: TYPE_NORMAL
- en: We started with this way to create a thread because it is easiest to understand.
    It’s definitely not the most common way. It’s more common to implement the `Runnable`
    interface. Let’s see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: The Runnable interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An alternative approach to creating threads is by implementing the `Runnable`
    interface. This is a built-in functional interface that can be used to create
    threads in Java. The `Runnable` interface has a single method, `run()`, that you
    must implement in your class when you extend this interface. Instead of extending
    the `Thread` class, you pass an instance of your `Runnable` implementation to
    a `Thread` object. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And again, we can now instantiate `MyRunnable` at another spot. The second
    step is different though; we are going to instantiate the `Thread` class and pass
    our instance of `MyRunnable` to its constructor. This way, when we start the instance
    of the thread, whatever we specified in the `run()` method of the `Runnable` instance
    will be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And again, to execute what is in `MyRunnable`’s `run` method, we could have
    written `myRunnable.run()`, but this also would not have started a new thread!
    Let’s prove that we actually start a new thread. Every thread has a unique ID.
    By outputting the ID of the thread in the `run` method, we can prove it’s a different
    thread. Here’s the adjusted example for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s our adjusted `Main` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that the IDs might be different for you, but they will also be
    two different threads. The thread IDs remain consistent across multiple executions
    due to some background threads started by Java such as the garbage collector.
    Say we change the `start()` method to `run()`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The IDs are the same; this is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is already a bit more common, but more often we don’t create a class for
    `Runnable` and rather implement `Runnable` with a lambda expression. As you might
    be able to recall from the Lambda expression *Chapters 15* and *16*, we can implement
    any functional interface with a lambda expression. Let’s see how that is done.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda expressions with Runnable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the `Runnable` interface is a functional interface with a single method,
    you can use lambda expressions to create and run threads more concisely. Here’s
    an example using a lambda expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we don’t need a separate class for `Runnable` anymore. We can
    just do it *on the fly*. Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And as I mentioned a few times before, if you use `run()` instead of `start()`,
    you are getting the same output in this case, but this is not done by a new thread.
  prefs: []
  type: TYPE_NORMAL
- en: These are the basics of how to create threads. Let’s see how we can control
    the execution with `sleep()` and `join()`. So, join me for some sleep!
  prefs: []
  type: TYPE_NORMAL
- en: Thread management – sleep() and join()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This might be a weird statement, but threads can go to *sleep*. This means that
    the execution of the thread gets paused for a short while. Before we dive into
    how to do this, it is worth noting that this is something that is often considered
    to be a code smell. This means that it can be a problematic solution to, for example,
    a data race or a challenge with loading times. However, sometimes you will need
    this – for example, to slow down a background thread. Just make sure to proceed
    with caution here. Let’s see how we can make our threads go to sleep now.
  prefs: []
  type: TYPE_NORMAL
- en: The Thread.sleep() method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Thread.sleep()` method is a static method that causes the currently executing
    thread to go to sleep. That means pausing its execution for a specified period.
    It is useful for simulating delays, allowing other threads to execute, or performing
    time-based operations. The `sleep()` method takes a single argument, the duration
    of the sleep in milliseconds. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We need the `try`/`catch` block here because `sleep()` can be interrupted. This
    interrupt would result in the checked exception, `InterruptedException`, being
    thrown.
  prefs: []
  type: TYPE_NORMAL
- en: Handling InterruptedException
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine if the main thread decides that the execution is taking too long and
    wants to end the program. It can suggest the secondary thread stops by using the
    `interrupt` method. If the instance is called t, this can be done with `t.interrupt()`.
    Interrupting a sleeping thread throws `InterruptedException`.
  prefs: []
  type: TYPE_NORMAL
- en: This is a checked exception that you must handle if you use the `Thread.sleep()`
    method. We can also make our thread wait for another thread to be done. This is
    done with the `join()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Using the join() method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Threads can wait until another thread is done. The `join()` method allows the
    calling thread to wait until the specified thread has finished its execution.
    This is useful when you need to ensure that a particular thread has completed
    its work before proceeding. Here’s an example where the main thread is waiting
    for thread `t1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, as you can see, `t1.join()` is called. This makes the main thread wait until
    `t1` is done executing (and that includes 2 seconds of sleep) before the main
    thread continues. The main thread can also wait for a specified amount of time,
    for example, 1 second, by calling `t1.join(1000)` instead. This is a bit safer
    because our program would get stuck if `t1` for some reason hung indefinitely.
    You should go ahead and try to remove `join()` and run the program a few times
    to inspect the behavior and see if you can get it to hang indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we also need to catch `InterruptedException` when we use the
    `join()` method. This is in case the calling thread gets interrupted while waiting
    for the other thread to be done.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at how to avoid (or solve) some common issues with read and
    write operations in concurrent environments.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data integrity can easily be a problem in a concurrent program. Imagine two
    threads reading a value, and then both changing it and overwriting each other’s
    change right after. This could, for example, result in a counter that ends up
    being only one higher, while it should be two higher. Data integrity gets lost!
    This is where atomic classes come in.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic classes are used for atomic operations. That means that the read (getting
    a value) and write (changing a value) are considered one operation instead of
    two separate ones. This avoids the problems with data integrity that we just demonstrated.
    We’ll briefly discuss how to use `AtomicInteger`, `AtomicLong`, and `AtomicReference`.
  prefs: []
  type: TYPE_NORMAL
- en: AtomicInteger, AtomicLong, and AtomicReference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several atomic classes for basic data types. We have `AtomicInteger`
    to represent an integer value and support atomic operations on it. Similarly,
    we have `AtomicLong` for the `Long` type. We have `AtomicReference` for references
    to an object and to support atomic operations on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'These atomic classes provide methods for performing atomic operations, such
    as `get`, `set`, `compareAndSet`, and various arithmetic operations. Let’s have
    a look at an example that would be problematic without `AtomicInteger`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Without `AtomicInteger`, the value of the `counter` at the end of the program
    would differ. It could be 14387, 15673, 19876, and so on. (It could not be more
    than 20000). This is because multiple threads would read it at the same time (so
    reading the same value) and then update it in the next operations, thereby potentially
    writing a lower value than the current value of the `counter`.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, picture this. You’re in a room with two friends. On the table
    is a hat with a piece of paper in it. The piece of paper is folded and has a number
    on it, the number 4\. All three of you need to increment the value by 1\. If your
    friend reads the value, then puts the number back in the hat, and then starts
    to search the house for a piece of paper and a pen. Your other friend might read
    the value right after, before your friend had a chance to increment the number.
    The other friend has a piece of paper and pen available (quite a friend to not
    share with the other friend) and replaces the piece of paper with the new value
    5\. You then go next, read the value, see that it is 5, get your piece of paper
    and pen, write down the number 6 and put it in the hat. The other friend then
    finally comes back, and updates the piece of paper with the new value, which,
    according to his knowledge from when he was reading, should be 5\. The final value
    in the hat is then 5\. Even though it has been 6 before, it went back down. You
    and your friends behave like threads that treat reading and writing as two different
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that you are not just friends, but you are atomic friends. This would
    mean that you would treat reading and writing as one action. So instead of putting
    the piece of paper back in the hat right after reading it, you would not put it
    back before updating it with the new value. So now, if you would all have to increment
    it by 1, there would be no confusion and the value would end up being 7.
  prefs: []
  type: TYPE_NORMAL
- en: We have a Java way of doing this with the atomic classes. In the snippet above,
    the `getAndIncrement` method ensures that two threads cannot access the counter
    at the same time and guarantees that the counter will have the correct value.
    This is because getting and incrementing are not two separate operations, but
    one atomic operation. This is why atomic classes are particularly useful in multi-threaded
    environments where you need to ensure consume shared resources without using explicit
    synchronization. However, we can always work with explicit synchronization. Let’s
    explore the `synchronized` keyword next.
  prefs: []
  type: TYPE_NORMAL
- en: The synchronized keyword
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve just seen, working with many threads can bring potential new problems,
    such as data integrity. The **synchronized** keyword is a Java keyword that uses
    a lock mechanism to achieve synchronization. It is used to control access to critical
    sections of code for different threads. When a thread is inside a synchronized
    method or block, no other thread can enter any of the synchronized methods for
    the same object.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the need for synchronization, let’s consider another simple concurrent
    counting scenario where unexpected outcomes can occur. We have a class named `Count`
    with a static `counter` variable. This class also has a method, `incrementCounter`,
    which increments the value of `counter` by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This program, when run in a `for` loop 10 times in a single-threaded environment,
    will behave as expected, incrementing the counter sequentially from 0 to 10\.
    The value of the id of the thread would also be the same since it’s a single thread.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, imagine instead of a single-threaded environment, we have 10 threads,
    and each thread is tasked to increment the `counter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we have a problem! Here’s the output I got (yours might be different!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output becomes unpredictable because of a phenomenon called *thread interference*.
    In a multithreaded environment, multiple threads may read and increment the value
    of `counter` concurrently. This concurrent modification can lead to unexpected
    results, causing a loss of data integrity. This is again due to a race condition.
    We have seen how to solve that by using an atomic class, but we could also solve
    it by synchronizing the method. The best option would be the one that allows multiple
    threads in most part of the code, without creating data integrity problems. For
    this case, that would be the atomic classes. However, this is a great example
    to demonstrate how the `synchronized` keyword is working.
  prefs: []
  type: TYPE_NORMAL
- en: Using synchronized methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a synchronized method, you simply add the `synchronized` keyword
    before the method definition. This ensures that only one thread at a time can
    execute the method for a given object instance. Here’s the updated example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, if multiple threads call the `incrementCounter()` method simultaneously,
    the `synchronized` keyword ensures that only one thread at a time can access the
    method. This prevents race conditions. Without any changes to the `Main` class,
    this will be the output (your thread IDs might differ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You can imagine that synchronizing an entire method can be inefficient. Since
    this makes all the threads wait outside of the method and it creates a possible
    bottleneck for your performance. It is very possible that part of the code in
    the method can be executed by multiple threads at the same time without being
    a threat(sorry) to data integrity. Sometimes, you only need to synchronize a part
    of the method. This can be done with a synchronized block.
  prefs: []
  type: TYPE_NORMAL
- en: Using synchronized blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some cases, you may want to synchronize only a portion of a method, rather
    than the entire method. To do this, you can use a *synchronized block*. A synchronized
    block requires an object to lock on, and the code inside the block is executed
    while holding the lock. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, the `increment()` and `getCount()` methods use synchronized
    blocks instead of synchronized methods. The result is the same – the `count` variable
    is accessed and modified safely in a multi-threaded environment.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronized methods versus synchronized blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s a best practice to minimize the scope of synchronization to improve performance
    and reduce contention among threads. This concept is closely related to **lock
    granularity**, which refers to the size or scope of the code that is being locked.
    The finer the granularity, the smaller the locked section, allowing more threads
    to execute in parallel without waiting for each other.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing large sections of code or entire methods is considered coarse-grained
    locking and can lead to poorer performance. In this scenario, multiple threads
    may be queued up, waiting for a single lock to be released, which can create a
    bottleneck. While coarse-grained locking might be necessary for ensuring data
    integrity, it should be used with caution and only if there is no other option.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, fine-grained locking involves using synchronized blocks to
    limit the scope of synchronization to the smallest possible critical section.
    This allows for better concurrency, as threads are less likely to be blocked waiting
    for a lock, thereby improving the system’s throughput.
  prefs: []
  type: TYPE_NORMAL
- en: So, to achieve optimal performance without compromising data integrity, aim
    for fine-grained locking by using synchronized blocks whenever possible. This
    aligns well with the principle of minimizing the scope of synchronization. The
    `synchronized` keyword provides a low-level mechanism for synchronization. For
    more complex scenarios, consider using higher-level concurrency constructs, such
    as the `Lock` interface or concurrent collections. Let’s see the `Lock` interface
    next!
  prefs: []
  type: TYPE_NORMAL
- en: The Lock interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s talk about the `Lock` interface. This is an alternative to the `synchronized`
    keyword for handling concurrency control. While synchronized helps us achieve
    thread safety, it also introduces some drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Threads are blocked while waiting for a lock, potentially wasting processing
    time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s no mechanism to check whether a lock is available or to time out if
    a lock is held for too long
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you need to overcome these limitations, you can use the built-in `Lock`
    interface with implementations that offer more control over synchronization. We
    will discuss one of the most common implementations: `ReentrantLock`.'
  prefs: []
  type: TYPE_NORMAL
- en: ReentrantLock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ReentrantLock` class is a popular implementation of the `Lock` interface.
    `ReentrantLock` is used to protect a section of code similar to `synchronized`
    but provides additional features through its methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lock()`: This method locks the lock'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unlock()`: This method releases the lock'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tryLock()`: This method attempts to acquire the lock and returns a boolean
    indicating whether the lock was acquired'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tryLock(time, unit)`: This method attempts to acquire the lock for a specified
    duration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s update the example we used to demonstrate the synchronized keyword with
    `ReentrantLock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we replace the `synchronized` block with `ReentrantLock`.
    We lock the `Lock` before the critical section and unlock it afterward in the
    `finally` block. This unlocking in the `finally` block is of utmost importance;
    otherwise, the lock won’t be released when an exception occurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Main` class remains the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And this works like a charm. Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'But what if the block was locked already? We want to avoid waiting indefinitely.
    In that case, it may be better to use `tryLock`. If the lock is unavailable, the
    thread can continue with other tasks. This is one of the benefits compared to
    using the `synchronized` keyword! Here’s the updated code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we surround the `try` block with `tryLock()`. If the lock is
    not available, the thread proceeds to do other work. We could also have used the
    `tryLock(time, unit)` method to wait for the lock for a specific duration.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t go into detail due to the scope of this book, but there are other locks
    available – for example, the `ReadWriteLock` interface. It separates read and
    write operations, allowing multiple concurrent reads but exclusive writes. This
    can improve performance in read-heavy workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for working with locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with the `Lock` interface, it’s important to keep a few best practices
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Always unlock in a `finally` block to ensure the lock is released even in the
    case of an exception.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `tryLock()` for non-blocking operations, which can help avoid deadlocks
    and improve performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though we didn’t discuss it in detail, consider using `ReadWriteLock` for
    read-heavy workloads. This allows concurrent reads and exclusive writes. This
    improves the throughput of your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enough about locks! Let’s talk about the another key tool for working with
    concurrency in Java: concurrent collections!'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-threaded environments are important for performance, but in any multi-threaded
    environment, data integrity becomes an issue to consider. Imagine a situation
    where you have several threads interacting with a shared data structure, such
    as an `ArrayList` or `HashMap`. While one thread might be trying to read data
    from the structure, another could be writing to it. This can lead to data inconsistency
    and other types of errors.
  prefs: []
  type: TYPE_NORMAL
- en: One common problem that arises in such situations is known as a concurrent modification
    exception. This occurs when one thread is iterating over a data structure, and
    another thread attempts to modify it. Java recognizes that this can cause inconsistencies
    and throws an exception to prevent this dangerous operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example, where a `HashMap` is being used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re trying to iterate over `HashMap` and remove an entry
    during the process. This will throw `ConcurrentModificationException`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have guessed it; this is exactly why we have concurrent collections.
    A concurrent collection, such as `ConcurrentHashMap`, is a thread-safe alternative
    to `HashMap`, which means it can handle simultaneous reading and writing from
    multiple threads. With `ConcurrentHashMap`, you can modify the map while looping
    over it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We don’t get `ConcurrentModificationException` this time. `ConcurrentHashMap`
    allows us to remove items while iterating.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s not even all! Concurrent collections offer another advantage. They
    allow us to lock on a per-segment basis. This means that multiple threads can
    have read access simultaneously, which can enhance the performance without compromising
    data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent collection interfaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within the `java.util.concurrent` package, there are several interfaces designed
    to facilitate concurrent operations on collections. The two primary ones we will
    discuss are `ConcurrentMap` and `BlockingQueue`.
  prefs: []
  type: TYPE_NORMAL
- en: ConcurrentMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ConcurrentMap` is a sub-interface of the standard `java.util.Map`. It provides
    atomic operations for adding, removing, and replacing key-value pairs, enhancing
    thread safety. The two primary implementations of `ConcurrentMap` are `ConcurrentHashMap`
    and `ConcurrentSkipListMap`. It works very similarly to a regular `Map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`ConcurrentHashMap` is a thread-safe `Map` implementation that provides better
    performance than `Hashtable` (an older thread-safe alternative). It allows concurrent
    reads and writes with minimal contention.'
  prefs: []
  type: TYPE_NORMAL
- en: BlockingQueue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`BlockingQueue` is another interface, a subtype of `Queue`, optimized for multi-threaded
    operations. Unlike standard queues, `BlockingQueue` will block or time out when
    attempting to add an element to a full queue or retrieve an element from an empty
    queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: These interfaces provide additional functionality that becomes invaluable when
    working in a multi-threaded environment, enhancing both performance and data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are quite a few other concurrent implementations of collections that
    you might work with in the future, they work very similarly to their non-concurrent
    counterparts. We’ll talk about two categories: `SkipList` and `CopyOnWrite` collections.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SkipList collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ConcurrentSkipList` collections represent naturally ordered collections, which
    means they maintain their elements in a sorted manner. `ConcurrentSkipListSet`
    and `ConcurrentSkipListMap` are the two most common `ConcurrentSkipList` collections.
    They work very similar to the collections that we’re used to.'
  prefs: []
  type: TYPE_NORMAL
- en: ConcurrentSkipListSet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using `ConcurrentSkipListSet` is the same as using `TreeSet`, but it’s optimized
    for concurrent usage. Let’s take a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, when you print the set, the elements will be displayed
    in their natural order: `Adnane`, `Gaia`, and `Jonas`.'
  prefs: []
  type: TYPE_NORMAL
- en: ConcurrentSkipListMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ConcurrentSkipListMap` works similarly to `TreeMap`, but it’s designed for
    concurrent operations. Like `ConcurrentSkipListSet`, the map entries are maintained
    in the natural order of their keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, the map entries are printed in the alphabetical order of the
    keys: `Bass`, `Flute`, and `Piano`.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CopyOnWrite collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`CopyOnWrite` collections, as the name suggests, make a fresh copy of the collection
    every time it is modified. This means they perform well when there are more read
    operations than write operations but can be inefficient when there are more writes.
    Let’s discuss the common implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: CopyOnWriteArrayList
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CopyOnWriteArrayList` works just like a regular `ArrayList` but creates a
    new copy of the list every time it gets modified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Even though we’re modifying the list during iteration, it doesn’t result in
    `ConcurrentModificationException` because a new copy of the list is created when
    it’s modified.
  prefs: []
  type: TYPE_NORMAL
- en: CopyOnWriteArraySet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CopyOnWriteArraySet` is similar to `HashSet`, but it creates a new copy every
    time the set is modified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the size of the set remains the same after the loop because
    the set only contains unique objects.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronized collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Synchronized collections* are a different way to use collections in a multithreaded
    environment. The `Collections` class provides several static methods for returning
    synchronized versions of regular collections such as `List`, `Set`, and `Map`.
    Here’s an example for `List`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In this example, `syncList` is a thread-safe version of `regularList`. These
    synchronized collections are a good choice when you need to turn an existing collection
    into a thread-safe one, but if you know a collection will be used in a multithreaded
    environment upon creation, it’s better to use concurrent collections as they perform
    better.
  prefs: []
  type: TYPE_NORMAL
- en: The most important difference between synchronized and concurrent collections
    is that synchronized collections cannot be modified in a loop as they will throw
    `ConcurrentModificationException`. They are otherwise safe to use and don’t lead
    to issues with data integrity when used with multiple threads. Managing a lot
    of threads by hand would be quite a daunting task. Luckily, there is a special
    interface, `ExecutorService`, to help us with that!
  prefs: []
  type: TYPE_NORMAL
- en: ExecutorService and thread pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java’s `ExecutorService` is a mechanism for executing tasks asynchronously.
    As a part of the `java.util.concurrent` package, `ExecutorService` is an interface
    used to manage and control thread execution in a multithreaded environment. We
    have seen so far how we can manually control threads, and we’ll now see how we
    can use `ExecutorService` instead. We’ll see the details of `ExecutorService`
    and its implementations, such as `SingleThreadExecutor` and `ScheduledExecutorService`.
    Let’s see `SingleThreadExecutor` first.
  prefs: []
  type: TYPE_NORMAL
- en: Executing tasks using SingleThreadExecutor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firstly, let’s start with `SingleThreadExecutor`. This `ExecutorService` has
    one single worker thread to process tasks, guaranteeing that tasks are executed
    in the order they’re submitted. It’s useful when we need sequential execution.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario with an election where votes are being counted. To mimic
    this process, we’ll represent each vote as a task. For simplicity, let’s assume
    we’re counting votes for one candidate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we can do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we first create a `SingleThreadExecutor` instance.
    We then submit four tasks, each representing a vote being counted. Notice that
    we use `executor.execute()`, passing a `Runnable` lambda function as an argument.
    This function prints the vote number and the thread ID handling it. At the end,
    we shut down `ExecutorService` using `executor.shutdown()`. This is crucial to
    terminate the non-daemon thread of the executor and failing to do so will prevent
    your application from terminating. A non-daemon thread is one that prevents the
    program from ending. When you forget to do that, you’ll see that once you run
    the program, it will not stop. The stop button will stay visible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what it outputs (for me):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it will count four votes, printing the corresponding vote number
    and the same thread ID each time since all tasks are processed by a single thread.
    We can actually also invoke multiple tasks at the same time. Before we can do
    that, we need to understand `Callable` and `Future`. So, let’s see what that means
    first – is the future calling us?
  prefs: []
  type: TYPE_NORMAL
- en: The Callable interface and Future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the `Runnable` interface enables you to execute code concurrently, it
    does not return a result. In contrast, the `Callable` interface allows concurrent
    tasks to produce a result. It has a single `call` method that returns a value.
    So, it’s a functional interface too.
  prefs: []
  type: TYPE_NORMAL
- en: '`ExecutorService` not only executes `Runnable` tasks but also `Callable` tasks,
    which return a result. The `submit()` method is used to execute `Callable` tasks.
    This `submit()` method returns a `Future` object, which can be used to retrieve
    the result once it’s ready. If you’d like to think of a non-code example, you
    can compare it to placing an order at a restaurant: you receive a token (`Future`)
    and you can use it to collect your order (the result) when it’s ready.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Future` object represents the result of an ongoing computation—a placeholder
    of sorts. When you submit a `Callable` task to `ExecutorService`, it returns a
    `Future` object. You can use this `Future` object to check whether the computation
    is complete, wait for its completion, and retrieve the result. It’s time to see
    an example!
  prefs: []
  type: TYPE_NORMAL
- en: Submitting tasks and handling results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s simulate counting votes and maintaining a tally using `Callable` tasks.
    Here, we’ll use the `submit()` method, which returns a `Future` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is what it will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We are still using `SingleExecutorService` in the preceding code. To our `ExecutorService`,
    we submit four `Callable` tasks using `executorService.submit()` in the `getRandomVote`
    method. Each task waits for one second (simulating vote counting) and then returns
    `1` (representing one vote). Each submission returns a `Future` object, which
    is stored in `vote1`, `vote2`, `vote3`, and `vote4`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we wait in a loop until all `Future` objects report they’re done. Once
    all votes are counted (all `Future` objects are done), we retrieve the results
    using the `get()` method on each `Future` object. This is wrapped in a `try`/`catch`
    block to handle potential exceptions. Finally, we add all the votes and print
    the total votes. Before we continue, let’s talk a bit more about the methods on
    the `Future` class.
  prefs: []
  type: TYPE_NORMAL
- en: Future objects and their methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Future` object provides several methods to handle the results of asynchronous
    computations. We already used `isDone()` to check whether the task was done and
    the result was in. And we already used `get()` to get the result of the task when
    it was done. This `get()` method waits until the task is done executing. Here
    are some other methods that are important to know:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get(long timeout, TimeUnit unit)`: Retrieves the result only if it’s ready
    within the provided timeout duration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isCancelled()`: Checks whether the computation was canceled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cancel(boolean mayInterruptIfRunning)`: Attempts to cancel the task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the latest example, we submitted the four tasks one by one, but we can also
    submit multiple tasks at the same time. Let’s see how that is done.
  prefs: []
  type: TYPE_NORMAL
- en: Invoking multiple tasks and handling the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can submit multiple tasks and handle their results. For this, we’ll use the
    `invokeAny()` and `invokeAll()` methods, and represent tasks as `Callable` tasks
    instead of `Runnable` tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The `invokeAny()` method takes a collection of `Callable` objects and returns
    the result of a successfully executed task (the first one to finish), canceling
    all others. Conversely, `invokeAll()` executes all tasks and returns a list of
    `Future` objects representing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code, which is counting our votes again. In this case,
    people can vote for option 1 or option 2\. Counting is now implemented with the
    use of `Callable` and `Future`. We will demonstrate the use of `invokeAny` (not
    very democratic) and `invokeAll` in this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is what the code outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the code, we start by defining a list of `Callable` tasks. Each `Callable`
    returns an `Integer` after a certain sleep period (simulating the work done by
    the task). We then invoke the tasks using the `invokeAny()` and `invokeAll()`
    methods, displaying the results accordingly. The `try`/`catch` block is needed
    to handle potential exceptions that may arise during task execution.
  prefs: []
  type: TYPE_NORMAL
- en: This example gives us a good understanding of invoking tasks and handling results
    using `ExecutorService`. So far, we’ve only seen `SingleThreadExecutor`. There
    are also `ExecutorService` available that use multiple threads. That’s going to
    make it a lot more interesting (and complicated, so stay focussed!). Let’s see
    these next.
  prefs: []
  type: TYPE_NORMAL
- en: Thread pools and task execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thread pools are a key concept of concurrent programming. Thread pools can be
    compared to a crew of workers—multiple threads waiting for tasks. When a task
    is available, each thread can pick it up from the queue, execute it, and wait
    for new tasks rather than being destroyed. This is a lot more efficient compared
    to creating a new thread for every task.
  prefs: []
  type: TYPE_NORMAL
- en: There are different `ExecutorServices` to manage thread pools, and each has
    its specific use case. Let’s explore `FixedThreadPool` first.
  prefs: []
  type: TYPE_NORMAL
- en: FixedThreadPool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`FixedThreadPool` maintains a fixed number of threads. If a task is submitted
    and all threads are active, the task waits in a queue until a thread becomes available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we’ve had a single thread do all the vote counting for us. Instead,
    consider an election scenario where you have three polling stations to count the
    votes from all 100 voting stations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Every time you run the program, it will be somewhat different! The count for
    each vote is carried out asynchronously, as demonstrated by the randomly assigned
    sleep times. This is because we’re simulating a scenario in which each of the
    three threads (in this case with id `22`, `23`, and `24`) corresponds to a polling
    station that starts counting the votes. Even if there is a large number of votes,
    there will still only be three threads (polling stations) performing the counting.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the order of the voting stations is no longer the same. This
    is because multiple threads are working simultaneously. And that’s not a problem,
    as it doesn’t influence the end result.
  prefs: []
  type: TYPE_NORMAL
- en: CachedThreadPool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CachedThreadPool`, on the other hand, creates new threads as needed and reuses
    previously constructed threads if they are available. Threads in this pool that
    haven’t been used for a certain amount of time are terminated and removed from
    the cache.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine an election with numerous mobile polling stations that move around
    to different locations and count votes as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This code can output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `CachedThreadPool` creates as many threads as needed to process
    the votes simultaneously, leading to faster vote counting. However, this comes
    at the cost of system resources since an uncontrolled number of threads could
    be created.
  prefs: []
  type: TYPE_NORMAL
- en: Another option that we have is to schedule commands to run after a given delay
    or to execute periodically. This is done with `ScheduledExecutorService`. Let’s
    see how we can schedule tasks to run after a certain delay or periodically.
  prefs: []
  type: TYPE_NORMAL
- en: ScheduledExecutorServices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we’re going to take a look at `ScheduledExecutorService`. As the name suggests,
    `ScheduledExecutorService` allows you to schedule tasks to be executed after a
    certain delay, or to be executed periodically. This is incredibly useful when
    you need a task to be executed at regular intervals without having to manually
    reschedule it each time.
  prefs: []
  type: TYPE_NORMAL
- en: To use `ScheduledExecutorService`, you first create one using the `Executors`
    class. There are multiple options, but we’ll only use `newScheduledThreadPool()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see some example code. Assume that we are building a simple voting system
    where we need to schedule a task to close the voting process after a certain period,
    say 1 hour. Here’s how we can do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We create `ScheduledExecutorService` with a single thread. We then use the
    `schedule()` method to schedule the `closeVoting()` method to be executed after
    1 hour. The `schedule()` method takes three arguments: the method to execute,
    the delay before execution, and the time unit of the delay.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple example. You could also schedule tasks to be executed periodically.
    For example, if you wanted to remind voters every 15 minutes that voting will
    close soon, you could do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we use the `scheduleAtFixedRate()` method to schedule the `remindVoters()`
    method to be executed every 15 minutes. The `scheduleAtFixedRate()` method takes
    four arguments: the method to execute, the initial delay before execution, the
    period between executions, and the time unit of the delay and period.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is what it outputs with these modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Remember, when you are done with your `ScheduledExecutorService`, don’t forget
    to shut it down. This will stop any new tasks from being accepted and allow the
    existing tasks to complete. If you don’t shut down `ScheduledExecutorService`,
    your application might not terminate because the non-daemon threads in the pool
    will keep it running.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s all you need to know to get started with `ScheduledExecutorService`.
    Let’s explore the data race problem in a bit more detail before moving on to other
    Java tools, such as atomic classes for dealing with concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Data races
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of starting by explaining atomic classes, let’s start with explaining
    a problem called a data race with an example. We already have seen how to fix
    this problem with the use of atomic classes, synchronized keyword, and locks.
    Can you spot the problem in the following code snippet?
  prefs: []
  type: TYPE_NORMAL
- en: '`public class` `Main {`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a static `int` counter that is being incremented 10,000 times by two
    threads. You’d expect the counter to be 20,000 then, right? Yet, if we print the
    value of the counter, this is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we run it again, this is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'A third time? This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Long story short, we have a problem! But why? Well, we are looking at the result
    of a data race. A data race occurs when two or more threads access shared data
    simultaneously, and at least one of them modifies the data. So, in our case, `thread1`
    reads the value and wants to increase it, but at the same time, `thread2` read
    the value and increases it too. Let’s say the value was 2,000 at the time. Both
    `thread1` and `thread2` increase it to 2,001\. There are a few other variations
    possible, for example, `thread1` writing 4,022, and then `thread2` overwriting
    the value with a much lower value, such as 3,785.
  prefs: []
  type: TYPE_NORMAL
- en: This happens because these `++` and `--` operators are not atomic operators.
    This means that getting the value and increasing it are two separate operations,
    allowing it to be intersected by another thread. In order to avoid this, we can
    work with atomic classes. As we have seen, for atomic classes, getting and modifying
    the value is just one single operation, avoiding this issue.
  prefs: []
  type: TYPE_NORMAL
- en: A non-code data race
  prefs: []
  type: TYPE_NORMAL
- en: Let’s tell you a true story to give you a non-code example (and a first-world
    problem) about a data race I had to experience myself. I love it when my friends
    have wish lists for their birthdays so that I can get them something they want
    instead of spending lots of time thinking about what to get them. So apparently,
    I and another friend saw that the kid of one of our friends wanted an inflatable
    bouncy unicorn. So, we both checked the list at almost the same time and we saw
    that the bouncy unicorn was still an available gift option. We both crossed it
    off and got the unicorn. (Okay, to be honest, I remember that I had to cross it
    off twice actually, but I figured it was a glitch.)
  prefs: []
  type: TYPE_NORMAL
- en: Turns out that we were looking at that list, at the very same time, ordering
    the unicorn and crossing it off. Can’t say it ended up being a real problem, because
    what is better to a 6-year-old than one inflatable bouncy unicorn? Yup, two inflatable
    bouncy unicorns!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the common problems such as the data race mentioned here before we
    wrap up this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Threading problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with concurrency, we have the opportunity to increase performance!
    However, with great power comes great responsibility; things can go awfully wrong
    as well. Therefore, we must be aware of several potential problems that can arise
    due to incorrect or inefficient synchronization. Let’s discuss four common threading
    problems: data races, race conditions, deadlocks, livelocks, and starvation.'
  prefs: []
  type: TYPE_NORMAL
- en: Data races
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have just talked quite a bit about data races already. They occur when two
    or more threads access shared data concurrently, and at least one of them modifies
    the data, leading to unpredictable results. Here’s an example of an innocent-looking
    snippet of code that can lead to a data race in multithreaded environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: If multiple threads call the `increment()` method simultaneously, the `count`
    variable’s value may not be updated correctly, resulting in an incorrect final
    count.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies to prevent data races
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prevent data races, you can use various synchronization techniques as we
    have seen in this chapter, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `synchronized` keyword on methods or blocks of code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using atomic classes, such as `AtomicInteger`, `AtomicLong`, and `AtomicReference`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using locks, such as `ReentrantLock` or `ReadWriteLock`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Race conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A race condition is a situation in concurrent programming where the program’s
    outcome can change based on the sequence or timing of thread scheduling and execution.
    It is a flaw that occurs when the timing or order of events affects the program’s
    correctness. Unlike a data race, where concurrent access to shared data is the
    issue, a race condition is about multiple threads sequencing their operations
    incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example code snippet to illustrate the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: If two threads (representing two customers) called the `bookSeat` method at
    the same time when only one seat is left, they could both pass the `if(seatsAvailable
    > 0)` check before either of them had the chance to decrement `seatsAvailable`.
    As a result, two customers might book the last seat, which is a race condition.
  prefs: []
  type: TYPE_NORMAL
- en: This situation is an example of a race condition because the order of operations
    (checking the availability and then decrementing the number of seats) matters
    for correctness. Specifically, there is a critical section of code (`if(seatsAvailable
    > 0)` and `seatsAvailable--;`) that needs to be executed atomically (without interruption)
    to prevent errors.
  prefs: []
  type: TYPE_NORMAL
- en: To make sure that we understand the difference from a data race, a data race
    specifically involves simultaneous access to shared data where at least one operation
    is a write operation. A data race could occur in our example if multiple threads
    attempted to decrement `seatsAvailable` at the same time, potentially leading
    to one thread reading the value of `seatsAvailable` before another thread had
    finished decrementing it.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies to prevent race conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid these types of problems, we need to ensure that the critical section
    of code is executed atomically, which can be achieved by synchronization. For
    instance, we can use the `synchronized` keyword to prevent multiple threads from
    executing the critical section simultaneously. You should consider these general
    strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`synchronized` keyword or explicit locks to ensure that only one thread can
    execute a critical section at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atomic operations**: Use atomic operations that are completed in a single
    step without the possibility of being interrupted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential design**: Design your program so that thread access to shared
    data is sequenced or coordinated in a manner that eliminates the timing or order
    of events as a factor, reducing the chance of race conditions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.util.concurrent` package provides higher-level synchronization utilities
    such as `Semaphores`, `CountDownLatches`, and `CyclicBarriers`, which can be used
    to coordinate operations between threads and thus prevent race conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deadlocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A deadlock occurs when two or more threads wait for each other to release a
    resource, resulting in a circular waiting pattern. Here’s an example of a deadlock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: So, please mind the problem here is the incorrect use of the `synchronized`
    keyword! The `thread1` variable acquires a lock on `resourceA` and `thread2` acquires
    a lock on `resourceB`. Then, both threads attempt to acquire a lock on the other
    resource, leading to a deadlock. Meaning that both threads are stuck indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies to prevent and resolve deadlocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prevent and resolve deadlocks, you can employ the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoid nested locks**: Ensure that you only lock one resource at a time, or
    acquire locks in a specific order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use lock timeouts**: Set a timeout for acquiring locks and release them if
    the timeout expires.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Livelocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A livelock occurs when two or more threads are stuck in a loop, repeatedly
    releasing and re-acquiring resources, without making any progress. Here’s a silly
    example of a livelock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: In this example, two `PhoneCall` objects, `Patricia` and `Patrick`, are trying
    to hang up a phone call using a shared `hangUpButton` object. The `hangUpButton`
    object can have only one owner at a time. `Patricia` and `Patrick` both seem to
    have the rule that if they own `hangUpButton` and the other person hasn’t hung
    up yet, they will pass `hangUpButton` to the other person. This leads to a situation
    where the two of them are perpetually passing `hangUpButton` back and forth to
    each other because they’re always seeing that the other person hasn’t hung up
    yet, which is a livelock situation.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that in this particular silly code, there is no mechanism to break
    out of the livelock (the infinite `while` loop in the `callWith` method). In real-world
    scenarios, a mechanism to detect and recover from the livelock should be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies to prevent and resolve livelocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prevent and resolve livelocks, consider the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use a backoff algorithm**: Introduce a (very small) random delay or an exponential
    backoff before retrying an operation to minimize the chances of livelock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritize resources or threads**: Assign priorities to resources or threads
    to avoid contention and ensure that higher-priority tasks can proceed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detect and recover from livelocks**: Monitor the application for livelocks
    and take corrective action, such as restarting threads or reassigning priorities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A livelock is a special case of resource starvation. It’s a condition where
    two or more processes continuously change their state in response to changes in
    the other process(es) without doing any useful work. Let’s talk about starvation
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Starvation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Starvation occurs when a thread is unable to access shared resources for an
    extended period, hindering its progress. This usually happens when higher-priority
    threads monopolize resources, causing lower-priority threads to be starved. Here’s
    an example of starvation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the high-priority thread monopolizes the shared resource for
    a long time, causing the low-priority thread to be starved. Please mind that thread
    priorities are only hints of how to the scheduler. It’s up to the OS implementation
    to decide.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies to prevent and resolve starvation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prevent and resolve starvation, you can employ the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReentrantLock` with the `fair` parameter set to `true`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor resource usage**: Keep track of resource usage and adjust thread
    priorities or access patterns to avoid monopolization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use time-sharing**: Limit the time for which a thread can hold a resource
    or ensure that each thread gets a chance to access the resource periodically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And that’s it for now! There’s a lot more to know about concurrency in fact,
    we could write an entire book on it – but this will be enough to get you started.
    Time to roll up your sleeves and get started with the hands-on part!
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`FeedingActivity` and `CleaningActivity`. Make `FeedingActivity` extend the
    `Thread` class and `CleaningActivity` implement the `Runnable` interface. In both,
    override the `run` method to print out the activity’s name and a message indicating
    that the activity is happening.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ParkOperations` class with two threads, one for feeding and another for cleaning.
    Start both threads and then use `sleep()` to simulate a time delay for the feeding
    activity. Use `join()` to ensure cleaning only happens after feeding is complete.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TaskAssigner` class where you use `ExecutorService` to assign tasks to employees.
    Tasks could be represented as `Runnable` or `Callable` objects, and employees
    could be represented as threads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solve race conditions in the following code snippet. `updater1` and `updater2`
    both are trying to update the status of the same dinosaur object. Since they run
    concurrently, it might lead to inconsistent outputs. Use the `synchronized` keyword
    or `AtomicReference` to prevent data inconsistency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Project – Park Operations System – the calm before the storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While our park thrives with lively dinosaurs and excited visitors, our behind-the-scenes
    operations must run concurrently and seamlessly. The use of concurrency can ensure
    that tasks such as feeding dinosaurs, tracking dinosaur movements, and scheduling
    staff shifts are handled efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite our best efforts, things start to go awry. A few dinosaurs
    become restless, security systems begin to glitch, and staff reports mysterious
    occurrences. Could this be the calm before the storm, and will we relive what
    happened in a famous competing park they made a movie about?
  prefs: []
  type: TYPE_NORMAL
- en: Update the following *Park Operations System* so that it concurrently safely
    handles different park operations. Use low-level threading, `ExecutorService`,
    atomic classes, synchronized blocks, and the `Lock` interface to manage concurrent
    access to shared resources. Prevent and handle race conditions, deadlocks, livelocks,
    and starvation scenarios to keep things under control as tensions rise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the problematic code that causes the issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is a race condition when accessing and modifying `foodStock`.
    Additionally, these threads will run indefinitely, creating the potential for
    starvation of other threads in the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some hints for modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Lock` interface, or an atomic class. Remember, the goal is to ensure that
    the `reduceFood()` method and the reading of `foodStock` happen atomically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ExecutorService`: Rather than creating threads directly, you could use `ExecutorService`
    to manage the threads. This provides more flexibility and utility methods for
    handling threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run` methods of `FeedingDinosaurs` and `TrackingMovements` run indefinitely.
    You could use conditions to control these loops and ensure `ExecutorService` shuts
    down after the operations are complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deadlocks, livelocks, and starvation**: To simulate and prevent these, consider
    adding more shared resources and threads, and experiment with different locking
    orders, lock-releasing mechanisms, and thread priorities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember to be cautious when modifying the code to prevent and handle these
    concurrency issues. Incorrect modifications could cause more problems than they
    solve. Thank you for saving the day!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is a fundamental concept in modern software development, allowing
    applications to perform multiple tasks simultaneously, and efficiently utilizing
    system resources. In this chapter, we explored various aspects of concurrent programming
    in Java, from basic thread creation and management to advanced techniques for
    handling synchronization and shared data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We started by introducing concurrency and its importance, followed by walking
    through creating threads using the `Thread` class, the `Runnable` interface, and
    implementing the `Runnable` interface with lambda expressions. We then moved on
    to two thread management methods: `sleep()` and `join()`. Next, we talked about
    `ExecutorService`, which provides a higher level of abstraction for managing thread
    execution and made our lives a little easier (after making it harder first).'
  prefs: []
  type: TYPE_NORMAL
- en: A crucial aspect of concurrent programming is avoiding data races. We demonstrated
    a data race example and discussed strategies to resolve them, including the use
    of atomic classes and the `synchronized` keyword. We also explore the `Lock` interface
    as an alternative to the `synchronized` keyword. This gave us more flexibility
    and control.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent collections, such as `ConcurrentHashMap`, `CopyOnWriteArrayList`,
    and `ConcurrentLinkedQueue`, provide thread-safe alternatives to standard Java
    collections. We briefly mentioned their benefits and use cases and saw some examples
    of their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we examined common threading problems, including data races, race conditions,
    deadlocks, livelocks, and starvation. We provided examples and strategies to prevent
    and resolve these issues. By this point, you should have a solid understanding
    of concurrent programming in Java and be equipped with the skills to deal with
    multi-threaded applications.
  prefs: []
  type: TYPE_NORMAL
