- en: Build a Breast Cancer Prognosis Pipeline with the Power of Spark and Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breast cancer is the leading cause of death among women each year, leaving others
    in various stages of the disease. Lately, **machine learning** (**ML**) has shown
    great promise for physicians and researchers working towards better outcomes and
    lowering the cost of treatment. With that in mind, the **Wisconsin Breast Cancer** **Da****ta
    Set** represents a combination of suitable features that are useful enough to
    generate ML models, models that are able to predict a future diagnostic outcome
    by learning from predetermined or historical breast mass tissue sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is  the dataset we refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository: Breast Cancer Wisconsin (Original) Data Set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data
    Set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessed July 13, 2018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Website URL: [https://archive.ics.uci.edu/ml/datasets/Breast Cancer Wisconsin
    (Original)](https://archive.ics.uci.edu/ml/datasets/Breast%20Cancer%20Wisconsin%20(Original))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will be focusing on implementing and training a logistic
    regression multiclass classifier for making a prediction on whether a breast cancer
    mass is malignant or not. The Wisconsin Breast Cancer Data Set is a classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: The overarching learning objective of this chapter is to be able to implement
    a Scala solution that will predict cancer outcomes. Starting from the **UCI Machine
    Learning Repository** breast cancer dataset, we will lean on the Spark ML library's
    ML APIs and its supporting libraries to build a breast cancer prediction pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list is a section-wise breakdown of individual learning outcomes
    for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest breast cancer pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LR breast cancer pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breast cancer classification problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the moment supervised learning is the most common class of ML problems in
    the business domain. In [Chapter 1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml),
    *Predict the Class of a Flower from the Iris Dataset*, we approached the Iris
    classification task by employing a powerful supervised learning classification
    algorithm called **Random Forests**, which at its core depends on a categorical
    response variable. In this chapter, besides the Random Forest approach, we also
    turn to yet another intriguing yet popular classification technique, called **logistic
    regression**. Both approaches present a unique solution to the prediction problem
    of breast cancer prognosis, while an iterative learning process is a common denominator.
    The logistic regression technique occupies center stage in this chapter, taking
    precedence over Random Forests. However, both learn from a test dataset containing
    samples with predetermined measurements and compute a prediction on new, unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed further, a quick note on ML terminology. The literature in
    this rapidly expanding field is sometimes seen to be replete with terms from other
    overlapping fields, leading to differing perceptions, even though two apparently
    different terms refer to the same thing or are mostly equivalent in regard to
    semantics. Sometimes, two terms that are often used interchangeably in the literature
    might actually be quite different; for example, the terms **multivariate** and
    **multivariable** are two such terms. We will avoid using multivariable in this
    chapter. That said, let's take up the Wisconsin Breast Cancer Data Set and understand
    the terms around it, prior to problem formulation and implementation.
  prefs: []
  type: TYPE_NORMAL
- en: First, we must download the dataset from the UCI Machine Learning Repository.
    It is available in the `ModernScalaProjects_Code` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer dataset at a glance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wisconsin Breast Cancer Data Set contains 699 rows of data. Each row corresponds
    to a single sample (the term **example** is sometimes used interchangeably with
    a **sample** in the ML literature) containing nine feature measurements of digitized
    images of a fine needle aspirate of a breast mass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into the details, here is a table listing the key characteristics
    of the 699 rows (instances) of the Wisconsin Breast Cancer Data Set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddc04472-19c3-40fa-a0f1-1f2111bf31e7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Breast cancer dataset characteristics
  prefs: []
  type: TYPE_NORMAL
- en: The preceding table lists nine cell nuclei attributes of the breast cancer dataset,
    where each attribute bears a single value. All of the nine cell nuclei attribute
    values are measurements that have been captured from digitized images of a certain
    sample. 699 of these breast cancer tissue samples, then, should constitute our
    ML experimental unit of 699 input vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reflect on what input vectors are, we invite readers to draw upon their
    previous experience with the Iris dataset supervised learning problem; this was
    a classification task characterized by two fundamental aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: An input vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A response variable value `Y` with two possible outcomes for its input vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Y` is represented by the class column and is sometimes known as a supervisory
    signal.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Two outcomes (for example, either heads or tails) implies more than one class
    label. An outcome represents a *classification*, as in a classification task.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding two aspects are also shared by our breast cancer supervised learning
    problem—the task at hand. The following points describe the task at hand by offering
    more insight:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a 699-input vector multiclass classification task. This task is characterized
    by historical (predetermined) categorical data and more than one dependent or
    outcome variable (or label).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This task is performed on a dataset that has 699 observations/measurements
    (instances), where each observation row may be described further as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row is composed of 10 attributes; each of these attributes is a predictor
    variable (inputs, `X`), which are also known as input variables (`X`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the 699 observations is historical or predetermined (with the exception
    of certain incomplete observations/rows), and represent (breast mass cell nuclei)
    characteristics of cell nuclei from a breast cancer tissue sample from a needle
    aspirate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are 10 characteristics of the aforementioned breast mass cell nuclei;
    these are just the breast's mass cell nuclei measurements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The classification task is also multidimensional, owing to the fact that 10
    (input) attribute values exist as feature parameters that are passed into `Model`
    to carry out a diagnosis classification on the so-called target class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each instance (row) in the breast cancer dataset represents measurements (from
    digitized images) made on breast mass tissue samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of the classification task is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To identify (or classify) the diagnosis on a new breast cancer sample as belonging
    to either of two diagnoses: malignant (cancerous) or benign (non-cancerous).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To derive a predicted value for the response from the predictors by a process
    of learning (or fitting) a discrete number of targets or category labels (`Y`). The
    predicted value is a categorical response (outcome) variable (output `Y`), also
    known as a response or outcome variable (`Y`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn a predictive function also known as a model; this computes a predictor
    function that predetermines the feature measurements on 10 attributes that will
    be able to classify or identify, the type of diagnosis (benign or malignant).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In [Chapter 1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict the Class
    of a Flower from the Iris Dataset*, we used a supervised learning classification
    algorithm called Random Forests. In this chapter, we will employ what is known
    as the logistic regression classification technique (a Spark ML algorithm). This
    will be the heart of our predictive analysis classification pipeline. To sum this
    up, a high-level view of the breast cancer classification task can be compartmentalized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The classifier algorithm**: This involves the creation of a discriminant
    function or model function that discovers patterns, relationships, or interactions
    between several independent variables and one dependent variable (indexed by the
    model to a binary dummy variable) that is either a nominal or ordinal variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predetermined features**: Measurements or observations that have been labeled
    malignant or otherwise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '** Predicted labels**: Labeling unseen data before arriving at a prediction
    on new, unseen data after a learning process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The end goal of logistical regression is to produce a model that is as well
    fitted (trained) as possible and one that emits a prediction. A prediction, of
    course, is a variable of interest.
  prefs: []
  type: TYPE_NORMAL
- en: The next section is a prelude to a broader discussion on the application of
    the logistic regression statistical technique.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **logistic regression** (**LR**) algorithm, which is employed when building
    a data pipeline in this chapter, is a fresh approach to making a prediction on
    whether a breast cancer mass is malignant or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the outset, the key to understanding the LR algorithm boils down to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '"if (categorical) feature x = …”, then it treats the label as an output that
    is something like this: “label =..”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speaking of categorical features, we may want to understand the relationship
    between two or more of them in the breast cancer dataset. In addition, we are
    also interested in building LR ML models as an efficient data inference to derive
    the concurrent effects of multiple categorical variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will present you with a high-level overview of the LR
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: Salient characteristics of LR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table lists the salient characteristics of LR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf9cb42f-114d-4898-a245-e45c8ae6e1cf.png)'
  prefs: []
  type: TYPE_IMG
- en: LR at a glance
  prefs: []
  type: TYPE_NORMAL
- en: The regression or classification model that we implement for the pipeline in
    this chapter is a specialized type of a generalized linear regression model called
    **binary logistic regression**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we talk about binary logistic regression, we will take a step back,
    refer back to [Chapter 1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict
    the Class of a Flower from the Iris Dataset*, and remind ourselves of the different
    types of variables. One such variable type is that of a response variable, a variable
    whose changes are explained by a so-called explanatory variable. An explanatory
    variable is plotted on the *x *axis of a scatter plot, whereas the response variable
    plotted on the *y* axis is dependent on the former. The following diagram is an
    example of a scatter plot that, though not directly relevant to this chapter,
    has some significance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ba8a82f-d08d-44a3-9cdd-1e93c471ebd2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A scatter plot example
  prefs: []
  type: TYPE_NORMAL
- en: 'Going one step further, we get into the topic of linear regression. This type
    of prediction model takes a bunch of explanatory categorical values as hyperparameters
    that play a direct role in predicting expected response variable values. What
    are the odds of our response variable taking on a certain value? Those odds are
    represented in mathematical terms, which is a probability model that translates
    to a predictor function. A function like this does two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Accepts more than one explanatory (or input) variable feature measurement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models the probabilities of response variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we apply all of this to our breast cancer dataset, we will cite a (fictitious)
    example of a mathematics program admission process from the Case Western Reserve
    University. This process can be described in terms of the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: It is supposed to be a fair, nondiscriminatory process, one that admits students
    coming from various categories or groups to academic programs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An admission process predictor model would predict the probability of the successful
    (or not) admission of a student, given that they belong to a certain gender, race,
    or economic background.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important question to pose is what are the odds of student A being successfully
    admitted into this program? In other words, how do we come up with a `StudentAdmission`
    predictor function (model) that will predict the odds of the `admission status`
    response variable taking a specific value?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `StudentAdmission` model receives a group of explanatory variables. This
    group consists of independent variables representing some characteristics of the
    individual. These are multiple feature measurements. Some features can include
    gender, race, income group, and many more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All that being said, we want to know how binary logistic regression finds its
    niche as an extension of the linear regression model approach. Two examples of
    usage are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, that a binary logistic regression model simply predicts
    whether an incident (an event) took place or not. A researcher with earthquake
    data is interested in analyzing whether an earthquake is bound to happen sometime
    in the future. This kind of response variable is **discrete**. In other words,
    it is noncontinuous, static, or a one-time limited occurrence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The university admission process could be modeled as a binary logistic regression
    model, one that involves more than one explanatory variable or course. This model
    will predict the odds (or probability) of the response variable (`admission status`)
    taking a certain value. Taking this a step further, the predicted discrete value
    in the student admission process is a value of either `0` or `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will list assumptions that will help us formulate a logistical regression
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Binary logistic regression assumptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some assumptions that are made for a binary logistic regression classification
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: The dependent variable should be dichotomous, representing mutually exclusive
    states. There is more than one independent predictor variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlations between predictor variables are represented by the elements of
    a correlation matrix. They can be no higher than 0.9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers must be absent. Outlier presence or absence can be determined by transforming
    predictors into standardized statistical scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The only dataset that is relevant to us in this chapter is the breast cancer
    dataset. This is a classification task whose analysis solution will be a binary
    logistic regression. Before we get to that, we will present this as a much simpler
    dataset to illustrate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Independent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will illustrate the bottom line of linear regression
    with a fictitious example from a survey on males and their luck with snagging
    the next date.
  prefs: []
  type: TYPE_NORMAL
- en: A fictitious dataset and LR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we are going to present you with a fictitious dataset to merely present
    our case on LR so that it can be a candidate for our breast cancer dataset classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: The following example lists data that has been created by a dating website about
    male singles. Listed in the table is a dependent variable that represents whether
    the guy was lucky, which means they were able to work up to a second date with
    the same person within a week after the first date.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two independent variables, and they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Did the guy attend a dating workshop to firm up his dating skills?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second variable measures the guy's desperation on a desperation scale. The
    higher the score, the more desperate the guy is, with 100 being the most desperate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following dating survey table tabulates data pertaining to the dating logistical
    regression problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cf47e02-461d-4b78-9e7a-5711946ce0c3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logistical regression example
  prefs: []
  type: TYPE_NORMAL
- en: An inspection of the dataset tells us that a little more than half of all single
    guys have had a second date in less than a week. This is assuming that the dating
    survey company does not possess any other background data on these singles and
    is applying the best techniques in this (fictitious) dating help industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to know whether those in the **Workshop** group are more likely
    to have another date. Looking at the fourth column, **Cool**, greater coolness
    translates to a better chance of a second date:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf651e9a-b4df-4e18-bd02-f488fe9126b4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Correlation coefficient table
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the table for  **Correlation Coefficients**, those men not in the **Workshop**
    were less likely to have another date and those with a higher **Cool** factor
    were more likely to have a second date.
  prefs: []
  type: TYPE_NORMAL
- en: 'A logistical regression applied on this dataset would have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Response (dependent) variable: **Date**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levels of measurement: **Mean** and **standard deviation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regression function: **Logistic** or **logit**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of feature rows: **20**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we have given you a better understanding of LR. We will wade
    deeper into this topic in the next section. We talked about response variables
    and correlation coefficients, otherwise known as dichotomous variables, and got
    a good handle on all of these. However, we have not yet formulated the LR model
    in mathematical terms. We want to know if using linear regression for building
    a model is appropriate for the breast cancer classification task. As it turns
    out, we will not be able to apply linear regression models to the task at hand.
    Why we turned to LR and not linear regression is one of the points of discussion
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: LR as opposed to linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before choosing between logistic and linear regression methods, here are a
    few pointers that reiterate or restate what we talked about in this chapter and [Chapter
    1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict the Class of a Flower
    from the Iris Dataset*:'
  prefs: []
  type: TYPE_NORMAL
- en: A data scientist working on their experimental data unit is seeking to build
    a model. Naturally, the follow-up question might be why are they interested in
    building an (ML) model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One answer to the previous question might be that the model helps discover patterns
    or the underlying relationship between the predictor (explanatory or independent)
    variables and their response counterparts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speaking of response variables, response variables in a breast cancer dataset
    are categorical, as opposed to other ML classification tasks where the response
    variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unbounded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This brings us clarity, and with it the following working hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression approach for our breast cancer classification task models
    may not work. After all, the response variable `Y` is neither continuous unbounded,
    or normally distributed. Before writing off the linear regression approach for
    our purposes, we will attempt such a formulation anyway, and in the process, shed
    more light on why LR is what we really want.
  prefs: []
  type: TYPE_NORMAL
- en: Formulation of a linear regression classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basis for a mathematical formulation of a linear regression model may be
    broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Left-hand side: The predicted variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Right-hand side represented by `y`: A linear construct that is made up of coefficients,
    a predictor (independent variable), and the arithmetic operators `+` for addition
    and `*` for multiplication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming there are four predictor variables, `PX1`, `PX2`, `PX3`, and `PX4`,
    each of these variables represents the so-called `X`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the outset, we could write an equation representing a linear regression
    model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a catch! Our new linear regression model is characterized by response
    variables that are actually **non-dichotomous**. Well, we could put up a stand
    and say that it is still possible to come up with an improved version of this
    equation, which will represent a linear regression model with dichotomous response
    variables. It turns out that such an improved linear model will not work in actuality,
    for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Our (dichotomous) response variables need to be arbitrarily assigned `0` and
    `1`. This is because we want to represent two mutually exclusive categorical states.
    Those states can be either benign or malignant, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second reason has to do with the fact that because the response variable
    value `Y` is categorical, the predicted value is really the probability that this
    variable accepts a certain value, and not the value itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, our thought process appears to be decidedly in favor of at least
    a regression model with dichotomous response variables, as opposed to a linear
    regression model. How so?
  prefs: []
  type: TYPE_NORMAL
- en: It may be that the model we are looking to build is a function of probabilities,
    an LR equation, distinguished by a left-hand-side representation of a logit of
    `Y` rather than `Y` itself. The next section will weave the ideas presented here
    and build a mathematical formulation of the LR as an equation.
  prefs: []
  type: TYPE_NORMAL
- en: Logit function as a mathematical equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuing on from where we left off in the previous section, this section is
    an attempt at translating those ideas and conclusions into a newer narrative.
    The goal of this section is to have a high-level mathematical formulation for
    LR.
  prefs: []
  type: TYPE_NORMAL
- en: However, with LR being a much more complicated case, we will formulate a simpler
    equation for what is known as a logit function, logit model, or logit odds.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, the logit function at a high level is expressed as `Logit(p)`
    and can be expanded to the mean logit of the odds of `Y`, rather than `Y` itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, here are a few mathematical concepts to help us understand and write
    a logit function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euler number**: Euler number (*e*) = 2.718228183'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural logarithm**: If *e* can be raised to the power *y*, as in *e** ^y* = *x*,
    then the logarithm of *x* to the base *e* is *log[e]*(*x*) = *y*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, a formulation of the logit function becomes something like the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our newly-minted logit function is an equation based on the natural logarithm
    of the ratio of odds or probabilities. The logit function is a model, which is
    characterized by the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: In this logit function, `Ln[p/(1-p)]`, `p/(1-p)` is called the odds of our sample
    being labeled as benign. For example, `Ln[p/(1-p)]` is the natural logarithm or
    log-odds, or simply the logit as `Ln[p/(1-p)]` varying between drop down -∞ to
    +∞.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logit function can be written as `fnLogistic(p) = ln(p/1-p)`, where `p`
    is between `0` and `1`, and where `0` and `1` are the maximum and minimum values
    plotted on the *x* axis, for example, `fnLogistic(0.9) = 2.197224577336`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LR0`, `LR1`, `LR2`, and so on are known as model coefficients or correlation
    coefficients. These model coefficients relate to the predictor (explanatory) variable
    of the predicted (response) variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PX1`, `PX2`, and `PX3` are predictor variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logit function is also a **link function**. It is a link function because
    it links the natural log of the probabilities on the left of the logit function
    to the linear equation made up of predictor variables and their respective coefficients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p` is said to be bounded between 0 and 1, which is the case here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A typical logit function curve looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b835b690-a0ca-4189-9417-826d70661550.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logit model graph
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we haven't discussed LR, which is a little more complicated than
    the logit model.
  prefs: []
  type: TYPE_NORMAL
- en: 'An interpretation of the graph is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The nonlinear graph will depict the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* axis: Logit values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y* axis: Probabilities (or odds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the graph, for a probability of **0**, the logit value is **0.5**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To recap, we started with linear regression and then progressed to a discussion
    on the logit model. Understanding what the logit function is sets the stage for
    LR in the context of the breast cancer classification task.
  prefs: []
  type: TYPE_NORMAL
- en: LR function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We said before that LR is harder than the logit function. However, the LR formulation,
    as we shall see, is a good fit for this problem. We want to make a prediction
    on the fate of a sample being either benign or malignant. In other words, a prediction
    on a particular breast cancer tissue sample can only take one of two mutually
    exclusive values, based on feature measurements such as clump thickness, uniformity
    of cell size, and many more. Each of these feature measurements can be `X1`, `X2`,
    and `X3`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the beginning of a formulation of the LR function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core concept behind the LR function is the so-called inverse function,
    which is written down as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca1342bb-fb83-4473-8ba0-5a38a02994ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a brief interpretation of the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**p** in the preceding equation is simply a function of feature measurements
    of breast cancer samples represented by `X1`, `X2`, `X3`, and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rewriting **p** as `fLogistic(X1,X2,..)`, we have a complete function definition
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It turns out that the logit function we discussed earlier and our logistic regression
    function are inverses of each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important points to remember about logistic regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a dichotomous response variable representing the odds of an outcome
    occurring or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A non-linear relationship between:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The categorical input (independent feature measurements) values plotted on the *x*
    axis. These are also known as predictor variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probabilities on the *y*-axis. These are predicted values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Very important: The coefficients `LR0`, `LR1`, and `LR2` are computed from
    our training dataset. Our training dataset has known or predetermined input measurements
    and output labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we have what we need to switch focus to the Spark ML API for
    an implementation of the LR mathematical model that we just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will build two data pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline using the Random Forests algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pipeline using the LR method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are familiar with Random Forests from [Chapter 1, ](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml)*Predict
    the Class of a Flower from the Iris Dataset*. LR is a proven method that is backed
    by established statistical techniques that ML has found very handy for solving
    binary classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: The following *Getting started* section will get you started with the implementation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best way to get started is by understanding the bigger picture—gauging
    the magnitude of the work ahead of us. In this sense, we have identified two broad
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the prerequisite software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developing two pipelines, starting with data collection and building a workflow
    sequence that could end with predictions. Those pipelines are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Random Forests pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A logistical regression pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will talk about setting up the prerequisite software in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up prerequisite software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, please refer back to the *Setting up the prerequisite software* section
    in [Chapter 1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict the Class
    of a Flower from the Iris Dataset*, to review your existing infrastructure. If
    need be, you might want to install everything again. The chances of you having
    to substantively change anything are slim.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, here are the upgrades I recommend:'
  prefs: []
  type: TYPE_NORMAL
- en: JDK upgrade to 1.8.0_172, if you have not already done so
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala from 2.11.12 to an early stable version of 2.12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 2.2 to 2.3 where 2.3, is a major release with numerous bug fixes, which
    is why hence it is recommended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing this book, Java 9 and 10 don't appear to work with Spark.
    That might change. For the purposes of this chapter, your local Spark shell will
    be the development environment of choice.
  prefs: []
  type: TYPE_NORMAL
- en: With the prerequisites out of the way, we are ready to jump right into developing
    pipelines. This journey starts in the *Implementation objectives* section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We fulfilled our implementation objectives for [Chapter 1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml),
    *Predict the Class of a Flower from the Iris Dataset*. In that chapter, early
    on, we developed the beginnings of a workflow process. This is depicted in the
    following diagram, which will help us frame the implementation objectives for
    this chapter as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ae72d8d-a9ba-4d16-88f5-cebbd65d6a66.png)'
  prefs: []
  type: TYPE_IMG
- en: Stages in a preliminary workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the current chapter also deals with a multiclass classification task
    like the one before it, the four boxes that are shown in the preceding diagram
    are our guide to setting up implementation objectives for this chapter. The broad
    high-level objectives are:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Data Collection** step followed by an **Exploratory Data Analysis** (**EDA**)
    step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Data Cleaning**/**Preprocessing** step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handing off data to an algorithm; there are models to be trained (fitted) and
    predictions to be generated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This paves the way for a more complete list of implementation objectives, and
    they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the breast cancer dataset from the UCI Machine Learning Repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving a dataframe for EDA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carrying out preliminary EDA in the Sandbox Zeppelin Notebook environment (or
    Spark shell), and running a statistical analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developing the pipeline incrementally in Zeppelin and porting the code into
    IntelliJ. What this means is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new Scala project in IntelliJ, or importing an existing empty project
    into IntelliJ, and creating Scala artifacts from code that was incrementally developed
    in the Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not forget to wire up all the necessary dependencies in the `build` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpreting the results of the pipeline:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How well did the classifier perform?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How close are the predicted values to those in the original dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will start working on these implementations one by one, starting with
    getting the Wisconsin Breast Cancer Data Set from the UCI Machine Learning Repository.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objective 1 – getting the breast cancer dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Head over to the UCI Machine Learning Repository website at [https://archive.ics.uci.edu/ml/datasets/bcw](https://archive.ics.uci.edu/ml/datasets/bcw)
    and download the `Data` folder by clicking on Data Folder. Extract this folder
    someplace convenient and copy `bcw.csv` into the root of your project folder,
    which we will call `Chapter2`. At this point, `Chapter2` will be empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may refer back to the project overview for an in-depth description of the
    breast cancer dataset. We depict the contents of the `bcw.data` file here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fc9f404-d847-4dfe-94a9-049f4b0af745.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A snapshot of the breast cancer dataset with 699 rows
  prefs: []
  type: TYPE_NORMAL
- en: The breast cancer dataset that we just downloaded is multivariate, meaning it
    includes a set of more than one independent variable. Before performing any EDA
    on it, we need to create an abstraction over the dataset, which we call a dataframe. How
    we create a dataframe as a prelude to EDA is the goal of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objective 2 – deriving a dataframe for EDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We downloaded the Wisconsin Breast Cancer data file into the `Chapter2` folder
    and renamed it `bcw.csv`. The process of `DataFrame` creation starts with loading
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will invoke the `read` method on `SparkSession` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `read` method that has been returned produces `DataFrameReader`. Because
    our dataset is a CSV file, we want to tell Spark about it by invoking the `format`
    method on `DataFrameReader` by passing in the `com.databricks.spark.csv` format
    specifier string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, `DataFrameReader` needs an input data source option in the form
    of a key-value pair. Invoke the `option` method with two arguments, a key `"header"`
    of type string and its value `true` of type Boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the code is invoking the `option` method again (on the `DataFrameReader)`
    with an argument called `inferSchema` and a `true` value. With the `inferSchema`
    method call, we want Spark to figure out the schema of our input data source and
    return our `DataFrameReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load `bcw.csv` by invoking the `load` method and passing it to the path
    to the dataset file. External data sources such as our dataset require a path
    for Spark to be able to load the data so that `DataFrameReader` can process the
    file and return the `DataFrame`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We now have a breast cancer dataframe! This completes the *Implementation objective
    2 – deriving a dataframe for EDA* section. Our next step is to run a preliminary
    statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, before we move on to the next step, here is a view of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd772ab5-cd71-410d-8bd1-748be5055d54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dataframe with raw data
  prefs: []
  type: TYPE_NORMAL
- en: It looks like we have data in our `DataFrame`, which is now ready for EDA.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – conducting preliminary EDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we will perform a fairly simple statistical analysis on our dataset.
    This will provide us with useful, though preliminary, statistical insights such
    as mean, median, range, and standard deviation, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'To proceed with the preliminary EDA, let''s invoke the `describe` method with
    the required column names as parameters. This will give us a new `DataFrame` called
    `stats`. Invoking a `show` method on `stats` will produce a table of statistical
    results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f92ee6f5-15cb-4485-b4d3-ac9030619620.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Statistical analysis
  prefs: []
  type: TYPE_NORMAL
- en: Although the output is ugly to look at and is mangled, we see statistical numbers
    such as the `count`, `mean`, standard deviation, minimum, and maximum. Yes, the
    dataset has 699 rows of continuous, discrete (or categorical) values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the preliminary exploratory data analysis is complete, we proceed to
    the next step, where we will load the dataset into Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – loading data and converting it to an RDD[String]
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we will load the data again, but in a slightly different manner.
    The goal of this phase of the data analysis is to produce a `DataFrame` where
    the data has been read into an `RDD[String]`. First, we will need a path to the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We have just created `dataSetpath`. In the following code, we will pass the
    path to the dataset into the `textFile` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `textFile` method returned an `RDD[String]`. To check if data was loaded
    into the RDD, we need to invoke the `first` method on `firstRDD` to give us the `header`
    content. We will leave this as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to know the number of partitions in our RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `getNumPartitions` method returned the number of partitions in `firstRDD`. 
    Since an RDD allows us to work with data at a low level, we will continue to reorganize
    and massage this data as required.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we want to inspect the RDD. We want to reorganize and repackage
    data into arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – splitting the resilient distributed dataset and reorganizing individual
    rows into an array
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To split the dataset, we will start with the RDD partitions. It is helpful to
    think of an RDD partition in the following manner.
  prefs: []
  type: TYPE_NORMAL
- en: Each partition can be visualized as one long string consisting of rows of data
    separated by `"\n"`. We want to break down this long string into its constituent
    string, by splitting them along the `"\n"` separator. Shortly, we will try a `flatMap`
    operation on our RDD, `firstRDD`. Each constituent string is a `Row` that represents
    a row in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do `flatMap` and pass to it an anonymous function, which will be invoked
    on rows separated by a `"\n"` character, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code does a `flatMap` flattening operation, resulting in the
    creation of a new `RDD[String]` that will hold all of these strings (each string
    is a row in our dataset). At this point, we will `split` (along the comma between
    individual characters of that row) a `String` producing an `RDD[Array[String]]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `RDD[Array[String]]` naturally implies that the RDD contains more than one
    `Array[String]`. How many of these arrays are in this RDD?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Invoking `count` on our RDD returns an array count of `700`, which is what it
    should be.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – purging the dataset of rows containing question mark characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we go any further, let's eyeball the raw dataset again. If you looked
    closely, you will notice that the dataset contains a `?` character in some places.
    Actually, this character starts appearing in some rows in the seventh column,
    starting on the 25^(th) row. The 25^(th) row, with the `?` character, is displayed
    in the following diagram. That them is a problem, which needs a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, a visual inspection of the dataset can reveal the presence of extraneous
    characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a snapshot of the Wisconsin Breast Cancer Data Set with the
    `?` character in the 25^(th) row and the sixth column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb958bd9-a451-4a2e-a353-2116ffe8bb41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dataset showing ? characters
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, it is not just row 25^(th) that has a `?` character. There are likely
    other rows with the extraneous `?` character that needs to be purged. One solution
    appears to be to invoke a `filter` operation on our `rddArrayString`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As evident from the preceding code, we just ran the `filter` operation, which
    returned a new `RDD[Array[String]` that we called `purgedRDD`. Naturally, we may
    want to count the number of rows left in the dataset that we believe are relevant
    for data analysis. That is the goal of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – running a count after purging the dataset of rows with questionable
    characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now run `count` on our new `purgedRDD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, in the preceding code, we invoked the `count` method on `purgedRDD`. Spark
    returned a value of `684`. Apparently, 16 rows contained `?` characters. After
    all, many datasets like this one need a preprocessing step or two. For now, we
    will proceed with the next steps in data analysis, secure in the knowledge that
    Spark will probably not report an error, especially at the point where we want
    a new two-column `DataFrame` containing a consolidated feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to get rid of `header`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – getting rid of header
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each of the inner arrays shown earlier holds rows that represent feature measurements,
    and a row representing the dataset `header`. The following line of code converts
    our RDD into an `Array`, containing arrays that themselves contain rows as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `drop` method got rid of `header`. Next, we will move on and create a new
    `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – creating a two-column DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are close. In this section, the goal is to create an input feature vector,
    and the steps are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `Vectors` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the `map` operation on the `Array`, we will iterate over each row of
    our header-free dataset. Then, we transform each row in turn, operating on every
    single column containing predetermined cell nuclei measurements. These columns
    are converted to doubles by using the `dense` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `map` operation processes the entire dataset and produces `featureVectorArray,`
    a structure of type `Array[(Input Feature Vector, String representing the Class)]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Okay, we created `featureVectorArray`, an `Array` consisting of a set of `(Vector,
    String)` tuples. This `Array` is now ready to be converted into `DataFrame`. That
    is the goal of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Step 8 – creating the final DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of this section is to create a final version of our analysis-ready
    `DataFrame`. The `createDataFrame` method available on `SparkSession` is suitable,
    and is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As seen earlier, the new `DataFrame` has two columns, which are not very readable
    named `_1` and `_2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we want is a renamed `DataFrame` with two readable columns as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A feature vector column that is named `bc-diagnosis-label-column` and a target
    variable label column named `bc-indexed-category-column`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the way, a target variable in ML terminology denotes what is being predicted.
    For example, it could be a `0` for benign or `1` for malignant. Since a target
    variable is associated with the output, it can also be termed an outcome or output
    variable. Defining a target variable is an integral part of the binary classification
    model creation step; a target variable in statistics terminology is the same as
    a response variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get a renamed `DataFrame`, we will transform it a little, and we will do
    this by creating two methods as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following line of code, invoke the `toDF` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking the `toDF` method creates a `DataFrame` with the desired column names.
    Invoking `show` on `dataFrame2` will result in the following display:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding listing confirms that the `DataFrame` you wanted is what you
    got. In the next section, we will use this `DataFrame` to build a data pipeline
    with two algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: The Random Forest algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will build a Random Forest pipeline first.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest breast cancer pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good way to start this section off is to download the `Skeleton` SBT project
    archive file from the `ModernScalaProjects_Code` folder. Here is the structure
    of the `Skeleton` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d7e9856-8def-4c5a-b296-ce02f3458bec.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Project structure
  prefs: []
  type: TYPE_NORMAL
- en: Instructions to readers: Copy and paste the file into a folder of your choice
    before extracting it. Import this project into IntelliJ, drill down to the package
    `"com.packt.modern.chapter"`, and rename it `"com.packt.modern.chapter2"`. If
    you would rather choose a different name, choose something appropriate. The breast
    cancer pipeline project is already set up with `build.sbt`, `plugins.sbt`, and
    `build.properties`. You only need to make appropriate changes to the organization
    element in `build.sbt`. Once these changes are done, you are all set for development.
    For an explanation of dependency entries in `build.sbt`, please refer back to
    [Chapter 1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict the Class of
    a Flower from the Iris Dataset*. Unless we introduce new dependencies for this
    project, we will stick with the `build.sbt` that came bundled in the `Skeleton`
    project.
  prefs: []
  type: TYPE_NORMAL
- en: All that being said, we will now start the implementation. The first step will
    be to create Scala code files in IntelliJ. Note that the complete code is available
    in your downloaded folder, `ModernScalaProjects_Code`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – creating an RDD and preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create a Scala file called `BreastCancerRfPipeline.scala` in the `com.packt.modern.chapter2` package.
    Up until now, we relied on `SparkSession` and `SparkContext`, which are what `spark-shell`
    gave us. We need to create our `SparkSession` now, which will give us `SparkContext`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `BreastCancerRfPipeline.scala`, after the package statement, place the following
    import statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` inside a trait, which we shall call `WisconsinWrapper`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Just one `SparkSession` is made available to all classes extending from `WisconsinWrapper`.
    Create `val` to hold the `bcw.csv` file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method to build the `DataFrame`. This method takes in the complete
    path to the breast cancer dataset as a `String` and returns `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `DataFrame` class by updating the previous `import` statement for
    `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a nested function inside `buildDataFrame` to process the raw dataset.
    Name this function `getRows`. The `getRows` function takes no parameters but returns
    `Array[(Vector, String)]`. The `textFile` method on the `SparkContext` variable
    processes `bcw.csv` into `RDD[String]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The resulting RDD contains two partitions. Each partition, in turn, contains
    rows of strings separated by a newline character, `"\n"`. Each row in the RDD
    represents its original counterpart in the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will preprocess this RDD; this entails creating a single
    consolidated input `features` column out of the original four feature columns.
    We start this process by invoking `flatMap` and passing a function block to it.
    After successive transformations, which are listed in the following code, we should
    be able to create an array of type `Array[(org.apache.spark.ml.linalg.Vector,
    String)]`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Vector`, in this case, represents a row of feature measurements. The Scala
    code to give us `Array[(org.apache.spark.ml.linalg.Vector, String)]` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, drop the `header` column, but not before performing a `collect` that
    returns an `Array[Array[String]]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `header` column is now eliminated. We will now import the `Vectors` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, transform the `Array[Array[String]]` into `Array[(Vector, String)]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will invoke the `createDataFrame` method with a parameter called `getRows`.
    This returns a `DataFrame` with `featureVector` and `speciesLabel` (for example,
    `bcw-Setos`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The new `DataFrame` contains two rows:'
  prefs: []
  type: TYPE_NORMAL
- en: A column named `bcw-features-column`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A column named `bcw-species-label-column`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to index `species-label-column` by converting the `"bcw-Setosa"`, `"bcw-Virginica"`,
    and `"bcw-Versicolor"` strings into doubles. We will use a `StringIndexer` to
    do that.
  prefs: []
  type: TYPE_NORMAL
- en: Now, create a file called `bcwPipeline.scala`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an object called `bcwPipeline` that extends our `bcwWrapper` trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `StringIndexer` algorithm class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a `StringIndexer` algorithm instance. `StringIndexer` will map `species-label-column`
    to an indexed learned column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The indexer transforms the `bcw` type column into a column of type double. This
    is an example where a categorical variable is disguised as a quantitative variable.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – creating training and test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s split our dataset in two by providing a random seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The new `splitDataset` contains two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training `Dataset` is a dataset containing `Array[(Vector, bcw-species-label-column:
    String)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test `Dataset` is a dataset containing `Array[(Vector, bcw-species-label-column:
    String)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confirm that the new `Dataset` is of size `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the training `Dataset` to the `trainSet` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the testing `Dataset` to the `testSet` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will see how to create a Random Forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a classifier and pass into it hyperparameters. We will set the following
    parameters first:'
  prefs: []
  type: TYPE_NORMAL
- en: A `"features"` column name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An indexed `"label"` column name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of features to be considered per split (we have 150 observations
    and four features) that will make our `max_features 2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since `bcw` is a classification problem, the `''sqrt''` setting for `featureSubsetStrategy`
    is what we need. In addition, we will pass in other parameters such as impurity,
    the number of trees to train, and many more, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Impurity settings—values can be gini and entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of trees to train (since the number of trees is greater than 1, we set
    the tree maximum depth, which is a number equal to the number of nodes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required minimum number of feature measurements (sampled observations),
    also known as the minimum instances per node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This time, we will employ an exhaustive grid search-based model selection process,
    based on combinations of parameters, where parameter value ranges are specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `randomForestClassifier` instance. Set the features and `featureSubsetStrategy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Start building a `Pipeline` that has two stages, an `indexer` and a `Classifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Next, set the hyperparameter `num_trees` (number of trees) on the classifier
    to `15`, a `Max_Depth` parameter, and an impurity with two possible values of
    gini and entropy. Then, build a parameter grid with all three hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training the Random Forest classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will split our existing training set (the one used to train the model)
    into two:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Validation set**: This is a subset of the training dataset which is used
    to get a preliminary estimate of the effectiveness of the level of skillfulness
    attained by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training set**: A training set is that percentage of dataset that the model
    learns from. This learning process is called training the model. Also because
    the model learns from this data, the data is said fit the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can accomplish a split by creating an instance of the `TrainValidationSplit`
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: On this variable, set a seed, set `EstimatorParamMaps,` set the `Estimator`
    with `bcwPipeline,` and finally set the training ratio to `0.8`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, do a fit and transform with our training `Dataset` and testing `Dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: Great! Now, the classifier is trained. In the next step, we will apply this
    classifier to the testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – applying the classifier to the test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of our validation set is to be able to make a choice between models.
    We want an evaluation metric as well as hyperparameter tuning. Now, we will create
    an instance of a validation estimator called `TrainValidationSplit`, which will
    split the training set into a validation set and a training set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will fit this estimator over the training dataset to produce a model
    and a transformer that we will use to transform our testing dataset. Finally,
    we will perform validation for hyperparameter tuning by applying an evaluator
    for a metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new `ValidatedTestResults DataFrame` should contain the following columns,
    including three newly generated columns—`rawPrediction`, `probability`, and `prediction`,
    and some additional ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bcw-features-column`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bcw-species-column`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rawPrediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probability`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let''s generate a new dataset. Invoke the `select` method on the `validatedTestResults` dataset
    and pass the column expressions for `prediction` and `label` into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We will revisit these test results towards the close of this chapter, where
    we will be evaluating the classifier. At that point, we will explain how to interpret
    these results and how they tie into the main goal of this chapter predicting the
    class of a breast cancer mass diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – evaluating the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will evaluate the accuracy of the mode output results on
    the test result. Evaluation starts by creating an instance of `MulticlassEvaluator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, on `MulticlassEvaluationEvaluator`, we set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `"label"` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metric name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction column `label`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we invoke the `evaluate` method with the `validatedTestResults` dataset.
    Note the accuracy of the model output results for the testing dataset from the
    `modelOutputAccuracy` variable. The other metric of note to evaluate is how close
    the predicted label value in the `predicted` column is to the actual label value
    in the (indexed) `"label"` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to extract the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`MulticlassMetrics` includes two computed metrics that we extract by giving
    a reading of the `accuracy` and `weightedMetrics` variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – running the pipeline as an SBT application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the root of your project folder, issue the `sbt console` command, and in
    the Scala shell, import the `bcwPipeline` object and then invoke the `main` method
    of `bcwPipeline` with the `bcw` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The classifier reported on two metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will package the application.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – packaging the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the root folder of your SBT application, we want to generate an Uber JAR.
    We will run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This command generates an Uber JAR file, which can then be easily deployed
    into `[local]` in standalone deploy mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d350f370-8839-476f-8510-b638736fe56f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The application JAR file
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline JAR file is available under the target folder. In the next section,
    we will deploy the application into Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Step 8 – deploying the pipeline app into Spark local
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the root of the application folder, issue the `spark-submit` command with
    the class and JAR file path arguments, respectively. If everything went well,
    the application will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load up the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performs EDA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create training, test, and validation datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Random Forest classifier model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the accuracy of the model, which is the most important part of the ML classification
    task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To accomplish step 6, we apply our trained Random Forest classifier model to
    the test dataset, which is data that has not been seen by the model yet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unseen data could be likened to new data that the classifier needs to predict
    on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our goal at the beginning of this was to classify the diagnosis of a breast
    cancer mass that is exemplified by specific features in the test dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the model to the test dataset results in a prediction of the diagnosis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pipeline runs an evaluation process, which is all about checking the model
    reports the correct diagnosis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, pipeline reports back on how important a certain feature of the breast
    cancer dataset turned out to be in relation to the others. As a matter of fact,
    it turns out that a certain feature is more important than others in carrying
    out the classification task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding summary listing concludes the Random Forests section and brings
    us to the beginning of a brand new section on the topic of creating a logistic
    regression pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: LR breast cancer pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before getting down to the implementation of a logistic regression pipeline,
    refer back to the earlier table in section *Breast cancer dataset at a glance *where
    nine breast cancer tissue sample characteristics (features) are listed, along
    with one class column. To recap, those characteristics or features are listed
    as follows for context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**c****lump_thickness**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size_uniformity**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**shape_uniformity**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**marginal_adhesion**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**epithelial_size**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bare_nucleoli**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bland_chromatin**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**normal_nucleoli**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mitoses**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s get down to a high-level formulation of the logistic regression
    approach in terms of what it is meant to achieve. The following diagram represents
    the elements of such a formulation at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba37e64b-0234-427d-bfa4-589093753c8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Breast cancer classification formulation
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram represents a high-level formulation of a logistic classifier
    pipeline that we are aware needs to be translated into an implementation in Spark
    and Scala. Here are a few helpful points to get you started:'
  prefs: []
  type: TYPE_NORMAL
- en: What are some interesting attributes that we can choose to come up with predictions?
    Attributes or features are like `if` statements and the predicted label is the
    answer. For example, if it looks like a fish, is 100 feet long, and is a mammal,
    it must be a whale. We must identify those `if` statements or attributes with
    the express purpose of making predictions. Of course, a prediction must classify
    a tissue sample as either malignant or benign.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a classifier model with LR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is our class column in the breast cancer dataset, which represents the
    label. This column holds known (or predetermined) label values that `"label"`
    each feature measurement row with either malignant or benign.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the entire dataset, an experimental unit of known labels for known measurements,
    is said to be labeled either malignant or benign.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will lay out our implementation objectives, what our
    implementation goals will be, and how we plan on implementing them.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will kickstart this section by listing the following implementation objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation objective 1**: Depicting what we believe are fundamental pipeline
    building blocks, rough workflow stages in the actual pipeline, and where each
    block is visualized as being connected to the next, implying flow of data and
    the transformation of data. A state of connection implies a set of workflow stages
    placed in a sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation objective 2**: Core building blocks of the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation objective 3**: Spark ML Workflow for the breast cancer classification
    task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation objective 4**: Developing two pipeline stages and assigning
    an indexer and logit model to each of these stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation objective 5**: Evaluating the binary classifier''s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we get on with implementation objectives 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objectives 1 and 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following diagram depicts a **DataFrame** block that progresses through
    a transformation process into the **FeatureVector creation** block. A **Feature
    Vector** and an unindexed label (not shown in the following diagram for simplicity)
    make up a new (transformed) **DataFrame**. The **FeatureVector creation** block
    (or stage) is a precursor to **Classifier Model **creation. The last block is
    a **Prediction** stage where predictions are generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a succinct description of what is to be implemented in the code later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1d596f3-8a93-4aed-8902-3a1aae9420db.png)'
  prefs: []
  type: TYPE_IMG
- en: Core building blocks of the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram makes no mention of splitting the `DataFrame[Feature
    Vector and Label]` into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: A training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A testing dataset, which is input data on which the model is fitted (trained)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two datasets are represented by a **training** block and a **testing**
    block in the diagram in the next section instead. Implementation objectives 1
    and 2 were laid out in this section. Implementation objective 3 is laid out in
    the topic that follows.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objective 3 – Spark ML workflow for the breast cancer classification
    task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start working on implementation objective 3\. This forms the basis for
    our logistic regression pipeline. This pipeline is divided into two functional
    areas—a training block depicted in the diagram as training, and a testing block
    depicted as testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We filled out the **training** block with four pipeline stages, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading Data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Extraction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Fitting (Training)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Likewise, the testing block has four stages of its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading Data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Extraction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two blocks don''t appear to be all that different. However, there is more
    to it than meets the eye. We will now lay out a new ML workflow diagram in terms
    of the training, testing, and Spark ML components, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b05ece70-273c-4948-b0be-4dc9a9f6dcad.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark ML workflow for the breast cancer classification task
  prefs: []
  type: TYPE_NORMAL
- en: An arrow from the training block to the testing block indicates a data transformation
    workflow starting in the training block and proceeding into the testing block.
  prefs: []
  type: TYPE_NORMAL
- en: Our preceding ML workflow diagram is a step forward. It is a precursor of sorts
    to the actual pipeline, whose implementation details will be laid out in *Implementation
    objective 4—coding steps for building the indexer and logit machine learning model *section.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we must note that the implementation critically depends on leveraging
    the following Spark ML API components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics Evaluator**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have enough information to get to the next level that is *Implementation*
    *objective 4—coding steps for building the indexer and logit machine learning
    model* section where we will go through all the motions of building a two-stage
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objective 4 – coding steps for building the indexer and logit
    machine learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the outset, fulfilling implementation objective 5 requires that we import
    the following. Create an empty Scala file in the following package and add the
    following imports in.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the imports are in, create a new Scala object called `BreastCancerLrPipeline`
    and have this class extend from the `WisconsinWrapper` trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `WisconsinWrapper` trait contains code that creates a `SparkSession` for
    us. It also contains a method that takes in the dataset and creates a `DataFrame`
    from it. The imports also bring in the following, which is important for implementation
    tasks. For example, you will note that they are necessary for importing the Spark
    ML API for `LogisticRegression`, which is one of the algorithms used in binary
    classification. We needed APIs to compute binary classification metrics. That
    said, we will move on to the next task, where we talk more about our trait, `WisconsinWrapper`.
  prefs: []
  type: TYPE_NORMAL
- en: Extending our pipeline object with the WisconsinWrapper trait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`WisconsionWrapper` contains a trait called `WisconsinWrapper`, which contains
    the following code components:'
  prefs: []
  type: TYPE_NORMAL
- en: A `SparkSession` called `lazy val`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `val` representing the path to the breast cancer dataset, `bcw.csv` (this
    file is available in the root of the project folder)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tuple holding string representations of columns for `"features"` and `"label"`
    in `DataFrame`, which we will create shortly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method to build `DataFrame`. It takes in the fully qualified path to the dataset
    path with a method named `buildDataFrame()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Wrapper` trait is depicted as follows, and includes all four code components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c98695e5-286d-43b8-8f5e-ee0be9ad2af2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: WisconsinWrapper
  prefs: []
  type: TYPE_NORMAL
- en: Readers may grab a copy of the `WisconsinWrapper` trait and paste this file
    into their SBT project. The SBT project is available under the `ModernScalaProjects_Code`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a declaration for `object BreastCancerLrPipeline` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We now have an empty `object` body, so we will add in an `import` for the `StringIndexer`.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the StringIndexer algorithm and using it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need the `StringIndexer` algorithm to index values in the `"label"` column.
    That is why `StringIndexer` is imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`StringIndexer` is an algorithm that can take a list of hyperparameters. Two
    such parameters are set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created an `indexer StringIndexer` algorithm instance. The next step
    will be to fit the model on a new generic `DataFrame` that we will build using
    the `buildDataFrame` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting `StringIndexerModel` is transformed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `show` method now displays the first `20` rows of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/171ec8a6-1b46-42f3-8b6a-71398790669a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Result of the fit and transform of StringIndexer
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a reason for the `"features"` column to coexist with `"bcw-diagnoses-column"`.
    The `"features"` that are present in the feature vector are attributes that are
    closely tied to the diagnosis of a certain tissue sample. `"bcw-diagnoses-column"`
    represents the class column in the original dataset. This is a binary classification
    problem where the category cannot have a numerical measure, so we must artificially
    assign a value of either `0` or `1`. In this case, `2` and `4` are standing in
    for benign and malignant, respectively. The `"label"` column beside the `"features"`
    column bears two kinds of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `StringIndexer` indexed values in the `"bcw-diagnoses-column"` from the
    original `DataFrame` are produced by the `buildDataFrame` method and are assigned
    values of `0.0` to `2.0` and `1.0` to `4.0`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will venture deeper into ML territory. As with any ML exercise, it
    is common to split a dataset into a training set and testing set. That is exactly
    what we will do in the next coding step.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the DataFrame into training and test datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will split our `DataFrame` in two:'
  prefs: []
  type: TYPE_NORMAL
- en: Training set—75%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing set—25%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The training set is used to train (fit) the model, and the remaining 25% will
    be put to use for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: To verify that our split went well, we will run the `count` method on both the
    `trainDataFrame` and `testDataFrame` dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: We will leave this as an exercise to the reader. Next, we will move on to creating
    a `LogisticRegression` classifier model and passing parameters into it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LogisticRegression classifier and setting hyperparameters on it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `LogisticRegression` classifier can take hyperparameters, which we will
    set by using the appropriate setter methods from the `LogisticRegression` API.
    Since Spark ML has support for elastic net regularization, we will pass this as
    a parameter first. We also want two additional parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The `"features"` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An indexed `"label"` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: What we just did was start the training process by creating an LR classifier
    model. We are now in a position to train our LR model on the training dataset
    by making an association between input feature measurements and their labeled
    output. To recap, we passed in a `"features"` column, a `"label"` column, and
    an elastic net coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will execute our model with a transformation operation and testing
    dataset data.
  prefs: []
  type: TYPE_NORMAL
- en: Running the LR model on the test dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now invoke the `transform` method on our `LogisticRegression` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The `transform` method invocation returns a new `DataFrame`. Not only that,
    that model transformation step resulted in three new columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rawPrediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`probability`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictions`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will display the first `25` rows of this `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'See the following table to look at the displayed predictions, which were made
    by running our LR models on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fe43848-6883-416c-88bc-75e88bf3558f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Three new columns resulting from model transformation
  prefs: []
  type: TYPE_NORMAL
- en: Spark's pipeline API provides us with the necessary tools to help us build a
    pipeline. A pipeline is a workflow and consists of a sequence of stages that we
    call pipeline stages. As we shall see later, each of these stages is executed
    in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being said, we will now get on with the next order of business—creating
    a data pipeline with the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: A logit model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An indexer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a breast cancer pipeline with two stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the pipeline and add two stages to it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s do something with our pipeline. We can do the following things
    right away:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the pipeline with the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run a `transform` operation with the test set data on our derived `pipelineModel`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will make predictions by running a `transform` operation on the `pipelineModel`
    with the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step focuses on obtaining quantifiable measures. These are key performance
    indicators, metrics that with facts and figures assess how each one of our algorithms
    fared. How do they stack up with respect to one another? What graphical evaluation
    tools are available that help us assess the performance of a certain algorithm
    contributing to a particular binary classification? To understand what it takes
    to perform this evaluation, we might want to ask the following question first:
    how close is the predicted value of a certain breast cancer sample to a predetermined
    label?'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objective 5 – evaluating the binary classifier's performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section is all about obtaining evaluations, metrics, and supporting Spark
    ML APIs. In this section, we will go into depth on the importance of the evaluation
    step regarding quantifiable measures of effectiveness, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Our breast cancer classification task is a supervised learning classification
    problem. In such a problem, there's a so-called **true output**, and a classifier
    or ML model generated prediction output for each individual feature measurement
    or data point in our breast cancer dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have now turned our attention to evaluating the performance of our binary
    classification algorithm by deriving certain metrics. That said, the question
    is this: is pure accuracy enough to gauge the correctness of our classifier''s
    evaluation effort? Here, pure accuracy is trying to simply tell whether the prediction
    was correct or not. Okay, what is a better method? We can employ a binary classification
    evaluator to evaluate the correctness and hence have a measure of the performance
    regarding this kind of accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Circling back to the same topic of pure accuracy, the question to ask again
    is this: is this a good enough metric? It turns out that pure accuracy is not
    a great metric, because it does not take into account the type of error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the aforementioned reason, we will derive better metrics, such as the **area
    under ROC curve** (**AUC**) and the **area under the precision recall curve**
    (**AUPCR**). We will employ Spark's `BinaryClassificationMetrics` to compute such
    metrics for us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will get on with the implementation part of the metric derivation
    process. First, we will create a `BinaryClassificationEvaluator`. We will reiterate
    this evaluator and evaluate predictions with a kind of metric or score that we
    will call pure accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We just computed a so-called pure accuracy score that we called `modelOutputAccuracy`
    in the preceding line of code. As an exercise, readers are invited to determine
    their pure accuracy score. The question they could pose is this: is this score
    useful? Is it a naive score?'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that accomplished, we will now turn our attention to the next task at
    hand, deriving a new dataframe by running a `select` operation on our predictions
    `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We are not done yet. We will convert our `"predictions"` and `"label" DataFrame`
    that we derived in the preceding code to an `RDD[Double, Double]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The RDD is now at hand. But why are we doing this? The answer is this: we talked
    about deriving better, meaningful, and not naive metric scores. We will now create
    a `BinaryClassificationMetrics` instance that we will call `classifierMetrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`BinaryClassificationMetrics` provides us with tools to derive meaningful evaluation
    metrics for our breast cancer binary classification task. At its core, binary
    classification is an ML approach to classifying new, unclassified, incoming data
    under either of two mutually exclusive categories. For example, our classifier
    classified a breast cancer sample as either benign or malignant, but not both,
    of course. More specifically, the breast cancer binary classifier pipeline predicted
    the probability of a target breast cancer sample belonging to one of two outcomes,
    either benign or malignant. This looks like a straightforward yes or no-type prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Sure, our model performed and did the heavy lifting. However, we want to put
    its performance to the test. To do that, we need numeric metrics that, if properly
    thought through and computed, will tell us the model's performance story in ways
    that are meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are those measures and when do they assume relevance? When an experimental
    analysis unit is balanced—the number of breast cancer samples like our breast
    cancer dataset—the following measures assume relevance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive Rate** (**TPR**): This is a ratio of the **true positives**
    (abbreviated as **TPs**) in the predicted output to the **false negatives** (abbreviated
    as **FNs**). Mathematically, it is represented as follows: *TPR* = *TPs* / (*TPs*
    + *FNs*). TPR is known by the terms **Hit Rate**, **Recall**, or **Sensitivity**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive Rate** (**FPR**): Mathematically, it is represented as *FPR*
    = *1* - (*TNs */ (*TNs *+ *FPs*), where **TNs** is a stand-in for **true negatives**
    and **FPs**, which is a stand-in for **false positives**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we proceed any further, some explanations are in order for TPs, TNs,
    FNs, and FPs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True positives point to those predictions that turn out to be truly malignant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True negatives point to those predictions that turn out to be truly benign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False negatives point to those breast cancer samples that were wrongly labeled
    benign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False positives point to those breast cancer samples that were wrongly labeled
    malignant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Receiver Operating Characteristic** (**ROC**) **curve**:The area under this
    curve is a measure of binary classification performance. This curve is a graphical
    plot of FPR on the *x* axis and TPR on the *y* axis. A typical plot is shown later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The area under the ****Precision-Recall (PR) curve**: This is a graphical
    plot of precision on the *y* axis and accuracy on the *x* axis. To plot the curve,
    we need computed (precision value, accuracy value) pairs. To compute these values,
    the following mathematical equations for accuracy and precision are applied as
    follows: Precision = *TPs* / (*TPs* + *FPs*); Accuracy = *TPs* / (*TPs* +*FNs*).
    The ROC curve is shown later, followed by the PR curve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both the ROC and PR curves represent binary classifier performance. How so?
    The area under the ROC curve becomes a measure of the binary classifier''s performance.
    If a curve plotted for a particular algorithm snakes higher up to the top-right
    reading from left to right (the greater area under it), it has a lower **false
    positive rate**, making it a better classifier than a curve for a different algorithm
    that has a higher **false positive rate**. This is useful, though not necessarily
    the best metric. A typical ROC curve is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12467032-121d-4dad-a2ae-233d01346a05.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: The next metric that is worth mentioning is the so-called precision-recall curve
    or PR curve for short. This curve involves the computation of two separate metrics,
    precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: A PR curve is plotted with accuracy on the *x* axis and precision on the *y*
    axis. In this case, a particular algorithm fared better than others in its binary
    classification task if its curve snaked higher up towards the top-right. There
    are more TPs and considerably fewer FNs. This indicates a better classification.
  prefs: []
  type: TYPE_NORMAL
- en: This completes our discussion on binary classifier metrics, and their computation
    marks an important milestone regarding the breast cancer data analysis initiative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, if the logit model we built performed well, the area under the ROC
    discriminant curve should represent a meaningful measure of prediction performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The metrics are ready. Here are the results. Running the pipeline should generate
    the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: That concludes our breast cancer classification task. In the previous section,
    on a Random Forests breast cancer pipeline, we showed you how to deploy your pipeline
    application into Spark. Likewise, in a similar fashion, we can deploy our logistic
    regression pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to implement a binary classification task using
    two approaches such as, an ML pipeline using the Random Forest algorithm and an
    secondly using the logistic regression method.
  prefs: []
  type: TYPE_NORMAL
- en: Both pipelines combined several stages of data analysis into one workflow. In
    both pipelines, we calculated metrics to give us an estimate of how well our classifier
    performed. Early on in our data analysis task, we introduced a data preprocessing
    step to get rid of rows that were missing attribute values that were filled in
    by a placeholder, `?`. With 16 rows of unavailable attribute values eliminated
    and 683 rows with attribute values still available, we constructed a new `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: In each pipeline, we also created training, training, and validation datasets,
    followed by a training phase where we fit the models on training data. As with
    every ML task, the classifier may learn by rotating the training set details,
    a preponderant phenomenon called overfitting. We got around this problem by arriving
    at a reduced but optimal number of attributes. We did this by fitting our classifier
    models with various combinations of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move our development efforts away from a local
    `spark-shell`. This time, we will take advantage of a Zeppelin Notebook running
    inside a **Hortonworks Development Platform** (**HDP**) Sandbox Virtual Machine.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this chapter, we will move on to the last section where we pose
    a set of questions to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now list a set of questions to test your knowledge of what you have
    learned so far:'
  prefs: []
  type: TYPE_NORMAL
- en: What do you understand by logistical regression? Why is it important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does logistical regression differ from linear regression?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name one powerful feature of `BinaryClassifier`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the feature variables in relation to the breast cancer dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The breast cancer dataset problem is a classification task that can be approached
    with other machine learning algorithms as well. Prominent among other techniques
    are **Support Vector Machine** (**SVM**), **k-nearest neighbor**, and **decision
    trees**. When you run the pipelines developed in this chapter, compare the time
    it took to build a model in each case and how many of the input rows of the dataset
    were classified correctly by each algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes this chapter. The next chapter implements a new kind of pipeline,
    which is a stock prediction task pipeline. We shall see how we can use Spark to
    work on larger datasets. Stock price prediction is not an easy problem to solve.
    How we shall tackle this is the subject of the next chapter.
  prefs: []
  type: TYPE_NORMAL
