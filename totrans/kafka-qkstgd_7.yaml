- en: KSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we wrote Java code to manipulate data streams with Kafka,
    and we also we built several Java processors for Kafka and Kafka Streams. In this
    chapter, we will use KSQL to achieve the same results.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: KSQL in a nutshell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running KSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the KSQL CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data with KSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing to a topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KSQL in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With Kafka Connect, we can build clients in several programming languages:
    JVM (Java, Clojure, Scala), C/C++, C#, Python, Go, Erlang, Ruby, Node.js, Perl,
    PHP, Rust, and Swift. In addition to this, if your programming language is not
    listed, you can use the Kafka REST proxy. But the Kafka authors realized that
    all programmers, especially data engineers, can all talk the same language: **Structured
    Query Language** (**SQL**). So, they decided to create an abstraction layer on
    Kafka Streams in which they could manipulate and query streams using SQL.'
  prefs: []
  type: TYPE_NORMAL
- en: KSQL is a SQL engine for Apache Kafka. It allows writing SQL sentences to analyze
    data streams in real time. Remember that a stream is an unbounded data structure,
    so we don't know where it begins, and we are constantly receiving new data. Therefore,
    KSQL queries usually keep generating results until you stop them.
  prefs: []
  type: TYPE_NORMAL
- en: KSQL runs over Kafka Streams. To run queries over a data stream, the queries
    are parsed, analyzed, and then a Kafka Streams topology is built and executed,
    just as we did at the end of each `process()` method when running Kafka Streams
    applications. KSQL has mapped the Kafka Streams concepts one to one, for example,
    tables, joins, streams, windowing functions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: KSQL runs on KSQL servers. So if we need more capacity, we can run one or more
    instances of KSQL servers. Internally, all the KSQL instances work together, sending
    and receiving information through a dedicated and private topic called `_confluent-ksql-default__command_topic`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all Kafka technologies, we can also interact with KSQL through a REST
    API. Also, KSQL, has its own fancy **command-line interface** (**CLI**). If you
    want to read more about KSQL read the online documentation at the following URL:
    [https://docs.confluent.io/current/ksql/docs/index.html](https://docs.confluent.io/current/ksql/docs/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Running KSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, KSQL is shipped with the Confluent Platform. When
    we start the Confluent Platform, automatically at the end it starts a KSQL server,
    as shown in the *Figure 7 .1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90352a54-1b1d-4e12-994e-9b7fae1ccfdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Confluent Platform startup'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the KSQL server alone (not recommendable), we can use the `ksql-server-start`
    command. Just type `./ksql` from the bin directory, as shown in *Figure 7.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3dede300-c03e-4ea1-bb21-427859df15e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: KSQL CLI start screen'
  prefs: []
  type: TYPE_NORMAL
- en: Using the KSQL CLI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The KSQL CLI is a command prompt to interact with KSQL; it is very similar to
    the one that comes with relational databases such as MariaDB or MySQL. To see
    all the possible commands, type help and a list with the options will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, we have not informed KSQL of anything. We must declare that something
    is a table or a stream. We will use the information produced from previous chapters
    with the producers that write JSON information to the `healthchecks` topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember, the data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'KSQL can read JSON data, and can also read data in Avro format. To declare
    a stream from the `healthchecks` topic, we use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To review the structure of an existing `STREAM`, we can use the `DESCRIBE`
    command, which is shown here and that tells us the data types and their structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that at the beginning, two extra fields are shown: `ROWTIME` (the message
    timestamp) and `ROWKEY` (the message key).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we created the stream, we declared that the Kafka topic is `healthchecks`.
    So, if we execute the `SELECT` command, we obtain a list of the events that are
    in the topic to which our stream points in real time (remember to run a producer
    to obtain fresh data). The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `SELECT` command shows the data from the Kafka topic declared in the stream.
    The query never stops, so it will run till you stop it. New records are printed
    as new lines, as new events are produced in the topic. To stop a query, type *Ctrl*
    + *C*.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data with KSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we took the data from the `healthchecks` topic, calculated
    the `uptimes` of the machines, and pushed this data into a topic called `uptimes`.
    Now, we are going to do this with KSQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, KSQL does not yet have a function to compare two dates,
    so we have the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Code a **u****ser-defined function** (**UDF**) for KSQL in Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the existing functions to make our calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As creating a new UDF is out of scope for now, let''s go for the second option:
    use the existing functions to make our calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to parse the startup time using the `STRINGTOTIMESTAMP` function,
    shown as follows (remember that we declared the date in string format, because
    KSQL doesn''t yet have a `DATE` type):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to compare these dates to the current date. In KSQL, at the
    moment, there is no function to get today''s date either, so let''s use the `STRINGTOTIMESTAMP`
    function to parse today''s date, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compare these two dates and calculate the number of days between
    them, shown as follows (1 day = 86,400 seconds = 24 hours x 60 minutes x 60 seconds,
    1 second = 1,000 milliseconds):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Perfect, now we have calculated the uptime for every machine.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to a topic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have processed the data and printed the results in real time. To
    send these results to another topic, we use a `CREATE` command modality, where
    it is specified from a `SELECT`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by writing the uptime as a string and writing the data in a comma-delimited
    format, shown as follows (remember that KSQL supports comma-delimited, JSON, and
    Avro formats). At the moment, it''s enough because we''re only writing one value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Our query is running in the background. To see it is running, we could use
    a console consumer of the `uptimes` topic, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The results are correct; however, we forgot to use the machine serial number
    as the message key. To do this, we have to rebuild our query and our stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to use the `show queries` command, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `Query ID`, use the `terminate <ID>` command, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To delete the stream, use the `DROP STREAM` command, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To write the events key correctly, we must use the `PARTITION BY` clause. First,
    we regenerate our stream with a partial calculation, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This stream has two fields (`serialNumber` and `uptime`). To write these calculated
    values to a topic, we use `CREATE STREAM`, `AS SELECT` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run a console consumer to show the results, demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, close the KSQL CLI (*Ctrl* + *C* and close the command window). As the
    queries are still running in KSQL, you still see the outputs in the console consumer
    window.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you have built a Kafka Streams application with a few KSQL
    commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'To unveil all the power of KSQL, it is important to review the official documentation
    at the following address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.confluent.io/current/ksql/docs/tutorials/index.html](https://docs.confluent.io/current/ksql/docs/tutorials/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KSQL is still very new, but the product has gained adoption among developers.
    We all hope it continues to be extended to support more data formats (Protobuffers,
    Thrift, and so on) and more functions (more UDFs, such as geolocation and IoT,
    that are quite useful).
  prefs: []
  type: TYPE_NORMAL
- en: So again, congratulations! In this chapter, we did the same as in the previous
    ones, but without writing a single line of Java code. This makes KSQL the preferred
    tool for people who are not programmers, but are dedicated to data analysis.
  prefs: []
  type: TYPE_NORMAL
