<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0; Measuring Performance on the JVM"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2.   Measuring Performance on the JVM  </h1></div></div></div><p>In the previous chapter, we introduced and defined important concepts that are related to performance. While extremely valuable, our journey so far has been somewhat academic, and you may grow impatient to exercise your newly acquired knowledge. Luckily, this second chapter does just this! We will take a close look at a real-world scenario and dive head first into the code to profile the application and measure its performance characteristics. This hands-on chapter focuses on one of MV Trading's most successful products: the order book. This is a critical piece of software that was developed to deliver high throughput while maintaining low latency. In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Benchmarking the latency and throughput of an application</li><li class="listitem" style="list-style-type: disc">Profiling a system with Flight Recorder</li><li class="listitem" style="list-style-type: disc">Using JMH to microbenchmark our code</li></ul></div><div class="section" title="A peek into the financial domain"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>A peek into the financial domain</h1></div></div></div><p>This month marks the one year anniversary for <span class="strong"><strong>MV Trading</strong></span> (<span class="strong"><strong>MVT</strong></span>). In the last year, the company delivered great returns to its clients by capitalizing on novel trading strategies. These strategies work most effectively when trades can be placed within milliseconds of receiving new price information. To support trading with low latency, the MVT engineering team directly integrated into a stock market exchange. Exchange integration involved datacenter work to collocate the trading system with the exchange and development effort to build the trading system.</p><p>A key component of the trading system, known as the order book, holds the state of the exchange. The goal of the exchange is to keep track of how many buyers and sellers have an active interest in a stock and at what price each side is willing to trade. As traders, such as MVT, submit orders to buy and sell a stock, the exchange determines when a trade happens and notifies the buyer and the seller about the transaction. The state managed by the exchange and by extension, MVT, is interesting because orders do not always execute when they reach the exchange. Instead, orders can remain in an open or pending state for up to the length of the trading day (on the order of six hours). This first version of the order book allows traders to place orders called limit orders. Limit orders include a constraint on the minimally-acceptable price. For buys, the limit represents the highest price that the trader wishes to pay, and for sells, this indicates the lowest price the trader wishes to receive in exchange for the stock. Another operation that is supported by the order book is the cancelation of an outstanding limit order, which removes its presence from the book. To help summarize the possible order states, the following table itemizes possible outcomes to support exchange actions:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Exchange action</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Outcome</strong></span>
</p>
</td></tr><tr><td>
<p>Limit order with a price worse than best bid or offer submitted.</p>
</td><td>
<p>The order rests on the book, meaning that the order remains in a pending state until an order from the opposing side generates a trade or the submitted order is canceled.</p>
</td></tr><tr><td>
<p>Limit order with a price better than or equal to best bid or offer submitted.</p>
</td><td>
<p>The order crosses the book. Crossing the book is industry jargon that indicates an order caused a trade to happen because its price matched one or more orders from the opposing side of the book. A trade consists of two executions, one per side.</p>
</td></tr><tr><td>
<p>Cancelation submitted for a resting order.</p>
</td><td>
<p>The resting order is removed from the book.</p>
</td></tr><tr><td>
<p>Cancelation submitted for an already executed or non-existent order.</p>
</td><td>
<p>The cancelation request is rejected.</p>
</td></tr></tbody></table></div><p>Let's suppose that as a newly-hired MVT employee, you just joined the engineering team that is in charge of maintaining and improving the order book. Today is your first day, and you are planning to take most of the morning to calmly skim through the code and get familiar with the application.</p><p>After checking out the source code repository, you start with the domain model:</p><pre class="programlisting">case class Price(value: BigDecimal) 
case class OrderId(value: Long) 
 
sealed trait LimitOrder { 
  def id: OrderId 
  def price: Price 
} 
 
case class BuyLimitOrder(id: OrderId, price: Price)  
  extends LimitOrder 
case class SellLimitOrder(id: OrderId, price: Price)  
  extends LimitOrder 
 
case class Execution(orderId: OrderId, price: Price) 
</pre><p>The class and trait definitions in the preceding code represent the business concepts. We especially notice the two kinds of orders (<code class="literal">BuyLimitOrder</code> and <code class="literal">SellLimitOrder</code>) that are identified by their unique ID and the associated price assumed to be in USD.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note9"/>Note</h3><p>You may wonder why we decided to create distinct class definitions for <code class="literal">Price</code> and <code class="literal">OrderId</code> while they only serve as mere wrappers for a unique attribute (respectively a <code class="literal">BigDecimal</code> for the price, and a <code class="literal">Long</code> for the unique ID). Alternatively, we can instead rely directly on the primitive types.</p><p>A <code class="literal">BigDecimal</code> could represent a lot of different things, including a price, but also a tax rate or a latitude on the globe. Using a specific type, named <code class="literal">Price</code>, gives contextual meaning to the <code class="literal">BigDecimal</code> and makes sure that the compiler helps us catch possible errors. We believe that it is good practice to always define explicit types to represent business concerns. This technique is part of the best practices known as domain-driven design, and we often apply these principles throughout the book. To learn more about this approach to software development, we recommend the excellent book <span class="emphasis"><em>Domain-Driven Design: Tackling Complexity in the Heart of Software</em></span> (<a class="ulink" href="http://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215">http://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215</a>) by Eric Evans.</p></div></div><p>The <code class="literal">OrderBook</code> module leverages the domain model to define the available commands and the resulting events that can be produced by the book:</p><pre class="programlisting">object OrderBook { 
  // all the commands that can be handled by the OrderBook module 
  object Commands { 
    sealed trait Command 
    case class AddLimitOrder(o: LimitOrder) extends Command 
    case class CancelOrder(id: OrderId) extends Command 
  } 
 
  // events are the results of processing a command 
  object Events { 
    sealed trait Event 
    case class OrderExecuted(buy: Execution, sell: Execution) 
      extends Event 
    case object LimitOrderAdded extends Event 
    case object OrderCancelRejected extends Event 
    case object OrderCanceled extends Event 
  } 
 
  // the entry point of the module - the current book and  
  // the command to process are passed as parameters,  
  // the new state of the book and the event describing the  
  // result of processing the command are returned 
  def handle(book: OrderBook, command: Command): (OrderBook, Event) = // omitted for brevity 
} 
</pre><p>Let's suppose that you are about to look at the implementation of the <code class="literal">handle</code> function in detail when you notice a message in your IM client from your technical lead, Alice: "Everybody in the conference room. We have a problem in production!"</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Readers with financial domain expertise likely realize that the presented actions reflect a subset of the functionality of an actual financial exchange. One evident example is the absence of quantity from an order. In our examples, we assume each order represents a desire to buy an equal number of shares (for example, 100 shares). Experienced readers are aware that order volume further complicates order book state management, for example, by introducing the notion of partial executions. We are deliberately simplifying the domain to balance working on a realistic problem while minimizing the barrier to comprehension for readers who are new to the domain.</p></div></div></div></div>
<div class="section" title="Unexpected volatility crushes profits"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Unexpected volatility crushes profits</h1></div></div></div><p>Alice and the head trader, Dave, kick off the meeting by summarizing the production problem. You digested a lot of insight into the problem from the meeting. You learned that currently, there is high market volatility and the rapid swings in prices amplify opportunities to generate profitable trades. Unfortunately for MVT, in recent weeks, the high volatility has brought with it unseen levels of order volume. Traders have been flooding the markets with limit orders and cancelations to react to the quickly changing price action. The MVT order book is certified via load testing to handle a maximum of 15,000 orders per second (OPS) with a 99<sup>th</sup> percentile latency of 10 milliseconds (ms). The current market conditions are producing sustained levels of 45,000 OPS, which is destroying tail-end order book latencies. In production, the deployed version of the order book is now producing 99<sup>th</sup> percentile latencies of up to 80 ms and maximum latencies reaching 200 ms. In the trading business, a slow reaction can quickly turn a profitable trade into a sizable loss. This is exactly what has been happening at MVT. Typically, in times of volatility, MVT is able to generate healthy returns, but in recent weeks, there have been staggering losses. Our goal is to apply the techniques that we learned in <a class="link" href="ch01.html" title="Chapter 1.  The Road to Performance">Chapter 1</a>, <span class="emphasis"><em>The Road to Performance</em></span>, to make inroads on the performance woes.</p><p>The traders at MVT are looking for a quick performance win to take advantage of the current market environment. The traders believe that the market volatility will persist for another week before subsiding. Once the volatility disappears, so do money-making opportunities. Therefore, it's been stressed to the engineering team that an incremental reduction in 99<sup>th</sup> percentile latency to 40 ms should halt trading strategy losses and actually produce small profits. Once the volatility subsides, more in-depth and extensive performance improvements are welcomed. For now, the clock is ticking, and we need to find a way to stop the losses by improving performance incrementally.</p></div>
<div class="section" title="Reproducing the problem"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Reproducing the problem</h1></div></div></div><p>This is not quite the first day you were expecting, but what an exciting challenge ahead of you! We start our investigation of the performance issue by reproducing the problem. As we mentioned in the previous chapter, it is always critical to properly benchmark an application to establish a baseline. The baseline is used to evaluate the effectiveness of any improvement that we may try to implement. We create two simple benchmarks to reproduce the load observed in production and measure the throughput and latency of the system.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>But wait, I have not finished studying how <code class="literal">OrderBook</code> is actually implemented! You are correct! You still have no idea of the implementation of the module. However, production is broken, and we need to act fast! More seriously, this is our way of highlighting an important characteristic of benchmarking that we mentioned in <a class="link" href="ch01.html" title="Chapter 1.  The Road to Performance">Chapter 1</a>, <span class="emphasis"><em>The Road to Performance</em></span>. Benchmarks treat the application as a black box. You had time to study the module interface, and this is enough to write good benchmarks.</p></div></div><div class="section" title="Throughput benchmark"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/>Throughput benchmark</h2></div></div></div><p>Our first benchmark measures the throughput of our application. The operations team provided us with historical data that was logged from the production environment. This data contains several hundred thousand actual commands that were processed by the order book. We use this sample and replay these messages against a testing environment to get an accurate idea of the system's behavior.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Recall from <a class="link" href="ch01.html" title="Chapter 1.  The Road to Performance">Chapter 1</a>, <span class="emphasis"><em>The Road to Performance</em></span>, that it is important to run all benchmarks under the exact same environment to be able to compare them. To ensure consistency across tests, we created a command generator that is capable of outputting a static set of commands. We encourage you to review it.</p></div></div><p>Here is the code for our throughput benchmark, which you can find under the chapter subproject:</p><pre class="programlisting">object ThroughputBenchmark { 
 
  def main(args: Array[String]): Unit = { 
    val commandSample = DataCodec.read(new File(args(0))) 
    val commandCount = args(1).toInt 
 
    jvmWarmUp(commandSample) 
 
    val commands = generateCount(commandSample, commandCount) 
 
    val start = System.currentTimeMillis() 
    commands.foldLeft(OrderBook.empty)(OrderBook.handle(_, _)._1) 
    val end = System.currentTimeMillis() 
    val delayInSeconds = (end - start) / 1000.0 
 
    println { 
      s""" 
         |Processed ${commands.size} commands 
         |in $delayInSeconds seconds 
         |Throughput: ${commands.size / delayInSeconds} operations/sec""" 
        .stripMargin 
    } 
  } 
} 
</pre><p>The code is fairly straightforward, so let's walk through it. First, we read our input arguments, the first one being the path to the file that contains our historical data, and the second one is the number of commands that we want to run. Note that in our implementation, if we ask for more commands than what is contained in our static file, the program will just loop through the provided commands. We then warm up the JVM by executing up to 100,000 commands without recording any throughput information. The point of a warm-up is to give the JVM the opportunity to exercise the code and find possible hotspots that can be optimized by the <span class="strong"><strong>just-in-time</strong></span> (<span class="strong"><strong>JIT</strong></span>) compiler.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>The JIT compiler is a compiler that runs after the application has been started. It compiles the bytecode (that is, the result of the first compilation by the <code class="literal">javac</code> compiler) on-the-fly into an optimized form, usually native instructions for the operating system. The JIT is able to optimize the code, based on runtime usage. This is something that the traditional compiler cannot do because it runs before the code can be executed.</p></div></div><p>The next part of the code is where we actually record the throughput. We save the starting timestamp, execute all the commands against an initially empty order book, and record the end timestamp. Our throughput in operations per second is easily calculated by dividing the command count by the elapsed time to execute them all. As our throughput is measured in seconds, millisecond precision is sufficient for our benchmarking needs. Lastly, we print out the interesting results. We can run this benchmark parameterized with 250,000 commands by issuing:</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G")' 'runMain highperfscala.benchmarks.ThroughputBenchmark src/main/resources/historical_data 250000'</strong></span>
</pre><p>Running the benchmark over a range of command counts yields the following results:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Command count</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Processing time (seconds)</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Throughput (operations per second)</strong></span>
</p>
</td></tr><tr><td>
<p>250,000</p>
</td><td>
<p>2.2</p>
</td><td>
<p>112,309</p>
</td></tr><tr><td>
<p>500,000</p>
</td><td>
<p>6.1</p>
</td><td>
<p>81,886</p>
</td></tr><tr><td>
<p>750,000</p>
</td><td>
<p>12.83</p>
</td><td>
<p>58,456</p>
</td></tr><tr><td>
<p>1,000,000</p>
</td><td>
<p>22.56</p>
</td><td>
<p>44,328</p>
</td></tr></tbody></table></div><p>We can see that when the command count increases, our throughput decreases. One explanation could be that the order book grows in size when receiving more orders, and thus becomes less efficient. At this point, we are able to evaluate the throughput of our application. In the next section, we focus on measuring the latency of the program.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>Our benchmark and the ones that we will write later in this chapter are naive. It runs the test and the order book in the same JVM. A more realistic example would involve an order book with a server that maintains TCP connections to clients exchanging FIX messages (FIX being the most widely-used protocol in finance) to place or cancel orders. Our benchmark would impersonate one of these clients to simulate production load on our order book. For the sake of simplicity and to allow us to focus on more interesting subjects, we decided to leave this concern aside.</p></div></div></div><div class="section" title="Latency benchmark"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Latency benchmark</h2></div></div></div><p>Recall from <a class="link" href="ch01.html" title="Chapter 1.  The Road to Performance">Chapter 1</a>, <span class="emphasis"><em>The Road to Performance</em></span>, that the latency is the time that it takes for an operation to happen, where the definition of an operation depends on your domain. In our case, we define an operation as the processing of a command from the time it is received to the time a new order book and a corresponding event are generated.</p><div class="section" title="The first latency benchmark"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec0"/>The first latency benchmark</h3></div></div></div><p>The following listing shows a first version of our latency benchmark:</p><pre class="programlisting"> object FirstLatencyBenchmark { 
 
  def main(args: Array[String]): Unit = { 
 
    val commandSample = DataCodec.read(new File(args(0))) 
    val (commandsPerSecond, iterations) = (args(1).toInt, args(2).toInt) 
    val totalCommandCount = commandsPerSecond * iterations 
 
    jvmWarmUp(commandSample) 
 
    @tailrec 
    def sendCommands( 
      xs: List[(List[Command], Int)], 
      ob: OrderBook, 
      testStart: Long, 
      histogram: HdrHistogramReservoir): (OrderBook, HdrHistogramReservoir) 
      = 
      xs match { 
        case head :: tail =&gt; 
          val (batch, offsetInSeconds) = head 
          val shouldStart = testStart + (1000 * offsetInSeconds) 
 
          while (shouldStart &gt; System.currentTimeMillis()) { 
            // keep the thread busy while waiting for the next batch to be 
            sent 
          } 
 
          val updatedBook = batch.foldLeft(ob) { 
            case (accBook, c) =&gt; 
              val operationStart = System.currentTimeMillis() 
              val newBook = OrderBook.handle(accBook, c)._1 
              val operationEnd = System.currentTimeMillis() 
              // record latency 
              histogram.update(operationEnd - operationStart) 
              newBook 
          } 
 
          sendCommands(tail, updatedBook, testStart, histogram) 
        case Nil =&gt; (ob, histogram) 
      } 
 
    val (_, histogram) = sendCommands( 
      // Organizes commands per 1 second batches 
      generateCount(commandSample, totalCommandCount) 
        .grouped(commandsPerSecond).zipWithIndex 
        .toList, 
      OrderBook.empty, 
      System.currentTimeMillis(), 
      new HdrHistogramReservoir()) 
 
    printSnapshot(histogram.getSnapshot) 
  } 
} 
</pre><p>The beginning of this code is similar to what we had in our throughput benchmark. We use an HdrHistogram to record each operation's latency. The tail-recursive method <code class="literal">sendCommands</code> is where most of the interesting things happen (we take a closer look at tail-recursion in a later chapter). Our commands are grouped by batches of size and <code class="literal">commandsPerSecond</code>, meaning that we will send one batch per second. We record the current time before sending a command (<code class="literal">operationStart</code>) and after receiving a response (<code class="literal">operationEnd</code>). These timestamps are used to update the histogram.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>HdrHistogram is an efficient implementation of a histogram. This is specifically designed to be used in latency and performance-sensitive applications. It maintains a fixed cost both in space and time. It does not involve memory allocation operations, and its memory footprint is constant. To learn more about HdrHistogram, visit <a class="ulink" href="http://hdrhistogram.org/">http://hdrhistogram.org/</a>.</p></div></div><p>At the end, after all batches have been processed, we take a snapshot of the state of the histogram and print interesting metrics. Let's give this a run:</p><pre class="programlisting"><span class="strong"><strong>    sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G")' 'runMain highperfscala.benchmarks.FirstLatencyBenchmark src/main/resources/historical_data 45000 10'&#13;
    ... // removed for brevity&#13;
    [info] Processed 450000 commands&#13;
    [info] 99p latency: 1.0 ms&#13;
    [info] 99.9p latency: 1.0 ms&#13;
    [info] Maximum latency: 24 ms&#13;
</strong></span></pre><p>We exercise our system with a rate of 45,000 operations per second for 10 seconds, and we see a latency of 1 ms for the 99.9<sup>th</sup> percentile. These are outstanding results! They are also completely wrong. In our hurry to write a latency benchmark, we overlooked a too often ignored issue: the coordinated omission problem.</p></div><div class="section" title="The coordinated omission problem"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec1"/>The coordinated omission problem</h3></div></div></div><p>Our benchmark is broken because we measure the time required to process a command without taking into account the time the command had to wait to be processed. This becomes a problem if the previous command took longer than expected to be processed. Take our previous example: we ran 45,000 commands per second, that is, 45 commands per millisecond. What if processing the first 45 commands takes longer than 1 millisecond? The next 45 commands have to wait before being picked up and processed. However, with our current benchmark, we ignore this waiting time. Let's take an example of a web application serving pages over HTTP. A typical benchmark may record request latency by measuring the elapsed time between the moment a request is handled by the web server and the time a response is ready to be sent back. However, this would not account for the time it took for the web server to read the request and actually send back the response. A better benchmark will measure the latency as the time between the moment the client sent the request and the moment it actually received a response. To learn more about the coordinated omission problem, refer to the discussion thread containing direct links to articles and presentations at <a class="ulink" href="https://groups.google.com/forum/#!msg/mechanical-sympathy/icNZJejUHfE/BfDekfBEs_sJ">https://groups.google.com/forum/#!msg/mechanical-sympathy/icNZJejUHfE/BfDekfBEs_sJ</a>.</p><p>To fix this problem, we need to record <code class="literal">operationStart</code> not when we start processing a batch of commands, but when the batch of commands should have been processed, regardless of whether the system is late.</p></div><div class="section" title="The second latency benchmark"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec2"/>The second latency benchmark</h3></div></div></div><p>In our second attempt, we make sure to start the clock to take into account when a command is meant to be sent, as opposed to when it is ready to be processed.</p><p>The benchmark code remains unchanged except for the recording of latency, which now uses <code class="literal">shouldStart</code> instead of <code class="literal">operationStart</code>:</p><pre class="programlisting">histogram.update(operationEnd - shouldStart) 
</pre><p>After this change, this is the new benchmark output:</p><pre class="programlisting"><span class="strong"><strong>    sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G")' 'runMain highperfscala.benchmarks.SecondLatencyBenchmark src/main/resources/historical_data 45000 10'&#13;
    ... // removed for brevity&#13;
    [info] Processed 450000 commands&#13;
    [info] 99p latency: 743.0 ms&#13;
    [info] 99.9p latency: 855.0 ms&#13;
    [info] Maximum latency: 899 ms&#13;
</strong></span></pre><p>The results are very different when compared to our first benchmark. Actually, this new code also has a flaw. This assumes that all the requests sent in the same second are supposed to be processed at the very beginning of this second. While technically possible, it is more likely that these commands will be sent at different times during the second (a few during the first millisecond, some more during the second millisecond, and so on). Our current benchmark probably greatly overestimates our latency by starting the timer too soon for most commands.</p></div><div class="section" title="The final latency benchmark"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec3"/>The final latency benchmark</h3></div></div></div><p>We will attempt to fix the latest issue and finally come up with a reliable benchmark. At this point, we are trying to address the problem of the distribution of the commands over each second. The best way to solve this issue would be to use real production data. If the recorded commands that we are using for our benchmark had a timestamp attached to them (that is, the moment they were received by the production system), we could replicate the distribution of commands observed in production.</p><p>Unfortunately, our current order book application does not record the timestamp when logging data. We could go different routes. One option is to randomly send the commands over a second. Another is to assume an even distribution of the commands (that is, the same amount is sent on each millisecond).</p><p>We choose to modify the benchmark assuming the latter. To accomplish this goal, we modify the generation of events. As our new strategy distributes commands over time rather than batching commands, for a single instant, the new command list return type changes from <code class="literal">List[(List[Command], Int)]</code> to <code class="literal">List[(Command, Int)]</code>. The logic to generate the command list changes to account for our new strategy, as follows:</p><pre class="programlisting">  generateCount(sampleCommands, totalCommandCount) 
    .grouped(cps.value) 
    .toList.zipWithIndex 
    .flatMap { 
      case (secondBatch, sBatchIndex) =&gt; 
        val batchOffsetInMs = sBatchIndex * 1000 
        val commandIntervalInMs = 1000.0 / cps.value 
        secondBatch.zipWithIndex.map { 
          case (command, commandIndex) =&gt; 
            val commandOffsetInMs = 
              Math.floor(commandIntervalInMs * commandIndex).toInt 
            (command, batchOffsetInMs + commandOffsetInMs) 
        } 
    }  
</pre><p>The creation of our set of commands is a bit more involved. We now calculate an offset for each command, taking into account the amount of milliseconds that should elapse between each command. Our final results with this benchmark are as follows:</p><pre class="programlisting"><span class="strong"><strong>    sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G")' 'runMain highperfscala.benchmarks.FinalLatencyBenchmark src/main/resources/historical_data 45000 10'&#13;
    [info] Processed 450000 commands&#13;
    [info] 99p latency: 92.0 ms&#13;
    [info] 99.9p latency: 137.0 ms&#13;
    [info] Maximum latency: 145 ms&#13;
</strong></span></pre><p>We finally established a good latency benchmark for our system, and sure enough, our results come close to what is currently being observed in production.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>Hopefully, this exercise made you reflect on your own production system and what kind of operation you may want to benchmark. The main thing to take away from this section is the importance of properly recording an operation's latency and taking into account the coordinated omission problem. What do you think would be the best way to measure the latency of your system? If you already have benchmarks in place, do they account for the coordinated omission effect?</p></div></div></div></div><div class="section" title="Locating bottlenecks"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Locating bottlenecks</h2></div></div></div><p>Now that we are able to consistently reproduce the bad performance in production with our benchmark, we have confidence that the impact of any of the changes that we make can be accurately measured. The benchmarks treated the order book as a black box, meaning you have no insight into what areas of the order book are causing our performance woes. If you were previously familiar with this code, you could use your intuitions as a heuristic to make educated guesses about the subcomponents that require a deeper focus. As this is day one for you at MVT, you do not have previous intuition to rely on. Instead of guessing, we will profile the order book to gain deeper insights into various facets of our black box.</p><p>The JDK bundles an excellent profiler that is named Flight Recorder. Flight Recorder is free to use in nonproduction environments. Refer to Oracle's license, <a class="ulink" href="http://docs.oracle.com/javacomponents/jmc-5-5/jfr-runtime-guide/about.htm">http://docs.oracle.com/javacomponents/jmc-5-5/jfr-runtime-guide/about.htm</a>, to learn more about commercial usage. The existence of a great profiler is another reason that Scala is a pragmatic choice for production-quality functional programming. Flight Recorder works using internal JVM hooks to record events that are emitted by the JVM at runtime. The events that are captured by Flight Recorder cover memory allocation, thread state changes, IO activity, and CPU activity. To learn more about Flight Recorder internals, refer to Oracle's Flight Recorder documentation: <a class="ulink" href="http://docs.oracle.com/javacomponents/jmc-5-5/jfr-runtime-guide/about.htm#sthref7">http://docs.oracle.com/javacomponents/jmc-5-5/jfr-runtime-guide/about.htm#sthref7</a>. In contrast to third-party profilers, which do not have access to JVM internals, Flight Recorder is able to access data outside of JVM safepoints. A JVM safepoint is a time when all threads are suspended from execution. Safepoints are necessary to coordinate global JVM activities, including stop-the-world garbage collection. To read more about JVM safepoints, check out this excellent blog article by Alexey Ragozin at <a class="ulink" href="http://blog.ragozin.info/2012/10/safepoints-in-hotspot-jvm.html">http://blog.ragozin.info/2012/10/safepoints-in-hotspot-jvm.html</a>. If a profiler is only able to inspect at safepoints, it is likely the profiler is missing useful data points because only a partial picture emerges.</p><p>Let's take our first look inside the order book by setting up a Flight Recorder trial. To expedite cycle time, we set up the profiler via <code class="literal">sbt</code> while we execute a run of <code class="literal">ThroughputBenchmark</code> replaying historical data. We set up Flight Recorder with the following JVM parameters:</p><pre class="programlisting"><span class="strong"><strong>    sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G", "-XX:+UnlockCommercialFeatures", "-XX:+FlightRecorder", "-XX:+UnlockDiagnosticVMOptions", "-XX:+DebugNonSafepoints", "-XX:FlightRecorderOptions=defaultrecording=true,dumponexit=true,dumponexitpath=/tmp/order-book.jfr")'&#13;
</strong></span></pre><p>The max JVM heap size is set to match our benchmark runs, followed by four JVM parameters. The <code class="literal">-XX:+UnlockCommercialFeatures</code> and <code class="literal">-XX:+FlightRecorder</code> parameters are required to emit JVM events for Flight Recorder. The Flight Recorder documentation references <code class="literal">-XX:+UnlockDiagnosticVMOptions</code> and <code class="literal">-XX:+DebugNonSafepoints</code> to improve sampling quality. These options instruct the compiler to generate metadata that enables Flight Recorder to capture samples that are not at safepoints. The final argument configures Flight Recorder to begin recording as soon as the program starts and to dump profiling results to the provided path when the program exits. In our case, this means that the profile will begin when the benchmark starts and terminate when the benchmark concludes. Alternatively, it is possible to configure Flight Recorder to delay its start time and to record for a fixed time by the following configurations:</p><pre class="programlisting"><span class="strong"><strong>    sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G", "-XX:+UnlockCommercialFeatures", "-XX:+FlightRecorder", "-XX:+UnlockDiagnosticVMOptions", "-XX:+DebugNonSafepoints", "-XX:StartFlightRecording=delay=10s,duration=60s,name=Recording,filename=/tmp/order-book.jfr")'&#13;
</strong></span></pre><p>The preceding options configure Flight Recorder to start after five seconds (the <code class="literal">delay</code> option) and record for one minute (the <code class="literal">duration</code> option). The result is stored in <code class="literal">/tmp/order-book.jfr</code>.</p><p>We are now ready to generate profile results. Next, we run the benchmark configured to replay 2,000,000 requests. The more requests played back, the more opportunities there are for the profiler to capture JVM events. All other things equal, prefer longer profiles over shorter ones. The following output shows the benchmark invocation and the resulting output:</p><pre class="programlisting"><span class="strong"><strong>    sbt 'project chapter2' 'set javaOptions := Seq("-Xmx1G", "-XX:+UnlockCommercialFeatures", "-XX:+FlightRecorder", "-XX:+UnlockDiagnosticVMOptions", "-XX:+DebugNonSafepoints", "-XX:FlightRecorderOptions=defaultrecording=true,dumponexit=true,dumponexitpath=/tmp/order-book.jfr")' 'runMain highperfscala.benchmarks.ThroughputBenchmark src/main/resources/historical_data 2000000'&#13;
    [info] &#13;
    [info] Processed 2000000 commands&#13;
    [info] in 93.276 seconds&#13;
    [info] Throughput: 21441.742784853555 commands/sec&#13;
</strong></span></pre><p>To have a look at the profiling results, we use <span class="strong"><strong>Java Mission Control</strong></span> (<span class="strong"><strong>JMC</strong></span>), a free, bundled GUI that supports, among other features, inspecting Flight Recorder results, and running Flight Recorder profile sessions. JMC exists in the same directory as the Java executable, which means that it is accessible on your path by just typing the following:</p><pre class="programlisting">
<span class="strong"><strong>jmc</strong></span>
</pre><p>Once the GUI loads, open the profiler results by navigating to <span class="strong"><strong>File </strong></span>| <span class="strong"><strong>Open</strong></span>. Browse to the profile results and click on <span class="strong"><strong>OK</strong></span> to load them. As we look at the results, we will build a checklist of questions to consider when reviewing profiler results. These probing questions are intended to make you critically analyze the results. These questions ensure that the experiment results truly address the hypothesis that led to the profile. At the end of this chapter, we will present the questions in a single checklist to make it easier to revisit later.</p><div class="section" title="Did I test with the expected set of resources?"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec4"/>Did I test with the expected set of resources?</h3></div></div></div><p>If the test environment was set up to use incorrect resources, the results are invalidated. For this reason, it makes sense to double-check the environment setup first. Fortunately, Flight Recorder captures much of this information for you.</p><p>The <span class="strong"><strong>General </strong></span>and <span class="strong"><strong>System </strong></span>tab groups are the areas to focus on for this checklist item. In <span class="strong"><strong>General</strong></span>, click on <span class="strong"><strong>JVM Information</strong></span> to identify key JVM facts. In this section, confirm the following:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>Areas to focus on</p>
</th><th>
<p>Reason to focus on the area</p>
</th></tr></thead><tbody><tr><td>
<p>JVM start time</p>
</td><td>
<p>This is a quick spot check that confirms that this test executed when you think it did. With a few profile results, ensuring that you are reviewing the correct results is trivial. As you collect more information and investigate additional hypotheses, this simple check ensures that you are not conflating results.</p>
</td></tr><tr><td>
<p>JVM version</p>
</td><td>
<p>Variations in JVM version can yield different results. Ensure that you are using the same JVM version as your production environment.</p>
</td></tr><tr><td>
<p>JVM command-line arguments and JVM flags</p>
</td><td>
<p>It is common to tune the JVM via command-line arguments. Often, the parameterization will change between runs, making it difficult to recall later on which run corresponded to which set of arguments. Reviewing this information provides useful context to review the results.</p>
</td></tr><tr><td>
<p>Java application arguments</p>
</td><td>
<p>Similar to the previous concern, the goal is to ensure that you are confident that you understand the inputs to your test.</p>
</td></tr></tbody></table></div><p>To supplement confirmation of JVM configuration, view the <span class="strong"><strong>GC Configuration</strong></span> tab under the <span class="strong"><strong>Memory </strong></span>tab group. This tab details garbage collection configuration, which reflects a combination of user-supplied configuration and JVM defaults. As you are likely aware, small changes in the garbage collection configuration can yield significant runtime performance changes. Given the sensitivity of application performance to garbage collection configuration, you should reflect on each parameter in this tab. Questions to consider while reviewing are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If I configured this parameter, did the configured value take effect?</li><li class="listitem" style="list-style-type: disc">If I did not configure this parameter, what effect might tuning this value have? This question often helps you to create hypotheses for future profile runs.</li></ul></div><p>Next, we focus on the <span class="strong"><strong>System </strong></span>tab group. The <span class="strong"><strong>Overview </strong></span>tab itemizes non-JVM resources to make it clear which resources were used to create this profile. Continuing with the theme of questions from the <span class="strong"><strong>General </strong></span>tab group, the overarching goals are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To confirm whether recorded information matches the resources that you expected to use (for example, does the amount of available RAM match how much you thought was present?)</li><li class="listitem" style="list-style-type: disc">To look for unexpected differences between the test resources and a production environment (for example, my local machine uses kernel v3.18.x while production is on an older minor version, v3.17.x)</li></ul></div><p>If you are configuring your system via environment variables, there is an <span class="strong"><strong>Environment Variables</strong></span> tab that should be reviewed. Like the <span class="strong"><strong>Overview </strong></span>tab, you are looking to ensure your test resources are provisioned and configured as you intended. It bears repeating that any unexpected differences in your test resources always invalidate test results.</p></div><div class="section" title="Was the system environment clean during the profiling?"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec5"/>Was the system environment clean during the profiling?</h3></div></div></div><p>Once you are comfortable that the appropriate resources were used, the next step is to confirm that only the application being profiled used resources. This is an important diagnostic step prior to reviewing test results because it ensures that the profiled application was truly isolated for the test. Fortunately, Flight Recorder catalogs useful information to answer this question. In the <span class="strong"><strong>System </strong></span>tab group, the <span class="strong"><strong>Processes </strong></span>tab captures all the processes running during the profiling. Scan this list with the following questions in mind:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">When I scan the list of processes, do I see anything that should not be running?</li><li class="listitem" style="list-style-type: disc">When I filter by the command-line column and enter <code class="literal">java</code>, do I see the expected set of JVM applications running?</li><li class="listitem" style="list-style-type: disc">When I scan the list of processes, do I see any duplicate processes?</li></ul></div><p>Next, inspect the <span class="strong"><strong>Recording </strong></span>tab under the <span class="strong"><strong>General </strong></span>tab group. Flight Recorder provides the ability to create concurrent recordings. The profile results will contain the union of the concurrent recordings. If there are multiple recordings unexpectedly happening in only one of several runs, then you may not have an apples-to-apples results comparison between recordings.</p><p>The next area to focus on is system CPU usage over the duration of the profiling. Within the <span class="strong"><strong>General</strong></span> tab group, select the <span class="strong"><strong>Overview </strong></span>tab. This view displays the CPU usage panel, which provides you with the ability to inspect machine CPU usage throughout the recording. Here, you are looking for unexpected divergences between JVM and machine total CPU usage. The following screenshot depicts a scenario where there is a divergence worth investigating:</p><div class="mediaobject"><img src="graphics/image_02_001.jpg" alt="Was the system environment clean during the profiling?"/><div class="caption"><p>Unexpected non-JVM CPU usage highlighted by the CPU Usage panel</p></div></div><p>In the preceding screenshot, the combination of <span class="strong"><strong>JVM + Application (User)</strong></span> and <span class="strong"><strong>JVM + Application (Kernel)</strong></span> indicates the JVM-under-test's CPU usage, and <span class="strong"><strong>Machine Total</strong></span> indicates machine (that is, JVM-under-test and all other processes) CPU usage. For the majority of this recording, the JVM-under-test CPU usage represents the majority of machine CPU usage. If your application should be the only process using system resources, then the small delta between the small delta between <span class="strong"><strong>JVM + Application (User)</strong></span> and <span class="strong"><strong>Machine Total</strong></span> represents the desired state. The divergences near the middle of the recording period indicate that another process or other processes were using CPU resources. These spikes suggest abnormal behavior that can negatively impact profiling results. It is worth considering what other processes are using system resources and whether or not your test results remain valid.</p><p>This is a good opportunity to introduce the range navigator, which is the small horizontal widget at the top of each tab containing red marks. The range navigator is a timeline that displays events from the current tab that happen over time with red marks. By default, the entire timeline is selected and the duration of the profiling is displayed above the center of the timeline. You can select a subset of the timeline to zoom in on an area of interest. For example, you may wish to zoom in on CPU usage when the machine CPU usage spikes up. When selecting a subset of the timeline and data is only available for a specific point in time (for example, at start of recording) or the data does not represent a time series (for example, the JVM version), then the data is hidden or replaced with N/A.</p><p>A final spot check is to check the used machine physical memory in the <span class="strong"><strong>Memory Usage</strong></span> panel in the <span class="strong"><strong>Overview </strong></span>tab under the <span class="strong"><strong>Memory </strong></span>tab group. During this spot, check whether you are trying to assess if the amount of used machine physical memory is a sensible value. If there is little machine physical memory remaining and the reserved heap is a small fraction of the total machine physical memory, you should pause to consider what other processes are using memory. This scenario is illustrated in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_02_002.jpg" alt="Was the system environment clean during the profiling?"/><div class="caption"><p>Non-JVM processes using most of the available memory captured by the Memory Usage panel</p></div></div><p>In the preceding example, the reserved heap space is 2 GB out of an available 16 GB system memory, and 13 GB of system memory is used. This implies that 11 GB of system memory is consumed by processes other than the profiled JVM. Unless you expect to have other processes running in your test environment, this type of memory-usage discrepancy warrants further investigation. For example, if your application makes use of off-heap memory, this discrepancy may invalidate your test results because your application may be unable to allocate memory as needed, or may result in excessive system swapping.</p></div><div class="section" title="Are the JVM's internal resources performing to expectations?"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec6"/>Are the JVM's internal resources performing to expectations?</h3></div></div></div><p>We began our profiling checklist with the widest possible criterion by verifying that the resources are provisioned and configured correctly. We continue to tighten the scope of our checklist by focusing on JVM configuration to ensure that test results are created from valid configurations. Now, we introspect JVM internals to continue verifying that the profile has not been compromised.</p><p>Nearly all production applications involve multithreading to make better use of multiple CPU cores or to separate I/O intensive work from CPU-centric work. The <span class="strong"><strong>Threads </strong></span>tab group helps you familiarize yourself with the division of labor within the application and provides hints for where it may make sense to look deeper. The following table outlines areas of focus within the <span class="strong"><strong>Threads </strong></span>tab group and highlights questions that you need to consider when you are new or unfamiliar with the application that is being profiled and when you have several profile results to compare:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th>
<p><span class="strong"><strong>Focus area</strong></span></p>
</th><th>
<p><span class="strong"><strong>New to the application</strong></span></p>
</th><th>
<p><span class="strong"><strong>Familiar with the application</strong></span></p>
</th></tr></thead><tbody><tr><td>
<p>
<span class="strong"><strong>Thread Count</strong></span> panel in the <span class="strong"><strong>Overview </strong></span>tab</p>
</td><td>
<p>How many total threads exist? Is this different than the count you might expect? For example, if the application is CPU-bound and there are ten times the number of threads than cores, this may be a warning sign that the application is poorly tuned.</p>
</td><td>
<p>Are there qualitative changes in the thread count across profiles? For example, a doubling or halving of the thread count could indicate a configuration error.</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>Hot Threads</strong></span> panel in the <span class="strong"><strong>Hot Threads</strong></span> tab</p>
</td><td>
<p>Is the distribution of sample counts even, or are there a couple of threads that dominate the sample count? The hottest threads are likely indicative of threads to dive deeper into and also areas of the code you should be most familiar with.</p>
</td><td>
<p>Have there been significant changes in thread sample count distribution? If so, do these changes seem sensible to you?</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>Top Blocking Locks</strong></span>, <span class="strong"><strong>Top Blocked Threads</strong></span>, and <span class="strong"><strong>Top Blocking Threads</strong></span> in the <span class="strong"><strong>Contention</strong></span> tab</p>
</td><td>
<p>Familiarize yourself with where locks exist and which threads block most often. This can be useful information to bear in mind when considering what factors are affecting critical path performance.</p>
</td><td>
<p>Compared to previous profile results, is there an increase in either the frequency or distribution of thread blocking? Are there new locks appearing in the results?</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>Latency Stack Traces</strong></span> in the <span class="strong"><strong>Latencies </strong></span>tab</p>
</td><td>
<p>Familiarize yourself with the different maximum latencies per event type to better understand which operations affect the application most. Make mental notes to dive deeper into the more latent sections of the application.</p>
</td><td>
<p>Are maximum latencies qualitatively increasing, decreasing, or similar to previous profile results? When multiple top-level stack traces exist for an event type, consider the ones affecting critical path performance the most.</p>
</td></tr></tbody></table></div><p>After thoroughly inspecting the <span class="strong"><strong>Threads </strong></span>tab group, you should begin to have a mental picture forming about how this application functions and which areas are likely to be most interesting for further study. We now turn to a topic that links closely to JVM threading: <span class="strong"><strong>I/O</strong></span>.</p><p>The <span class="strong"><strong>I/O</strong></span> tab group provides valuable information about the profiled application's file and socket access. Like the review of the <span class="strong"><strong>Threads </strong></span>tab group, this section may provide hints that your application has unexpected or undesirable behavior. Before diving into this tab group, pause to consider when or what causes disk reads and writes, and network reads and writes. As you review the <span class="strong"><strong>Overview </strong></span>tab, do you see divergences between your thinking and the profiler results? If so, you should identify why this discrepancy exists and whether it invalidates your test results.</p><p>An example of unexpected I/O behavior could be excessive writes to standard out. This might happen accidentally when a debugging statement is left behind. Imagine if this side-effect occurs on the critical path of your application. This will negatively impact profile results and invalidates testing. In Flight Recorder, writes to standard out are captured by a blank write path. The following screenshot shows file write results from a simple, one-off application that repeatedly writes to standard out at a high frequency:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_003.jpg" alt="Are the JVM's internal resources performing to expectations?"/><div class="caption"><p>Identifying unexpected writes to standard out via the I/O tab group</p></div></div><p>
</p><p>This example is exaggerated for effect, which is why there is a continuous block of constant writes over time. By inspecting the <span class="strong"><strong>File Write</strong></span> tab, we can also see how much time was spent writing to standard out, how much data was written, and how many writes occurred. In approximately 15 seconds of execution, a whopping 40,000 writes took place! The file write stack trace provides invaluable information, allowing us to backtrack to identify which parts of the application are responsible for the writes. Flight Recorder also allows you to view writes by thread. In a production application with dedicated writer threads, you can quickly isolate undesired I/O access.</p><p>Monitoring I/O reads and writes presents a good opportunity to discuss how to configure Flight Recorder recording parameters. The <code class="literal">-XX:+FlightRecorderOptions</code> accepts a parameter named <code class="literal">settings</code>, which, by default, points to <code class="literal">$JAVA_HOME/jre/lib/jfr/default.jfc</code>. You can either provide your own configuration file or modify the default file. In this configuration file, you can tweak the events that are recorded by Flight Recorder and the frequency at which to capture certain events. For example, by default, the <code class="literal">java/file_write</code> event has a threshold of 20 ms. This is a reasonable default, but you may wish to tune the value lower if you are focused on profiling file writes. Setting the threshold lower means that more event samples are captured. Tune carefully because more is not always better. A lower threshold implies higher overhead and more information to sift through.</p><p>A final area to investigate is the <span class="strong"><strong>Exceptions </strong></span>tab under the <span class="strong"><strong>Code </strong></span>tab group. Even if you are closely monitoring application logs during a profiling, you may not be persisting the logs for historical analysis. Fortunately, Flight Recorder captures exceptions and errors for review. Scanning the exceptions and errors, take note of how many total exceptions and errors occurred and which ones happen most frequently. Shrink the time horizon with the range navigator to better understand if exceptions and errors are concentrated at application startup, later in the profiling, or uniformly occurring. The timing at which exceptions and errors occur often provides insight into whether or not the root cause is misconfiguration or unexpected runtime behavior. As always, if you notice an alarming number of exceptions or errors, consider invalidating the profile results until you have a deeper understanding about why they are happening.</p></div><div class="section" title="Where are the CPU bottlenecks?"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec7"/>Where are the CPU bottlenecks?</h3></div></div></div><p>At this stage, we have completed all checks necessary to maximize the likelihood that the profiler results are valid and worth investigating further. Now, we begin arguably the most fun part of the profiling process: identifying CPU bottlenecks. This is an enjoyable process because the profiler gives you a detailed look inside the black box of your application. It is an opportunity to objectively test your hypotheses and mental model of how the application works by comparing with the profiler results. Once you identify the bottlenecks, you will feel a sense of relief that you now know where to pinpoint your next set of changes to improve application performance.</p><p>Let's start on the <span class="strong"><strong>Overview</strong></span> tab of the <span class="strong"><strong>Code</strong></span> tab group. This view is useful to sensitize yourself from the bottom-up on which areas of the code are most expensive. The following figure displays the <span class="strong"><strong>Overview</strong></span> tab for a sample run of the order book:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_004.jpg" alt="Where are the CPU bottlenecks?"/><div class="caption"><p>The Code Overview tab summarizing expensive packages and classes</p></div></div><p>
</p><p>In this view, the initial goal is to get a sense for the distribution of CPU time in the application. The <span class="strong"><strong>Hot Packages</strong></span> panel quickly makes it clear to us that the order book is heavily reliant upon code from the <code class="literal">scala.collection </code>package. The <span class="strong"><strong>Hot Classes</strong></span> panel shows that the order book is spending a significant amount of time, approximately 55% of the time, performing some type of iteration operations. In this screenshot, we also see that only a subset of the profile duration is selected with the range navigator. It is often helpful to view different subsets of the profile period to determine if hot spots remain constant over time. In this example, the early part of the profile results are excluded because they include time spent preparing requests to be sent to the order book. Selecting this subset allows us to focus purely on order book operations without pollution from test setup.</p><p>It is important to note that the percentage column indicates the amount of application time spent executing in the displayed package and class. This means that this view, along with the <span class="strong"><strong>Hot Methods</strong></span> tab, are bottom-up views rather than top-down views. In a top-down view, the percentage column indicates the sum total amount of time spent in part of a stack trace. This view is captured in the <span class="strong"><strong>Call Tree</strong></span> tab. This distinction is key because these two views help us answer different questions. The following table explores several topics from both perspectives to better understand when each view is most helpful:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Topic</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>Top-down view (Call Tree)</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>Bottom-up view (Overview/Hot Methods)</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>Point of view</p>
</td><td>
<p>This is from a macro picture to a micro picture</p>
</td><td>
<p>This is from a micro picture to a macro picture</p>
</td></tr><tr><td>
<p>Determining hot spots</p>
</td><td>
<p>These are areas of the code base that delegate to the most expensive function calls</p>
</td><td>
<p>These are the most expensive functions</p>
</td></tr><tr><td>
<p>Example questions best answered by each view</p>
</td><td>
<p>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Is the order book cancel operation more expensive than adding a resting limit order?</li><li class="listitem" style="list-style-type: disc">Where is CPU time spent on the critical path?</li></ul></div><p>
</p>
</td><td>
<p>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Did switching from a <code class="literal">Double</code> price representation to a <code class="literal">BigDecimal</code> representation create any hot spots?</li><li class="listitem" style="list-style-type: disc">Relative to the rest of the application, which tree operations are most costly?</li></ul></div><p>
</p>
</td></tr></tbody></table></div><p>As a first-time profiler of the order book, you now know from the <span class="strong"><strong>Overview </strong></span>tab that the order book makes heavy usage of Scala collections, but you do not yet have a feel for which order book operations are causing the performance to suffer. To deepen your understanding about the cost of different order book operations, you take a top-down view by investigating the <span class="strong"><strong>Call Tree</strong></span> tab:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_005.jpg" alt="Where are the CPU bottlenecks?"/><div class="caption"><p>Investigating order book bottlenecks using Call Tree</p></div></div><p>
</p><p>Drilling down <span class="strong"><strong>Call Tree</strong></span>, it becomes abundantly clear that the cancelation operation is the bottleneck. Overwhelmingly, CPU time is spent canceling orders rather than adding them. You call over the company's sharpest trader, Dave, to share this finding. Dave's eyes light up when you mention that cancelations are costly. As a trading domain expert, he is well aware that in volatile times, the frequency of cancelations increases significantly as compared to calmer market periods. Dave explains that traders frequently cancel orders in volatile markets to quickly adjust to swinging prices. Cancelations beget cancelations because traders are learning valuable pricing information. Ending the conversation, he tells you to cancel everything else that you are doing (no pun intended!) and focus on improving the performance of order canceling.</p><p>You walk away from the conversation feeling better knowing that you have identified the bottleneck that is likely to be the source of MVT trade losses. To get a better feel, you dig a bit deeper into the <span class="strong"><strong>Call Tree</strong></span> to reveal which functions within <span class="strong"><strong>Cancel Order</strong></span> are expensive. This is the process of moving from a macro view to a micro view. You end up looking at the <span class="strong"><strong>Call Tree</strong></span> tab, which is displayed as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_006.jpg" alt="Where are the CPU bottlenecks?"/><div class="caption"><p>Understanding which cancel order functions are the slowest via Call Tree</p></div></div><p>
</p><p>The <span class="strong"><strong>Call Tree</strong></span> shows that canceling an order involves an invocation of the <code class="literal">RedBlackTree find()</code> function and the <code class="literal">exists()</code> function. As you look deeper into the call, you also notice that the percentage column becomes smaller. This is because in a top-down view, the percentage column represents the sum total CPU time spent on a particular function and all the functions beneath it. According to the results, 84.48% of CPU time was spent <code class="literal">executingOrderBook$$anonfun$handleCancelOrder$1.apply()</code> and the functions deeper in the <span class="strong"><strong>Call Tree</strong></span>. From this view, we also see that of the 84.48% of CPU time, 53.24% of the time is spent <code class="literal">withinAbstractIterator.exists()</code> and deeper function calls. This looks like the biggest bottleneck, with the invocation of <code class="literal">Queue.iterator()</code> in second place, taking 31.24% of CPU time.</p><p>Reflecting on this information, you are curious to start at the bottom, so-to-speak, with the most expensive functions, and work your way through the backtrace to identify affected order book operations. To address your curiosity, you investigate the <span class="strong"><strong>Hot Methods </strong></span>tab and see the following view:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_007.jpg" alt="Where are the CPU bottlenecks?"/><div class="caption"><p>Going from the micro to the macro view with Hot Methods</p></div></div><p>
</p><p>By an order of magnitude, you discover the same two culprits from the <span class="strong"><strong>Call Tree</strong></span> investigation are the hottest methods. This is a reassuring finding because it builds confidence that changes to the implementation of canceling an order are likely to yield qualitative benefits. As you have not spent any time studying the source code, there is still a significant amount of mystery about the implementation. Taking a step back to consider the situation, you think about the operations being modeled in the abstract. Canceling an order involves finding the existing order that could have any price and then once found, modifying the state of the order book to remove the order. Your intuition suggests that some of these operations are likely linear, or possibly logarithmic at best. As you begin considering what other implementation could be faster, Dave interrupts your thoughts.</p><p>In a rushed voice, you hear, "Have you fixed the order book? We need to get it deployed now!" Of course, you have no idea how to respond, and the thought of deploying code on day one makes you a bit uneasy. You share your findings with Dave, hoping that your findings will satisfy his appetite for progress and buy you more time to think. Unfortunately, Dave is not thrilled to hear the order book performance mystery remains unsolved, "We're losing money everyday because of this!" You acknowledge that you understand the gravity of the situation and that you are moving as fast as you can. It is your first day, after all! Dave sighs and acknowledges he is being a bit tough, and that his exasperation is causing him to overreact. As the conversation is winding down, Dave mentions his appreciation for how quickly you came up to speed, and he makes some small talk about how he cannot understand how his brand new smartphone, loaded with extra memory, still runs slowly. "Nothing seems to be working quickly anymore!" he exclaims. His mention of memory causes you to have an epiphany.</p><p>You remember that you have not yet reviewed memory usage results. You are hoping that there are some easy wins available by tuning the garbage collector to improve performance without making code changes. Before making any changes, you check out the <span class="strong"><strong>Memory </strong></span>tab group for insight into memory allocation patterns.</p></div><div class="section" title="What are the memory allocation patterns?"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec8"/>What are the memory allocation patterns?</h3></div></div></div><p>The <span class="strong"><strong>Memory </strong></span>tab group is the final area that remains to dive into for our analysis. Even though we have not spent time looking at the order book source code, the <span class="strong"><strong>Code </strong></span>tab group illustrated the relative costs of the different order of operations. Studying the <span class="strong"><strong>Hot Methods</strong></span> provides insight into the types of objects that are used by various areas of the order book. Looking into the memory allocation patterns, we want to identify young and old generation garbage collection trends and which objects are most and least allocated.</p><p>The default Flight Recorder configuration settings do not track object allocations. For a more complete view of memory consumption, the following configuration settings should be enabled:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Allocation-profiling-enabled</li><li class="listitem" style="list-style-type: disc">Heap-statistics-enabled</li><li class="listitem" style="list-style-type: disc">gc-enabled-all</li><li class="listitem" style="list-style-type: disc">Allocation-profiling-enabled for both <code class="literal">java/object_alloc_in_new_TLAB</code> and <code class="literal">java/object_alloc_outside_TLAB</code> events</li></ul></div><p>Once a profile is generated with all the preceding parameters enabled, you will get a first glimpse into application memory allocation patterns in the <span class="strong"><strong>Heap</strong></span> panel on the <span class="strong"><strong>Garbage Collections</strong></span> tab:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_008.jpg" alt="What are the memory allocation patterns?"/><div class="caption"><p>Visualizing memory allocation patterns via the Garbage Collections tab</p></div></div><p>
</p><p>This view shows a shape that is commonly referred to as a sawtooth pattern. There are frequent garbage collections creating a tooth-like pattern in the data as the JVM is constantly freeing young generation memory. Garbage collection tuning is a vast topic that is beyond the scope of this book. We encourage you to dig deeper into this area by reading through this well-written blog post entitled, "Understanding Java Garbage Collection" (<a class="ulink" href="http://www.cubrid.org/blog/dev-platform/understanding-java-garbage-collection/">http://www.cubrid.org/blog/dev-platform/understanding-java-garbage-collection/</a>).</p><p>As shown in the following screenshot, Flight Recorder also provides summary metrics per garbage collection category in the <span class="strong"><strong>GC Times</strong></span> tab:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_009.jpg" alt="What are the memory allocation patterns?"/><div class="caption"><p>Summarizing garbage per collection event type</p></div></div><p>
</p><p>The following are some questions worth considering when inspecting a visualization of heap usage and a breakdown of garbage collection per collection type:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Question</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>Thoughts to consider</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>On average, does memory usage remain constant, slope downwards, or slope upwards?</p>
</td><td>
<p>An upward slope in memory can point to a memory leak. In this scenario, the heap will grow until the old generation fills, causing an old generation collection. If there is a memory leak, old generation collections will not clear much memory and eventually cause an out of memory error. The order book's memory usage appears constant for the profiled period. When making this type of judgement, obtain the longest possible profile to ensure you are viewing as complete of a picture as possible.</p>
</td></tr><tr><td>
<p>Do outlier pause times correlate with other major events?</p>
</td><td>
<p>According to the garbage collection breakdown, the maximum collection time is an order of magnitude that is larger than the average collection time. Scan the <span class="strong"><strong>Heap</strong></span> panel for collection pauses that are qualitatively larger than the average. Do you see a pattern among the outliers? Consider your application's interaction with external systems and the machine it is deployed onto. Could there be an explanation for the occurrence of extreme pauses? It may also be worthwhile to compare outliers across profiles to determine whether the pattern is specific to a single profile or appears to be systemic.</p>
</td></tr><tr><td>
<p>What is the frequency of collections and how long is a typical collection lasting?</p>
</td><td>
<p>All other things being equal, a lower collection count is preferable because it suggests garbage is generated at a slower rate. That said, a lower collection count can be the result of an increased heap size, which may cause an increase in the average collection time. The takeaway is that inspecting collection count and latencies should be taken with a grain of salt. For this reason, the total garbage collection time metric is insightful. The total collection time reflects the effects of collection frequency and duration. Additionally, this does not suffer from loss like the average collection duration.</p>
</td></tr><tr><td>
<p>What is the lifespan of an object for important use cases?</p>
</td><td>
<p>While studying these breakdowns of garbage collection performance, it is important to build an intuition for how memory is allocated for different use cases in your application. Understanding this relationship may help you figure out why certain allocation patterns occur. In volatile markets, we expect that the order book has a lot of short-lived objects because traders are frequently canceling orders. In less-volatile markets, we likely expect that the average age of an order resting on the book is higher, which implies more long-lived objects.</p>
</td></tr></tbody></table></div><p>Studying these views of memory allocation provides a summary of memory allocation activity. Investigating the <span class="strong"><strong>Allocations </strong></span>tab provides several different ways to see which parts of the application are applying memory pressure. Flight Recorder provides three allocation views: by class, by thread, and by profile:
</p><div class="mediaobject"><img src="graphics/image_02_010.jpg" alt="What are the memory allocation patterns?"/><div class="caption"><p>Correlating high-pressure List allocations via Allocations by Class</p></div></div><p>
</p><p>Class and profile allocations are shown in the preceding screenshot. Note that Allocations by Thread are skipped in this case because the order book is single-threaded.</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_011.jpg" alt="What are the memory allocation patterns?"/><div class="caption"><p>Confirming memory allocation pressure using top-down Allocation Profile view</p></div></div><p>
</p><p>When you are reviewing these allocation views, you should consider the following questions. As you read through these questions, reflect on how you would answer them to better understand how to improve order book performance:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Question</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>Thoughts to consider</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>When inspecting <span class="strong"><strong>Allocation by Class</strong></span>, is there a positive correlation between classes with heavy collection pressure and classes that are referenced on the critical path?</p>
</td><td>
<p>If you determine that the classes creating the most garbage collection pressure are also often allocated on the critical path, then you have good reason to believe that if you optimize the critical path, there will be both algorithm speed and garbage collection benefits. The order book results indicate that List allocations is the worst offender. The backtrace shows that the allocations are almost entirely coming from handling <span class="strong"><strong>Cancel Orders</strong></span>, which we know to be the bottleneck.</p>
</td></tr><tr><td>
<p>When inspecting <span class="strong"><strong>Allocation by Thread</strong></span>, what does the distribution of garbage collection pressure look like?</p>
</td><td>
<p>Noting which threads are responsible for generating the most garbage collection pressure can direct you towards areas of the code to focus intently on. Ameliorating garbage collection pressure by the worst offenders will be reflected in the total pause time.</p>
</td></tr><tr><td>
<p>When drilling down the <span class="strong"><strong>Allocation Profile</strong></span> stacktrace, do known CPU time bottlenecks correlate with high garbage collection pressure?</p>
</td><td>
<p>The order book shows that approximately 99% of garbage is generated when handling <span class="strong"><strong>Cancel Orders</strong></span>. This is affirmation that handling <span class="strong"><strong>Cancel Orders</strong></span> is computationally expensive and is further slowing down the system due to high object allocation rates. Establishing this correlation provides strong evidence that code changes to this section of the code will yield qualitative performance improvements.</p>
</td></tr></tbody></table></div></div><div class="section" title="Trying to save the day"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec9"/>Trying to save the day</h3></div></div></div><p>Knowing that Dave will soon ask you again about improvements to order book performance, you take a few minutes to reflect on your findings. It is clear that handling <span class="strong"><strong>Cancel Orders</strong></span> is both the CPU time and the memory allocation bottleneck. With more time to think about the problem, you are confident you can change the order book implementation to address either concern or possibly both. Unfortunately, time is one thing you do not currently have. One interesting observation from the memory allocations is that most garbage tends to be short-lived in a volatile market. Two inexpensive options to test come to mind:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>JVM memory tuning options</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>Hypothesis</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>Switch from the default old generation collection, parallel old, to <span class="strong"><strong>Concurrent Mark Sweep</strong></span> (<span class="strong"><strong>CMS</strong></span>).</p>
</td><td>
<p>The CMS collector is designed to keep your application responsive. Switching to the CMS collector may not improve order book throughput, but it may provide more consistent response latency during highly-volatile market movements.</p>
</td></tr><tr><td>
<p>Increase the new size from the default, approximately one-third of maximum heap size, to three-fourths of maximum heap size.</p>
</td><td>
<p>The order book has 1 GB of heap to store state, and it is currently only using approximately 380 MB to store young generation objects. You want to leverage the intuition that frequent cancels lead to frequent short-lived objects. Increasing the new generation size is a bet that there will be less than 250 MB of tenured objects and that an increased young generation heap improves order book throughput due to more infrequent collections.</p>
</td></tr></tbody></table></div><p>The following table summarizes the results for each experiment:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Setup</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Command</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>99<sup>th</sup> percentile in ms</strong></span>
</p>
</td></tr><tr><td>
<p>Original</p>
</td><td>
<p>
</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'set   javaOptions := Seq("-Xmx1G")' 'runMain   highperfscala.benchmarks.FinalLatencyBenchmark src/main/resources/historical_data   45000 10'</strong></span>
</pre><p>
</p>
</td><td>
<p>92</p>
</td></tr><tr><td>
<p>CMS collector</p>
</td><td>
<p>
</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'set   javaOptions := Seq("-Xmx1G",   "-XX:+UseConcMarkSweepGC")' 'runMainhighperfscala.benchmarks.FinalLatencyBenchmark   src/main/resources/historical_data 45000 10'</strong></span>
</pre><p>
</p>
</td><td>
<p>118</p>
</td></tr><tr><td>
<p>750M new size</p>
</td><td>
<p>
</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'set   javaOptions := Seq("-Xmx1G", "-XX:NewSize=750M")' 'runMain   highperfscala.benchmarks.FinalLatencyBenchmark   src/main/resources/historical_data 45000 10'</strong></span>
</pre><p>
</p>
</td><td>
<p>74</p>
</td></tr><tr><td>
<p>CMS collector and 750M new size</p>
</td><td>
<p>
</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'set   javaOptions := Seq("-Xmx1G", "-XX:NewSize=750M", "-XX:+UseConcMarkSweepGC")'   'runMain highperfscala.benchmarks.FinalLatencyBenchmark   src/main/resources/historical_data 45000 10'</strong></span>
</pre><p>
</p>
</td><td>
<p>148</p>
</td></tr></tbody></table></div><p>It looks like it will be more complicated than expected to improve the performance of the order book. At least one of the options, increasing the new size, seems to yield a better overall latency. We suggest that you take the time to go over this chapter again and repeat the process of benchmarking and profiling the application with these new sets of options. Observe what new behaviors these JVM options introduce, and try to understand the resulting increase or decrease in latency.</p></div><div class="section" title="A word of caution"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec10"/>A word of caution</h3></div></div></div><p>We want to take a moment to highlight some context about the profiling results that we interpreted in the previous section. We worked through an example that exhibited multiple real-world concerns. We laid out a pragmatic approach to working through the performance problem that has been applied by your authors at their day jobs. The point to be cautious of is that the order book is likely much simpler than most applications you work on day-to-day. We deliberately chose an example that was complicated enough to illustrate how to work through a performance problem, but also simple enough to understand without hours of code review. In practice, you will possibly need to repeat profiling numerous times, each time testing out a new hypothesis, in order to gain traction with your performance problem. Applying the structured approach that we walked through will ensure that you validate your results before analyzing them, and it will also ensure that you have well-founded evidence to pinpoint where to make changes.</p></div><div class="section" title="A profiling checklist"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec11"/>A profiling checklist</h3></div></div></div><p>We worked through each item on the profiling checklist throughout the chapter. We present the entire checklist for ease of reference, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Did I test with the expected set of resources?</li><li class="listitem">Was the system environment clean during the profiling?</li><li class="listitem">Are resources internal to the JVM performing as I would expect?</li><li class="listitem">Where are the CPU bottlenecks?</li><li class="listitem">What are the memory allocation patterns?</li></ol></div></div></div><div class="section" title="Taking big steps with microbenchmarks"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Taking big steps with microbenchmarks</h2></div></div></div><p>In the coming chapters, we will share techniques from the functional paradigm and from the Scala language that enable you to write more performant software. However, you should not accept our prescriptions at face value. Measuring the performance is the objective way to determine whether the changes improve performance. A microbenchmark is a term used to describe a benchmark that exercises a small, isolated portion of a larger application. As microbenchmarks, by design, test a small piece of code, it is often easier to run a microbenchmark than to benchmark an entire application when a nuanced change is made.</p><p>Unfortunately, accurately observing the performance of nuanced changes is difficult, particularly on the JVM. Consider these order book-related examples of changes that warrant microbenchmarking. </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Replacing the data structure holding resting limit orders with one that handles cancels more efficiently</li><li class="listitem" style="list-style-type: disc">Normalizing stock prices from a double representation to an integer representation to perform order matching with a lower overhead</li><li class="listitem" style="list-style-type: disc">Determining the performance boost of reordering a set of branch statements to reflect the order you perceive to be accessed most frequently</li></ul></div><p>How would you measure the performance before and after each change? You may try writing small benchmark programs, which are similar to the <code class="literal">ThroughputBenchmark</code>. This approach is likely to provide you with untrustworthy results due to the JVM's cleverness. The JVM applies a number of heuristics to make runtime optimizations. In a production environment, these changes are welcome because improved performance is always welcomed. However, in a microbenchmark, these changes are not welcomed because they decrease confidence that the microbenchmark is isolating only the nuanced change. Examples of changes the JVM is capable of making include the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Dead-code elimination</li><li class="listitem" style="list-style-type: disc">Just-in-time optimization (refer to our earlier sidebar regarding the JIT compiler)</li><li class="listitem" style="list-style-type: disc">Constant folding (an optimization to avoid the evaluation on each call of a function with constant arguments and a return value dependent on these parameters)</li></ul></div><p>We encourage you to read more about JVM optimizations by reading Oracle's <span class="emphasis"><em>The Java HotSpot Performance Engine Architecture</em></span> (<a class="ulink" href="http://www.oracle.com/technetwork/java/whitepaper-135217.html">http://www.oracle.com/technetwork/java/whitepaper-135217.html</a>). Given that this is a challenge to isolate small code changes, how can we write a proper microbenchmark? Fortunately, the OpenJDK team recognized these same challenges and introduced a library for this purpose named JMH, the Java microbenchmarking harness (<a class="ulink" href="http://openjdk.java.net/projects/code-tools/jmh/">http://openjdk.java.net/projects/code-tools/jmh/</a>). JMH is designed for the express purpose of overcoming the limitations that we referenced in order to isolate the performance impact of your changes. The process to work with JMH is similar to other testing libraries. Similar to JUnit, JMH defines a set of annotations to control test setup and execution. Although tests can be run in several ways, we focus on executing tests via the sbt plugin, <code class="literal">sbt-jmh</code> (<a class="ulink" href="https://github.com/ktoso/sbt-jmh">https://github.com/ktoso/sbt-jmh</a>), for ease of use. Let's walk through the process of creating, running, and analyzing a microbenchmark with the order book. In future chapters, we will leverage our JMH knowledge to objectively measure the performance impact of prescribed changes.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>Are there changes that you made recently to your application that could benefit from microbenchmarking? If you did not microbenchmark the change, do you think microbenchmarking could have led you towards alternative solutions?</p></div></div><div class="section" title="Microbenchmarking the order book"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec12"/>Microbenchmarking the order book</h3></div></div></div><p>Having made progress towards improving performance by tweaking JVM memory settings, you set your eyes towards better understanding cancel performance. Based on the existence of a <code class="literal">scala.collection.immutable.Queue</code> in the profiler results, you hypothesize that there may be a linear time traversal of a FIFO queue to support order cancels. One way to test this hypothesis is to devise a microbenchmark that measures the cancelation performance in different scenarios. You brainstormed the following scenarios:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Canceling a nonexistent order</li><li class="listitem" style="list-style-type: disc">Canceling the first order in line for a price level</li><li class="listitem" style="list-style-type: disc">Canceling the last order in line for a price level</li></ul></div><p>Canceling a nonexistent order happens in the real world when a resting order is crossed before the cancel request arrives. This is an interesting scenario because you are unsure whether there is early termination logic to make this operation cheaper or whether canceling a nonexistent order requires inspecting the entire order book. The remaining two scenarios focus on the fill guarantees that are provided by stock exchanges. When multiple orders are placed at the same price, they are guaranteed to be filled on a first-come, first-served basis. You are speculating that the FIFO queue seen in the profile results is preserving the time ordering of resting orders for a price level. You expect canceling the first order in line to be faster by a linear factor than canceling the final order in line.</p><p>After reading through the excellent JMH examples at <a class="ulink" href="http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/">http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/</a> and some deep thinking, you are able to put together the following tests that capture the scenarios that you are interested in. The code is in-full and is followed by a walkthrough, displayed as follows:</p><pre class="programlisting">@BenchmarkMode(Array(Throughput)) 
@OutputTimeUnit(TimeUnit.SECONDS) 
class CancelBenchmarks { 
 
  @Benchmark 
  def cancelLastOrderInLine(b: BookWithLargeQueue): (OrderBook, Event) = OrderBook.handle(b.book, b.cancelLast) 
 
  @Benchmark 
  def cancelFirstOrderInLine(b: BookWithLargeQueue): (OrderBook, Event) = OrderBook.handle(b.book, b.cancelFirst) 
 
  @Benchmark 
  def cancelNonexistentOrder(b: BookWithLargeQueue): (OrderBook, Event) = OrderBook.handle(b.book, b.cancelNonexistent) 
} 
 
object CancelBenchmarks { 
 
  @State(Scope.Benchmark) 
  class BookWithLargeQueue { 
    private val p = Price(BigDecimal(1.00)) 
    private val firstId: Int = 1 
    private val defaultCancelLast = CancelOrder(OrderId(-1)) 
 
    @Param(Array("1", "100", "1000")) 
    var enqueuedOrderCount: Int = 0 
 
    var book: OrderBook = OrderBook.empty 
 
    @Setup(Level.Trial) 
    def setup(): Unit = { 
      if (enqueuedOrderCount &lt; 0) 
        sys.error(s"Invalid enqueued order count = $enqueuedOrderCount") 
      assert(book == OrderBook.empty) 
      assert(cancelLast == defaultCancelLast) 
 
      cancelLast = CancelOrder(OrderId(enqueuedOrderCount)) 
      book = { 
        (firstId to enqueuedOrderCount).foldLeft(OrderBook.empty) { 
           case (ob, i) =&gt; 
             OrderBook.handle(ob, AddLimitOrder(BuyLimitOrder(OrderId(i), 
             p)))._1 
         } 
      } 
 
      assert(cancelLast != defaultCancelLast) 
      if (enqueuedOrderCount &gt; 0) 
        assert(book.bids.head._2.size == enqueuedOrderCount, 
          s"Book built incorrectly! Expected book to contain " + 
            s"$enqueuedOrderCount bids for $p, but actual book is $book") 
   } 
 
    var cancelLast: CancelOrder = defaultCancelLast 
    val cancelFirst: CancelOrder = CancelOrder(OrderId(firstId)) 
    val cancelNonexistent: CancelOrder = CancelOrder(OrderId(-1)) 
  } 
} 
</pre><p>Rather than duplicating JMH documentation, we will focus on the specific segments of interest and expect you to also investigate the JMH samples for additional context. Surveying the <code class="literal">CancelBenchmarks</code> class, you see the use of annotations to define benchmarks and control the benchmark outputs. Several benchmark modes exist. We are using the throughput mode to measure the number of times the benchmark completes in a fixed period of time. The implementation of each cancellation benchmark differs only by the ID of the order being canceled. Let's switch focus to the <code class="literal">CancelBenchmarks</code> object, which provides the necessary scaffolding to set up each benchmark.</p><p>The <code class="literal">CancelBenchmarks</code> object defines the <code class="literal">BookWithLargeQueue</code> state, which we observed is an argument to each benchmark. Defining the state that is required by the test is the first step towards parameterizing the benchmark. For this set of tests, we simplify the test setup by creating an order book with only a single price level at $1.00. We focus on sweeping the number of orders enqueued for the $1.00 price level in order to help identify the runtime behavior that we believe to be operating in linear time. The use of the <code class="literal">param</code> annotation supplies a set of default values to sweep for enqueued order count. We use the <code class="literal">setup</code> annotation to instruct JMH to prepare the state of the order book prior to invoking each of the three benchmarks. For each enqueued order count value, JMH invokes the <code class="literal">setup</code> method to create an order book with the desired number of resting orders at the $1.00 level.</p><p>Next, we run the benchmarks from sbt. JMH provides a number of command-line flags that control test configuration, which can be viewed from <code class="literal">sbt</code> using the following command:</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'jmh:run -help'</strong></span>
</pre><p>All parameters that are configured as annotations can be overridden by supplying the associated command-line flag. The following is a sample invocation of <code class="literal">CancelBenchmarks</code>:</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter2' 'jmh:run CancelBenchmarks -wi 3 -w 5s -i 30 -r 10s -jvmArgs "-Xmx1G -Xms1G" -gc true -foe true -p enqueuedOrderCount=1,10,50,100'</strong></span>
</pre><p>In this invocation of JMH, we configure three warm-up iterations, each running for 5 seconds. Warm-up iterations do not count toward the output throughput result. We configure 30 recorded iterations, each lasting 10 seconds to compute throughput. We supply a 1 GB heap size for this test and switch on exiting the benchmark on uncaught exceptions to defend against a regression in the code. Lastly, we parameterize the enqueued order counts that we wish to sweep, indicating that we want to run three warm-up iterations and 30 recorded iterations for an enqueued order count of 1, 10, 50, and 100 orders.</p><p>With a single order in the book, we hypothesize that all operations should be approximately equally expensive. As we believe that cancel operations run in linear time, our expectation is that each benchmark should be approximately five times slower when the enqueued order count is 50 than when the count is 10. We cap testing at 100 enqueued orders because in discussion with Dave, we learned that in his experience, he has never analyzed a book with more than 85 orders in a level. Capping at 100 orders ensures that we understand performance characteristics at a level that we do not expect to see in production but could conceivably occur.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>Imagine that you are writing a microbenchmark for the most performance-sensitive use case in your system. What variables would be important to sweep to have a complete understanding of how the system performs in this use case? How would you go about identifying a base case, step values, and a maximum value to parameterize your tests? Consider speaking with domain experts or using production data to guide decision making.</p></div></div><p>After executing the test, we see the following results summarized, as follows:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/><col/></colgroup><thead><tr><th>
<p><span class="strong"><strong>Benchmark</strong></span></p>
</th><th>
<p><span class="strong"><strong>Enqueued order count</strong></span></p>
</th><th>
<p><span class="strong"><strong>Throughput (ops per second)</strong></span></p>
</th><th>
<p><span class="strong"><strong>Error (ops per second)</strong></span></p>
</th><th>
<p><span class="strong"><strong>Error as percentage of throughput</strong></span></p>
</th></tr></thead><tbody><tr><td>
<p>Cancel first order</p>
</td><td>
<p>1</p>
</td><td>
<p>6,688,878.23</p>
</td><td>
<p>Â±351,518.041</p>
</td><td>
<p>Â±5.26</p>
</td></tr><tr><td>
<p>Cancel first order</p>
</td><td>
<p>10</p>
</td><td>
<p>2,202,233.77</p>
</td><td>
<p>Â±103,557.824</p>
</td><td>
<p>Â±4.70</p>
</td></tr><tr><td>
<p>Cancel first order</p>
</td><td>
<p>50</p>
</td><td>
<p>555,592.56</p>
</td><td>
<p>Â±18,632.547</p>
</td><td>
<p>Â±3.35</p>
</td></tr><tr><td>
<p>Cancel first order</p>
</td><td>
<p>100</p>
</td><td>
<p>305,615.75</p>
</td><td>
<p>Â±14,345.296</p>
</td><td>
<p>Â±4.69</p>
</td></tr><tr><td>
<p>Cancel last order</p>
</td><td>
<p>1</p>
</td><td>
<p>7,365,825.52</p>
</td><td>
<p>Â±284,773.895</p>
</td><td>
<p>Â±3.87</p>
</td></tr><tr><td>
<p>Cancel last order</p>
</td><td>
<p>10</p>
</td><td>
<p>1,691,196.48</p>
</td><td>
<p>Â±54,903.319</p>
</td><td>
<p>Â±3.25</p>
</td></tr><tr><td>
<p>Cancel last order</p>
</td><td>
<p>50</p>
</td><td>
<p>509,339.60</p>
</td><td>
<p>Â±15,582.846</p>
</td><td>
<p>Â±3.06</p>
</td></tr><tr><td>
<p>Cancel last order</p>
</td><td>
<p>100</p>
</td><td>
<p>242,049.87</p>
</td><td>
<p>Â±8,967.785</p>
</td><td>
<p>Â±3.70</p>
</td></tr><tr><td>
<p>Cancel nonexistent order</p>
</td><td>
<p>1</p>
</td><td>
<p>13,285,699.96</p>
</td><td>
<p>Â±374,134.340</p>
</td><td>
<p>Â±2.82</p>
</td></tr><tr><td>
<p>Cancel nonexistent order</p>
</td><td>
<p>10</p>
</td><td>
<p>3,048,323.44</p>
</td><td>
<p>Â±140,983.947</p>
</td><td>
<p>Â±4.62</p>
</td></tr><tr><td>
<p>Cancel nonexistent order</p>
</td><td>
<p>50</p>
</td><td>
<p>772,034.39</p>
</td><td>
<p>Â±16,535.652</p>
</td><td>
<p>Â±2.14</p>
</td></tr><tr><td>
<p>Cancel nonexistent order</p>
</td><td>
<p>100</p>
</td><td>
<p>404,647.90</p>
</td><td>
<p>Â±3,889.509</p>
</td><td>
<p>Â±0.96</p>
</td></tr></tbody></table></div><p>From this result set, we can answer a number of interesting questions that help us characterize order book performance. Here are some questions for you to answer by inspecting the results:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Does the base case of a single enqueued order result in qualitatively similar performance across the three benchmarks?</li><li class="listitem" style="list-style-type: disc">Does each benchmark exhibit linear throughput degradation as enqueued order count increases?</li><li class="listitem" style="list-style-type: disc">As enqueued order count increases, are there changes in relative performance between benchmarks (for example, between canceling the first and last order when the enqueued order count is 100 instead of 10)?</li><li class="listitem" style="list-style-type: disc">Does it appear that there is early termination logic in place when evaluating nonexistent orders?</li></ul></div><p>In addition to these questions that are specific to the order book, it is critical to ask yourself the following questions of any benchmark result:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Do the results pass a test of reasonableness?</li><li class="listitem" style="list-style-type: disc">What could have gone wrong with the test (that is, producing invalid results), and have we put in place safeguards to prevent these shortcomings from occurring?</li></ul></div><p>Errors in measurement or test setup destroy the integrity of your results. These questions aim to make you critically analyze the microbenchmark results. The test of reasonableness requires you to develop a mental model for the executed test. Using the order book, we need to consider what is a plausible number of cancel operations per second. One way to answer this question is to take one of the throughput results, for example, cancel last order with 50 queued orders, and compute the average milliseconds per operation. This is useful because we have a sense for the cost on a per-operation basis from earlier benchmarks; 509,339.595 cancels per second translates to approximately 0.002 ms per operation. This result might be surprisingly low, but bear in mind these results do not account for coordinated omission because there is no targeted throughput rate (that is, the test attempts to send as many cancels per second as possible). The other reason the cost might be lower than expected is because there is only one price level in the book. Typically, the book contains numerous price levels on the buying and selling sides. This may direct us toward designing benchmarks that sweep the number of price levels to better inform our understanding of the cost of managing multiple price levels.</p><p>The second question forces us to critically analyze the test setup and the test methodology. For example, how do we know that the setup function produces the intended order book? One way to defend against this is to add assertions to enforce intended constraints. Another concern to verify is that each cancel invocation in the benchmark yields the intended event. Adding assertions to the benchmark for a single trial may address this concern. However, leaving assertions in the benchmark code is likely to affect performance, and this should be used sparingly, if at all. For added safety, it could make sense to write unit tests for the scenarios that are being tested to ensure that the desired behavior occurs and ensure that the unit test and performance test code are shared.</p><p>When interpreting JMH results, it is important to consider the significance of the computed error. JMH computes error by constructing a confidence interval from a benchmark's iterations. The confidence interval assumes that the results follow a normal distribution, and the error represents the range of the computed 99.9% confidence interval. This suggests that all other things being equal, running more benchmark iterations improves your confidence in the results. The final column in the results table is illustrative of the variability of the results. The lower the variability, the more inclined you should be to trust the results. High result variability suggests that there is an error in measurement or that there is something inhibiting your ability to measure true performance characteristics. This is often a warning sign that you need to revisit your testing methodology and that you should put little trust in the results.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>For our example, we ran 30 iterations to record throughput information. What do you think are the implications of running with fewer iterations? Alternatively, consider the effects of running fewer iterations with increased duration. For example, 10 iterations, each lasting 30 seconds. Build a hypothesis and then run JMH to see the results. Developing awareness for the sensitivity of different benchmark parameters is another way to build an intuition for how to approach future benchmarks.</p></div></div><p>As our JMH configuration does not account for coordinated omission and instead sends a firehose of cancel requests to the order book, we should focus on the relative results rather than the absolute throughput values. The order book-related questions that are posed after the results hone in on relative differences that should be visible independent of the testing environment (for example, available cores or RAM). There is value in focusing on relative concerns because the answers should be more robust to change. If future code changes cause significant relative changes, for example, causing an exponential instead of linear cancel performance degradation, you can have higher confidence that this degradation is due to a code change instead of an environmental change.</p><p>In this section, we saw how to set up, execute, and interpret a JMH microbenchmark. Along the way, we looked at the shortcomings of microbenchmarking without JMH, and the concerns to be aware of during benchmark result analysis. We've only scratched the surface of the capabilities of JMH. We will build on this introduction to JMH in future chapters.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Summary</h1></div></div></div><p>Congratulations, in this chapter you helped improve MVT order book performance, which is going to directly translate to increased company profits and reduced losses! Along the way, you took an in-depth look at how to benchmark and profile on the JVM and what shortcomings to avoid. You also worked through a JMH microbenchmarking primer that will allow you to objectively assess performance improvements in future chapters. In the next chapter, we will look at how Scala language features can be used to write functional software, and we will assess their performance impacts using the skills that we learned in this chapter.</p></div></body></html>