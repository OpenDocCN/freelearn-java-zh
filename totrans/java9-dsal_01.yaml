- en: Chapter 1. Why Bother? – Basic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since you already know Java, you have of course written a few programs, which
    means you have written algorithms. "Well then, what is it?" you might ask. An
    algorithm is a list of well-defined steps that can be followed by a processor
    mechanically, or without involving any sort of intelligence, which would produce
    a desired output in a finite amount of time. Well, that's a long sentence. In
    simpler words, an algorithm is just an unambiguous list of steps to get something
    done. It kind of sounds like we are talking about a program. Isn't a program also
    a list of instructions that we give the computer to follow, in order to get a
    desired result? Yes it is, and that means an algorithm is really just a program.
    Well not really, but almost. An algorithm is a program without the details of
    the particular programming language that we are coding it in. It is the basic
    idea of the program; think of it as an abstraction of a program where you don't
    need to bother about the program's syntactic details.
  prefs: []
  type: TYPE_NORMAL
- en: Well, since we already know about programming, and an algorithm is just a program,
    we are done with it, right? Not really. There is a lot to learn about programs
    and algorithms, that is, how to write an algorithm to achieve a particular goal.
    There are, of course, in general, many ways to solve a particular problem and
    not all ways may be equal. One way may be faster than another, and that is a very
    important thing about algorithms. When we study algorithms, the time it takes
    to execute is of utmost importance. In fact, it is the second most important thing
    about them, the first one being their correctness.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a deeper look into the following ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asymptotic complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why asymptotic complexity matters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why an explicit study of algorithms is important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance of an algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No one wants to wait forever to get something done. Making a program run faster
    surely is important, but how do we know whether a program runs fast? The first
    logical step would be to measure how many seconds the program takes to run. Suppose
    we have a program that, given three numbers, *a*, *b*, and *c*, determines the
    remainder when *a* raised to the power *b* is divided by *c*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say *a=2*, *b=10*, and *c = 7*, *a* raised to the power *b = 2^(10)
    = 1024*, *1024 % 7 = 2*. So, given these values, the program needs to output `2`.
    The following code snippet shows a simple and obvious way of achieving this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now estimate the time it takes by running the program a billion times
    and checking how long it took to run it, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: On my computer, it takes 4,393 milliseconds. So the time taken per call is 4,393
    divided by a billion, that is, about 4.4 nanoseconds. Looks like a very reasonable
    time to do any computation. But what happens if the input is different? What if
    I pass power = `1000`? Let's check that out. Now it takes about 420,000 milliseconds
    to run a billion times, or about 420 nanoseconds per run. Clearly, the time taken
    to do this computation depends on the input, and that means any reasonable way
    to talk about the performance of a program needs to take into account the input
    to the program.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so we can say that the number of nanoseconds our program takes to run
    is 0.42 X power, approximately.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the program with the input (`2`, `1000`, and `7`), you will get
    an output of `0`, which is not correct. The correct output is `2`. So, what is
    going on here? The answer is that the maximum value that a long type variable
    can hold is one less than 2 raised to the power 63, or 9223372036854775807L. The
    value 2 raised to the power 1,000 is, of course, much more than this, causing
    the value to overflow, which brings us to our next point: how much space does
    a program need in order to run?'
  prefs: []
  type: TYPE_NORMAL
- en: In general, the memory space required to run a program can be measured in terms
    of the bytes required for the program to operate. Of course, it requires the space
    to at least store the input and the output. It may as well need some additional
    space to run, which is called auxiliary space. It is quite obvious that just like
    time, the space required to run a program would, in general, also be dependent
    on the input.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of time, apart from the fact that the time depends on the input,
    it also depends on which computer you are running it on. The program that takes
    4 seconds to run on my computer may take 40 seconds on a very old computer from
    the nineties and may run in 2 seconds in yours. However, the actual computer you
    run it on only improves the time by a constant multiplier. To avoid getting into
    too much detail about specifying the details of the hardware the program is running
    on, instead of saying the program takes 0.42 X power milliseconds approximately,
    we can say the time taken is a constant times the power, or simply say it is proportional
    to the power.
  prefs: []
  type: TYPE_NORMAL
- en: Saying the computation time is proportional to the power actually makes it so
    non-specific to hardware, or even the language the program is written in, that
    we can estimate this relationship by just looking at the program and analyzing
    it. Of course, the running time is sort of proportional to the power because there
    is a loop that executes power number of times, except, of course, when the power
    is so small that the other one-time operations outside the loop actually start
    to matter.
  prefs: []
  type: TYPE_NORMAL
- en: Best case, worst case and the average case complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, the time or space required for an algorithm to process a certain
    input depends not only on the size of the input, but also on the actual value
    of the input. For example, a certain algorithm to arrange a list of values in
    increasing order may take much less time if the input is already sorted than when
    it is an arbitrary unordered list. This is why, in general, we must have a different
    function representing the time or space required in the different cases of input.
    However, the best case scenario would be where the resources required for a certain
    size of an input take the least amount of resources. The would also be a worst
    case scenario, in which the algorithm needs the maximum amount of resources for
    a certain size of input. An average case is an estimation of the resources taken
    for a given size of inputs averaged over all values of the input with that size
    weighted by their probability of occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of asymptotic complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We seem to have hit upon an idea, an abstract sense of the running time. Let's
    spell it out. In an abstract way, we analyze the running time of and the space
    required by a program by using what is known as the asymptotic complexity.
  prefs: []
  type: TYPE_NORMAL
- en: We are only interested in what happens when the input is very large because
    it really does not matter how long it takes for a small input to be processed;
    it's going to be small anyway. So, if we have *x³* *+ x²*, and if *x* is very
    large, it's almost the same as *^(x3)*. We also don't want to consider constant
    factors of a function, as we have pointed out earlier, because it is dependent
    on the particular hardware we are running the program on and the particular language
    we have implemented it in. An algorithm implemented in Java will perform a constant
    times slower than the same algorithm written in C. The formal way of tackling
    these abstractions in defining the complexity of an algorithm is called an asymptotic
    bound. Strictly speaking, an asymptotic bound is for a function and not for an
    algorithm. The idea is to first express the time or space required for a given
    algorithm to process an input as a function of the size of the input in bits and
    then looking for an asymptotic bound of that function.
  prefs: []
  type: TYPE_NORMAL
- en: We will consider three types of asymptotic bounds—an upper bound, a lower bound
    and a tight bound. We will discuss these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic upper bound of a function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An upper bound, as the name suggests, puts an upper limit of a function's growth.
    The upper bound is another function that grows at least as fast as the original
    function. What is the point of talking about one function in place of another?
    The function we use is in general a lot more simplified than the actual function
    for computing running time or space required to process a certain size of input.
    It is a lot easier to compare simplified functions than to compare complicated
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a function *f*, we define the notation *O*, called **big O**, in the following
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x) = O(f(x))*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, *x³* *= O(x³**)*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *f(x) = O(g(x))*, then *k f(x) = O(g(x))* for any non-zero constant *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, *5x³* *= O(x³**)* and *2 log x = O(log x)* and *-x³* *= O(x³**)*
    (taking *k= -1*).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *f(x) = O(g(x))* and *|h(x)|<|f(x)|* for all sufficiently large *x*, then
    *f(x) + h(x) = O(g(x))*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, *5x³* *- 25x²* *+ 1 = O(x³**)* because for a sufficiently large
    *x*, *|- 25x²* *+ 1| = 25x²* *- 1* is much less that *| 5x³**| = 5x³*. So, *f(x)
    + g(x) = 5x³* *- 25x²* *+ 1 = O(x³**)* as *f(x) = 5x³* *= O(x³**)*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can prove by similar logic that *x³* *= O( 5x³* *- 25x²* *+ 1)*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: if *f(x) = O(g(x))* and *|h(x)| > |g(x)|* for all sufficiently large *x*, then
    *f(x) = O(h(x))*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, *x³* *= O(x⁴**)*, because if *x* is sufficiently large, *x⁴* *>
    x³*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that whenever there is an inequality on functions, we are only interested
    in what happens when *x* is large; we don't bother about what happens for small
    *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To summarize the above definition, you can drop constant multipliers (rule 2)
    and ignore lower order terms (rule 3). You can also overestimate (rule 4). You
    can also do all combinations for those because rules can be applied any number
    of times.
  prefs: []
  type: TYPE_NORMAL
- en: We had to consider the absolute values of the function to cater to the case
    when values are negative, which never happens in running time, but we still have
    it for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is something about the sign *=* that is not usual. Just because *f(x)
    = O(g(x))*, it does not mean, *O(g(x)) = f(x)*. In fact, the last one does not
    even mean anything.
  prefs: []
  type: TYPE_NORMAL
- en: It is enough for all purposes to just know the preceding definition of the big
    O notation. You can read the following formal definition if you are interested.
    Otherwise you can skip the rest of this subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding idea can be summarized in a formal way. We say the expression
    *f(x) = O(g(x))* means that positive constants *M* and *x⁰* exist, such that *|f(x)|
    < M|g(x)|* whenever *x > x⁰*. Remember that you just have to find one example
    of *M* and *x⁰* that satisfy the condition, to make the assertion *f(x) = O(g(x))*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, *Figure 1* shows an example of a function *T(x) = 100x²* *+2000x+200*.
    This function is *O(x²* *)*, with some *x⁰* *= 11* and *M = 300*. The graph of
    *300x²* overcomes the graph of *T(x)* at *x=11* and then stays above *T(x)* up
    to infinity. Notice that the function *300x²* is lower than *T(x)* for smaller
    values of *x*, but that does not affect our conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Asymptotic upper bound of a function](img/00002.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Asymptotic upper bound
  prefs: []
  type: TYPE_NORMAL
- en: To see that it's the same thing as the previous four points, first think of
    *x⁰* as the way to ensure that *x* is sufficiently large. I leave it up to you
    to prove the above four conditions from the formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will, however, show some examples of using the formal definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*5x²* *= O(x²**)* because we can say, for example, *x⁰* *= 10* and *M = 10*
    and thus *f(x) < Mg(x)* whenever *x > x⁰*, that is, *5x²* *< 10x²* whenever *x
    > 10*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also true that *5x²* *= O(x³**)* because we can say, for example, *x⁰*
    *= 10* and *M = 10* and thus *f(x) < Mg(x)* whenever *x > x⁰*, that is, *5x²*
    *< 10x³* whenever *x > 10*. This highlights a point that if *f(x) = O(g(x))*,
    it is also true that *f(x) = O(h(x))* if *h(x)* is some functions that grows at
    least as fast as *f(x)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How about the function *f(x) = 5x²* *- 10x + 3*? We can easily see that when
    *x* is sufficiently large, *5x²* will far surpass the term *10x*. To prove my
    point, I can simply say *x>5, 5x²**> 10x*. Every time we increment *x* by one,
    the increment in *5x²* is *10x + 1* and the increment in *10x* is just a constant,
    *10*. *10x+1 > 10* for all positive *x*, so it is easy to see why *5x²* is always
    going to stay above *10x* as *x* goes higher and higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, any polynomial of the form *a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+
    a[n-2]* *x^(n-2)* *+ … + a[0]* *= O(x^n)*. To show this, we will first see that
    *a[0] = O(1)*. This is true because we can have *x⁰ = 1* and *M = 2|a[0]|*, and
    we will have *|a[0]**| < 2|a[0]* *|* whenever *x > 1*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us assume it is true for some n. Thus, *a[n]* *x^n* *+ a[n-1]* *x^(n-1)*
    *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *= O(x^n)*. What it means, of course, is that
    some *M[n]* and *x⁰* exist, such that *|a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+ a[n-2]*
    *x^(n-2)* *+ … + a[0]* *| < M[n]* *x^n* whenever *x>x⁰*. We can safely assume
    that *x⁰* *>2*, because if it is not so, we can simply add *2* to it to get a
    new *x⁰*, which is at least *2*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, *|a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *|
    < M[n]* *x^n* implies *|a[n+1]* *x^(n+1) + a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+
    a[n-2]* *x^(n-2)* *+ … + a[0]* *| ≤ |a[n+1]* *x^(n+1)* *| + |a[nxn]* *+ a[n-1]*
    *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *| < |a[n+1]* *x^(n+1)* *| + M[n]*
    *x^n*.
  prefs: []
  type: TYPE_NORMAL
- en: This means *|a[n+1]* *x^(n+1)* *| + M[n]* *x^n* *> |a[n]* *x^n* *+ a[n-1]* *x^(n-1)*
    *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *|*.
  prefs: []
  type: TYPE_NORMAL
- en: If we take *M[n+1]* *= |a[n+1]* *| + M[n]*, we can see that *M[n+1]* *x[n+1]*
    *= |a[n+1]* *| x[n+1]* *+ M[n]* *x^(n+1)* *=|a[n+1]* *x^(n+1)* *| + M[n]* *x^(n+1)*
    *> |a[n+1]* *x^(n+1)* *| + M[n]* *x^n* *> |a[n+1]* *x^(n+1)* *+ a[n]* *x^n* *+
    a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *|*.
  prefs: []
  type: TYPE_NORMAL
- en: That is to say, *|a[n+1]* *x^(n+1)* *+ a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)*
    *+ … + a[0]* *|< M[n+1]* *x^(n+1)* for all *x > x[0]*, that is, *a[n+1]* *x^(n+1)*
    *+ a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *= O(x^(n+1)*
    *)*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have it true for *n=0*, that is, *a0 = O(1)*. This means, by our last
    conclusion, *a* *1[x] + a[0]* *= O(x)*. This means, by the same logic, *a[2]*
    *x²* *+ a[1]* *x + a[0]* *= O(x²* *)*, and so on. We can easily see that this
    means it is true for all polynomials of positive integral degrees.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic upper bound of an algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Okay, so we figured out a way to sort of abstractly specify an upper bound
    on a function that has one argument. When we talk about the running time of a
    program, this argument has to contain information about the input. For example,
    in our algorithm, we can say, the execution time equals O(power). This scheme
    of specifying the input directly will work perfectly fine for all programs or
    algorithms solving the same problem because the input will be the same for all
    of them. However, we might want to use the same technique to measure the complexity
    of the problem itself: it is the complexity of the most efficient program or algorithm
    that can solve the problem. If we try to compare the complexity of different problems,
    though, we will hit a wall because different problems will have different inputs.
    We must specify the running time in terms of something that is common among all
    problems, and that something is the size of the input in bits or bytes. How many
    bits do we need to express the argument, power, when it''s sufficiently large?
    Approximately *log[2]* *(power)*. So, in specifying the running time, our function
    needs to have an input that is of the size *log[2]* *(power)* or *lg (power)*.
    We have seen that the running time of our algorithm is proportional to the power,
    that is, constant times power, which is constant times *2 lg(power) = O(2x)*,where
    *x= lg(power)*, which is the the size of the input.'
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic lower bound of a function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, we don't want to praise an algorithm, we want to shun it; for example,
    when the algorithm is written by someone we don't like or when some algorithm
    is really poorly performing. When we want to shun it for its horrible performance,
    we may want to talk about how badly it performs even for the best input. An a
    symptotic lower bound can be defined just like how greater-than-or-equal-to can
    be defined in terms of less-than-or-equal-to.
  prefs: []
  type: TYPE_NORMAL
- en: 'A function *f(x) = Ω(g(x))* if and only if *g(x) = O(f(x))*. The following
    list shows a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Since *x³* *= O(x³**)*, *x³* *= Ω(x³**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since *x³* *= O(5x³**)*, *5x³* *= Ω(x³**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since *x³* *= O(5x³* *- 25x²* *+ 1)*, *5x³* *- 25x²* *+ 1 = Ω(x³**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since *x³* *= O(x⁴**)*, *x⁴* *= O(x³**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, for those of you who are interested, we say the expression *f(x) = Ω(g(x))*
    means there exist positive constants *M* and *x[0]*, such that *|f(x)| > M|g(x)|*
    whenever *x > x[0]*, which is the same as saying *|g(x)| < (1/M)|f(x)|* whenever
    *x > x[0]*, that is, *g(x) = O(f(x))*.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding definition was introduced by Donald Knuth, which was a stronger
    and more practical definition to be used in computer science. Earlier, there was
    a different definition of the lower bound *Ω* that is more complicated to understand
    and covers a few more edge cases. We will not talk about edge cases here.
  prefs: []
  type: TYPE_NORMAL
- en: While talking about how horrible an algorithm is, we can use an asymptotic lower
    bound of the best case to really make our point. However, even a criticism of
    the worst case of an algorithm is quite a valid argument. We can use an asymptotic
    lower bound of the worst case too for this purpose, when we don't want to find
    out an asymptotic tight bound. In general, the asymptotic lower bound can be used
    to show a minimum rate of growth of a function when the input is large enough
    in size.
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic tight bound of a function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is another kind of bound that sort of means equality in terms of asymptotic
    complexity. A theta bound is specified as *f(x) =* *Ͽ(g(x))* if and only if *f(x)
    = O(g(x))* and *f(x) = Ω(g(x))*. Let''s see some examples to understand this even
    better:'
  prefs: []
  type: TYPE_NORMAL
- en: Since *5x³**=O(x³**)* and also *5x³**=Ω(x³**)*, we have *5**x³**=**Ͽ**(x³**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since *5x³* *+* *4x²**=O(x³**)* and *5x³* *+ 4x²**=**Ω(x³**)*, we have *5x³*
    *+ 4x²**=O(x³**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, even though *5x³* *+ 4x²* *=O(x⁴**)*, since it is not *Ω(x⁴**)*, it
    is also not *Ͽ**(**x⁴**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, *5x³* *+ 4x²* is not *Ͽ**(x²**)* because it is not *O(x²**)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, you can ignore constant multipliers and lower order terms while determining
    the tight bound, but you cannot choose a function which grows either faster or
    slower than the given function. The best way to check whether the bound is right
    is to check the *O* and the condition separately, and say it has a theta bound
    only if they are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Note that since the complexity of an algorithm depends on the particular input,
    in general, the tight bound is used when the complexity remains unchanged by the
    nature of the input.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we try to find the average case complexity, especially when the
    upper bound really happens only in the case of an extremely pathological input.
    But since the average must be taken in accordance with the probability distribution
    of the input, it is not just dependent on the algorithm itself. The bounds themselves
    are just bounds for particular functions and not for algorithms. However, the
    total running time of an algorithm can be expressed as a grand function that changes
    it's formula as per the input, and that function may have different upper and
    lower bounds. There is no sense in talking about an asymptotic average bound because,
    as we discussed, the average case is not just dependent on the algorithm itself,
    but also on the probability distribution of the input. The average case is thus
    stated as a function that would be a probabilistic average running time for all
    inputs, and, in general, the asymptotic upper bound of that average function is
    reported.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of our algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into actually optimizing algorithms, we need to first correct
    our algorithm for large powers. We will use some tricks to do so, as described
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing the problem with large powers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equipped with all the toolboxes of asymptotic analysis, we will start optimizing
    our algorithm. However, since we have already seen that our program does not work
    properly for even moderately large values of power, let's first fix that. There
    are two ways of fixing this; one is to actually give the amount of space it requires
    to store all the intermediate products, and the other is to do a trick to limit
    all the intermediate steps to be within the range of values that the `long` datatype
    can support. We will use binomial theorem to do this part.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, binomial theorem says *(x+y)^n* *= x^n* *+ ^n* *C[1]* *x^(n-1)*
    *y + ^n* *C[2]* *x^(n-2)* *y²* *+ ^n* *C[3]* *x^(n-3)* *y³* *+ ^n* *C[4]* *x^(n-4)*
    *y⁴* *+ … ^n* *C[n-1]* *x¹* *y^(n-1)* *+ y^n* for positive integral values of
    *n*. The important point here is that all the coefficients are integers. Suppose,
    *r* is the remainder when we divide a by *b*. This makes *a = kb + r* true for
    some positive integer *k*. This means *r = a-kb*, and *r^n* *= (a-kb)^n*.
  prefs: []
  type: TYPE_NORMAL
- en: If we expand this using binomial theorem, we have *r^n* *= a^n* *- ^n* *C[1]*
    *a^(n-1)* *.kb + ^n* *C[2]* *a^(n-2)* *.(kb)²* *- ^n* *C[3]* *a^(n-3)* *.(kb)³*
    *+ ^n* *C[4]* *a^(n-4)* *.(kb)⁴* *+ … ^n* *C[n-1]* *a¹* *.(kb)^(n-1)* *± (kb)^n*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that apart from the first term, all other terms have *b* as a factor. Which
    means that we can write *r^n* *= a^n* *+ bM* for some integer *M*. If we divide
    both sides by *b* now and take the remainder, we have *r^n* *% b = a^n* *% b*,
    where *%* is the Java operator for finding the remainder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea now would be to take the remainder by the divisor every time we raise
    the power. This way, we will never have to store more than the range of the remainder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This program obviously does not change the time complexity of the program; it
    just fixes the problem with large powers. The program also maintains a constant
    space complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Improving time complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The current running time complexity is *O(2^x* *)*, where *x* is the size of
    the input as we have already computed. Can we do better than this? Let's see.
  prefs: []
  type: TYPE_NORMAL
- en: What we need to compute is *(base^(power)* *) % divisor*. This is, of course,
    the same as *(base²)^(power/2)* *% divisor*. If we have an even *power*, we have
    reduced the number of operations by half. If we can keep doing this, we can raise
    the *power* of *base* by *2^n* in just *n* steps, which means our loop only has
    to run *lg(power)* times, and hence, the complexity is *O(lg(2^x* *)) = O(x)*,
    where *x* is the number of bits to store *power*. This is a substantial reduction
    in the number of steps to compute the value for large powers.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a catch. What happens if the *power* is not divisible by
    *2*? Well, then we can write *(base^(power)* *)% divisor = (base ((base^(power-1)*
    *))%divisor = (base ((base²)^(power-1)* *)%divisor*, and *power-1* is, of course,
    even and the computation can proceed. We will write up this code in a program.
    The idea is to start from the most significant bit and move towards less and less
    significant bits. If a bit with *1* has *n* bits after it, it represents multiplying
    the result by the base and then squaring *n* times after this bit. We accumulate
    this squaring by squaring for the subsequent steps. If we find a zero, we keep
    squaring for the sake of accumulating squaring for the earlier bits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'First reverse the bits of our `power` so that it is easier to access them from
    the least important side, which is more easily accessible. We also count the number
    of bits for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we extract one bit at a time. Since we have already reversed the order
    of bit, the first one we get is the most significant one. Just to get an intuition
    on the order, the first bit we collect will eventually be squared the maximum
    number of times and hence will act like the most significant bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We test the performance of the algorithm; we compare the time taken for the
    same computation with the earlier and final algorithms with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first algorithm takes 130,190 milliseconds to complete all 1,000 times execution
    on my computer and the second one takes just 2 milliseconds to do the same. This
    clearly shows the tremendous gain in performance for a large power like 10 million.
    The algorithm for squaring the term repeatedly to achieve exponentiation like
    we did is called... well, exponentiation by squaring. This example should be able
    to motivate you to study algorithms for the sheer obvious advantage it can give
    in improving the performance of computer programs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how we can think about measuring the running time of
    and the memory required by an algorithm in seconds and bytes, respectively. Since
    this depends on the particular implementation, the programming platform, and the
    hardware, we need a notion of talking about running time in an abstract way. Asymptotic
    complexity is a measure of the growth of a function when the input is very large.
    We can use it to abstract our discussion on running time. This is not to say that
    a programmer should not spend any time to make a run a program twice as fast,
    but that comes only after the program is already running at the minimum asymptotic
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw that the asymptotic complexity is not just a property of the problem
    at hand that we are trying to solve, but also a property of the particular way
    we are solving it, that is, the particular algorithm we are using. We also saw
    that two programs solving the same problem while running different algorithms
    with different asymptotic complexities can perform vastly differently for large
    inputs. This should be enough motivation to study algorithms explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will study the most used algorithmic tricks and
    concepts required in daily use. We will start from the very easy ones that are
    also the building blocks for the more advanced techniques. This book is, of course,
    by no means comprehensive; the objective is to provide enough background to make
    you comfortable with the basic concepts and then you can read on.
  prefs: []
  type: TYPE_NORMAL
