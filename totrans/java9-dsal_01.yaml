- en: Chapter 1. Why Bother? – Basic
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since you already know Java, you have of course written a few programs, which
    means you have written algorithms. "Well then, what is it?" you might ask. An
    algorithm is a list of well-defined steps that can be followed by a processor
    mechanically, or without involving any sort of intelligence, which would produce
    a desired output in a finite amount of time. Well, that's a long sentence. In
    simpler words, an algorithm is just an unambiguous list of steps to get something
    done. It kind of sounds like we are talking about a program. Isn't a program also
    a list of instructions that we give the computer to follow, in order to get a
    desired result? Yes it is, and that means an algorithm is really just a program.
    Well not really, but almost. An algorithm is a program without the details of
    the particular programming language that we are coding it in. It is the basic
    idea of the program; think of it as an abstraction of a program where you don't
    need to bother about the program's syntactic details.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Well, since we already know about programming, and an algorithm is just a program,
    we are done with it, right? Not really. There is a lot to learn about programs
    and algorithms, that is, how to write an algorithm to achieve a particular goal.
    There are, of course, in general, many ways to solve a particular problem and
    not all ways may be equal. One way may be faster than another, and that is a very
    important thing about algorithms. When we study algorithms, the time it takes
    to execute is of utmost importance. In fact, it is the second most important thing
    about them, the first one being their correctness.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a deeper look into the following ideas:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of an algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asymptotic complexity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why asymptotic complexity matters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why an explicit study of algorithms is important
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance of an algorithm
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No one wants to wait forever to get something done. Making a program run faster
    surely is important, but how do we know whether a program runs fast? The first
    logical step would be to measure how many seconds the program takes to run. Suppose
    we have a program that, given three numbers, *a*, *b*, and *c*, determines the
    remainder when *a* raised to the power *b* is divided by *c*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say *a=2*, *b=10*, and *c = 7*, *a* raised to the power *b = 2^(10)
    = 1024*, *1024 % 7 = 2*. So, given these values, the program needs to output `2`.
    The following code snippet shows a simple and obvious way of achieving this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can now estimate the time it takes by running the program a billion times
    and checking how long it took to run it, as shown in the following code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: On my computer, it takes 4,393 milliseconds. So the time taken per call is 4,393
    divided by a billion, that is, about 4.4 nanoseconds. Looks like a very reasonable
    time to do any computation. But what happens if the input is different? What if
    I pass power = `1000`? Let's check that out. Now it takes about 420,000 milliseconds
    to run a billion times, or about 420 nanoseconds per run. Clearly, the time taken
    to do this computation depends on the input, and that means any reasonable way
    to talk about the performance of a program needs to take into account the input
    to the program.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的计算机上，它需要4,393毫秒。所以每次调用的耗时是4,393除以十亿，即大约4.4纳秒。看起来做任何计算都是一个合理的时间。但如果输入不同呢？如果我传递power
    = `1000`会怎样？让我们检查一下。现在运行一亿次需要大约420,000毫秒，或者每次运行大约420纳秒。显然，做这个计算所需的时间依赖于输入，这意味着任何关于程序性能的合理讨论都需要考虑程序的输入。
- en: Okay, so we can say that the number of nanoseconds our program takes to run
    is 0.42 X power, approximately.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，所以我们可以说，我们的程序运行所需的时间大约是0.42 X次幂纳秒。
- en: 'If you run the program with the input (`2`, `1000`, and `7`), you will get
    an output of `0`, which is not correct. The correct output is `2`. So, what is
    going on here? The answer is that the maximum value that a long type variable
    can hold is one less than 2 raised to the power 63, or 9223372036854775807L. The
    value 2 raised to the power 1,000 is, of course, much more than this, causing
    the value to overflow, which brings us to our next point: how much space does
    a program need in order to run?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用输入（`2`，`1000`和`7`）运行程序，你会得到一个输出为`0`的结果，这是不正确的。正确的输出应该是`2`。那么这里发生了什么？答案是，长整型变量可以持有的最大值是2的63次幂减1，即9223372036854775807L。2的1000次幂当然比这个大得多，导致值溢出，这把我们带到了下一个问题：程序运行需要多少空间？
- en: In general, the memory space required to run a program can be measured in terms
    of the bytes required for the program to operate. Of course, it requires the space
    to at least store the input and the output. It may as well need some additional
    space to run, which is called auxiliary space. It is quite obvious that just like
    time, the space required to run a program would, in general, also be dependent
    on the input.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，运行程序所需的内存空间可以用程序操作所需的字节数来衡量。当然，它需要至少存储输入和输出的空间。它可能还需要额外的空间来运行，这被称为辅助空间。很明显，就像时间一样，运行程序所需的内存空间在一般情况下也会依赖于输入。
- en: In the case of time, apart from the fact that the time depends on the input,
    it also depends on which computer you are running it on. The program that takes
    4 seconds to run on my computer may take 40 seconds on a very old computer from
    the nineties and may run in 2 seconds in yours. However, the actual computer you
    run it on only improves the time by a constant multiplier. To avoid getting into
    too much detail about specifying the details of the hardware the program is running
    on, instead of saying the program takes 0.42 X power milliseconds approximately,
    we can say the time taken is a constant times the power, or simply say it is proportional
    to the power.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间的情况下，除了时间依赖于输入之外，它还取决于你在哪台计算机上运行它。在我的计算机上运行需要4秒钟的程序，在90年代的一台非常旧的计算机上可能需要40秒钟，而在你的计算机上可能只需要2秒钟。然而，你实际运行的计算机只通过一个常数倍数来提高时间。为了避免过多地详细说明程序运行的硬件细节，我们不说程序大约需要0.42
    X次幂毫秒，而可以说所需时间是常数乘以次幂，或者简单地说它是与次幂成比例的。
- en: Saying the computation time is proportional to the power actually makes it so
    non-specific to hardware, or even the language the program is written in, that
    we can estimate this relationship by just looking at the program and analyzing
    it. Of course, the running time is sort of proportional to the power because there
    is a loop that executes power number of times, except, of course, when the power
    is so small that the other one-time operations outside the loop actually start
    to matter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 说计算时间与次幂成比例实际上使得它与硬件无关，甚至与程序编写的语言也无关，我们可以仅通过查看程序并分析它来估计这种关系。当然，运行时间在某种程度上与次幂成比例，因为有一个循环执行了次幂次，除非，当然，次幂非常小，以至于循环外的其他一次操作实际上开始变得重要。
- en: Best case, worst case and the average case complexity
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳情况、最坏情况和平均情况复杂度
- en: In general, the time or space required for an algorithm to process a certain
    input depends not only on the size of the input, but also on the actual value
    of the input. For example, a certain algorithm to arrange a list of values in
    increasing order may take much less time if the input is already sorted than when
    it is an arbitrary unordered list. This is why, in general, we must have a different
    function representing the time or space required in the different cases of input.
    However, the best case scenario would be where the resources required for a certain
    size of an input take the least amount of resources. The would also be a worst
    case scenario, in which the algorithm needs the maximum amount of resources for
    a certain size of input. An average case is an estimation of the resources taken
    for a given size of inputs averaged over all values of the input with that size
    weighted by their probability of occurrence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，算法处理特定输入所需的时间或空间不仅取决于输入的大小，还取决于输入的实际值。例如，如果输入已经排序，则用于按升序排列值列表的某些算法可能花费的时间要少得多，而如果它是一个任意无序列表，则花费的时间要多。这就是为什么，通常，我们必须有不同的函数来表示不同情况下所需的时间或空间。然而，最佳情况是，对于特定大小的输入所需资源最少。也会有最坏情况，其中算法需要为特定大小的输入最多资源。平均情况是对给定大小输入的资源消耗的估计，通过对具有该大小的所有输入值进行加权平均，权重为它们发生的概率。
- en: Analysis of asymptotic complexity
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 渐近复杂性的分析
- en: We seem to have hit upon an idea, an abstract sense of the running time. Let's
    spell it out. In an abstract way, we analyze the running time of and the space
    required by a program by using what is known as the asymptotic complexity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们似乎已经找到了一个想法，一个关于运行时间的抽象概念。让我们把它讲清楚。以抽象的方式，我们通过使用所谓的渐近复杂性来分析程序的运行时间和所需空间。
- en: We are only interested in what happens when the input is very large because
    it really does not matter how long it takes for a small input to be processed;
    it's going to be small anyway. So, if we have *x³* *+ x²*, and if *x* is very
    large, it's almost the same as *^(x3)*. We also don't want to consider constant
    factors of a function, as we have pointed out earlier, because it is dependent
    on the particular hardware we are running the program on and the particular language
    we have implemented it in. An algorithm implemented in Java will perform a constant
    times slower than the same algorithm written in C. The formal way of tackling
    these abstractions in defining the complexity of an algorithm is called an asymptotic
    bound. Strictly speaking, an asymptotic bound is for a function and not for an
    algorithm. The idea is to first express the time or space required for a given
    algorithm to process an input as a function of the size of the input in bits and
    then looking for an asymptotic bound of that function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心当输入非常大时会发生什么，因为处理小输入所需的时间实际上并不重要；无论如何它都会很小。所以，如果我们有 *x³* *+ x²*，并且如果 *x*
    非常大，它几乎等同于 *^(x3)*。我们也不考虑函数的常数因子，因为我们之前已经指出，它依赖于我们运行程序的特定硬件和特定语言。用 Java 实现的算法将比用
    C 语言编写的相同算法慢一个常数倍。处理这些抽象定义算法复杂性的正式方法称为渐近界。严格来说，渐近界是针对函数的，而不是针对算法。其想法是首先将给定算法处理输入所需的时间或空间表示为输入大小的函数，然后寻找该函数的渐近界。
- en: We will consider three types of asymptotic bounds—an upper bound, a lower bound
    and a tight bound. We will discuss these in the following sections.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑三种类型的渐近界——上界、下界和紧界。我们将在以下章节中讨论这些内容。
- en: Asymptotic upper bound of a function
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数的渐近上界
- en: An upper bound, as the name suggests, puts an upper limit of a function's growth.
    The upper bound is another function that grows at least as fast as the original
    function. What is the point of talking about one function in place of another?
    The function we use is in general a lot more simplified than the actual function
    for computing running time or space required to process a certain size of input.
    It is a lot easier to compare simplified functions than to compare complicated
    functions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，上界为函数的增长设定了一个上限。上界是至少与原始函数一样快速增长的另一个函数。谈论一个函数代替另一个函数有什么意义呢？我们使用的函数通常比实际用于计算处理特定大小输入所需运行时间或空间的函数要简单得多。比较简化函数比比较复杂函数容易得多。
- en: 'For a function *f*, we define the notation *O*, called **big O**, in the following
    ways:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个函数 *f*，我们以下列方式定义符号 *O*，称为**大O**：
- en: '*f(x) = O(f(x))*.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f(x) = O(f(x))*.'
- en: For example, *x³* *= O(x³**)*.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，*x³* *= O(x³**)*。
- en: If *f(x) = O(g(x))*, then *k f(x) = O(g(x))* for any non-zero constant *k*.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *f(x) = O(g(x))*，那么对于任何非零常数 *k*，*k f(x) = O(g(x))*。
- en: For example, *5x³* *= O(x³**)* and *2 log x = O(log x)* and *-x³* *= O(x³**)*
    (taking *k= -1*).
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，*5x³* *= O(x³**)*，*2 log x = O(log x)* 和 *-x³* *= O(x³**)*（取 *k= -1*）。
- en: If *f(x) = O(g(x))* and *|h(x)|<|f(x)|* for all sufficiently large *x*, then
    *f(x) + h(x) = O(g(x))*.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *f(x) = O(g(x))* 并且对于所有足够大的 *x*，*|h(x)|<|f(x)|*，那么 *f(x) + h(x) = O(g(x))*。
- en: For example, *5x³* *- 25x²* *+ 1 = O(x³**)* because for a sufficiently large
    *x*, *|- 25x²* *+ 1| = 25x²* *- 1* is much less that *| 5x³**| = 5x³*. So, *f(x)
    + g(x) = 5x³* *- 25x²* *+ 1 = O(x³**)* as *f(x) = 5x³* *= O(x³**)*.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，*5x³* *- 25x²* *+ 1 = O(x³**)*，因为对于足够大的 *x*，*|- 25x²* *+ 1| = 25x²* *- 1*
    远小于 *| 5x³**| = 5x³*。所以，*f(x) + g(x) = 5x³* *- 25x²* *+ 1 = O(x³**)*，因为 *f(x)
    = 5x³* *= O(x³**)*。
- en: We can prove by similar logic that *x³* *= O( 5x³* *- 25x²* *+ 1)*.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以用类似的逻辑证明 *x³* *= O( 5x³* *- 25x²* *+ 1)*。
- en: if *f(x) = O(g(x))* and *|h(x)| > |g(x)|* for all sufficiently large *x*, then
    *f(x) = O(h(x))*.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *f(x) = O(g(x))* 并且对于所有足够大的 *x*，*|h(x)| > |g(x)|*，那么 *f(x) = O(h(x))*。
- en: For example, *x³* *= O(x⁴**)*, because if *x* is sufficiently large, *x⁴* *>
    x³*.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，*x³* *= O(x⁴**)*，因为如果 *x* 足够大，*x⁴* > *x³*。
- en: Note that whenever there is an inequality on functions, we are only interested
    in what happens when *x* is large; we don't bother about what happens for small
    *x*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每当函数上有不等式时，我们只对 *x* 大时发生的情况感兴趣；我们不会关心 *x* 小时发生的情况。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To summarize the above definition, you can drop constant multipliers (rule 2)
    and ignore lower order terms (rule 3). You can also overestimate (rule 4). You
    can also do all combinations for those because rules can be applied any number
    of times.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结上述定义，你可以省略常数乘数（规则2）并忽略低阶项（规则3）。你也可以进行高估（规则4）。你也可以对这些组合进行所有操作，因为规则可以多次应用。
- en: We had to consider the absolute values of the function to cater to the case
    when values are negative, which never happens in running time, but we still have
    it for completeness.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须考虑函数的绝对值，以适应值可能为负的情况，这在运行时永远不会发生，但我们仍然保留它以示完整。
- en: Note
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There is something about the sign *=* that is not usual. Just because *f(x)
    = O(g(x))*, it does not mean, *O(g(x)) = f(x)*. In fact, the last one does not
    even mean anything.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于符号 *=* 有一点不寻常。仅仅因为 *f(x) = O(g(x))*，并不意味着 *O(g(x)) = f(x)*。事实上，后者甚至没有任何意义。
- en: It is enough for all purposes to just know the preceding definition of the big
    O notation. You can read the following formal definition if you are interested.
    Otherwise you can skip the rest of this subsection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有目的来说，只需要知道前面的大O符号的定义就足够了。如果你感兴趣，可以阅读以下正式定义。否则，你可以跳过本小节的其余部分。
- en: The preceding idea can be summarized in a formal way. We say the expression
    *f(x) = O(g(x))* means that positive constants *M* and *x⁰* exist, such that *|f(x)|
    < M|g(x)|* whenever *x > x⁰*. Remember that you just have to find one example
    of *M* and *x⁰* that satisfy the condition, to make the assertion *f(x) = O(g(x))*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述想法可以用一种正式的方式总结。我们说表达式 *f(x) = O(g(x))* 的意思是存在正的常数 *M* 和 *x⁰*，使得当 *x > x⁰*
    时，*|f(x)| < M|g(x)|*。记住，你只需要找到一个满足条件的 *M* 和 *x⁰* 的例子，就可以断言 *f(x) = O(g(x))*。
- en: For example, *Figure 1* shows an example of a function *T(x) = 100x²* *+2000x+200*.
    This function is *O(x²* *)*, with some *x⁰* *= 11* and *M = 300*. The graph of
    *300x²* overcomes the graph of *T(x)* at *x=11* and then stays above *T(x)* up
    to infinity. Notice that the function *300x²* is lower than *T(x)* for smaller
    values of *x*, but that does not affect our conclusion.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图1* 展示了一个函数 *T(x) = 100x²* *+2000x+200* 的例子。这个函数是 *O(x²* *)*，其中 *x⁰* *=
    11* 和 *M = 300*。当 *x=11* 时，*300x²* 的图像超过了 *T(x)* 的图像，然后一直保持在 *T(x)* 之上直到无穷大。注意，对于较小的
    *x* 值，*300x²* 的图像低于 *T(x)*，但这并不影响我们的结论。
- en: '![Asymptotic upper bound of a function](img/00002.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![函数的渐近上界](img/00002.jpeg)'
- en: Figure 1\. Asymptotic upper bound
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 渐近上界
- en: To see that it's the same thing as the previous four points, first think of
    *x⁰* as the way to ensure that *x* is sufficiently large. I leave it up to you
    to prove the above four conditions from the formal definition.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明它与前面的四点相同，首先考虑 *x⁰* 是确保 *x* 足够大的方式。我把它留给你去证明上述四个条件来自形式定义。
- en: 'I will, however, show some examples of using the formal definition:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我将展示一些使用形式定义的例子：
- en: '*5x²* *= O(x²**)* because we can say, for example, *x⁰* *= 10* and *M = 10*
    and thus *f(x) < Mg(x)* whenever *x > x⁰*, that is, *5x²* *< 10x²* whenever *x
    > 10*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*5x²* *= O(x²**)*，因为我们可以说，例如，*x⁰* *= 10* 和 *M = 10*，因此当 *x > x⁰* 时，*f(x) <
    Mg(x)*，即 *5x²* *< 10x²* 当 *x > 10*。'
- en: It is also true that *5x²* *= O(x³**)* because we can say, for example, *x⁰*
    *= 10* and *M = 10* and thus *f(x) < Mg(x)* whenever *x > x⁰*, that is, *5x²*
    *< 10x³* whenever *x > 10*. This highlights a point that if *f(x) = O(g(x))*,
    it is also true that *f(x) = O(h(x))* if *h(x)* is some functions that grows at
    least as fast as *f(x)*.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How about the function *f(x) = 5x²* *- 10x + 3*? We can easily see that when
    *x* is sufficiently large, *5x²* will far surpass the term *10x*. To prove my
    point, I can simply say *x>5, 5x²**> 10x*. Every time we increment *x* by one,
    the increment in *5x²* is *10x + 1* and the increment in *10x* is just a constant,
    *10*. *10x+1 > 10* for all positive *x*, so it is easy to see why *5x²* is always
    going to stay above *10x* as *x* goes higher and higher.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, any polynomial of the form *a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+
    a[n-2]* *x^(n-2)* *+ … + a[0]* *= O(x^n)*. To show this, we will first see that
    *a[0] = O(1)*. This is true because we can have *x⁰ = 1* and *M = 2|a[0]|*, and
    we will have *|a[0]**| < 2|a[0]* *|* whenever *x > 1*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us assume it is true for some n. Thus, *a[n]* *x^n* *+ a[n-1]* *x^(n-1)*
    *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *= O(x^n)*. What it means, of course, is that
    some *M[n]* and *x⁰* exist, such that *|a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+ a[n-2]*
    *x^(n-2)* *+ … + a[0]* *| < M[n]* *x^n* whenever *x>x⁰*. We can safely assume
    that *x⁰* *>2*, because if it is not so, we can simply add *2* to it to get a
    new *x⁰*, which is at least *2*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Now, *|a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *|
    < M[n]* *x^n* implies *|a[n+1]* *x^(n+1) + a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+
    a[n-2]* *x^(n-2)* *+ … + a[0]* *| ≤ |a[n+1]* *x^(n+1)* *| + |a[nxn]* *+ a[n-1]*
    *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *| < |a[n+1]* *x^(n+1)* *| + M[n]*
    *x^n*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: This means *|a[n+1]* *x^(n+1)* *| + M[n]* *x^n* *> |a[n]* *x^n* *+ a[n-1]* *x^(n-1)*
    *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *|*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: If we take *M[n+1]* *= |a[n+1]* *| + M[n]*, we can see that *M[n+1]* *x[n+1]*
    *= |a[n+1]* *| x[n+1]* *+ M[n]* *x^(n+1)* *=|a[n+1]* *x^(n+1)* *| + M[n]* *x^(n+1)*
    *> |a[n+1]* *x^(n+1)* *| + M[n]* *x^n* *> |a[n+1]* *x^(n+1)* *+ a[n]* *x^n* *+
    a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *|*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: That is to say, *|a[n+1]* *x^(n+1)* *+ a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)*
    *+ … + a[0]* *|< M[n+1]* *x^(n+1)* for all *x > x[0]*, that is, *a[n+1]* *x^(n+1)*
    *+ a[n]* *x^n* *+ a[n-1]* *x^(n-1)* *+ a[n-2]* *x^(n-2)* *+ … + a[0]* *= O(x^(n+1)*
    *)*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have it true for *n=0*, that is, *a0 = O(1)*. This means, by our last
    conclusion, *a* *1[x] + a[0]* *= O(x)*. This means, by the same logic, *a[2]*
    *x²* *+ a[1]* *x + a[0]* *= O(x²* *)*, and so on. We can easily see that this
    means it is true for all polynomials of positive integral degrees.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Asymptotic upper bound of an algorithm
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Okay, so we figured out a way to sort of abstractly specify an upper bound
    on a function that has one argument. When we talk about the running time of a
    program, this argument has to contain information about the input. For example,
    in our algorithm, we can say, the execution time equals O(power). This scheme
    of specifying the input directly will work perfectly fine for all programs or
    algorithms solving the same problem because the input will be the same for all
    of them. However, we might want to use the same technique to measure the complexity
    of the problem itself: it is the complexity of the most efficient program or algorithm
    that can solve the problem. If we try to compare the complexity of different problems,
    though, we will hit a wall because different problems will have different inputs.
    We must specify the running time in terms of something that is common among all
    problems, and that something is the size of the input in bits or bytes. How many
    bits do we need to express the argument, power, when it''s sufficiently large?
    Approximately *log[2]* *(power)*. So, in specifying the running time, our function
    needs to have an input that is of the size *log[2]* *(power)* or *lg (power)*.
    We have seen that the running time of our algorithm is proportional to the power,
    that is, constant times power, which is constant times *2 lg(power) = O(2x)*,where
    *x= lg(power)*, which is the the size of the input.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们找到了一种抽象地指定一个单参数函数上界的方法。当我们谈论程序的运行时间时，这个参数必须包含有关输入的信息。例如，在我们的算法中，我们可以说，执行时间等于
    O(幂)。这种直接指定输入的方法对于所有解决相同问题的程序或算法都是完美的，因为它们的输入都是相同的。然而，我们可能还想用同样的技术来衡量问题的复杂性：这是解决该问题的最有效程序或算法的复杂性。如果我们试图比较不同问题的复杂性，那么我们会遇到障碍，因为不同的问题会有不同的输入。我们必须用所有问题共有的某个东西来指定运行时间，那就是输入的大小，以比特或字节为单位。当我们需要表达足够大的参数，幂时，我们需要多少比特？大约
    *log[2]* *(幂)*。因此，在指定运行时间时，我们的函数需要一个大小为 *log[2]* *(幂)* 或 *lg (幂)* 的输入。我们已经看到，我们算法的运行时间与幂成正比，即常数乘以幂，这是常数乘以
    *2 lg(power) = O(2x)*，其中 *x= lg(power)*，这是输入的大小。
- en: Asymptotic lower bound of a function
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数的渐近下界
- en: Sometimes, we don't want to praise an algorithm, we want to shun it; for example,
    when the algorithm is written by someone we don't like or when some algorithm
    is really poorly performing. When we want to shun it for its horrible performance,
    we may want to talk about how badly it performs even for the best input. An a
    symptotic lower bound can be defined just like how greater-than-or-equal-to can
    be defined in terms of less-than-or-equal-to.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们不想赞扬一个算法，我们想避开它；例如，当算法是由我们不喜欢的某人编写的，或者当某个算法表现真的很差时。当我们想因为它糟糕的性能而避开它时，我们可能想谈谈它在最佳输入下表现得多糟糕。渐近下界可以定义得就像大于等于可以用小于等于来定义一样。
- en: 'A function *f(x) = Ω(g(x))* if and only if *g(x) = O(f(x))*. The following
    list shows a few examples:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数 *f(x) = Ω(g(x))* 当且仅当 *g(x) = O(f(x))*. 以下是一些例子：
- en: Since *x³* *= O(x³**)*, *x³* *= Ω(x³**)*
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 *x³* = O(x³)*，*x³* = Ω(x³)*
- en: Since *x³* *= O(5x³**)*, *5x³* *= Ω(x³**)*
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 *x³* = O(x³)*，*5x³* = Ω(x³)*
- en: Since *x³* *= O(5x³* *- 25x²* *+ 1)*, *5x³* *- 25x²* *+ 1 = Ω(x³**)*
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 *x³* = O(5x³ - 25x² + 1)，*5x³ - 25x² + 1 = Ω(x³)*
- en: Since *x³* *= O(x⁴**)*, *x⁴* *= O(x³**)*
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 *x³* = O(x⁴)*，*x⁴* = O(x³)*
- en: Again, for those of you who are interested, we say the expression *f(x) = Ω(g(x))*
    means there exist positive constants *M* and *x[0]*, such that *|f(x)| > M|g(x)|*
    whenever *x > x[0]*, which is the same as saying *|g(x)| < (1/M)|f(x)|* whenever
    *x > x[0]*, that is, *g(x) = O(f(x))*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，对于那些感兴趣的人，我们说表达式 *f(x) = Ω(g(x))* 意味着存在正的常数 *M* 和 *x[0]*，使得当 *x > x[0]* 时，*|f(x)|
    > M|g(x)|*，这等同于说当 *x > x[0]* 时，*|g(x)| < (1/M)|f(x)|*，也就是说，*g(x) = O(f(x))*。
- en: The preceding definition was introduced by Donald Knuth, which was a stronger
    and more practical definition to be used in computer science. Earlier, there was
    a different definition of the lower bound *Ω* that is more complicated to understand
    and covers a few more edge cases. We will not talk about edge cases here.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义是由唐纳德·克努特提出的，这是一个更强、更实用的定义，用于计算机科学。在此之前，有一个更复杂、覆盖更多边缘情况的下界 *Ω* 的不同定义。我们在这里不会讨论边缘情况。
- en: While talking about how horrible an algorithm is, we can use an asymptotic lower
    bound of the best case to really make our point. However, even a criticism of
    the worst case of an algorithm is quite a valid argument. We can use an asymptotic
    lower bound of the worst case too for this purpose, when we don't want to find
    out an asymptotic tight bound. In general, the asymptotic lower bound can be used
    to show a minimum rate of growth of a function when the input is large enough
    in size.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈论一个算法有多糟糕时，我们可以使用最佳情况的渐近下界来真正地表达我们的观点。然而，即使是对算法最坏情况的批评也是一个相当有说服力的论点。当我们不想找出渐近紧界时，我们也可以使用最坏情况的渐近下界。一般来说，渐近下界可以用来显示当输入足够大时函数的最小增长率。
- en: Asymptotic tight bound of a function
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数的渐近紧界
- en: 'There is another kind of bound that sort of means equality in terms of asymptotic
    complexity. A theta bound is specified as *f(x) =* *Ͽ(g(x))* if and only if *f(x)
    = O(g(x))* and *f(x) = Ω(g(x))*. Let''s see some examples to understand this even
    better:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种界限，在渐近复杂性的意义上相当于相等。theta界被指定为*f(x) = ω(g(x))*当且仅当*f(x) = O(g(x))*和*f(x)
    = Ω(g(x))*. 让我们通过一些例子来更好地理解这一点：
- en: Since *5x³**=O(x³**)* and also *5x³**=Ω(x³**)*, we have *5**x³**=**Ͽ**(x³**)*
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于*5x³ = O(x³)*和*5x³ = Ω(x³)*，因此*5x³ = ω(x³)*
- en: Since *5x³* *+* *4x²**=O(x³**)* and *5x³* *+ 4x²**=**Ω(x³**)*, we have *5x³*
    *+ 4x²**=O(x³**)*
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于*5x³ + 4x² = O(x³)*和*5x³ + 4x² = Ω(x³)*，因此*5x³ + 4x² = O(x³)*
- en: However, even though *5x³* *+ 4x²* *=O(x⁴**)*, since it is not *Ω(x⁴**)*, it
    is also not *Ͽ**(**x⁴**)*
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，尽管*5x³ + 4x² = O(x⁴)*，因为它不是*Ω(x⁴)*，所以它也不是*ω**(x⁴)*
- en: Similarly, *5x³* *+ 4x²* is not *Ͽ**(x²**)* because it is not *O(x²**)*
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，*5x³ + 4x²*不是*ω**(x²)*，因为它不是*O(x²)*
- en: In short, you can ignore constant multipliers and lower order terms while determining
    the tight bound, but you cannot choose a function which grows either faster or
    slower than the given function. The best way to check whether the bound is right
    is to check the *O* and the condition separately, and say it has a theta bound
    only if they are the same.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在确定紧界时，你可以忽略常数乘数和低阶项，但不能选择一个比给定函数增长更快或更慢的函数。检查界是否正确最好的方法是分别检查*O*和条件，只有当它们相同时，才可以说它有一个theta界。
- en: Note that since the complexity of an algorithm depends on the particular input,
    in general, the tight bound is used when the complexity remains unchanged by the
    nature of the input.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于算法的复杂性取决于特定的输入，一般来说，当复杂性不受输入性质的影响时，使用紧界。
- en: In some cases, we try to find the average case complexity, especially when the
    upper bound really happens only in the case of an extremely pathological input.
    But since the average must be taken in accordance with the probability distribution
    of the input, it is not just dependent on the algorithm itself. The bounds themselves
    are just bounds for particular functions and not for algorithms. However, the
    total running time of an algorithm can be expressed as a grand function that changes
    it's formula as per the input, and that function may have different upper and
    lower bounds. There is no sense in talking about an asymptotic average bound because,
    as we discussed, the average case is not just dependent on the algorithm itself,
    but also on the probability distribution of the input. The average case is thus
    stated as a function that would be a probabilistic average running time for all
    inputs, and, in general, the asymptotic upper bound of that average function is
    reported.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们试图找到平均情况复杂度，特别是在上限仅在极端病态输入的情况下发生时。但由于平均必须根据输入的概率分布来取，它不仅仅依赖于算法本身。界限本身只是特定函数的界限，而不是算法的界限。然而，算法的总运行时间可以表示为一个根据输入改变公式的广义函数，该函数可能具有不同的上界和下界。讨论渐近平均界是没有意义的，因为我们已经讨论过，平均情况不仅仅依赖于算法本身，还依赖于输入的概率分布。因此，平均情况被表述为一个函数，它将是所有输入的概率平均运行时间，并且通常报告该平均函数的渐近上界。
- en: Optimization of our algorithm
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们算法的优化
- en: Before we dive into actually optimizing algorithms, we need to first correct
    our algorithm for large powers. We will use some tricks to do so, as described
    below.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入优化算法之前，我们首先需要纠正大幂次算法的错误。我们将使用一些技巧来实现这一点，如下所述。
- en: Fixing the problem with large powers
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决大幂次问题
- en: Equipped with all the toolboxes of asymptotic analysis, we will start optimizing
    our algorithm. However, since we have already seen that our program does not work
    properly for even moderately large values of power, let's first fix that. There
    are two ways of fixing this; one is to actually give the amount of space it requires
    to store all the intermediate products, and the other is to do a trick to limit
    all the intermediate steps to be within the range of values that the `long` datatype
    can support. We will use binomial theorem to do this part.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了所有渐近分析的工具箱，我们将开始优化我们的算法。然而，既然我们已经看到我们的程序对于即使是适度的大的幂值也无法正常工作，那么让我们首先解决这个问题。有两种方法可以解决这个问题；一种是为存储所有中间产品所需的空间量实际给出一个数值，另一种是通过一个技巧将所有中间步骤限制在`long`数据类型可以支持的值范围内。我们将使用二项式定理来完成这部分工作。
- en: As a reminder, binomial theorem says *(x+y)^n* *= x^n* *+ ^n* *C[1]* *x^(n-1)*
    *y + ^n* *C[2]* *x^(n-2)* *y²* *+ ^n* *C[3]* *x^(n-3)* *y³* *+ ^n* *C[4]* *x^(n-4)*
    *y⁴* *+ … ^n* *C[n-1]* *x¹* *y^(n-1)* *+ y^n* for positive integral values of
    *n*. The important point here is that all the coefficients are integers. Suppose,
    *r* is the remainder when we divide a by *b*. This makes *a = kb + r* true for
    some positive integer *k*. This means *r = a-kb*, and *r^n* *= (a-kb)^n*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，二项式定理说，对于正整数*n*，有*(x+y)^n* = *x^n* + ^n* *C[1]* *x^(n-1)* *y + ^n* *C[2]*
    *x^(n-2)* *y²* + ^n* *C[3]* *x^(n-3)* *y³* + ^n* *C[4]* *x^(n-4)* *y⁴* + … ^n*
    *C[n-1]* *x¹* *y^(n-1)* + *y^n*。这里的重要点是所有系数都是整数。假设*r*是除以*b*后a的余数。这使得*a = kb +
    r*对于某个正整数*k*成立。这意味着*r = a-kb*，并且*r^n* = (a-kb)^n*。
- en: If we expand this using binomial theorem, we have *r^n* *= a^n* *- ^n* *C[1]*
    *a^(n-1)* *.kb + ^n* *C[2]* *a^(n-2)* *.(kb)²* *- ^n* *C[3]* *a^(n-3)* *.(kb)³*
    *+ ^n* *C[4]* *a^(n-4)* *.(kb)⁴* *+ … ^n* *C[n-1]* *a¹* *.(kb)^(n-1)* *± (kb)^n*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用二项式定理展开这个式子，我们得到*r^n* = *a^n* - ^n* *C[1]* *a^(n-1)* *.kb + ^n* *C[2]*
    *a^(n-2)* *.(kb)² - ^n* *C[3]* *a^(n-3)* *.(kb)³ + ^n* *C[4]* *a^(n-4)* *.(kb)⁴
    + … ^n* *C[n-1]* *a¹* *.(kb)^(n-1)* ± (kb)^n*。
- en: Note that apart from the first term, all other terms have *b* as a factor. Which
    means that we can write *r^n* *= a^n* *+ bM* for some integer *M*. If we divide
    both sides by *b* now and take the remainder, we have *r^n* *% b = a^n* *% b*,
    where *%* is the Java operator for finding the remainder.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了第一项之外，所有其他项都有*b*作为因子。这意味着我们可以写出*r^n* = *a^n* + *bM*，其中*M*是一个整数。如果我们现在将等式两边都除以*b*并取余数，我们得到*r^n*
    % *b* = *a^n* % *b*，其中*%*是Java中用于找到余数的运算符。
- en: 'The idea now would be to take the remainder by the divisor every time we raise
    the power. This way, we will never have to store more than the range of the remainder:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的想法是在每次求幂时都取除数的余数。这样，我们永远不需要存储比余数范围更大的值：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This program obviously does not change the time complexity of the program; it
    just fixes the problem with large powers. The program also maintains a constant
    space complexity.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序显然不会改变程序的时间复杂度；它只是解决了大幂值的问题。程序还保持常数空间复杂度。
- en: Improving time complexity
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高时间复杂度
- en: The current running time complexity is *O(2^x* *)*, where *x* is the size of
    the input as we have already computed. Can we do better than this? Let's see.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当前运行时间复杂度是*O(2^x*)*，其中*x*是我们已经计算过的输入大小。我们能做得比这更好吗？让我们看看。
- en: What we need to compute is *(base^(power)* *) % divisor*. This is, of course,
    the same as *(base²)^(power/2)* *% divisor*. If we have an even *power*, we have
    reduced the number of operations by half. If we can keep doing this, we can raise
    the *power* of *base* by *2^n* in just *n* steps, which means our loop only has
    to run *lg(power)* times, and hence, the complexity is *O(lg(2^x* *)) = O(x)*,
    where *x* is the number of bits to store *power*. This is a substantial reduction
    in the number of steps to compute the value for large powers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要计算的是*(base^(power) *) % divisor*。这当然等同于*(base²)^(power/2) * % divisor*。如果我们有一个偶数*power*，我们就将操作次数减半。如果我们能一直这样做，我们就可以在*n*步内将*base*的*power*提高到*2^n*，这意味着我们的循环只需要运行*lg(power)*次，因此，复杂度是*O(lg(2^x*
    *)) = O(x)*，其中*x*是存储*power*所需的位数。这对于计算大幂值的步骤数量是一个实质性的减少。
- en: 'However, there is a catch. What happens if the *power* is not divisible by
    *2*? Well, then we can write *(base^(power)* *)% divisor = (base ((base^(power-1)*
    *))%divisor = (base ((base²)^(power-1)* *)%divisor*, and *power-1* is, of course,
    even and the computation can proceed. We will write up this code in a program.
    The idea is to start from the most significant bit and move towards less and less
    significant bits. If a bit with *1* has *n* bits after it, it represents multiplying
    the result by the base and then squaring *n* times after this bit. We accumulate
    this squaring by squaring for the subsequent steps. If we find a zero, we keep
    squaring for the sake of accumulating squaring for the earlier bits:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个陷阱。如果`power`不能被`2`整除会发生什么？嗯，那么我们可以写成`(base^(power) % divisor) = (base
    ((base^(power-1) % divisor) = (base ((base²)^(power-1) % divisor*)，其中`power-1`当然是偶数，计算可以继续。我们将把这个代码写在一个程序中。想法是从最高有效位开始，逐渐向不那么重要的比特移动。如果一个带有`1`的比特后面有`n`个比特，它表示将结果乘以基数，然后在此比特之后平方`n`次。我们通过平方后续步骤来累积这个平方。如果我们找到一个零，我们为了累积之前比特的平方而继续平方：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'First reverse the bits of our `power` so that it is easier to access them from
    the least important side, which is more easily accessible. We also count the number
    of bits for later use:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先反转我们的`power`比特，使其更容易从最不重要的侧面访问，这更容易访问。我们还计算比特的数量以供以后使用：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we extract one bit at a time. Since we have already reversed the order
    of bit, the first one we get is the most significant one. Just to get an intuition
    on the order, the first bit we collect will eventually be squared the maximum
    number of times and hence will act like the most significant bit:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们一次提取一个比特。由于我们已经反转了比特的顺序，所以我们首先得到的是最高有效位。为了对顺序有一个直观的认识，我们收集的第一个比特最终会被平方最多的次数，因此它将像最高有效位一样起作用：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We test the performance of the algorithm; we compare the time taken for the
    same computation with the earlier and final algorithms with the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试算法的性能；我们通过以下代码比较相同计算与早期和最终算法所需的时间：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first algorithm takes 130,190 milliseconds to complete all 1,000 times execution
    on my computer and the second one takes just 2 milliseconds to do the same. This
    clearly shows the tremendous gain in performance for a large power like 10 million.
    The algorithm for squaring the term repeatedly to achieve exponentiation like
    we did is called... well, exponentiation by squaring. This example should be able
    to motivate you to study algorithms for the sheer obvious advantage it can give
    in improving the performance of computer programs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个算法在我的电脑上完成所有1,000次执行需要130,190毫秒，而第二个算法只需2毫秒就能完成同样的任务。这清楚地显示了对于像1000万这样的大幂次，性能的巨大提升。我们反复平方项以实现像我们那样进行指数运算的算法被称为...嗯，平方乘法。这个例子应该能够激励你研究算法，因为它可以明显地提高计算机程序的性能。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you saw how we can think about measuring the running time of
    and the memory required by an algorithm in seconds and bytes, respectively. Since
    this depends on the particular implementation, the programming platform, and the
    hardware, we need a notion of talking about running time in an abstract way. Asymptotic
    complexity is a measure of the growth of a function when the input is very large.
    We can use it to abstract our discussion on running time. This is not to say that
    a programmer should not spend any time to make a run a program twice as fast,
    but that comes only after the program is already running at the minimum asymptotic
    complexity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你看到了我们如何分别以秒和字节为单位考虑算法的运行时间和所需的内存。由于这取决于特定的实现、编程平台和硬件，我们需要一个关于以抽象方式讨论运行时间的概念。渐进复杂度是当输入非常大时函数增长的度量。我们可以用它来抽象我们对运行时间的讨论。这并不是说程序员不应该花时间使程序运行速度提高一倍，但这只有在程序已经以最小渐进复杂度运行之后才会发生。
- en: We also saw that the asymptotic complexity is not just a property of the problem
    at hand that we are trying to solve, but also a property of the particular way
    we are solving it, that is, the particular algorithm we are using. We also saw
    that two programs solving the same problem while running different algorithms
    with different asymptotic complexities can perform vastly differently for large
    inputs. This should be enough motivation to study algorithms explicitly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，渐近复杂度不仅是我们试图解决的特定问题的属性，而且是我们解决它的特定方式（即我们使用的特定算法）的属性。我们还看到，两个解决相同问题但在运行时使用不同算法和不同渐近复杂度的程序，对于大输入可以执行得非常不同。这应该足以激发我们专门研究算法的动机。
- en: In the following chapters, we will study the most used algorithmic tricks and
    concepts required in daily use. We will start from the very easy ones that are
    also the building blocks for the more advanced techniques. This book is, of course,
    by no means comprehensive; the objective is to provide enough background to make
    you comfortable with the basic concepts and then you can read on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将研究在日常生活中最常用的算法技巧和概念。我们将从非常简单的那些开始，这些也是更高级技术的基础。当然，这本书绝不是全面的；目标是提供足够的背景知识，让你对基本概念感到舒适，然后你可以继续阅读。
