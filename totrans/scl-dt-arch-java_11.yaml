- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring Performance and Benchmarking Your Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we learned how to architect a solution for data ingestion
    and data publishing problems. We also discussed how we can choose the correct
    technology stack and platform to implement a cost-effective and scalable solution.
    Apart from these, we learned about various architectural patterns for data ingestion.
    We also discussed data governance and data security. However, as an architect,
    our job is not only to create a scalable solution but a high-performing one. This
    is where the role of performance engineering comes into a data architect’s toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the meaning of performance engineering and
    why is it so important. We will also learn how is it different from performance
    testing. Then, we will learn how to plan our performance tests and other performance
    engineering activities. Then, we will briefly discuss performance benchmarking
    techniques. Finally, we will learn about the common methodologies to fine-tune
    the performance of our solution to mitigate or avoid various kinds of performance
    bottlenecks during data ingestion or data publishing.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know what performance engineering is and
    how to plan for it. You will know how to benchmark and publish performance results.
    You will know what the available performance tools are and when to use them. Finally,
    you will know how to fine-tune performance to create an optimized, highly performant
    solution for the data problem, as well as how to perform performance benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance engineering and planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools for performance engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing performance benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance engineering and planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Software performance engineering** (**SPE**) is a systematic and quantitative
    software-based approach to designing, architecting, and implementing solutions
    optimally to meet various **non-functional requirements** (**NFRs**) such as performance,
    capacity, scalability, availability, and reliability. Earlier in this book, we
    dealt with scalability, availability, and reliability. In this chapter, we will
    focus on performance and capacity. Alternatively, SPE is defined as a proactive
    and continuous process of performance testing and monitoring. It involves different
    stakeholders such as testers, developers, performance engineers, business analysts,
    and architects. As we will discuss later in this chapter, performance engineering
    is a seamless process that runs in parallel with development activities, providing
    a continuous feedback loop to the developers and architects so that performance
    requirements are imbibed while the software is developed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve defined performance engineering, let’s discuss the phases of
    a performance engineering life cycle and how it progresses alongside the **software
    development life cycle** (**SDLC**) activities. The following diagram shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Performance engineering life cycle ](img/B17084_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Performance engineering life cycle
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the following are the various stages of the performance engineering
    life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NFR gathering**: To develop a high-performant data pipeline, it is important
    to understand the NFRs of the solution. For example, while designing a DaaS, it
    makes more sense to know what the required **transactions per second** (**TPS**)
    are and what the average parallel load to the system would be. There may be another
    requirement while designing the pipeline, which is that it should be able to be
    integrated with Datadog for monitoring purposes. In such a scenario, a technology
    stack should be chosen so that Datadog integration is supported. To gather all
    this information, we need a close connection between product owners, system analysts,
    **subject matter experts** (**SMEs**), architects, the SPE team, and DevOps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design for performance and performance modeling**: In the waterfall model,
    performance testing and optimization are done after the functional and integrational
    test cycles are over. The problem with this approach is that sometimes, architectures
    that work beautifully with small datasets may not work at all in terms of load
    testing. Due to this, we have to re-engineer the solution again. This causes a
    lot of waste in terms of effort, time, and money. As modern data engineering teams
    have increasingly adopted agile methodologies, the opportunity has increased for
    simultaneously adopting performance engineering. After the nonfunctional requirements
    have been gathered, designing for performance requires that the following criteria
    be fulfilled:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Satisfy the NFRs with optimal speed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution must be scalable enough to have similar performance, even with
    increased load
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our architecture should be designed to scale as business data increases exponentially.
    This brings to life ideas such as design to fail, scaling out rather than scaling
    up the application, and auto-scaling resources in the cloud. Another method that’s
    commonly applied during performance-oriented design is **performance modeling**.
  prefs: []
  type: TYPE_NORMAL
- en: Performance modeling is the process of modeling application performance based
    on features involved in the growth rate of data to find out probable breaches
    of SLA. It also helps validate design decisions and infrastructure decisions.
    Let’s look at an example – suppose input messages are coming in an application
    at a rate of *x*, and the service rate of the application is *y*. What happens
    if the arrival rate of messages quadruples itself? How do we ensure the same response
    time? Do we need to quadruple the service rate or double the service rate? This
    kind of decision can be made by doing performance modeling. In modern data architectures
    where applications tend to run on containerized or virtualized platforms, this
    can determine how to allocate resources to scale in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Modular performance tests and optimization**: In this phase, using NFRs,
    **non-functional tests** (**NFTs**) are classified as stress tests, load tests,
    or soak tests. Once the test cases have been classified, a test case document
    that maps NFTs with detailed steps to run the test scenario is prepared. Optionally,
    an NFT to NFR performance matrix is created. Then, using these documents, NFRs
    can be tested as a module gets developed and functionally tested. Running these
    test cases along with functional testing ensures early detection of any performance
    glitches. Optimization and performance tuning can be done as required by the development
    or DevOps teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully integrated performance test**: Once the modular performance test is
    done, end-to-end performance tests can be run. As the **quality assurance** (**QA**)
    of a scenario finishes, that scenario moves to be tested for integration performance.
    Usually, in this layer, not much tuning or optimization is required. However,
    optimization of the overall end-to-end flow may be required during this activity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and capacity management**: Once the pipeline is in production,
    we need to continuously monitor and evaluate any unusual activities. Based on
    the future and current state of the workloads, capacity can be predicted and managed
    properly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned the various stages of the performance engineering
    life cycle. Next, let’s understand the differences between performance engineering
    and performance testing.
  prefs: []
  type: TYPE_NORMAL
- en: Performance engineering versus performance testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The key differences between performance engineering and performance testing
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing is a QA activity that runs test cases to check the quality
    of NFRs and find any issues. It is performed to check how the system will behave
    in terms of production load and anticipates any issues that could come up during
    heavy loads. On the other hand, performance engineering is a holistic process
    that runs hand-in-hand with SDLC. Unlike performance testing, performance engineering
    starts as early as the analysis phase. It also facilitates the discovery of performance
    issues early in the development life cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance testing follows the waterfall model of the software development
    process. It is only done when software development and functional testing have
    been completed. The problem with such an approach is that if the application fails
    to perform with production loads, we may need to redesign and re-implement, causing
    unnecessary time and financial loss. However, performance engineering is a continuous
    process that goes hand-in-hand with all phases of SDLC and is usually implemented
    by agile teams with a continuous feedback loop to the development and design team.
    By providing early analysis of performance needs and early discovery of issues,
    performance engineering helps us save time and money.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance testing is conducted by the QA team, whereas performance engineering
    involves architects, developers, SMEs, performance engineers, and QA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned about performance engineering, why is it needed,
    and its life cycle. We also discussed the difference between performance testing
    and performance engineering. In the next section, we will briefly discuss the
    performance engineering tools available in the market.
  prefs: []
  type: TYPE_NORMAL
- en: Tools for performance engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will briefly discuss various performance engineering tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the different categories of performance engineering tools
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observability tools**: These tools monitor and gather information about the
    application. These tools potentially help to identify bottlenecks, track throughput
    and latency, memory usage, and so on. In data engineering, each system is different,
    and the throughput and latency requirements are also different. Observability
    tools help identify if our application is lagging in terms of throughput or latency
    and by how much. They also help identify hidden issues that may only show up in
    the long run, in production. For example, a small memory leak in the application
    may not be noticeable within a few days of deployment. When such an application
    keeps on running, the tenured region of JVM heap space keeps slowly increasing
    until it overruns the heap space. The following are a few examples of observability
    tools:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datadog**: This is a very popular monitoring tool that can do application
    monitoring, network monitoring, database monitoring, container monitoring, serverless
    monitoring, and more. It has an inbuilt dashboard and capabilities to customize
    your dashboard according to your needs. It has alerting, log integration, and
    other cool features. It is a paid product, which provides enterprise support.
    For more information, please visit [https://www.datadoghq.com/](https://www.datadoghq.com/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grafana with Graphite/Prometheus**: This is an open source monitoring and
    dashboarding tool. It is either used with Prometheus or Graphite. Both Prometheus
    and Graphite are open source monitoring toolkits that help generate and publish
    various metrics. Prometheus has a data collector module that can pull data to
    generate metrics. On the other hand, Graphite can only passively listen for data
    but can’t collect it. Some other tool, such as Collectd, needs to collect and
    push data to Graphite. To query Graphite metrics, functions are used, whereas
    PromQL is used to query Prometheus metrics. These generated metrics are integrated
    with Grafana to create different kinds of dashboards, such as stats dashboards,
    time series monitoring, status timeline and history, alerting dashboards, and
    so on. For more information, please visit [https://grafana.com/](https://grafana.com/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynatrace**: Dynatrace is another commercial monitoring and dashboarding
    tool that has very similar features to Datadog. It also provides an AI assistant
    to help answer your queries dynamically. It supports DevOps and CloudOps integrations
    such as CI/CD pipelines and so on. For more information, please visit [https://www.dynatrace.com/](https://www.dynatrace.com/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confluent Control Center**: This is an inbuilt confluent Kafka monitoring
    tool that is shipped with an Enterprise (Licensed) version of Confluent Kafka.
    It helps monitor various Kafka components such as topics, producers, consumers,
    Kafka Connect clusters, KSQL queries, as well as overall Kafka cluster health.
    For more information, please visit [https://docs.confluent.io/platform/current/control-center/index.xhtml](https://docs.confluent.io/platform/current/control-center/index.xhtml).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lenses**: This is a tool that provides observability of Kafka topics, clusters,
    and streams. Lenses not only supports observability but also DataOps for Kafka
    clusters. For more information, please visit [https://docs.lenses.io/](https://docs.lenses.io/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance testing and benchmarking tools**: These tools are used to do
    all kinds of performance tests, such as smoke tests, load tests, and stress tests.
    Some of them also provide benchmarking features. The following are a few of the
    tools that can be used for performance testing and benchmarking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JMeter**: This is a free open source tool written in Java that does performance
    tests. It is especially helpful for big data performance testing and any performance
    testing of APIs, such as REST and GraphQL. JMeter Hadoop plugins are available
    to do big data performance testing. We can run the load tests and export the result
    to a file in multiple formats. For more information, please visit [https://jmeter.apache.org/](https://jmeter.apache.org/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SoapUI**: This is another open source performance test tool that is used
    for functional testing as well. It supports load testing with multiple users,
    threads, and parallelism for web services such as REST, SOAP, and GraphQL. It
    has a professional commercial edition as well called ReadyAPI. The commercial
    edition supports more advanced features and specific plugins for testing GraphQL
    and Kafka streaming applications. For more information, please visit [https://www.soapui.org/](https://www.soapui.org/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blazemeter**: This is another open source performance test tool that’s used
    to run scalable tests for microservices such as REST or GraphQL APIs. It supports
    a few monitoring functionalities as well. For more information, please visit [https://www.blazemeter.com/](https://www.blazemeter.com/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LoadRunner**: LoadRunner is a commercial product from Microfocus that enables
    load testing for various workloads and various kinds of applications. It supports
    testing of over 50 types of applications such as microservices, HTML, MQTT, Oracle,
    and so on. For more information, please visit [https://www.microfocus.com/en-us/products/loadrunner-professional/overview](https://www.microfocus.com/en-us/products/loadrunner-professional/overview).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SandStorm**: This is a commercial benchmarking and enterprise-level performance
    testing tool. It has huge support for various applications and tools, from JDBC
    connections to big data testing. It supports NoSQL databases such as Cassandra,
    HBase, and MongoDB, as well as other big data components such as Hadoop, Elasticsearch,
    and Solar. It also provides support for messaging platforms such as Kafka and
    RabbitMQ. For more information, please visit [http://www.sandstormsolution.com/](http://www.sandstormsolution.com/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kafka-producer-perf-test.sh` and `kafka-consumer-perf-test.sh`. While the
    former script is used to test producer performance, the latter is used to test
    consumer performance. To learn more about this feature, go to [https://docs.cloudera.com/runtime/7.2.10/kafka-managing/topics/kafka-manage-cli-perf-test.xhtml](https://docs.cloudera.com/runtime/7.2.10/kafka-managing/topics/kafka-manage-cli-perf-test.xhtml).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenMessaging Benchmark Framework**: This is a set of tools that allows you
    to benchmark distributed messaging systems over the cloud easily. It supports
    multiple message platforms such as Apache Kafka, Apache Pulsar, Apache RocketMQ,
    and so on. For more information, please visit [https://openmessaging.cloud/docs/benchmarks/](https://openmessaging.cloud/docs/benchmarks/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we briefly discussed multiple tools that can be used for performance
    engineering. Now that we have a fair idea of what performance engineering is,
    how to do it, and the tools we can use for it, let’s look at how we can create
    performance benchmarks using the knowledge that we have gained.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing performance benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about performance benchmarking and how to develop
    and publish them. We will start by defining what a performance benchmark is. A
    benchmark in software performance testing is defined as a point of reference against
    which the quality measures of a software solution can be assessed. It can be used
    to do a comparative study of different solutions to the same problem or compare
    software products.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks are like stats or metrics to determine the quality of software. Just
    like in sports such as soccer, each player’s worth or quality is determined by
    various stats such as their overall number of goals scored, number of goals scored
    per match, number of goals scored tournament-wise, and so on. These stats help
    compare different players under different specifications. Similarly, benchmarks
    in the software world help determine the worth of a software product or solution
    under specific conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s practically run some performance tests and create a performance benchmark.
    We will use the REST API that we developed in [*Chapter 9*](B17084_09.xhtml#_idTextAnchor144),
    *Exposing MongoDB as a Service*, to do the performance testing. We will use JMeter
    to test and record the performance of the application. We chose JMeter since it
    is easy to use and is an open source product based on Java.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to do a performance test and benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Add a thread group**:  First, we must add a thread group. To add a thread
    group, we need to do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the JMeter tool.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From the tree in the left pane, select **Test Plan**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Right-click on **Test Plan**. Then, click **Add** | **Threads (Users)** | **Thread
    Group** to add a thread group, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure-11.2 – Adding a thread group ](img/B17084_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure-11.2 – Adding a thread group
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Thread Group** creation wizard, fill in the thread group’s name and
    enter the thread’s properties, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The Thread Group creation wizard ](img/B17084_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – The Thread Group creation wizard
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, **Number of Threads (users)**, **Loop Count**, and **Ramp-up
    period (seconds)** have been configured. Let’s try to understand these terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of Threads (users)** corresponds to the number of users making the
    request simultaneously, as shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Number of threads versus loop count ](img/B17084_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Number of threads versus loop count
  prefs: []
  type: TYPE_NORMAL
- en: '`5`. So, a user makes 5 requests to the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`50` and `50`. Hence, there is a delay of 1 second before it starts the next
    user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have configured our thread group, let’s try adding the JMeter elements.
  prefs: []
  type: TYPE_NORMAL
- en: '**Configure the JMeter elements**: Now, we will add a JMeter config element
    known as an HTTP Request. This helps make REST or web service calls for load tests.
    Right-click **REST Thread group** and select **Add** | **Sampler** | **HTTP Request**,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Adding a sampler ](img/B17084_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Adding a sampler
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up `http`, `8080` (if running on a local machine), `GET`, and our **Path**,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Configuring the HTTP Request sampler ](img/B17084_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Configuring the HTTP Request sampler
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have configured the HTTP Request sampler, we need to add a few JMeter
    elements that can monitor and publish performance reports. For this, we must configure
    one or more listeners.
  prefs: []
  type: TYPE_NORMAL
- en: '**Add a listener**: To create performance benchmarks, we must add three listeners,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Summary Report**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Aggregate Report**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Response Time Graph**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps to add a listener are similar to configuring any new listener. Here,
    we will demonstrate how to add an aggregated performance benchmark, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click **REST Thread Group** and then select **Add** | **Listener** |
    **Aggregate Report**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Adding a listener ](img/B17084_11_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Adding a listener
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename the report to `Aggregate Performance Benchmark` in the configuration
    wizard of **Aggregate Report**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Configuring the Aggregate Report listener ](img/B17084_11_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Configuring the Aggregate Report listener
  prefs: []
  type: TYPE_NORMAL
- en: Follow similar steps to set up the **Summary Report** and **Response Time Graph**
    listeners. By doing so, we will be set to run the test and generate the report.
  prefs: []
  type: TYPE_NORMAL
- en: '**Run load tests and create a benchmark**: Run the tests by clicking the start
    symbol that’s encircled in red in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Running performance tests by clicking the Run button ](img/B17084_11_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Running performance tests by clicking the Run button
  prefs: []
  type: TYPE_NORMAL
- en: 'On successfully executing the performance tests, the following reports are
    generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary Report**: This report provides a summary of the performance benchmark
    and shows the average, minimum, and maximum response time of the request. You
    can also see the average throughput of the application. The following screenshot
    shows a summary of the benchmark results:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Generated summary benchmark report ](img/B17084_11_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Generated summary benchmark report
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the **# Samples** column in the preceding screenshot; its value is **250**.
    The value of the samples is derived using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_11.1.jpg)![](img/Formula_11.2.jpg)![](img/Formula_11.3.jpg)![](img/Formula_11.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Aggregate Report**: This denotes the aggregated benchmark report. Apart from
    showing the average, median, and maximum response time, it has columns such as
    **90% Line** and **95% Line**. A **90% Line** column denotes the average response
    time of 90% of the request. It assumes that 10% of the request contains outliers.
    Similarly, a **95% Line** assumes that 5% of requests are outliers. The following
    screenshot shows the aggregated performance benchmark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Generated aggregated benchmark report ](img/B17084_11_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Generated aggregated benchmark report
  prefs: []
  type: TYPE_NORMAL
- en: '**Response Time Graph**: A performance benchmark can contain multiple charts
    or tables to benchmark the application performance. A Response Time Graph depicts
    the recorded response time at different timelines:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Generated Response Time Graph ](img/B17084_11_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Generated Response Time Graph
  prefs: []
  type: TYPE_NORMAL
- en: In performance benchmarking activities, a lot of the time, we do comparative
    studies where these reports are used to create a combined report or graph visualization
    comparing the performance benchmarks of two different solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned why benchmarking is needed and what needs to be
    considered while benchmarking our solutions. Benchmarking provides us with a way
    to categorize or classify our application performance as good, bad, or just fine.
    In the next section, we will find out how can we improve our performance and optimize
    our data engineering solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main reasons that benchmarking, performance testing, and monitoring
    of applications and systems are done is because of a goal – to optimize the performance
    so that the system can work to its best potential. The difference between extraordinary
    software and ordinary software is determined by how well the system is tuned for
    better performance. In this section, we will learn about various techniques you
    can use to fine-tune your data engineering pipeline. Although performance tuning
    is a vast topic, when it comes to various data engineering solutions, we will
    try to cover the basics of optimizing Java-based data engineering solutions. In
    the following subsection, we will briefly look at various performance tuning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Java Virtual Machine and garbage collection optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Java Virtual Machine** (**JVM**) performance tuning is the process of adjusting
    the various JVM arguments or parameters to suit the need of our application so
    that it performs the best it can.'
  prefs: []
  type: TYPE_NORMAL
- en: 'JVM tuning involves two kinds of optimization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Heap space optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Garbage collection** (**GC**) optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But before we talk about these optimizations, it is worth noting that JVM tuning
    needs to be the last resort to tune the performance of an application. We should
    start by tuning application code bases, databases, and resource availability.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the JVM heap space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we deep dive into JVM and GC tuning, let’s spend some time understanding
    the JVM heap space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows what a JVM heap space looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – JVM heap space ](img/B17084_11_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – JVM heap space
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the JVM heap space is divided into four compartments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Meta or Perm Space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eden Space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Survivor Space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tenured Space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Meta Space (known as Perm Space for older JDK versions) stores the metadata
    of the heap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Java objects get promoted from Eden Space to Tenured Space, based on the tenure
    for which they are alive. The following steps show how the Java objects get promoted:'
  prefs: []
  type: TYPE_NORMAL
- en: The newly created objects from the Java application get stored in *Eden Space*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When *Eden Space* is full, a minor GC event occurs and the objects that are
    still referenced by the Java application are promoted to *Survivor Space 0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, in the next cycle, when *Eden Space* is full, a second minor GC event
    gets triggered. At first, this moves all the objects that are still referenced
    by the application from *Survivor Space 0* to *Survivor Space 1*, and then it
    promotes referenced objects from *Eden Space* to *Survivor Space 0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A major GC event occurs when referenced objects are promoted from *Survivor
    Space 1* to *Tenured Space*. Objects that get promoted to tenured space are called
    old-generation objects. *Eden Space*, *Survivor Space 0*, and *Survivor Space
    1* objects are young-generation objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Earlier, we discussed how minor and major GC happens to free up the heap space.
    But is there a single way or multiple ways to do GC? If so, what should we choose
    and when? We’ll explore this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Garbage collection** is a process that automatically determines what memory
    in the JVM is no longer being used by a Java application and recycles that memory
    for other usages.'
  prefs: []
  type: TYPE_NORMAL
- en: Types of garbage collector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the different types of GCs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serial garbage collector**: A single GC is suitable for single-threaded applications.
    It freezes all application threads while doing garbage collection and does so
    using a single thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel garbage collector**: A parallel GC also freezes all threads from
    the application but uses multiple threads to do garbage collection. Hence, the
    pause interval of the application threads reduces considerably. It is designed
    to work for multi-processor environments or multi-threaded environments with medium
    and large data sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent Mark Sweep (CMS) garbage collector**: As evident from the name,
    the garbage collection job is performed concurrent to the application. So, it
    doesn’t require application threads to pause. Instead, it shares threads with
    the application thread for concurrent sweep execution. However, it needs to pause
    application threads shortly for an **initial mark pause**, where it marks the
    live objects initially. Then, a second pause, called a **remark pause**, suspends
    application threads and is used to find any Java objects that need to be collected.
    These Java objects are created during the concurrent tracing phase. The following
    diagram explains the difference between serial, parallel, and CMS GCs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Difference between the single, parallel, and CMS garbage collectors
    ](img/B17084_11_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Difference between the single, parallel, and CMS garbage collectors
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the serial collector uses a single thread for garbage collection,
    while pausing all application threads. The parallel collector pauses the application
    threads but since it uses multiple threads to do its job, the pause time is less.
    CMS, on the other hand, runs concurrently along with the application threads after
    the initial mark phase.
  prefs: []
  type: TYPE_NORMAL
- en: '**G1 garbage collector**: This is a relatively new GC introduced in Java 7
    and later. It depends on a new algorithm for concurrent garbage collection. It
    runs its longer job alongside the application threads and quicker jobs by pausing
    the threads. It works using the evacuation style of memory cleaning. For the evacuation
    style of memory cleaning, the G1 collector divides the heap into regions. Each
    region is a small, independent heap that can be dynamically assigned to Eden,
    Survivor, or Tenured Space. The following diagram shows how the G1 collector sees
    the data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.15 – G1 garbage collector divides heap space into regions ](img/B17084_11_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – G1 garbage collector divides heap space into regions
  prefs: []
  type: TYPE_NORMAL
- en: The GC simply copies data from one region to another. This needs to be retained
    and marks the older region as blank.
  prefs: []
  type: TYPE_NORMAL
- en: '**Z garbage collector**: This is an experimental GC for very scalable low latency
    implementations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following points should be kept in mind regarding GC:'
  prefs: []
  type: TYPE_NORMAL
- en: Minor GC events should collect as many dead objects as possible to reduce the
    frequency of full GC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More efficient object cleanup is possible when more memory is available for
    a GC event. More efficient object cleanup ensures a lower frequency of full GC
    events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of performance tuning using GC, you can only tune two parameters
    out of the three – that is, throughput, latency, and memory usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s see how to tune the performance using a GC.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning performance using a GC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Performance tuning using GC settings is also known as GC tuning. We must follow
    these steps to perform GC tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Investigate the memory footprint**: One of the most commonly used methods
    to look for any performance issue caused due to GC can be found in the memory
    footprint and is present in the GC logs. GC logs can be enabled and generated
    from a Java application without affecting performance. So, it is a popular tool
    to investigate performance issues in production. You can enable GC logs using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot shows what a typical GC log looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure – 11.16 – Sample GC log ](img/B17084_11_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure – 11.16 – Sample GC log
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, each line shows various GC information. Let’s
    focus on the third line of the log, which shows a full GC event:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure – 11.17 – Anatomy of a GC log statement ](img/B17084_11_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure – 11.17 – Anatomy of a GC log statement
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, let’s pick apart the GC log statement and
    understand the various parts of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`2022-07-01T16:46:0.434`: The timestamp when the GC event occurred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Full GC`: This field describes the type of GC. It can either be a full GC
    or GC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[PSYoungGen: 10752K->0K(141824K)]`: After the GC event occurred, all the space
    used in the young generation was recycled. Also, the value inside brackets (`141824K`)
    denotes the total allocated space in the young generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[ParOldGen: 213644K->215361K(459264K)]`: After the GC ran, the old generation’s
    used space increased from `213644K` to `215361K`. Total allocated memory for the
    old generation is `459264K`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`224396K->215361K(601088K)`: After the GC ran, the total memory of used space
    was reduced from `224396K` to `215361K`. The total allocated memory is `601088K`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[Metaspace: 2649K->2649K(1056768K)]`: No memory was reclaimed from the Meta
    space as a result of the GC. The total allocated Meta space is `1056768K`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3.4609247 secs`: Total time taken by GC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[Times: user=3.40 sys=0.02, real=3.46 secs]`: This part of the log statement
    tells us about the time taken to do the garbage collection. The `user` time tells
    the processor time that the GC took to execute. The `sys` time denotes the time
    taken by I/O and other system activities. Finally, the `real` time denotes the
    total time taken to finish the GC event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these footprints, we can determine whether we need to increase the
    heap space or increase the meta space and specifies any memory leaks that are
    happening in the application.
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory tuning**: A memory leak may occur if the following observations are
    listed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The JVM heap size is being filled frequently
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The young generation space is being completely recycled, but the old generation
    used space is increasing with every GC run
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before deciding whether it is a genuine memory leak problem or not, you should
    increase the heap space using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If this doesn’t help, then the root cause is most likely a memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we see that the young generation space is getting filled frequently or that
    the Meta space is being heavily used, we can plan to change the total allocated
    space in all these regions using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-XX:MaxMetaspaceSize`: This sets the maximum amount of memory that can be
    allocated for the class metadata. The default value is `infinite` (or the same
    as the heap space).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:MetaspaceSize`: Sets the threshold size of the allocated class metadata
    above which the GC will be triggered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:MinMetaspaceFreeRatio`: The minimum percentage of the Meta space memory
    region that needs to be available after garbage collection. If the amount of memory
    left is below the threshold, the Meta space region will be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:MaxMetaspaceFreeRatio`: The maximum percentage of the Meta space memory
    region that needs to be available after garbage collection. If the amount of memory
    left is above the threshold, the Meta space region will be resized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:NewSize`: This sets the initial size of the young generation space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XXMaxNewSize`: This specifies the maximum size of young generation space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-Xmn`: This specifies the size of the entire young generation space, meaning
    Eden and the two survivor spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:+G1GC` command.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `-Xmx` and `-Xms` to the same value to reduce application pause intervals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `-XX:+AlwaysPreTouch` flag to `true` so that the memory pages are loaded
    when the application is started.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are using G1, check whether the minor GC or full GC is taking more time.
    If the minor GC is taking more time, we can reduce the values of `-XX:G1NewSizePercent`
    and `-XX:G1MaxNewSizePercent` . If the major GC is taking more time, we can increase
    the value of the `-XX:G1MixedGCCountTarget` flag, which will help spread the tenured
    GC into multiple runs and reduce the frequency of full GC events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--XX:MaxGCPauseMillis` property to clean more garbage in a single GC run.
    However, this may affect your latency.*   Load memory pages into memory at the
    start of the application by setting the `-XX:+AlwaysPreTouch` and `-XX:+UseLargePages`
    flags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency** is the total time elapsed to process and send an event, message,
    or data to its destination. On the other hand, **throughput** is the number of
    records, events, or messages processed within a specified period.'
  prefs: []
  type: TYPE_NORMAL
- en: Although JVM and GC tuning is a vast topic, we briefly tried to cover a few
    important JVM and GC tuning techniques to improve throughput and latency. In the
    next section, we will discuss how can we optimize big data loads.
  prefs: []
  type: TYPE_NORMAL
- en: Big data performance tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance tuning for big data is a huge topic. For brevity, we will limit
    ourselves to a few performance tuning tips and tricks that are usually applied
    to the most popular big data processing technologies, namely Spark and Hive.
  prefs: []
  type: TYPE_NORMAL
- en: Hive performance tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Hive performance tuning** is the collective process and technique to improve
    and accelerate the performance of your Hive environment. The following are some
    commonly faced Hive performance issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Slow-running queries**: Often, you will notice that your Hive query is taking
    a huge amount of time to finish. There can be several reasons for slow-running
    queries. The following are a few commonly encountered scenarios of slow-running
    queries and their solution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poorly written queries result in a cross-join or full outer join. An unintended
    cross-join can happen when the join columns in either of the tables have duplicates
    or a self-join is happening in the query. Fine-tune your Hive queries to avoid
    cross-joins as much as possible.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The speed of a slow-running query can be improved by applying a map-side join
    if one of the join tables contains a small amount of data. In a map-side join,
    the smaller dataset is broadcast to all mapper nodes so that the join happens
    locally without much shuffling. The downside of a map-side join is that the data
    in the smaller table needs to be small enough to fit into memory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For a scenario suitable for a map-side join, we can further improve the speed
    of the tables needed for the join to be bucketed. **Bucketing** is the technique
    by which data in a Hive table is broken into a fixed number of ranges or clusters,
    based on the join column(s). A bucketed table can be used for a **bucket map-join**
    or **sort-merge-bucket (SMB) map-join**, both of which perform better than normal
    map-joins. However, bucketed tables can be joined with each other, only if the
    total buckets of one table are multiples of the number of buckets in the other
    table. For example, Table1 has 2 buckets and Table2 has 4 buckets. Since 4 is
    a multiple of 2, these tables can be joined.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, data grows rapidly, which makes Hive jobs slow. In such cases, map-side
    operations take a huge amount of time. To overcome such issues, use partitioning.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you notice that data read is slow or data shuffle is quite slow, check for
    the *data format* and *compression* that were used. For example, columnar structures
    such as Parquet and ORC have 5% to 10% higher performance than JSON. The columnar
    format also is known to have around a 90% higher compression ratio than JSON.
    More compressed data can reduce a good amount of network latency and hence improve
    overall performance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Hive execution speed can be considerably improved by changing the *execution
    engine* from map-reduce to Tez or Spark. You can do this by setting the following
    parameter in the Hive configuration file:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`TimeoutException`, chances are that you have encountered a *small file issue*.
    This occurs when millions of small files (whose size is less than the block size
    of 128 MB) are written into the Hive table’s external path. Each HDFS file has
    metadata stored in Hadoop’s name-node. Too many small files in a Hive table cause
    too much metadata to be read by the job. Hence, the job fails either due to memory
    overrun or timeout exceptions. In such cases, either run a compaction job (refer
    to the *Core batch processing patterns* section of [*Chapter 7*](B17084_07.xhtml#_idTextAnchor110),
    *Core Architectural Design Patterns*) or store the data in sequential files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we’ve discussed various Hive optimization techniques that are used
    to fine-tune Hive query speeds and performance. Now, let’s look at Spark performance
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Spark performance tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark is the most popular big data processing engine. When Spark applications
    are tuned properly, it lowers the resource cost while maintaining the SLA for
    critical processes. This is important for both cloud and on-premise environments.
  prefs: []
  type: TYPE_NORMAL
- en: A Spark tuning job starts by debugging and observing the problems that occur
    during a Spark job’s execution. You can observe various metrics using the Spark
    UI or any profiling application such as Datadog.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few best practices for Spark optimization that are commonly
    applied while handling Spark applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serialization**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Problem*: Slow data read from a Hive table or HDFS'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cause*: The default Java serializer slows down the read speed of the data
    from Hive or HDFS'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution*: To overcome this problem, we should set the default serializer
    to Kyro Serializer, which performs a much faster Serde operation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition sizes**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Problem*: The Spark job runs slowly since one or two executors among many
    executors take much more time to finish the task. Sometimes, the job hangs due
    to a slow-performing executor.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cause*: A possible cause can be data skew. In such a scenario, you will find
    that the slow-performing executor is processing the bulk of the data. Hence, the
    data is not well-partitioned.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution*: Repartition the data loaded into Spark DataFrames before processing
    the data further.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at another common problem that’s encountered related to partitioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Problem*: The Spark jobs are slowing down as all executors are heavily loaded
    and all executors are taking a long time to process the job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cause*: The likely cause is that the data has outgrown and you are running
    the Spark job with far fewer partitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution*: Repartition the data loaded in the Spark DataFrame and increase
    the number of partitions. Adjust the number of partitions until optimum performance
    is achieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another common problem that’s encountered in Spark is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Problem*: A Spark job needs to write a single file as an output but the job
    gets stuck in the last step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cause*: Since you have used `repartition()` to create a single output file,
    a full data shuffle takes place, choking the performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution*: Use `coalesce()` instead of `repartition()`. `coalesce()` avoids
    doing a full shuffle as it collects data from all other partitions and copies
    data to a selected partition that contains the maximum amount of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OutofMemoryException`, GC overhead memory exceeded, or JVM heap space overrun.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cause*: Inefficient configuration of the driver and executor memory and CPU
    cores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution*: There is no one solution. Here, we will discuss various best practices
    that can be used to avoid inefficient driver and executor resource configuration.
    Before we begin, we will assume that you have basic familiarity with Apache Spark
    and know its basic concepts. If you are new to Apache Spark, you can read this
    concise blog about the basics of the Spark architecture: [https://www.edureka.co/blog/spark-architecture/](https://www.edureka.co/blog/spark-architecture/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For both executor and driver sizing, there is a total of five properties that
    we need to set for Spark job optimization. They are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`5` by using the following property:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/Formula_11.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, Nc denotes the number of cores per node, Ec denotes the executor cores
    per executor, and Tn denotes the total number of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In this use case, *Nc* is 16, *Ec* is 5, and *Tn* is 3\. Hence, we reach the
    same result of 8 (`FLOOR`(16*3/5) -1).
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have discussed how to optimally set the driver and executor properties.
    However, for Spark 3.0 and above running on a YARN cluster, it makes more sense
    to enable dynamic allocation. Although you can set a minimum and a maximum number
    of executor cores, you must let the environment itself determine the number of
    executors needed. Optionally, you may want to set a cap on the maximum number
    of executors (by using the `spark.dynamicAllocation.maxExecutors` property) after
    discussing this with your Hadoop admin.
  prefs: []
  type: TYPE_NORMAL
- en: '**Executor memory or executor-memory**: To calculate the optimal executor memory,
    we need to understand the total amount of memory that’s used by an executor. The
    total memory that’s used by an executor is the total of the executor memory and
    the memory overhead:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Total memory used by executor = memory overhead + executor-memory
    ](img/B17084_11_018.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Total memory used by executor = memory overhead + executor-memory
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory overhead of the executor defaults to a greater amount between 10%
    of the executor’s memory size or 384 MB. Now, to find the correct executor memory
    size, we need to look at the YARN resource manager’s **Nodes** tab. Each record
    in the **Nodes** tab will have a **Total Memory Column**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Total memory available for the executors in a node ](img/B17084_11_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Total memory available for the executors in a node
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, a total of **112 GB** of memory is available from
    the node for executors and drivers, after memory has been set aside for the cluster
    manager. Now, we must calculate the memory overhead for this executor by using
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_11.6.jpg)![](img/Formula_11.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s try to use the example described earlier for calculating the executor
    memory. We will divide this available node memory by the total number of executors
    per node. Then, we will divide this by 1.1\. The formula to calculate executor
    memory is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_11.8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Nm* is the total node memory and *Ne* is the number of executors per
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Using the preceding formula in our scenario, the executor memory should be 36.94
    GB (112/3 – .384). So, we can set the executor memory to 36 GB.
  prefs: []
  type: TYPE_NORMAL
- en: '`1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Driver memory or driver-memory**: Driver memory is either less than or equal
    to the executor memory. Based on a specific scenario, this value can be set to
    less than or equal to the executor’s memory. One of the optimizations that is
    advisable in the case of driver memory issues is to make the driver memory the
    same as the executor memory. This can speed up performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Directed acyclic graph (DAG) optimization**: Let’s look at some issues that
    you can resolve using DAG optimization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Problem*: In the DAG, we can see that more than one stage is identical in
    terms of all its tasks or operations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cause*: The DataFrame, denoted as d1, is being used to derive more than one
    DataFrame (for example – d2, d3, and d4). Since Spark is lazily computed, when
    creating each DataFrame (d2, d3, and d4), d1 is recalculated every time.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution*: In such a scenario, we must persist the dependent DataFrame (d1)
    using the `Dataset<T> persist(StorageLevel newLevel)` method.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve discussed performance tuning for big data, let’s learn how to
    tune real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing streaming applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s learn how to optimize streaming applications. Here, our discussion
    will mainly focus on Kafka since we discussed this earlier in this book. When
    it comes to streaming applications, we can tune their latency and throughput.
    In this section, we will learn how to observe a performance bottleneck in Kafka.
    Then, we will learn how to optimize producers and consumers. Finally, we will
    discuss a few tips and tricks that help tune overall Kafka cluster performance.
  prefs: []
  type: TYPE_NORMAL
- en: Observing performance bottlenecks in streaming applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing about any tuning is monitoring and finding out what the problem
    is and where it’s occurring. For real-time stream processing applications, this
    is of utmost importance. There are quite a few Kafka observability tools available
    such as Lenses and **Confluent Control Center** (**C3**). Here, we will see how
    C3 helps us observe anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you navigate to any topic in C3, you will see three tabs – **Producer**,
    **Consumer**, and **Consumer lag**. The **Consumer lag** tab can tell if a consumer
    is slow or not. The following screenshot shows that the consumer (group) is lagging
    by **1,654** records while reading data from the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Consumer lag ](img/B17084_11_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Consumer lag
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot also shows the number of active consumers in this
    consumer group. This can indicate whether the consumer is set to utilize the full
    performance potential of the topic or not. To learn more, click on the **Consumer
    group ID** property (in the preceding screenshot, it is **analyticsTeam**). You
    will see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – Partition-wise consumer lag  ](img/B17084_11_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – Partition-wise consumer lag
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the lag of the consumer group partition-wise.
    From *Figure 11.19* and *Figure 11.20*, it is evident that only one consumer is
    running but that the topic has three partitions. As we can see, there is scope
    for tuning the consumer application by increasing the number of consumers in the
    consumer group. Similarly, we can look at the producer speed versus the consumer
    speed of a topic and find any slowness in the producer.
  prefs: []
  type: TYPE_NORMAL
- en: Producer tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are a few common tuning techniques for Kafka producers:'
  prefs: []
  type: TYPE_NORMAL
- en: When the producer sends a message to the Kafka broker, it receives an acknowledgment.
    The `acks=all` property, along with the value of `min.insync.replicas`, determine
    the throughput of a producer. If `acks=all` is set and the acknowledgment takes
    more time to come (because it must write all its replicas before sending the acknowledgment),
    then the producer cannot produce any further messages. This reduces the throughput
    considerably. We must make a choice here between durability and throughput.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Idempotent producers guarantee exactly-once delivery of messages. However,
    this comes with a cost: it reduces the throughput of a producer. So, a system
    must choose between deduplication and throughput. While using idempotent producers,
    you can improve throughput slightly by increasing the value of `max.in.flight.requests.per.connection`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to improve throughput is to increase the property value of `batch.size`.
    Although a producer is used to send the message, it does so asynchronously (for
    most applications). Producers usually have a buffer where producing records are
    batched before they’re sent to the Kafka broker. Sending the records to the broker
    in batches improves the throughput of the producer. However, latency increases
    as the value of `batch.size` increases. Set `batch.size` to a balanced value so
    that we get optimum throughput without affecting the latency considerably.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Linger.ms` is another property that is affected when the messages in the Kafka
    producer buffer are sent to the Kafka broker. The higher the value of `linger.ms`,
    the higher the throughput and latency will be. Again, it must be set in a balanced
    fashion to have optimum throughput as well as latency. For extremely huge loads
    of data, a higher value of `linger.ms` can give a considerable boost to performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we have briefly discussed the techniques for producer optimization.
    Now, let’s find out how we can optimize the performance of Kafka consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are a few tips and tricks you can utilize to optimize the consumer
    to make full use of the potential improvements the consumer can have:'
  prefs: []
  type: TYPE_NORMAL
- en: For a consumer to consume in the most optimized fashion, the total number of
    consumers should be equal to the number of partitions. In a multithreaded Kafka
    consumer, the total number of threads across all the consumers in the consumer
    group should be equal to the number of topic partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another typical scenario that makes the consumer slow is too much rebalancing
    of the consumer. This can happen if the time that’s taken to poll and process
    `max.poll.records` is more than the value of `max.poll.interval.ms`. In such cases,
    you may want to either increase the value of `max.poll.interval.ms` or decrease
    the value of `max.poll.records`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rebalancing can happen in scenarios where a consumer can’t send the heartbeat
    or the heartbeat packages reach slowly due to network latency. If we are okay
    with static consumers (the consumer statically maps to a partition), we can configure
    a unique `group.instance.id` value for each consumer instance in the consumer
    group. This will increase latency for the consumer that goes down but will ensure
    great latency and throughput for other partitions as it will avoid unnecessary
    consumer rebalancing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since `consumer.commitSync()` blocks your thread unless a commit is done successfully,
    it may be slower in most cases compared to `consumer.commitAsync()`. However,
    if there is a fatal error, it makes sense to use `commitSync()` to ensure the
    messages are committed before the consumer application goes down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we mainly focused this discussion on Apache Kafka, other alternative
    products that enable stream processing such as Apache Pulsar and AWS Kinesis have
    similar performance tuning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Database tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Database tuning is an important activity when it comes to performance tuning.
    It includes SQL tuning, data read and write tuning from database tables, database-level
    tuning, and making optimizations while creating data models. Since these all vary
    considerably from one database to another (including SQL and NoSQL databases),
    database tuning is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s summarize what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by understanding what performance engineering is and
    learning about the performance engineering life cycle. We also pointed out the
    differences between performance engineering and performance testing. Then, we
    briefly discussed the various tools that are available to help with performance
    engineering. We learned about the basics of performance benchmarking and what
    to consider while creating a benchmark. Then, we learned about various performance
    optimization techniques and how to apply them to Java applications, big data applications,
    streaming applications, and databases.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have learned how to do performance engineering for both batch-based
    and real-time data engineering problems. In the next chapter, we will learn how
    to evaluate multiple architectural solutions and how to present the recommendations.
  prefs: []
  type: TYPE_NORMAL
