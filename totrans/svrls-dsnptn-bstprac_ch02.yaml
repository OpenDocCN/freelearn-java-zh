- en: A Three-Tier Web Application Using REST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It should be safe to say that the vast majority of developers know what REST
    is. A three-tier web application consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Presentation layer (HTML and CSS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business logic layer (application code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data layer (Relational Database Management System or another type of data store)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The three-tier web application is extremely well known and one of the most common
    designs on the web today. Readers are likely familiar with this design when thinking
    about a web application's static content (that is, HTML, JavaScript, and CSS)
    which are served from a **content delivery network** (**CDN**), which talks to
    a RESTful API hosted on a web server, which, in turn, talks to a database.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through the process of building a three-tier web
    application using HTML, JavaScript, and CSS for our presentation layer, a REST
    API for our business logic, and a Postgres database for our data tier. Most importantly,
    and keeping in line with this book, this will all be accomplished using serverless
    technologies or services where you do not need to manage servers yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you can expect to know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to author a REST API using **Amazon Web Services** (**AWS**) Lambda and
    Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build, manage, and deploy static assets to a CDN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create and maintain an RDS Postgres database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Options for designing RESTful APIs, including different languages, frameworks,
    and layouts of functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static asset life cycle management and caching for optimal performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless tooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since this is the very first chapter that has application code and working
    examples, it''s important to talk through some of the tooling and systems to set
    the stage for this and subsequent chapters. In this and the following chapters
    on web applications, our toolchain will consist of services from AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS API Gateway as the HTTP proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Lambda for computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS S3 for static file serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS CloudFront for the CDN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS RDS for RDBMS management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Certificate Manager** (**ACM**) for free certificate management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While AWS is the dominant player in the Platform as a service (PaaS) ecosystem,
    it is by no means the only choice. While reading this chapter and others in this
    book, remember that the patterns presented should apply to any cloud provider,
    albeit sometimes with a certain degree of adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: You may be questioning the reasoning behind discussing other services such as
    S3 and RDS. Very often, perhaps usually, when people say *serverless* they are
    talking about functions as a service with AWS Lambda, or equivalent services from
    different cloud providers. This question is a valid one, but it's also critical
    to remember that our definition for serverless architectures in this book is complete
    systems where you don't need to manage any operating systems yourself. Your goal
    is not to maintain a single real or virtual server and push the hard work to your
    favorite cloud provider, allowing you to focus on your application. Admittedly,
    not all cloud providers have the vast number of services at our disposal as in
    the AWS ecosystem. Take this into consideration when choosing a PaaS upon which
    to build your serverless applications.
  prefs: []
  type: TYPE_NORMAL
- en: Building a system with a Function as a service (FaaS) backbone for the business
    logic is a step in the right direction; however, if you are still managing a database,
    are you serverless? Managing your own RDBMS or web server for the serving of static
    assets puts you outside of the serverless architecture box, even if your compute
    layer is serverless.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](svrls-dsnptn-bstprac_ch01.html), *Introduction*, I listed a few
    of the popular frameworks that allow us to manage, deploy, and maintain serverless
    applications. All of our sample applications in this book will use the Serverless
    Framework ([https://serverless.com/](https://serverless.com/)), which will help
    us glue various services and systems together, allow us to deploy code, and provide
    much more functionality to make the development process faster and more straightforward. Just
    as with cloud providers, you have multiple choices for frameworks and utilities.
    Covering all of the options is outside the scope of this book and, for simplicity,
    we will stick with the Serverless Framework, which is mature, well used, and frequently
    updated.
  prefs: []
  type: TYPE_NORMAL
- en: From this point on, I will refer to the Serverless Framework when talking about
    the framework itself to differentiate it from the general serverless topic. Due
    to the name *Serverless*, the Serverless Framework can be a bit confusing in the
    context of a book about serverless design patterns. From here on, simply be on
    the lookout for the Serverless Framework if we're discussing details of how to
    deploy, manage, or otherwise control resources on our cloud provider, AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Our example project will be a REST API for professional coffee evaluation, called
    cupping in the coffee realm. Coffee cupping at its core is nothing more than a
    fancy spreadsheet of scores for individual coffees, where scores are applied to
    one or more criteria, such as acidity, body, uniformity, and so on. If you enjoy
    coffee and APIs as I do, you should enjoy this and subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: System architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first glance, a three-tier web application using a REST API can be an easy
    topic and pattern. After all, there are only three layers, which are responsible
    for very discrete tasks, and the final result is just a web application after
    all. However, there are many nuances and areas for tweaking with any web application.
    Serverless web applications are no different. This chapter will attempt to cover
    as many areas as possible, but it's impossible to include every possible configuration
    or design option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seeing as we are responsible software designers, let''s sketch out our architecture
    at a high level and drill down into more detail as we work through the different
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8fc4399-0f77-4b6a-b771-4090fdf6daa5.png)'
  prefs: []
  type: TYPE_IMG
- en: This diagram should look familiar as it's the backbone of many client-server
    web applications out there today. Let's walk through the different layers, going
    from the top down. After discussing these layers at a high level, we'll get into
    the implementation details with a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all of the code in this chapter in the following repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/brianz/serverless-design-patterns/tree/master/ch2](https://github.com/brianz/serverless-design-patterns/tree/master/ch2)'
  prefs: []
  type: TYPE_NORMAL
- en: Even though a common and arguably simple pattern, there are many possible complexities
    when deploying a stack like this on AWS with a serverless architecture. While
    AWS is the PaaS of choice for this and subsequent chapters, there are many topics
    that cannot be covered in great depth due to the size of AWS as a topic by itself.
    If you get stuck or are confused by any missing content, feel free to open a GitHub
    issue at the preceding repository to begin a dialog and I'll do my best to help.
  prefs: []
  type: TYPE_NORMAL
- en: Presentation layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Static files may be served from numerous places. In our case, we will use two
    different AWS services which will provide respectable performance shipping assets
    down to the client, as well as fault tolerance and caching, among other things.
    Our HTML, CSS, and JavaScript files will all live in an S3 bucket. We will then
    create a CDN using CloudFront, AWS's CDN offering. CloudFront will not only give
    us better performance than S3 by itself; we will gain the ability to globally
    distribute and cache our content, in addition to serving files from our very own
    custom domain using a free TLS certificate from AWS Certificate Manager.
  prefs: []
  type: TYPE_NORMAL
- en: Logic layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The logical layer is the guts of our application, our code. In this and other
    examples, we'll use Python as our programming language and deploy our functions
    as isolated compute units in AWS Lambda. The Serverless Framework will make this
    quite painless, and this will be the foundation for moving fast and iterating
    on our code.
  prefs: []
  type: TYPE_NORMAL
- en: Data layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not the core focus of this book, running databases is an integral part
    of modern-day web applications. In this pattern, we'll use a hosted PostgreSQL,
    which the AWS **Relational Data Store** (**RDS**) service will manage for us.
  prefs: []
  type: TYPE_NORMAL
- en: Logic layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application code is likely the area of most interest and the layer that has
    the most changes from a traditional web application hosted on a managed server,
    so let's start with that.
  prefs: []
  type: TYPE_NORMAL
- en: Application code and function layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s differentiate two classifications of our organization for the logical
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Organization of the Lambda functions themselves, within AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organization of the application code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda functions are the unit of work in Lambda and other FaaS providers (for
    simplicity and clarity, I will refer to these as Lambda functions from here on
    out). A single Lambda function may be updated or deployed in isolation without
    affecting other Lambda functions.
  prefs: []
  type: TYPE_NORMAL
- en: Organization of the Lambda functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With a REST API, there are a few options you have as to how each API endpoint
    maps to a function. The primary options in this design are whether to have a single
    Lambda function handle a single HTTP verb/resource combination, or whether to
    have a single lambda function handle all HTTP verbs for a particular resource.
    It should become more evident as we work through this chapter that Lambda function
    organization and application code organization are related, but not the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b20d300f-52c8-4424-984b-d03a3cda4ab6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we see a possible Lambda function layout for a REST
    API. On the left, unique functions handle a unique CRUD event and resource combination.
    On the right, Lambda functions perform actions on a single resource but with different
    actions (create, read, update, or delete).With the first model (left side of the
    diagram), each REST endpoint is mapped to a single Lambda function. This design
    provides fine-grained control for updating functions, allowing for the deployment
    of a single API endpoint without the danger of inadvertently affecting other APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The major downside is that this may quickly become unwieldy as the API grows.
    Imagine the case of an API with 20 resources (`session`, `user`, and so on), each
    with three to four actions `/` HTTP verbs. If you follow this scenario through
    with some basic multiplication, the quick growth of the Lambda functions that
    you'll need to manage and navigate will become obvious.
  prefs: []
  type: TYPE_NORMAL
- en: With the next design, logical groups of REST endpoints are grouped and triggered,
    in effect the main function that routes the request to the appropriate handler.
    If you imagine the simple case of listing `sessions` from this API, an HTTP `GET`
    would come into the `/session` endpoint, which would trigger the `handle_sessions()`
    function. As a part of this payload, our application code would know that a `GET`
    method was invoked and would then invoke a `get_sessions()` function, perhaps
    the same as in the previous design. The significant benefit of this architecture
    is that the number of Lambda functions is drastically reduced over the previous
    design. The downside is that deploying updates affects all REST endpoints, which
    are handled by a single function. However, this may also be a benefit. If there
    were a bug in some shared code that affected all `/session/{id}` endpoints (`GET`,
    `PUT`, and `DELETE`), we'd only need to update a single function to fix them all.
    With the previous design, we would need to update three functions individually.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we will use the grouped design so that we have a single Lambda
    function for groups of REST endpoints. Each group will share a common URL pattern,
    and the HTTP verb will be used to trigger different functions within the application
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Organization of the application code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Organization of our application code is entirely different than our prior discussion,
    although there is a bit of overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Laying out application code in a serverless project is slightly different than
    in a traditional web application. While the differences aren't that drastic, I
    find serverless projects a bit more susceptible and intolerant of designs or layouts
    that are not thought through in detail. Because it's so easy to get started, it's
    also easy to start moving in the wrong direction before thinking through and answering
    essential design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the years, these are a few of the big lessons I''ve learned when writing
    serverless application code:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration should be done with environment variables, rather than different
    configuration files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application code should be well structured, highly modular, and namespaced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think through how many functions you need before coding begins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration with environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're familiar with the Twelve-Factor App or have worked with Docker much,
    you'll know that configuration may be done using environment variables rather
    than managing multiple disparate configuration files. According to The Twelve-Factor
    App ([https://12factor.net/config](https://12factor.net/config)):[](https://12factor.net/config))
  prefs: []
  type: TYPE_NORMAL
- en: '"Env vars are easy to change between deploys without changing any code; unlike
    config files, there is little chance of them being checked into the code repo
    accidentally; and unlike custom config files, or other config mechanisms such
    as Java System Properties, they are a language- and OS-agnostic standard."'
  prefs: []
  type: TYPE_NORMAL
- en: Using environment variables for FaaS enables code deployments to different systems
    (dev, QA, production, and so on). Changing configuration can be as simple as updating
    a variable in your function's config. However, for safety and repeatability, environment
    variable changes should go through some process such as CI/CD to minimize the
    chance of errors.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, if using file-based configuration, updating the application
    typically requires updating a file, possibly checking into source control and
    redeploying the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, there is an enormous increase in productivity using environment
    variables when creating new systems or deploying between different systems. To
    perform a new stack deployment or update of an existing stack, you merely load
    up new environment variables and executes a standard set of steps that don't change
    between stacks. Due to the ease and speed with which you can do this, it encourages
    separation of stacks for different purposes (development, testing, production,
    and much more).
  prefs: []
  type: TYPE_NORMAL
- en: Code structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With any new application, there are many ways to organize code on disk. The
    following structure has worked very well for my colleagues and me across multiple
    projects. I encourage readers to use this as a starting point and adapt as needed.
    If using Node.js or another supported language for your FaaS provider, this may
    look slightly different. Throughout this chapter, we will fill in our example
    coffee `cupping` API and will add more files as we build the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A `Makefile` may be something you skip. I use Docker as a host for application
    development since it's a reasonably easy way to manage environment variables during
    deployment and testing. A simple `Makefile` can make tedious tasks much less verbose
    by hiding the complexity behind a make target.
  prefs: []
  type: TYPE_NORMAL
- en: For details on this Docker methodology, I'll point you to a detailed blog post
    at [http://blog.brianz.bz/post/structuring-serverless-applications-with-python/](http://blog.brianz.bz/post/structuring-serverless-applications-with-python/).
    It'd be perfectly acceptable to run your serverless application development on
    your primary computer without any extra virtualization (Docker, VirtualBox, and
    so on). This Docker/Makefile pattern has worked quite well for me recently across
    multiple projects. I still edit files using my host system, but all runtime work
    (testing, deployment, building packages, and so on) is done from within a Docker
    container.
  prefs: []
  type: TYPE_NORMAL
- en: The `envs` directory holds environment variable files, which are simple `key=value`
    pairs. Each environment has a corresponding file of the same name. Looking at
    preceding the example, it should be clear where the configuration resides for
    each environment and what you'd need to do when creating a new environment.
  prefs: []
  type: TYPE_NORMAL
- en: We place all the code into the `serverless` directory, including application
    code we write, as well as its dependencies. Our application code is namespaced
    into the `cupping` directory. We will install third-party libraries into `lib`.
    Of course, as you write your application, your application code will be namespaced
    to something that is appropriate for your project. I recommend using a meaningful
    name rather than something generic such as `code` or `app`, just to aid new developers
    who come after you in navigating the source tree and for general clarity and explicitness.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside the application code live two files—`serverless.yml`, which defines
    and controls the Serverless Framework, and `handler.py`, which is the main entry
    point for any API calls. In the preceding diagram, we discussed how logical groupings
    of API endpoints would be handled by a common function within a given file, `handler.py`,
    which will be the entry point for these API calls and delegate the hard to work
    to other functions. In some ways, `handler.py` has a straightforward job, which
    will become apparent.
  prefs: []
  type: TYPE_NORMAL
- en: As responsible developers, we will make sure our code is well tested. With `pytest`
    as our testing framework of choice in this project, all unit test files live in
    a single `test` folder with some additional helpers and configuration utilities.
    In the preceding example, there are only two test files; more will be added to
    the final project. Your exact testing strategy isn't as important as the simple
    fact of having well-written tests. Serverless projects are incredibly fast to
    deploy, and there may be an inclination to forego tests. Why write unit tests
    when you can just deploy it for real and test it manually? One cannot overstate
    the benefit of having robust unit tests. Regardless of your tooling or language,
    all serverless projects of any decent size or complexity will benefit from tests,
    which you may run locally or on a continuous integration platform or system. Tests
    give us the confidence to deploy quickly and often and also set us up for continuous
    delivery moving forward, allowing a CD system to deploy our code automatically
    only after our tests have passed.
  prefs: []
  type: TYPE_NORMAL
- en: Function layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our example, we will implement a pattern where a single Lambda function
    will handle a single grouping of URL endpoints. The initial set of endpoints that
    we will implement will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List coffee `cupping` sessions: `GET /session`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a coffee `cupping` session: `POST /session`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get coffee `cupping` session details: `GET /session/{id}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delete a coffee `cupping` session: `DELETE /session/{id}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two unique URLs will map to two Lambda functions. These lambda functions will
    be responsible for inspecting the HTTP request passed in from API Gateway, determining
    what HTTP method is being called, and invoking the appropriate application code
    to fulfill the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fb9edd0-ed99-4fa7-abdb-36d005f70c56.png)'
  prefs: []
  type: TYPE_IMG
- en: Request routing for a `/session` endpoint. The application code will inspect
    the HTTP method and route to the appropriate application code for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Presentation layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Presentation layers are not necessarily the most exciting area but, in reality,
    they are the entry point for your entire web application, and you should think
    through the details carefully. Naive deployments of HTML, CSS, and JavaScript
    files may result in slow load times, which has a noticeable impact on user experience.
  prefs: []
  type: TYPE_NORMAL
- en: When building serverless systems on top of AWS, there are a few different services
    that enable us to host static assets quite easily. Other PaaS systems have similar
    offerings, although there may not be a one-to-one comparison with all of the AWS
    services.
  prefs: []
  type: TYPE_NORMAL
- en: File storage with S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any frontend assets need a filesystem as a home. In this case, the natural choice
    is AWS **Simple Storage Service** (**S3**), which is Amazon's high durability
    object storage service. S3 advertises 99.999999999% durability, so it's safe to
    say our files will be available when we need them. While it's possible to serve
    content from S3 as a website on a custom domain, it's not the best choice for
    this scenario. AWS CloudFront will aid us in distributing files to end users quickly
    and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: CDN with CloudFront
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CloudFront is Amazon's CDN service. A CDN's primary focus is to improve the
    delivery of static assets to the end user. This task is typically accomplished
    by running multiple **points of presence** (**POPs**) around the globe and distributing
    your contents in those various geographic locations. When a user somewhere on
    the planet requests one or more of your files, the CDN can fetch the content that
    is closest to the user to minimize latency. Of course, this is only one small
    and dumbed-down explanation of a single CDN feature. The bottom line is that CDNs
    help us to speed up delivery of our content and should be used in any web application.
  prefs: []
  type: TYPE_NORMAL
- en: CloudFront has some very nice features that will allow us to integrate with
    other AWS services. We will create a CloudFront distribution that will pull our
    content from S3\. In this way, CloudFront is a layer that aids in the acceleration
    of content delivery but does not own any content itself. We'll be able to configure
    caching controls to suit our needs and will also be able to serve our content
    over a custom domain with a free TLS certificate from AWS Certificate Manager.
    All of this is possible thanks to CloudFront.
  prefs: []
  type: TYPE_NORMAL
- en: Data layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's safe to say that most web applications today have some data store, whether
    it's a relational database (PostgreSQL, MySQL, SQLServer, and so on), a non-relational
    database (MongoDB, Redis, Cassandra, and so on), or even static file storage (S3,
    OS filesystem, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: AWS RDS service will manage a PostgreSQL database for our coffee `cupping` application.
    RDS offers different RDBMS choices, most notably PostgreSQL, MySQL, Oracle, and
    SQLServer. There are other choices, and I encourage you to take a look at the
    various offerings. For this exercise, we'll be using a standard PostgreSQL database
    hosted on RDS. Many configuration options come with RDS, which we won't cover.
    Just know that it's possible and quite simple to run, configure, and manage a
    high-availability RDBMS instance using RDS. Other PaaS providers offer similar
    services for relational databases.
  prefs: []
  type: TYPE_NORMAL
- en: Writing our logic layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we've covered the overall design and different layers, let's get down
    to the implementation of our application code.
  prefs: []
  type: TYPE_NORMAL
- en: Application entrypoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every application, web or otherwise, needs a primary entry point. In our case,
    we''ll use `handler.py` to begin application execution when a Lambda function
    is invoked. Serverless Framework applications will generate a `handler.py` file
    when you bootstrap a new project, so this pattern should be familiar to anyone
    who has used Serverless before. If you''ve never worked with the Serverless Framework,
    what follows will be a thorough introduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our `handler.py` code isn't very complicated and delegates most application
    logic to different parts of our application code (namespaced into the `cupping`
    package). This pattern of having a single entry point for all Lambda functions
    is advantageous for a few reasons.
  prefs: []
  type: TYPE_NORMAL
- en: When Lambda functions execute, they only know what they know. That is, we as
    application developers are used to installing extra packages in some known location
    (which is the default system package location) or perhaps creating a Python `virtualenv`
    and configuring our server to look there during the import cycle. During a Lambda,
    we are responsible for managing this ourselves. Application code has no idea where
    to look for packages beyond the built-in libraries without being told where to
    look. The code block below shows how to manipulate Python's `path` so that it
    can find any extra packages we wish to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: These four lines of code accomplish the task of resolving the current directory
    of our `handler.py` file and appending a `/lib` onto it. The result is the absolute
    path of the `lib` directory where we've installed all of our system packages.
    During the deployment step, the Serverless Framework will package all directories
    and files that reside at or below the same directory level as the `serverless.yml`
    file, resulting in our `lib` being available to our application code during runtime.
    Any import statement for a third-party library will work as expected, only after
    the addition of the full path to `lib` being manually added to the system path.
    In the preceding example, there is an import for the third-party `simplejson`
    module. Had this import been placed above the `sys.path.insert` call, it would
    have failed.
  prefs: []
  type: TYPE_NORMAL
- en: When this path manipulation occurs as soon as possible (that is, as soon as
    `handler.py` is invoked), other parts of our application code can import packages
    without the danger of the import failing. If this path manipulation is done across
    different files only when a particular package is needed, errors will be inevitable
    as you will at some point forget to include this logic. Additionally, doing this
    work in a single place means there is no duplication of logic.
  prefs: []
  type: TYPE_NORMAL
- en: Application logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since our handler function is so simple, let''s take a look at the application
    code to see what exactly is going on. Our coffee cupping API is fairly simple
    and only handles a single resource type at this point, a coffee `cupping` session
    object. Before moving forward, it''s helpful to take a look at the shape of this
    resource type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Much of the logic of this application is simply the transformation back and
    forth between JSON and database records. The actual application code isn't that
    important in the context of this book. If you'd like to learn more about the actual
    implementation, I encourage you to view the source code at [https://github.com/brianz/serverless-design-patterns](https://github.com/brianz/serverless-design-patterns).
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic in `handler.py` will delegate the API requests to the `cupping/handlers/session.py`
    file, which you can see in the following code. The purpose here is to service
    requests for a particular URL pattern (which is `/session`, `/session/{id}`) and
    particular HTTP verb (that is, `GET`, `POST`, `DELETE`) and execute the appropriate
    application code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The final two functions are the gateway into this part of our application, where
    HTTP verbs are mapped to different functions.
  prefs: []
  type: TYPE_NORMAL
- en: Wiring handler.py to Lambda via API Gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we need to wire up our API endpoints to Lambda and our `handler.py` entry
    point. This wiring looks like this in a `serverless.yml` configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We define two Lambda functions that have different configuration options, `HandleSession`
    and `HandleSessionDetail`.
  prefs: []
  type: TYPE_NORMAL
- en: Under each function's name, there are multiple statements that control configuration.
    Look at both sections and you'll notice the `handler:` statement, which instructs
    Lambda what code to call when the Lambda function is executed. For both, we'll
    be running one of the Python functions in `handler.py` that we covered in the
    preceding code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: But what calls these Lambda functions in the first place? The `events:` section
    is responsible for setting up invocation points and making the connection between
    a particular event and our Lambda function. Across the FaaS landscape, functions
    are invoked in response to an event. In the AWS landscape, the number of events
    that can trigger a Lambda function is quite large. In this scenario, we are configuring
    events to be HTTP endpoints with a particular path and HTTP method. API Gateway
    is the proxy that will provide us with unique HTTPS URLs, which get wired up to
    our Lambda functions according to our configuration. As you read through the configuration,
    our design and intent should be apparent. Again, there are a seemingly infinite
    number of ways to set up an API with these technologies and this example just
    scratches the surface to discuss the overall pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the frontend JavaScript code will be making HTTP requests to the serverless
    backend, which is hosted on a different domain, CORS will need to be set up for
    each API endpoint. Controlling CORS is simple to do by adding `cors: true` for
    each endpoint in `serverless.yml`. In addition to this setting, the application
    code will explicitly need to return the proper headers in the responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the REST API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now the fun part, we'll deploy our REST API using the Serverless Framework.
    At this point, we have not discussed the various configuration options when implementing
    serverless architectures on AWS. I'll cover different possibilities, and our particular
    configuration, later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: My pattern of using Docker as a build and deployment tool makes this process
    a bit easier. You are not required to do this, and there are likely other ways
    to make the process even simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do all package building and deployment from inside a running Docker
    container, which I start and enter with the following `Makefile` target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This equates to the following Docker command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There is nothing magical here. We're starting up a Docker container from an
    image that contains the Serverless Framework as well as some other Python packages
    for a Python 3 runtime. The main trick is that, based on the `ENV` setting upon
    creation of the container, we pull environment variables from the desired `envs`
    files and load them into the running container. Those environment variables can
    then be referenced from within `serverless.yml` and injected into the Lambda functions,
    hence controlling configuration of the final application by starting from files
    on our local system. Full details are out of scope, but can be reviewed at [http://blog.brianz.bz/post/structuring-serverless-applications-with-python/](http://blog.brianz.bz/post/structuring-serverless-applications-with-python/).[](http://blog.brianz.bz/post/structuring-serverless-applications-with-python/)
  prefs: []
  type: TYPE_NORMAL
- en: The `Makefile` and commands I'm running here are not very sophisticated; however,
    they may appear to be so if you are unfamiliar with Docker or make. I encourage
    those unfamiliar with them to read through the `Makefile` targets and do a bit
    of exploration on their own at [https://github.com/brianz/serverless-design-patterns/blob/master/ch2/Makefile](https://github.com/brianz/serverless-design-patterns/blob/master/ch2/Makefile).
    Feel free to open a GitHub issue if you get stuck or need more clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''re inside a container with all of our configuration set from environment
    variables, we can deploy the entire stack. Our first step is to ensure we have
    our libraries built and installed into the `lib` directory. In the Python world,
    the `pip` command can help us. Take a look at the `Makefile` in the repository
    for details. Our steps for doing the initial deployment are, therefore, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Deploying the Postgres database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many frameworks for working with AWS serverless architectures expose access
    to `CloudFormation`, AWS's tool for managing multiple related resources as a single
    entity. The Serverless Framework is no different and, in fact, the `CloudFormation`
    interface is verbatim `CloudFormation` templating with a few nice add-ons specifically
    for variables, environment variables included. A common theme here is that this
    is a huge topic and the details are out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CloudFormation` creates the RDS instance on our behalf with several lines
    of setup in `serverless.yml`. Details aside, note how there are multiple references
    to `${env:VPC_ID}` and other calls to `${env:}`. The `${env}` syntax is a method
    for pulling variables from the environment that exists in the Docker container
    from our process of starting up the container. You may accomplish the same thing
    on your host system provided you have a way of managing environment variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Much of the complexity of this setup comes from the fact that Lambda functions
    by default will not have network access to AWS resources inside a **virtual private
    cloud** (**VPC**). Since RDS instances need to run inside a VPC, the Lambda functions
    need to be configured to run inside the same VPC and permissions set up accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: During deployment, the Serverless Framework will add any defined `Resources`
    into the default `CloudFormation` template and deploy them together. Having our
    database described, we can perform a `make deploy` and see our dedicated PostgreSQL
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: RDS and other hosted data stores are not silver bullets. These systems can still
    go down, and there are real constraints concerning computing power. However, a
    significant benefit of using a hosted data store is the hard work of managing,
    monitoring, and configuring is delegated to someone else. Serverless is not accurate
    in this case for a variety of reasons. I will assert that a hosted database eases
    much of the burden of managing your system and is an excellent fit in a truly
    serverless architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up static assets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up an S3 bucket and CloudFront distribution to host static media isn't
    complicated and, in theory, we could add this to the `Resources` section of our
    `serverless.yml` file. The ability of Serverless to manage so many resources via
    `CloudFormation` is a slippery slope, since setting up systems can quickly become
    an exercise in learning and debugging `CloudFormation`. Another downside of a
    growing `Resources` section in the `serverless.yml` file is that deployments will
    take longer and longer. It's possible to only deploy application code during development,
    which results in single-digit second deployments; but when some system resource
    is updated, including environment variables, the entire `CloudFormation` stack
    needs to updated.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than creating the S3 bucket and CloudFront distribution via `serverless.yml`,
    we can use a separate `CloudFormation` template designed just for this purpose.
    Another reason for splitting this out into a separate step is that this layer
    rarely changes. Once the CloudFront distribution is set up, there is a good chance
    you won't need to change anything for a very long time, if ever.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following repository contains a `CloudFormation` template, a helper script,
    and documentation to set up a single - page web application on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/verypossible/cloudfront-single-page-app](https://github.com/verypossible/cloudfront-single-page-app)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, you may read the details of this stack creation in the GitHub repository.
    After we choose one this stack with the necessary variables, we will end up with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An S3 bucket, which will host all of our static content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A CloudFront distribution, which will pull and cache content from S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A free TLS certificate for `*.cupperslog.com`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Route53 record, which does the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Points [https://cupperslog.com](https://cupperslog.com) to the CloudFront distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Redirects any `http://` traffic to `https://`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caches static assets for 10 minutes:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'CloudFront distributions can take from several minutes to a couple of hours
    to be created, which is another good reason that we''re doing this separate from
    our application code and database. Once finished, all that is required is uploading
    static assets to the S3 bucket that `CloudFormation` created for you. Ensure that
    the access control policy is set to `public-read` since this is a publicly accessible
    website. Uploading is accomplished via many tools, the AWS CLI tool being one
    of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Other tools I've used for S3 file management are Cyberduck for OS X, S3 Organizer
    for Firefox, and the regular AWS web interface. They all do more or less the same
    thing, so pick what works for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example application, the frontend will consist of a simple React application
    that allows users to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: List coffee `cupping` sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: View coffee `cupping` session details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new coffee `cupping` session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete a coffee `cupping` session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear that there is no authentication and no notion of a user in
    these examples. This application was built to demonstrate a serverless pattern,
    and even critical details such as user authentication and authorization wouldn't
    fit in this single chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the deployed web application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With everything in place, we can now upload our frontend assets to S3\. We won't
    review the actual frontend React code, but if you're curious, you can take a look
    at that UI code in the GitHub repository at [https://github.com/brianz/serverless-design-patterns/tree/master/ch2/ui](https://github.com/brianz/serverless-design-patterns/tree/master/ch2/ui).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding `aws s3 cp` command, a final production build of the frontend
    code is uploaded to S3 and ultimately serves the content as requested by the CloudFront
    CDN. When the first page is rendered, a request is made to our serverless backend
    to get a listing of all coffee `cupping` sessions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db072595-3241-419d-b12f-177f5ac01c14.png)'
  prefs: []
  type: TYPE_IMG
- en: A very common issue, and one that people often forget about, is cross-origin
    resource sharing, which is a security measure put in place by browsers. Our serverless
    backend was set up to sidestep this issue, making development much quicker. For
    a real production system, it's best to only allow CORS for your own domain or,
    better yet, run the serverless backend on your own domain rather than the autogenerated
    domain from API Gateway. Running the serverless API on your own custom domain
    is possible using AWS API Gateway, but this is out of the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on a single row, the detail page for the particular session is loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cd15c11-60e5-4801-a0b6-a04b4d10a84c.png)'
  prefs: []
  type: TYPE_IMG
- en: Running tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we're responsible developers, we have written a full suite of unit tests
    for our application. For now, tests are run manually inside our Docker container.
    The Docker image used has `py.test` installed, as well as some coverage tools.
  prefs: []
  type: TYPE_NORMAL
- en: The only dependency to running tests is PostgreSQL. Docker again makes it very
    simple to run a PostgreSQL container and hook it up to our application container.
    Multiple strategies exist for this, from running Docker Compose to merely starting
    up a container with `docker run` and linking the containers manually. For simplicity,
    I use the latter option. See the targets in the repository `Makefile` for details.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run tests, inside the container, we execute `make tests`. I have trimmed
    much of the output for brevity and clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an `htmlcov/index.html` file that visually shows test coverage
    throughout the application and highlights lines that were not executed during
    the test run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbbfa9ba-735f-4f9c-8f35-3133b08bd294.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image is a test coverage report from the `pytest` and coverage
    libraries
  prefs: []
  type: TYPE_NORMAL
- en: 'Coverage may also be displayed on the console if we ask for it specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Iteration and deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inevitably, there will be multiple deployments when developing an application
    such as this, and even once the first production version has shipped. Serverless
    speeds up this process dramatically, and once you experience the increased velocity,
    you may have a hard time going back to your old ways.
  prefs: []
  type: TYPE_NORMAL
- en: A deployment with the Serverless Framework consists of one command with a couple
    of variations.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the entire stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To deploy everything in the `serverless.yml` file, the `deploy` command is
    used, specifying the `stage (-s)` variable (which defaults to `dev`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `make deploy` target in use for this chapter's example executes this exact
    command.
  prefs: []
  type: TYPE_NORMAL
- en: When doing a full deployment like this, Serverless will upload your Lambda resources
    and execute the entire `CloudFormation` template. Even with a simple `CloudFormation`
    template, this can take several seconds. With bigger stacks, it can be even longer.
    It's unfortunate that some people believe this is the only method of deploying
    application code with this framework. To make application code deployments even
    faster, we can specify precisely which functions to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the application code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you are in the state of code iteration and redeployment, you''ll want
    to make that loop as short as possible. To accomplish this, specifying the function
    name when doing the deployment step goes through the process of uploading your
    Lambda function, but skips the `CloudFormation` update. In my experience, this
    results in deployments that are typically low single-digit seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: I can hear you thinking, what about doing deployments to a production system
    that is serving live traffic? Behind the scenes, AWS Lambda is using container
    technology to respond to events. During a deployment, any Lambda invocations continue
    doing their jobs as instructed. At a certain point, a new Lambda function will
    complete its upload and configuration process. Only at that time will the new
    function begin serving traffic. In short, the tricky dance of draining active
    connections and handing off new connections to new application state is handled
    for you. This behavior should be a standard feature among other FaaS providers.
    Users of other platforms should verify this on their own.
  prefs: []
  type: TYPE_NORMAL
- en: The `Makefile` used in this chapter's example has a target which helps speed
    up the deployment process even more. `make deploy function=FunctionName` may be
    used to deploy a single Lambda function, where `FunctionName` should be a name
    listed in the `serverless.yml` file (for example, `make deploy function=HandleSesion`).
    This works by skipping the `CloudFormation` update and only packaging and uploading
    a single function. `CloudFormation` updates will take a few to many seconds, whereas
    a single function deployment or update is typically low single-digit.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we walked through the entire process of creating a three-tier
    web application with a serverless architecture consisting of a view layer, data
    layer, and application layer and which is powered by AWS Lambda. All services
    employed in the example web application are from AWS, and none require managing
    a virtual machine or operating system directly.
  prefs: []
  type: TYPE_NORMAL
- en: Readers should have a good understanding of the advantages of such a system
    and how to start the process of structuring their application using this pattern.
    I presented several helpers and shortcuts that should aid readers in speeding
    up their development.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](svrls-dsnptn-bstprac_ch03.html), *A* *Three-Tier Web Application
    Pattern with GraphQL*, we will work through a similar pattern by porting the example
    application from a RESTful interface to a GraphQL interface.
  prefs: []
  type: TYPE_NORMAL
