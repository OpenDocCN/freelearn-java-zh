<html><head></head><body>
      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Be Fault-Tolerant</h1>
            
         </header>
         
         
         <article>
            
            
            <p class="mce-root">For years, Java EE has been about putting the maximum number of applications inside
               a single application server, but it has been changing for a few years now. It has
               become more common to deploy a single application in a container instance and to reduce
               the application size to handle a single responsibility. The direct implication of
               such a paradigm change is that a system, as a whole, is now composed of far more applications
               than before, and we rely more and more on remote communications.
            </p>
            
            <p>In such a context, the performance of one application directly depends on another
               application, and it is important to be able to limit the side effects between applications. To
               ensure that your applications identified the impact of its environment and can work
               with such constraints, we will cover the following topics in this chapter:
            </p>
            
            <ul>
               
               <li class="mce-root">Load balancing on clients and servers</li>
               
               <li>Fail-overs</li>
               
               <li>Circuit breaker</li>
               
               <li>Bulkhead usage</li>
               
               <li>Timeout handling</li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">It will fail, no doubt!</h1>
            
         </header>
         
         
         <article>
            
            
            <p>When developing an application, we often spend most of the time on the <em>passing</em> code path, as the code path gives the application its actual feature. However, it
               is important to not forget all the unexpected cases. It can sound weird to try to
               solve something we don't control but, here, the idea is to follow the Murphy's law
               which is often summarized as follows: <em>anything that can go wrong, will go wrong</em>. It doesn't mean that the system will never work, but it means that if there is a
               potential issue, it will become your reality one day or another.
            </p>
            
            <p>In terms of a modern system and, more particularly, Java EE deployment, <span>the typical consequence</span> is that you can lose the connectivity to a related resource or application. Another
               common failure case you can desire to address is about the JVM failing (no more memory,
               OS issue, and so on), but this is linked to the infrastructure (potentially Kubernetes),
               and it is beyond the scope of this book.
            </p>
            
            <p>We will illustrate this with a very simple system where three Java EE applications
               are chained:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="82" src="assets/a2530673-657f-4fdb-94fd-111c3f6500e5.jpg" width="189"/></div>
            
            <p>With such an architecture, we can assume that a front layer exposes some customer
               features or API. Then, the front application delegates the actual logic to an internal
               system owned by another team. Finally, the internal system relies on a data system
               which is again owned and developed by another team.
            </p>
            
            <div class="packt_infobox">It is not rare to use the same sort of architecture, but with external systems. In
               such a case, you often have a support phone number, but it is rarely as efficient
               as calling a colleague, which makes you even more dependent on this system/company
               in the case of a failure.
            </div>
            
            <p>What is important with such systems is that if the data fails (because the data engineers
               did an upgrade that didn't work as expected), then all the internal systems will start
               failing because the data doesn't answer anymore. Transitively, the front system will
               fail because the internal system fails.
            </p>
            
            <p>You may think that this will just make the system inefficient and that there is no
               link with the performance. This is not really the case. In the previous schema, the
               data system looks quite central. If the company adds a second internal system (let's
               call it <em>internal2</em>), then we can assume that the load on the data store will be multiplied by two. Nonetheless,
               if the data is not sized for the load increase, it will be slower to answer and will
               potentially return more errors. Here again, all consumer services, including transitive
               services, will start being slower, as they depend on the data system.
            </p>
            
            <p>This is not something you can actually avoid. You can limit the effect of an unexpected
               failure, but it is almost impossible to guarantee that it will not happen. If you
               are in a big company with an operation team in charge of all the applications, this
               kind of issue will likely be sorted by priority and performance degradation will be
               less important than a failing system. When a distributed system like this starts to
               fail, each brick often fails slowly, just because of the relationships. Thus, all
               the applications will be seen in <em>red</em> by the monitoring team, which doesn't help them to solve the issue, when only one
               part of the whole system is failing (our data system in this example). This is why
               ensuring that your system is ready to fail will make sure that your system is fixed
               faster if there is an issue, and that the performance impact on the other parts of
               the system will be reduced if some related application go uncontrolled.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Load balancing – pick the best one</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Load balancing is about defining how to select the backend node that will process
               the request. It can be done on the server or client side, but strategies are globally
               the same. The fact that it is <strong>Client</strong> or <strong>Server</strong> is mainly a deployment concern because when the <strong>Load Balancer</strong> is an instance (software), then you actually add a <strong>Client</strong> in the chain between the final clients and your middlewares.
            </p>
            
            <p>At very high level, a <strong>Load Balancer</strong> can be schematized as follows:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="66" src="assets/55bfe10b-d8e7-4760-ba44-e58315df4fb4.jpg" width="280"/></div>
            
            <p>The global idea is to add a layer in between the <strong>Client</strong> and the <strong>Server</strong>, which will orchestrate the way the requests are distributed to the servers depending
               on different strategies. This picture represents four clients calling the same application
               through a <strong>Load Balancer</strong>, which will delegate the request processing to three servers (one server will process
               two of the four client requests).
            </p>
            
            <p>This is a common representation of server-side load balancing, but it can also be
               applied on the client side. The only difference is that the <strong>Load Balancer</strong> will be <em>deployed</em> inside the JVM of the client and, instead of taking the incoming requests through
               a network protocol (websocket, HTTP, and so on), it will take it from the inside of
               the JVM (normal method invocation).
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Transparent load balancing – client versus server</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The main advantage of using a server load balancer is that the clients don't really
               care about a load balancer. Concretely, all the clients will use the same endpoint
               (let's say, <em>quote-manager.demo.packt.com</em>) and the load balancer will distribute the requests without requiring any knowledge
               of the clients. This is very important in terms of the infrastructure, since you can
               update your infrastructure without notifying or updating the clients (which can be
               impossible if not owned by your own system).
            </p>
            
            <p>For instance, if you start with two machines and decide to add a third one a month
               later because you get more load or to support the <em>black friday</em> additional load, then you will just register this third machine against the load
               balancer and it will distribute the work load against the three machines instead of
               only two. It is also true for the opposite way: if you need to do some maintenance
               on a machine, you can remove it from the cluster behind the load balancer, assuming
               that removing one machine still supports the load, but it should be taken into account
               during the sizing phase of the infrastructure. Then, do your maintenance offline,
               and once done, just add back the machine into the cluster.
            </p>
            
            <p>So, this analysis makes it sound like server load balancing is the best solution and
               the one to choose. However, modern systems have efficient client-side load balancers
               if you own the clients (which is often the case for microservice-oriented systems).
               What makes server load balancing strategy better than client load balancing?<span>—</span>The fact that the server can be updated without notifying the clients. This means
               that if the clients are autoupdated from the server/backend changes, then we achieve
               the same on the server side. In practice, this is done using a service registry that
               can enumerate the list of URLs that you can use to contact a service. In practice,
               the client load balancer will contact this registry service to get the list of endpoints
               it can use for a particular service and update this list from time to time with a
               configuration policy close to the pool ones that we saw in the previous chapter. It
               sill means that this <em>registry</em> service must be reliable and should likely use a server load balancer solution, but
               then, all other services can use point-to-point connection (without an intermediate
               load balancer instance). In terms of application impact, it means that adding (or
               removing) a server must imply (de)registration against the registry instead of the
               load balancer, but it is the same sort of work in both the cases.
            </p>
            
            <p>At this point, we see that both the client and server load balancing can achieve the
               same sort of features, so what can be differentiating? There are two main criteria
               you can use to choose between both:
            </p>
            
            <ul>
               
               <li>Who is responsible for the infrastructure and load balancing? If it is the dev(ops)
                  team, both the solutions will work well. However, if you are working in a company
                  that splits development and operations into teams, you will likely prefer to delegate
                  the part to the operations team and, thus, use a server load balancer, which they
                  will fully control without impacting the application development.
               </li>
               
               <li>What kind of logic do you want to put in place inside the load balancer? Server-side
                  load balancers have the most common strategies already implemented and, often, a small
                  scripting language that you can use to customize. However, if you have a very custom
                  strategy (potentially depending on your application state), then you will want to
                  code the load balancing strategy inside the client.
               </li>
               
            </ul>
            
            <p>To summarize, client-side load balancing is more impacting in terms of development
               because you need to handle it on the client side, which means in all the clients instead
               of a single instance for the server side, but it gives you really more power for very
               advanced needs.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Common strategies</h1>
            
         </header>
         
         
         <article>
            
            
            <p>How to distribute the request is the central piece of a load balancer. In this section,
               we will go through the most common solutions that you will encounter while configuring
               a load balancer.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The round-robin algorithm</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The round-robin algorithm is certainly the most known of all the strategies. It considers
               the list of available members of the cluster (the <em>servers</em>) as a ring and continuously iterates over this ring each time a request comes.
            </p>
            
            <p>For instance, if you have three servers (<kbd>server1.company.com</kbd>, <kbd>server2.company.com</kbd>, <kbd>server3.company.com</kbd><span>), here is how the first requests will be served:</span></p>
            
            <table style="width: 533px;height: 476px">
               
               <tbody>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><strong>Request number</strong></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><strong>Selected server</strong></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">1</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd>server1.company.com</kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">2</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server2.company.com</span></kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">3</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server3.company.com</span></kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">4</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server1.company.com</span></kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">5</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server2.company.com</span></kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">6</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server3.company.com</span></kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">7</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server1.company.com</span></kbd></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">...</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">...</div>
                        
                     </td>
                     
                  </tr>
                  
               </tbody>
               
            </table>
            
            <p> </p>
            
            <p>You will note that to have a <em>fair</em> distribution, the load-balancer strategy must lock or synchronize the list every
               time it selects a server. There are other flavors of this algorithm where the implementation
               is lock-free but the fairness of the distribution is not fully guaranteed. However,
               keep in mind that it is rarely something you'd really care about, as you want to have
               something that looks like being fair.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Random load balancing</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Random load balancing also takes a list of servers to target but every time a request
               comes, it picks one randomly. If random implementation is equally distributed, it
               leads to a distribution close to the round-robin solution. However, it can potentially
               scale better, since it doesn't need to synchronize the list to pick the <em>current</em> server to use.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Link to failover</h1>
            
         </header>
         
         
         <article>
            
            
            <p>We will talk more about failover in the next section, but it is important here to
               mention that load balancing can be used to implement a failover strategy. Here, the
               goal will be to try the request against another machine if the <em>current</em> one fails. This can be sort of seen as round-robin, but instead of using each request
               as a trigger for the iteration over the hosts (to change targeted instance), a failure
               would be the trigger. Here is an example sequence using the failover strategy, considering
               that we have the same three hosts as in the round-robin part:
            </p>
            
            <table>
               
               <tbody>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><strong>Request number</strong></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><strong>Selected server</strong></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><strong>Status</strong></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">1</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd>server1.company.com</kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">OK</div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">2</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server1.company.com</span></kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><span>OK</span></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">3</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server1.company.com</span></kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><span>OK</span></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">4</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server1.company.com</span></kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><span>OK</span></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><em>5</em></div>
                        
                     </td>
                     
                     <td>
                        
                        <div><kbd>server1.company.com</kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><em>OK</em></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">6</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server2.company.com</span></kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><span>OK</span></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">7</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><kbd><span>server2.company.com</span></kbd></div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"><span>OK</span></div>
                        
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">...</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign">...</div>
                        
                     </td>
                     
                     <td>
                        
                        <div class="CDPAlignCenter CDPAlign"/>
                        
                     </td>
                     
                  </tr>
                  
               </tbody>
               
            </table>
            
            <p> </p>
            
            <p>As you can see in the preceding table, each request is using the same host (<kbd>server1.company.com</kbd>) until a request fails (request #5), in which case, the algorithm iterates over the
               host list and starts using <kbd>server2.company.com</kbd>.
            </p>
            
            <p>Indeed, there are some variants to this algorithm. For instance, the failed request
               can be retried (or not) with the <em>next</em> host in the list, or you can configure a number of failures to wait before switching
               the host or even configure what failure means (the default is generally a 5xx HTTP
               status, but you can also configure it to be any HTTP status &gt; 399, or base this choice
               on a header or any other part of the response).
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Sticky session</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Sticky session routing is generally used because of a business use case. The idea
               is to always route a client to the same backend server if a session is started. Java
               EE defines <span>three session modes</span> through <kbd>SessionTrackingMode</kbd>:
            </p>
            
            <ul>
               
               <li><strong>COOKIE</strong>: The session is tracked through its identifier (<kbd>JSESSIONID</kbd>) inside cookies, so it hits the browser (client) and is sent with each request in
                  the cookies.
               </li>
               
               <li><strong>URL</strong>: The <kbd>JSESSIONID</kbd> is sent to the client through the URL. Example:  <kbd>http://sample.packt.com/quote-manager/index.html;jessionid=1234</kbd></li>
               
               <li><strong>SSL</strong>: This uses the HTTPS native mechanism to identify sessions.
               </li>
               
            </ul>
            
            <p>Each time, the tracking works by passing a shared <em>identifier</em> between the client and the server. If you add a load balancer in between, the communication
               can be broken if you don't target the same host. Here is a diagram representing this
               statement:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="153" src="assets/4b60f497-449b-4f73-a2e9-b2294897c114.jpg" width="389"/></div>
            
            <p>This diagram represents a <strong>Client</strong> serving two requests. The first request (<strong>1</strong>) will hit the <strong>Load Balancer</strong>, which will redirect the request to <strong>Server 1</strong> (<strong>1'</strong>) and the request processing will create a session on <strong>Server 1</strong>. It implies that the response of this first request will create a <kbd>JSESSIONID</kbd> (or its SSL replacement). Now, the client issues a second request (<strong>2</strong>) and, here, the <strong>Load Balancer</strong> redirects the request, respecting the stickiness of the strategy, to the second server
               (<strong>2'</strong>). During the processing on <strong>Server 2</strong>, the application tries to access back the session information created during the
               first request (the identified user for instance), but since we switched the node,
               the session is not here. So, the request processing fails (red cross).
            </p>
            
            <p>To ensure that this workflow works, there are two main solutions:</p>
            
            <ul>
               
               <li>Ensure that the session state is distributed and shared across all the nodes. This
                  solution sets up a kind of distributed storage space between the servers. It generally
                  either implies a lot of latency (if synchronously done), or some potential miss (failure)
                  if asynchronously done, which can lead to the same issue as the previous schema. It
                  also implies to configure a solution other than the server's default session handling,
                  which is local only out of the box.
               </li>
               
               <li>Ensure that the load balancer always hits the same backend node once a session is
                  created. This is what we call the <em>sticky session</em> mode. The load balancer will check if a <kbd>JSESSIONID</kbd> (or an SSL connection) exists and, if so, will store which node created it. If it
                  sees again this identifier in a request, it will redirect the request to the same
                  node ignoring any distribution strategy.
               </li>
               
            </ul>
            
            <p>This means that the sticky session mode is often coupled with another strategy, which
               will define the distribution, since the sticky session only applies once a request
               has already been served for a client.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The scheduling algorithm</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The scheduling algorithm is a wide category of strategies based on some statistical
               criteria. The idea here is to be more accurate about the way load distribution is
               done regarding the available resources in the backend servers. The most common criteria
               are as follows:
            </p>
            
            <ul>
               
               <li><strong>By request</strong>: The distribution is based on the number of requests served by the node. Note that
                  each server node can have a weight associated with this distribution to bias the distribution
                  if a machine is less powerful than the other.
               </li>
               
               <li><strong>By traffic</strong>: This is the same sort of distribution as the previous one, but instead of counting
                  the requests, it uses the transmitted bytes.
               </li>
               
               <li><strong>By busyness</strong>: This is also based on the number of requests, but only the live number. The least <em>busy</em> node is selected.
               </li>
               
               <li><strong>Heartbeat</strong>: This is not a distribution solution per se but is more of an alternate evaluation
                  solution. It uses a heartbeat or <em>agent</em> to evaluate the load a node has and, based on this information, it distributes to
                  the node that can handle the most load. It is generally a time statistics, which is,
                  therefore, dynamic and autoadaptive.
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title"> Load balancing or proxy – additional features</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Even if you set up a load balancing to distribute the load with one of the previous
               strategies, a load balancer solution is often a full proxy and, therefore, also provides
               additional features. It generally concerns the server middlewares more than the client
               ones, but it remains very interesting. Part of the features that you can get from
               the backend (servers) point of view are as follows:
            </p>
            
            <ul>
               
               <li><strong>Compression:</strong> Your backend server can serve plain text and the load balancer/proxy layer will automatically
                  add GZIP compression on text resources (HTML, JavaScript, CSS, and so on). Since the
                  client (browser) to load balancer communication is generally slower than the load
                  balancer to server/backend communication, it will allow you to save precious time
                  for these requests.
               </li>
               
               <li><strong>TCP buffering:</strong> Here, the idea is to buffer the response sent by the backend server in the load balancer
                  layer to free the backend from this load and let it serve other requests. This is
                  useful for slow clients that would hold a connection on the backend but not imply
                  any processing/computing work.
               </li>
               
               <li><strong>HTTP caching:</strong> We saw, in the previous section, that HTTP defines some caching. The proxy layer
                  can handle it for you for free without having to request the backend server. This
                  generally concerns only static resources that are moved to the proxy layer in this
                  condition.
               </li>
               
               <li><strong>Encryption:</strong> The proxy layer can encrypt a part of the request to prevent the end users from knowing
                  enough about the backend to understand how it works or even access some sensitive
                  data.
               </li>
               
            </ul>
            
            <p>When the load balancer layer adds features more business-oriented than communication/network-oriented,
               we often call it a <em>gateway.</em> However, technically, it is pretty much the same sort of middleware. Here are the
               features you can find in gateways:
            </p>
            
            <ul>
               
               <li><strong>Security handling</strong>: The load balancer layer can authenticate and validate the permissions from the request
                  (generally from a header).
               </li>
               
               <li><strong>Version handling:</strong> Depending on the incoming request, the route (requested endpoint of the backend)
                  can change, allowing us to automatically handle the versioning of the backend points.
               </li>
               
               <li><strong>Rate limiting:</strong> This limits the access to the backend with a particular rate, either by application
                  or per user, if the authentication is associated with the rate limiting. It is generally
                  expressed as the number of allowed requests per unit time, for example, 1,000 requests
                  per minute.
               </li>
               
               <li><strong>Concurrent limiting:</strong> This controls the number of requests that can be sent in parallel/concurrently. As
                  for rate limiting, it can be done for the entire application or per user (or other
                  units if relevant). For example, 512 requests.
               </li>
               
            </ul>
            
            <p>As you can see there are several features and all are not related to the performance.
               However, most of them will have performance impacts depending on your final environment.
               HTTP caching, for instance, will allow your backend server to handle more actual load
               and, therefore, will scale more easily. The rate/concurrent limiting features can
               enable you to control the performances and ensure that they are not degraded under
               unexpected load circumstances, but other features such as the security ones can have
               a very strong impact on your performance if the gateway layer can use a hardware encryption
               solution instead of a software encryption, like you would generally do in a Java EE
               application.
            </p>
            
            <p>What is important to keep in mind here is to think of the application as a system
               solution and not to try to put everything inside the application just because it is
               easier or more portable. Relying on well-optimized hardware solutions will yield good
               performance compared with optimizing a software solution, which can lead you to use
               native integration, particularly, when it comes to security and cryptography,  and
               will affect the portability of your application.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Failover</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In a distributed system, it is very important to ensure that you know how to handle
               failures. Java EE applications being more and more connected to other systems, they
               face this challenge more and more, so it is important to know how you will deal with
               failover when it happens.
            </p>
            
            <p>The first meaning of failover is, indeed, to fail over. It can be rephrased as <em>the capability to switch to a backup system when the primary system fails</em>. In Java EE applications, there are lots of places where this can be set up, but
               they are all related to external systems:
            </p>
            
            <ul>
               
               <li><strong>Databases</strong>: If a database connection fails, how to still handle the requests?
               </li>
               
               <li><strong>JMS</strong>: If a broker fails, what to do?
               </li>
               
               <li><strong>Other network API (such as SOAP or REST API)</strong>: If the remote server is down, what to do?
               </li>
               
               <li><strong>WebSocket</strong>: If the target server closes the connection or fails, what to do?
               </li>
               
            </ul>
            
            <p>In general, each time your application relies on something it doesn't control (a.k.a.
               external systems), it can need a plan B to still be functional if the primary solution
               is no more responding or working.
            </p>
            
            <p>There are several ways to handle the failovers, which either rely on selecting another
               system or are based on some default/caching implementation.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Switching to another system</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The easiest implementation of a failover is switching to another system when an error
               occurs. This is what we saw in the previous section with load balancing. The only
               condition for us to be able to implement a failover is to be able to identify the
               error encountered by the system.
            </p>
            
            <p>Here is an example using the JAX-RS client API of Java EE to illustrate this logic:</p>
            <pre><span>@ApplicationScoped<br/></span><span>public class </span>ExternalServiceClient {<br/>    <span>@Inject<br/></span><span>    </span><span>private </span>Client <span>client</span>;<br/><br/>    <span>public </span>Data get() {<br/>        <span>return </span>Stream.<span>of</span>(<span>"http://server1.company.com"</span>,<br/>        <span>"http://server2.company.com"</span>)<br/>                .map(server -&gt; {<br/>                    <span>try </span>{<br/>                        <span>return </span><span>client</span>.target(server)<br/>                                .path(<span>"/api/quote"</span>)<br/>                                .request(<span>APPLICATION_JSON_TYPE</span>)<br/>                                .get(Data.<span>class</span>);<br/>                    } <span>catch </span>(<span>final </span>WebApplicationException wae) {<br/>                        <span>if </span>(supportsFailover(wae)) {<br/>                            <span>return null</span>;<br/>                        }<br/>                        <span>throw </span>wae;<br/>                    }<br/>                })<br/>                .filter(Objects::<span>nonNull</span>)<br/>                .findFirst()<br/>                .orElseThrow(() -&gt; new IllegalStateException("No<br/>                available target<br/>                server"));<br/>    }<br/>}</pre>
            <p>This snippet replaces the direct call to the remote API with <kbd>Stream</kbd>. The usage of <kbd>Stream</kbd> is fancy here compared with the usage of <kbd>Collection</kbd>, since the leaf of the stream will trigger the flow to be executed (but by element)
               and will enable us to stop the <em>iteration</em> if we encounter a final condition early. Concretely, it prevents us from iterating
               over all the elements if irrelevant, which is exactly what we want for failover. In
               terms of implementation, here is the flow:
            </p>
            
            <ul>
               
               <li>From a server, we invoke the endpoint we want.</li>
               
               <li>We process the response of the server. If it requires a failover, we return <kbd>null</kbd>; otherwise, we keep the default behavior of the invocation.
               </li>
               
               <li>We remove the <kbd>null</kbd> response from the flow, since the previous step defines <kbd>null</kbd> as a failover condition.
               </li>
               
               <li>We use the first available response as being valid, which will avoid doing the invocation
                  for all servers.
               </li>
               
               <li>If all the servers fail, then we throw <kbd>IllegalStateException</kbd>.
               </li>
               
            </ul>
            
            <p>What is missing in the previous snippet is the way to evaluate that we want a failover.
               In the previous code, we are basing this decision on <kbd>WebApplicationExceptinon</kbd>, so the client can throw in the case of an error. A default implementation of <kbd>supportsFailover()</kbd> will just be to return <kbd>true</kbd>, but we can be more fancy:
            </p>
            <pre><span>private boolean </span>supportsFailover(<span>final </span>WebApplicationException wae) {<br/>    <span>final </span>Response response = wae.getResponse();<br/>    <span>if </span>(response == <span>null</span>) { <span>// client error, no need to retry<br/></span><span>        </span><span>return false</span>;<br/>    }<br/>    <span>return </span>response.getStatus() &gt; <span>412</span>;<br/>    <span>// 404, 412 are correct answers we don't need to retry<br/></span>}</pre>
            <p>This is still a simple implementation but we use the HTTP status code to retry only
               if its value is more than 412, which means we will not retry if we get HTTP 404 (not
               found) or HTTP 412 (precondition failed), since both the cases will lead to sending
               the same request to another server and getting the same response.
            </p>
            
            <p>Of course, you can customize a lot of this logic (and this can even be service-dependent),
               but luckily, Java EE provides you with all you need to do.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Local fallback</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The previous failover implementation was considering that there is an alternative
               system to contact if the primary one fails. This is not always the case and you may
               desire to replace a remote invocation by a local default in the case of an error.
               This sort of solution is available in the hystrix framework, which can be integrated
               with Java EE using the concurrency utilities that we saw earlier.
            </p>
            
            <p>The high-level logic of defaulting is as follows:</p>
            <pre>try {<br/>    return getRemoteResponse();<br/>    } catch (final UnexpectedError error) {<br/>    return getLocalResponse();<br/>}</pre>
            <p>This is actually a generalization of the previous implementation. Instead of seeing
               a list of hosts to contact, you need to consider the remote invocations that you can
               do as a list of tasks. Concretely, we can rewrite this as follows:
            </p>
            <pre><span>@ApplicationScoped<br/></span><span>public class </span>ExternalServiceClient {<br/>    <span>@Inject<br/></span><span>    </span><span>private </span>Client <span>client</span>;<br/><br/>    <span>public </span>Data get() {<br/>        <span>return </span>Stream.&lt;Supplier&lt;Data&gt;&gt;of(<br/>                    () -&gt; getData(<span>"http://server1.company.com"</span>),<br/>                    () -&gt; getData(<span>"http://server1.company.com"</span>))<br/>                .filter(Objects::<span>nonNull</span>)<br/>                .findFirst()<br/>                .orElseThrow(() -&gt; <span>new </span>IllegalStateException(<span>"No<br/>                available remote<br/>                server"</span>));<br/>    }<br/>    <br/>    <span>private </span>Data getData(<span>final </span>String host) {<br/>        <span>try </span>{<br/>            <span>return </span><span>client</span>.target(host)<br/>                    .path(<span>"/api/quote"</span>)<br/>                    .request(<span>APPLICATION_JSON_TYPE</span>)<br/>                    .get(Data.<span>class</span>);<br/>        } <span>catch </span>(<span>final </span>WebApplicationException wae) {<br/>            <span>if </span>(supportsFailover(wae)) {<br/>                <span>return null</span>;<br/>            }<br/>            <span>throw </span>wae;<br/>        }<br/>    }<br/><br/>    // supportsFailover() as before<br/>}</pre>
            <p>Here, we just moved the client invocation to a method and we replaced our stream of
               hosts by a stream of invocations. The stream logic is exactly the same—that is, it
               will take the first working result of the list.
            </p>
            
            <p>The direct gain of such a solution is that, since you are passing tasks to the failover
               logic and not hosts, you can implement every task as you wish. For instance, if you
               want to default to a hardcoded value, you can do this:
            </p>
            <pre>Stream.&lt;Supplier&lt;Data&gt;&gt;of(<br/>            () -&gt; getData(<span>"http://server1.company.com"</span>),<br/>            () -&gt; <span>new </span>Data(<span>"some value"</span>))</pre>
            <p>With this definition, we will first try to contact <kbd>http://server1.company.com</kbd>, and if it fails, we will just create a <kbd>Data</kbd> instance locally.
            </p>
            
            <p>Before seeing what type of strategy can be used for these fallbacks, let's just take
               a moment to see what it can mean in terms of code organization.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Fallback code structure</h1>
            
         </header>
         
         
         <article>
            
            
            <p>When you were just handling a failover across servers, it was not very complicated,
               as you would probably have the list of hosts in the <kbd>Client</kbd> class, and iterating over them was almost natural. Now that we iterate over different
               implementations, it is less natural and we need an <em>orchestrator</em> bean. Concretely, for the previous example, which <span>first </span>calls a remote service and then falls back on a local hardcoded instantiation, we
               would need the following:
            </p>
            
            <ul>
               
               <li>A remote client</li>
               
               <li>A local <em>mock</em> implementation
               </li>
               
               <li>A facade (orchestration) service, which is used everywhere, so we can leverage this
                  failover logic
               </li>
               
            </ul>
            
            <p>When you start integrating lots of services, it is not very convenient.</p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Microprofile to the rescue</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Microprofile includes in its scope a specification helping you to handle your fallback
               logic in a "standard" way. The specification allows to define a fallback method reference
               or handler on a method in case this last one fails. Here what it can look like:
            </p>
            <pre><span class="pl-k">@Fallback</span>(method = "getDataFallback")
<span class="pl-k">public</span> <span class="pl-smi">Data</span> getData() {
    <span class="pl-k">return</span> client.target("http://server1.company.com")<br/>               .request(APPLICATION_JSON_TYPE)<br/>               .get(Data.Class);
}<br/><br/>private Data getDataFallback() {<br/>    return new Data(...);<br/>}</pre>
            <p>Here, you will call <kbd>getData</kbd> from all the consumers of the enclosing service, and if the method fails, the microprofile
               fallback handling will automatically call <kbd>getDataFallback</kbd>.
            </p>
            
            <p>This implementation also supports <kbd>@Retry</kbd>, which allows you to define how many times you execute the primary method (<kbd>getData</kbd> in the previous example) before falling back on the fallback handling/method.
            </p>
            
            <p>This API is nice and straightforward but couples the different implementations together,
               since the primary and secondary methods are linked with the <kbd>@Fallback</kbd> API.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Failover extension</h1>
            
         </header>
         
         
         <article>
            
            
            <p>With CDI, you can define a small extension, which will automatically handle the failover,
               exactly as we did with streams previously, without much effort. The extension will
               be composed of two main parts:
            </p>
            
            <ul>
               
               <li>It will identify all the implementations of a particular logic</li>
               
               <li>It will register a bean for chaining the implementations in the right order with the
                  failover logic
               </li>
               
            </ul>
            
            <p>To do so, we need a few API elements:</p>
            
            <ul>
               
               <li>To find a service implementation, we will mark an interface method with <kbd>@Failoverable</kbd> to identify that we need to create a failover implementation for this interface;
                  we will also use this annotation to mark the implementations.
               </li>
               
               <li>To sort the services, we will use <kbd>@Priority</kbd>. We will just use the priority value as a sorting order for the sake of simplicity.
               </li>
               
            </ul>
            
            <p>In terms of what it will look like from a user's point of view, here is the previous
               example:
            </p>
            
            <table>
               
               <tbody>
                  
                  <tr>
                     
                     <td>
                        <pre>@Failoverable<br/>public interface GetData {<br/>    Data fetch();<br/>}</pre></td>
                     
                     <td>
                        <pre>@ApplicationScoped<br/>@Priority(0)<br/>@Failoverable<br/>public class RemoteGetData {<br/>    @Inject<br/>    private Client client;<br/><br/>    @Override<br/>    public Data fetch() {<br/>        return client.<br/>        ....<br/>        get(Data.class);<br/>    }<br/>}</pre></td>
                     
                     <td>
                        <pre>@ApplicationScoped<br/>@Priority(1000)<br/>@Failoverable<br/>public class LocalGetData {<br/>    @Override<br/>    public Data fetch() {<br/>        return new Data(...);<br/>    }<br/>}</pre></td>
                     
                  </tr>
                  
               </tbody>
               
            </table>
            
            <p> </p>
            
            <p>The interface defines the methods that we want to support failover. Then, we have
               two different implementations with their priority. Such an organization will allow
               us to add another strategy and insert it into the chain, easily and automatically,
               without having to modify all other implementations and respecting CDI loose coupling.
               Now, any user can just inject the <kbd>GetData</kbd> bean into any service and call <kbd>fetch()</kbd> with automatic failover.
            </p>
            
            <div class="packt_infobox">This example doesn't define any parameter to the method, but this is not a limitation
               and you can use this strategy with any method, even very complex ones.
            </div>
            
            <p>In terms of caller code, it really looks like any CDI bean invocation:</p>
            <pre>public class MyService {<br/>    @Inject<br/>    private GetData dataService;<br/><br/>    public void saveData() {<br/>        final Data data = dataService.fetch();<br/>        doSave(data);<br/>    }<br/><br/>    // implement doSave as you need<br/>}</pre>
            <p>And that's it! No need to implement the <kbd>GetData</kbd> failover for the end user; it is done by the extension.
            </p>
            
            <p>Now that we saw how the API looks, let's see how the CDI allows us to do it easily.</p>
            
            <p>The first step is to define our API; the only API that is not in Java EE is <kbd>@Failoverable</kbd>. It is a plain annotation without anything special, except that it must be usable
               on an interface:
            </p>
            <pre><span>@Target</span>(<span>TYPE</span>)<br/><span>@Retention</span>(<span>RUNTIME</span>)<br/><span>public </span>@<span>interface </span><span>Failoverable </span>{<br/>}</pre>
            <p>Then, we just need an extension identifying the implementations of the interfaces
               decorated with this annotation, sorting them, and defining a bean for each interface:
            </p>
            <pre><span>public class </span>FailoverExtension <span>implements </span>Extension {<br/>  <span>private final </span>Map&lt;Class&lt;?&gt;, Collection&lt;Bean&lt;?&gt;&gt;&gt; <span>beans </span>= <span>new<br/>  </span>HashMap&lt;&gt;();<br/>  <span>private final </span>Annotation <span>failoverableQualifier </span>= <span>new<br/>  </span>AnnotationLiteral&lt;<span>Failoverable</span>&gt;() {<br/>  };<br/><br/>  // ensure our @Failoverable annotation is qualifying the beans who<br/>  used this<br/>  annotation<br/>  // to avoid any ambiguous resolution during the startup<br/>  <span>void </span>addQualifier(<span>@Observes </span><span>final </span>BeforeBeanDiscovery<br/>  beforeBeanDiscovery) {<br/>    beforeBeanDiscovery.addQualifier(<span>Failoverable</span>.<span>class</span>);<br/>  }<br/><br/>  // find all API we want to have support for failover<br/>  <span>void </span>captureFailoverable(<span>@Observes<br/>  @WithAnnotations</span>(<span>Failoverable</span>.<span>class</span>) <span>final<br/>  </span>ProcessAnnotatedType&lt;?&gt; processAnnotatedType) {<br/>    <span>final </span>AnnotatedType&lt;?&gt; annotatedType =<br/>    processAnnotatedType.getAnnotatedType();<br/>    <span>final </span>Class&lt;?&gt; javaClass = annotatedType.getJavaClass();<br/>    <span>if </span>(javaClass.isInterface() &amp;&amp;<br/>    annotatedType.isAnnotationPresent(<span>Failoverable</span>.<span>class</span>)) {<br/>      getOrCreateImplementationsFor(javaClass);<br/>    }<br/>  }<br/><br/>  // find all implementations of the failover API/interfaces<br/>  <span>void </span>findService(<span>@Observes </span><span>final </span>ProcessBean&lt;?&gt; processBean) {<br/>    extractFailoverable(processBean)<br/>        .ifPresent(api -&gt;<br/>        getOrCreateImplementationsFor(api).add(<span>processBean</span>.getBean()));<br/>  }<br/><br/>  // iterates over all API and create a new implementation for them<br/>  which is<br/>  added<br/>  // as a new CDI bean with @Default (implicit) qualifier.<br/>  // to do that we use the new CDI 2.0 configurator API (addBean())<br/>  which allows<br/>  // us to define a bean "inline".<br/>  <span>void </span>addFailoverableImplementations(<span>@Observes </span><span>final<br/>  </span>AfterBeanDiscovery<br/>  afterBeanDiscovery, <span>final </span>BeanManager beanManager) {<br/>    <span>beans</span>.forEach((api, implementations) -&gt;<br/>        <span>afterBeanDiscovery</span>.addBean()<br/>          .types(api, Object.<span>class</span>)<br/>          .scope(<span>ApplicationScoped</span>.<span>class</span>)<br/>          .id(<span>Failoverable</span>.<span>class</span>.getName() + <span>"(" </span>+ api.getName() + <span>")"</span>)<br/>          .qualifiers(<span>Default</span>.Literal.<span>INSTANCE</span>, <span>Any</span>.Literal.<span>INSTANCE</span>)<br/>          .createWith(ctx -&gt; {<br/>            <span>final </span>Collection&lt;Object&gt; delegates =<br/>            <span>implementations</span>.stream()<br/>                .sorted(Comparator.<span>comparingInt</span>(b -&gt; getPriority(b,<br/>                <span>beanManager</span>)))<br/>                .map(b -&gt; <span>beanManager</span>.createInstance()<br/>                    .select(b.getBeanClass(),<br/>                    <span>failoverableQualifier</span>).get())<br/>                .collect(<span>toList</span>());<br/>            <span>final </span>FailoverableHandler handler = <span>new<br/>            </span>FailoverableHandler(delegates);<br/>            <span>return </span>Proxy.<span>newProxyInstance</span>(<span>api</span>.getClassLoader(), <span>new<br/>            </span>Class&lt;?&gt;[<br/>            ]{<span>api</span>}, handler);<br/>          }));<br/>    <span>beans</span>.clear();<br/>  }<br/><br/>  // helper method to extract the priority of an implementation<br/>  // to be able to sort the implementation and failover properly<br/>  // on lower priority implementations<br/>  <span>private int </span>getPriority(<span>final </span>Bean&lt;?&gt; bean, <span>final </span>BeanManager<br/>  beanManager) {<br/>    <span>final </span>AnnotatedType&lt;?&gt; annotatedType =<br/>    beanManager.createAnnotatedType(bean.getBeanClass());<br/>    <span>return<br/>  </span>Optional.<span>ofNullable</span>(annotatedType.getAnnotation(<span>Priority</span>.<span>class</span>))<br/>        .map(<span>Priority</span>::value)<br/>        .orElse(<span>1000</span>);<br/>  }<br/><br/>  // if the api doesn't have yet a "bucket" (list) for its<br/>  implementations<br/>  // create one, otherwise reuse it<br/>  <span>private </span>Collection&lt;Bean&lt;?&gt;&gt; getOrCreateImplementationsFor(<span>final </span>Class<br/>  api) {<br/>    <span>return </span><span>beans</span>.computeIfAbsent(api, i -&gt; <span>new </span>ArrayList&lt;&gt;());<br/>  }<br/><br/>  // if the bean is an implementation then extract its API.<br/>  // we do it filtering the interfaces of the implementation<br/>  <span>private </span>Optional&lt;Class&gt; extractFailoverable(<span>final </span>ProcessBean&lt;?&gt;<br/>  processBean) {<br/>    <span>return <br/>  </span>processBean.getBean().getQualifiers().contains(<span>failoverableQualifier</span>)<br/>    ?<br/>        processBean.getBean().getTypes().stream()<br/>          .filter(Class.<span>class</span>::isInstance)<br/>          .map(Class.<span>class</span>::cast)<br/>          .filter(i -&gt; i.isAnnotationPresent(<span>Failoverable</span>.<span>class</span>))<br/>          .flatMap(impl -&gt; Stream.<span>of</span>(impl.getInterfaces()).filter(i -&gt;<br/>          i !=<br/>          Serializable.<span>class</span>))<br/>          .findFirst() : Optional.<span>empty</span>();<br/>  }<br/>}</pre>
            <p>This extension has four main entry points:</p>
            
            <ul>
               
               <li><kbd>captureFailoverable</kbd>: This will ensure that any <kbd>@Failoverable</kbd> interface is registered and will automatically get a default implementation even
                  if there is no service implementing it. It avoids having a <kbd>bean not found</kbd> error at the time of deployment and will instead throw our failover implementation
                  exception to ensure a consistent exception handling for all our beans. Note that it
                  only works if the scanning mode of the module containing the interface includes interfaces
                  (that is, not <kbd>annotated</kbd> ). If not, we may get <kbd>UnsatisfiedResolutionException</kbd> or equivalent during deployment.
               </li>
               
               <li><kbd>findService</kbd>: This captures all the implementations of a <kbd>@Failoverable</kbd> interface.
               </li>
               
               <li><kbd>addFailoverableImplementations</kbd>: This adds a bean with the <kbd>@Default</kbd> qualifier implementing the <kbd>@Failoverable</kbd> interface.
               </li>
               
               <li><kbd>addQualifier</kbd>: This just adds our <kbd>@Failoverable</kbd> API as a qualifier to avoid ambiguous resolutions, since all the services (implementations)
                  will implement the same API and we want to use the <kbd>@Default</kbd> qualifier (implicit qualifier) for our facade. Note that we could have added <kbd>@Qualifier</kbd> on the annotation as well.
               </li>
               
            </ul>
            
            <p>Also, to register this extension, don't forget to create a <kbd>META-INF/services/javax.enterprise.inject.spi.Extension</kbd> file containing the fully qualified name of the class.
            </p>
            
            <p>The implementation of the facade bean is done with a proxy. All the failover logic
               will be passed to the handler, which takes, as input, a list of delegates that are
               actually the implementations we identified in <kbd>findService</kbd>:
            </p>
            <pre><span>class </span>FailoverableHandler <span>implements </span>InvocationHandler {<br/>    <span>private final </span>Collection&lt;Object&gt; <span>delegates</span>;<br/><br/>    FailoverableHandler(<span>final </span>Collection&lt;Object&gt; implementations) {<br/>        <span>this</span>.<span>delegates </span>= implementations;<br/>    }<br/><br/>    <span>@Override<br/></span><span>    </span><span>public </span>Object invoke(<span>final </span>Object proxy, <span>final </span>Method method, <span>final<br/>    </span>Object[]<br/>    args) <span>throws </span>Throwable {<br/>        <span>for </span>(<span>final </span>Object delegate : <span>delegates</span>) {<br/>            <span>try </span>{<br/>                <span>return </span>method.invoke(delegate, args);<br/>            } <span>catch </span>(<span>final </span>InvocationTargetException ite) {<br/>                <span>final </span>Throwable targetException =<br/>                ite.getTargetException();<br/>                <span>if </span>(supportsFailover(targetException)) {<br/>                    <span>continue</span>;<br/>                }<br/>                <span>throw </span>targetException;<br/>            }<br/>        }<br/>        <span>throw new </span>FailoverException(<span>"No success for " </span>+ method + <span>"<br/>        between " </span>+<br/>        <span>delegates</span>.size() + <span>" services"</span>);<br/>    }<br/><br/>    <span>private boolean </span>supportsFailover(<span>final </span>Throwable targetException) {<br/>        <span>return <br/>    </span>targetException.getClass().isAnnotationPresent(<span>Failoverable</span>.<span>class</span>);<br/>    }<br/>}</pre>
            <p>This implementation is probably the most straightforward:</p>
            
            <ul>
               
               <li>The list of delegates is already sorted (see the previous extension).</li>
               
               <li>It iterates over the delegate and tries to do the invocation for each of them; the
                  first one to succeed provides the returned value.
               </li>
               
               <li>If no invocation succeeds, then <kbd>FailoverException</kbd> is thrown, which includes the case where no implementation was provided (that is,
                  the <kbd>delegates</kbd> list is empty).
               </li>
               
               <li>If an exception is thrown, it is tested to see if the failover should occur and the
                  next delegate should be used. In this implementation, it is done by ensuring that
                  the exception has <kbd>@Failoverable</kbd> on it, but it could also test some well-known exceptions such as <kbd>WebApplicationException</kbd> or <kbd>IllegalStateException</kbd>, for instance.
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Fallback handling – caching, an alternative solution</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In the previous subsection, we saw how to handle a fallback using another strategy,
               which can be a hardcoded default value, or can be an alternative way to compute the
               service, including how to contact another service from another provider potentially.
               This is the straightforward implementation of a failover.
            </p>
            
            <p>However, if you step back and think about why you set up some failover mechanisms,
               you'll realize that it was to ensure your service can run even if an external system
               is down. Therefore, there is another solution, which is not a failover, strictly speaking,
               but fulfills the same goal, that is, caching. We saw in a previous section how JCache
               can help your application go faster, enabling you to bypass computation. Caching data
               from external systems also allows you to be more resilient and can <span>potentially </span>prevent you from implementing a failover mechanism for them.
            </p>
            
            <p>Let's take a very simple case to illustrate this. In our quote application (<a href="f8931396-0636-41a9-8bf7-2b67bb424b76.xhtml">Chapter 1</a>, <em>Money – The Quote Manager Application</em>), we grab the list of symbols to use from CBOE and query Yahoo!Finance to get the
               price of each quote. If one of the two services is down, then we don't get any price
               update. However, if we've already executed this logic once, then the price and the
               list of symbols will be in our database, which is a sort of <em>persistent caching</em>. This means that our application, which serves the price of quotes through our JAX-RS
               endpoint, will still work for clients even if the background update process fails.
               If we want to go one step further, we can update this logic to fall back on selecting
               all the symbols of the database if the CBOE service is no more available, which will
               allow the application to at least get price updates and still be more accurate than
               if we fail the whole update process because CBOE is down but not Yahoo!Finance.
            </p>
            
            <p>More generally, if a remote system is not fully reliable and data can be cached, which
               implies that the data is regularly (re)used and can be a bit outdated for your business,
               then caching is a very good alternative to failover.
            </p>
            
            <p>In terms of implementation, you have two main options:</p>
            
            <ul>
               
               <li>Manually handle the cache and the fallback (not recommended)</li>
               
               <li>Use the cache as a data source and fall back on the external system if the data is
                  outdated or missing
               </li>
               
            </ul>
            
            <p>The first option is plain failover handling but the fallback implementation is based
               on cache access. This solution, indeed, considers that you'll fill the cache when
               the primary source works; otherwise, the fallback will just return <kbd>null</kbd>.
            </p>
            
            <p>The second option can be implemented through the solutions we saw in the previous
               part, either using JCache CDI integration or the <kbd>Cache</kbd> API as the primary source manually in your application. You will note that this reverses
               the failover paradigm, as the primary source (remote system) becomes secondary because
               the cache is checked first. But that's how caching works and if the remote system
               supports caching, you will, most of the time, get more benefit from it.
            </p>
            
            <p>To provision the cache, you can use the <kbd>@CacheResult</kbd> API but don't forget to add <kbd>skipGet=true</kbd> to just provision the cache and not bypass the logic. For instance, here is what
               it can look like:
            </p>
            <pre>@ApplicationScoped<br/>public class QuoteServiceClient {<br/>    @Inject<br/>    private Client client;<br/><br/>    @CacheResult(skipGet = true)<br/>    public Data fetch() {<br/>        return client.target(....)....get(Data.class);<br/>    }<br/>}</pre>
            <p>Enforcing JCache to skip the get phase of the interceptor associated with <kbd>@CacheResult</kbd> enables you to put the result in the cache when the method succeeds, but to not use
               the cached data if it is already in the cache. Therefore, if this service is chained
               with a fallback service reading the data from the cache, it will correctly implement
               the failover based on the cached data.
            </p>
            
            <p>However, note that there is a trick here—you need to use the right cache name and
               key. To do so, don't hesitate to just use another method relying on JCache as well:
            </p>
            <pre>@ApplicationScoped<br/>public class QuoteServiceCache {<br/><br/>    @CacheResult(cacheName =<br/>    "com.company.quote.QuoteServiceClient.fetch()")<br/>    public Data fetch() {<br/>        return null;<br/>    }<br/>}</pre>
            <p>The implementation is quite straightforward; it returns <kbd>null</kbd> to represent it doesn't have any data if it is not already in the cache. An alternative
               implementation could be to throw an exception, depending on the caller behavior you
               want to provide. Then, to ensure that we use the same cache as the previous primary
               service, we name the cache with the previous method's name. Here, we used the default
               name for the primary service and set this name to the secondary service but you can
               also use a more business-oriented cache name through the <kbd>cacheName</kbd> configuration that we saw in the <a href="8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml">Chapter 6</a>, <em>Be Lazy; Cache Your Data</em>.
            </p>
            
            <p>Now, if we go back to the caching-first solution, reversing the primary and secondary
               sources, we can implement it a bit differently. If the cache is the source, we can
               still use the CDI integration, but the provisioning of the cache (which is the secondary
               source now) can be done through a native JCache mechanism. Concretely, our service
               can look as follows:
            </p>
            <pre>@ApplicationScoped<br/>public class QuoteServiceClient {<br/>    @Inject<br/>    private Client client;<br/><br/>    @CacheResult<br/>    public Data fetch() {<br/>        return client.....get(Data.Class);<br/>    }<br/>}</pre>
            <p>This is the standard way of using it, but there is an alternative way to do it that
               would also work better with manual cache handling—that is, without CDI integration.
               Instead of using the method as a fallback and cache its result, we programmatically
               set the way the cache is lazily provisioned when configuring the cache. In this case,
               our service can become the following:
            </p>
            <pre>@ApplicationScoped<br/>public class QuoteServiceClient {<br/>    @CacheResult<br/>    public Data fetch() {<br/>        return null;<br/>    }<br/>}</pre>
            <p>Yes, you saw it correctly: we don't even implement the loading logic into the service!
               So where does it go? This service will trigger <kbd>cache.get(...)</kbd>, so we need to inject our data when <kbd>get()</kbd> is called if the data is not already available. To do so, we can use the <kbd>CacheLoader</kbd> API, which is initialized on the cache itself.
            </p>
            
            <p>To configure the cache, you can use a custom <kbd>CacheResolver</kbd> (see the previous chapter for more details), which will set <kbd>CacheLoader</kbd> into the cache configuration:
            </p>
            <pre><span>new </span>MutableConfiguration&lt;&gt;()<br/> .setCacheLoaderFactory(<span>new </span>FactoryBuilder.SingletonFactory&lt;&gt;(<span>new </span>QuoteLoader()))</pre>
            <p>The loader implementation can now be the following:</p>
            <pre><span>@ApplicationScoped<br/></span><span>public class </span>QuoteLoader <span>implements </span>CacheLoader&lt;QuoteGeneratedCacheKey, Quote&gt; {<br/>    <span>@Inject<br/></span><span>    </span><span>private </span>QuoteClient <span>client</span>;<br/><br/>    <span>@Override<br/></span><span>    </span><span>public </span>Quote load(QuoteGeneratedCacheKey generatedCacheKey) <span>throws<br/>    </span>CacheLoaderException {<br/>        <span>return client.load(key.extractSymbol())</span>;<br/>    }<br/><br/>    <span>@Override<br/></span><span>    </span><span>public </span>Map&lt;QuoteGeneratedCacheKey, Quote&gt; loadAll(<span>final </span>Iterable&lt;?<br/> <span>extends<br/>    Quote</span>GeneratedCacheKey&gt; iterable) <span>throws </span>CacheLoaderException {<br/>        <span>return </span>StreamSupport.<span>stream</span>(<br/>            Spliterators.<span>spliteratorUnknownSize</span>(iterable.iterator(),<br/>            Spliterator.<span>IMMUTABLE</span>), <span>false</span>)<br/>                .collect(<span>toMap</span>(<span>identity</span>(), this::load));<br/>    }<br/>}</pre>
            <p>The <kbd>loadAll</kbd> method just delegates to the <kbd>load</kbd> method, so it is not very interesting, but in some cases you can bulk-load multiple
               values at once and it makes sense to have a different implementation. The <kbd>load</kbd> method delegates the loading to a CDI bean. We can consider that we call the remote
               service here without any failover.
            </p>
            
            <p>The important point for this solution is to have a custom <kbd>GeneratedKey</kbd> key to be able to unwrap it and extract the business key (<kbd>extractSymbol()</kbd> in the previous example) to be able to execute the actual business. As a quick reminder
               of the previous chapter, <kbd>GeneratedKey</kbd> is the key deduced from the method signature in JCache CDI integration, so <span>you need to ensure you can work with such a key </span>using <kbd>@CacheResult</kbd>. As we saw in <a href="8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml">Chapter 6</a><span>, </span><em>Be Lazy; Cache Your Data</em>, using a custom <kbd>CacheKeyGenerator</kbd> allows you to fulfill this requirement for this solution.
            </p>
            
            <p>In terms of usage, when should you use <kbd>CacheLoader</kbd> instead of a method implementation that behaves as an implicit cache loader after
               all? The cache loader makes more sense when you don't use the CDI integration because,
               in such a case, you manipulate a more natural key (such as a string for symbols) and
               get the same behavior:
            </p>
            <pre>Cache&lt;String, Quote&gt; quotes = getNewQuoteCacheWithLoader();<br/>Quote pckt = quotes.get("PCKT");</pre>
            <p>If the cache is set up to load the data from a remote service if not already present
               in the cache, then the second line of this snippet will call the remote service and
               transparently initialize the cache with the data.
            </p>
            
            <div class="packt_tip">This kind of caching usage also works in the case of a remote service, which is rate-limited.
               It will allow you to rely on its data more than you would be allowed to without a
               cache. For instance, if the service accepts only 1,000 requests per minute with your
               credentials, you can, with the cache, call it 10,000 times per minute.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Circuit breaker</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The circuit breaker involves allowing the application to disable a code path if it
               is known or estimated as failing.
            </p>
            
            <p>For instance, if you call a remote service and this service has failed 10 times already,
               then you can say: <em>don't call this service anymore for 5 minutes</em>. The main idea is to bypass errors when possible.
            </p>
            
            <p>A circuit breaker generally has three states:</p>
            
            <ul>
               
               <li><strong>CLOSED</strong>: The system is considered to be working, so use it (default case).
               </li>
               
               <li><strong>OPEN</strong>: The system is considered not working, so bypass it.
               </li>
               
               <li><strong>HALF-OPEN</strong>: The system must be reevaluated. Try an invocation: if it fails, go back to the OPEN
                  state; otherwise, go to the CLOSED state.
               </li>
               
            </ul>
            
            <p>Then, all the conditions to go from a state to the other are configurable. For instance,
               what triggers a CLOSED state depends on the way you configure it (it can be an exception,
               an HTTP status, a timeout, and so on). The same applies for when the system enters
               into the HALF-OPEN state—it can be a timeout, a number of requests, and so on.
            </p>
            
            <p>There are multiple available implementations of circuit breaks but the most known
               ones for Java EE are in these projects:
            </p>
            
            <ul>
               
               <li>Hystrix</li>
               
               <li>Failsafe</li>
               
               <li>Microprofile fault-tolerant specification</li>
               
               <li>commons-lang3 project.</li>
               
            </ul>
            
            <p>Using a circuit breaker is important for the system health to ensure that your system
               is always healthy even if one functionality is not. However, it can also be used for
               performance because it will keep them under control if the system starts failing and
               avoid a domino effect where each connection between a failing system and another system
               implies the other system to fail too. To ensure that the impact of the circuit breaker
               is as expected, you need to associate two solutions with it:
            </p>
            
            <ul>
               
               <li>A failover solution to ensure that your system behaves correctly (as much as possible,
                  since it is not always feasible)
               </li>
               
               <li>A monitoring solution to ensure that you properly report that you are no more fully
                  functional to let the support/operation team efficiently work and enable your circuit
                  breaker to automatically recover once the failing system is fixed
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Bulk head</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Bulk head is a pattern designed to ensure that no part of the system deeply impacts
               other system areas. The name comes from a common solution used in ships to ensure
               that if there is a hole in the hull, it doesn't lead to the boat sinking.
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img src="assets/c266fe64-4177-4da8-961b-1552fcdb04ce.jpg"/></div>
            
            <p>Here, for instance, the second area has a hole and the water is coming into the section
               but the boat will not sink, as the other sections are isolated.
            </p>
            
            <div class="packt_tip">The Titanic used this technique but the isolation was not fully done from down to
               top for passengers and crew members comfort. And we all know the outcome of that choice.
               That is why, if you go with isolation, it is important to make sure that it is complete;
               otherwise better not do anything.
            </div>
            
            <p>What does it mean for an application? It means that each service that can sink your
               application should be isolated from the other ones. This isolation is mainly about
               the execution and, therefore, the execution environment of the service invocation.
               Concretely, it is generally about which pool (thread, connection, and so on) and which
               context to load. To be able to isolate services, you need to be able to identify which
               service you are calling. If we take our quote manager application, we can identify
               the <em>quote finder</em> service, which can be isolated from the <em>quote update</em> service, for instance. This is a business criteria for the isolation, but in practice,
               you will often want to go further to isolate the execution of your services, including
               the use of the tenants. It is not rare to want to use a different pool for different
               tenants. This is actually often even related to different contracts clauses.
            </p>
            
            <p>To illustrate this concept in a Java EE application, we will update our <kbd>QuoteResource#findId</kbd> method to apply this pattern. The first step will be to isolate the invocation from
               the servlet container context/threads. To do so, we will make the method asynchronous,
               using the JAX-RS <kbd>@Suspended</kbd> API and a Java EE concurrency utilities thread pool:
            </p>
            <pre><span>@Resource</span>(name = <span>"threads/quote-manager/quote/findById"</span>)<br/><span>private </span>ManagedExecutorService <span>findByIdPool</span>;<br/><br/><span>@GET<br/></span><span>@Path</span>(<span>"{id}"</span>)<br/><span>public void </span>findById(<span>@PathParam</span>(<span>"id"</span>) <span>final long </span>id,<br/>                     <span>@Suspended </span><span>final </span>AsyncResponse response) {<br/>    <span>findByIdPool</span>.execute(() -&gt; {<br/>        <span>final </span>Optional&lt;JsonQuote&gt; result = <span>quoteService</span>.findById(<span>id</span>)<br/>                .map(quote -&gt; {<br/>                    <span>final </span>JsonQuote json = <span>new </span>JsonQuote();<br/>                    json.setId(quote.getId());<br/>                    json.setName(quote.getName());<br/>                    json.setValue(quote.getValue());<br/>                    json.setCustomerCount(<span>ofNullable<br/></span>                    (quote.getCustomers())<br/>                    .map(Collection::size).orElse(<span>0</span>));<br/>                    <span>return </span>json;<br/>                });<br/>        <span>if </span>(result.isPresent()) {<br/>            <span>response</span>.resume(result.get());<br/>        } <span>else </span>{<br/>            <span>response</span>.resume(<span>new<br/>            </span>WebApplicationException(Response.Status.<span>NO_CONTENT</span>));<br/>        }<br/>    });<br/>}</pre>
            <p>Here, we created and configured, into the container, a dedicated pool for a part of
               the application called <kbd>threads/quote-manager/quote/findById</kbd>. The <kbd>findById</kbd> method executes its original logic in a task submitted to this dedicated pool. Using
               the JAX-RS asynchronous API, we manually <kbd>resume</kbd> the request response, since the container doesn't handle the execution of the logic
               anymore, but we do it ourselves.
            </p>
            
            <p>This implementation works only if the thread pool has a maximum size to ensure that
               the execution is controlled. If you use an unbounded thread pool, this will not help
               control your application at all.
            </p>
            
            <p>There are other ways to implement a bulkhead not relying on different threads, such
               as using <kbd>Semaphore</kbd> as we saw in the threading chapter, but they don't allow you to isolate the application
               logic from the container threads. Thus, it can have side-effects on the overall application
               (or even cross-applications if you use the same HTTP container thread pool). The main
               advantage of using a no thread-related implementation is that it is generally faster
               even if it doesn't isolate the executions as well as the thread-based implementation.
               Here again, make sure to benchmark your application to know which implementation best
               fits your case.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Timeouts</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The last and very important criteria to ensure control over the performance and to
               ensure that the performance is bounded (your application doesn't start being very
               slow) is related to timeouts.
            </p>
            
            <p>An application has timeouts everywhere even if you don't always see them:</p>
            
            <ul>
               
               <li>The HTTP connector, or any network connector in general, has timeouts to force the
                  release of clients connected for too long.
               </li>
               
               <li>Databases generally have timeouts as well. It can be a client-side (network) timeout
                  or a server-side setting. For instance, MySQL will cut any connection that lasts for
                  more than 8 hours by default.
               </li>
               
               <li>Thread pools can handle timeouts if an execution is too long.</li>
               
               <li>The JAX-RS client supports vendor-specific timeout configuration to avoid blocking
                  the network later.
               </li>
               
            </ul>
            
            <p>Configuring timeouts enables you to ensure that if something starts being wrong in
               your system, including a remote system being slow or unresponsive, you will be able
               to respond in a correct (or, at least, bounded) duration. Of course, you will cumulate
               all the timeouts in the worst case, considering you use them synchronously and not
               concurrently, but at least, you will know the maximum duration a request can take
               in your system.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Timeouts for code without timeouts</h1>
            
         </header>
         
         
         <article>
            
            
            <p>A trick for adding timeouts to methods that weren't designed to handle timeouts is
               to use a thread pool. A thread pool allows you to execute tasks and wait for them
               for a certain duration. Of course, it means that you will block the calling thread,
               but you will block it for a certain amount of time:
            </p>
            <pre><span>return threadPool</span>.submit(() -&gt; {<br/>    // execute your logic<br/>    }).get(<span>10</span>, TimeUnit.<span>SECONDS</span>);</pre>
            <p>This code will compute some value or throw <kbd>TimeoutException</kbd> if it lasts for more than 10 seconds. Wrapping any code inside such a block will
               allow you to handle a timeout for any sort of code. However, this doesn't mean that
               the wrapped code ends after 10 seconds; it just means that the caller doesn't wait
               for it anymore. The task, however, is still submitted and can last forever taking
               a thread. To stop you from needing to keep a reference on <kbd>Future</kbd>, the <kbd>submit</kbd> method will return and cancel the task, allowing you to interrupt the execution thread:
            </p>
            <pre>Future&lt;?&gt; task = <span>null</span>;<br/><span>try </span>{<br/>    task = threadPool.submit(() -&gt; {<br/>        // some logic<br/>    });<br/>    task.get(10, TimeUnit.<span>SECONDS</span>);<br/>} <span>catch </span>(<span>final </span>TimeoutException te) {<br/>    <span>if </span>(task != <span>null</span>) {<br/>        task.cancel(<span>true</span>);<br/>    }<br/>}</pre>
            <p>Now, if we get a timeout, we can cancel the running task, so as to not leak tasks,
               and potentially fill the thread pool even if we have timeouts.
            </p>
            
            <div class="packt_tip">If you want to handle some failover on a timeout, you can add it in a block that catches <kbd>TimeoutException</kbd>.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Summary</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In this chapter, we saw that properly handling faults in an application is the key
               to ensuring that you can still control the response time of your system and keep it
               under control in any circumstance. This is true for any system but with the spreading
               of microservices, it is even more true for systems using this architecture.
            </p>
            
            <p>Well, defining how your Java EE application is deployed and what is needed to ensure
               you control all the potential failures of your system is a full time job; we often
               forget to work on the <em>main</em> codepath of our applications, but doing so ensures that the production deployment
               and behavior are sane.
            </p>
            
            <p>This chapter showed you some common patterns used to handle fault tolerance more or
               less transparently and reliably.
            </p>
            
            <p>Logging is another important factor when issues start popping up in a system. It will
               allow you to investigate what happened and identify the issue to fix it. However,
               logging too much or without reflection can be very costly. This is what our next chapter
               will be about: ensure that you are correctly leveraging your logging regarding the
               performance factor.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   </body></html>