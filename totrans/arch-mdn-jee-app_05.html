<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Container and Cloud Environments with Java EE</h1>
                </header>
            
            <article>
                
<p>The last years have shown a lot of interest in container as well as cloud technology. The vast majority of companies building software is at least considering migrating environments to these modern approaches. In all of my recent projects these technologies have been a point of discussion. Especially, introducing container orchestration technologies greatly affects the way how applications are run.</p>
<p>What are the benefits of container technologies? And why should companies care about the cloud? It seems a lot of these concerns are used as buzzwords, as a <em>silver bullet</em> approach. This chapter will examine the motivations behind these technologies. We will also see if and how the Java EE platform is ready for this new world.</p>
<p>This chapter will cover:</p>
<ul>
<li>How infrastructure as code supports operations</li>
<li>Container technologies and orchestration</li>
<li>Why especially Java EE fits these technologies</li>
<li>Cloud platforms and their motivations</li>
<li>12-factor, cloud native enterprise applications</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivations and goals</h1>
                </header>
            
            <article>
                
<p>What are the motivations behind containers, container orchestration, and cloud environments? Why do we see such momentum in this area?</p>
<p>Traditionally, enterprise application deployment worked something like the following. Application developers implemented some business logic and built the application into a packaged artifact. This artifact was deployed manually on an application server that was managed manually as well. During deployment or reconfiguration of the server, the application usually faced a downtime.</p>
<p>Naturally, this approach is a rather high-risk process. Human, manual tasks are error-prone and are not guaranteed to be executed in the same manner each and every time. Humans are rather bad at executing automated, repetitive work. Processes such as installing application servers, operating systems and servers in general, require precise documentation, especially for future reproducibility.</p>
<p>In the past, tasks for operation teams typically were ordered using a ticket system and performed manually. By doing so, installation and configuration of servers held the risk of transforming the system into a non-reproducible state. Setting up a new environment identical to the current one required a lot of manual investigation.</p>
<p>Operational tasks need to be automated and reproducible. Installing a new server, operating system or runtime should always execute in exactly the same manner. Automated processes not only speed up the execution but introduce transparency, revealing which precise steps have been executed. Reinstalling environments should produce exactly the same runtime including all configuration and setup as before.</p>
<p>This also includes deployment and configuration of the application. Instead of manually building and shipping applications, Continuous Integration servers are in charge of building software in an automated, reliable, and reproducible way. CI servers act as <em>golden source of truth</em> for software builds. The artifacts produced there are deployed on all involved environments. A software artifact is built once, on the Continuous Integration server, and then verified in integration and end-to-end tests, until it ends up in production. The same application binary that is deployed to production is therefore reliably tested upfront.</p>
<p>Another very important aspect is to be explicit in the software versions that are being used. This includes all used software dependencies, from the application server and Java runtime, down to the operating system and its binaries. Rebuilding or reinstalling software should result in exactly the same state each and every time. Software dependencies are a complex subject which comes with a lot of possibilities for potentials errors. Applications are tested to work properly on specific environments with specific configurations and dependencies. In order to guarantee that the application will work as expected, it is shipped in exactly that configuration that has been verified before.</p>
<p>This aspect also implies that test and staging environments which are used to verify the application's behavior should be as similar to production as possible. In theory this constraint sounds reasonable. From experience the used environments vary quite a lot from production in terms of software versions being used, network configuration, databases, external systems, number of server instances, and so on. In order to test applications properly these differences should be erased as much as possible. In section <em>Containers</em> we will see how container technology supports us here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Infrastructure as code</h1>
                </header>
            
            <article>
                
<p>A logical conclusion to enable reproducible environments is to make use of <strong>infrastructure as code</strong> (<strong>IaC</strong>). The idea is that all required steps, configuration, and versions are explicitly defined as code. These code definitions are directly used to configure the infrastructure. Infrastructure as code can be implemented in a procedural form, such as scripts, or in a declarative way. The latter approach specifies the desired target state and is executed using additional tooling. No matter which approach is preferred, the point is that the whole environment is specified as code, being executed in an automated, reliable, and reproducible way, always producing the same results.</p>
<p>In any way, the approach implies that the manual steps are kept to a minimum. The easiest form of infrastructure as code are shell scripts. The scripts should be executed from soup to nuts without human involvement. The same holds true for all IaC solutions.</p>
<p>Naturally the responsibility of installing and configuring environments moves from an operations team more toward developers. Since the development team sets certain requirements on the required runtime it makes sense for all engineering teams to work together. This is the idea behind the DevOps movement. In the past the mindset and method of operating too often was that application developers implemented software and literally passed the software and responsibilities toward operations - without further involvement on their side. Potential errors in production primarily concerned the operations team. This unfortunate process not only leads to tensions between engineering teams but ultimately lower quality. However, the overall goal should be to deliver high quality software that fulfills a purpose.</p>
<p>This goal requires the accountability of application developers. By defining all required infrastructure, configuration, and software as code, all engineering teams naturally move together. DevOps aims toward accountability of the software team as a whole. Infrastructure as code is a prerequisite which increases reproducibility, automation, and ultimately software quality.</p>
<p>In the topic <em>Containers</em> and <em>Container orchestration frameworks</em>, we will see how the presented technologies implement IaC.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stability and production readiness</h1>
                </header>
            
            <article>
                
<p>The practices of Continuous Delivery include what needs to be done in order to increase the quality and value of the software. This includes the stability of the application. Reconfiguring and redeploying software does not have to result in any downtime. New features and bugfixes do not have to be shipped exclusively during maintenance windows. Ideally the enterprise software can continuously improve and move forward.</p>
<p>A <em>zero downtime</em> approach involves a certain effort. In order to avoid an application being unavailable, at least one other instance of the software needs to be present at a time. A load balancer or proxy server upfront needs to direct the traffic to an available instance. <em>Blue-green</em> deployments make use this technique:</p>
<div style="padding-left: 60px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/6e8d1123-39ac-485a-971c-22ecd5258517.png"/></div>
<p>The application instances including their databases are replicated and proxied by a load balancer. The involved applications typically represent different software versions. Switching the traffic from the <em>blue</em> to the <em>green</em> path and vice versa instantly changes the version, without any downtime. Other forms of blue-green deployments can include scenarios of multiple application instances that are all configured to use the same database instance.</p>
<p>This approach obviously does not have to be realized using some shiny new technology. We have seen blue-green deployments that enable zero-downtime in the past using home-grown solutions. However, modern technologies support these techniques increasing stability, quality, and production-readiness out of the box without much engineering effort required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Containers</h1>
                </header>
            
            <article>
                
<p>The last years have shown a lot of interest in <strong>Linux container</strong> technology. Technically this approach is not that new. Linux operating systems such as <strong>Solaris</strong> supported containers a long time ago. However, <strong>Docker</strong> made a the breakthrough in this technology by providing features to build, manage and ship containers in a uniform way.</p>
<p>What is the difference between containers and <strong>virtual machines</strong> (<strong>VMs</strong>) and what makes containers that interesting?</p>
<p>Virtual machines act like a computer in a computer. They allow the runtime to be easily managed from the outside such as creating, starting, stopping, and distributing machines in a fast and ideally automated way. If new servers need to be setup, a blueprint, an image, of the required type can be deployed without installing software from scratch every time. Snapshots of running environments can be taken to easily backup the current state.</p>
<p>In many ways containers behave like virtual machines. They are separated from the host as well as other containers, run in their own network with their own file system and potentially own resources. The difference is that virtual machines run on a hardware abstraction layer, emulating a computer including operating system, whereas containers run directly in the host's kernel. Unlike other kernel processes, containers are separated from the rest of the system using operating system functionality. They manage their own file system. Therefore, containers behave like separate machines but with native performance without the overhead of an abstraction layer. The performance of virtual machines is naturally decreased by their abstraction. Whereas virtual machines provide full flexibility in choosing operating systems, containers will always run in the same kernel and therefore in the same version as the host operating system. Containers therefore do not ship their own Linux kernel and can be minimized to their required binaries.</p>
<p>Container technologies such as Docker provide functionality to build, run, and distribute containers in a uniform way. Docker defines building container images as IaC which again enables automation, reliability, and reprocibility. Dockerfiles define all the steps that are required to install the application including its dependencies, for example, an application container and the Java runtime. <span>Each step in the Dockerfile corresponds to a command that is executed at image build time.</span> Once a container is started from an image it should contain everything which is required to fulfill its task.</p>
<p>Containers usually contain a single Unix process which represents a running service, for example an application server, a web server, or a database. If an enterprise system consists of several running servers, they run in individual containers.</p>
<p>One of the biggest advantages of Docker containers is that they make use of a <strong>copy-on-write</strong> file system. Every build step, as well as every running container later on, operates on a layered file system, which does not change its layers but only adds new layers on top. Built images therefore comprise multiple layers.</p>
<p>Containers that are created from images are always started with the same initial state. Running containers potentially modify files as new, temporary layers in the file system, which are discarded as soon as the containers are stopped. By default, Docker containers are therefore stateless runtime environments. This encourages the idea of reproducibility. Every persistent behavior needs to be defined explicitly.</p>
<p>The multiple layers are beneficial when rebuilding and redistributing images. Docker caches intermediate layers and only rebuilds and retransmits what has been changed.</p>
<p>For example, an image build may consist of multiple steps. System binaries are added first, then the Java runtime, an application server, and finally our application. When changes are made to the application and a new build is required, only the last step is re-executed; the previous steps are cached. The same is true for transmitting images over the wire. Only the layers that have been changed and that are not yet existent on the target registry, are actually retransmitted.</p>
<p>The following illustrates the layers of a Docker image and their individual distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img height="267" width="482" src="assets/b978dac1-a27b-435c-8c26-dfda3b4ad020.png"/></div>
<p>Docker images are either built from scratch, that is from an empty starting point, or built upon an existing base image. There are tons of base images available, for all major Linux distributions containing package managers, for typical environment stacks as well as for Java-based images. Base images are a way to build upon a common ground and provide basic functionality for all resulting images. For example, it makes sense to use a base image including a Java runtime installation. If this image needs to be updated, for example, to fix security issues, all dependent images can be rebuilt and receive the new contents by updating the base image version. As said before, software builds need to be repeatable. Therefore we always need to specify explicit versions for software artifacts such as images.</p>
<p>Containers that are started from previously built Docker images need access to these images. They are distributed using Docker registries such as the publicly available DockerHub or company-internal registries to distribute own images. Locally built images are pushed to these registries and retrieved on the environments that will start new containers later on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java EE in the container</h1>
                </header>
            
            <article>
                
<p>As it turns out the approach of a layered file system matches Java EE's approach of separating the application from the runtime. Thin deployment artifacts only contain the actual business logic, the part which changes and which is rebuilt each and every time. These artifacts are deployed onto an enterprise container which does not change that often. Docker container images are built step-by-step, layer-by-layer. Building an enterprise application image includes an operating system base image, a Java runtime, an application server and finally the application. If only the application layer changes, only this step will have to be re-executed and retransmitted - all the other layers are touched only once and then cached.</p>
<p>Thin deployment artifacts leverage the advantages of layers since only a matter of kilobytes has to be rebuilt and redistributed, respectively. Therefore, zero-dependency applications is the advisable way of using containers.</p>
<p>As seen in the previous chapter, it makes sense to deploy one application per application server. Containers execute a single process which in this case is the application server containing the application. The application server therefore needs to run on a dedicated container that is included in the container as well. Both the application server and the application are added at image build time. Potential configuration, for example regarding datasources, pooling, or server modules, is also made during build time, usually by adding custom configuration files. Since the container is owned by the single application these components are configured without affecting anything else.</p>
<p>Once a container is started from the image it should already contain everything that is required to fulfill its job. The application as well as all required configuration must already be present. Therefore, applications are not deployed on a previously running container anymore but added during the image build time, to be present at container runtime. This is usually achieved by placing the deployment artifact into the container's auto-deployment directory. As soon as the configured application server starts, the application is deployed.</p>
<p>The container image is built only once and then executed on all the environments. Following the idea of reproducible artifacts before, the same artifacts that run in production have to be tested upfront. Therefore the same Docker image that has been verified will be published to production.</p>
<p>But what if applications are configured differently in various environments? What if different external systems or databases need to be communicated with? In order to not interfere with several environments, at least the used database instances will differ. Applications shipped in containers are started from the same image but sometimes still need some variations.</p>
<p>Docker offers the possibility of changing several aspects of running containers. This includes networking, adding volumes, that is, injecting files and directories that reside on the Docker host, or adding Unix environment variables. The environment differences are added by the container orchestration from outside of the container. The images are only built once for a specific version, used and potentially modified in different environments. This brings the big advantage that these configuration differences are not modeled into the application rather than managed from the outside. The same is true for networking and connecting applications and external systems, which we will see in the coming sections.</p>
<p>Linux containers, by the way, solve the business-politically motivated issue of shipping the application together with the implementation in a single package for the reason of flexibility. Since containers include the runtime and all dependencies required, including the Java runtime, the infrastructure only has to provide a Docker runtime. All used technology including the versions are the responsibility of the development team.</p>
<p>The following code snippet shows the definition of a <kbd>Dockerfile</kbd> building an enterprise application <kbd>hello-cloud</kbd> onto a <strong>WildFly</strong> base image.</p>
<pre>FROM jboss/wildfly:10.0.0.Final

COPY target/hello-cloud.war /opt/jboss/wildfly/standalone/deployments/</pre>
<p>The <kbd>Dockerfile</kbd> specifies the <kbd>jboss/wildfly</kbd> base image in a specific version which already contains a Java 8 runtime and the WildFly application server. It resides in the application's project directory, pointing to the <kbd>hello-cloud.war</kbd> archive which was previously built by a Maven build. The WAR file is copied to WildFly's auto-deployment directory and will be available at that location at container runtime. The <kbd>jboss/wildfly</kbd> base image already specifies a run command, how to run the application server, which is inherited by the Dockerfile. Therefore it doesn't have to specify a command as well. After a Docker build the resulting image will contain everything from the <kbd>jboss/wildfly</kbd> base image including the <em>hello-cloud</em> application. This matches the same approach of installing a WildFly application server from scratch and adding the WAR file to the auto-deployment directory. When distributing the built image, only the added layer including the thin WAR file needs to be transmitted.</p>
<p>The deployment model of the Java EE platform fits the container world. Separating the application for the enterprise container leverage the use of copy-on-write file systems, minimizing the time spent on builds, distribution, or deployments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container orchestration frameworks</h1>
                </header>
            
            <article>
                
<p>Let's go up one abstraction layer from containers. Containers include everything required to run specific services as stateless, self-contained artifacts. However, the containers need to be orchestrated to run in the correct network, being able to communicate with other services and being started with the correct configuration, if required. The straightforward approach is to develop home-grown scripts that run the required containers. However, in order to realize a more flexible solution that also enables production-readiness such as zero-downtime, the use of container orchestration frameworks is advisable.</p>
<p>Container orchestration frameworks such as <strong>Kubernetes</strong>, <strong>DC/OS</strong> or <strong>Docker Compose</strong> are not only responsible to run containers, but to orchestrate, connect and configure them appropriately. The same motivations and principles apply that are true for container technologies as well: automation, reproducibility and IaC. Software engineers define the desired target state as code and let the orchestration tool reliably setup the environments as required.</p>
<p>Before going into a specific orchestration solution, let's have a closer look at the rough concepts.</p>
<p>Orchestration frameworks enable us to connect multiple containers together. This usually involves service lookup using logic names via DNS. If multiple physical hosts are used, the framework resolves IP addresses over these nodes. Ideally an application running in a container just connects to an external system using a logical service name that is resolved by the container orchestration. For example, a car manufacturing application using the <em>vehicle</em> database connects using the <kbd>vehicle-db</kbd> hostname. This hostname is then resolved via DNS, depending on the environment which the application runs in. Connecting via logical names reduces the required configuration in the application code, since the configured connection is always the same. The orchestration just connects the desired instance.</p>
<p>This is true for all offered systems. Applications, databases, and other servers are abstracted to logical service names which are accessed and resolved during runtime.</p>
<p>Configuring containers depending on their environment is another aspect that orchestration frameworks solve. In general it's advisable to reduce the required configuration in applications. However, there are cases where some configuration effort is required. It is the framework's responsibility to provide container configuration by dynamically injecting files or environment variables depending on the circumstances.</p>
<p><span>The production-readiness features that some of the container orchestration frameworks offer represent o</span>ne of their biggest advantages. Ongoing development of an application triggers new project builds and result in new container image versions. The running containers need to be replaced by containers that are started from these new versions. In order to avoid downtime the container orchestration swaps the running containers using a zero-downtime deployment approach.</p>
<p>In the same way, container orchestration makes it possible to increase the workload by scaling up the number of container instances. In the past, certain applications ran on multiple instances simultaneously. If the number of instances needed to be increased, more application servers had to be provisioned. In a container world the same goal is achieved by simply starting more of the stateless application containers. The developers increase the configured number of container replicas; the orchestration framework implements this change by starting more container instances.</p>
<p>In order to run containers in production some orchestration aspects have to be considered. Experience shows that some companies tend to build their own solutions rather than using de facto standard technology. However, container orchestration frameworks already solve these issues well and it is highly advisable to at least consider them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Realizing container orchestration</h1>
                </header>
            
            <article>
                
<p>We've now seen which challenges container orchestration framework tackle. This section will show you the core concepts of <strong>Kubernetes</strong>, a solution originally developed by Google to run their workloads. At the time of writing this book Kubernetes has a enormous momentum and is also the basis for other orchestration solutions such as <strong>OpenShift</strong> by RedHat. I chose this solution because of its popularity but also because I believe that it does the job of orchestration very well. However, the important point is less about comprehending the chosen technology rather than the motivations and concepts behind it.</p>
<p>Kubernetes runs and manages Linux containers in a cluster of nodes. The Kubernetes master node orchestrates the worker nodes which do the actual work, that is, to run the containers. The software engineers control the cluster using the API provided by the master node, via a web-based GUI or command-line tool.</p>
<p>The running cluster consists of so-called resources of a specific type. The core resource types of Kubernetes are <strong>pods</strong>, <strong>deployments</strong>, and <strong>services</strong>. A pod is an atomic workload unit, running one or more Linux container. This means the application runs in a pod.</p>
<p>The pods can be started and managed as standalone, single resources. However, it makes a lot of sense to not directly specify separate pods but to define a deployment, which encapsulates and manages running pods. Deployments enable the functionality that provide production-readiness such as upscaling and downscaling of pods or rolling updates. They are responsible for reliably running our applications in the specified versions.</p>
<p>A system defines services in order to connect to running applications from outside of the cluster or within other containers. The services provide the logical abstraction described in the last section that embraces a set of pods. All pods that run a specific application are abstracted by a single service which directs the traffic onto active pods. The combination of services routing to active pods and deployments managing the rolling update of versions enables zero-downtime deployments. Applications are always accessed using services which direct to corresponding pods.</p>
<p>All core resources are unique within a Kubernetes <strong>namespace</strong>. Namespaces encapsulate aggregates of resources and can be used to model different environments. For example, services that point to external systems outside of the cluster can be configured differently in different namespaces. The applications that use the external systems always use the same logical service name which are directed to different endpoints.</p>
<p>Kubernetes supports resources definition as IaC using JSON or YAML files. The YAML format is a human-readable data serialization format, a superset of JSON. It became the de facto standard within Kubernetes.</p>
<p>The following code snippet shows the definition of a service of the <kbd>hello-cloud</kbd> application:</p>
<pre>---
kind: <strong>Service</strong>
apiVersion: v1
metadata:
  name: <strong>hello-cloud</strong>
spec:
  selector:
    app: hello-cloud
  ports:
    - port: <strong>8080</strong>
---</pre>
<p>The example specifies a service which directs traffic on port <kbd>8080</kbd> toward <kbd>hello-cloud</kbd> pods that are defined by the deployment.</p>
<p>The following shows the <kbd>hello-cloud</kbd> deployment:</p>
<pre>---
kind: <strong>Deployment</strong>
apiVersion: apps/v1beta1
metadata:
  name: <strong>hello-cloud</strong>
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hello-cloud
    spec:
      containers:
      - name: hello-cloud
        image: <strong>docker.example.com/hello-cloud:1</strong>
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /
            port: 8080
        readinessProbe:
          httpGet:
            path: /hello-cloud/resources/hello
            port: 8080
      restartPolicy: Always
---</pre>
<p>The deployment specifies one pod from the given template with the provided Docker image. As soon as the deployment is created Kubernetes tries to satisfy the pod specifications by starting a container from the image and testing the container's health using the specified probes.</p>
<p>The container image <kbd>docker.example.com/hello-cloud:1</kbd> includes the enterprise application which was built and distributed to a Docker registry earlier.</p>
<p>All these resource definitions are applied to the Kubernetes cluster by either using the web-based GUI or the CLI.</p>
<p>After creating both the deployment and the service, the <em>hello-cloud</em> application is accessible from within the cluster via the service. To be accessed from the outside of the cluster a route needs to be defined, for example using an <strong>ingress</strong>. Ingress resources route traffic to services using specific rules. The following shows an example ingress resource that makes the <kbd>hello-cloud</kbd> service available:</p>
<pre>---
kind: <strong>Ingress</strong>
apiVersion: extensions/v1beta1
metadata:
  name: <strong>hello-cloud</strong>
spec:
  rules:
  - host: <strong>hello.example.com</strong>
    http:
      paths:
      - path: <strong>/</strong>
        backend:
          serviceName: <strong>hello-cloud</strong>
          servicePort: <strong>8080</strong>
---</pre>
<p>These resources now specify the whole application, which is deployed onto a Kubernetes cluster, accessible from the outside and abstracted in a logical service inside of the cluster. If other applications need to communicate with the application, they can do so via the Kubernetes-internal, resolvable <kbd>hello-cloud</kbd> DNS hostname and port <kbd>8080</kbd>.</p>
<p>The following diagram shows an example setup of the <em>hello-cloud</em> application with a replica of three pods that runs in a Kubernetes cluster of two nodes:</p>
<div style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign"><img height="308" width="390" src="assets/cb7873c2-fc69-4174-a05d-7ccd2a9bfaca.png"/></div>
<p>Besides service lookup using logical names, some applications still need additional configuration. Therefore Kubernetes as well as other orchestration technology has the possibility of inserting files and environment variables into the container dynamically at runtime. The concept of <strong>config maps</strong>, key-value-based configuration is used for this. The contents of config maps can be made available as files, dynamically mounted into a container. The following defines an example config map, specifying the contents of a properties file:</p>
<pre>---
kind: <strong>ConfigMap</strong>
apiVersion: v1
metadata:
  name: <strong>hello-cloud-config</strong>
data:
  <strong>application.properties</strong>: |
    <strong>hello.greeting=Hello from Kubernetes
    hello.name=Java EE</strong>
---</pre>
<p>The config map is being used to mount the contents as files into containers. The config map's keys will be used as file names, mounted into a directory, with the value representing the file contents. The pod definitions specify the usage of config maps mounted as volumes. The following shows the previous deployment definition of the <em>hello-cloud</em> application, using <kbd>hello-cloud-config</kbd> in a mounted volume:</p>
<pre>---
kind: Deployment
apiVersion: apps/v1beta1
metadata:
  name: hello-cloud
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hello-cloud
    spec:
      containers:
      - name: hello-cloud
        image: docker.example.com/hello-cloud:1
        imagePullPolicy: IfNotPresent
        <strong>volumeMounts:
        - name: config-volume
          mountPath: /opt/config</strong>
        livenessProbe:
          httpGet:
            path: /
            port: 8080
        readinessProbe:
          httpGet:
            path: /hello-cloud/resources/hello
            port: 8080
      <strong>volumes:
      - name: config-volume
        configMap:
          name: hello-cloud-config</strong>
      restartPolicy: Always
---</pre>
<p>The deployment defines a volume which references to the <kbd>hello-cloud-config</kbd> config map. The volume is mounted to the path <kbd>/opt/config</kbd> resulting in all key-value pairs of the config map being inserted as files in this directory. With the config map demonstrated previously this would result in a <kbd>application.properties</kbd> file containing the entries for keys <kbd>hello.greeting</kbd> and <kbd>hello.name</kbd>. The application expects that at runtime the file resides under this location.</p>
<p>Separate environments will specify different contents of the config maps, depending on the desired configuration values. Configuring applications using dynamic files is one approach. It is also possible to inject and override specific environment variables. The following code snippet demonstrates this example as well. This approach is advisable when the number of configuration values is limited:</p>
<pre># similar to previous example
# ...
        image: docker.example.com/hello-cloud:1
        imagePullPolicy: IfNotPresent
        <strong>env:
        - name: GREETING_HELLO_NAME
          valueFrom:
            configMapRef:
              name: hello-cloud-config
              key: hello.name</strong>
          livenessProbe:
# ...</pre>
<p>Applications need to configure credentials, used for example to authorize against external systems or as database accesses. These credentials are ideally configured in a different place than uncritical configuration values. Besides config maps, Kubernetes therefore also includes the concept of <strong>secrets</strong>. These are similar to config maps, also representing key-value pairs, but obfuscated for humans as Base64-encoded data. Secrets and their contents are typically not serialized as infrastructure as code since the credentials should not have unrestricted access.</p>
<p>A common practice is to make credentials accessible in containers using environment variables. The following code snippet shows how to include a value configured in secret <kbd>hello-cloud-secret</kbd> into the <em>hello-cloud</em> application:</p>
<pre># similar to previous example
# ...
        image: docker.example.com/hello-cloud:1
        imagePullPolicy: IfNotPresent
        <strong>env:
        - name: TOP_SECRET
          valueFrom:
            secretKeyRef:
              name: hello-cloud-secret
              key: topsecret</strong>
          livenessProbe:
# ...</pre>
<p>The environment variable <kbd>TOP_SECRET</kbd> is created from referencing the <kbd>topsecret</kbd> key in secret <kbd>hello-cloud-secret</kbd>. This environment variable is available at container runtime and can be used from the running process.</p>
<p>Some applications packaged in containers cannot solely run as stateless applications. Databases are a typical example of this. Since containers are discarded after their processes have exited, the contents of their file system are also gone. Services such as databases need persistent state though. To solve this issue Kubernetes includes <strong>persistent volumes</strong>. As the name suggests these volumes are available beyond the life cycle of the pods. Persistent volumes dynamically make files and directories available which are used within the pod and retain after it has exited.</p>
<p>Persistent volumes are backed by network attached storage or cloud storage offerings, d<span>epending on the cluster installation</span>. They <span>make it possible to run storage services such as databases in container orchestration clusters as well.</span> However, as a general advise, persistent state in containers should be avoided.</p>
<p>The YAML IaC definitions are kept under version control in the application repository. The next chapter covers how to apply the file contents to a Kubernetes cluster as part of a Continuous Delivery pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java EE in orchestrated containers</h1>
                </header>
            
            <article>
                
<p>The orchestration framework orchestrates and integrates enterprise applications in clustered environments. It takes a lot of work off the used application technology. Container orchestration also vastly simplifies how to configure applications and how to connect to external services. This section will showcase this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connecting external services</h1>
                </header>
            
            <article>
                
<p>Client controls require URLs to connect against in order to integrate <span>external services</span>. The URLs traditionally have been configured in files, which potentially differed in various environments. In an orchestrated environment the application can resolve external services using a logical name, via DNS. The following code snippet shows how to connect against the <em>cloud processor</em> application:</p>
<pre>@ApplicationScoped
public class HelloCloudProcessor {

    private Client client;
    private WebTarget target;

    @PostConstruct
    private void initClient() {
        client = ClientBuilder...
        target = client.target("http://<strong>cloud-processor:8080</strong>/processor/resources/hello");
    }

    public String processGreeting() {
        ...
    }
}</pre>
<p>The same holds true for other URLs, for example datasources definitions. The application server configuration can simply point to the name of the database service and use it to resolve the corresponding instance at runtime.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring orchestrated applications</h1>
                </header>
            
            <article>
                
<p>Resolving services by logical names already eliminates a lot of configuration in the application. Since the same container image is being used in all environments, potentially different configuration needs to be inserted from the orchestration environment. As shown in the previous example, Kubernetes config maps tackle this situation. The <em>hello-cloud</em> application expects that at runtime a properties file will reside under <kbd>/opt/config/application.properties</kbd>. The project code will therefore access this location. The following demonstrates the integration of the properties file using a CDI producer:</p>
<pre>public class HelloGreeter {

    @Inject
    <strong>@Config("hello.greeting")</strong>
    String greeting;

    @Inject
    <strong>@Config("hello.name")</strong>
    String greetingName;

    public String processGreeting() {
        return greeting + ", " + greetingName;
    }
}</pre>
<p>The CDI producer is defined similarly to the configuration example shown previously:</p>
<pre>@ApplicationScoped
public class ConfigurationExposer {

    private final Properties properties = new Properties();

    @PostConstruct
    private void initProperties() {
        try (<strong>InputStream inputStream =
                new FileInputStream("/opt/config/application.properties")</strong>) {
            properties.load(inputStream);
        } catch (IOException e) {
            throw new IllegalStateException("Could not init configuration", e);
        }
    }

    @Produces
    <strong>@Config("")</strong>
    public String exposeConfig(InjectionPoint injectionPoint) {
        Config config = injectionPoint.getAnnotated().getAnnotation(Config.class);
        if (config != null)
            return properties.getProperty(config.value());
        return null;
    }
}</pre>
<p>The definition of the <kbd>@Config</kbd> qualifier is similar to the previous example in <a href="">Chapter 3</a>, <em>Implementing Modern Java Enterprise Applications</em>. The application loads the contents of the properties file into the properties map and produces the configured values using CDI. All managed beans can inject these values which emerge from the Kubernetes config map.</p>
<p>In order to realize secret configuration values, Kubernetes includes the concept of secrets as previously shown. A common practice is to make the contents of the secrets accessible in containers using environment variables.</p>
<p>Java applications use the <kbd>System.getenv()</kbd> method to access environment variables. This functionality is used for both secrets and config map values, respectively.</p>
<p>The demonstrated approaches and examples enable an enterprise application to be deployed, managed, and configured in a container orchestration cluster. They are sufficient for the majority of use cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">12-factor applications and Java EE</h1>
                </header>
            
            <article>
                
<p>As of writing this book, <strong>12-factor</strong> applications has emerged as a way of <span>developing</span> <strong>Software as a Service</strong> <span>(</span><strong>SaaS</strong><span>) applications</span>. The 12-factor application approach define 12 software development principles. The motivations behind these principles aim to minimize time and effort, avoid software erosion, and embrace Continuous Delivery and cloud platforms.</p>
<p>In other words the 12-factors aim to to implement enterprise applications in a modern way. Some of the principles sound obvious to most engineers, while others seem to contradict the common practice of building enterprise applications.</p>
<p>The list of the 12-factors includes:</p>
<ul>
<li>I. Have one codebase tracked in revision control, many deploys</li>
<li>II. Explicitly declare and isolate dependencies</li>
<li>III. Store config in the environment</li>
<li>IV. Treat backing services as attached resources</li>
<li>V. Strictly separate build and run stages</li>
<li>VI. Execute the app as one or more stateless processes</li>
<li>VII. Export services via port binding</li>
<li>VIII. Scale out via the process model</li>
<li>IX. Maximize robustness with fast startup and graceful shutdown</li>
<li>X. Keep development, staging, and production as similar as possible</li>
<li>XI. Treat logs as event streams</li>
<li>XII. Run admin/management tasks as one-off processes</li>
</ul>
<p>The following explains the motivations of each principle and its realization with Java EE.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Have one codebase tracked in revision control, many deploys</h1>
                </header>
            
            <article>
                
<p>This principle sounds pretty obvious to software engineers, declaring that a software code should be kept under version control, a single repository, even for multiple <em>deploys</em>. Deploys relate to software instances, running on specific environments. Therefore the codebase of a single application is tracked in a single repository, not several codebases per application or vice versa, containing all specifications for potentially different environments.</p>
<p>This principle leverages developer productivity since all information is found under one repository. It is indifferent to the chosen technology and therefore supported by Java EE applications, as well.</p>
<p>The repository should contain all source files that are required to build and run the enterprise application. Besides Java sources and configuration files, this includes infrastructure as code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explicitly declare and isolate dependencies</h1>
                </header>
            
            <article>
                
<p>Software dependencies and their versions that are required in order to run the application must be specified explicitly. This includes not only dependencies which the application is programmed against, for example third-party APIs, but also implicit dependencies on the Java runtime or operating system, respectively. Explicitly specifying the required versions leads to far less compatibility issues in production. A composition of software versions is sufficiently tested during the development workflow. Dependency versions that differ when rebuilding binaries introduce potential issues. It is therefore advisable to explicitly declare all software versions to reduce probability of error and enable reproducibility.</p>
<p>Container technology simplifies this principle by explicitly specifying all software installation steps. Versions of used base images should be explicitly declared, so that image rebuilds result in the same result. The Docker <kbd>latest</kbd> tag should therefore be avoided in favor of definite versions. All software installations specified in Dockerfiles should point to explicit versions as well. Docker rebuilds, with or without cache, should produce the same outcome.</p>
<p>Java applications specify their dependencies using build systems. The first chapter already covered what is necessary to enable reproducible builds using both Maven and Gradle. In Java EE applications these dependencies are ideally minimized to the Java EE API.</p>
<p>Whenever possible, it's advisable to specify explicit dependency versions, not just <em>latest</em> ones. Only software using explicit versions can be tested reliably.</p>
<p>Isolating dependencies is a necessity for distributed development throughout the software team. Software artifacts should be accessible via well-defined processes, for example artifact repositories. Dependencies, which are added during the software build, no matter whether Java runtime installations, Java artifacts, or operating system components, need to be distributed from a central place. Repositories such as <strong>Maven Central</strong>, <strong>DockerHub</strong> or company-internal repositories enable this approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Store config in the environment</h1>
                </header>
            
            <article>
                
<p>Application configuration, that differ for separate environments, such as databases, external systems, or credentials, need to be existent at runtime. This configuration should not be reflected in the source code but dynamically modifiable from outside of the application. This implies that configuration is retrieved via files, environment variables or other external concerns.</p>
<p>Container technology and orchestration frameworks support these approaches as previously shown. Configuration for different environments, such as <em>test</em>, <em>staging</em>, and <em>production</em> is stored in Kubernetes config maps and dynamically used in pods in volumes or environment variables.</p>
<p>The 12-factor principles state that an application "[...] stores config in environment variables". Environment variables are a straightforward way of inserting specific variations that is supported by all kinds of technology. However, if configuring the application involves a lot of individual configuration values, engineers may consider to use configuration files contained in container volumes, instead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Treat backing services as attached resources</h1>
                </header>
            
            <article>
                
<p>Databases and external systems that are accessed in the application are called <em>resources</em>. It should make no difference to the system where an external service or database is part of the application. The <em>resources</em> should be attached to the application in a loosely coupled way. External systems and databases should be able to be replaced by new instances without affecting the application.</p>
<p>Applications abstract the accessed external system, first of all in the communication technology being used. Communication via HTTP or JDBC, for example, abstracts the implementations and enables systems to be replaced by others. By doing so, applications are only coupled to their contract: the communication protocol and defined schemas. JPA, JAX-RS, and JSON-B are examples that support this approach.</p>
<p>Container orchestration frameworks take this approach even further and abstract services into logic names. As shown previously applications can use service names as hostnames, resolved by DNS.</p>
<p>In general, application developers should loosely couple systems together, ideally only depending on protocols and schemas. At a code level backing services are abstracted into own components, such as individual controls with clean interfaces. This minimizes changes if attached resources change.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strictly separate build and run stages</h1>
                </header>
            
            <article>
                
<p>This principle advises to separate the application build, the deployment, and the run processes. This is a well-known approach to Java enterprise developers. Application binaries are built, deployed, and run in separate steps. Software or configuration changes happen in the source code or in the deployment step, respectively, and not directly in production. The deployment step brings application binaries and potential configuration together. Well-defined change and release management processes keep the integrity of the enterprise software.</p>
<p>For the vast majority of software projects, it is common practice to separate these steps and orchestrate stages in a Continuous Integration server. This is necessary to ensure reliability and reproducibility. <a href="599c6821-8971-4489-931c-9e11b5e23afd.xhtml">Chapter 6</a>, <em>Application Development Workflows</em> covers this topic in depth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Execute the app as one or more stateless processes</h1>
                </header>
            
            <article>
                
<p>Ideally, applications run as stateless processes where every use case is executed self-sufficiently, without affecting other running processes. Potential state is either stored in an attached resource such as a database or discarded. Session state that lives longer than a single request is therefore a violation of this principle. The challenge with traditional user session state is that it only resides in a local application instance and not accessible from other instances. The need for so-called <em>sticky sessions</em> on load balancers is an indicator for not having a stateless application.</p>
<p>A lot of modern technology supports this approach. Docker containers with their copy-on-write file system are an example. Stopped containers will be discarded and therefore all of their state is gone as well. Stateless EJBs are based on a similar motivation. However, instances of stateless session beans are pooled and reused, therefore developers need to ensure that no state retains after the business use case invocations.</p>
<p>Enterprise applications should be able to be restarted from scratch without affecting their behavior. This also implies that applications share no state except via well-defined attached resources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Export services via port binding</h1>
                </header>
            
            <article>
                
<p>Web applications are traditionally deployed to a certain software stack. Java enterprise applications for examples are deployed to an enterprise or Servlet container whereas server-side scripting languages such as PHP run on top of a web server. The applications therefore depend on their immediate runtime.</p>
<p>This 12-factor principle advise to develop self-sufficient applications that expose their functionality via network ports. Since web-based enterprise applications will communicate via the network, binding services to ports is the way of least coupling.</p>
<p>Java EE applications that run in a container support this approach, only exporting a port which is used to communicate with the application. Containers only depend on the Linux kernel, the application runtime is therefore transparent. Container orchestration frameworks leverage this idea, connecting services to pods via logical names and ports, as shown in a previous example. Java EE supports the use of containers and therefore this principle as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scale out via the process model</h1>
                </header>
            
            <article>
                
<p>Modern applications as well as their environments should enable scalability when the workload on them increases. Applications ideally are able to scale out horizontally, rather than just vertically. The difference is that scaling horizontally aims to adds more individual, self-contained nodes to the software whereas scaling vertically increases the resources on single nodes or processes. However, scaling vertically is limited, since resources on physical nodes cannot be increased infinitely.</p>
<p>12-factor applications describe the procedure of adding concurrency to the software with adding more self-contained, <em>shared-nothing</em> processes. Workloads should be distributable within several physical hosts, by increasing the number of processes. The processes represent the request or worker threads who handle the system's workload.</p>
<p>This approach shows the necessity of implementing stateless application in a shared-nothing manner. Containers that run stateless Java enterprise applications enable the system to scale out. Kubernetes managed scalability in deployments via managing the number of replicas.</p>
<p>The bottleneck of enterprise applications, however, is typically not the application instances rather than central databases. <a href="">Chapter 8</a>, <em>Microservices and System Architecture</em> and <a href="">Chapter 9</a>, <em>Monitoring, Performance, and Logging</em> cover the topics of scalability in distributed systems as well as performance in Java EE projects in general.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximize robustness with fast startup and graceful shutdown</h1>
                </header>
            
            <article>
                
<p><a href="f0a49441-e411-49c4-a4b6-c6193ba36094.xhtml">Chapter 4</a>, <em>Lightweight Java EE</em> already showed the necessity of fast turnarounds. This principle of 12-factor applications requires technology that enables velocity and elasticity. In order to rapidly scale up, the software should startup in a matter of seconds, making it possible to tackle a growing workload.</p>
<p>Application shutdowns should gracefully finish in-flight requests and properly close all open connections and resources. Especially requests and transactions that are executed while the shutdown signal occurs should be finished properly not to maliciously abort client use cases. In a Unix process approach shutdown signals are sent as <kbd>SIGTERM</kbd> signals. Linux containers are stopped in the same way, giving the container process a chance to shutdown properly. When building container images, developers should pay attention that the process handles Unix signals properly, resulting in a graceful shutdown of the application server when it receives a <kbd>SIGTERM</kbd> signal.</p>
<p>Java EE supports both fast startups and graceful shutdowns. As shown previously, modern application servers start up and deploy applications in a matter of seconds.</p>
<p>Since the application servers manage beans, resources, pooling, and threading, they take care of closing the resources properly at JVM shutdown. The developers don't need to take care of this aspect themselves. Beans that manage custom resources or handles that need to be closed, use pre-destroy methods to implemented proper closing. The following shows a client control using a JAX-RS client handle which is closed on server shutdown:</p>
<pre>@ApplicationScoped
public class CoffeePurchaser {

    private Client client;

    ...

    <strong>@PreDestroy</strong>
    public void closeClient() {
        <strong>client.close();</strong>
    }
}</pre>
<p>The platform guarantees that the pre-destroy methods of all managed beans are called once the application server shuts down.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keep development, staging, and production as similar as possible</h1>
                </header>
            
            <article>
                
<p>This 12-factor principle aims to minimize differences between environments.</p>
<p>Enterprise applications traditionally have quite some differences between the environments of the development process. There are development environments, maybe several of them, such as local workstations or dedicated server environments and finally there is the production environment. These environments differ in regard of time when software artifacts in certain versions and configuration are deployed during the development process. The longer the time span of simultaneously having different versions in the set of environments the greater this difference becomes.</p>
<p>There is also a difference in teams and people. Traditionally software developers maintain their own development environment while an operations team takes care of production. This introduces potential gaps in communication, processes, and used technology.</p>
<p>The technical difference between environments contains the biggest risk. Development or test environments that use different tools, technology, external services and configuration than production introduce the risk that these differences will lead to errors. Software is tested automatically on these environments before going to production. Every difference from production that is not tested can and eventually will introduce bugs that could have been prevented. The same is true for exchanging tools, backend services, or used stacks for lightweight alternatives in development or local environments.</p>
<p>It is therefore advisable to keep the environments as similar as possible. Especially, container technologies and orchestration frameworks highly support this approach. As we saw previously, differences in configuration, services, and technology are minimized or at least explicitly defined via the environment. Ideally, software landscapes are identical on development, test environments, staging, and production. If that is not possible, service abstractions as well as environment-managed configuration support to manage the differences.</p>
<p>The difference in time and people is tackled by usage of Continuous Delivery, not just from a technical but also organizational point of view. The overall time to production should be as small as possible, enabling fast delivery of features and bug fixes. Implementing Continuous Delivery naturally moves teams and responsibilities together. The DevOps movement describes how all engineers are responsible for the overall software. This leads to a culture where all teams closely work together or merge into single teams of software engineers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Treat logs as event streams</h1>
                </header>
            
            <article>
                
<p>Enterprise applications traditionally write logs to log files on disk. Some engineers argue that this information is one of the most important insights into the application. The software project usually includes configuration of the contents and format of these logfiles. However, storing log data in log files is first of all just an output format, usually having a single log event per line.</p>
<p>This principle of 12-factor applications argues that logging should be treated as a stream of log events, that are emitted by the application. Applications should, however, not concern themselves with routing and storing the log file into specific output formats. Instead they log to the process' standard output. The standard out is captured and processed by the runtime environment.</p>
<p>This approach is uncommon to most enterprise developers with all logging frameworks, output formats and tools being around. However, environments where a lot of services run in parallel need to capture and process log events externally anyway. Solutions such as <strong>Elasticsearch</strong>, <strong>Logstash</strong>, and <strong>Kibana</strong> have proven themselves well to process and comprehend complex situations with log events from several sources. Storing log events in log files not necessarily supports these approaches.</p>
<p>Logging to the application's standard out not only simplifies development, since routing and storing is not a responsibility of the application anymore. It also reduces the need for external dependencies, such as logging frameworks. Zero-dependency applications support this approach. The environment such as a container orchestration framework takes care of capturing and routing the event stream. In <a href="">Chapter 9</a>, <em>Monitoring, Performance, and Logging</em>, we will cover the topic of logging, its necessity and shortcomings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Run admin/management tasks as one-off processes</h1>
                </header>
            
            <article>
                
<p>This principle describes that administrative or management tasks should be executed as separate short-lived processes. The technology ideally supports command execution in a shell that operates on the running environment.</p>
<p>Although containers encapsulate Unix processes, they provide additional functionality to execute single commands or to open a remote shell into the container. Engineers can therefore execute the management and administration scripts provided by the Java EE application server. Still, in Java EE applications, the number of required administration and management tasks are limited. A Container runs the application server process, which auto-deploys the application; no further application life cycle management is required.</p>
<p>Administrative tasks are usually required for debugging and troubleshooting purposes. Therefore containers and container orchestration frameworks offer possibilities to open remote shells into the containers or execute one-time commands. Apart from that, the <a href="">Chapter 9</a>, <em>Monitoring, Performance, and Logging</em> will show you what is necessary to gather further monitoring information about enterprise applications.</p>
<p>The motivations of the 12-factors are to develop stateless, scalable enterprise applications that embrace Continuous Delivery and modern environment platforms, optimize time and effort spent in development and try to avoid software erosion. 12-factor application have a clean contract with their underlying system and ideally declarative infrastructure definitions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud, Cloud native, and their benefits</h1>
                </header>
            
            <article>
                
<p>As of writing this book, there is a lot of interest in cloud platforms. We currently see big companies moving their IT infrastructure into cloud offerings. But what benefits does <em>the cloud</em> have to offer?.</p>
<p>First of all, we have to be aware that modern environments do not necessarily have to run on top of a cloud platform. All the benefits of container technology and container orchestration frameworks can equally be achieved using company-internal infrastructure. On premise installations of platforms such as Kubernetes or OpenShift at first provide the same advantages for software teams. In fact, one of the biggest benefits of container runtimes is to abstract the environment where the containers are running. Why are cloud platforms interesting for companies then?</p>
<p>As mentioned in the beginning of this book, the software world is moving faster than ever. The key for companies to keep pace with the trends in their business is to embrace agility and velocity in terms of moving fast. The time to market of new products and features thereof need to be as short as possible. Moving in iterative steps, adapting to customers' needs and continuously improving software meets this demand. In order to realize this goal, IT infrastructure, as well as all other aspects of software engineering, needs to be fast and flexible. New environments should be setup via automated, reliable and reproducible processes. The same principles for continuous software delivery apply to server environments. Cloud platforms offer this possibility.</p>
<p>Companies that want to embrace agility and to adapt to their customers' demands need to ask themselves the question: <em>How long does it take to provision new environments?</em> This is the prerequisite of being able to adapt quickly. Provisioning whole new environments should be a matter of minutes, should not require overly complex processes and ideally no human intervention. As said before it is definitely possible to realize such approaches on premises. Cloud offerings, however, offer these benefits out of the box with sufficient, scalable resources. <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>) or <strong>Platform as a Service</strong> (<strong>PaaS</strong>) offerings take a lot of work off the hands of companies, enabling them to focus on building their products.</p>
<p>Still, big companies are often skeptical when it comes to cloud services, especially in terms of data security. Interestingly, experience of projects shows that when comparing infrastructure environments down to earth, cloud platforms run by sophisticated enterprises offer more secure environments than most on premises. Cloud platform providers put a lot of time and effort into building proper solutions. Especially combining cloud platform offerings with orchestration solutions, such as Docker Compose, Kubernetes, or OpenShift hold a lot of potential.</p>
<p>Interestingly, one of the main arguments of companies to move their IT into the cloud is because of economic reasons. From experience, a lot of companies want to save costs by using cloud platforms. In fact, when taking the whole process of migrating and transforming environments, teams, technology, and most of all know-how, into account, <span>on premises solutions are usually</span> still cheaper. However, the main advantage of cloud offerings is flexibility and the ability to move fast. If an IT company maintains a well-orchestrated landscape, including automation, reliable and reproducible processes, it is advisable to keep, and continuously improve, this approach. That said, the question about modern environments is less about whether to use cloud platforms than about processes, team mindsets, and reasonable technology.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud native</h1>
                </header>
            
            <article>
                
<p>Besides the interest in cloud technology there is a lot of interest in the term <strong>cloud native</strong> which describes applications that, besides following the 12-factors, have a strong relationship to cloud platforms. Cloud native and 12-factor applications are not synonymous; rather than cloud native includes the 12-factors, among other things.</p>
<p>Cloud native applications are designed to run on cloud PaaS offerings with all their benefits and challenges, embrace container technology and elastic scalability. They are built with the claim to provide modern, scalable, stateless and resilient applications, manageable within modern orchestration environments. Unlike the term <em>native</em> suggests, applications that follow this approach do not necessarily have to be built as <em>green-field</em> projects that support cloud technology from day one.</p>
<p>Important aspects for cloud native applications beyond the 12-factors are monitoring and application health concerns, which can be summarized as telemetry. Telemetry for enterprise applications include responsiveness, monitoring, domain-specific insights, health checks, and debugging. As we have seen previously, container orchestration supports us at least with the last two concerns: health checks and debugging. Running applications are probed whether they are still alive and healthy. Debugging and troubleshooting is possible by evaluating the log event streams, connecting into the running containers or executing processes.</p>
<p>Application monitoring need to be exposed by the running container. This requires a bit more effort from software developers. Domain-specific metrics need to be defined by the business experts first. It depends which metrics are interesting to business departments and will be exposed by the application. Technical metrics are gathered from the running application as well. <a href="">Chapter 9</a>, <em>Monitoring, Performance, and Logging</em> covers the topic of monitoring in regard to modern environments.</p>
<p>Another aspect that the 12-factors don't include are APIs and security thereof. SaaS applications communicate via exposed APIs that have to be made known to other teams of developers. The nature and structure of web services needs to be documented and agreed upon during development. This is especially the case when HTTP APIs don't implement Hypermedia. The applications need to know the nature and structure of exchanged information - ideally as early as possible in the development process. This also covers authentication and authorization. Application developers should be aware of security mechanisms they need to address before communicating to other services. In general it is not advisable to only think of security aspects after development. <a href="2c990001-2bf1-4ede-b2cd-f4939754b6df.xhtml">Chapter 10</a>, <em>Security</em> covers this topic concerning cloud environments and integration into Java EE applications.</p>
<p>In order to build an umbrella for all technologies that embrace cloud platforms, the <strong>Cloud Native Computing Foundation</strong> was formed by several software vendors. It is part of the Linux Foundation, representing an foundation for cloud native Open Source Software. It contains technology that orchestrates, manages, monitors, traces or in some other way supports containerized <strong>microservices</strong> running in modern environments. As of writing this book, examples for technology projects being part of the Cloud Native Computing Foundation are <strong>Kubernetes</strong>, <strong>Prometheus</strong>, <strong>OpenTracing</strong>, or <strong>containerd</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Operational tasks need to be automated. Setting up application environments should always produce the same outcome, including installations, network, and configuration. Container technologies as well as infrastructure as code support this by defining, automating and distributing software installations and configuration. They fulfill the necessity of rebuilding software and systems in a fast and reproducible way.</p>
<p>Infrastructure as code definitions specify the required infrastructure together with all dependencies as part of the application code, kept under version control. This approach supports the ideas behind the DevOps movement. The responsibilities of not only defining the application but also its runtime with all requirements move different teams together. It should be a responsibility of all engineers to deliver quality software that serves a business purpose.</p>
<p>Container technologies such as Docker provides functionality to build, manage, and ship containers in a uniform way. Docker's copy-on-write layered file system enables us to minimize build and publishing times by only re-executing steps that have changed. Java EE zero-dependency applications encourage the use of container technology by separating the application logic from its implementation. The changing layer therefore only contains business code.</p>
<p>Container orchestration frameworks such as Kubernetes manage containers in their life cycle, network, and external configuration. They are responsible to lookup services, provide production readiness such as zero-downtime deployments and scale up and down application instances. Container orchestration supports infrastructure as code definitions, that contain the configuration of the whole runtime environment required by the application.</p>
<p>The 12-factor and cloud native approaches aim to develop modern enterprise applications with minimal time and effort, avoiding software erosion, and supporting Continuous Delivery and cloud platforms. The 12-factor principles target software dependencies, configuration, dependent services, runtime environments, logging and administration processes. Similarly, cloud native applications aim to build enterprise software that works well on cloud platforms, supporting monitoring, resilience, application health, and security. Since these approaches are not bound to a specific technology, they are realizable using Java EE. We have seen the motivations why to follow these principles.</p>
<p>The following chapter will show you how to build productive application development workflows, that are based on container technologies.</p>


            </article>

            
        </section>
    </body></html>