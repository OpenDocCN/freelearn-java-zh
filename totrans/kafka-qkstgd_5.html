<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Schema Registry</h1>
                </header>
            
            <article>
                
<p><span>In the previous chapter, we saw how to produce and consume data in JSON format. In this chapter, we will see how to serialize the same messages with Apache Avro.</span></p>
<p class="mce-root">This chapter covers the following topics:</p>
<ul>
<li class="mce-root">Avro in a nutshell</li>
<li class="mce-root">Defining the schema</li>
<li class="mce-root">Starting the Schema Registry</li>
<li class="mce-root">Using the Schema Registry</li>
<li class="mce-root">How to build a Java <kbd>AvroProducer</kbd>, a consumer, and a processor</li>
<li class="mce-root">How to run the Java <kbd>AvroProducer</kbd> and the processor</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Avro in a nutshell</h1>
                </header>
            
            <article>
                
<p class="mce-root">Apache Avro is a binary serialization format. The format is schema-based so, it depends on the definition of schemas in JSON format. These schemas define which fields are mandatory and their types. Avro also supports arrays, enums, and nested fields.</p>
<p class="mce-root">One major advantage of Avro is that it supports schema evolution. In this way, we can have several historical versions of the schema.</p>
<p class="mce-root">Normally, the system must adapt to the changing needs of the business. For this reason, we can add or remove fields from our entities, and even change the data types. To support forward or backward compatibility, we must consider which fields are indicated as optional.</p>
<p class="mce-root">Because Avro converts the data into arrays of bytes (serialization), and Kafka's messages are also sent in binary data format, with Apache Kafka, we can send messages in Avro format. The real question is, where do we store the schemas for Apache Avro to work?</p>
<p class="mce-root"/>
<p class="mce-root">Recall that one of the main functions of an enterprise service bus is the format validation of the messages it processes, and what better if it has a historical record of these formats?</p>
<p class="mce-root">The Kafka Schema Registry is the module responsible for performing important functions. The first is to validate that the messages are in the appropriate format, the second is to have a repository of these schemas, and the third is to have a historical version format of these schemas.</p>
<p class="mce-root">The Schema Registry is a server that runs in the same place as our Kafka brokers. It runs and stores the schemas, including the schema versions. When messages are sent to Kafka in Avro format, the messages contain an identifier of a schema stored in the Schema Registry.</p>
<p class="mce-root">There is a library that allows for message serialization and deserialization in Avro format. This library works transparently and naturally with the Schema Registry.</p>
<p class="mce-root">When a message is sent in Avro format, the serializer ensures that the schema is registered and obtains the schema ID. If we send an Avro message that is not in the Schema Registry, the current version of the schema is registered automatically in the Registry. If you do not want the Schema Registry to behave in this way, you can disable it by setting the <kbd>auto.register.schemas</kbd> flag to <kbd>false</kbd>.</p>
<p class="mce-root">When a message is received in Avro format, the deserializer tries to find the schema ID in the Registry and fetch the schema to deserialize the message in Avro format.</p>
<p class="mce-root">Both the Schema Registry and the library for the serialization and deserialization of messages in Avro format are under the Confluent Platform. It is important to mention that when you need to use the Schema Registry, you must use the Confluent Platform.</p>
<p class="mce-root">It is also important to mention that with the Schema Registry, the Confluent library should be used for serialization in Avro format, as the Apache Avro library doesn't work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the schema</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first step is to define the Avro schema. As a reminder, our <kbd>HealthCheck</kbd> class looks like <em>Listing 5.1</em>:</p>
<pre class="mce-root">public final class HealthCheck {<br/> private String event;<br/> private String factory;<br/> private String serialNumber;<br/> private String type;<br/> private String status;<br/> private Date lastStartedAt;<br/> private float temperature;<br/> private String ipAddress;<br/>}</pre>
<div class="packt_figref CDPAlignCenter CDPAlign">Listing 5.1:<span> HealthCheck.java</span></div>
<p class="mce-root">Now, with the representation of this message in Avro format, the schema (that is, the template) of all the messages of this type in Avro would be <em>Listing 5.2</em>:</p>
<pre>{<br/> "name": "HealthCheck",<br/> "namespace": "kioto.avro",<br/> "type": "record",<br/> "fields": [<br/> { "name": "event", "type": "string" },<br/> { "name": "factory", "type": "string" },<br/> { "name": "serialNumber", "type": "string" },<br/> { "name": "type", "type": "string" },<br/> { "name": "status", "type": "string"},<br/> { "name": "lastStartedAt", "type": "long", "logicalType": "timestamp-<br/>    millis"},<br/> { "name": "temperature", "type": "float" },<br/> { "name": "ipAddress", "type": "string" }<br/> ]<br/>}</pre>
<div class="CDPAlignCenter CDPAlign packt_figref">Listing 5.2:<span> healthcheck.avsc</span></div>
<p class="mce-root">This file must be saved in the <kbd>kioto</kbd> project in the <kbd>src/main/resources </kbd>directory.</p>
<p class="mce-root">It is important to note that there are the types <kbd>string</kbd>, <kbd>float</kbd>, and <kbd>double</kbd>. But, in the case of <kbd>Date</kbd>, it can be stored as a <kbd>long</kbd> or as a <kbd>string</kbd>.</p>
<p class="mce-root">For this example, we will serialize <kbd>Date</kbd> as a <kbd>long</kbd>. Avro doesn't have a dedicated <kbd>Date</kbd> type; we have to choose between a <kbd>long</kbd> and a <kbd>string</kbd> (an ISO-8601 <kbd>string</kbd> is usually better), but the point with this example is to show how to use different data types.</p>
<p class="mce-root">For more information about Avro schemas and how to map the types, check the Apache Avro specification at the following URL:<br/>
<a href="http://avro.apache.org/docs/current/spec.html">http://avro.apache.org/docs/current/spec.html</a></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Starting the Schema Registry</h1>
                </header>
            
            <article>
                
<p class="mce-root">Well, we have our Avro schema; now, we need to register it in the Schema Registry.<br/>
When we start the Confluent Platform, the Schema Registry is started, as shown in the following code:</p>
<pre class="mce-root"><strong>$./bin/confluent start</strong><br/><strong>Starting zookeeper</strong><br/><strong>zookeeper is [UP]</strong><br/><strong>Starting kafka</strong><br/><strong>kafka is [UP]</strong><br/><strong>Starting schema-registry</strong><br/><strong>schema-registry is [UP]</strong><br/><strong>Starting kafka-rest</strong><br/><strong>kafka-rest is [UP]</strong><br/><strong>Starting connect</strong><br/><strong>connect is [UP]</strong><br/><strong>Starting ksql-server</strong><br/><strong>ksql-server is [UP]</strong><br/><strong>Starting control-center</strong><br/><strong>control-center is [UP]</strong></pre>
<p class="mce-root">If we want just to start the Schema Registry, we need to run the following command:</p>
<pre class="mce-root"><strong>$./bin/schema-registry-start etc/schema-registry/schema-registry.properties</strong></pre>
<p class="mce-root">The output is similar to the one shown here:</p>
<pre class="mce-root"><strong>...</strong><br/><strong>[2017-03-02 10:01:45,320] INFO Started NetworkTrafficServerConnector@2ee67803{HTTP/1.1,[http/1.1]}{0.0.0.0:8081}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Schema Registry</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, the Schema Registry is running on port <kbd>8081</kbd>. To interact with the Schema Registry, there is a REST API. We can access it with <kbd>curl</kbd>. The first step is to register a schema in the Schema Registry. To do so, we have to embed our JSON schema in another JSON object, and we have to escape some special characters and add a payload:</p>
<ul>
<li>At the beginning, we have to add <kbd>{ \"schema\": \"</kbd></li>
<li>All the double quotation marks (<kbd>"</kbd>) should be escaped with a backslash (<kbd>\"</kbd>)</li>
<li>At the end, we have to add <kbd>\" }</kbd></li>
</ul>
<p class="mce-root">Yes, as you can guess, the API has several commands to query the Schema Registry.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Registering a new version of a schema under a – value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To register the Avro schema <kbd>healthcheck.avsc</kbd>, located in the <kbd>src/main/resources/</kbd> <span>path </span>listed in <em>Listing 5.2</em>, using the <kbd>curl</kbd> command, we use the following:</p>
<pre class="mce-root"><strong>$ curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \</strong><br/><strong>--data '{ "schema": "{ \"name\": \"HealthCheck\", \"namespace\": \"kioto.avro\", \"type\": \"record\", \"fields\": [ { \"name\": \"event\", \"type\": \"string\" }, { \"name\": \"factory\", \"type\": \"string\" }, { \"name\": \"serialNumber\", \"type\": \"string\" }, { \"name\": \"type\", \"type\": \"string\" }, { \"name\": \"status\", \"type\": \"string\"}, { \"name\": \"lastStartedAt\", \"type\": \"long\", \"logicalType\": \"timestamp-millis\"}, { \"name\": \"temperature\", \"type\": \"float\" }, { \"name\": \"ipAddress\", \"type\": \"string\" } ]} " }' \</strong><br/><strong>http://localhost:8081/subjects/healthchecks-avro-value/versions</strong></pre>
<p class="mce-root">The output should be something like this:</p>
<pre class="mce-root"><strong>{"id":1}</strong></pre>
<p class="mce-root">This means that we have registered the <kbd>HealthChecks</kbd> schema with the version <kbd>"id":1</kbd> (congratulations, your first version).</p>
<p class="mce-root">Note that the command registers the schema on a subject called <kbd>healthchecks-avro-value</kbd>. The Schema Registry doesn't have information about topics (we still haven't created the <kbd>healthchecks-avro</kbd> topic). It is a convention, followed by the serializers/deserializers, to register schemas under a name following the &lt;topic&gt;-value format. In this case, since the schema is used for the message values, we use the suffix-value. If we wanted to use Avro to identify our messages keys, we would use the &lt;topic&gt;-key format.</p>
<p class="mce-root">For example, to obtain the ID of our schema, we use the following command:</p>
<pre class="mce-root"><strong>$ curl http://localhost:8081/subjects/healthchecks-avro-value/versions/</strong></pre>
<p class="mce-root">The following output is the schema ID:</p>
<pre class="mce-root"><strong>[1]</strong></pre>
<p class="mce-root">With the schema ID, to check the value of our schema, we use the following command:</p>
<pre class="mce-root"><strong>$ curl http://localhost:8081/subjects/healthchecks-avro-value/versions/1</strong></pre>
<p class="mce-root"/>
<p class="mce-root">The output is the schema value shown here:</p>
<pre class="mce-root"><strong>{"subject":"healthchecks-avro-value","version":1,"id":1,"schema":"{\"type\":\"record\",\"name\":\"HealthCheck\",\"namespace\":\"kioto.avro\",\"fields\":[{\"name\":\"event\",\"type\":\"string\"},{\"name\":\"factory\",\"type\":\"string\"},{\"name\":\"serialNumber\",\"type\":\"string\"},{\"name\":\"type\",\"type\":\"string\"},{\"name\":\"status\",\"type\":\"string\"},{\"name\":\"lastStartedAt\",\"type\":\"long\",\"logicalType\":\"timestamp-millis\"},{\"name\":\"temperature\",\"type\":\"float\"},{\"name\":\"ipAddress\",\"type\":\"string\"}]}"}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Registering a new version of a schema under a – key subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">As an example, to register a new version of our schema under the <kbd>healthchecks-avro-key</kbd> <span>subject,</span> we would execute the following command (don't run it; it is just to exemplify):</p>
<pre class="mce-root"><strong>curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json"\</strong><br/><strong>--data 'our escaped avro data' \</strong><br/><strong>http://localhost:8081/subjects/healthchecks-avro-key/versions</strong></pre>
<p class="mce-root">The output should be something like this:</p>
<pre class="mce-root"><strong>{"id":1}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Registering an existing schema into a new subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's suppose that there is an existing schema registered on a subject called <kbd>healthchecks-value1</kbd>, and we need this schema available on a subject called <kbd>healthchecks-value2</kbd>. </p>
<p>The following command reads the existing schema from <kbd>healthchecks-value1</kbd> and registers it to <kbd>healthchecks-value2</kbd> (assuming that the <kbd>jq</kbd> tool is already installed):</p>
<pre class="mce-root"><strong>curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json"\</strong><br/><strong>--data "{\"schema\": $(curl -s http://localhost:8081/subjects/healthchecks-value1/versions/latest | jq '.schema')}" \</strong><br/><strong>http://localhost:8081/subjects/healthchecks-value2/versions</strong></pre>
<p class="mce-root"/>
<p class="mce-root">The output should be something like this:</p>
<pre class="mce-root"><strong>{"id":1}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Listing all subjects</h1>
                </header>
            
            <article>
                
<p>To list all the subjects, you can use the following command:</p>
<pre class="mce-root"><strong>curl -X GET http://localhost:8081/subjects</strong></pre>
<p class="mce-root">The output should be something like this:</p>
<pre class="mce-root"><strong>["healthcheck-avro-value","healthchecks-avro-key"]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fetching a schema by its global unique ID</h1>
                </header>
            
            <article>
                
<p><span>To fetch a schema, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X GET http://localhost:8081/schemas/ids/1</strong></pre>
<p class="mce-root">The output should be something like this:</p>
<pre class="mce-root"><strong>{"schema":"{\"type\":\"record\",\"name\":\"HealthCheck\",\"namespace\":\"kioto.avro\",\"fields\":[{\"name\":\"event\",\"type\":\"string\"},{\"name\":\"factory\",\"type\":\"string\"},{\"name\":\"serialNumber\",\"type\":\"string\"},{\"name\":\"type\",\"type\":\"string\"},{\"name\":\"status\",\"type\":\"string\"},{\"name\":\"lastStartedAt\",\"type\":\"long\",\"logicalType\":\"timestamp-millis\"},{\"name\":\"temperature\",\"type\":\"float\"},{\"name\":\"ipAddress\",\"type\":\"string\"}]}"}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Listing all schema versions registered under the healthchecks–value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To list all schema versions registered under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X GET http://localhost:8081/subjects/healthchecks-value/versions</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>[1]</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fetching version 1 of the schema registered under the healthchecks-value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To fetch version 1 of the schema registered under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X GET http://localhost:8081/subjects/ healthchecks-value/versions/1</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>{"subject":" healthchecks-value","version":1,"id":1}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting version 1 of the schema registered under the healthchecks-value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To delete version 1 of the schema registered under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X DELETE http://localhost:8081/subjects/healthchecks-value/versions/1</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>1</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting the most recently registered schema under the healthchecks-value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To delete the most recently registered schema under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X DELETE http://localhost:8081/subjects/healthchecks-value/versions/latest</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>2</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting all the schema versions registered under the healthchecks–value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To delete all the schema versions registered under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X DELETE http://localhost:8081/subjects/healthchecks-value</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>[3]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checking whether a schema is already registered under the healthchecks–key subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To check whether a schema is already registered under the <kbd>healthchecks-key</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json"\</strong><br/><strong>--data 'our escaped avro data' \</strong><br/><strong>http://localhost:8081/subjects/healthchecks-key</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>{"subject":"healthchecks-key","version":3,"id":1}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing schema compatibility against the latest schema under the healthchecks–value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To test the schema compatibility against the latest schema under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json"\</strong><br/><strong>--data 'our escaped avro data' \</strong><br/><strong>http://localhost:8081/compatibility/subjects/healthchecks-value/versions/latest</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>{"is_compatible":true}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the top-level compatibility configuration</h1>
                </header>
            
            <article>
                
<p class="mce-root">To get the top level compatibility configuration<span>, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X GET http://localhost:8081/config</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>{"compatibilityLevel":"BACKWARD"}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Globally updating the compatibility requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">To globally <span>update</span> the compatibility requirements<span>, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" \</strong><br/><strong>--data '{"compatibility": "NONE"}' \</strong><br/><strong>http://localhost:8081/config</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>{"compatibility":"NONE"}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the compatibility requirements under the healthchecks–value subject</h1>
                </header>
            
            <article>
                
<p class="mce-root">To update the compatibility requirements under the <kbd>healthchecks-value</kbd><span> subject, you</span><span> can use the following command:</span></p>
<pre class="mce-root"><strong>curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" \</strong><br/><strong>--data '{"compatibility": "BACKWARD"}' \</strong><br/><strong>http://localhost:8081/config/healthchecks-value</strong></pre>
<p><span>The output should be something like this:</span></p>
<pre class="mce-root"><strong>{"compatibility":"BACKWARD"}</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java AvroProducer</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, we should modify our Java Producer to send messages in Avro format. First, it is important to mention that in Avro there are two types of messages:</p>
<ul>
<li class="mce-root"><strong>Specific records</strong>: The file with the Avro schema (avsc) is sent to a specific Avro command to generate the corresponding Java classes.</li>
<li class="mce-root"><strong>Generic records</strong>: In this approach, a data structure similar to a map dictionary is used. This means that you set and get the fields by their names and you must know their corresponding types. This option is not type-safe, but it offers much more flexibility than the other, and here the versions are much easier to manage over time. In this example, we will use this approach.</li>
</ul>
<p>Before we start with the code, remember that in the last chapter we added the library to support Avro to our Kafka client. If you recall, the <kbd>build.gradle</kbd> file has a special repository with all this libraries.</p>
<p>Confluent's repository is specified in the following line:</p>
<pre>repositories {<br/> ...<br/> maven { url 'https://packages.confluent.io/maven/' }<br/> }</pre>
<p>In the dependencies section, we should add the specific Avro libraries:</p>
<pre class="mce-root">dependencies {<br/> ...<br/> compile 'io.confluent:kafka-avro-serializer:5.0.0'<br/> }</pre>
<p class="mce-root">Do not use the libraries provided by Apache Avro, because they will not work.<br/>
As we already know, to build a Kafka message producer, we use the Java client library; in particular, the producer API. As we already know, there are two requisites that all the Kafka producers should have: to be a <kbd>KafkaProducer</kbd> and to set the specific <kbd>Properties</kbd>, such as <em>Listing 5.3</em>:</p>
<pre class="mce-root">import io.confluent.kafka.serializers.KafkaAvroSerializer; <br/>import org.apache.avro.Schema;<br/>import org.apache.avro.Schema.Parser;<br/>import org.apache.avro.generic.GenericRecord;<br/>import org.apache.kafka.clients.producer.KafkaProducer;<br/>import org.apache.kafka.clients.producer.Producer;<br/>import org.apache.kafka.common.serialization.StringSerializer;<br/><br/>public final class AvroProducer {<br/>  private final Producer&lt;String, GenericRecord&gt; producer; //1<br/>  private Schema schema;<br/>  <br/>  public AvroProducer(String brokers, String schemaRegistryUrl) { //2<br/>    Properties props = new Properties();<br/>    props.put("bootstrap.servers", brokers);<br/>    props.put("key.serializer", StringSerializer.class); //3<br/>    props.put("value.serializer", KafkaAvroSerializer.class); //4<br/>    props.put("schema.registry.url", schemaRegistryUrl) //5<br/>    producer = new KafkaProducer&lt;&gt;(props);<br/> <br/>    try {<br/>      schema = (new Parser()).parse( new <br/>      File("src/main/resources/healthcheck.avsc")); //6<br/>    } catch (IOException e) {<br/>      // deal with the Exception<br/>    }<br/>  }<br/>  ...<br/>}</pre>
<div class="CDPAlignCenter CDPAlign packt_figref">Listing 5.3: <span>AvroProducer Constructor</span></div>
<p>An analysis of the <kbd>AvroProducer</kbd> constructor shows the following:</p>
<ul>
<li class="mce-root">In line <kbd>//1</kbd>, the values now are of type <kbd>org.apache.avro.generic.GenericRecord</kbd></li>
<li class="mce-root">In line <kbd>//2</kbd>, the constructor now receives the Schema Registry URL</li>
<li class="mce-root">In line <kbd>//3</kbd>, the Serializer type for the messages' keys remains as <kbd>StringSerializer</kbd></li>
<li class="mce-root">In line <kbd>//4</kbd>, the Serializer type for the messages' values now is a <kbd>KafkaAvroSerializer</kbd></li>
<li class="mce-root">In line <kbd>//5</kbd>, the Schema Registry URL is added to the Producer properties</li>
<li class="mce-root">In line <kbd>//6</kbd>, the avsc file with the schema definition is parsed with a Schema Parser</li>
</ul>
<p>Because we have chosen the use of generic records, we have to load the schema. Note that we could have obtained the schema from the Schema Registry, but this is not safe because we do not know which version of the schema is registered. Instead of this, it is a smart and safe practice to store the schema along with the code. In this way, our code will always produce the correct data types, even when someone else changes the schema registered in the Schema Registry.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Now, in the <kbd>src/main/java/kioto/avro</kbd> directory, create a file called <kbd>AvroProducer.java</kbd> with the contents of <em>Listing 5.4</em>:</p>
<pre style="padding-left: 30px">package kioto.avro;<br/>import ...<br/>public final class AvroProducer {<br/> /* here the Constructor code in Listing 5.3 */<br/> <br/> public final class AvroProducer {<br/><br/>  private final Producer&lt;String, GenericRecord&gt; producer;<br/>  private Schema schema;<br/><br/>  public AvroProducer(String brokers, String schemaRegistryUrl) {<br/>    Properties props = new Properties();<br/>    props.put("bootstrap.servers", brokers);<br/>    props.put("key.serializer", StringSerializer.class);<br/>    props.put("value.serializer", KafkaAvroSerializer.class);<br/>    props.put("schema.registry.url", schemaRegistryUrl);<br/>    producer = new KafkaProducer&lt;&gt;(props);<br/>    try {<br/>      schema = (new Parser()).parse(new                   <br/>      File("src/main/resources/healthcheck.avsc"));<br/>    } catch (IOException e) {<br/>      e.printStackTrace();<br/>    }<br/>  }<br/><br/>  public final void produce(int ratePerSecond) {<br/>    long waitTimeBetweenIterationsMs = 1000L / (long)ratePerSecond;<br/>    Faker faker = new Faker();<br/><br/>    while(true) {<br/>      HealthCheck fakeHealthCheck =<br/>          new HealthCheck(<br/>              "HEALTH_CHECK",<br/>              faker.address().city(),<br/>              faker.bothify("??##-??##", true),<br/>              Constants.machineType.values()                                                                                                                 <br/>              [faker.number().numberBetween(0,4)].toString(),<br/>              Constants.machineStatus.values()                                        <br/>              [faker.number().numberBetween(0,3)].toString(),<br/>              faker.date().past(100, TimeUnit.DAYS),<br/>              faker.number().numberBetween(100L, 0L),<br/>              faker.internet().ipV4Address());<br/>              GenericRecordBuilder recordBuilder = new                                       <br/>              GenericRecordBuilder(schema);<br/>              recordBuilder.set("event", fakeHealthCheck.getEvent());<br/>              recordBuilder.set("factory", <br/>              fakeHealthCheck.getFactory());<br/>              recordBuilder.set("serialNumber",                                          <br/>              fakeHealthCheck.getSerialNumber());<br/>              recordBuilder.set("type", fakeHealthCheck.getType());<br/>              recordBuilder.set("status", fakeHealthCheck.getStatus());<br/>              recordBuilder.set("lastStartedAt",                                      <br/>              fakeHealthCheck.getLastStartedAt().getTime());<br/>              recordBuilder.set("temperature",                                          <br/>              fakeHealthCheck.getTemperature());<br/>              recordBuilder.set("ipAddress",   <br/>              fakeHealthCheck.getIpAddress());<br/>              Record avroHealthCheck = recordBuilder.build();<br/>              Future futureResult = producer.send(new ProducerRecord&lt;&gt;               <br/>              (Constants.getHealthChecksAvroTopic(), avroHealthCheck));<br/>      try {<br/>        Thread.sleep(waitTimeBetweenIterationsMs);<br/>        futureResult.get();<br/>      } catch (InterruptedException | ExecutionException e) {<br/>        e.printStackTrace();<br/>      }<br/>    }<br/>  }<br/><br/>  public static void main( String[] args) {<br/>    new AvroProducer("localhost:9092",                                       <br/>    "http://localhost:8081").produce(2);<br/>  }<br/>}</pre>
<div class="CDPAlignCenter CDPAlign packt_figref">Listing 5.4:<span> AvroProducer.java</span></div>
<p>An analysis of the <kbd>AvroProducer</kbd> class shows the following:</p>
<ul>
<li class="mce-root">In line <kbd>//1</kbd>, <kbd>ratePerSecond</kbd> is the number of messages to send in a 1-second period</li>
<li class="mce-root">In line <kbd>//2</kbd>, to simulate repetition, we use an infinite loop (try to avoid this in production)</li>
<li class="mce-root">In line <kbd>//3</kbd>, now we can create <kbd>GenericRecord</kbd> objects using <kbd>GenericRecordBuilder</kbd></li>
<li class="mce-root">In line <kbd>//4</kbd>, we use a Java Future to send the record to the <kbd>healthchecks-avro</kbd> topic</li>
<li class="mce-root">In line <kbd>//5</kbd>, we wait this time to send messages again</li>
<li class="mce-root">In line <kbd>//6</kbd>, we read the result of the Future</li>
<li class="mce-root">In line <kbd>//7</kbd>, everything runs on the broker on the localhost in port <kbd>9092</kbd>, and with the Schema Registry running on the localhost in port <kbd>8081</kbd>, sending two messages in an interval of 1 second</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the AvroProducer</h1>
                </header>
            
            <article>
                
<p class="mce-root">To build the project, run the following command from the <kbd>kioto</kbd> directory:</p>
<pre class="mce-root"><strong>$ gradle jar</strong></pre>
<p class="mce-root">If everything is OK, the output is something like the one shown here:</p>
<pre class="mce-root"><strong>BUILD SUCCESSFUL in 3s</strong><br/><strong> 1 actionable task: 1 executed</strong></pre>
<ol>
<li class="mce-root">If it is not running yet, go to Confluent's directory and start it:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./bin/confluent start</strong></pre>
<ol start="2">
<li class="mce-root">The broker is running on port <kbd>9092</kbd>. To create the <kbd>healthchecks-avro</kbd> topic, execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./bin/kafka-topics --zookeeper localhost:2181 --create --topic healthchecks-avro --replication-factor 1 --partitions 4</strong></pre>
<ol start="3">
<li>Note that we are just creating a normal topic and nothing indicates the messages' format.</li>
<li class="mce-root">Run a console consumer for the <kbd>healthchecks-avro</kbd> topic:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic healthchecks-avro</strong></pre>
<ol start="5">
<li class="mce-root">From our IDE, run the main method of the <kbd>AvroProducer</kbd>.</li>
<li class="mce-root">The output on the console consumer should be similar to the one shown here:</li>
</ol>
<pre>HEALTH_CHECKLake JeromyGE50-GF78HYDROELECTRICRUNNING�����Y,B227.30.250.185<br/>HEALTH_CHECKLockmanlandMW69-LS32GEOTHERMALRUNNING֗���YB72.194.121.48<br/>HEALTH_CHECKEast IsidrofortIH27-WB64NUCLEARSHUTTING_DOWN�̤��YB88.136.134.241<br/>HEALTH_CHECKSipesshireDH05-YR95HYDROELECTRICRUNNING����Y�B254.125.63.235<br/>HEALTH_CHECKPort EmeliaportDJ83-UO93GEOTHERMALRUNNING���Y�A190.160.48.125</pre>
<p>Binary is a horrible format for humans to read, isn't it? We can just read the strings but not the rest of the record.</p>
<p><span>To solve our readability problem, we should use</span> <kbd>kafka-avro-console-consumer</kbd> instead. This fancy consumer deserializes the Avro records and prints them as human-readable JSON objects.</p>
<p><span>From the command line, run an</span> Avro console consumer for the <kbd>healthchecks-avro</kbd> topic:</p>
<pre><strong>$ ./bin/kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic healthchecks-avro</strong></pre>
<p>The output on the console consumer should be similar to this:</p>
<pre><strong>{"event":"HEALTH_CHECK","factory":"Lake Jeromy","serialNumber":" GE50-GF78","type":"HYDROELECTRIC","status":"RUNNING","lastStartedAt":1537320719954,"temperature":35.0,"ipAddress":"227.30.250.185"}</strong><br/><strong>{"event":"HEALTH_CHECK","factory":"Lockmanland","serialNumber":" MW69-LS32","type":"GEOTHERMAL","status":"RUNNING","lastStartedAt":1534188452893,"temperature":61.0,"ipAddress":"72.194.121.48"}</strong><br/><strong>{"event":"HEALTH_CHECK","factory":"East Isidrofort","serialNumber":" IH27-WB64","type":"NUCLEAR","status":"SHUTTING_DOWN","lastStartedAt":1539296403179,"temperature":62.0,"ipAddress":"88.136.134.241"}</strong><br/><strong>...</strong></pre>
<p>Now, we are finally producing Kafka messages in Avro format. With the help of the Schema Registry and the Confluent library, this task is quite simple. As described, after much frustration in productive environments, the generic records scheme is better than the specific records scheme, because it is better to know specifically with which schema we are producing data. Keeping a copy of the schema along with the code gives you that guarantee.</p>
<p class="mce-root">What happens if we fetch the schema from the Schema Registry before producing the data? The correct answer is it depends, and it depends on the<br/>
<kbd>auto.register.schemas</kbd> property. If this property is set to true, when you request a schema that is not in the Schema Registry, it will automatically be registered as a new schema (this option in production environments is not recommended, as it is prone to error). If the property is set to false, the schema will not be stored and, since the schema will not match, we will have a nice exception (do not believe me, reader; go and get proof of this).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java AvroConsumer</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's create a Kafka <kbd>AvroConsumer</kbd> that we will use to receive the input records. As we already know, there are two prerequisites that all the Kafka Consumers should have: to be a <kbd>KafkaConsumer</kbd> and to set specific properties, such as in <em>Listing 5.5</em>:</p>
<pre class="mce-root">import io.confluent.kafka.serializers.KafkaAvroDeserializer;<br/>import org.apache.avro.generic.GenericRecord;<br/>import org.apache.kafka.clients.consumer.KafkaConsumer;<br/>import org.apache.kafka.clients.consumer.Consumer;<br/>import org.apache.kafka.common.serialization.StringSerializer;<br/><br/>public final class AvroConsumer {<br/>  private Consumer&lt;String, GenericRecord&gt; consumer; //1<br/>  public AvroConsumer(String brokers, String schemaRegistryUrl) { //2<br/>     Properties props = new Properties();<br/>     props.put("group.id", "healthcheck-processor");<br/>     props.put("bootstrap.servers", brokers);<br/>     props.put("key.deserializer", StringDeserializer.class); //3<br/>     props.put("value.deserializer", KafkaAvroDeserializer.class); //4<br/>     props.put("schema.registry.url", schemaRegistryUrl); //5<br/>     consumer = new KafkaConsumer&lt;&gt;(props); //6<br/>  }<br/> ...<br/>}</pre>
<div class="CDPAlignCenter CDPAlign packt_figref">Listing 5.5:<span> AvroConsumer constructor</span></div>
<p>An analysis of the changes in the <kbd>AvroConsumer</kbd> constructor shows the following:</p>
<ul>
<li class="mce-root">In line <kbd>//1</kbd>, the values now are of type <kbd>org.apache.avro.generic.GenericRecord</kbd></li>
<li class="mce-root">In line <kbd>//2</kbd>, the constructor now receives the Schema Registry URL</li>
<li class="mce-root">In line <kbd>//3</kbd>, the deserializer type for the messages' keys remains as <kbd>StringDeserializer</kbd></li>
<li class="mce-root">In line <kbd>//4</kbd>, the deserializer type for the values is now <kbd>KafkaAvroDeserializer</kbd></li>
<li class="mce-root">In line <kbd>//5</kbd>, the Schema Registry URL is added to the consumer properties</li>
<li class="mce-root">In line <kbd>//6</kbd>, with these <kbd>Properties</kbd>, we build a <kbd>KafkaConsumer</kbd> with string keys and <kbd>GenericRecord</kbd> values: <kbd>&lt;String, GenericRecord&gt;</kbd></li>
</ul>
<p>It is important to note that when defining the Schema Registry URL for the deserializer to fetch schemas, the messages only contain the schema ID and not the schema itself.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java AvroProcessor</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, in the <kbd>src/main/java/kioto/avro</kbd> directory, create a file called <kbd>AvroProcessor.java</kbd> with the contents of <em>Listing 5.6</em>:</p>
<pre class="mce-root">package kioto.plain;<br/>import ...<br/>public final class AvroProcessor {<br/>  private Consumer&lt;String, GenericRecord&gt; consumer;<br/>  private Producer&lt;String, String&gt; producer;<br/><br/>  public AvroProcessor(String brokers , String schemaRegistryUrl) {<br/>    Properties consumerProps = new Properties();<br/>    consumerProps.put("bootstrap.servers", brokers);<br/>    consumerProps.put("group.id", "healthcheck-processor");<br/>    consumerProps.put("key.deserializer", StringDeserializer.class);<br/>    consumerProps.put("value.deserializer", KafkaAvroDeserializer.class);<br/>    consumerProps.put("schema.registry.url", schemaRegistryUrl);<br/>    consumer = new KafkaConsumer&lt;&gt;(consumerProps);<br/>    <br/>    Properties producerProps = new Properties();<br/>    producerProps.put("bootstrap.servers", brokers);<br/>    producerProps.put("key.serializer", StringSerializer.class);<br/>    producerProps.put("value.serializer", StringSerializer.class);<br/>    producer = new KafkaProducer&lt;&gt;(producerProps);<br/> }</pre>
<div class="CDPAlignCenter CDPAlign packt_figref">Listing 5.6:<span> AvroProcessor.java (part 1)</span></div>
<p>An analysis of the first part of the <kbd>AvroProcessor</kbd> class shows the following:</p>
<ul>
<li>In the first section, we declare an <kbd>AvroConsumer</kbd>, as in <em>Listing 5.5</em></li>
<li>In the second section, we declare an <kbd>AvroProducer</kbd>, as in <em>Listing 5.4</em></li>
</ul>
<p>Now, in the <kbd>src/main/java/kioto/avro</kbd> directory, let's complete the <kbd>AvroProcessor.java</kbd> file with the contents of <em>Listing 5.7</em>:</p>
<pre class="mce-root">public final void process() {<br/>  consumer.subscribe(Collections.singletonList(<br/>    Constants.getHealthChecksAvroTopic())); //1<br/>    while(true) {<br/>      ConsumerRecords records = consumer.poll(Duration.ofSeconds(1L));<br/>      for(Object record : records) {<br/>        ConsumerRecord it = (ConsumerRecord) record;<br/>        GenericRecord healthCheckAvro = (GenericRecord) it.value(); //2<br/>        HealthCheck healthCheck = new HealthCheck ( //3<br/>          healthCheckAvro.get("event").toString(),<br/>          healthCheckAvro.get("factory").toString(),<br/>          healthCheckAvro.get("serialNumber").toString(),<br/>          healthCheckAvro.get("type").toString(),<br/>          healthCheckAvro.get("status").toString(),<br/>          new Date((Long)healthCheckAvro.get("lastStartedAt")),<br/>          Float.parseFloat(healthCheckAvro.get("temperature").toString()),<br/>          healthCheckAvro.get("ipAddress").toString());<br/>          LocalDate startDateLocal= <br/>          healthCheck.getLastStartedAt().toInstant()<br/>                      .atZone(ZoneId.systemDefault()).toLocalDate(); //4<br/>          int uptime = Period.between(startDateLocal,     <br/>          LocalDate.now()).getDays(); //5<br/>          Future future =<br/>               producer.send(new ProducerRecord&lt;&gt;(<br/>                             Constants.getUptimesTopic(),<br/>                             healthCheck.getSerialNumber(),<br/>                             String.valueOf(uptime))); //6<br/>          try {<br/>            future.get();<br/>          } catch (InterruptedException | ExecutionException e) {<br/>            // deal with the exception<br/>          }<br/>        }<br/>      }<br/>    }<br/><br/>    public static void main(String[] args) {<br/>       new      <br/>  AvroProcessor("localhost:9092","http://localhost:8081").process();//7<br/>    }<br/>}</pre>
<div class="CDPAlignCenter CDPAlign packt_figref">Listing 5.7:<span> AvroProcessor.java (part 2)</span></div>
<p>An analysis of the <kbd>AvroProcessor</kbd> shows the following:</p>
<ul>
<li class="mce-root">In line <kbd>//1</kbd>, the consumer is subscribed to the new Avro topic.</li>
<li class="mce-root">In line <kbd>//2</kbd>, we are consuming messages of type <kbd>GenericRecord</kbd>.</li>
<li class="mce-root">In line <kbd>//3</kbd>, the Avro record is deserialized to extract the <kbd>HealthCheck</kbd> object.</li>
<li class="mce-root">In line <kbd>//4</kbd>, the start time is transformed into the format in the current time zone.</li>
<li class="mce-root">In line <kbd>//5</kbd>, the uptime is calculated.</li>
<li class="mce-root">In line <kbd>//6</kbd>, the uptime is written to the <kbd>uptimes</kbd> topic, using the serial number as the key and the uptime as the value. Both values are written as normal strings.</li>
<li class="mce-root">In line <kbd>//7</kbd>, everything runs on the broker on the localhost in port <kbd>9092</kbd> and with the Schema Registry running on the localhost in port <kbd>8081</kbd>.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root">As mentioned previously, the code is not type-safe; all the types are checked at runtime. So, be extremely careful with that. For example, the strings are not <kbd>java.lang.String</kbd>; they are of type <kbd>org.apache.avro.util.Utf8</kbd>. Note that we avoid the cast by calling the <kbd>toString()</kbd> method directly on the objects. The rest of the code remains equal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the AvroProcessor</h1>
                </header>
            
            <article>
                
<p class="mce-root">To build the project, run the following command from the <kbd>kioto</kbd> directory:</p>
<pre class="mce-root"><strong>$ gradle jar</strong></pre>
<p class="mce-root">If everything is correct, the output will be something like this:</p>
<pre class="mce-root"><strong>BUILD SUCCESSFUL in 3s</strong><br/><strong> 1 actionable task: 1 executed</strong></pre>
<p class="mce-root">Run a console consumer for the <kbd>uptimes</kbd> topic, as shown here:</p>
<pre><strong>$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic uptimes --property print.key=true</strong></pre>
<ol>
<li>From the IDE, run the main method of the <kbd>AvroProcessor</kbd></li>
<li class="mce-root">From the IDE, run the main method of the <kbd>AvroProducer</kbd></li>
<li class="mce-root">The output on the console consumer for the <kbd>uptimes</kbd> topic should be similar to this:</li>
</ol>
<pre style="padding-left: 60px"><strong>EW05-HV36 33</strong><br/><strong>BO58-SB28 20</strong><br/><strong>DV03-ZT93 46</strong><br/><strong>...</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we showed, instead of sending data in JSON format, how to use AVRO as the serialization format. The main benefit of AVRO (over JSON, for example) is that the data must conform to the schema. Another advantage of AVRO over JSON is that the messages are more compact when sent in binary format, although JSON is human readable.</p>
<p class="mce-root">The schemas are stored in the Schema Registry, so that all users can consult the schema version history, even when the code of the producers and consumers for those messages is no longer available.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Apache Avro also guarantees backward and forward compatibility of all messages in this format. Forward compatibility is achieved following some basic rules, such as when adding a new field, declaring its value as optional.</p>
<p class="mce-root">Apache Kafka encourages the use of Apache Avro and the Schema Registry for the storage of all data and schemas in Kafka systems, instead of using only plain text or JSON. Using that winning combination, you guarantee that your system can evolve.</p>


            </article>

            
        </section>
    </body></html>