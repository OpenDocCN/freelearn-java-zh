- en: Concurrent Programming in Scala
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala中的并发编程
- en: '"Yesterday is not ours to recover, but today is to try and tomorrow is to win
    or lose."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “昨天不属于我们恢复，但今天可以尝试，明天可以赢或输。”
- en: '- Anonymous'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: —— 匿名
- en: 'The idea that modern computers with multicore architectures give better performance
    is based on the fact that multiple processors can run separate processes simultaneously.
    Each process can run more than one thread to complete specific tasks. Picturing
    this, we can write programs with multiple threads working simultaneously to ensure
    better performance and responsiveness. We call this concurrent programming. In
    this chapter, our goal is to understand Scala''s offerings in concurrent programming.
    There are multiple ways we can use constructs to write concurrent programs. We''ll
    learn about them in this chapter. Let''s check out what will be here for us:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现代计算机的多核架构能够提供更好的性能这一观点是基于这样一个事实：多个处理器可以同时运行不同的进程。每个进程可以运行多个线程以完成特定任务。想象一下，我们可以编写具有多个线程同时工作以确保更好的性能和响应性的程序。我们称之为并发编程。在本章中，我们的目标是了解Scala在并发编程方面的提供。我们可以使用多种方式使用结构来编写并发程序。我们将在本章中学习它们。让我们看看这里将有什么：
- en: Concurrent programming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发编程
- en: 'Building blocks of concurrency:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发构建块：
- en: Process and threads
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程和线程
- en: Synchronization and locks
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步和锁
- en: Executor and ExecutionContext
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器和执行上下文
- en: Lock free programming
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无锁编程
- en: Asynchronous programming using Futures and Promises
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Futures和Promises进行异步编程
- en: Parallel Collections
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行集合
- en: Before we start learning about the ways we can write concurrent programs, it's
    important to understand the underlying picture. Let's start understanding concurrent
    programming and then we'll go through the basic building blocks of concurrency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始学习我们可以编写并发程序的方法之前，了解底层图景非常重要。让我们开始了解并发编程，然后我们将探讨并发的基本构建块。
- en: Concurrent programming
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发编程
- en: It's a programming approach where a set of computations can be performed simultaneously.
    These set of computations might share the same resources such as memory. How's
    it different from sequential programming? In sequential programming, every computation
    can be performed one after another. In the case of concurrent programs, more than
    one computation can be performed in the same time period.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种编程方法，其中可以同时执行一系列计算。这些计算可能共享相同的资源，例如内存。它与顺序编程有何不同？在顺序编程中，每个计算可以一个接一个地执行。在并发程序的情况下，可以在同一时间段内执行多个计算。
- en: By executing multiple computations, we can perform multiple logical operations
    in the program at the same time, resulting in better performance. Programs can
    run faster than before. This may sound cool; concurrency actually makes implementing
    real scenarios easier. Think about an internet browser; we can stream our favorite
    videos and download some content at the same time. The download thread does not
    affect the streaming of the video in any way. This is possible because content
    download and video streams on a browser tab are separate logical program parts,
    and hence can run simultaneously.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行多个计算，我们可以在程序中同时执行多个逻辑操作，从而提高性能。程序可以比以前运行得更快。这听起来可能很酷；并发实际上使得实现真实场景变得更加容易。想想互联网浏览器；我们可以同时流式传输我们最喜欢的视频和下载一些内容。下载线程不会以任何方式影响视频的流式传输。这是可能的，因为浏览器标签上的内容下载和视频流是独立的逻辑程序部分，因此可以同时运行。
- en: Similarly, in programs where we might have the user to perform some I/O operations
    for input, at the same time we want to run the program. We need these two parts
    to run simultaneously. Running these parts together makes it responsive to user
    interactions. Writing a concurrent program comes in handy in such cases. From
    a very cool web application running on an internet browser to games running on
    your mobile device, responsiveness and good user experience is possible because
    of concurrent programs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在可能需要用户执行一些I/O操作以输入，同时我们想要运行程序的情况下，我们需要这两个部分同时运行。将这两个部分一起运行使其对用户交互做出响应。在这种情况下编写并发程序很有用。从运行在互联网浏览器上的非常酷的Web应用程序到运行在您的移动设备上的游戏，响应性和良好的用户体验都是由于并发程序而成为可能的。
- en: This is why learning about concurrency abstractions is important, and what's
    more important is to keep them simple in our program implementations. So, let's
    go through the basic building blocks of concurrency.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么了解并发抽象很重要，更重要的是在我们的程序实现中保持它们简单。因此，让我们来探讨并发的基本构建块。
- en: Building blocks of concurrency
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发构建块
- en: Scala is a JVM-based language, so programs written in Scala run in JVM. **JVM**,
    as we already know, is **Java Virtual Machine**, and runs as a single process
    in our operating system. In JVM, one of the basic concurrency constructs is a
    *thread*; we can create/use multiple threads as part of our Scala program. So,
    for a basic understanding of processes and threads, let's go through them.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Scala是一种基于JVM的语言，因此用Scala编写的程序在JVM中运行。**JVM**，正如我们已知的，是**Java虚拟机**，并在我们的操作系统中作为一个单独的进程运行。在JVM中，一个基本并发结构是*线程*；我们可以在Scala程序中创建/使用多个线程。因此，为了对进程和线程有一个基本理解，让我们来探讨它们。
- en: Understanding processes and threads
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解进程和线程
- en: Think of a process as a program or application that our computer might have
    to run. This process is going to have some code that's executable, a **process
    identifier** (**pid**), and at least one thread of execution. The process might
    consume some computer resources as well, such as memory. Every process isolates
    itself from other processes when it comes to consume memory; this means two processes
    cannot use the same memory block.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个过程视为一个程序或应用，我们的计算机可能需要运行它。这个过程将包含一些可执行的代码，一个**进程标识符**（**pid**），以及至少一个执行线程。这个过程可能还会消耗一些计算机资源，例如内存。每当一个过程需要消耗内存时，它会将自己与其他过程隔离开来；这意味着两个进程不能使用相同的内存块。
- en: Modern computers come with multiple processor cores. These cores are assigned
    tasks as executable program parts for execution in certain time slices. The task
    of assigning these executable parts is done by the operating system. Most of the
    operating systems nowadays use a mechanism called **pre-emptive multitasking**,
    which is the simultaneous execution of multiple executable parts from all running
    processes. These executable parts are nothing but threads. It means that each
    process needs to have at least one thread, we can call it the main thread, in
    order to run properly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现代计算机具有多个处理器核心。这些核心被分配为在特定时间片中执行的程序部分。分配这些可执行部分的任务由操作系统完成。如今的大多数操作系统都使用一种称为**抢占式多任务**的机制，这是从所有运行进程同时执行多个可执行部分。这些可执行部分不过是线程。这意味着每个进程至少需要有一个线程，我们可以称之为主线程，以便正确运行。
- en: 'It''s clear that the process within an operating system uses some memory resources,
    and it can contain multiple threads. Now, these threads from a particular process
    are free to share the memory block assigned, but two processes cannot do the same.
    It''ll be easier to understand this with the help of the following figure:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，操作系统内的进程使用一些内存资源，并且可以包含多个线程。现在，来自特定进程的这些线程可以自由共享分配的内存块，但两个进程不能这样做。借助以下图示，这将更容易理解：
- en: '![](img/00056.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: The previous diagram is a simplified version for a system with two processor
    cores, a *pre-emptive multitasking operating system* and *memory*. We have a separate
    memory resource allocated for different processes running, in our case, **Process
    1**, **Process 2**, and **Process 3**. The memory block for **Process 1** has
    no access to the memory block for **Process 2** or **Process 3**. Each process
    contains more than one thread. Each thread can access the memory allocated from
    the parent process. These threads can share the memory allocated. Now, what happens
    is the operating system assigns these executable blocks, in other words *threads*,
    to process cores for execution, as shown in our preceding diagram.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图是针对具有两个处理器核心、*抢占式多任务操作系统*和*内存*的系统的简化版本。我们为运行的不同进程分配了单独的内存资源，在我们的例子中，是**进程1**、**进程2**和**进程3**。**进程1**的内存块无法访问**进程2**或**进程3**的内存块。每个进程包含多个线程。每个线程都可以访问从父进程分配的内存。这些线程可以共享分配的内存。现在，操作系统将这些可执行块，换句话说就是*线程*，分配给处理核心进行执行，正如我们在先前的图中所示。
- en: 'In a particular time slice:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定的时间片中：
- en: '**Core 1** is executing **Thread 1** from **Process 1**'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心1**正在执行**进程1**中的**线程1**'
- en: '**Core 2** is executing **Thread 2** from **Process 3**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心2**正在执行**进程3**中的**线程2**'
- en: These two executions are happening simultaneously. We already know that JVM
    runs as a process; the programs we write are going to have threads as entities.
    For our program to run, we need at least a *main thread* that can be the entry
    point to our application. We can create more threads as instances of the `java.lang.Thread`
    class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个执行是同时发生的。我们已经知道JVM作为一个进程运行；我们编写的程序将具有线程作为实体。为了我们的程序运行，我们需要至少一个*主线程*，它可以作为我们应用程序的入口点。我们可以创建更多线程作为`java.lang.Thread`类的实例。
- en: Now that we know we can have multiple parts of our application running together,
    it's important to understand that we need some way to synchronize them. By synchronizing,
    we can ensure one particular execution is not going to affect any other. Threads
    within a process have access to the same memory block, hence it might be possible
    that two threads try to access the memory at the same time—this might cause problems.
    Threads are low-level concurrency abstractions in Scala, and as the number of
    concurrent parts or threads increases, complexity also increases with them. To
    understand how do we restrict other threads to access some block of code simultaneously,
    first we need to understand how synchronization works.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们可以让应用程序的多个部分同时运行，重要的是要理解我们需要某种方式来同步它们。通过同步，我们可以确保特定的执行不会影响其他执行。进程内的线程可以访问相同的内存块，因此可能两个线程会同时尝试访问内存——这可能会引起问题。线程是Scala中的低级并发抽象，随着并发部分或线程数量的增加，复杂性也随之增加。为了理解我们如何限制其他线程同时访问某些代码块，首先我们需要理解同步是如何工作的。
- en: Locks and synchronization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 锁和同步
- en: 'We talked about threads in previous sections—we''ll first try to create a few
    ourselves before discussing them further. Let''s write some code for that:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中讨论了线程——我们首先尝试自己创建一些线程，然后再进一步讨论。让我们为这个目的编写一些代码：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A few pointers for the preceding code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前面代码的一些提示：
- en: We simply created an object extending `App` to create the application entry
    point.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们简单地创建了一个扩展`App`的对象来创建应用程序的入口点。
- en: We created a class named `FirstThread` that extends `Thread`, which is nothing
    but the same `java.lang.Thread` we talked about in the previous section.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`FirstThread`的类，它扩展了`Thread`类，这实际上就是我们之前章节中提到的相同的`java.lang.Thread`。
- en: When we create a thread, we might want to specify what it needs to run. That
    can be defined via overriding the `run` method.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们创建一个线程时，我们可能想要指定它需要运行的内容。这可以通过重写`run`方法来定义。
- en: Until *point 3*, we have defined our thread class; now, to run the thread, we'll
    create its instance, and then call the `start` method.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到*第3点*，我们已经定义了我们的线程类；现在，为了运行线程，我们将创建其实例，然后调用`start`方法。
- en: The `start` method triggers the execution of the thread.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`start`方法触发线程的执行。'
- en: Finally, we printed the thread's name. First, the `main` current thread, and
    then the `firstThread` class name.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印了线程的名称。首先，是`main`主线程，然后是`firstThread`类的名称。
- en: 'Running the application will give us the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 运行应用程序将给出以下输出：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So from the first run, it''s clear that a thread called `main` runs the application,
    and as we create more and more threads, these threads also come into the picture.
    It''s great to have multiple threads working together for us to perform some computation.
    We know from our previous discussions that an OS performs the scheduling of task
    execution, so it''s out of our control which thread will get executed in which
    order. Now, think of a scenario where you might want to perform a read and write
    operation to a variable in your program. With multiple threads performing such
    a task, it might be possible to see inconsistencies in the result. It means that
    this execution is exposed to *race conditions*; in other words, it depends on
    the execution schedule of statements by the OS. To better understand this, let''s
    try out the scenario we discussed:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从第一次运行来看，很明显一个名为`main`的线程运行了应用程序，随着我们创建越来越多的线程，这些线程也进入了画面。有多个线程一起工作以执行某些计算是非常好的。我们知道从之前的讨论中，操作系统执行任务执行的调度，所以哪个线程以何种顺序执行不在我们的控制范围内。现在，考虑一个你可能想要对你的程序中的变量执行读写操作的场景。当多个线程执行此类任务时，可能会看到结果的不一致性。这意味着这个执行暴露于*竞态条件*；换句话说，它取决于操作系统对语句执行的调度。为了更好地理解这一点，让我们尝试我们讨论的场景：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this small application, we first create a variable `counter`; we are going
    to read and write to this variable using two threads. Next, we have two methods,
    first `readWriteCounter` and `printCounter`. The `readWriteCounter` method is
    doing as the name says. This method increments the counter (reading operation)
    and assigns the `incrementedCounter` to the `counter` variable. The second method, `printCounter`,
    takes an integer parameter to increment counter the number of times specified
    and prints that.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小型应用中，我们首先创建了一个变量`counter`；我们将使用两个线程来读取和写入这个变量。接下来，我们有两个方法，第一个是`readWriteCounter`，第二个是`printCounter`。`readWriteCounter`方法正如其名。这个方法增加计数器（读取操作）并将增加后的计数器`incrementedCounter`赋值给`counter`变量。第二个方法`printCounter`接受一个整数参数，指定增加计数器的次数，并打印出来。
- en: After defining all these, we created a thread with the name `First` and called
    our `printCounter` method, overriding the `run` method. To observe the behavior,
    we're supposed to call the `printCounter` from this `First` thread and the main
    application thread. Since two threads are working simultaneously, it's expected
    that output of these two shouldn't contain the same number. We also called `printCounter`
    from the application as the final statement of the program.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了所有这些之后，我们创建了一个名为`First`的线程并调用了我们的`printCounter`方法，覆盖了`run`方法。为了观察行为，我们应该从这个`First`线程和主应用程序线程中调用`printCounter`。由于两个线程是同时工作的，我们预期这两个线程的输出不应该包含相同的数字。我们还从应用程序中调用了`printCounter`作为程序的最终语句。
- en: Running the program couple of times (if you're lucky, for the first time), you
    might be able to see some inconsistent behavior.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 运行程序几次（如果你很幸运，第一次），你可能会看到一些不一致的行为。
- en: 'Run:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the output from both the threads, we can see the number `1` appeared twice,
    while we know it shouldn''t have happened. We see that behaviour due to read and
    write operations happening to our `counter` variable via multiple threads in the
    following snippet:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个线程的输出中，我们可以看到数字`1`出现了两次，而我们知道这不应该发生。我们看到这种行为是因为在我们的`counter`变量上通过多个线程进行的读写操作。如下所示：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: By the time the `counter = incrementCounter` statement gets a chance to execute,
    the `counter` variable gets incremented twice (by multiple threads). This is causing
    inconsistency. The problem lies in the execution of these two statements; these
    have to be atomic in nature to give a consistent output where the same number
    cannot appear for different threads. By *atomic*, we mean these two statements
    have to be executed together by the same thread.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当`counter = incrementCounter`语句有机会执行时，`counter`变量被增加了两次（由多个线程）。这导致了不一致。问题出在这两个语句的执行上；这些语句必须是原子的，以提供一致的输出，其中相同的数字不能出现在不同的线程中。当我们说*原子*时，意味着这两个语句必须由同一个线程一起执行。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the application, it can be seen that our concerned method block is not guarded
    by this *synchronized* statement:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用中，我们可以看到我们关心的方法块没有被这个*synchronized*语句保护：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'By using this, we make the synchronized statement refer to the current object
    to guard the block. We could also make a particular instance of some type, let''s
    say *Any* and that instance can work as a guard to our synchronised clock. It''s
    shown as following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个，我们让synchronized语句指向当前对象以保护这个块。我们也可以创建某个类型的特定实例，比如说*Any*，这个实例可以作为我们的同步时钟的守卫。如下所示：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating a thread is an expensive operation—if you have more computations, you
    want to perform concurrently and you create several threads to compute those.
    It'll be less performant and with some shared data access, your life will be worse.
    So, to prevent this costly operation from happening, JDK has come up with the
    concept of *thread-pools.* In *thread-pools*, there are multiple thread instances
    provided. These threads within a pool remain in *waiting* state; when you want
    to perform some computation, we can run these. The job of running these is done
    by the `executor`*.* Let's try to understand it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个线程是一个昂贵的操作——如果你有更多的计算需要并发执行，你创建了几个线程来计算这些。这将导致性能下降，并且由于一些共享数据访问，你的生活将变得更糟。所以，为了防止这种昂贵的操作发生，JDK提出了*线程池*的概念。在*线程池*中，提供了多个线程实例。这些线程在池中保持*等待*状态；当你想要执行一些计算时，我们可以运行这些线程。运行这些线程的工作由`executor`*.*来完成。让我们试着理解它。
- en: Executor and ExecutionContext
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Executor和ExecutionContext
- en: Executor is an interface that encapsulates the *thread-pool* and deals with
    executing computations via one of the threads or the caller thread itself. One
    example of an executor is `java.util.concurrent.ForkJoinPool`*.* Scala's implementation
    of such an executor is `ExecutionContext` which internally uses the same `ForkJoinPool`*.*
    Before going further to see an example, why not think of the need for this `Executor`
    mechanism?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是一个接口，它封装了*线程池*，并通过其中一个线程或调用者线程本身来执行计算。执行器的一个例子是`java.util.concurrent.ForkJoinPool`*.*
    Scala对这种执行器的实现是`ExecutionContext`，它内部使用相同的`ForkJoinPool`*.* 在进一步查看示例之前，为什么不思考一下这种`Executor`机制的需求呢？
- en: 'As programmers, while writing performance-efficient concurrent applications,
    we might have to deal with two major tasks, the first being defining instances
    of concurrency abstraction, let''s say *threads*, and making sure they handle
    our data/state in the right manner. Second, to use these *threads* in our program.
    Now, the creation of all these threads, if created by us, are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作为程序员，在编写性能高效的并发应用程序时，我们可能需要处理两个主要任务，第一个是定义并发抽象的实例，比如说*线程*，并确保它们以正确的方式处理我们的数据/状态。第二个，在我们的程序中使用这些*线程*。现在，如果我们自己创建所有这些线程，它们将是：
- en: Costly operations
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本高昂的操作
- en: Complex to manage
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以管理
- en: 'Hence, mechanisms like `Executor` take away the work of creating these threads.
    We don''t explicitly decide which thread will execute the logic we provide; we
    also don''t need to manage those. Executor implementations, when used, create
    daemon and worker threads. When we assign computation via the `execute` method,
    a particular worker thread is assigned the task. Shutting down a daemon thread
    shuts down all the worker threads. This''ll be easier to understand with the help
    of the following code snippet:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像`Executor`这样的机制可以免除创建这些线程的工作。我们不必明确决定哪个线程将执行我们提供的逻辑；我们也不需要管理它们。当使用执行器实现时，会创建守护线程和工作线程。当我们通过`execute`方法分配计算时，特定的工作线程被分配任务。关闭守护线程将关闭所有工作线程。这可以通过以下代码片段的帮助更容易理解：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the application, we used two `Executor` implementations; the first is from
    `java.util.concurrent.ForkJoinPool` and the second is similar to Scala-specific
    `ExecutionContext`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序中，我们使用了两个`Executor`实现；第一个来自`java.util.concurrent.ForkJoinPool`，第二个类似于Scala特定的`ExecutionContext`：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For both implementations, we have an execute method, which expects a `Runnable`
    instance. To create `Runnable` instances, we have to define a run method. It's
    another way to create a thread instance. In the definition of the run method,
    we just printed the executor thread's name.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种实现，我们都有一个`execute`方法，它期望一个`Runnable`实例。为了创建`Runnable`实例，我们必须定义一个`run`方法。这是创建线程实例的另一种方式。在`run`方法的定义中，我们只是打印了执行器线程的名称。
- en: 'But running the above program gives you no output. The reason for such behavior
    is that both the implementations create a *daemon thread,* which shuts down after
    the first run. Shutdown of a daemon thread kills all worker threads. Calling the
    `execute` method wakes up `workerthreads`*.* These `workerthreads` execute the
    run method asynchronously. Hence, we''ll try to include some timeout to wait for
    a small duration as the last statement by calling the `Thread.sleep` method:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但是运行上述程序没有输出。这种行为的原因是两种实现都创建了一个*守护线程*，在第一次运行后关闭。守护线程的关闭会杀死所有工作线程。调用`execute`方法唤醒`workerthreads`*.*
    这些`workerthreads`异步执行run方法。因此，我们将尝试通过调用`Thread.sleep`方法来包含一些超时，等待一小段时间作为最后一条语句：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After including some waiting time for the execution by the worker threads,
    we get the output. As shown, the output tells us the *thread* names: both are
    *worker threads.* The first one, named `scala-execution-context-global-11`, is
    from Scala''s `ExecutionContext` and the second, named `ForkJoinPool-1-worker-1`,
    is from Java''s `ForkJoinPool`*.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作线程执行一些等待时间后，我们得到输出。如图所示，输出告诉我们*线程*名称：两者都是*工作线程*。第一个，名为`scala-execution-context-global-11`，来自Scala的`ExecutionContext`，第二个，名为`ForkJoinPool-1-worker-1`，来自Java的`ForkJoinPool`*.*
- en: These *thread-pools* and their implementations become the basis for higher-level
    concurrency abstractions. We also encountered a bit of asynchrony in the example
    when we waited for the result of the execution. It's not wrong to say asynchrony
    is subsumed in *concurrency*, as asynchronous programs tend to execute outside
    of the main flow of the program. Hence, multiple async computations can be performed
    at the same time; once we get back the results for these computations, we can
    perform the desired operation then.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些线程池及其实现成为高级并发抽象的基础。在示例中，当我们等待执行结果时，我们也遇到了一点异步。说异步包含在并发中并没有错，因为异步程序倾向于在程序的主流程之外执行。因此，可以同时执行多个异步计算；一旦我们得到这些计算的结果，我们就可以执行所需的操作。
- en: Scala provides constructs from the standard library for asynchronous programming,
    as well as multiple libraries that provide async constructs to make it easier
    to develop programs for us, the developers. Let's go through those constructs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Scala提供了标准库中的异步编程构造，以及多个库，这些库提供了异步构造，使开发者更容易开发程序。让我们来看看这些构造。
- en: Asynchronous programming
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步编程
- en: If we try to define *asynchronous programming,* we come up with something that
    states that it's a programming approach in which computations, which can be *tasks
    or threads,* execute outside of the basic program flow. In programming terminologies,
    these computations execute on different call stacks, not the current one. Because
    of this, it's possible for us to think of more than one async computation happening
    at the same time; we can wait for each to happen so that aggregation of a result
    or some other result manipulation is possible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试定义异步编程，我们会得出这样一个结论：它是一种编程方法，其中计算（可以是任务或线程）在基本程序流程之外执行。在编程术语中，这些计算在不同的调用栈上执行，而不是当前的调用栈。正因为如此，我们可以同时考虑多个异步计算；我们可以等待每个计算发生，以便进行结果聚合或其他结果操作。
- en: 'Up until now, we''ve looked at three of these terminologies such as *concurrency*,
    *multithreading*, and *asynchronous*. We tend to confuse these but given our discussions,
    it''s clear that *asynchronous* subsumes *concurrency* and not *multithreading.*
    We know that asynchrony can be achieved using scheduling:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了这些术语中的三个，如并发、多线程和异步。我们往往容易混淆这些概念，但根据我们的讨论，很明显异步包含并发，而不是多线程。我们知道异步可以通过调度来实现：
- en: '![](img/00057.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00057.jpeg)'
- en: Well, the fact that we have to compose the results of multiple async problems
    running at the same time means we might end up needing some sort of synchronization.
    Fortunately, we don't have to deal with managing these cumbersome tasks, as Scala's
    offerings manage those using the `ExecutionContext`*.* One of those asynchronous
    offerings is *Futures* in Scala. Let's talk about `Futures` in Scala.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们必须组合同时运行的多个异步问题的结果这一事实意味着我们可能需要某种形式的同步。幸运的是，我们不必处理这些繁琐的任务，因为Scala的`ExecutionContext`提供了管理这些任务的方法。这些异步提供之一是Scala中的`Futures`。让我们来谈谈Scala中的`Futures`。
- en: Working with Futures
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Futures
- en: 'The idea here is simple; we have our simple program flow. If we make some complex
    time-consuming computation in the main program flow, it''ll be blocking and the
    experience won''t be good. Hence, we want to perform that time-consuming computation
    outside of the basic program flow, and continue doing other stuff in the main
    program flow, keeping the value (that will be available at a later point in time)
    of computation. Once the value is available, we use it via a mechanism. The way
    we can picture this is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法很简单；我们有一个简单的程序流程。如果我们把一些复杂且耗时的计算放在主程序流程中，它将会阻塞，用户体验不会好。因此，我们希望在基本程序流程之外执行这些耗时的计算，同时在主程序流程中继续做其他事情，保持计算值（将在稍后某个时间点可用）的值。一旦值可用，我们通过某种机制使用它。我们可以这样想象这个过程：
- en: '![](img/00058.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00058.jpeg)'
- en: For now, the two entities we can think of are: `Futurecomputation` and a future
    `value`*.* These two are different; a Future computation is the time-consuming
    part you want to compute asynchronously and future value is the `value` reference
    on which we depend on our program flow. Once the Future computation starts in
    a separate flow, the program does not stop its execution and when the value becomes
    available, the part gets executed. The `ExecutionContext` takes care of the execution
    of that part, where we may use the `value` Future*.*
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们可以想到的两个实体是：`Futurecomputation` 和未来的 `value`*.* 这两者是不同的；Future computation
    是你想要异步计算耗时部分，而 future value 是我们依赖于程序流程的 `value` 引用。一旦 Future computation 在一个单独的流程中开始，程序不会停止执行，当值变得可用时，该部分才会执行。`ExecutionContext`
    负责执行这部分，我们可能使用 `value` Future*.*
- en: It's clear that whenever we start some Future computation, we might have to
    provide an execution context with it. In Scala, the Future type resides in the `scala.concurrent`
    package, the same package that has `ExecutionContext` and its executor `ExecutionContextExecutor`*.*
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，每次我们开始一些 Future computation 时，我们可能需要提供与之相关的执行上下文。在 Scala 中，`Future` 类型位于
    `scala.concurrent` 包中，这个包还有 `ExecutionContext` 和它的执行器 `ExecutionContextExecutor`*.*
- en: 'The Future value is represented by `Future[T]`, where `T` is the type of the
    value that''ll be available at some later point in time. Hence in our programs,
    whenever we need some value that''s a result of some asynchronous computation,
    we represent that value with this mentioned type. An example will clear this up:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Future value 用 `Future[T]` 表示，其中 `T` 是将在某个未来的时间点可用的值的类型。因此，在我们的程序中，每当我们需要一些异步计算的结果值时，我们用这个提到的类型来表示那个值。一个例子可以澄清这一点：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Run:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this simple application, we specified a  file with some content. The file
    has information about few football players. Now, to read the file contents as
    well as parsing and encoding them to `Player` instances may take some time, hence
    we decided to take the `load`*,* `parse`, and `encode` step as a Future computation,
    and the resulting value is going to be a Future value of type `Future[List[Player]]`*.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的应用程序中，我们指定了一个包含一些内容的文件。这个文件有关于几个足球运动员的信息。现在，为了读取文件内容以及解析和编码成 `Player`
    实例可能需要一些时间，因此我们决定将 `load`*、* `parse` 和 `encode` 步骤作为一个 Future computation，结果值将是一个类型为
    `Future[List[Player]]`*.* 的 Future value。
- en: 'Now, after defining such a computation, we checked if computation completed.
    Then we waited for some time and again tried to check if it completed. Running
    the application gives us `false` and then `true` respectively. If we think of
    this example via a diagram the flow might look like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在定义了这样的计算之后，我们检查了计算是否完成。然后我们等待一段时间，再次尝试检查它是否完成。运行应用程序给我们的是 `false` 和然后 `true`。如果我们通过图表来思考这个例子，流程可能看起来像这样：
- en: '![](img/00059.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00059.jpeg)'
- en: It's easy to understand the execution flow with the help of this diagram; for
    the duration in which the computation is in progress, the `isCompleted` flag remains
    false. After completion, it's set to true. After that, we can use the value of
    future, but in this example, we didn't use the value; also, the question arises
    of how we can use it. Do we have to check again and again for the value to be
    available? This sounds bad, so another way is to register a *callback* for this
    async computation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这张图，我们可以很容易地理解执行流程；在计算进行期间，`isCompleted` 标志保持为 false。完成后，它被设置为 true。之后，我们可以使用
    future 的值，但在这个例子中，我们没有使用这个值；此外，也出现了如何使用它的疑问。我们是否需要再次检查值是否可用？这听起来很糟糕，所以另一种方法是为此异步计算注册一个
    *回调*。
- en: 'Okay, what''s a callback? To answer this, let''s first extend our program to
    register one for our Future computation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，什么是回调？为了回答这个问题，让我们首先扩展我们的程序来为我们的 Future computation 注册一个：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It''s worth noting that callbacks are one of the ways we handle the result
    of an async computation. At the same time, we know that we need to provide some
    execution context that manages when and where the computation takes place as well
    as when the *callback* gets executed. This allows us to register more than one
    callback to a single async computation with a random execution order. The execution
    of callback in a random manner can be explained by this extended version of the
    previous diagram; now, we have callback as well in the diagram:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，回调是我们处理异步计算结果的一种方式。同时，我们知道我们需要提供一个执行上下文，该上下文管理计算何时何地发生以及 *回调* 何时执行。这允许我们为单个异步计算注册多个回调，并且执行顺序是随机的。回调的随机执行可以通过之前图表的扩展版本来解释；现在，图表中也有回调了：
- en: '![](img/00060.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: The callback execution only happens after the completion of the Future computation,
    as depicted in the diagram. The execution of *callbacks* takes place only if the
    computation gets successfully completed. In other cases, there should be a way
    to tell the program that things went wrong so that we can do something with it.
    Let's see what we can do about this big question.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回调执行仅在 Future 计算完成后发生，如图所示。只有当计算成功完成时，才会执行 *回调*。在其他情况下，应该有一种方法让程序知道出了问题，这样我们就可以对它进行处理。让我们看看我们能做些什么来解决这个问题。
- en: What if Future computations go wrong?
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果 Future 计算出错怎么办？
- en: 'The Future computation might succeed, resulting in a value, or fail, ending
    up throwing an exception. We need a mechanism to handle both the scenarios. Scala''s
    Future has this mechanism as a method, `onComplete`*.* First let''s see it in
    practice; to do that, let''s comment the callback code snippet we added last time,
    and add this snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Future 计算可能成功，产生一个值，或者失败，最终抛出异常。我们需要一个机制来处理这两种情况。Scala 的 Future 通过一个方法，`onComplete`*.*
    首先让我们看看实际应用；为了做到这一点，让我们注释掉上次添加的回调代码片段，并添加以下片段：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The function expects a function to execute; we have to also provide the execution
    context. The function literal is of type `Try[T] => U`. Fortunately, the execution
    context is taken from the implicit scope. Hence, we can directly provide the partial
    function to execute; in our case, we provided the same. Now, there''s a possibility
    that one async call depends on the result of another async call, and in this case
    we might have to perform nesting of callback functions. This might look something
    like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 函数期望一个要执行的功能；我们还需要提供执行上下文。函数字面量是类型 `Try[T] => U`。幸运的是，执行上下文来自隐式作用域。因此，我们可以直接提供部分函数来执行；在我们的情况下，我们提供了相同的。现在，有一种可能性是一个异步调用依赖于另一个异步调用的结果，在这种情况下，我们可能需要执行回调函数的嵌套。这可能会看起来像这样：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the previous code, we have only two futures nested along with *callbacks*,
    and this already seems like it should be done in simpler manner. Now think about
    more of such futures and callbacks together. It'll be a callback hell. Hence,
    what's needed here is composition. That's one of the powers of Scala Futures;
    you can compose two futures to perform some complex logic that contains the nesting
    of callbacks. How can we do that? By using the set of higher order functions given
    to us in Scala's Future API. Let's check it out.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们只有两个嵌套的 Future，以及 *回调*，这已经看起来应该以更简单的方式进行。现在考虑更多这样的 Future 和回调组合。这将是一场回调地狱。因此，这里需要的是组合。这正是
    Scala Futures 的一个强大功能；你可以组合两个 Future 来执行一些包含回调嵌套的复杂逻辑。我们该如何做到这一点？通过使用 Scala Future
    API 给我们的高阶函数集合。让我们来看看。
- en: Why not compose two or more Futures?
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不组合两个或更多的 Future？
- en: 'Now that we''ve got the previous toy example where we had two Futures called
    in sequence, let''s compose over those two. What we''ll do is first call the `flatMap`
    function on `firstFuture`*, *and that''ll give us the value. We''ll take that
    value and call `secondFuture`*.* Finally, we''ll call the `map` function to perform
    the print operation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了之前的玩具示例，其中我们按顺序调用了两个 Future，让我们在这两个 Future 上进行组合。我们将首先在 `firstFuture`*
    上调用 `flatMap` 函数，这将给我们一个值。我们将使用这个值来调用 `secondFuture`*。最后，我们将调用 `map` 函数来执行打印操作：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Run:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The whole callback logic is magically gone and we have used Future composition
    to achieve the same. All the magic happens in the line:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 整个回调逻辑神奇地消失了，我们使用了 Future 组合来实现同样的效果。所有神奇的事情都发生在这行代码中：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, let''s try to understand this with the help of a diagram:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们借助图表来尝试理解这一点：
- en: '![](img/00061.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.jpeg)'
- en: 'As shown, stars here represent a Future; we take the first future and call
    the `flatMap` function on it. The `flatMap` function''s signature looks like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，星号代表Future；我们首先获取第一个future，并在其上调用`flatMap`函数。`flatMap`函数的签名如下：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Comparing the signature to the diagram, we can see that the `flatMap` function
    takes the future value and calls subsequent calls to get another Future. The output
    of the `flatMap` function happens to be another Future value, hence we call a `map`
    function that sucks the value out of future, and then we can perform whatever
    operation we want to perform; in our case, we just printed the value. And from
    our previous knowledge, we know, comprehension works as a syntactic hack to our
    `flatMap` and map call. So, the following code also works well for our future''s
    composition:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将签名与图进行比较，我们可以看到`flatMap`函数接受future值并调用后续调用以获取另一个Future。`flatMap`函数的输出恰好是另一个Future值，因此我们调用一个`map`函数从future中提取值，然后我们可以执行我们想要的任何操作；在我们的例子中，我们只是打印了值。根据我们之前的知识，我们知道comprehension作为语法上的技巧，可以用于我们的`flatMap`和`map`调用。因此，以下代码也适用于我们的future组合：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A point to note is that in our `for` comprehension, the second statement only
    gets executed once the first value, `value1`, is available. That lets us use the
    first value in the second statement, as shown in the example. So, that's all for
    future's composition. This mechanism lets us chain several future/async calls
    together. This composition makes Scala's Future so powerful.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在我们的for comprehension中，第二个语句只有在第一个值`value1`可用时才会执行。这让我们可以在第二个语句中使用第一个值，如示例所示。所以，关于future组合就到这里。这种机制让我们可以将多个future/async调用串联起来。这种组合使得Scala的Future功能强大。
- en: So, we've just discussed the way we create a Future computation by creating
    a future object; it's worth knowing that Scala also provides a mechanism to assign
    a particular value to this future object. That mechanism exists in the form of
    Promises. Let's introduce ourselves to Scala's Promises.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们刚刚讨论了通过创建future对象来创建Future计算的方式；了解Scala也提供了一种将特定值赋给此future对象的方法是有价值的。这种机制以Promise的形式存在。让我们来了解一下Scala的Promise。
- en: Working with Promises
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与Promise一起工作
- en: 'As we talked about, Promises are used to assign a value to a future object.
    A Promise itself is an object that corresponds to a particular future object.
    We can access this `future` object by calling `future` method on the respective
    Promise. Let''s start by creating a Promise object first:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，Promise用于将值赋给future对象。Promise本身是一个与特定future对象相对应的对象。我们可以通过在相应的Promise上调用`future`方法来访问这个`future`对象。让我们首先创建一个Promise对象：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Run:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the previous code, we created a `Promise` instance by simply calling the `Promise.apply`
    method:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们通过简单地调用`Promise.apply`方法创建了一个`Promise`实例：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, the `apply` method takes no parameters, hence the `Promise` instance
    contains no value in itself; the assignment of values to this object can be done
    using one of the methods available in the Promise API. Methods like `success`,
    `failure`, and `complete` are used to assign values to a `Promise` instance. Each
    `Promise` instance''s corresponding Future can be obtained by calling the `future`
    method. In our example, we called the `success` method on the `Promise` object
    to assign a value to a linked future. We also used this for comprehension to retrieve
    the future''s value and print it. Running this program will yield the result we
    passed via this call to success:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`apply`方法没有参数，因此`Promise`实例本身不包含任何值；可以通过Promise API中可用的一种方法将值赋给此对象。`success`、`failure`和`complete`等方法用于将值赋给`Promise`实例。可以通过调用`future`方法获取每个`Promise`实例对应的Future。在我们的例子中，我们在`Promise`对象上调用`success`方法将值赋给关联的future。我们还使用了它来获取future的值并打印出来。运行此程序将产生通过此`success`调用传递的结果：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can also assign a failure object to linked futures via a call to the `failure`
    method. There are a few points to note:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过调用`failure`方法将一个失败对象赋值给关联的future。以下是一些需要注意的点：
- en: Calling the `Promise.apply` method creates an instance without values, just
    like we did with Futures
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`Promise.apply`方法创建了一个不带值的实例，就像我们为Futures所做的那样
- en: Promises do not start any asynchronous computations
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Promise不会启动任何异步计算
- en: Each Promise corresponds to only one `Future` object
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Promise只对应一个`Future`对象
- en: Each Promise object can be assigned a value only once
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Promise对象只能被赋予一个值
- en: Promises provide a way to assign values to `Future` objects
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Promise提供了一种将值赋给`Future`对象的方法
- en: These points clear up the concept of Promises, and also give us a hint about
    the implementation of the Future API in Scala.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些点阐明了Promise的概念，并给我们关于Scala中Future API实现的提示。
- en: Futures and Promises provide a simple abstraction over low-level constructs
    to achieve asynchrony in our programs. We've seen the ways we can use and compose
    these Futures to chain multiple async calls to get things done. There are other
    async libraries available in Scala to perform asynchronous programming. Some of
    the examples of these libraries are `scala-async` ([https://github.com/scala/scala-async](https://github.com/scala/scala-async)))
    and `monix` ([https://github.com/monix/monix)](https://github.com/monix/monix)).
    You may want to check out these libraries to understand and try out other asynchronous
    programming constructs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Futures和Promises为我们程序中的低级异步构造提供了一个简单的抽象。我们已经看到了我们可以如何使用和组合这些Futures来链式调用多个异步调用以完成任务。Scala中还有其他异步库可用于执行异步编程。这些库的一些示例是`scala-async`([https://github.com/scala/scala-async](https://github.com/scala/scala-async)))和`monix`([https://github.com/monix/monix](https://github.com/monix/monix))).您可能想查看这些库以了解和尝试其他异步编程结构。
- en: There might be use cases where a large collection of data needs to be manipulated
    to perform some logic. Let's take an example of our `football.csv` file. We've
    read the data and converted the lines from that to `List[String]`, and now every
    element can be parsed to a `Player` object giving us `List[Player]`*.* If we think
    a bit, the step where we need to parse `String` to `Player` does not need to be
    executed in sequence and can be done in parallel. Now, Scala comes up with the
    concept of *parallel c*ollections. Hence, if you need to do some functionality
    on data in some collections, functionality can be done in parallel. You have an
    option to convert the collection to its parallel counterpart by calling a simple
    method `par` on the usual collection. Let's look at parallel collections in Scala
    and try this out.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在一些用例，需要操纵大量数据以执行某些逻辑。让我们以我们的`football.csv`文件为例。我们已经读取了数据，并将这些行转换为`List[String]`，现在每个元素都可以解析为`Player`对象，从而得到`List[Player]`。如果我们稍微思考一下，将`String`解析为`Player`的步骤不需要按顺序执行，可以并行完成。现在，Scala提出了并行集合的概念。因此，如果您需要在某些集合上执行某些功能，功能可以并行执行。您可以通过在常规集合上调用简单的`par`方法将集合转换为它的并行对应物。让我们看看Scala中的并行集合，并尝试一下。
- en: Parallel collections
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行集合
- en: Well, before discussing parallel collections in Scala, it's important to have
    some insight about what parallel computation is. How's it different from concurrent
    and asynchronous?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在讨论Scala中的并行集合之前，了解什么是并行计算是很重要的。它与并发和异步有何不同？
- en: 'Well, we have spent some time understanding that asynchronous computation is
    non-blocking, hence we know that async computation happens from outside of the
    main program flow and gives you the value once the computation gets completed.
    To understand the difference between *concurrent* and *parallel* computation,
    let''s look at the following example:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们已经花了一些时间来理解异步计算是非阻塞的，因此我们知道异步计算发生在主程序流程之外，一旦计算完成就会给出值。为了理解并发和并行计算之间的区别，让我们看看以下示例：
- en: '![](img/00062.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00062.jpeg)'
- en: 'In the example, we are given a collection of numbers where we want to apply
    a function to each element of the collection to get a new collection. One method
    is to take a value out from the starting collection, add one to it, and put that
    value in a new collection, until the first collection is empty. Now, this process
    can be made faster by introducing *two* threads to perform the task of adding
    one to an element of the collection; let''s put it another way by saying we can
    create two *threads* to enable *concurrent access* to our collection:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们有一个数字集合，我们想要对集合中的每个元素应用一个函数来得到一个新的集合。一种方法是从起始集合中取出一个值，加一，然后将这个值放入一个新的集合中，直到第一个集合为空。现在，通过引入两个线程来执行对集合中元素加一的任务，这个过程可以变得更快；让我们换一种说法，说我们可以创建两个线程来使我们的集合能够进行并发访问：
- en: '![](img/00063.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00063.jpeg)'
- en: 'Another way can be to break down the collection into two sub collections and
    perform the task of adding in parallel. This parallelism is possible because the
    kind of operation we perform is not related to the sequence of elements in the
    starting collection, nor does it depend on any other element in the collection.
    Hence, the operation can be carried out in a separate manner in parallel. That''s
    what the difference between concurrent and parallel computations is. The semantics
    themselves explain whether parallelism is applicable or not:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将集合分解为两个子集合，并并行执行添加任务。这种并行性是可能的，因为我们执行的操作与起始集合中元素的顺序无关，也不依赖于集合中的任何其他元素。因此，操作可以以独立的方式并行执行。这就是并发计算和并行计算之间的区别。语义本身解释了并行性是否适用：
- en: '![](img/00064.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00064.jpeg)'
- en: 'This becomes the basis of *parallel collections* in Scala. Let''s try out our
    well-known example with the `football.csv` file. We''ll convert the `List[String]`
    to its parallel counterpart and then perform the parsing logic in parallel:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这成为了 Scala 中 *并行集合* 的基础。让我们用我们熟知的例子来尝试，即 `football.csv` 文件。我们将 `List[String]`
    转换为其并行对应版本，然后在并行中执行解析逻辑：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the example, we converted the `List[String]` to a `ParSeq` that is *a parallel*
    counterpart of our Scala collection `List`*.* After converting to a parallel collection,
    we called a `map` method on the parallel collection and performed the parsing
    operation. The parallel collection API is so consistent that it looks so normal
    to call the`map` method and perform some operation, but underlying task execution
    is taken care of by multiple processors at the same time; in other words, computations
    are happening in parallel. Running the previous code will print out the list of
    players as expected.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，我们将 `List[String]` 转换为 `ParSeq`，它是 Scala 集合 `List` 的并行对应版本*.* 在转换为并行集合后，我们在并行集合上调用
    `map` 方法并执行解析操作。并行集合 API 非常一致，以至于调用 `map` 方法执行一些操作看起来很正常，但底层任务执行是由多个处理器同时处理的；换句话说，计算是在并行进行的。运行前面的代码将按预期打印出球员列表。
- en: Scala's parallel collections reside in the `scala.collection.parallel` package.
    To create one parallel collection, we can either use the new keyword along with
    the collection name or we can convert a sequential collection to its parallel
    counterpart by calling the `par` function, which we did in our example.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 的并行集合位于 `scala.collection.parallel` 包中。要创建一个并行集合，我们可以使用与集合名称一起的新关键字，或者我们可以通过调用
    `par` 函数将顺序集合转换为它的并行对应版本，就像我们在示例中所做的那样。
- en: 'A few of the available parallel collections are:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的几个并行集合包括：
- en: '`ParArray`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParArray`'
- en: '`ParVector`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParVector`'
- en: '`mutable.ParHashMap`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`immutable.ParHashMap`'
- en: '`mutable.ParHashSet`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`immutable.ParHashSet`'
- en: '`immutable.ParHashMap`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`immutable.ParHashMap`'
- en: '`immutable.ParHashSet`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`immutable.ParHashSet`'
- en: '`ParRange`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParRange`'
- en: '`ParTrieMap`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParTrieMap`'
- en: We can instantiate these parallel collections the same way we do for sequential
    collections; that's the power of Scala's parallel collections. This makes a lot
    of collection-based computations faster to perform. With this, we can go and summarize
    our chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像对顺序集合那样实例化这些并行集合；这是 Scala 并行集合的力量。这使得许多基于集合的计算执行速度更快。有了这个，我们可以总结我们的章节。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the building blocks of concurrency in Scala.
    It was fascinating to understand the underlying blocks of concurrency in OS and
    JVM. We learned the difference between processes and threads. We discussed `ExecutionContext`
    and why we need one. Then, we talked about asynchronous programming using Future
    and Promises. Finally, we discussed parallel collections in Scala.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了 Scala 中并发的基础构建块。了解操作系统中 JVM 的并发底层块非常令人着迷。我们学习了进程和线程之间的区别。我们讨论了 `ExecutionContext`
    以及为什么我们需要它。然后，我们讨论了使用 Future 和 Promises 的异步编程。最后，我们讨论了 Scala 中的并行集合。
- en: In our next chapter, we'll be discussing another important and much talked about
    reactive programming abstraction available in Scala. We'll go through the reactive
    extensions available in Scala.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论 Scala 中另一个重要且广受讨论的响应式编程抽象。我们将了解 Scala 中的响应式扩展。
