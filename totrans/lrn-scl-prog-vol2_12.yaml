- en: Concurrent Programming in Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Yesterday is not ours to recover, but today is to try and tomorrow is to win
    or lose."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Anonymous'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea that modern computers with multicore architectures give better performance
    is based on the fact that multiple processors can run separate processes simultaneously.
    Each process can run more than one thread to complete specific tasks. Picturing
    this, we can write programs with multiple threads working simultaneously to ensure
    better performance and responsiveness. We call this concurrent programming. In
    this chapter, our goal is to understand Scala''s offerings in concurrent programming.
    There are multiple ways we can use constructs to write concurrent programs. We''ll
    learn about them in this chapter. Let''s check out what will be here for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building blocks of concurrency:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process and threads
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization and locks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Executor and ExecutionContext
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lock free programming
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous programming using Futures and Promises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel Collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we start learning about the ways we can write concurrent programs, it's
    important to understand the underlying picture. Let's start understanding concurrent
    programming and then we'll go through the basic building blocks of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's a programming approach where a set of computations can be performed simultaneously.
    These set of computations might share the same resources such as memory. How's
    it different from sequential programming? In sequential programming, every computation
    can be performed one after another. In the case of concurrent programs, more than
    one computation can be performed in the same time period.
  prefs: []
  type: TYPE_NORMAL
- en: By executing multiple computations, we can perform multiple logical operations
    in the program at the same time, resulting in better performance. Programs can
    run faster than before. This may sound cool; concurrency actually makes implementing
    real scenarios easier. Think about an internet browser; we can stream our favorite
    videos and download some content at the same time. The download thread does not
    affect the streaming of the video in any way. This is possible because content
    download and video streams on a browser tab are separate logical program parts,
    and hence can run simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in programs where we might have the user to perform some I/O operations
    for input, at the same time we want to run the program. We need these two parts
    to run simultaneously. Running these parts together makes it responsive to user
    interactions. Writing a concurrent program comes in handy in such cases. From
    a very cool web application running on an internet browser to games running on
    your mobile device, responsiveness and good user experience is possible because
    of concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: This is why learning about concurrency abstractions is important, and what's
    more important is to keep them simple in our program implementations. So, let's
    go through the basic building blocks of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks of concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scala is a JVM-based language, so programs written in Scala run in JVM. **JVM**,
    as we already know, is **Java Virtual Machine**, and runs as a single process
    in our operating system. In JVM, one of the basic concurrency constructs is a
    *thread*; we can create/use multiple threads as part of our Scala program. So,
    for a basic understanding of processes and threads, let's go through them.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding processes and threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think of a process as a program or application that our computer might have
    to run. This process is going to have some code that's executable, a **process
    identifier** (**pid**), and at least one thread of execution. The process might
    consume some computer resources as well, such as memory. Every process isolates
    itself from other processes when it comes to consume memory; this means two processes
    cannot use the same memory block.
  prefs: []
  type: TYPE_NORMAL
- en: Modern computers come with multiple processor cores. These cores are assigned
    tasks as executable program parts for execution in certain time slices. The task
    of assigning these executable parts is done by the operating system. Most of the
    operating systems nowadays use a mechanism called **pre-emptive multitasking**,
    which is the simultaneous execution of multiple executable parts from all running
    processes. These executable parts are nothing but threads. It means that each
    process needs to have at least one thread, we can call it the main thread, in
    order to run properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s clear that the process within an operating system uses some memory resources,
    and it can contain multiple threads. Now, these threads from a particular process
    are free to share the memory block assigned, but two processes cannot do the same.
    It''ll be easier to understand this with the help of the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram is a simplified version for a system with two processor
    cores, a *pre-emptive multitasking operating system* and *memory*. We have a separate
    memory resource allocated for different processes running, in our case, **Process
    1**, **Process 2**, and **Process 3**. The memory block for **Process 1** has
    no access to the memory block for **Process 2** or **Process 3**. Each process
    contains more than one thread. Each thread can access the memory allocated from
    the parent process. These threads can share the memory allocated. Now, what happens
    is the operating system assigns these executable blocks, in other words *threads*,
    to process cores for execution, as shown in our preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a particular time slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core 1** is executing **Thread 1** from **Process 1**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Core 2** is executing **Thread 2** from **Process 3**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two executions are happening simultaneously. We already know that JVM
    runs as a process; the programs we write are going to have threads as entities.
    For our program to run, we need at least a *main thread* that can be the entry
    point to our application. We can create more threads as instances of the `java.lang.Thread`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know we can have multiple parts of our application running together,
    it's important to understand that we need some way to synchronize them. By synchronizing,
    we can ensure one particular execution is not going to affect any other. Threads
    within a process have access to the same memory block, hence it might be possible
    that two threads try to access the memory at the same time—this might cause problems.
    Threads are low-level concurrency abstractions in Scala, and as the number of
    concurrent parts or threads increases, complexity also increases with them. To
    understand how do we restrict other threads to access some block of code simultaneously,
    first we need to understand how synchronization works.
  prefs: []
  type: TYPE_NORMAL
- en: Locks and synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We talked about threads in previous sections—we''ll first try to create a few
    ourselves before discussing them further. Let''s write some code for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A few pointers for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We simply created an object extending `App` to create the application entry
    point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We created a class named `FirstThread` that extends `Thread`, which is nothing
    but the same `java.lang.Thread` we talked about in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we create a thread, we might want to specify what it needs to run. That
    can be defined via overriding the `run` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Until *point 3*, we have defined our thread class; now, to run the thread, we'll
    create its instance, and then call the `start` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `start` method triggers the execution of the thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we printed the thread's name. First, the `main` current thread, and
    then the `firstThread` class name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Running the application will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So from the first run, it''s clear that a thread called `main` runs the application,
    and as we create more and more threads, these threads also come into the picture.
    It''s great to have multiple threads working together for us to perform some computation.
    We know from our previous discussions that an OS performs the scheduling of task
    execution, so it''s out of our control which thread will get executed in which
    order. Now, think of a scenario where you might want to perform a read and write
    operation to a variable in your program. With multiple threads performing such
    a task, it might be possible to see inconsistencies in the result. It means that
    this execution is exposed to *race conditions*; in other words, it depends on
    the execution schedule of statements by the OS. To better understand this, let''s
    try out the scenario we discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this small application, we first create a variable `counter`; we are going
    to read and write to this variable using two threads. Next, we have two methods,
    first `readWriteCounter` and `printCounter`. The `readWriteCounter` method is
    doing as the name says. This method increments the counter (reading operation)
    and assigns the `incrementedCounter` to the `counter` variable. The second method, `printCounter`,
    takes an integer parameter to increment counter the number of times specified
    and prints that.
  prefs: []
  type: TYPE_NORMAL
- en: After defining all these, we created a thread with the name `First` and called
    our `printCounter` method, overriding the `run` method. To observe the behavior,
    we're supposed to call the `printCounter` from this `First` thread and the main
    application thread. Since two threads are working simultaneously, it's expected
    that output of these two shouldn't contain the same number. We also called `printCounter`
    from the application as the final statement of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Running the program couple of times (if you're lucky, for the first time), you
    might be able to see some inconsistent behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the output from both the threads, we can see the number `1` appeared twice,
    while we know it shouldn''t have happened. We see that behaviour due to read and
    write operations happening to our `counter` variable via multiple threads in the
    following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By the time the `counter = incrementCounter` statement gets a chance to execute,
    the `counter` variable gets incremented twice (by multiple threads). This is causing
    inconsistency. The problem lies in the execution of these two statements; these
    have to be atomic in nature to give a consistent output where the same number
    cannot appear for different threads. By *atomic*, we mean these two statements
    have to be executed together by the same thread.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the application, it can be seen that our concerned method block is not guarded
    by this *synchronized* statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'By using this, we make the synchronized statement refer to the current object
    to guard the block. We could also make a particular instance of some type, let''s
    say *Any* and that instance can work as a guard to our synchronised clock. It''s
    shown as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Creating a thread is an expensive operation—if you have more computations, you
    want to perform concurrently and you create several threads to compute those.
    It'll be less performant and with some shared data access, your life will be worse.
    So, to prevent this costly operation from happening, JDK has come up with the
    concept of *thread-pools.* In *thread-pools*, there are multiple thread instances
    provided. These threads within a pool remain in *waiting* state; when you want
    to perform some computation, we can run these. The job of running these is done
    by the `executor`*.* Let's try to understand it.
  prefs: []
  type: TYPE_NORMAL
- en: Executor and ExecutionContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Executor is an interface that encapsulates the *thread-pool* and deals with
    executing computations via one of the threads or the caller thread itself. One
    example of an executor is `java.util.concurrent.ForkJoinPool`*.* Scala's implementation
    of such an executor is `ExecutionContext` which internally uses the same `ForkJoinPool`*.*
    Before going further to see an example, why not think of the need for this `Executor`
    mechanism?
  prefs: []
  type: TYPE_NORMAL
- en: 'As programmers, while writing performance-efficient concurrent applications,
    we might have to deal with two major tasks, the first being defining instances
    of concurrency abstraction, let''s say *threads*, and making sure they handle
    our data/state in the right manner. Second, to use these *threads* in our program.
    Now, the creation of all these threads, if created by us, are:'
  prefs: []
  type: TYPE_NORMAL
- en: Costly operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex to manage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, mechanisms like `Executor` take away the work of creating these threads.
    We don''t explicitly decide which thread will execute the logic we provide; we
    also don''t need to manage those. Executor implementations, when used, create
    daemon and worker threads. When we assign computation via the `execute` method,
    a particular worker thread is assigned the task. Shutting down a daemon thread
    shuts down all the worker threads. This''ll be easier to understand with the help
    of the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the application, we used two `Executor` implementations; the first is from
    `java.util.concurrent.ForkJoinPool` and the second is similar to Scala-specific
    `ExecutionContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For both implementations, we have an execute method, which expects a `Runnable`
    instance. To create `Runnable` instances, we have to define a run method. It's
    another way to create a thread instance. In the definition of the run method,
    we just printed the executor thread's name.
  prefs: []
  type: TYPE_NORMAL
- en: 'But running the above program gives you no output. The reason for such behavior
    is that both the implementations create a *daemon thread,* which shuts down after
    the first run. Shutdown of a daemon thread kills all worker threads. Calling the
    `execute` method wakes up `workerthreads`*.* These `workerthreads` execute the
    run method asynchronously. Hence, we''ll try to include some timeout to wait for
    a small duration as the last statement by calling the `Thread.sleep` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After including some waiting time for the execution by the worker threads,
    we get the output. As shown, the output tells us the *thread* names: both are
    *worker threads.* The first one, named `scala-execution-context-global-11`, is
    from Scala''s `ExecutionContext` and the second, named `ForkJoinPool-1-worker-1`,
    is from Java''s `ForkJoinPool`*.*'
  prefs: []
  type: TYPE_NORMAL
- en: These *thread-pools* and their implementations become the basis for higher-level
    concurrency abstractions. We also encountered a bit of asynchrony in the example
    when we waited for the result of the execution. It's not wrong to say asynchrony
    is subsumed in *concurrency*, as asynchronous programs tend to execute outside
    of the main flow of the program. Hence, multiple async computations can be performed
    at the same time; once we get back the results for these computations, we can
    perform the desired operation then.
  prefs: []
  type: TYPE_NORMAL
- en: Scala provides constructs from the standard library for asynchronous programming,
    as well as multiple libraries that provide async constructs to make it easier
    to develop programs for us, the developers. Let's go through those constructs.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we try to define *asynchronous programming,* we come up with something that
    states that it's a programming approach in which computations, which can be *tasks
    or threads,* execute outside of the basic program flow. In programming terminologies,
    these computations execute on different call stacks, not the current one. Because
    of this, it's possible for us to think of more than one async computation happening
    at the same time; we can wait for each to happen so that aggregation of a result
    or some other result manipulation is possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up until now, we''ve looked at three of these terminologies such as *concurrency*,
    *multithreading*, and *asynchronous*. We tend to confuse these but given our discussions,
    it''s clear that *asynchronous* subsumes *concurrency* and not *multithreading.*
    We know that asynchrony can be achieved using scheduling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Well, the fact that we have to compose the results of multiple async problems
    running at the same time means we might end up needing some sort of synchronization.
    Fortunately, we don't have to deal with managing these cumbersome tasks, as Scala's
    offerings manage those using the `ExecutionContext`*.* One of those asynchronous
    offerings is *Futures* in Scala. Let's talk about `Futures` in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea here is simple; we have our simple program flow. If we make some complex
    time-consuming computation in the main program flow, it''ll be blocking and the
    experience won''t be good. Hence, we want to perform that time-consuming computation
    outside of the basic program flow, and continue doing other stuff in the main
    program flow, keeping the value (that will be available at a later point in time)
    of computation. Once the value is available, we use it via a mechanism. The way
    we can picture this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For now, the two entities we can think of are: `Futurecomputation` and a future
    `value`*.* These two are different; a Future computation is the time-consuming
    part you want to compute asynchronously and future value is the `value` reference
    on which we depend on our program flow. Once the Future computation starts in
    a separate flow, the program does not stop its execution and when the value becomes
    available, the part gets executed. The `ExecutionContext` takes care of the execution
    of that part, where we may use the `value` Future*.*
  prefs: []
  type: TYPE_NORMAL
- en: It's clear that whenever we start some Future computation, we might have to
    provide an execution context with it. In Scala, the Future type resides in the `scala.concurrent`
    package, the same package that has `ExecutionContext` and its executor `ExecutionContextExecutor`*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The Future value is represented by `Future[T]`, where `T` is the type of the
    value that''ll be available at some later point in time. Hence in our programs,
    whenever we need some value that''s a result of some asynchronous computation,
    we represent that value with this mentioned type. An example will clear this up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this simple application, we specified a  file with some content. The file
    has information about few football players. Now, to read the file contents as
    well as parsing and encoding them to `Player` instances may take some time, hence
    we decided to take the `load`*,* `parse`, and `encode` step as a Future computation,
    and the resulting value is going to be a Future value of type `Future[List[Player]]`*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, after defining such a computation, we checked if computation completed.
    Then we waited for some time and again tried to check if it completed. Running
    the application gives us `false` and then `true` respectively. If we think of
    this example via a diagram the flow might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It's easy to understand the execution flow with the help of this diagram; for
    the duration in which the computation is in progress, the `isCompleted` flag remains
    false. After completion, it's set to true. After that, we can use the value of
    future, but in this example, we didn't use the value; also, the question arises
    of how we can use it. Do we have to check again and again for the value to be
    available? This sounds bad, so another way is to register a *callback* for this
    async computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, what''s a callback? To answer this, let''s first extend our program to
    register one for our Future computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s worth noting that callbacks are one of the ways we handle the result
    of an async computation. At the same time, we know that we need to provide some
    execution context that manages when and where the computation takes place as well
    as when the *callback* gets executed. This allows us to register more than one
    callback to a single async computation with a random execution order. The execution
    of callback in a random manner can be explained by this extended version of the
    previous diagram; now, we have callback as well in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The callback execution only happens after the completion of the Future computation,
    as depicted in the diagram. The execution of *callbacks* takes place only if the
    computation gets successfully completed. In other cases, there should be a way
    to tell the program that things went wrong so that we can do something with it.
    Let's see what we can do about this big question.
  prefs: []
  type: TYPE_NORMAL
- en: What if Future computations go wrong?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Future computation might succeed, resulting in a value, or fail, ending
    up throwing an exception. We need a mechanism to handle both the scenarios. Scala''s
    Future has this mechanism as a method, `onComplete`*.* First let''s see it in
    practice; to do that, let''s comment the callback code snippet we added last time,
    and add this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The function expects a function to execute; we have to also provide the execution
    context. The function literal is of type `Try[T] => U`. Fortunately, the execution
    context is taken from the implicit scope. Hence, we can directly provide the partial
    function to execute; in our case, we provided the same. Now, there''s a possibility
    that one async call depends on the result of another async call, and in this case
    we might have to perform nesting of callback functions. This might look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we have only two futures nested along with *callbacks*,
    and this already seems like it should be done in simpler manner. Now think about
    more of such futures and callbacks together. It'll be a callback hell. Hence,
    what's needed here is composition. That's one of the powers of Scala Futures;
    you can compose two futures to perform some complex logic that contains the nesting
    of callbacks. How can we do that? By using the set of higher order functions given
    to us in Scala's Future API. Let's check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Why not compose two or more Futures?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve got the previous toy example where we had two Futures called
    in sequence, let''s compose over those two. What we''ll do is first call the `flatMap`
    function on `firstFuture`*, *and that''ll give us the value. We''ll take that
    value and call `secondFuture`*.* Finally, we''ll call the `map` function to perform
    the print operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The whole callback logic is magically gone and we have used Future composition
    to achieve the same. All the magic happens in the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s try to understand this with the help of a diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown, stars here represent a Future; we take the first future and call
    the `flatMap` function on it. The `flatMap` function''s signature looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Comparing the signature to the diagram, we can see that the `flatMap` function
    takes the future value and calls subsequent calls to get another Future. The output
    of the `flatMap` function happens to be another Future value, hence we call a `map`
    function that sucks the value out of future, and then we can perform whatever
    operation we want to perform; in our case, we just printed the value. And from
    our previous knowledge, we know, comprehension works as a syntactic hack to our
    `flatMap` and map call. So, the following code also works well for our future''s
    composition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A point to note is that in our `for` comprehension, the second statement only
    gets executed once the first value, `value1`, is available. That lets us use the
    first value in the second statement, as shown in the example. So, that's all for
    future's composition. This mechanism lets us chain several future/async calls
    together. This composition makes Scala's Future so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: So, we've just discussed the way we create a Future computation by creating
    a future object; it's worth knowing that Scala also provides a mechanism to assign
    a particular value to this future object. That mechanism exists in the form of
    Promises. Let's introduce ourselves to Scala's Promises.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Promises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we talked about, Promises are used to assign a value to a future object.
    A Promise itself is an object that corresponds to a particular future object.
    We can access this `future` object by calling `future` method on the respective
    Promise. Let''s start by creating a Promise object first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we created a `Promise` instance by simply calling the `Promise.apply`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `apply` method takes no parameters, hence the `Promise` instance
    contains no value in itself; the assignment of values to this object can be done
    using one of the methods available in the Promise API. Methods like `success`,
    `failure`, and `complete` are used to assign values to a `Promise` instance. Each
    `Promise` instance''s corresponding Future can be obtained by calling the `future`
    method. In our example, we called the `success` method on the `Promise` object
    to assign a value to a linked future. We also used this for comprehension to retrieve
    the future''s value and print it. Running this program will yield the result we
    passed via this call to success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also assign a failure object to linked futures via a call to the `failure`
    method. There are a few points to note:'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the `Promise.apply` method creates an instance without values, just
    like we did with Futures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promises do not start any asynchronous computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each Promise corresponds to only one `Future` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each Promise object can be assigned a value only once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promises provide a way to assign values to `Future` objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These points clear up the concept of Promises, and also give us a hint about
    the implementation of the Future API in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Futures and Promises provide a simple abstraction over low-level constructs
    to achieve asynchrony in our programs. We've seen the ways we can use and compose
    these Futures to chain multiple async calls to get things done. There are other
    async libraries available in Scala to perform asynchronous programming. Some of
    the examples of these libraries are `scala-async` ([https://github.com/scala/scala-async](https://github.com/scala/scala-async)))
    and `monix` ([https://github.com/monix/monix)](https://github.com/monix/monix)).
    You may want to check out these libraries to understand and try out other asynchronous
    programming constructs.
  prefs: []
  type: TYPE_NORMAL
- en: There might be use cases where a large collection of data needs to be manipulated
    to perform some logic. Let's take an example of our `football.csv` file. We've
    read the data and converted the lines from that to `List[String]`, and now every
    element can be parsed to a `Player` object giving us `List[Player]`*.* If we think
    a bit, the step where we need to parse `String` to `Player` does not need to be
    executed in sequence and can be done in parallel. Now, Scala comes up with the
    concept of *parallel c*ollections. Hence, if you need to do some functionality
    on data in some collections, functionality can be done in parallel. You have an
    option to convert the collection to its parallel counterpart by calling a simple
    method `par` on the usual collection. Let's look at parallel collections in Scala
    and try this out.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, before discussing parallel collections in Scala, it's important to have
    some insight about what parallel computation is. How's it different from concurrent
    and asynchronous?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we have spent some time understanding that asynchronous computation is
    non-blocking, hence we know that async computation happens from outside of the
    main program flow and gives you the value once the computation gets completed.
    To understand the difference between *concurrent* and *parallel* computation,
    let''s look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the example, we are given a collection of numbers where we want to apply
    a function to each element of the collection to get a new collection. One method
    is to take a value out from the starting collection, add one to it, and put that
    value in a new collection, until the first collection is empty. Now, this process
    can be made faster by introducing *two* threads to perform the task of adding
    one to an element of the collection; let''s put it another way by saying we can
    create two *threads* to enable *concurrent access* to our collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way can be to break down the collection into two sub collections and
    perform the task of adding in parallel. This parallelism is possible because the
    kind of operation we perform is not related to the sequence of elements in the
    starting collection, nor does it depend on any other element in the collection.
    Hence, the operation can be carried out in a separate manner in parallel. That''s
    what the difference between concurrent and parallel computations is. The semantics
    themselves explain whether parallelism is applicable or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This becomes the basis of *parallel collections* in Scala. Let''s try out our
    well-known example with the `football.csv` file. We''ll convert the `List[String]`
    to its parallel counterpart and then perform the parsing logic in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In the example, we converted the `List[String]` to a `ParSeq` that is *a parallel*
    counterpart of our Scala collection `List`*.* After converting to a parallel collection,
    we called a `map` method on the parallel collection and performed the parsing
    operation. The parallel collection API is so consistent that it looks so normal
    to call the`map` method and perform some operation, but underlying task execution
    is taken care of by multiple processors at the same time; in other words, computations
    are happening in parallel. Running the previous code will print out the list of
    players as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Scala's parallel collections reside in the `scala.collection.parallel` package.
    To create one parallel collection, we can either use the new keyword along with
    the collection name or we can convert a sequential collection to its parallel
    counterpart by calling the `par` function, which we did in our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few of the available parallel collections are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ParArray`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ParVector`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mutable.ParHashMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mutable.ParHashSet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`immutable.ParHashMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`immutable.ParHashSet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ParRange`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ParTrieMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can instantiate these parallel collections the same way we do for sequential
    collections; that's the power of Scala's parallel collections. This makes a lot
    of collection-based computations faster to perform. With this, we can go and summarize
    our chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the building blocks of concurrency in Scala.
    It was fascinating to understand the underlying blocks of concurrency in OS and
    JVM. We learned the difference between processes and threads. We discussed `ExecutionContext`
    and why we need one. Then, we talked about asynchronous programming using Future
    and Promises. Finally, we discussed parallel collections in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we'll be discussing another important and much talked about
    reactive programming abstraction available in Scala. We'll go through the reactive
    extensions available in Scala.
  prefs: []
  type: TYPE_NORMAL
