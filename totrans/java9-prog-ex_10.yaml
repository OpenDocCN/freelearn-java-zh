- en: Finalizing Java Knowledge to a Professional Level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you have learned the most important areas and topics needed for a professional
    Java developer. What we still have ahead of us in this book is to discuss some
    topics that will lead you from being a junior developer to a senior developer.
    Reading this chapter will not make anyone a senior developer, though. The previous
    chapters were the roads that we walked through. This chapter is only the map.
    If each of the previous chapters covered a short walk of a few miles in the journey
    of coding to reach the harbor, then this chapter is the nautical map to discover
    a new continent.
  prefs: []
  type: TYPE_NORMAL
- en: We will briefly bite into some very deep and high-level professional areas,
    such as creating a Java agent, compile-time annotation processing, polyglot programming,
    a bit of architecture design and tools, and techniques to work in teams. We'll
    do it just for the taste. Now, you have enough knowledge to understand the importance
    of these topics, and getting a taste will create an appetite for the coming years
    of self-development, or, at least, that is my intention to make you, the reader,
    addicted.
  prefs: []
  type: TYPE_NORMAL
- en: Java deep technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will list three technologies:'
  prefs: []
  type: TYPE_NORMAL
- en: Java agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polyglot programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotation processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing them is not a must for a Java professional. Knowing about them is. Java
    agents are used mainly in development environments and in operation. They are
    complex runtime technologies that interact with the already running *JVM*. Annotation
    processing is the other end. Annotation processors are plugged into the Java compiler.
    Polyglot programming is in the middle. It is JVM programming, just like programming
    in Java, but by using some different language or, perhaps, some different language
    and Java together. Or even many languages, such as Jython, Groovy, Clojure, and
    Java together.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss these technologies so that we will get some idea about what
    they are and where to look for further information in case we want to learn more
    about them.
  prefs: []
  type: TYPE_NORMAL
- en: Java agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Java agent is a Java program that is loaded by the Java runtime in a special
    way and can be used to interfere with the byte code of the loaded classes, altering
    them. They can be used to:'
  prefs: []
  type: TYPE_NORMAL
- en: List or log, and report the loaded classes during runtime, as they are loaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the classes so that the methods will contain extra code to report runtime
    behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support debuggers to alter the content of a class as the developer modifies
    the source code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This technology is used in, for example, the products **JRebel** and **XRebel**
    from [https://zeroturnaround.com/](https://zeroturnaround.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Although Java agents work in the deep details of Java, they are not magic. They
    are a bit complex and you need a deep understanding of Java, but anyone who can
    program in Java can write a Java agent. All that is required is that the class,
    which is the agent, has some predefined methods packaged into a *JAR* file along
    with the other classes of the agent and has a `META-INF/MANIFEST.MF` file that
    defines the names of the classes implementing the `premain` and/or `agentmain`
    methods, and some other fields.
  prefs: []
  type: TYPE_NORMAL
- en: The detailed and precise reference documentation is part of the *JDK JavaDoc*
    available at [http://download.java.net/java/jdk9/docs/api/](http://download.java.net/java/jdk9/docs/api/)
    in the documentation of the `java.lang.instrument` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a Java application is started with a Java agent, the command line has
    to contain the following option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `jarpath` points to the JAR file that contains the agent class and the
    manifest file. The class must have a method named `premain` or `agentmain`. It
    may have one or two arguments. The JVM tries to call the two-argument version
    first after the JVM is initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If a two-argument version does not exist, then the one-argument version is
    used, which is essentially the same as the two-argument version but misses the
    instrumentation argument, which, in my opinion, does not make too much sense since
    a Java agent cannot do much without the `Instrumentation` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `agentArgs` parameter is the string passed as an option on the command line.
    The second argument, `Instrumentation`, provides methods to register class transformers
    that can modify class byte codes and also methods that can ask the JVM to perform
    redefinition or retransformation of classes during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Java applications can also load an agent after the program has already started.
    In such a case, the agent cannot be invoked before the main method of the Java
    application, since it has already started by that time. To separate the two cases,
    JVM calls `agentmain` in such a scenario. Note that either `premain` or `agentmain`
    is invoked for an agent and never both. A single agent can implement both so that
    it is capable of performing its task loaded at the startup, specified on the command
    line or after the JVM started.
  prefs: []
  type: TYPE_NORMAL
- en: If `agentmain` is used, it has the same arguments as `premain`.
  prefs: []
  type: TYPE_NORMAL
- en: There is one major and important difference between the invocation of `premain`
    and `agentmain`. If an agent cannot be loaded during startup, for example, if
    it cannot be found, if the JAR file does not exist, if the class does not have
    the `premain` method, or if it throws an exception, the JVM will abort. If the
    agent is loaded after the *JVM* is started (in this case, `agentmain` is to be
    used), the JVM will not abort if there is some error in the agent.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is fairly reasonable. Imagine that there is a server application
    that runs on the Tomcat servlet container. When a new version is started, the
    system is down for a maintenance period. If the new version cannot be started
    because the agent is not behaving well, then it is better not started. The damage
    to debug the situation and fix it, or roll back the application to the old version
    and call for a longer fixing session may be less than starting up the application
    and not having the proper agent functionality. If the application starts up only
    without the agent, then the suboptimal operation may not immediately be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, when an agent is attached later, the application is already
    running. An agent is attached to an already running application to get information
    from an already running instance. To stop the already running instance and fail
    it, especially in an operational environment, is more damaging than just not attaching
    the agent. It may not go unnoticed anyway because the agent that is most probably
    attached is used by operational personnel.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `premain` or `agentmain` agent gets an `Instrumentation` object as the second
    argument. This object implements several methods. One of them is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The agent implements the transformer, and it has the `transform` method signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This method is called by the JVM when a class is loaded or when it is to be
    transformed. The method gets the class object itself, but, more importantly, it
    gets the byte array containing the byte code of the class. The method is expected
    to return the byte code of the transformed class. Modifying the byte code needs
    some knowledge of how the byte code is built and what the structure of a class
    file is. There are libraries that help to do that, such as Javassist ([http://www.javassist.org/](http://www.javassist.org/)
    ) or ASM ([http://asm.ow2.org/](http://asm.ow2.org/)). Nevertheless, I will not
    start coding before getting acquainted with the structure of the byte code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Agents, running in a separate thread and presumably interacting with the user
    or the filesystem and based upon some external observation at any time, may call
    the following method to perform the retransformation of the classes using the
    registered transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The agent can also call the following method, which will redefine the classes
    given as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `ClassDefinition` class is simply a `Class` and a `byte[]` pair. This will
    redefine the classes through the class maintaining mechanism of the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: Note that these methods and Java agents interact with the deep, low-level part
    of the JVM. This also bears the consequence that it is very easy to destroy the
    whole JVM. The byte code is not checked, unlike during the loading of the class,
    and thus, if there is some error in it, the consequence may not only be an exception
    but also the crashing of the JVM. Also, the redefinition and the transformations
    should not alter the structure of the classes. They should not change their inheritance
    footprint, add, rename, or remove methods, or change the signature of the methods,
    and this is also true for fields.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that the already created objects will not be affected by the changes;
    they will still use the old definition of the class and only new instances will
    be affected.
  prefs: []
  type: TYPE_NORMAL
- en: Polyglot programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polyglot programming is the technique when there are different programming languages
    used in the same application. Such an approach is not only appropriate when a
    different part of the application runs on a different environment. For example,
    the client executes in the browser using JavaScript, CSS, and HTML while the server
    is programmed to run in a Tomcat environment in Java. This is a different story,
    and, usually, this is not the typical use when someone is speaking about polyglot
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: When the application that runs on the server partially runs in Java and also
    in some other language, then we can speak about polyglot programming. For example,
    we create the order handling application in Java and some of the code that checks
    the correctness of the order based on the product-specific codes that the order
    contains is written in JavaScript. Does it ring a bell? We have already done that
    in this book to demonstrate the scripting API of the JDK. That was real polyglot
    programing even if we did not mention it that way.
  prefs: []
  type: TYPE_NORMAL
- en: The JVM that runs the compiled Java code is a very good target for different
    language compilers, and thus, there are many languages that compile for it. When
    the JVM runs the byte code of a class, it does not know what the source language
    was, and it does not really care; some compiler created the byte code and it just
    executes that.
  prefs: []
  type: TYPE_NORMAL
- en: We can use different languages, such as Jython, Groovy, and Scala, to name a
    few popular ones that compile for the JVM. We can write one class using one language
    and the other one using another. When they are put together into a JAR, WAR, or
    an EAR file, the runtime system will just run them.
  prefs: []
  type: TYPE_NORMAL
- en: When do we use polyglot programming?
  prefs: []
  type: TYPE_NORMAL
- en: Polyglot configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, we turn towards polyglot programming when we want to create an application
    that is more flexible and more configurable. Applications that get installed in
    many instances, usually, at different customer sites have some configurations.
    These configurations can be XML files, properties files, and INI files (those
    come from Windows). As the programs develop sooner or later, these static configuration
    possibilities reach their limits. Application developers soon see that they need
    to configure some functionality that is cumbersome to describe using these technologies.
    Configuration files start being larger and, also, the code that reads and interprets
    the configuration files grow large. Good developers have to realize that this
    is the situation, and before the configuration files and the code handling them
    become unmanageable, some scripting configuration, polyglot programming has to
    be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.gif)'
  prefs: []
  type: TYPE_IMG
- en: Decent developer teams may reach a point when they develop their configuration
    language and the interpreter of that language. It can be based on XML, or it can
    just be any other language. After all, writing a language is fun; I have done
    it a few times myself. Most of these were, however, hobbies and not professional
    projects. Usually, there is no customer value in crafting another language. We
    can better use an existing one.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of configuration, Groovy is a very handy language that supports
    complex closure and meta-class syntax and implementation. This way, the language
    is extremely suitable to create a domain-specific language. Since Groovy is compiled
    to JVM, Groovy classes can be invoked directly from Java, and in the other way
    round, reading the configuration is essentially invoking the class compiled from
    the configuration file. The compilation can be during application build time,
    but in the case of configuration, it makes more sense to do it during application
    startup. We have already seen that the Groovy implementation of the scripting
    API or the special API that Groovy provides is absolutely capable of doing that.
  prefs: []
  type: TYPE_NORMAL
- en: Have we seen examples of this in our book? It may be a surprise to you, but
    we have in fact used Groovy to describe some configuration many times. *Gradle*
    build files are nothing more than Groovy DSL developed mainly in Groovy to support
    project build configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Polyglot scripting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Configuration is not the only application of polyglot programming. Configuration
    is executed at the program startup and the configuration data is used as static
    data afterwards. We can execute scripts during the application's execution any
    time and not only during its startup. This can be used to provide extra functionality
    to the program's user with installations that use the same application but are
    furnished with different scripts.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first applications that provided such scripting capability was the
    emacs editor. The core of the application was written in C language and it contained
    a Lisp interpreter that let the user to write scripts, which were executed in
    the editor environment. The engineering program, AutoCAD, also used a Lisp interpreter
    for similar purposes. Why was Lisp used for this purpose?
  prefs: []
  type: TYPE_NORMAL
- en: Lisp has very simple syntax, and therefore, it is easy to parse Lisp code. At
    the same time, the language is powerful, and last but not least, there were open
    source Lisp interpreters (at least one) available by the time.
  prefs: []
  type: TYPE_NORMAL
- en: To get this kind of flexibility, applications, many times, provide plugin APIs,
    which a developer can use to extend the application. This, however, requires that
    the developer sets up coding tools, including IDE, build tool, continuous integration,
    and so on, that is, a professional programming environment. When the task to be
    solved by the plugin is simple, the overhead is simply too large. In such a case,
    a scripting solution is handier.
  prefs: []
  type: TYPE_NORMAL
- en: Scripting is not a solution for everything. When the scripts extending the application
    tend to become too complex, it means that the scripting possibility is just too
    much. It is difficult, however, to take back a toy from a child. If users get
    used to the scripting possibility, then they will not take it easy if the next
    version of the application we release does not provide that possibility. Thus,
    it is extremely important to assess the possible use of the scripting capability
    in our application. Scripting and, more generally, any feature of our program
    will not be used for what we intended them for. They will be used for whatever
    it is possible to use them for. Users can go beyond all imagination when it comes
    to abusing some feature. It may be a good idea to think about limiting the scripting
    possibility beforehand, limiting the running time of the scripts or the size of
    the script our program agrees to work with. If these limitations are set reasonably,
    and the users understand and accept these, a plugin structure in addition to the
    scripting capability has to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: The security of an application, including plugin or scripting extension, is
    also very important. The scripts or plugins run on the same JVM as the core application.
    Some scripting languages provide some fence around the scripts that limits the
    access to the core application's objects and classes, but this is an exception.
    Usually, scripts run with the same privilege as the core application and that
    way they can do just anything. Thus, scripts should be trusted the same way as
    the core application. Script installation or modification should never be possible
    for an unprivileged user of the application. Such an action is almost always reserved
    for the system administrator.
  prefs: []
  type: TYPE_NORMAL
- en: If an unprivileged user can upload a script to the server and then have it executed,
    we just opened a security hole in our application. Since access restrictions are
    enforced by the application, it is easy to override these limitations using an
    uncontrolled script. The hacker can just access other users' data easily, which
    he is not entitled to, and read and modify our database.
  prefs: []
  type: TYPE_NORMAL
- en: Business DSL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polyglot programming may also come into the picture when the application's code
    can be separated into business code and technology code. The business code contains
    the top-level business logic that we actually write the application for, and this
    is the code that contains the logic that the customer pays for. The technology
    code is to support the algorithms coded in the business DSL.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the enterprise applications contain these two types of code but many
    do not separate them. This leads to a monolithic application that contains repetitive
    code. When you feel that you are writing the same type of code when you need persistence
    or networking, and again the same type of code while coding some business rules,
    then this is the code smell that suggests that the two code types are not separated.
    DSL and scripting are not a magic wand and do not solve all the problems that
    stem from a wrong application structure. In such a situation, the code has to
    be refactored first to separate the business logic and the infrastructure code,
    and it is only the second step to implement a DSL and a business API supporting
    it and to rewrite the business code into the DSL. Every step of such a project
    delivers value for the application and even if it never gets to DSL and scripting,
    the effort invested is not wasted.
  prefs: []
  type: TYPE_NORMAL
- en: The business DSL scripting is very similar to pluggable scripts, except that
    this time it is not the application that calls the scripts from time to time to
    execute some special extension functionality. Instead, the DSL code calls the
    application through the business API that it provides. The advantage of providing
    the API and using a DSL is that the code that implements the business logic gets
    rid of the technical details, can be very abstract, and, this way, be much closer
    to a business-level description of the problem rather than just program code.
    Even some businessperson can understand a business DSL, and though it is not a
    goal in real-life examples, they could even write code.
  prefs: []
  type: TYPE_NORMAL
- en: At TU Vienna, we also used a similar approach to make semiconductor simulation
    more usable for the semiconductor design engineer. The core calculating code was
    written in Fortran. A C language framework that handled the massive simulation
    data input and output and that embedded the XLISP interpreter executed these programs.
    The Lisp code contained the simulation configuration data and could also contain
    simple loops when the simulation was to be executed for many configuration points.
  prefs: []
  type: TYPE_NORMAL
- en: It was polyglot programming, except that we did not know that this is going
    to be the name years after this application coding style.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with polyglot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polyglot programming is not only all about advantages. Before jumping into this
    direction, developers making the decision have to consider a lot of things.
  prefs: []
  type: TYPE_NORMAL
- en: Using another language for the application needs knowledge. Finding people who
    can code in the languages that are used is eventually more difficult than finding
    developers who only know Java. (This is also true if the kernel application language
    is not Java.) Different languages require different mindsets and, many times,
    different people. The team should also have some members who are proficient in
    both languages, and it is also an advantage if most of the people know at least
    a bit about the other language.
  prefs: []
  type: TYPE_NORMAL
- en: The toolset supporting Java is outstanding. The build tools, integrated development
    environment, libraries, debugging possibilities, and logging frameworks, to name
    a few, are all extremely good compared with other languages. Polyglot development
    needs support for the other language as well, which may not be as advanced as
    the support for Java. Many times, it is really an issue to debug DSL solutions
    and IDE support may also be lagging.
  prefs: []
  type: TYPE_NORMAL
- en: When we program in Java, many times, we take for granted that the IDE reads
    the meta-data of the libraries and whenever we need to call a method, or reference
    a class, the IDE suggests the best possibility. XML and properties files may also
    be supported and the IDE may know some of the most used frameworks, such as *Spring*,
    and understand the XML configuration handling the names of the classes as hyperlinks,
    even when the class names are inside some attribute strings.
  prefs: []
  type: TYPE_NORMAL
- en: This is far from being this easy in the case of other languages. For the languages
    that have a wide user base, the tooling support may be good, but if you pick some
    exotic language, you are on your own. The more exotic the language the less support
    you may have.
  prefs: []
  type: TYPE_NORMAL
- en: You can create some tool to support your DSL that you develop. It is not hard
    to do so using tools such as [http://www.eclipse.org/Xtext/](http://www.eclipse.org/Xtext/).
    In such a case, you are tied to *Eclipse*, which may or may not be a problem.
    You can pick a special language, for example, *Kotlin*, which is extensively supported
    by *IntelliJ*, because the same company supports the language and the IDE, but
    again, you buy into a special technology that can be expensive to replace in case
    you have to. It is generally true not only for languages but also for any technology
    you include into your development. When you select one, you should consider the
    support and the cost of getting off the horse if or when it starts dying.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already discussed annotations in great detail. You may recall that
    we defined our annotation interfaces using the following annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This told the Java compiler to keep the annotation and put it into the JVM
    code so that the code can access it during runtime using reflection. The default
    value is `RetentionPolicy.CLASS`, which means that the annotation gets into the
    byte code, but the JVM does not make it available for the runtime system. If we
    use `RetentionPolicy.SOURCE`, the annotation does not even get into the class
    file. In this case, there is only one possibility to do anything with the annotation:
    compile time.'
  prefs: []
  type: TYPE_NORMAL
- en: How can we write code that runs during compile time? Java supports the notion
    of annotation processors. If there is a class on the classpath of the compiler
    that implements the `javax.annotation.processing.Processor` interface, then the
    compiler will invoke the implemented methods one or more times, passing information
    about the source file that the compiler is actually processing. The methods will
    be able to access the compiled methods, classes, or whatever is annotated, and
    also the annotation that triggered the processor invocation. It is important,
    however, that this access is not the same as in runtime. What the annotation processor
    accesses is neither a compiled nor a loaded class, that is, it is available when
    the code uses reflection. The source file at this time is under compilation; thus,
    the data structures that describe the code are actually structures of the compiler,
    as we will see in our next example.
  prefs: []
  type: TYPE_NORMAL
- en: The annotation processor is called one or more times. The reason it is invoked
    many times is that the compiler makes it possible for the annotation processors
    to generate source code based on what it sees in the partially compiled source
    code. If the annotation processor generates any Java source file, the compiler
    has to compile the new source code and perhaps compile some of the already compiled
    files again. This new compilation phase needs annotation processor support until
    there are no more rounds to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation processors are executed one after the other, and they work on the
    same set of source files. There is no way to specify the order of the annotation
    processor executions; thus, two processors working together should perform their
    tasks, no matter in what order they are invoked. Also, note that these codes run
    inside the compiler. If an annotation processor throws an exception, then the
    compilation process will most probably fail. Thus, throwing an exception out of
    the annotation processor should only be done if there is an error that cannot
    be recovered and the annotation processor decides that the compilation after that
    error cannot be complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the compiler gets to the phase to execute the annotation processors, it
    looks at the classes that implement the `javax.annotation.processing.Processor`
    interface and creates instances of these classes. These classes have to have a
    public no-argument constructor. To streamline the execution of the processors
    and to invoke a processor only for the annotations that it can handle, the interface
    contains two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getSupportedSourceVersion` to return the latest version the annotation processor
    can support'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getSupportedAnnotationTypes` to return a set of `String` objects containing
    the fully qualified class name of the annotations that this processor can handle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an annotation processor was created for Java 1.8, it may work with Java 9,
    but it may also not work. If it declares that the latest supported version is
    1.8, then the compiler in a Java 9 environment will not invoke it. It is better
    not to invoke an annotation processor than calling it and messing up the compilation
    process, which may even create compiled but erroneous code.
  prefs: []
  type: TYPE_NORMAL
- en: The values returned by these methods are fairly constant for an annotation processor.
    An annotation processor will return the same source version it can handle and
    will return the same set of annotations. Therefore, it would be clever to have
    some way to define these values in the source code in a declarative manner.
  prefs: []
  type: TYPE_NORMAL
- en: It can be done when we extend the `javax.annotation.processing.AbstractProcessor`
    class instead of directly implementing the `Processor` interface. This abstract
    class implements these methods. Both of them get the information from the annotation
    so that we can decorate the class that extends the abstract class. For example,
    the `getSupportedAnnotationTypes` method looks at the `SupportedAnnotationTypes` annotation
    and returns an array of annotation type strings that are listed in the annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this is a bit brain twisting and can also be confusing at first. We are
    executing our annotation processor during compile time. But the compiler itself
    is a Java application, and in this way, the time is runtime for the code that
    runs inside the compiler. The code of `AbstractProcessor` accesses the `SupportedAnnotationTypes`
    annotation as a runtime annotation using reflection methods. There is no magic
    in it. The method in the JDK 9 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: (The code has been edited for brevity.)
  prefs: []
  type: TYPE_NORMAL
- en: 'To have an example, we will sort of look at the code of a polyglot annotation
    processor. Our very simple annotation processor will process one simple annotation:
    `com.javax0.scriapt.CompileScript`, which can specify a script file. The annotation
    processor will load the script file and execute it using the scripting interface
    of Java 9.'
  prefs: []
  type: TYPE_NORMAL
- en: This code was developed as a demonstration code by the author of this book a
    few years ago and is available with the Apache license from GitHub. Thus, the
    package of the classes is retained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The annotation processor contains two code files. One of the annotation itself
    that the processor will work on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this annotation will not get into the class file after compilation;
    thus, there will be no trace during runtime so that any class source may occasionally
    use this annotation. `Target` of the annotation is `ElementType.TYPE`, meaning
    that this annotation can only be applied to those Java 9 language constructs that
    are some kind of types: `class`, `interface`, and `enum`.'
  prefs: []
  type: TYPE_NORMAL
- en: The annotation has two parameters. The value should specify the name of the
    script file, and the engine may optionally define the type of the script that
    is in that file. The implementation we'll create will try to identify the type
    of the script from the filename extension, but if somebody would like to bury
    some Groovy code into a file that has the `.jy` extension (which is usually for
    Jython), so be it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processor extends `AbstractProcessor` and, in this way, some of the methods
    are inherited at the expense of some annotations used in the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There is no need to implement the `getSupportedAnnotationTypes` and `getSupportedSourceVersion`
    methods. These are replaced by the use of the annotations on the class. We support
    only one annotation in this processor, the one that we defined in the previously
    listed source file, and we are prepared to manage the source code up to Java version
    9\. The only method we have to override is `process`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual annotation is not available during compile time as we already mentioned.
    Hence, what we have available is only a compile time mirror image of the annotation.
    It has the `AnnotationMirror` type, which can be used to get the actual type of
    the annotation and, also, the values of the annotation. The type of the annotation
    is available during compile time. The compiler needs it; otherwise, it could not
    compile the annotation. The values are available from the annotation itself. Our
    `processAnnotation` method handles each annotation it gets as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `@CompileScript` annotation defines two parameters. The first value is
    the script filename and the second one is the scripting engine name. If the second
    one is not specified, then an empty string is set as the default value. The `execute`
    method is called for each and every occasion of the annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The method tries to load the script, based on the filename, and tries to instantiate
    the script engine, based on the given name. If there is no name given, then the
    filename extension is used to identify the scripting engine. By default, the JavaScript
    engine is on the classpath as it is part of the JDK. If any other JVM-based scripting
    engine is in use, then it has to be made available on the classpath or on the
    module path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last method of the class is a simple script manipulation method, nothing
    special. It just chops off the filename extension so that the engine can be identified
    based on the extension string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And just for the sake of completeness, we have the closing brace of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Programming in the enterprise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a professional works for an enterprise, she does not work alone. There
    are a lot of people, developers as well as other coworkers, we have to cooperate
    with. The older the IT department of the enterprise is, and the larger the enterprise
    is, the more specialized roles people are in. You will certainly meet business
    analysts, project managers, test engineers, build engineers, subject-matter experts,
    testers, architects, scrum masters, and automation engineers, to name a few roles.
    Some of these roles may overlap, no person may have more than one responsibility,
    and while in other cases, some roles could even be more specialized. Some of the
    roles are very technical and require less business-related knowledge; others are
    more business oriented.
  prefs: []
  type: TYPE_NORMAL
- en: Working together as a team with so many people and with so many different roles
    is not simple. The complexity of the task may be overwhelming for a novice developer
    and cannot be done without definite policies that all members of the operation
    follow, more or less. Perhaps your experience will show that it is more times
    less than more, but that is a different story.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the way developers work together, there are well-established industry practices.
    These support the **Software Development Lifecycle** (**SDLC**) using waterfall,
    agile, or a mix of the two models in some way. In the following sections, we will
    look at tools and techniques that are, or at least should have been, used in every
    software development organization. These are:'
  prefs: []
  type: TYPE_NORMAL
- en: Static code analysis tools that control the quality of the code examining the
    source code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code version control that stores all the versions of the source code
    and help get the source code for any old version of the development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software versioning to keep some order of how we identify the different versions
    and do not get lost among the different versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code review and tools that help in pin-pointing bugs that are not revealed by
    tests and aid knowledge sharing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge base tools to record and document the findings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issue tracking tools that record bugs, customer issues, and other tasks that
    somebody has to attend to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selection process and considerations for external products and libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration that keeps the software in a consistent state and reports
    immediately if there is some error in it before the error propagates to other
    versions or other code, depending on how the erroneous code gets developed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release management, which keeps track of the different release versions of the
    software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code repository, which stores the compiled and packed artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the most widely used tools for these tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.gif)'
  prefs: []
  type: TYPE_IMG
- en: Static code analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Static code analysis tools read the code just like the compiler and analyze
    it, but instead of compilation, they try to find errors or mistakes in it. Not
    the syntax errors. For that, we already have the Java compiler. Mistakes, such
    as using a loop variable outside a loop, which may be absolutely valid but is
    usually bad style and, many times, such usage comes from some simple mistakes.
    They also check that the code follows the styling rules that we set.
  prefs: []
  type: TYPE_NORMAL
- en: Static code analyzers help identify many small and obvious errors in the code.
    Sometimes, they are annoying, warning about something that may not be really a
    problem. In such a case, it is better to code the program a bit differently, not
    because we want the static code analysis to run without warning. We should never
    modify the code because of a tool. If we code something in such a way that it
    passes some quality check tool and not because it is better that way, then we
    are serving the tools instead of the tools serving us.
  prefs: []
  type: TYPE_NORMAL
- en: The reason to change the code to pass the code analysis is that it is very probable
    that the code is more readable to an average programmer if it does not violate
    the coding style. You or the other team members can be excellent programmers who
    understand the code very easily even if it uses some special construct. However,
    you cannot say that about all the programmers who will maintain your code in the
    future. The code lives a long life. I work with some programs that have been written
    50 years ago. They are still running and maintained by young professionals around
    the age of 30\. It means that they were not even born when the code was developed.
    It can easily happen that the person maintaining your code is not even born by
    the time you write the code. You cannot tell anything about their cleverness and
    coding practices. The best we can do is to prepare for the average and that is
    exactly what static code analysis tools are set for.
  prefs: []
  type: TYPE_NORMAL
- en: The checks that these tools perform are not hardwired into the tools. Some special
    language inside the tools describes the rules and they can be deleted, other rules
    can be added, and rules can be modified. This way, you can accommodate the coding
    standards of the enterprise you work for. The different rules can be categorized
    as cosmetic, minor, major, and critical. Cosmetic things are mainly warnings and
    we do not really care about them, even though it is nice to fix even these issues.
    Sometimes, these small things may signal some really big issue. We can set limits
    for the number of minor and major bugs before the check is declared as failing
    and also for the critical errors. In the last case, this limit is usually zero.
    If a coding error seems to be critical, then better not have any in the code.
  prefs: []
  type: TYPE_NORMAL
- en: The most frequently used tools are **Checkstyle**, **FindBugs**, and **PMD**.
    The execution of these tools is usually automated, and though they can be executed
    from the IDE or from the developer's command line, their main use is on the **continuous
    integration** (**CI**) server. During the build, these tools are configured on
    the CI server to run, and it can be configured such that the build should be broken
    if the static code analysis fails with some limit. Executing the static code analysis
    is usually the next step after compilation and unit test execution, and before
    the actual packaging.
  prefs: []
  type: TYPE_NORMAL
- en: The **SonarQube** tool ([https://www.sonarqube.org/](https://www.sonarqube.org/))
    is a special tool in addition to being a static code analysis tool. SonarQube
    maintains the history of the previous checks as well as supports unit test code
    coverage and can report the change of the quality over time. This way, you can
    see how the quality, coverage percentage, and number of different qualifications
    of code style errors have changed. Many times, you can see that when approaching
    the release date, the code quality decreases as people are in a rush. This is
    very bad because this is the time when most of the bugs should be eliminated.
    Having a statistic about the quality may help change the practice by seeing the
    trends before the quality, and thus the maintainability of the code gets out of
    hand.
  prefs: []
  type: TYPE_NORMAL
- en: Source code version control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Source code version control systems store different versions of the source code.
    These days, we cannot imagine professional software development without it. This
    was not always the case, but the availability of free online repositories encouraged
    hobby developers to use some version control, and when these developers worked
    for enterprises later, it was evident that the use of these systems is kind of
    a must.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different revision control systems. The most widely used one
    is Git. The version control that was previously widely used was **SVN** and, even
    before that, **CVS**. These are less and less used these days. We can see **SVN**
    as a successor of **CVS** and Git as a successor of **SVN**. In addition to these,
    there are other version control systems such as **Mercurial**, **Bazaar**, or
    **Visual Studio Team Services**. For a comprehensive list of the available tools,
    visit the Wikipedia page at [https://en.wikipedia.org/wiki/List_of_version_control_software](https://en.wikipedia.org/wiki/List_of_version_control_software).
  prefs: []
  type: TYPE_NORMAL
- en: My bet is that you will meet Git in the first place and there is a high probability
    of you coming across SVN when programming for an enterprise. Mercury may appear
    in your practice but any of the others that currently exist are very rare, are
    used for a specific area, or are simply extinct.
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems allow the development team to store the different versions
    of the software in an organized manner on a storage that is maintained (backed
    up regularly in a reliable manner). This is important for different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing is that different versions of the software may be deployed to
    different instances. If we develop software for clients and we have many clients
    with whom we hope to have to make a terrific business, then different clients
    may have different versions. This is not only because some clients are reluctant
    to pay for the upgrade, and we just do not want to give the new version for free.
    Many times, the costs that rise on the side of the customer prevent the upgrade
    for a long time. Software products do not work on their own in an isolated environment.
    Different clients have different integrated environments; the software communicates
    with different other applications. When a new version is to be introduced in an
    enterprise environment, it has to be tested for whether it works with all the
    systems it has to cooperate with. This testing takes a lot of effort and money.
    If the new features or other values that the new version delivers over the old
    one do not justify the cost, then it would be waste of money to deploy the new
    version. The fact that there is a new version of our software does not mean that
    the old versions are not usable.
  prefs: []
  type: TYPE_NORMAL
- en: If there is some bug at the customer's end, then it is vital that we fix the
    bug in that version. To do so, the bug has to be reproduced in the development
    environment, which eventually means that the source code for that version has
    to be available for the developers.
  prefs: []
  type: TYPE_NORMAL
- en: This does require the customer database to contain references to the different
    versions of our software products that are installed at the customer site. To
    make it more complicated, a customer may have more than one version at a time
    in different systems and may also have different licenses, so the issue is more
    complex than it first seems. If we do not know which version the client has, then
    we are in trouble.
  prefs: []
  type: TYPE_NORMAL
- en: Since the database registering the versions for the customers and real life
    may get unsynchronized, software products log their version at startup. We have
    a separate section about versioning in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If the bug is fixed in the version that the client has, the incident at the
    customer's end may be solved after deployment. The problem, though, still remains
    if the version is not the previous version of the software. The bug fix introduced
    to an old version of the software may still be lurking around in the later or,
    for that matter, earlier versions. The development team has to identify which
    versions are relevant to clients. For example, an old version that is not installed
    any more at any of the clients' sites does not deserve the investigation. After
    that, the relevant versions have to be investigated to check whether they exhibit
    the bug. This can only be done if we have the source version. Some old versions
    may not have the bug if the code causing the bug is introduced in later versions.
    Some new versions may also be immune to the bug because the bug was already fixed
    in the previous version, or simply because the piece of code that caused the bug
    was refactored even before the bug manifested. Some bugs may even affect a specific
    version instead of a range of products. Big fixing may be applied to different
    versions and they may need slightly different fixes. All this needs a maintained
    source version repository.
  prefs: []
  type: TYPE_NORMAL
- en: Even when we do not have different customers with different versions, it is
    more than likely that we have more than one version of our software in development.
    The development of a major release is coming to an end, and therefore, one part
    of the team responsible for testing and bug fixing focuses on those activities.
    At the same time, the development of features for the next version still goes
    on. The code implementing the functionalities for the next version should not
    get into the version that is about to be released. The new code may be very fresh,
    untested, and may introduce new bugs. It is very common to introduce freeze times
    during the release process. For example, it may be forbidden to implement any
    new feature of the upcoming release. This is called feature freeze.
  prefs: []
  type: TYPE_NORMAL
- en: Revision control systems deal with these freeze periods, maintaining different
    branches of the code. The release will be maintained in one branch and the version
    for later releases in a different one. When the release goes out, the bug fixes
    that were applied to it should also be propagated to the newer version; otherwise,
    it might so happen that the next version will contain bugs that were already fixed
    in the previous version. To do so, the release branch is merged with the ongoing
    one. Thus, version control systems maintain a graph of the versions, where each
    version of the code is a node in the graph and the changes are vertices.
  prefs: []
  type: TYPE_NORMAL
- en: Git goes very far in this direction. It supports branch creation and merging
    so well that developers create separate branches for each change that they create
    and then they merge it back with the master branch when the feature development
    is done. This also makes for a good opportunity for code review. The developer
    making the feature development or bug fix creates a pull request in the GitHub
    application, and another developer is requested to review the change and perform
    the pull. This is a kind of four-eyed principle applied to code development.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the revision control systems keep the repository on a server and any
    change gets to the server. The advantage of this is that any change committed
    gets to a server disk that is regularly backed up and is thus safe. Since the
    server-side access is controlled, any code sent to the server cannot be rolled
    back without trace. All versions, even the wrong versions, are stored on the server.
    This may be required by some legal control. On the other hand, if commit requires
    network access and server interaction, it may be slow and this will, in the long
    run, motivate developers not to commit their changes frequently. The longer a
    change remains on the local machine, the more risk we have of losing some of the
    code, and merging becomes more and more difficult with time. To heal this situation,
    Git distributes the repository and the commit happens to the local repository,
    which is exactly the same as the remote one on some server. The repositories are
    synchronized when one repository pushes the changes to another one. This encourages
    the developers to make frequent commits to the repository, giving short commit
    messages, which helps in tracking the change made to the code.
  prefs: []
  type: TYPE_NORMAL
- en: Some older version control systems support file locking. This way, when a developer
    checks out a code file, others cannot work on the same piece of code. This essentially
    avoids the collisions during code merging. Over the years, this approach did not
    seem to fit the development methodologies. Merge issues are less of a problem
    than files that are checked out and forgotten. SVN supports file locking but this
    is not really serious and does not prevent one developer to commit changes to
    a file that somebody else locked. It is more of only a suggestion than real locking.
  prefs: []
  type: TYPE_NORMAL
- en: Source code repositories are very important but should not be confused with
    release repositories, which store the compiled released version of the code in
    binary. Source and release repositories work together.
  prefs: []
  type: TYPE_NORMAL
- en: Software versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software versioning is magic. Think about the different versions of Windows
    or Star Wars movies. Well, the latter is not really software versioning but it
    shows that the issue is very general. In the case of Java, versioning is not that
    complex. First of all, the version of Java we use now is 9\. The previous version
    was 1.8, before that 1.7, and so on, down to 1.0\. Earlier versions of Java were
    called Oak but that is history. After all, that is, who can tell what Java 2 was?
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, when we create a Java application, the situation is simpler. There
    has been a suggestion from Oracle, from the time of Java 1.3, about how to version JARs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.oracle.com/javase/7/docs/technotes/guides/extensions/versioning.html](http://docs.oracle.com/javase/7/docs/technotes/guides/extensions/versioning.html)'
  prefs: []
  type: TYPE_NORMAL
- en: This document distinguishes between specification version and implementation
    version. If the specification of a JAR content changes, the code has to behave
    differently from how it was behaving till then; the specification version should
    change. If the specification is not changed but the implementation does--for example,
    when we fix a bug--then the implementation version changes.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, nobody has used this scheme, although it is a brilliant idea to
    separate the implementation and specification versions, at least, in theory. I
    even bet that most of your colleagues have not even ever heard about this versioning.
    What we use in practice is semantic versioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic versioning ([http://semver.org/](http://semver.org/)) mixes the specification
    and implementation versions into one single version number triplet. This triplet
    has the format of **mmp,** that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**m**: major version number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**m**: minor version number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p**: patch number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The specification says that these numbers start with zero and increase by one.
    If the major number is zero, it means that the software is still in development.
    In this state, the API is not stable and may change without a new major version
    number. The major version number gets to 1 when the software is released. Later
    it has to be increased when the API of the application (library) has changed from
    the previous version and the application is not backward compatible with the previous
    version. The minor version number is increased when the change effects only the
    implementation but the change is significant, perhaps, even the API is also changing
    but in a backward-compatible manner. The patch version is increased when some
    bug is fixed, but the change is not major and the API does not change. The minor
    and the patch levels have to be reset to zero if any version number in the triplet
    in front of any of them is increased: major version number increase resets both
    minor and patch version; minor version number increase resets patch number.'
  prefs: []
  type: TYPE_NORMAL
- en: This way, semantic versioning keeps the first element of the triplet for the
    specification version. The minor is a mix of the specification and implementation
    versions. A patch version change is clearly an implementation version change.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these, semantic versioning allows appending a pre-release string,
    such as `-RC1` and `-RC2`. It also allows the appending of metadata, such as a
    date after a plus sign, for example, `+20160120` as a date.
  prefs: []
  type: TYPE_NORMAL
- en: The use of semantic versioning helps those that use the software to easily spot
    compatible versions and to see which version is older and which is newer.
  prefs: []
  type: TYPE_NORMAL
- en: Code review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we create programs in a professional way, it is done in teams. There is
    no one-man show programming other than in a hobby or going along with the tutorials.
    It is not only because it is more effective to work in teams but also because
    one person is vulnerable. If you work alone and get hit by the bus or you hit
    the lottery and lose your ability or motivation to work on the project, your customer
    is in trouble. That is not professional. Professional projects should be resilient
    to any member falling off.
  prefs: []
  type: TYPE_NORMAL
- en: Teamwork needs cooperation and one form of cooperation is code review. This
    is the process when a developer or a group of developers reads a part of the code
    that some other team members have written. There are direct gains from this activity;
  prefs: []
  type: TYPE_NORMAL
- en: The developers reading the code get more knowledge about the code; they learn
    the code. This way, if the developer creating the code gets out of the process
    for any reason, the others can continue the work with minimal bump.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding styles can be aligned. Developers, even seniors, paying careful attention
    make coding mistakes. This may be a bug or it may be a coding style violation.
    Coding style is important because the more readable the code is, the less possibility
    of it having unnoticed bugs. (Also see the next bullet point.) It is also important
    that the coding style is the same for the team. All team members should use the
    same style. Looking at a code that has a different style from the one I wrote
    is a bit harder to follow and understand. The differences may distract the reader
    and the team members have to be able to read the code. The code belongs to the
    team and not a single developer. Any team member should know the code and be able
    to modify it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During code review, a lot of bugs can be discovered. The parties looking at
    the code and trying to understand the working of it may occasionally discover
    bugs from the structure of the code, which are otherwise hard to discover using
    tests. If you want, code review is the whitest white box test. People think differently
    and different mindsets catch different bugs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code review can be done online and offline. It can be done in teams or peer-to-peer.
  prefs: []
  type: TYPE_NORMAL
- en: Most teams follow the code review process that GitHub supports, which is the
    simplest. Changes to the code are committed to a branch and are not merged with
    the code directly but, rather, a pull request is created on the web interface.
    The local policy may require that a different developer perform the pull. The
    web interface will highlight the changes and we can add comments to the changed
    code. If the comments are significant, then the original developer requesting
    the pull should modify the code to answer the comments and request the pull again.
    This ensures that at least two developers see any change; the knowledge is shared.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback is peer-to-peer. It is not a senior teaching a junior. That needs a
    different channel. Comments in GitHub are not good for this purpose; at least,
    there are better channels. Perhaps talking face to face. Comments may come from
    a senior to a junior or from a junior to a senior. In this work, giving feedback
    on the quality of the code, seniors and juniors, are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest and perhaps the most frequent comment is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I can see that Xyz.java was changed in the modification but I see no change
    made to **XyzTest.java.* This is almost an instant refusal for the merge. If a
    new feature is developed, unit tests have to be created to test that feature.
    If a bug is fixed, then unit tests have to be created to prevent the bug from
    coming back. I personally got this comment many times, even from juniors. One
    of them told me, "We know that you were testing us if we dared to give feedback."'
  prefs: []
  type: TYPE_NORMAL
- en: God knows I was not. They did not believe.
  prefs: []
  type: TYPE_NORMAL
- en: While change review and GitHub is a good tool during development, it may not
    be appropriate when a larger chunk of code has to be reviewed. In such a case,
    other tools, such as **FishEye**, have to be used. In this tool, we can select
    the source files for review even if they were not recently changed. We can also
    select reviewers and deadlines. Commenting is similar to GitHub. Finally, this
    type of code review finishes with a code review session, where the developers
    gather and discuss the code in person.
  prefs: []
  type: TYPE_NORMAL
- en: While organizing such a session, it is important that a person who has experience
    managing other people mediates these sessions. Code and discussion on styles can
    get very personal. At the same time, when attending such a meeting, you should
    also pay attention so as not to get personal. There will be enough participants
    who may not know this or are less disciplined.
  prefs: []
  type: TYPE_NORMAL
- en: Never attend a review session without reviewing the code first using the online
    tools. When you make comments, the language should be very polite for the reason
    I have already mentioned. Finally, the mediator of the meeting should be able
    to separate important and not so important issues and to stop debate on bagatelle
    things. Somehow, the less important issues are more sensitive. I personally do
    not care about formatting the tab size if it is two or four spaces and if the
    file should contain only spaces or if tab characters are allowed, but people tend
    to like to waste time on such issues.
  prefs: []
  type: TYPE_NORMAL
- en: The most important issue during code review sessions is that we are professional
    and it may happen that I review and comment your code today, but tomorrow, it
    will be just the opposite, and we work together and we have to work together as
    a team.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge base
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowledge base was a buzzword a few years ago. Few companies were evangelizing
    the idea of wiki technology and nobody was using it. Today, the landscape of knowledge
    base is totally different. All enterprises use some kind of wiki implementation
    that is there to share knowledge. They mostly use Confluence, but there are also
    other wiki solutions available, commercial and free as well.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge bases store information that you, as a developer, would write down
    in a paper notebook for your later reference, for example, the IP address of the
    development server, directories where to install the JAR files, what commands
    to use, what libraries you have collected, and why you use them. The major difference
    is that you write it in a formatted way into a wiki and it is available immediately
    for other developers. It is a bit of a burden on the developer to write these
    pages, and it needs some self-discipline first. Sticking to the example of the
    IP address of the development server and the install directories, you have to
    write not only the IP address of the server but also some text explaining what
    the information is, because the others may not understand it otherwise. It is
    also a bit of work to place the page with the information in the wiki system with
    a good name, linking it to other pages, or finding the appropriate position of
    the page in the tree of pages. If you were using the paper notebook, you could
    just write down the IP address and the directories on the first free page of the
    book and you would just remember all others.
  prefs: []
  type: TYPE_NORMAL
- en: The wiki approach will pay back when coworkers do not need to find the information
    themselves; you can find the information in an easier way because other coworkers
    have also recorded their findings in the knowledge base and, last but not least,
    a few months later, you find the information you recorded yourself. In the case
    of a paper notebook, you would turn the pages to find the IP address and you may
    or may not remember which one is the primary and which is the secondary server.
    You may even forget by then that there are two servers (or was it a double cluster?).
  prefs: []
  type: TYPE_NORMAL
- en: To have a long list of available wiki software, visit [https://en.wikipedia.org/wiki/Comparison_of_wiki_software](https://en.wikipedia.org/wiki/Comparison_of_wiki_software).
  prefs: []
  type: TYPE_NORMAL
- en: Issue tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Issue tracking systems keep track of issues, bugs, and other tasks. The first
    issue tracking systems were created to maintain the list of bugs and also the
    state of the bug fixing process to ensure that a bug, identified and recorded,
    will not get forgotten. Later, these software solutions developed and became full-fledged
    issue trackers and are unavoidable project management tools in every enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: The most widely used issue tracking application used in many enterprises is
    Jira, but on the [https://en.wikipedia.org/wiki/Comparison_of_issue-tracking_systems](https://en.wikipedia.org/wiki/Comparison_of_issue-tracking_systems)
    page, you can find many other applications listed.
  prefs: []
  type: TYPE_NORMAL
- en: The most important feature of an issue tracker application is that it has to
    record an issue in detail in an editable manner. It has to record the person who
    recorded the issue in case more information is needed during issue handling. The
    source of the issue is important. Similarly, issues have to be assigned to some
    responsible person, who is accountable for the progress of issue handling.
  prefs: []
  type: TYPE_NORMAL
- en: Modern issue tracking systems provide complex access control, workflow management,
    relation management, and integration with other systems.
  prefs: []
  type: TYPE_NORMAL
- en: Access control will only allow the person who has something to do with an issue
    access to it, so others cannot alter the state of an issue or even read the information
    attached to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'An issue may go through different workflow steps depending on the type of issue:
    a bug may be reported or reproduced, a root cause analyzed, a fix developed or
    tested, a patch created, a fix merged with the next release version or published
    in the release. This is a simple workflow with a few states.'
  prefs: []
  type: TYPE_NORMAL
- en: Relation management allows setting different relations between issues and allowing
    the user to navigate from issue to issue along these relations. For example, a
    client reports a bug, and the bug is identified as being the same as another already
    fixed. In such a case, it would be insane to go through the original workflow
    and creating a new patch for the same bug. Instead, the issue gets a relation
    pointing to the original issue and sets the state to be closed.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with other systems is also useful to keep a consistent development
    state. Version control may require that, for every commit, the commit message
    contains a reference to the issue that describes the requirement, bug, or change
    that the code modification supports. Issues may be linked to knowledge base articles
    or agile project management software tools using web links.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed testing when we talked about unit testing. Unit testing
    is extremely important in agile development and it helps keep the code clean and
    reduce the number of errors. But this is not the only type of testing that you
    will see in enterprise development.
  prefs: []
  type: TYPE_NORMAL
- en: Types of tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing is performed for many reasons but there are at least two reasons that
    we have to mention. One is to hunt the bugs and create error-free code as much
    as possible. The other is to prove that the application is usable and can be utilized
    for the purpose it was meant for. It is important from the enterprise point of
    view and considers a lot of aspects that unit test does not. While unit test focuses
    on one unit and, thus, is an extremely good tool to point out where the error
    is, it is totally unusable when it comes to discovering bugs that come from erroneous
    interfaces between modules. The unit tests mock external modules and, thus, test
    that the unit works as expected. However, if there is an error in this expectation
    and the other modules do not behave in the same way as the unit test mock, the
    error will not be discovered.
  prefs: []
  type: TYPE_NORMAL
- en: To discover the errors on this level, which is the next level above unit test,
    we have to use integration tests. During integration tests, we test how individual
    units can work together. When we program in Java, the units are usually classes;
    thus, the integration test will test how the different classes work together.
    While there is a consensus (more or less) about what a unit test is in Java programming,
    this is less so in the case of integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, the external dependencies, such as other modules reachable via
    the network or database layers may be mocked, or may be set up using some test
    instance during integration testing. The argument is not about whether these parts
    should be mocked or not, only the terminology. Mocking some components such as
    the database has advantages as well as drawbacks. As in the case of any mock,
    the drawback is the cost of setting up the mock as well as the fact that the mock
    behaves differently from the real system. Such a difference may result in some
    bugs still remaining in the system and lurking there until a later case of testing
    or, God forgive, production is used.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests are usually automated in a way similar to unit tests. However,
    they usually require more time to execute. This is the reason why these tests
    are not executed at each source code change. Usually, a separate maven or Gradle
    project is created that has a dependency on the application JAR and contains only
    integration test code. This project is usually compiled and executed daily.
  prefs: []
  type: TYPE_NORMAL
- en: It may happen that daily execution is not frequent enough to discover the integration
    issues in a timely manner, but a more frequent execution of the integration tests
    is still not feasible. In such a case, a subset of the integration test cases
    is executed more frequently, for example, every hour. This type of testing is
    called smoke testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the position of the different testing types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.gif)'
  prefs: []
  type: TYPE_IMG
- en: When the application is tested in a fully set up environment, the testing is
    called system testing. Such testing should discover all the integration bugs that
    may have been lurking and covered during the previous testing phases. The different
    type of system tests can also discover non-functional issues. Both functional
    testing and performance testing are done on this level.
  prefs: []
  type: TYPE_NORMAL
- en: Functional testing checks the functions of the application. It ensures that
    the application functions as expected or at least has functions that are worth
    installing in the production environment and can lead to cost saving or profit
    increase. In real life, programs almost never deliver all the functions that were
    envisioned in any requirement documentation, but if the program is usable in a
    sane manner, it is worth installing it, assuming that there are no security issues
    or other issues.
  prefs: []
  type: TYPE_NORMAL
- en: In case there are a lot of functions in the application, functional testing
    may cost a lot. In such a case, some companies perform a sanity test. This test
    does not check the full functionality of the application, only a subset to ensure
    that the application reaches a minimal quality requirement and it is worth spending
    the money on the functional testing.
  prefs: []
  type: TYPE_NORMAL
- en: There may be some test cases that are not envisioned when the application was
    designed and thus there is no test case in the functional test plan. It may be
    some weird user action, a user pressing a button on the screen when nobody thought
    it was possible. Users, even if benevolent, happen to press or touch anything
    and enter all possible unrealistic inputs into a system. Ad-hoc testing tries
    to amend this shortage. A tester during ad-hoc testing tries all the possible
    ways of use of the application that he or she can imagine at the moment the test
    is executed.
  prefs: []
  type: TYPE_NORMAL
- en: This is also related to security testing, also called penetration testing when
    the vulnerabilities of the system are discovered. These are special types of tests
    that are performed by professionals who have their core area of expertise in security.
    Developers usually do not have that expertise, but at least, the developers should
    be able to discuss issues that are discovered during such a test and amend the
    program to fix the security holes. This is extremely important in the case of
    Internet applications.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing checks that the application, in a reasonable environment,
    can handle the expected load that the user puts on the system. A load test emulates
    the users who attack the system and measures the response times. If the response
    time is appropriate, that is, lower than the required maximum under the maximum
    load, then the test passes; otherwise, it fails. If a load test fails, it is not
    necessarily a software error. It may so happen that the application needs more
    or faster hardware. Load tests usually test the functionality of the application
    in only a limited way and only test use scenarios that pose read load on the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many years ago, we were testing a web application that had to have a response
    time of 2 seconds. The load test was very simple: issue `GET` requests so that
    there are a maximum of 10,000 requests active at the same time. We started with
    10 clients, and then a script was increasing the concurrent users to 100, then
    1,000, and then stepping up by thousand every minute. This way, the load test
    was 12 minutes long. The script printed the average response time, and we were
    ready to execute the load test at 4:40 pm on a Friday.'
  prefs: []
  type: TYPE_NORMAL
- en: The average response time started from a few milliseconds and went up to 1.9
    seconds as the load was increased to 5,000 concurrent users, and from there, it
    was descending down to 1 second as the load was increased to 10,000 users. You
    can understand the attitude of the people on a Friday afternoon, being happy that
    we met the requirements. My colleagues left for the weekend happily. I remained
    testing a bit more because I was bothered by the phenomenon that the response
    time decreases as we increase the load above 5,000\. First, I reproduced the measurement
    and then started looking at the log files. At 7 pm, I already knew what the reason
    was.
  prefs: []
  type: TYPE_NORMAL
- en: When the load went above 5,000, the connections the Apache server was managing
    started to exhaust and the web server started to send back 500 internal error
    codes. That is something that Apache can very effectively do. It is very fast
    in telling you that you cannot be served. When the load was around 10,000 concurrent
    users, 70% of the responses already had 500 errors. The average went down, but
    the users were actually not served. I reconfigured the Apache server so that it
    could serve all the requests and forward each to our application just to learn
    that the response time of our application was around 10 seconds at the maximum
    load. Around 10 pm, when my wife was calling my mobile the third time, I also
    knew how large a memory I should set in the Tomcat startup file in the options
    for the JVM to get the desired 2-second response time in case of 10,000 concurrent
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Stress test is also a type of performance test that you may also face. This
    type of test increases the load on the system until it cannot handle the load.
    That test should ensure that the system can recover from the extreme load automatically
    or manually but, in no case, will do something that it shouldn't at all. For example,
    a baking system should not ever commit an unconfirmed transaction, no matter how
    big the load there is. If the load is too high, then it should leave the dough
    raw but should not bake extra bread.
  prefs: []
  type: TYPE_NORMAL
- en: The most important test at the top of the hierarchy is the user acceptance test.
    This is usually an official test that the customer, who buys the software, executes
    and in the case of successful execution, pays the price for the software. Thus,
    this is extremely important in professional development.
  prefs: []
  type: TYPE_NORMAL
- en: Test automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tests can be automated. It is not a question of whether it is possible to automatize
    a test, only whether it is worth doing so. Unit tests and integration tests are
    automated, and as time advances, more and more tests get automated as we move
    along to higher steps towards the **user acceptance test** (**UAT**). UAT is not
    likely to be automated. After all, this test checks the integration between the
    application and the user. While the user, as an external module, can be mocked
    using automation in lower levels, we should reach the level when the integration
    test happens without mocks.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools that help the automation of tests. The blocker for test
    automation, these days, is the cost of the tools to do so, the cost of learning
    and developing the tests, and the fear that the automated tests are not discovering
    some of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: It is true that it is easier to do something wrong with a program than without.
    This is so true for almost anything not only for testing. And still we do use
    programs; why else would you read this book? Some of the errors may not be discovered
    during automated functional testing, which would otherwise have been discovered
    using manual tests. At the same time, when the same test is executed the hundredth
    time by the same developer, it is extremely easy to skip an error. An automated
    test will not ever do that. And most importantly, the cost of the automated test
    is not 100 times the cost of running it once.
  prefs: []
  type: TYPE_NORMAL
- en: We have used test automation tools in this book. **SoapUI** is a tool that helps
    you create tests that can be executed automatically. Other testing tools that
    are worth looking at are **Cucumber**, **Concordion**, **Fintnesse**, and **JBehave**.
    There is a good comparison of tools at [https://www.qatestingtools.com/](https://www.qatestingtools.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Black box versus white box
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may have heard many times that a test is a black box test. This simply
    means that the test does not know anything about how the system under test (SUT)
    is implemented. The test relies only on the interface of the SUT that is exported
    for the outside world. A white box test, on the other end of the scale, tests
    the internal working of the SUT and very much relies on the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.gif)'
  prefs: []
  type: TYPE_IMG
- en: Both the approaches have advantages and disadvantages. We should use one, or
    the mixture of the two approaches, a way that fits the purpose of the actual testing
    needs the most. A black box test not relying on the implementation does not need
    to change if the implementation changes. If the interface of the tested system
    changes, then the test should also be changed. A white box test may need changes
    if the implementation changes, even if the interface remains the same. The advantage
    of the white box test is that, many times, it is easier to create such a test
    and the testing can be more effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the best of both worlds, systems are designed to be testable. Be careful,
    though. It means many times that the functionality internal to the tested system
    is propagated to the interface. That way, the test will use only the interface
    and, thus, can be declared to be a black box, but it does not help. If something
    changes in the internal working of the tested system, the test has to follow it.
    The only difference is that you may call it a black box test if the interface
    also changes. That does not save any work. Rather, it increases it: we have to
    check all the modules that rely on the interface if they also need any change.'
  prefs: []
  type: TYPE_NORMAL
- en: I do not say that we should not pay attention to creating testable systems.
    Many times making a system testable results in cleaner and simpler code. If the
    code, however, gets messier and much longer because we want to make it testable,
    then we are probably not going in the right way.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programming for the enterprise or even programming a moderately sized project
    cannot be done without the use of external libraries. In the Java world, most
    of the libraries that we use are open source and, more or less, free to use. When
    we buy a library that is sold for money, there is usually a standard process enforced
    by the purchasing department. In such a case, there is a written policy about
    how to select the vendor and the library. In the case of "free" software, they
    do not usually care, though they should. In such a case, the selection process
    mainly lies with the IT department and it is therefore important to know the major
    points to be considered before selecting a library even if for free.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous paragraph, I put the word free between quotes. That is because
    there is no software, which is free. There is no such thing as a free lunch, as
    they say. You have heard this many times but it may not be obvious in the case
    of an open source code library or framework you are going to select. The major
    selection factor for any purchase or implementation is the cost, the price. If
    the software is free, it means that you do not need to pay an upfront fee for
    the software. However, there is a cost in integrating it and using it. Support
    costs money. Somebody may say that the support is community support and also available
    free of charge. The thing is that the time you spend hunting for a workaround
    that helps you to get over a bug is still money. It is your time, or in case you
    are a manager, it is the time of the professional in your department whose time
    you pay for, or, as a matter of fact, it can be an external contractor who will
    hand you a huge bill in case you do not have the expertise in-house to solve the
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: Since free software does not have a price tag attached, we have to look at the
    other factors that are important in the selection process. At the end of the day,
    they all will affect the cost in some way. Sometimes, the way a criterion alters
    the cost is not obvious or easily calculable. However, for each one, we can set
    no-go levels that are based on technology decisions, and we can compare libraries
    for being better or worse along with each of the criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Fit for the purpose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps, this is the most important factor. Other factors may be argued about
    in terms of the scale of importance, but if a library is not appropriate for the
    purpose we want to use, then this is certainly not something to select, no matter
    what. It may be obvious in many cases, but you may be surprised how many times
    I have seen a product selected because it was the favorite of a person in some
    other project and the library was forced for use in the new project even though
    the requirements were totally different.
  prefs: []
  type: TYPE_NORMAL
- en: License
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The license is an important question as not all free software is free for all
    uses. Some of the licenses allow free use for hobby projects and education but
    require you to purchase the software for professional, profit-oriented use.
  prefs: []
  type: TYPE_NORMAL
- en: The most widely used licenses and their explanation (and the whole text of the
    license) is available on the web page of the **Open Source Initiative** ([https://opensource.org/licenses](https://opensource.org/licenses)).
    It lists nine different licenses, and to make the situation a bit more complex,
    these licenses have versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the oldest licenses is the **General Public License** (**GPL**) standing
    for GNU. This license contains the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you distribute copies of such a program, whether gratis or for
    a fee, you must pass on to the recipients the same freedoms that you received.
    You must make sure that they, too, receive or can get the source code.
  prefs: []
  type: TYPE_NORMAL
- en: If you create software for a for-profit enterprise and the company intends to
    sell software, you probably cannot use any line of code that is from a GPL-licensed
    software. It would imply that you are required to pass on your own source code,
    which may not be the best sales strategy. Apache license, on the other hand, may
    be okay for your company. This is something that the lawyers should decide.
  prefs: []
  type: TYPE_NORMAL
- en: Even though this is the lawyers' work, there is one important point that we
    developers must be aware of and pay close attention to. Sometimes, the libraries
    contain code from other projects and their license, as advertised, may not be
    the real one. A library may be distributed under the Apache license but contains
    code that is GPL-licensed. This is obviously a violation of the GPL license, which
    was committed by some open source developers. Why would you care? Here comes the
    explanation via an imagined situation.
  prefs: []
  type: TYPE_NORMAL
- en: You develop software for an enterprise. Let's say that this company is one of
    the largest car manufacturers of the world, or it is one of the largest banks,
    pharma, whatever. The owner of the GPL software seeks remedies for the misuse
    of her software. Will she sue the software developer, John Doe, who has a total
    wealth of 200K, or your company, claiming that you did not duly check the license
    of the code? She certainly will not dig for gold where there is none. Suing the
    company you work for may not be successful, but certainly not a good process you
    or anyone at the company wants.
  prefs: []
  type: TYPE_NORMAL
- en: What can we as software professionals do?
  prefs: []
  type: TYPE_NORMAL
- en: We have to use libraries that are well known, used widely. We can check the
    source code of the library to see whether there is some copied code. Some package
    names may present some clue. You can Google some part of the source code to find
    matches. Last but not least, the company can subscribe to services that provide
    similar research for the libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation is an important aspect. If the documentation is not appropriate,
    it will be hard to learn how to use the library. Some of the team members may
    have already known the library, but, again, this may not be the case for later
    team members. We should consider our colleagues, who are expected to be average
    programmers, and they will have to learn the use of the library. Thus documentation
    is important.
  prefs: []
  type: TYPE_NORMAL
- en: When we speak about documentation, we should not only think about the *JavaDoc*
    reference documentation but also tutorials and books if they are available.
  prefs: []
  type: TYPE_NORMAL
- en: Project alive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important not to select a library for use that is not alive. Have a look
    at the roadmap of the library, the last time a release was shipped, and the frequency
    of the commits. If the library is not alive, we should consider not using it.
    Libraries work in an environment and the environment changes. The library may
    connect to a database. The new version of the database may provide new features
    that give us better performance only if the library is modified to accommodate
    these new features. The library communicates over HTTP; will it support the new
    2.0 version of the protocol? If nothing else, the version of the Java environment
    will change over the years and the library we use should sooner or later follow
    it to leverage the new features.
  prefs: []
  type: TYPE_NORMAL
- en: There is no guarantee that an alive library will always stay alive. However,
    a library that is already dead will certainly not resurrect.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the project is alive at the moment, there are some points that may give
    some hints about the future of the library. If the company developing it is well-established
    and financially stable, and the library is developed with a reasonable business
    model, then there is a low risk that the project dies. If there are a lot of companies
    who use the library, then it is likely that the project will stay alive even if
    the original team stops working on it or the original financing structure changes.
    However, these are only small factors and not well-established facts. There is
    no guarantee, and telling the future is more an art than a science.
  prefs: []
  type: TYPE_NORMAL
- en: Maturity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Maturity is similar to the previous criterion. A project may very well be alive
    just starting up, but if it is in its infancy, we better not use the library for
    a large project. When a project is in its early phase, a lot of bugs can be in
    the code, the API may change radically, and presumably, there may only be a small
    number of companies relying on the code. This also means that the community support
    is lower.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if all the projects select only mature open source code, then no
    open source project would ever get to the mature state. We should assess the importance
    of the project. Is the project business-critical? Will the project become business-critical?
  prefs: []
  type: TYPE_NORMAL
- en: If the project is not business-critical, the company may afford to invent a
    fresh library that is not that mature. It may be reasonable if there are no mature
    libraries for the purpose because the technology you are going to use is relatively
    new. In such a case, the project in the company is probably also new and not business-critical
    yet. It will be business-critical, we hope, after some time, but by that time,
    the library will be mature, or may just die and we can select a competing solution
    before the project becomes too expensive to switch.
  prefs: []
  type: TYPE_NORMAL
- en: Judging the maturity of a library is always difficult and has to be aligned
    with the maturity and importance of the project that we want to use the library
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Number of users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the library is alive and mature but there are not many users, then something
    is smelly. Why don't people use the library if it is good? If the number of users
    for a library or framework is low and there are no large corporations among the
    users, then it is probably not a good one. Nobody using it may signal that our
    assessment of the other criteria may not be appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that if there are only a few users of the library, then the knowledge
    in the community is also scarce and we may not be able to get community support.
  prefs: []
  type: TYPE_NORMAL
- en: The "I like it" factor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Last but not least, the *I like it* factor is extremely important. The question
    is not whether you like the library but rather how much the developers like it.
    Developers will like a library that is easy to use and fun to work with, and this
    will result in low cost. If the library is hard to use and developers do not like
    it, then they will not learn to use it to the level of profession required for
    good quality, only to the level that is just needed. The end result will be suboptimal
    software.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration and deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous integration means that whenever a new version is pushed to the source
    code repository, the continuous integration server kicks in, pulls the code to
    its disk, and starts the build. It compiles the code first, then runs the unit
    tests, fires the static code analysis tools, and, if all goes right, it packages
    a snapshot release and deploys it on a development server.
  prefs: []
  type: TYPE_NORMAL
- en: CI servers have web interfaces that can be used to create a release. In such
    a case, the deployment can even go to the test servers or even to production depending
    on local business needs and on the policy that was created accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automating the build and deployment process has the same advantages as any
    other automation: repeated tasks can be performed without manual intervention,
    which is tedious, boring, and, thus, error-prone if done by a human. The outstanding
    advantage is that if there is some error in the source code that can be discovered
    by the automated build process, it will be discovered. Novice developers say that
    it is cheaper and easier to build the code locally, which the developers do anyway,
    and then push the code to the server if the build process is already checked.
    It is partly true. Developers have to check that the code is of good quality and
    builds well, before sending it to the central repo. However, this cannot always
    be achieved. Some errors may not manifest on local environments.'
  prefs: []
  type: TYPE_NORMAL
- en: It may so happen that one developer accidentally uses a newer version of Java
    than the one supported and uses a new feature of the new version. Enterprises
    do not generally use the latest technology. They tend to use versions that are
    proven, have many users, and are mature. This year, in 2017, when Java 9 is going
    to be released in July, huge enterprises still use Java 1.6 and 1.7\. Since Java
    9 has many new features that are not trivial to implement, I expect that the adoption
    of the technology may take even longer than the adoption of Java 1.8, which gave
    us functional programming and lambda.
  prefs: []
  type: TYPE_NORMAL
- en: It may also happen that a new library is added to the dependencies of the build
    and the developer who added it to the build file (`pom.xml`, or `build.gradle`)
    could use it without any problem on her local machine. It does not mean that the
    library is officially added to the project, and it may not be available in the
    central code repository (Artifactory, Nexus, or other implementations of the code
    repository). The library may have only been on the local repository of the developer,
    and she may have assumed that since the code compiles, the build is OK.
  prefs: []
  type: TYPE_NORMAL
- en: Some large organizations use different code repositories for different projects.
    The libraries get into these repositories following meticulous examination and
    decisions. Some libraries may get there, while others may not. The reason to have
    different repositories could be numerous. Some project is developed for one customer
    who has a different policy about an open source project than the other. If the
    enterprise develops code for itself, it may so happen that some library is phased
    out or not supported anymore, and can only be used for projects that are old.
    A maintenance release may not need to replace a library, but new projects may
    be not be allowed to use a dying software library.
  prefs: []
  type: TYPE_NORMAL
- en: The CI server can run on a single machine or it can run on several machines.
    In case it serves many projects, it may be set up as a central server with many
    agents running on different machines. When some build process has to be started,
    the central server delegates this task to one of the agents. The agents may have
    different loads, running several different build processes, and may have different
    hardware configuration. The build process may have requirements regarding the
    speed of the processor or about the available memory. Some agent may run simpler
    builds for smaller projects but would fail to execute the build of a large project
    or of some small project that still has a huge memory requirement to execute some
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: When a build fails, the build server sends out e-mails to the developers, and
    the person who sent the last update to the code repository is obligated to fix
    the bug without delay. This encourages the developers to commit frequently. The
    smaller the change, the fewer chances there are of a build problem. The build
    server web interface can be used to see the actual state of the projects, which
    project is failing to build, and which is just fine. If a build fails, there is
    a red sign in the line of the build, and if the build is OK, the sign is green.
  prefs: []
  type: TYPE_NORMAL
- en: Many times, these reports are continually displayed on some old machine using
    a huge display so that every developer or just anybody who enters the room can
    see the actual state of the builds. There is even special hardware that you can
    buy that has red, yellow, and green lamps to follow the state of the build and
    ring a bell when the build fails.
  prefs: []
  type: TYPE_NORMAL
- en: Release management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing software means a continuously changing code base. Not every version
    of the software is supposed to be installed in production. Most of the versions
    are pushed to the repository on a branch half complete. Some versions are meant
    only for testing and a few are meant to be installed in production even if only
    some of those will finally get to production.
  prefs: []
  type: TYPE_NORMAL
- en: Almost all the time, the releases follow the semantic versioning that we discussed
    in an earlier section. The versions that are meant only to be tested usually have
    the `-SNAPSHOT` modifier at the end of the version number. For example, the `1.3.12-SNAPSHOT`
    version is the version that was once debugged, and is going to become the `1.3.12`
    version. The snapshot versions are not definite versions. They are the code as
    it is by then. Because a snapshot release never gets installed in production,
    it is not needed to reproduce a snapshot version for maintenance. Thus, the snapshot
    versions are not increased continually. Sometimes, they may be changed, but that
    is a rare exception.
  prefs: []
  type: TYPE_NORMAL
- en: It may so happen that we work on a bug fix, `1.3.12-SNAPSHOT`, and during the
    development, we change so much code that we decide that it has to be `1.4.0` when
    it is released, and we rename the snapshot as `1.4.0-SNAPSHOT`. This is a rare
    case. Many times, the release creation creates a `1.4.0` version from `1.3.12-SNAPSHOT`
    as the decision about the new release number is taken by the time the release
    is created.
  prefs: []
  type: TYPE_NORMAL
- en: When the release process is started, usually from the web interface of the CI
    server, the developer creating the release has to specify the release version.
    This is usually the same as the snapshot version without the `-SNAPSHOT` postfix.
    The build process not only creates the build in this case but also tags the source
    code repository version it was using and loads the packaged program (artifact)
    to the code repository. The tag can be used later to access the exact version
    of the source code that was used to create the release. If there is a bug in a
    specific version, then this version has to be checked out on a developer machine
    to reproduce the bug and find the root cause.
  prefs: []
  type: TYPE_NORMAL
- en: If the build of a release fails, it can be rolled back, or you better just skip
    that release number and note it as a failed release build. An existing release
    can never have two versions. The source code is the only one that is for that
    release and the generated code has to be exactly the one in any storage. Subsequent
    compilation of the same source may result in slightly different code, for example,
    if a different version of Java is used to create the latter one. Even in such
    a case, the one that was created by the build server in the first place is the
    version that belongs to the release. When a bug is reproduced and the code is
    recompiled from the exact same source, it is already a snapshot version. Multiple
    releases may be possible from the same source version, for example, compiled with
    Java versions from 1.5 to 1.8 and version 9 but a single release always belongs
    to the exact same source code.
  prefs: []
  type: TYPE_NORMAL
- en: If a release that was supposed to be a release version fails during QA checks,
    then a new release has to be created and the failed release has to be noted as
    such. The version that marketing uses to name the different versions should not
    have a relation to the technical version numbers we work with. Many times, it
    is, and it causes much headache. If you realize that the two are totally different
    things and one does not have to do anything with the other, life gets simpler.
    Look at the different versioning of the Windows operating system or Java. As marketing,
    Java used 1.0 then 1.1, but Java 1.2 was advertised as Java 2 and still the code
    contained 1.2 (which now seven major releases later also becomes 9 instead of
    1.9)
  prefs: []
  type: TYPE_NORMAL
- en: The last part of release management is that deployments should register the
    version numbers. The company has to know which release is installed on which server,
    and of which client.
  prefs: []
  type: TYPE_NORMAL
- en: Code repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code repository stores the libraries and helps manage the dependencies of the
    different libraries. In the old times, when Java projects used ANT as a build
    tool and without the later added Ivy dependency management, the libraries that
    were needed by a project were downloaded to the source code, usually to the `lib`
    library. If a library needed another library, then those were also downloaded
    and stored manually, and this continued until all the libraries that one of the
    already downloaded libraries needed were copied to the source code tree.
  prefs: []
  type: TYPE_NORMAL
- en: This was a lot of manual work and, also, the library code was stored in the
    source code repository in many copies. A compiled library is not source code and
    has nothing to do in the source code repository. Manual work that can be automated
    has to be automated. Not because developers are lazy (yes, we are and we have
    to be) but because manual work is error prone and, thus, expensive.
  prefs: []
  type: TYPE_NORMAL
- en: This was when Apache Ivy was invented and Maven, following ANT, already supported
    repository management built in. They all stored the libraries structured in directories
    and supported metadata that described the dependencies to other libraries. Lucky
    that Gradle did not invent its own code repository. Instead, it supports both
    Maven and Ivy repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Using the repository, the build tools automatically download the libraries that
    are needed. In case a library has a new version, then the developer only has to
    update the version of the needed library in the build configuration and all tasks,
    including downloading all the new versions of the other libraries that are needed
    by that version, are done automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Walking up the ladder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you have got a lot of information that will rocket your start
    as an enterprise Java developer. You have got a base knowledge that you can build
    on. There is a long way to become a professional Java developer. There is a lot
    of documentation to read, a lot of code to scan and understand, and also a lot
    of code to write till you can claim to be a professional Java developer. You may
    probably face many years of continuous education. The good thing is that even
    after that, you can continue your journey and you can educate yourself, as being
    a professional Java developer is rarely a job people retire from. No, no! Not
    because they die while at it! Rather, professional software developers gaining
    experience start to code less and less and support the development process in
    different ways, which leverages more of their experience. They can become business
    analysts, project managers, test engineers, subject-matter experts, architects,
    scrum masters, automation engineers, and so on. Is it a familiar list? Yes, these
    are the people you will work with as a developer. Many of them may have started
    as a developer themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the relative position of these roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a bit more detailed look into what these roles perform in enterprise
    development:'
  prefs: []
  type: TYPE_NORMAL
- en: Business analysts work with the client and create the documents, specifications,
    use cases, and user stories needed by the developers to develop the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project managers administer the projects and help the team in getting things
    done in cooperation with other teams, caring for all the project matters that
    developers cannot attend to or would unnecessarily burn their time that they should
    have devoted to coding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subject-matter experts are more advanced in knowing the business needs, so it
    is a bit rare for a developer to become one, but in case the industry you work
    in is technology oriented, it may not be incredible to become one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test engineers control the QA process and understand not only the test methodologies
    and requirements of testing but also the development process so that they can
    support bug fixes and not only identify them, which would be poor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architects work with BAs and design a high-level structure of the applications
    and code, and document it in a way that helps the developers to focus on the actual
    tasks they have to perform. Architects are also responsible for the solution to
    use technologies, solutions, and structures which fit the purpose, are future
    proof, affordable, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrum mates help the development team to follow the agile methodology and help
    the team in controlling the administration and resolving problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many ways to go as a software developer and I only listed some of
    the positions that you can find in an enterprise today. As technology develops,
    I can imagine that in 20 years from today, software developers will teach and
    curate artificial intelligence systems and that will be what we refer to as programming
    today. Who can tell?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going in this direction is a good choice. Being a Java developer and becoming
    a professional in it is a profession that will pay well in the coming 10 to 20
    years for sure and perhaps even later. At the same time, I personally find this
    technology fascinating and interesting, and after more than 10 years of Java programming
    and more than 35 years of programming, I still learn something new in it every
    day.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, you learned the basics of Java programming. From time to time,
    I also mentioned issues, suggested directions, and warned you about pitfalls that
    are not Java-specific. However, we also did the homework of learning the Java
    language, the infrastructure, the libraries, development tools, and networking
    in Java. You also learned the most modern approaches that came only with Java
    8 and 9, such as functional programming in Java, streams, and reactive programming.
    If you know all that I have written in this book, you can start working as a Java
    developer. What's next? Go, and find your treasure in programming and in Java!
  prefs: []
  type: TYPE_NORMAL
