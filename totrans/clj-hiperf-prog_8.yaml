- en: Chapter 8. Application Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The earliest computing devices were built to perform automatic computations
    and, as computers grew in power, they became increasingly popular because of how
    much and how fast they could compute. Even today, this essence lives on in our
    anticipation that computers can execute our business calculations faster than
    before by means of the applications we run on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to performance analysis and optimization at a smaller component level,
    as we saw in previous chapters, it takes a holistic approach to improve performance
    at the application level. The higher-level concerns, such as serving a certain
    number of users in a day, or handling an identified quantum of load through a
    multi-layered system, requires us to think about how the components fit together
    and how the load is designed to flow through it. In this chapter, we will discuss
    such high-level concerns. Like the previous chapter, by and large this chapter
    applies to applications written in any JVM language, but with a focus on Clojure.
    In this chapter, we will discuss general performance techniques that apply to
    all layers of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch and compute in advance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Staging and batching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little's law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most non-trivial applications depend a great deal on third-party libraries for
    various functionality, such as logging, serving web requests, connecting to databases,
    writing to message queues, and so on. Many of these libraries not only carry out
    parts of critical business functionality but also appear in the performance-sensitive
    areas of our code, impacting the overall performance. It is imperative that we
    choose libraries wisely (with respect to features versus performance trade off)
    after due performance analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The crucial factor in choosing libraries is not identifying which library to
    use, rather it is having a performance model of our applications and having the
    use cases benchmarked under representative load. Only benchmarks can tell us whether
    the performance is problematic or acceptable. If the performance is below expectation,
    a drill-down profiling can show us whether a third-party library is causing the
    performance issue. In [Chapter 6](ch06.html "Chapter 6. Measuring Performance"),
    *Measuring Performance* and [Chapter 7](ch07.html "Chapter 7. Performance Optimization"),
    *Performance Optimization* we discussed how to measure performance and identify
    bottlenecks. You can evaluate multiple libraries for performance-sensitive use
    cases and choose what suits.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries often improve (or occasionally lose) performance with new releases,
    so measurement and profiling (comparative, across versions) should be an ongoing
    practice for the development and maintenance lifecycle of our applications. Another
    factor to note is that libraries may show different performance characteristics
    based on the use case, load, and the benchmark. The devil is in the benchmark
    details. Be sure that your benchmarks are as close as possible to the representative
    scenario for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Making a choice via benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's take a brief look at a few general use cases where performance of third-party
    libraries are exposed via benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Web servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Web servers are typically subject to quite a bit of performance benchmarking
    due to their generic nature and scope. One such benchmark for Clojure web servers
    exists here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ptaoussanis/clojure-web-server-benchmarks](https://github.com/ptaoussanis/clojure-web-server-benchmarks)'
  prefs: []
  type: TYPE_NORMAL
- en: Web servers are complex pieces of software and they may exhibit different characteristics
    under various conditions. As you will notice, the performance numbers vary based
    on keep-alive versus non-keep-alive modes and request volume—at the time of writing,
    Immutant-2 came out better in keep-alive mode but fared poorly in the non-keep-alive
    benchmark. In production, people often front their application servers with reverse
    proxy servers, for example Nginx or HAProxy, which make keep-alive connections
    to application servers.
  prefs: []
  type: TYPE_NORMAL
- en: Web routing libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several web routing libraries for Clojure, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/juxt/bidi#comparison-with-other-routing-libraries](https://github.com/juxt/bidi#comparison-with-other-routing-libraries)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same document also shows a performance benchmark with **Compojure** as
    the baseline, in which (at the time of writing) Compojure turns out to be performing
    better than **Bidi**. However, another benchmark compares Compojure, **Clout**
    (the library that Compojure internally uses), and **CalfPath** routing here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kumarshantanu/calfpath#development](https://github.com/kumarshantanu/calfpath#development)'
  prefs: []
  type: TYPE_NORMAL
- en: In this benchmark, as of this writing, Clout performs better than Compojure,
    and CalfPath outperforms Clout. However, you should be aware of any caveats in
    the faster libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Data serialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several ways to serialize data in Clojure, for example EDN and Fressian.
    Nippy is another serialization library with benchmarks to demonstrate how well
    it performs over EDN and Fressian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ptaoussanis/nippy#performance](https://github.com/ptaoussanis/nippy#performance)'
  prefs: []
  type: TYPE_NORMAL
- en: We covered Nippy in [Chapter 2](ch02.html "Chapter 2. Clojure Abstractions"),
    *Clojure Abstractions* to show how it uses transients to speed up its internal
    computations. Even within Nippy, there are several flavors of serialization that
    have different features/performance trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: JSON serialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Parsing and generating JSON is a very common use case in RESTful services and
    web applications. The Clojure contrib library clojure/data.json ([https://github.com/clojure/data.json](https://github.com/clojure/data.json))
    provides this functionality. However, many people have found out that the Cheshire
    library [https://github.com/dakrone/cheshire](https://github.com/dakrone/cheshire)
    performs much better than the former. The included benchmarks in Cheshire can
    be run using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Cheshire internally uses the Jackson Java library [https://github.com/FasterXML/jackson](https://github.com/FasterXML/jackson),
    which is known for its good performance.
  prefs: []
  type: TYPE_NORMAL
- en: JDBC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'JDBC access is another very common use case among applications using relational
    databases. The Clojure contrib library `clojure/java.jdbc` [https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    provides a Clojure JDBC API. Asphalt [https://github.com/kumarshantanu/asphalt](https://github.com/kumarshantanu/asphalt)
    is an alternative JDBC library where the comparative benchmarks can be run as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As of this writing, Asphalt outperforms `clojure/java.jdbc` by several micro
    seconds, which may be useful in low-latency applications. However, note that JDBC
    performance is usually dominated by SQL queries/joins, database latency, connection
    pool parameters, and so on. We will discuss more about JDBC in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logging is a prevalent activity that almost all non-trivial applications do.
    Logging calls are quite frequent, hence it is important to make sure our logging
    configuration is tuned well for performance. If you are not familiar with logging
    systems (especially on the JVM), you may want to take some time to get familiar
    with those first. We will cover the use of `clojure/tools.logging`, **SLF4J**
    and **LogBack** libraries (as a combination) for logging, and look into how to
    make them perform well:'
  prefs: []
  type: TYPE_NORMAL
- en: Clojure/tools.logging [https://github.com/clojure/tools.logging](https://github.com/clojure/tools.logging)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SLF4J: [http://www.slf4j.org/](http://www.slf4j.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LogBack: [http://logback.qos.ch/](http://logback.qos.ch/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why SLF4J/LogBack?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides SLF4J/LogBack, there are several logging libraries to choose from in
    the Clojure application, for example Timbre, Log4j and java.util.logging. While
    there is nothing wrong with these libraries, we are often constrained into choosing
    something that covers most other third-party libraries (also including Java libraries)
    in our applications for logging purposes. SLF4J is a Java logger facade that detects
    any available implementation (LogBack, Log4j, and so on) —we choose LogBack simply
    because it performs well and is highly configurable. The library clojure/tools.logging
    provides a Clojure logging API that detects SLF4J, Log4j or java.util.logging
    (in that order) in the classpath and uses whichever implementation is found first.
  prefs: []
  type: TYPE_NORMAL
- en: The setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's walk through how to set up a logging system for your application using
    LogBack, SLF4J and `clojure/tools.logging` for a project built using Leiningen.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Your `project.clj` file should have the LogBack, SLF4J and `clojure/tools.logging`
    dependencies under the `:dependencies` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The previously mentioned versions are current and work as of the time of writing.
    You may want to use updated versions, if available.
  prefs: []
  type: TYPE_NORMAL
- en: The logback configuration file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You need to create a `logback.xml` file in the `resources` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The previous `logback.xml` file is simple on purpose (for illustration) and
    has just enough configuration to get you started with logging using LogBack.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The optimization points are highlighted in the `logback.xml` file we saw earlier
    in this section. We set the `immediateFlush` attribute to `false` such that the
    messages are buffered before flushing to the appender. We also wrapped the regular
    file appender with an asynchronous appender and edited the `queueSize` and `discardingThreshold`
    attributes, which gets us much better results than the default.
  prefs: []
  type: TYPE_NORMAL
- en: Unless optimized, logging configurations are usually a common source of suboptimal
    performance in many applications. Usually, the performance problems show up only
    at high load when the log volume is very high. The optimizations discussed previously
    are only a few of the many possible optimizations that one can experiment with.
    The chapters in LogBack documentation, such as **encoders** ([http://logback.qos.ch/manual/encoders.html](http://logback.qos.ch/manual/encoders.html)),
    **appenders** ([http://logback.qos.ch/manual/appenders.html](http://logback.qos.ch/manual/appenders.html))
    and **configuration** ([http://logback.qos.ch/manual/configuration.html](http://logback.qos.ch/manual/configuration.html))
    have useful **information**. There are also tips [http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/](http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/)
    on the Internet that may provide useful pointers.
  prefs: []
  type: TYPE_NORMAL
- en: Data sizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cost of abstractions in terms of the data size plays an important role.
    For example, whether or not a data element can fit into a processor cache line
    depends directly upon its size. On a Linux system, we can find out the cache line
    size and other parameters by inspecting the values in the files under the `/sys/devices/system/cpu/cpu0/cache/`
    directory. Refer to [Chapter 4](ch04.html "Chapter 4. Host Performance"), *Host
    Performance*, where we discussed how to compute the size of primitives, objects,
    and data elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another concern we generally find with data sizing is how much data we hold
    at any time in the heap. As we noted in earlier chapters, GC has direct consequences
    on the application performance. While processing data, often we do not really
    need all the data we hold on to. Consider the example of generating a summary
    report of sold items for a certain period (months) of time. After the subperiod
    (month-wise) summary data is computed, we do not need the item details anymore,
    hence it''s better to remove the unwanted data while we add the summaries. See
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Had we not used `select-keys` in the previous `summarize` function, it would
    have returned a map with extra :`summary` data along with all other existing keys
    in the map. Now, such a thing is often combined with lazy sequences, so for this
    scheme to work it is important not to hold onto the head of the lazy sequence.
    Recall that in [Chapter 2](ch02.html "Chapter 2. Clojure Abstractions"), *Clojure
    Abstractions* we discussed the perils of holding onto the head of a lazy sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Reduced serialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed in earlier chapters that serialization over an I/O channel is a
    common source of latency. The perils of over-serialization cannot be overstated.
    Whether we read or write data from a data source over an I/O channel, all of that
    data needs to be prepared, encoded, serialized, de-serialized, and parsed before
    being worked upon. The less data that is involved, the better it is for every
    step in order to lower the overhead. Where there is no I/O involved (such as in-process
    communication), it generally makes no sense to serialize.
  prefs: []
  type: TYPE_NORMAL
- en: A common example of over-serialization is when working with SQL databases. Often,
    there are common SQL query functions that fetch all columns of a table or a relation—they
    are called by various functions that implement business logic. Fetching data that
    we do not need is wasteful and detrimental to performance for the same reason
    that we discussed in the previous paragraph. While it may seem more work to write
    one SQL statement and one database-query function for each use case, it pays off
    with better performance. Code that uses NoSQL databases is also subject to this
    anti-pattern—we have to take care to fetch only what we need even though it may
    lead to additional code.
  prefs: []
  type: TYPE_NORMAL
- en: There's a pitfall to be aware of when reducing serialization. Often, some information
    needs to be inferred in the absence of serialized data. In such cases, where some
    of the serialization is dropped so that we can infer other information, we must
    compare the cost of inference versus the serialization overhead. The comparison
    may not necessarily be only per operation, but rather on the whole, such that
    we can consider the resources we can allocate in order to achieve capacities for
    various parts of our systems.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking to reduce memory pressure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What happens when we slurp a text file regardless of its size? The contents
    of the entire file will sit in the JVM heap. If the file is larger than the JVM
    heap capacity, the JVM will terminate, throwing `OutOfMemoryError`. If the file
    is large, but not enough to force the JVM into OOM error, it leaves relatively
    less JVM heap space for other operations to continue in the application. Similar
    situations take place when we carry out any operation disregarding the JVM heap
    capacity. Fortunately, this can be fixed by reading data in chunks and processing
    them before reading more. In [Chapter 3](ch03.html "Chapter 3. Leaning on Java"),
    *Leaning on Java*, we briefly discussed memory mapped buffers, which is another
    complementary solution that you may like to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Sizing for file/network operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's take the example of a data ingestion process where a semi-automated job
    uploads large **Comma Separated File (CSV)** files via **File Transfer Protocol
    (FTP)** to a file server, and another automated job (written in Clojure) runs
    periodically to detect the arrival of files via a Network File System (NFS). After
    detecting a new file, the Clojure program processes the file, updates the result
    in a database, and archives the file. The program detects and processes several
    files concurrently. The size of the CSV files is not known in advance, but the
    format is predefined.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the previous description, one potential problem is, since there could
    be multiple files being processed concurrently, how do we distribute the JVM heap
    among the concurrent file-processing jobs? Another issue at hand could be that
    the operating system imposes a limit on how many files could be open at a time;
    on Unix-like systems you can use the `ulimit` command to extend the limit. We
    cannot arbitrarily slurp the CSV file contents—we must limit each job to a certain
    amount of memory, and also limit the number of jobs that can run concurrently.
    At the same time, we cannot read a very small number of rows from a file at a
    time because this may impact performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, we can specify the buffer size when reading from a file (or even
    from a network stream) so as to tune the memory usage and performance as appropriate.
    In the previous code example, we explicitly set the buffer size of the reader
    to facilitate the same.
  prefs: []
  type: TYPE_NORMAL
- en: Sizing for JDBC query results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Java''s interface standard for SQL databases, JDBC (which is technically not
    an acronym), supports *fetch size* for fetching query results via JDBC drivers.
    The default fetch size depends on the JDBC driver. Most of the JDBC drivers keep
    a low default value to avoid high memory usage and for internal performance optimization
    reasons. A notable exception to this norm is the MySQL JDBC driver that completely
    fetches and stores all rows in memory by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When using the Clojure contrib library `java.jdbc` ([https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    as of version 0.3.7), the fetch size can be set while preparing a statement as
    shown in the previous example. Note that the fetch size does not guarantee proportional
    latency; however, it can be used safely for memory sizing. We must test any performance-impacting
    latency changes due to fetch size at different loads and use cases for the particular
    database and JDBC driver. Another important factor to note is that the benefit
    of `:fetch-size` can be useful only if the query result set is consumed incrementally
    and lazily—if a function extracts all rows from a result set to create a vector,
    then the benefit of `:fetch-size` is nullified from a memory conservation point
    of view. Besides fetch size, we can also pass the `:max-rows` argument to limit
    the maximum rows to be returned by a query—however, this implies that the extra
    rows will be truncated from the result, and not whether the database will internally
    limit the number of rows to realize.
  prefs: []
  type: TYPE_NORMAL
- en: Resource pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several types of resources on the JVM that are rather expensive to
    initialize. Examples are HTTP connections, execution threads, JDBC connections,
    and so on. The Java API recognizes such resources and has built-in support for
    creating a pool of some of those resources, such that the consumer code borrows
    a resource from a pool when required and at the end of the job simply returns
    it to the pool. Java's thread pools (discussed in [Chapter 5](ch05.html "Chapter 5. Concurrency"),
    *Concurrency*) and JDBC data sources are prominent examples. The idea is to preserve
    the initialized objects for reuse. Even though Java does not support pooling of
    a resource type directly, one can always create a pool abstraction around custom
    expensive resources. Note that the pooling technique is common in I/O activities,
    but can be equally applicable to non-I/O purposes where initialization cost is
    high.
  prefs: []
  type: TYPE_NORMAL
- en: JDBC resource pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Java supports the obtaining of JDBC connections via the `javax.sql.DataSource`
    interface, which can be pooled. A JDBC connection pool implements this interface.
    Typically, a JDBC connection pool is implemented by third-party libraries or a
    JDBC driver itself. Generally, very few JDBC drivers implement a connection pool,
    so Open Source third-party JDBC resource pooling libraries such as Apache DBCP,
    c3p0, BoneCP, HikariCP, and so on are popular. They also support validation queries
    for eviction of stale connections that might result from network timeouts and
    firewalls, and guard against connection leaks. Apache DBCP and HikariCP are accessible
    from Clojure via their respective Clojure wrapper libraries Clj-DBCP ([https://github.com/kumarshantanu/clj-dbcp](https://github.com/kumarshantanu/clj-dbcp))
    and HikariCP ([https://github.com/tomekw/hikari-cp](https://github.com/tomekw/hikari-cp)),
    and there are Clojure examples describing how to construct C3P0 and BoneCP pools
    ([http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html](http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Connections are not the only JDBC resources that need to be pooled. Every time
    we create a new JDBC prepared statement, depending on the JDBC driver implementation,
    often the entire statement template is sent to the database server in order to
    obtain a reference to the prepared statement. As the database servers are generally
    deployed on separate hardware, there may be network latency involved. Hence, the
    pooling of prepared statements is a very desirable property of JDBC resource pooling
    libraries. Apache DBCP, C3P0, and BoneCP all support statement pooling, and the
    Clj-DBCP wrapper enables the pooling of prepared statements out-of-the-box for
    better performance. HikariCP has the opinion that statement pooling, nowadays,
    is already done internally by JDBC drivers, hence explicit pooling is not required.
    I would strongly advise running your benchmarks with the connection pooling libraries
    to determine whether or not it really works for your JDBC driver and application.
  prefs: []
  type: TYPE_NORMAL
- en: I/O batching and throttling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is well known that chatty I/O calls generally lead to poor performance. In
    general, the solution is to batch together several messages and send them in one
    payload. In databases and network calls, batching is a common and useful technique
    to improve throughput. On the other hand, large batch sizes may actually harm
    throughput as they tend to incur memory overhead, and components may not be ready
    to handle a large batch at once. Hence, sizing the batches and throttling are
    just as important as batching. I would strongly advise conducting your own tests
    to determine the optimum batch size under representative load.
  prefs: []
  type: TYPE_NORMAL
- en: JDBC batch operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JDBC has long had batch-update support in its API, which includes the `INSERT`,
    `UPDATE`, `DELETE` statements. The Clojure contrib library `java.jdbc` supports
    JDBC batch operations via its own API, as we can see as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Besides batch-update support, we can also batch JDBC queries. One of the common
    techniques is to use the SQL `WHERE` clause to avoid the `N+1` selects issue.
    The `N+1` issue indicates the situation when we execute one query in another child
    table for every row in a rowset from a master table. A similar technique can be
    used to combine several similar queries on the same table into just one, and segregate
    the data in the program afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example that uses clojure.java.jdbc 0.3.7 and the MySQL
    database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous example there are two tables: `orders` and `items`. The first
    snippet reads all order IDs from the `orders` table, and then iterates through
    them to query corresponding entries in the `items` table in a loop. This is the
    `N+1` selects performance anti-pattern you should keep an eye on. The second snippet
    avoids `N+1` selects by issuing a single SQL query, but may not perform very well
    unless the column `fk_order_id` is indexed.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch support at API level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing any service, it is very useful to provide an API for batch operations.
    This builds flexibility in the API such that batch sizing and throttling can be
    controlled in a fine-grained manner. Not surprisingly, it is also an effective
    recipe for building high-performance services. A common overhead we encounter
    when implementing batch operations is the identification of each item in the batch
    and their correlation across requests and responses. The problem becomes more
    prominent when requests are asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to the item identification issue is resolved either by assigning
    a canonical or global ID to each item in the request (batch), or by assigning
    every request (batch) a unique ID and each item in the request an ID that is local
    to the batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the exact solution usually depends on the implementation details.
    When requests are synchronous, you can do away with identification of each request
    item (see the Facebook API for reference: [http://developers.facebook.com/docs/reference/api/batch/](http://developers.facebook.com/docs/reference/api/batch/))
    where the items in response follow the same order as in the request. However,
    in asynchronous requests, items may have to be tracked via status-check call or
    callbacks. The desired tracking granularity typically guides the appropriate item
    identification strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have a batch API for order processing, every order would
    have a unique Order-ID that can be used in subsequent status-check calls. In another
    example, let's say there is a batch API for creating API keys for **Internet of
    Things** (**IoT**) devices—here, the API keys are not known beforehand, but they
    can be generated and returned in a synchronous response. However, if this has
    to be an asynchronous batch API, the service should respond with a batch request
    ID that can be used later to find the status of the request. In a batch response
    for the request ID, the server can include request item IDs (for example device
    IDs, which may be unique for the client but not unique across all clients) with
    their respective status.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling requests to services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As every service can handle only a certain capacity, the rate at which we send
    requests to a service is important. The expectations about the service behavior
    are generally in terms of both throughput and latency. This requires us to send
    requests at a specified rate, as a rate lower than that may lead to under-utilization
    of the service, and a higher rate may overload the service or result in failure,
    thus leading to client-side under-utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say a third-party service can accept 100 requests per second. However,
    we may not know how robustly the service is implemented. Though sometimes it is
    not exactly specified, sending 100 requests at once (within 20ms, let's say) during
    each second may lead to lower throughput than expected. Evenly distributing the
    requests across the one-second duration, for example sending one request every
    10ms (1000ms / 100 = 10ms), may increase the chance of attaining the optimum throughput.
  prefs: []
  type: TYPE_NORMAL
- en: For throttling, **Token bucket** ([https://en.wikipedia.org/wiki/Token_bucket](https://en.wikipedia.org/wiki/Token_bucket))
    and **Leaky bucket** ([https://en.wikipedia.org/wiki/Leaky_bucket](https://en.wikipedia.org/wiki/Leaky_bucket))
    algorithms can be useful. Throttling at a very fine-grained level requires that
    we buffer the items so that we can maintain a uniform rate. Buffering consumes
    memory and often requires ordering; queues (covered in [Chapter 5](ch05.html "Chapter 5. Concurrency"),
    *Concurrency*), pipeline and persistent storage usually serve that purpose well.
    Again, buffering and queuing may be subject to back pressure due to system constraints.
    We will discuss pipelines, back pressure and buffering in a later section in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Precomputing and caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While processing data, we usually come across instances where few common computation
    steps precede several kinds of subsequent steps. That is to say, some amount of
    computation is common and the remaining is different. For high-latency common
    computations (I/O to access the data and memory/CPU to process it), it makes a
    lot of sense to compute them once and store in digest form, such that the subsequent
    steps can simply use the digest data and proceed from that point onward, thus
    resulting in reduced overall latency. This is also known as staging of semi-computed
    data and is a common technique to optimize processing of non-trivial data.
  prefs: []
  type: TYPE_NORMAL
- en: Clojure has decent support for caching. The built-in `clojure.core/memoize`
    function performs basic caching of computed results with no flexibility in using
    specific caching strategies and pluggable backends. The Clojure contrib library
    `core.memoize` offsets the lack of flexibility in `memoize` by providing several
    configuration options. Interestingly, the features in `core.memoize` are also
    useful as a separate caching library, so the common portion is factored out as
    a Clojure contrib library called `core.cache` on top of which `core.memoize` is
    implemented.
  prefs: []
  type: TYPE_NORMAL
- en: As many applications are deployed on multiple servers for availability, scaling
    and maintenance reasons, they need distributed caching that is fast and space
    efficient. The open source memcached project is a popular in-memory, distributed
    key-value/object store that can act as a caching server for web applications.
    It hashes the keys to identify the server to store the value on, and has no out-of-the-box
    replication or persistence. It is used to cache database query results, computation
    results, and so on. For Clojure, there is a memcached client library called SpyGlass
    ([https://github.com/clojurewerkz/spyglass](https://github.com/clojurewerkz/spyglass)).
    Of course, memcached is not limited to just web applications; it can be used for
    other purposes too.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a situation where we have to carry out jobs at a certain throughput,
    such that each job includes the same sequence of differently sized I/O task (task
    A), a memory-bound task (task B) and, again, an I/O task (task C). A naïve approach
    would be to create a thread pool and run each job off it, but soon we realize
    that this is not optimum because we cannot ascertain the utilization of each I/O
    resource due to unpredictability of the threads being scheduled by the OS. We
    also observe that even though several concurrent jobs have similar I/O tasks,
    we are unable to batch them in our first approach.
  prefs: []
  type: TYPE_NORMAL
- en: As the next iteration, we split each job in stages (A, B, C), such that each
    stage corresponds to one task. Since the tasks are well known, we create one thread
    pool (of appropriate size) per stage and execute tasks in them. The result of
    task A is required by task B, and B's result is required by task C—we enable this
    communication via queues. Now, we can tune the thread pool size for each stage,
    batch the I/O tasks, and throttle them for an optimum throughput. This kind of
    an arrangement is a concurrent pipeline. Some readers may find this feebly resembling
    the actor model or **Staged Event Driven Architecture** (**SEDA**) model, which
    are more refined models for this kind of approach. Recall that we discussed several
    kinds of in-process queues in [Chapter 5](ch05.html "Chapter 5. Concurrency"),
    *Concurrency*.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With this approach, it is possible to scale out the job execution to multiple
    hosts in a cluster using network queues, thereby offloading memory consumption,
    durability, and delivery to the queue infrastructure. For example, in a given
    scenario there could be several nodes in a cluster, all of them running the same
    code and exchanging messages (requests and intermediate result data) via network
    queues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts how a simple invoice-generation system might
    be connected to network queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed pipelines](img/3642_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: RabbitMQ, HornetQ, ActiveMQ, Kestrel and Kafka are some well-known Open Source
    queue systems. Once in a while, the jobs may require distributed state and coordination.
    The Avout ([http://avout.io/](http://avout.io/)) project implements the distributed
    version of Clojure's atom and ref, which can be used for this purpose. Tesser
    ([https://github.com/aphyr/tesser](https://github.com/aphyr/tesser)) is another
    library for local and distributed parallelism using Clojure. The Storm ([http://storm-project.net/](http://storm-project.net/))
    and Onyx ([http://www.onyxplatform.org/](http://www.onyxplatform.org/)) projects
    are distributed, real-time stream processing systems implemented using Clojure.
  prefs: []
  type: TYPE_NORMAL
- en: Applying back pressure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed back pressure briefly in the last chapter. Without back pressure
    we cannot build a reasonable load-tolerant system with predictable stability and
    performance. In this section, we will see how to apply back pressure in different
    scenarios in an application. At a fundamental level, we should have a threshold
    of a maximum number of concurrent jobs in the system and, based on that threshold,
    we should reject new requests above a certain arrival rate. The rejected messages
    may either be retried by the client or ignored if there is no control over the
    client. When applying back pressure to user-facing services, it may be useful
    to detect system load and deny auxiliary services first in order to conserve capacity
    and degrade gracefully in the face of high load.
  prefs: []
  type: TYPE_NORMAL
- en: Thread pool queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JVM thread pools are backed by queues, which means that when we submit a job
    into a thread pool that already has the maximum jobs running, the new job lands
    in the queue. The queue is by default an unbounded queue, which is not suitable
    for applying back pressure. So, we have to create the thread pool backed by a
    bounded queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, on this pool, whenever there is an attempt to add more jobs than the capacity
    of the queue, it will throw an exception. The caller should treat the exception
    as a buffer-full condition and wait until the buffer has idle capacity again by
    periodically pooling the `java.util.concurrent.BlockingQueue.remainingCapacity()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Servlet containers such as Tomcat and Jetty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the synchronous **Tomcat** and **Jetty** versions, each HTTP request is given
    a dedicated thread from a common thread pool that a user can configure. The number
    of simultaneous requests being served is limited by the thread pool size. A common
    way to control the arrival rate is to set the thread pool size of the server.
    The **Ring** library uses an embedded jetty server by default in development mode.
    The embedded Jetty adapter (in Ring) can be programmatically configured with a
    thread pool size.
  prefs: []
  type: TYPE_NORMAL
- en: In the asynchronous (Async Servlet 3.0) versions of Tomcat and Jetty beside
    the thread pool size, it is also possible to specify the timeout for processing
    each request. However, note that the thread pool size does not limit the number
    of requests in asynchronous versions in the way it does on synchronous versions.
    The request processing is transferred to an ExecutorService (thread pool), which
    may buffer requests until a thread is available. This buffering behavior is tricky
    because this may cause system overload—you can override the default behavior by
    defining your own thread pool instead of using the servlet container's thread
    pool to return a HTTP error at a certain threshold of waiting requests.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP Kit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**HTTP Kit** ([http://http-kit.org/](http://http-kit.org/)) is a high-performance
    asynchronous (based on Java NIO implementation) web server for Clojure. It has
    built-in support for applying back pressure to new requests via a specified queue
    length. As of HTTP Kit 2.1.19, see the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the previous snippet, the worker thread pool size is 32 and the max queue
    length is specified as 600\. When not specified, 20480 is the default maximum
    queue length for applying back pressure.
  prefs: []
  type: TYPE_NORMAL
- en: Aleph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aleph ([http://aleph.io/](http://aleph.io/)) is another high-performance asynchronous
    web server based on the Java Netty ([http://netty.io/](http://netty.io/)) library,
    which in turn is based on Java NIO. Aleph extends Netty with its own primitives
    compatible with Netty. The worker thread pool in Aleph is specified via an option,
    as we can see in the following snippet as of Aleph 0.4.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, `tpool` refers to a bounded thread pool as discussed in the subsection
    *Thread pool queues*. By default, Aleph uses a dynamic thread pool capped at maximum
    512 threads aimed at 90 percent system utilization via the **Dirigiste** ([https://github.com/ztellman/dirigiste](https://github.com/ztellman/dirigiste))
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Back pressure not only involves enqueuing a limited number of jobs, but slows
    down the processing rate of a job when the peer is slow. Aleph deals with per-request
    back pressure (for example, when streaming response data) by "not accepting data
    until it runs out of memory" — it falls back to blocking instead of dropping data,
    or raising exceptions and closing connections
  prefs: []
  type: TYPE_NORMAL
- en: Performance and queueing theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we observe the performance benchmark numbers across a number of runs, even
    though the hardware, loads and OS remain the same, the numbers are rarely exactly
    the same. The difference between each run may be as much as -8 percent to 8 percent
    for no apparent reason. This may seem surprising, but the deep-rooted reason is
    that the performances of computer systems are *stochastic* by nature. There are
    many small factors in a computer system that make performance unpredictable at
    any given point of time. At best, the performance variations can be explained
    by a series of probabilities over random variables.
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise is that each subsystem is more or less like a queue where
    requests await their turn to be served. The CPU has an instruction queue with
    unpredictable fetch/decode/branch-predict timings, the memory access again depends
    on cache hit ratio and whether it needs to be dispatched via the interconnect,
    and the I/O subsystem works using interrupts that may again depend on mechanical
    factors of the I/O device. The OS schedules threads that wait while not executing.
    The software built on the top of all this basically waits in various queues to
    get the job done.
  prefs: []
  type: TYPE_NORMAL
- en: Little's law
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Little''s law states that, over steady state, the following holds true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Little''s law](img/3642_08_01.jpg)![Little''s law](img/3642_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a rather important law that gives us insight into the system capacity
    as it is independent of other factors. For an example, if the average time to
    satisfy a request is 200 ms and the service rate is about 70 per second, then
    the mean number of requests being served is *70 req/second x 0.2 second = 14 requests*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Little's law does not talk about spikes in request arrival rate or
    spikes in latency (due to GC and/or other bottlenecks) or system behavior in response
    to these factors. When the arrival rate spikes at one point, your system must
    have enough resources to handle the number of concurrent tasks required to serve
    the requests. We can infer here that Little's law is helpful to measure and tune
    average system behavior over a duration, but we cannot plan capacity based solely
    on this.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning with respect to Little's law
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to maintain good throughput, we should strive to maintain an upper
    limit on the total number of tasks in the system. Since there can be many kinds
    of tasks in a system and lot of tasks can happily co-exist in the absence of bottlenecks,
    a better way to say it is to ensure that the system utilization and bottlenecks
    remain in limit.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the arrival rate may not be within the control of a system. For such
    scenarios, the only option is to minimize the latency as much as possible and
    deny new requests after a certain threshold of total jobs in the system. You may
    be able to know the right threshold only through performance and load tests. If
    you can control the arrival rate, you can throttle the arrival (based on performance
    and load tests) so as to maintain a steady flow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing an application for performance should be based on the use cases and
    patterns of anticipated system load and behavior. Measuring performance is extremely
    important to guide optimization in the process. Fortunately, there are several
    well-known optimization patterns to tap into, such as resource pooling, data sizing,
    pre-fetch and pre-compute, staging, batching, and so on. As it turns out, application
    performance is not only a function of the use cases and patterns—the system as
    a whole is a continuous stochastic turn of events that can be assessed statistically
    and is guided by probability. Clojure is a fun language to do high-performance
    programming. This book prescribes many pointers and practices for performance,
    but there is no mantra that can solve everything. The devil is in the details.
    Know the idioms and patterns, experiment to see what works for your applications,
    and know which rules you can bend for performance.
  prefs: []
  type: TYPE_NORMAL
