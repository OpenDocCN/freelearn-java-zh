["```java\nprivate static final String EVENT_SCHEMA = \"event_time TIMESTAMP,event_type STRING,product_id LONG,category_id LONG,category_code STRING,brand STRING,price DOUBLE,user_id LONG,user_session STRING\";\n. . .\nDataset<Row> ecommerceEventDf = spark.read().option(\"header\",\"true\").schema(EVENT_SCHEMA).csv(inputPath);\n```", "```java\nDataset<Row> countAggDf = spark.sql(\"select year(event_time) as year,month(event_time) as month,category_id,product_id,count_if(event_type='purchase') as tot_sales,count_if(event_type='view') as tot_onlyview from ecommerceEventDf where event_type!='cart' group by year,month,category_id,product_id \");\n```", "```java\nDataset<Row> revenueAggDf = spark.sql(\"select year(event_time) as year,month(event_time) as month,category_id,product_id,sum(price) as sales_rev from ecommerceEventDf where event_type='purchase' group by year,month,category_id,product_id\");\n```", "```java\nDataset<Row> combinedAggDf = spark.sql(\"select cadf.year,cadf.month,cadf.category_id,cadf.category_id,cadf.product_id,tot_sales,tot_onlyview,sales_rev from countAggDf cadf LEFT OUTER JOIN revenueAggDf radf ON cadf.year==radf.year AND cadf.month== radf.month AND cadf.category_id== radf.category_id AND cadf.product_id == radf.product_id\");\nDataset<Row> combinedEnrichAggDf = combinedAggDf.na().fill(0.0,new String[]{\"sales_rev\"});\n```", "```java\nDataset<Row> finalTransformedDf = spark.sql(\"select year,month,category_id,product_id,tot_sales,tot_onlyview,sales_rev,dense_rank() over (PARTITION BY category_id ORDER BY sales_rev DESC) as rank_by_revenue,dense_rank() over (PARTITION BY category_id ORDER BY tot_sales DESC) as rank_by_sales from combinedEnrichAggDf\");\n```", "```java\nfinalTransformedDf.write().mode(SaveMode.Append).partitionBy(\"year\",\"month\").parquet(outputDirectory);\n```", "```java\nimport com.amazonaws.services.lambda.runtime.RequestHandler;\nimport com.amazonaws.services.lambda.runtime.events.S3Event;\n. . .\npublic class Handler implements RequestHandler<S3Event, Integer> {\n. . .\n```", "```java\nSet<String> s3ObjectNames = new HashSet<>();\nfor (S3EventNotificationRecord record:\n        s3Event.getRecords()) {\n    String s3Key = record.getS3().getObject().getKey();\n    String s3Bucket = record.getS3().getBucket().getName();\n    s3ObjectNames.add(\"s3://\"+s3Bucket+\"/\"+s3Key);\n}\ns3ObjectNames.forEach(inputS3path ->{\n    createClusterAndRunJob(inputS3path,logger);\n});\n```", "```java\nprivate void createClusterAndRunJob(String inputS3path, LambdaLogger logger) {\n    //Create a EMR object using AWS SDK\n    AmazonElasticMapReduce emr = AmazonElasticMapReduceClientBuilder.standard()\n            .withRegion(\"us-east-2\")\n            .build();\n\n    // create a step to submit spark Job in the EMR cluster to be used by runJobflow request object\n    StepFactory stepFactory = new StepFactory();\n    HadoopJarStepConfig sparkStepConf = new HadoopJarStepConfig()\n            .withJar(\"command-runner.jar\")\n            .withArgs(\"spark-submit\",\"--deploy-mode\",\"cluster\",\"--class\",\"com.scalabledataarchitecture.bigdata.EcomAnalysisDriver\",\"s3://jarandconfigs/EcommerceAnalysis-1.0-SNAPSHOT.jar\",inputS3path,\"s3://scalabledataarch/output\");\n    StepConfig sparksubmitStep = new StepConfig()\n            .withName(\"Spark Step\")\n            .withActionOnFailure(\"TERMINATE_CLUSTER\")\n            .withHadoopJarStep(sparkStepConf);\n    //Create an application object to be used by runJobflow request object\n    Application spark = new Application().withName(\"Spark\");\n    //Create a runjobflow request object\n    RunJobFlowRequest request = new RunJobFlowRequest()\n            .withName(\"chap5_test_auto\")\n            .withReleaseLabel(\"emr-6.4.0\")\n            .withSteps(sparksubmitStep)\n            .withApplications(spark)\n            .withLogUri(...)\n            .withServiceRole(\"EMR_DefaultRole\")\n            ...            ;\n    //Create and run a new cluster using runJobFlow method\n    RunJobFlowResult result = emr.runJobFlow(request);\n    logger.log(\"The cluster ID is \" + result.toString());\n}\n```", "```java\necommerceEventDf.persist(StorageLevel.MEMORY_AND_DISK());\n```", "```java\n    CREATE EXTERNAL TABLE IF NOT EXISTS ecom_odl(\n         category_id bigint,\n         product_id bigint,\n         tot_sales bigint,\n         tot_onlyview bigint,\n         sales_rev double,\n         rank_by_revenue int,\n         rank_by_sales int\n    ) PARTITIONED BY (year int, month int) STORED AS parquet LOCATION 's3://scalabledataarch/output/';\n    ```", "```java\n    MSCK REPAIR TABLE ecom_odl\n    ```"]