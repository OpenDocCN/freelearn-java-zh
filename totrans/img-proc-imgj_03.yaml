- en: Chapter 3. Advanced Image Processing with ImageJ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter showed you how to load and view images in ImageJ and how
    to make basic alterations to image intensity and pixel values. This chapter will
    deal with the techniques used to preprocess images. We will prepare them for image
    analysis and measurements. This chapter will apply some of the techniques we examined
    in the earlier chapters. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Correcting images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z-stack processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image and stack calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correcting images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to analyze images, we sometimes need to correct the problems that were
    present during acquisition. Problems such as noise, uneven illumination, and background
    fluorescence can cause many issues during image analysis. I will provide a little
    technical background on the sources of these problems and then follow this up
    with how they can be corrected in ImageJ.
  prefs: []
  type: TYPE_NORMAL
- en: Technical background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Of the many sources of noise that exist in imaging, a few can be corrected
    with correct acquisition settings. Others are inherent in the electronics and
    physical properties of the camera, and cannot be easily fixed. I will first deal
    with the source of noise that can be remedied with optimizing acquisition: **Shot**
    or **Poisson** noise. Next, we will look at **Electronic** or **Dark** noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Correcting Shot noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Shot noise is caused by the physical properties of light; light can be seen
    as packages of light or photons. The number of photons that are collected by each
    photo-detector site on the camera determines the final pixel intensity. If only
    a few photons hit the detector at any time, the differences in the number of photons
    could be large. This is called a Poisson process, and the signal-to-noise ratio
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correcting Shot noise](img/Insert_image_4909_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that the **signal-to-noise ratio** (**SNR**) will get larger as the
    number of photons (*N*) increases. By increasing the exposure time or the illumination
    intensity, the number of photons per pixel and the SNR will increase. A low SNR
    cannot be fixed with processing techniques in software.
  prefs: []
  type: TYPE_NORMAL
- en: Correcting dark noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another source of noise is called dark noise or dark current. This source of
    noise comes from the electronics in the camera and can be visualized by taking
    an image with the camera without illumination. In digital consumer cameras, exposing
    the image while the lens is completely covered can easily do this. You can even
    try it with the camera of your phone. Just cover the lens tightly and take a picture
    (make sure the flash is disabled!). As an example, the following figure shows
    a small region of an image taken by two different cameras, both with the same
    settings. The left-hand image is a small region of an image taken with a Sony
    α6000 (2014), while the image on the right-hand side is from a Canon EOS 550D
    (2010). The orange bar is there to delineate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correcting dark noise](img/Insert_image_4909_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The intensities of each image were equalized to show the pattern, and only
    the green channel is shown in this example. The settings for both cameras were
    as follows: 1/10sec exposure, ƒ5.6, and ISO 200\. It is clear from these images
    that the noise level from the electronics is quite different between the two camera
    sensors. Note that most scientific cameras, especially cooled **EM-CCD** (**Electron-Multiplying
    CCD**) cameras, have far lower levels of electronic noise. This allows some EM-CCD
    cameras to detect single photons and even count them.'
  prefs: []
  type: TYPE_NORMAL
- en: In order for the subtraction of the dark noise signal to work, the exposure
    duration needs to be identical to the exposure time during the acquisition to
    get the same level of dark noise. The duration of the exposure is directly linked
    to the amount of noise. A longer exposure results in more dark noise. This type
    of noise can be easily fixed in ImageJ using the image calculator that will be
    introduced a little later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the noise level of your own camera, take a picture with the lens
    covered (make sure it is completely blocked from all light). Ideally, you should
    do this with your camera capturing images as RAW files. When a camera acquires
    images as JPEG files, the camera already performs some noise reduction on the
    image. If you can only capture images in JPEG, check to see whether there is an
    option to switch off the noise reduction. Now, open the image in ImageJ, as was
    illustrated in the previous chapter, and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select your darkshot image window by clicking on it, making it active. In ImageJ,
    most commands will operate on the active image, or the last opened image. By clicking
    on an image window, that image becomes the active image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to determine the noise level, we can select an area that we want to
    measure. We will create a rectangle by specifying it by entering the specific
    values. To do this, go to **Edit** | **Selection** | **Specify…** and select the
    **Centered** checkbox, before entering `512` for the width and height. For the
    **X coordinate** and **Y coordinate**, enter half the width and height of your
    image (indicated in the image subtitle) and click on **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that the measurements are set to standard deviation. This can be done
    by going to **Analyze** | **Set Measurements** and selecting the **Standard deviation**
    checkbox. Selecting other parameters for measurement is fine, and in the output
    below **Area**, **Mean gray value** and **Min & max gray value** were also selected.
    For this exercise, the **Standard deviation** option is the only relevant parameter
    that is required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the measurement by pressing *Ctrl* + *M* or by going to **Analyze**
    | **Measure**. You can measure regions immediately after placing them, or you
    can add them to the **ROI Manager** (see the next chapter for more details) before
    measuring them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The results should now be visible in a new window labeled **Results**. Depending
    on which parameters you selected, the results in this window might deviate from
    the one shown here (I have included area and the minimum and maximum values):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correcting dark noise](img/Insert_image_4909_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first line contains the results from the α6000 camera, and the second line
    contains the results from the EOS 550D camera. The area is identical (*512 x 512
    = 262144 pixels*) for both measurements, but the standard deviation (a measure
    of the noise) is lower for the first camera by a factor of 6.3\. Also, the mean
    of the first camera is closer to 0, as you would expect the value to be when there
    is no light hitting the sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cameras can have pixels that no longer work (**dead pixels**). Dead pixels will
    show as black pixels in bright areas and always occur at the same location. The
    opposite can also happen. Very bright pixels in dark areas are called **hot pixels**.
    Hot pixels do not have to occur at the same location every time and are more common
    with very long exposure times. For EMCCD cameras, there is another source of bright
    pixels, which is caused by cosmic rays hitting the image sensor. These events
    are relatively common in long time series, and present themselves as very bright
    regions for only a single frame. The removal of dead and hot pixels follows the
    same methodology as the dark noise removal.
  prefs: []
  type: TYPE_NORMAL
- en: For most type of exposures, these levels of noise are so small that they do
    not cause degradation of your pictures. A picture at the beach with the sun in
    the sky will not require correction. With the enormous amount of light that is
    detected, the electronic noise is drowned out completely. However, one field of
    image acquisition where dark noise is a substantial factor is in the field of
    astrophotography or night-time photography. Whenever long exposures are required
    for image acquisition, the electronic noise becomes a substantial factor that
    can degrade your image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the effects of sensor noise in low-light conditions, you need to
    change the way you acquire your image slightly: instead of a single exposure,
    you need multiple exposures in quick succession. Some cameras support this automatically,
    using names such as handheld twilight (Sony) or multi-frame noise reduction (Pentax,
    Olympus etc.). In this mode you take 2 or more pictures in rapid succession and
    the final image is an average of the series of images. You can also do something
    like this in ImageJ by using the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the multiple images that you acquired in succession (make sure there are
    no other images opened!)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Image** | **Stacks** | **Images to Stack** from the menu. You will
    now have a single window where every slice represents one image that you took.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the noise-reduction image by selecting **Image** | **Stacks** | **Z Project…**
    from the menu and use Average Intensity as the projection type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A thing to keep in mind is the following: when anything moves between the individual
    exposures, this method will not provide good results. It is possible to correct
    for simple shifts, but this only works in the simplest of cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Uneven illumination – background subtraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an image is acquired under difficult lighting conditions, it can sometimes
    occur that the illumination is not even across the image sensor. This effect of
    uneven illumination is something that can easily be corrected in ImageJ. To show
    how this is done, we will take an image acquired with brightfield illumination
    on an inverted microscope using **Differential Interference Contrast** (**DIC**)
    optics.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DIC images provide contrast by looking at the difference in thickness of your
    specimen. A single light wave is split into two separate rays that are slightly
    separated but parallel and with the same phase. When one ray goes through an object
    with higher density than the parallel ray, the waves will shift out of phase.
    When they are recombined, the out-of-phase rays will partially cancel each other
    out (interference). This results in less light on the camera pixel, making the
    pixel darker. For cells, the strongest interference can be found close to the
    membrane of the cell. One ray will pass through the cell, while the parallel ray
    will pass through the water outside the cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image shows the effect of uneven illumination. The left-hand side of the
    frame is darker than the middle, and the gradient runs along the frame in a slightly
    diagonal direction. It is also clear that the field is not going in one direction.
    The middle is the brightest and the two edges, left and right, are darker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Uneven illumination – background subtraction](img/Insert_image_4909_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As a first attempt, we will use the background subtraction method to see whether
    this will fix the problem. To do so, we need to go to **Process** | **Subtract
    Background…** and use the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Uneven illumination – background subtraction](img/Insert_image_4909_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After applying the background subtraction, the image is altered, but the effect
    of the uneven background is still not fixed. The image is actually a little darker
    on the left-bottom side, and also the middle did not decrease in intensity that
    much (see the left-hand-side image). Note that when the **Light background** option
    was selected (see the right-hand-side image), there is a strong over compensation
    on both the left and right-hand sides. Not only is the contrast reduced on those
    sides, but also the illumination is now more uneven than it was before the correction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Uneven illumination – background subtraction](img/Insert_image_4909_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Disabling the **Sliding paraboloid** option also caused artifacts that were
    even more artificial and incorrect. The problem with this type of background subtraction
    is that it assumes a homogeneously decreasing change in background. This means
    that the changes in background should be smooth and go from high to low in a single
    direction (left to right, diagonal, and so on.). However, DIC images such as this
    one have a tendency to have a background that has more of a U-shape: high at the
    edges and low in the middle, or vice versa. Therefore, this method is unsuitable
    for this type of image, and other methods need to be explored to fix this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will try to eliminate the background using a method called pseudo-flatfield
    correction. This method is based on filtering the image using a Gaussian filter
    that blurs the details. This filter will capture the uneven illumination and separate
    it from the objects in the frame. The basis of how these filters work will be
    discussed in the next chapter in more detail. Let''s create the background image
    that we will use to correct the uneven illumination. You need to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we want to duplicate the image so that we keep the original image for
    subtraction. To do so, we will go to **Image** | **Duplicate…** or use *Ctrl*
    + *Shift* + *D* and name the duplicate image `background`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create a Gaussian low-pass filter, we will select the background image and
    go to **Process | Filters | Gaussian Blur…**, entering a value of `150` for the
    sigma (radius). When you check the preview checkbox, you will see that the image
    will look like it is defocused. You can see that the objects can no longer be
    distinguished, and what is left is the diagonal background illumination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can now subtract this background from the original image to correct the uneven
    illumination. To do this, we will start the image calculator by going to **Process**
    | **Image Calculator…** from the menu. Then, we will select the original image
    as **Image1** and the background image as **Image2**. Set the operation to **Subtract**
    and check the **Create new window** and **32-bit (float) result** checkboxes.
    The following image shows the effect of the subtraction and how it corrected the
    uneven illumination:![Uneven illumination – background subtraction](img/Insert_image_4909_03_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To enhance the contrast of an underexposed image, you can go to **Process |
    Enhance Contrast…** option, and select the **Normalize** checkbox. This stretches
    the gray values over the entire range of an 8-bit or 16-bit image. It does not
    work on RGB images. The following image shows the effect of the normalization,
    with the original image on the left-hand side and the normalized image on the
    right-hand side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image normalization](img/Insert_image_4909_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This also works for stacks or time series, where the normalization can be done
    for each frame separately. A similar effect can be obtained using the **Auto**
    option in the **Brightness/Contrast** window, as described in the previous chapter.
    Note that the normalization is applied to the image and modifies the pixel values
    irreversibly. If the signal should not change over time, this should not pose
    a big problem for measurements. However, for intensity changes over time, this
    method will distort or remove the changes.
  prefs: []
  type: TYPE_NORMAL
- en: Bleach correction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When imaging fluorescence, the illumination can cause the bleaching of the
    fluorophore under investigation. This effect is well established and is related
    to the intensity of the excitation light. To avoid this effect, it is better to
    use a long exposure with low intensity light. However, this may not always be
    possible. The amount of bleaching is related to the intensity at the beginning
    and decreases in an exponential fashion. To see whether an image series is affected
    by bleaching, we can make a quick measurement on the entire image for each frame
    to see what the mean intensity is. Note that if there are changes in illumination
    or background signal in individual frames, the results might not look like a smooth
    curve. To make a quick measurement, press *Ctrl* + *A* to select the entire frame
    and then press *Ctrl* + *M* to measure the intensity. Repeat the measurement for
    each frame and plot the mean intensity values against the frame number (or time,
    if you know the interval) in your favorite graphics program. In this case, I used
    **MATLAB** to create the plot, although you could also create the plot by selecting
    **Image** | **Stacks** | **Plot Z-axis Profile** from the menu in ImageJ. Here
    is an example of a bleaching curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bleach correction](img/Insert_image_4909_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This collection of points seems to follow a trend that is close to either a
    straight line or an exponential curve, although the trend in the first 2 seconds
    seems more exponential than linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to perform bleach correction, you can select the correction plugin
    in Fiji by going to **Image** | **Adjust** | **Bleach Correction**. There are
    three methods for correction:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential fit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple ratio is the best method if the decrease in intensity does not follow
    a regular shape, such as an exponential decay function. For most fluorescence
    imaging, this method yields good results and can be combined with fluorescence
    measurements. The histogram matching method performs better with noisy images,
    but is less suitable for intensity measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our trend looked more like an exponential decay model, we selected the
    second method, which fits a single exponential function to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bleach correction](img/Insert_image_4909_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This plot was generated using the parameters obtained by the **bleach correction**
    command and entering them in MATLAB. The ImageJ command itself also generates
    a plot with the data and the fitted curve. However, the axes are labeled *X* and
    *Y*. So, for the purpose of clarity, I have recreated the plot with the correct
    labels. The red line indicates the fitted function, which matches the curve with
    an R² value of .9954 (a very good fit). The model consists of three parameters,
    labeled *a*, *b*, and *c*. The value of *a* indicates how much above the asymptote,
    indicated by the *c* value, the first point lies. The asymptote is the value to
    which this exponential curve will go when given infinite time. The value of *b*
    indicates the rate at which the curve decays. If you want to know the time it
    takes to lose half of the initial fluorescence, you can use the following formula,
    using the *b* value from the fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bleach correction](img/Insert_image_4909_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding formula gives you the half-time of the fluorescence loss. Note
    that the *b* parameter for the fit is expressed in frames and not time. So, when
    using the preceding formula, you need to multiply the result with your frame interval
    to get the value in seconds (or minutes). In the graph shown earlier, the half-time
    is 30.587 seconds (using the formula with a *b* value of 0.0028327 and a frame
    interval of 0.125 seconds).
  prefs: []
  type: TYPE_NORMAL
- en: Stack processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ImageJ is very suitable to process information that has more than two dimensions:
    data acquired at different Z-levels or at different time points. We have already
    seen an example of stack processing in the section on noise correction. The next
    section will deal with time series consisting of frames. However, first, we will
    explore more options when dealing with image stacks containing slices (Z-stacks).'
  prefs: []
  type: TYPE_NORMAL
- en: Processing Z-stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Z-stacks are series of 2D images that were acquired at different heights or
    distances. In a microscope, this is done by moving the objective or the stage
    up or down and acquiring an image at specific intervals. In **Magnetic** **Resonance
    Imaging** (**MRI**), this is done by moving the patient through the center of
    the scanner. The scanner then creates an image for each position using radio pulses
    that create fluctuations in the magnetic field. These fluctuations can be measured
    by the detector in an MRI machine. This results in a single slice that can be
    combined into a single file. Some of the processing that you may want to perform
    on this type of image involves creating projections or 3D renders of the volume.
    We will first examine the projections that you can create. Then, you will understand
    why you would create them.
  prefs: []
  type: TYPE_NORMAL
- en: Stack projections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already seen an example of a Z-projection in the section on noise cancellation.
    In the previous section, we used the projection to create an average intensity
    for each pixel over the frames. For images that contain slices (Z-information),
    an average projection is usually not the most useful projection. However, there
    are other Z-projections available in ImageJ that are more applicable for Z-stacks.
    The following sections will deal with some examples of these projections.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum projection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A maximum projection uses the maximum intensity of each pixel across the slices.
    If a stack has 20 slices, then each pixel will contain the maximum value across
    the 20 slices. This type of projection can be helpful to reduce the third dimension
    of a Z-stack in order to create a two-dimensional representation of the data.
    This type of projection essentially flattens the image. When used on fluorescence
    images with sparse signal (few bright pixels) at the same location, this projection
    has the effect of showing all the objects in a single frame. It is also useful
    for fluorescent images that have thin objects that are in focus in different slices
    at different positions. By flattening the Z-stack, all the in-focus parts will
    be visible in one continuous shape. You can visualize this as a flight of stairs.
    Each step has a different Z-position, but if you would flatten the steps (assuming
    that the steps do not overlap), you would get a rectangular board. If you have
    an image that is not sparse, then this projection would be of little use. To demonstrate
    this projection, open the **Confocal Series** image from the sample images. Go
    to **Image** | **Stacks** | **Z Project…** and choose **Max Intensity** as the
    projection type. The following image shows the result of this projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum projection](img/Insert_image_4909_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As is visible in the preceding image, the maximum intensity projection shows
    the entire shape of the cell, but some information was lost. Specifically, the
    small details in the first frames are drowned out by the intense pixels from the
    middle of the volume. For some representations, this is fine. For some Z-stacks,
    the rough shape of the total volume is unclear in the individual slices, but the
    maximum projection shows the general shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this effect, we will open the **Bat Cochlea Volume** image by
    going to **File** | **Open Samples**. Looking at a few slices from this volume
    gives very little information about the shape of this sensory organ (the numbers
    indicate the slice number):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum projection](img/Insert_image_4909_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When we create a maximum intensity projection, the general shape of this organ
    becomes much more obvious: it is shaped like a twisted spiral (the cochlea is
    the shell-shaped cavity that is used for hearing). When you also open the **Bat
    Cochlea Renderings** image by going to **File** | **Open Samples**, you see the
    3D rendering of the volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum projection](img/Insert_image_4909_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The image on the left-hand side shows the maximum projection of the Z-stack,
    while the images on the right-hand side show the rendered volumes. From this image,
    it is clear that the maximum projection provides more information than the individual
    slices. However, some of the details are still lost in the process. In particular
    the top-left part of the cochlea is very unclear in the maximum projection. The
    start of the spiral is obscured because of the loop behind it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fiji has an option that allows you to create a maximum projection that contains
    more information: depth coding. Depth coding assigns a color to the Z-location
    of a pixel, resulting in different colors for different slices. To do so in Fiji,
    go to **Image** | **Hyperstacks** | **Temporal-Color Code** and select **Grays**
    for the LUT. This results in the middle representation in the preceding image
    (not exactly, but very similar). Note that you will get a message that the slices
    and frames were swapped. This is done because this plugin is designed for time
    series and not Z-stacks. The right image in the preceding figure is a 3D rendering
    of the volume, which will be covered in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Volume viewing and rendering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When images are acquired over a range of depths, the goal is usually to view
    this collection of images as a 3D volume. Another useful viewing aspect is to
    take a 3-dimensional volume, cut it along the *z* axis, and view the volume from
    the side. This latter view cannot be obtained from the two-dimensional images.
    For this example, we will use two different stacks from the sample images. Let''s
    start with viewing the volume of an MRI stack. To open the image, go to **File**
    | **Open Samples** and select **MRI stack**. This is an MRI stack where every
    slice is at a different level through the head (the numbers indicate the slice
    number):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Volume viewing and rendering](img/Insert_image_4909_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The eyes are clearly visible in slice number 6 as two dark orbs on the top of
    the image. Slice 11 shows the brain within the skull, and frame 16 shows the ventricles
    as black holes in the middle of the head. Slice 26 shows the top of the head.
    The area is much smaller indicating that the crown of the head is being reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'A lot of information is already present in the slices. The eyes can clearly
    be seen, as well as the sinuses and the nose (slice 1). The information in these
    slices is not complete, however. We lack shape along the *z* axis. To view the
    three-dimensional shape of this volume, we can use a viewer that comes with Fiji.
    Go to **Plugins** | **Volume Viewer** with the MRI stack selected. If you have
    the standard ImageJ, the **Volume Viewer** plugin can be downloaded and installed
    from the plugins page. Then, the following window will open:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Volume viewing and rendering](img/Insert_image_4909_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is **Volume** **Viewer**, which is included in Fiji and available as a
    plugin for ImageJ. On the left-hand side, there are three images that show different
    views of the volume: an **xy** slice (this is the top view for this stack), a
    **yz** slice (this is a side view for this stack), and finally, an **xz** slice
    (this is a front view for this stack). The large image in the middle of the viewer
    is the current selected view, in this case, the *xz* slice view. The position
    of this slice is indicated in the overview images on the left-hand side by the
    cyan (*xy*) and green (**yz**) lines. Note that I adjusted the **z-Aspect** by
    entering `5` and pressing *Enter* instead of the value of `1` based on the current
    calibration. The volume, otherwise, looks very squashed. The squashed appearance
    is caused by the fact that this image was not calibrated. Each voxel (a contraction
    of volume and pixel) is 1 x 1 x 1, without a unit. A typical value for the voxel
    size in MRI images is 1.5 x 1.5 x 3.0 mm, which can be set using the image properties
    as described in the previous chapter. We can now change the view by selecting
    the view buttons at the bottom of the viewer. The **yz** button will give us a
    side view of this volume. It is also possible to rotate the volume by clicking
    and dragging the mouse.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The size of the volume viewer window has a minimum size of ±1024 x 768 pixels.
    This can mean that depending on the pixel dimensions of your monitor, some of
    the controls might fall off the screen. For most modern displays, this should
    not be an issue. However, for some small screens or beamers, this can be a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at a different type of image: a Z-stack of a fly brain using
    fluorescent imaging. To open the image, go to **File** | **Open Samples** and
    select the **Fly Brain** image. The Z-stack will open, and you can go through
    the slices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Volume viewing and rendering](img/Insert_image_4909_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first slice contains no bright pixels, but as you move through the stack,
    the brain of the fruit fly starts to show defined features. This stack shows the
    brain from a (rotated) front view in contrast to the MRI stack, which was displayed
    as a top view. We will use the volume viewer to examine the entire volume and
    use it to create a short movie of the volume turning. To start, select the **Fly
    Brain** stack and go to **Plugins** | **Volume Viewer**. The initial image will
    be a slice view, but for this example, we want to switch to a different mode.
    Select **Volume (4)** mode using the selector at the top of the viewer. We will
    set the interpolation to **Tricubic sharp (3)** using the drop-down selector.
    On the right-hand side of the viewer, we will modify the transfer function to
    **2D Grad** to create a slightly more pleasing view. Next, we will set the rotation
    for *X*, *Y*, and *Z* at the bottom of the volume viewer to -90, 30, and 180 respectively.
    This will provide a side view of the brain.
  prefs: []
  type: TYPE_NORMAL
- en: By pressing the **Snapshot** button (top right) in the viewer, we will get a
    picture of the current view. Next, we will increase the value for the *Y* rotation
    with 10-degree increments and take a snapshot every time until you have reached
    210 degrees. We now made snapshots of the brain from one side of the brain to
    the other side (180 degrees). To turn this into an animation, all we have to do
    is go to **Image** | **Stacks** | **Images to Stack**. If you close the original
    stack, you only have to press OK in the dialog. Otherwise, you would have to enter
    `Volume_Viewer` in the **Title contains** field. You will now have a stack that
    can be played and saved as a movie for presentation purposes. For this example,
    we used increments of 10 degrees for the rotation, which gives an adequate result.
    However, if you take smaller increments, the result will look much smoother. Feel
    free to modify the angles at which you view the volume for different results as
    well as experiment with the other settings available within the volume viewer.
  prefs: []
  type: TYPE_NORMAL
- en: The volume viewer is a very powerful function in ImageJ that allows for the
    investigation and visualization of 3D objects. Use the **Slice (0)** mode to examine
    the volume as a cross-section and the **Volume (4)** mode to see a solid model.
  prefs: []
  type: TYPE_NORMAL
- en: Processing time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time series consists of images acquired over time, usually with a fixed interval.
    Movies can also be seen as time series with a fixed interval of 24 or 25 frames
    per second (**fps**). Processing of time series mostly focuses on two areas: fluctuations
    in intensity over time and background reduction and normalization. Fluctuations
    in intensity have been covered in the previous section where we looked at bleach
    correction. In the following section, we will look at ways to normalize the time
    series data.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Normalizing time series data will help in further analysis by providing a correction
    for the baseline intensity. Many times, the goal of time series is to look at
    changes in intensity or movement over time. Normalizing will yield cleaner time
    series data relative to the resting or baseline state. A very simple normalization
    is to calculate ΔF over F0 (dFF0). The basis for this metric is that the baseline
    fluorescence can be different between time series, but the changes in intensity
    *relative to the baseline* are similar. It is calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalizing time series data](img/Insert_image_4909_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The numerator is the difference between the current frame (Ft) and the baseline
    (F0). The baseline is the average of the first *n* frames. A value of *dFF0* larger
    than 1 indicates the signal increased relative to the baseline, while a value
    less than 1 indicates a decrease relative to the baseline. It is possible to perform
    this calculation only on the measured values of a time series (in Excel or MATLAB),
    but you can also transform the time series directly. I will now show you how to
    do this in ImageJ using the Z projection, image duplication, and image calculator.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we will open the `timeseries_events.tif` image, which is available
    in the online resources with this book. This is a time series of vesicles in a
    cell that are transported and fuse when the cell is stimulated with electrodes.
    It contains two channels: one with a red fluorescent marker and the other with
    a green fluorescent marker. The red marker is fluorescent at all times until the
    vesicle fuses, at which point it disappears. The green marker is not fluorescent
    while the cargo is within the vesicle, but as soon as it fuses, it becomes bright.
    To start the processing, we first want to split the channels into two different
    time series. To do this, select the time series and go to **Image** | **Color**
    | **Split Channels** to generate two time series: one for each channel. We will
    select the green channel, which was labeled `C1_timeseries_events.tif`, using
    the **split channels** command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now start with the first step in the creation of the `dFF0` time series:
    creating the baseline frame. We will go to **Image** | **Stacks** | **Z Project…**
    and set the method to **Average Intensity** and **Stop slice:** to `5`. What we
    do here is create an average of the first five frames. This effectively reduces
    the noise in the individual frames by averaging it out while leaving the bright
    objects present in the first frames intact. Let''s rename the resulting image
    to make it easier to identify later on. Right-click on the average image and select
    **Rename…** from the context menu. Rename the image to `F0` so that it will be
    easy to select later on.'
  prefs: []
  type: TYPE_NORMAL
- en: For the next step, we will create the ΔF image. As explained at the beginning
    of this section, this image is the raw image minus the baseline image. To get
    this image, we will use the image calculator by going to **Process** | **Image
    Calculator** from the **ImageJ** menu. Select the original time series as `Image1`
    and the **F0** image as `Image2`. Then, set the method to **Subtract**. Make sure
    that the **Create new window** option is selected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The order of the images is very important when one of them is a stack or time
    series and the other is a single frame. The stack always needs to be set at the
    `Image1` position if you wish to modify each slice or frame. For subtraction,
    this is usually obvious, but for multiplication, the order of the operation would
    not be important from a mathematical perspective (*A × B equals B × A*). However,
    if you place the time series or stack on `Image2` and the single frame on `Image1`,
    only the current slice or frame is used for the calculation!
  prefs: []
  type: TYPE_NORMAL
- en: We now have the ΔF stack, so let's rename it to make it easier. Right-click
    on the new time series, select **Rename…**, and enter `deltaF` as the new name.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can create the final time series that is normalized to the baseline.
    Note that the `deltaF` series by itself already provides an improvement over the
    original time series as it is corrected for the initial static background. To
    create the *dFF0* image, we will use the Image Calculator again. This time, we
    will select `deltaF` as `Image1` and `F0` as `Image2` and the **Divide** operation.
    Select the **Create new window** and **32-bit (float) result** options.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This time, the 32-bit result option is useful. As we noted earlier, in the definition
    of the calculation, we expected the results to be between 0 and infinity. This
    is denoted as *[0, ∞]* in mathematical notation. This means that that any value,
    including 0 and infinity, are within the range of possible values. When this option
    is not selected during the calculation, all the values below 1 will be rounded
    to 0, and information about these events are lost. Note that for the example used
    here, the events we wish to see will have a value larger than 1\. So, in this
    particular case, it is not crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new image is now the dFF0 image, which has been corrected for the baseline
    and normalized to the initial baseline intensity. The following image shows the
    effect of this normalization (second row), compared to the original images (first
    row):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalizing time series data](img/Insert_image_4909_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The clearest difference that can be seen is that the images before frame **300**
    are nearly black, indicating that nothing is happening relative to the baseline
    situation. At frame **300** and beyond, the increase in signal at different locations
    is very clear, indicating that the signal has increased in these locations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we investigated the processing of different types of images.
    We looked at different sources of noise that can corrupt images and degrade their
    quality. You learned how to apply different corrections to the images to fix these
    problems. We then looked at processing steps specifically aimed at Z-stacks and
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to separate pixels into different groups
    and how to clean up and filter the result for further processing.
  prefs: []
  type: TYPE_NORMAL
