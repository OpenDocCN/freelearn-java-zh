<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Measuring Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Measuring Performance</h1></div></div></div><p>Depending on the expected and actual performance, and the lack or presence of a measuring system, performance analysis and tuning can be a fairly elaborate process. Now we will discuss the analysis of performance characteristics and ways to measure and monitor them. In this chapter we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Measuring performance and understanding the results</li><li class="listitem" style="list-style-type: disc">What performance tests to carry out for different purposes</li><li class="listitem" style="list-style-type: disc">Monitoring performance and obtaining metrics</li><li class="listitem" style="list-style-type: disc">Profiling Clojure code to identify performance bottlenecks</li></ul></div><div class="section" title="Performance measurement and statistics"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Performance measurement and statistics</h1></div></div></div><p>Measuring <a class="indexterm" id="id401"/>performance is the stepping stone to performance <a class="indexterm" id="id402"/>analysis. As we noted earlier in this book, there are several performance parameters to be measured with respect to various scenarios. Clojure's built-in <code class="literal">time</code> macro is a tool to measure the amount of time elapsed while executing a body of code. Measuring performance factors is a much more involved process. The measured performance numbers may have linkages with each other that we need to analyze. It is a common practice to use statistical concepts to establish the linkage factors. We will discuss some basic statistical concepts in this section and use that to explain how the measured data gives us the bigger picture.</p><div class="section" title="A tiny statistics terminology primer"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec82"/>A tiny statistics terminology primer</h2></div></div></div><p>When we have a series<a class="indexterm" id="id403"/> of quantitative data, such as latency in milliseconds for the same operation (measured over a number of executions), we can observe a number of things. First, and the most obvious, are the minimum and maximum values in the data. Let's take an example dataset to analyze further:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>19</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td><td style="text-align: left" valign="top">
<p>168</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>29</p>
</td><td style="text-align: left" valign="top">
<p>172</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td></tr></tbody></table></div><div class="section" title="Median, first quartile, third quartile"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec24"/>Median, first quartile, third quartile</h3></div></div></div><p>We can see that the minimum latency here is 19 ms whereas the maximum latency is 172ms. We can also observe that the average latency here is about 40ms. Let's sort this data in ascending order:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>19</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td><td style="text-align: left" valign="top">
<p>29</p>
</td><td style="text-align: left" valign="top">
<p>168</p>
</td><td style="text-align: left" valign="top">
<p>172</p>
</td></tr></tbody></table></div><p>The center element of the <a class="indexterm" id="id404"/>previous dataset, that is the ninth element (value 23), is considered the <span class="strong"><strong>median</strong></span>
<a class="indexterm" id="id405"/> of the dataset. It is noteworthy that the median is a better representative of the center of the data than the <a class="indexterm" id="id406"/>
<span class="strong"><strong>average</strong></span> or<a class="indexterm" id="id407"/> <span class="strong"><strong>mean</strong></span>. The center element of the <a class="indexterm" id="id408"/>left half, that is the fifth element (value 21), is considered the <a class="indexterm" id="id409"/>
<span class="strong"><strong>first quartile</strong></span>. Similarly, the value in the center of the right half, that is the 13th element (value 26), is considered the <a class="indexterm" id="id410"/>
<span class="strong"><strong>third quartile</strong></span>
<a class="indexterm" id="id411"/> of the dataset. The difference between the third quartile and the first quartile is called<a class="indexterm" id="id412"/> <span class="strong"><strong>Inter Quartile Range (IQR)</strong></span>, which is 5 in this case. This can be <a class="indexterm" id="id413"/>illustrated with a <span class="strong"><strong>boxplot</strong></span>
<a class="indexterm" id="id414"/>, as follows:</p><div class="mediaobject"><img alt="Median, first quartile, third quartile" src="graphics/3642_06_01.jpg"/></div><p>A boxplot highlights the first quartile, median and the third quartile of a dataset. As you can see, two "outlier" latency numbers (168 and 172) are unusually higher than the others. Median makes no representation of outliers in a dataset, whereas the mean does.</p><div class="mediaobject"><img alt="Median, first quartile, third quartile" src="graphics/3642_06_02.jpg"/></div><p>A histogram (the diagram shown previously) is another way to display a dataset where we batch the data elements in <a class="indexterm" id="id415"/>
<span class="strong"><strong>periods</strong></span>
<a class="indexterm" id="id416"/> and expose the<a class="indexterm" id="id417"/> <span class="strong"><strong>frequency</strong></span> <a class="indexterm" id="id418"/>of such periods. A period contains the elements in a certain range. All periods in a histogram are generally the same size; however, it is not uncommon to omit certain periods when there is no data.</p></div><div class="section" title="Percentile"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec25"/>Percentile</h3></div></div></div><p>A <span class="strong"><strong>percentile</strong></span>
<a class="indexterm" id="id419"/> is expressed with a parameter, such as 99 percentile, or<a class="indexterm" id="id420"/> 95 percentile etc. A percentile is the value below which all the specified percentage of data elements exist. For example, 95 percentile means the value <span class="emphasis"><em>N</em></span> among a dataset, such that 95 percent of elements in the dataset are below <span class="emphasis"><em>N</em></span> in value. As a concrete example, 85 percentile from the dataset of latency numbers we discussed earlier in this section is 29, because out of 17 total elements, 14 (which is 85 percent of 17) other elements in the dataset have a value below 29. A quartile splits a dataset into chunks of 25 percent elements each. Therefore, the first quartile is actually 25 percentile, the median is 50 percentile, and the third quartile is 75 percentile.</p></div><div class="section" title="Variance and standard deviation"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec26"/>Variance and standard deviation</h3></div></div></div><p>The spread of the data, that is, how <a class="indexterm" id="id421"/>far away the data elements<a class="indexterm" id="id422"/> are<a class="indexterm" id="id423"/> from the<a class="indexterm" id="id424"/> center value, gives us further insight into the data. Consider the <span class="emphasis"><em>i<sup>th</sup></em></span> deviation as the difference between the <span class="emphasis"><em>i<sup>th</sup></em></span> dataset element value (in statistics terms, a "variable" value) and its mean; we can represent it as <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image006.jpg"/></span>. We can express its "variance" and "standard deviation" as follows:</p><p>Variance = <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image008.jpg"/></span>, standard deviation (σ) = <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image010.jpg"/></span> = <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image012.jpg"/></span>
</p><p>Standard deviation is shown as the Greek letter "sigma", or simply "s". Consider the following Clojure code to determine variance and standard deviation:</p><div class="informalexample"><pre class="programlisting">(def tdata [23 19 21 24 26 20 22 21 25 168 23 20 29 172 22 24 26])

(defn var-std-dev
  "Return variance and standard deviation in a vector"
  [data]
  (let [size (count data)
        mean (/ (reduce + data) size)
        sum (-&gt;&gt; data
                 (map #(let [v (- % mean)] (* v v)))                 (reduce +))
        variance (double (/ sum (dec size)))]
    [variance (Math/sqrt variance)]))

user=&gt; (println (var-std-dev tdata))
[2390.345588235294 48.89116063497873]</pre></div><p>You can use the <a class="indexterm" id="id425"/>Clojure-based platform<a class="indexterm" id="id426"/> Incanter (<a class="ulink" href="http://incanter.org/">http://incanter.org/</a>) for statistical <a class="indexterm" id="id427"/>computations. For <a class="indexterm" id="id428"/>example, you can find standard<a class="indexterm" id="id429"/> deviation using <code class="literal">(incanter.stats/sd tdata)</code> in Incanter.</p><p>The <span class="strong"><strong>empirical rule</strong></span>
<a class="indexterm" id="id430"/> states the relationship between the elements of a dataset and SD. It says that 68.3 percent of all elements in a dataset lie in the range of one (positive or negative) SD from the mean, 95.5 percent of all elements lie in two SDs from the mean, and 99.7 percent of all data elements lie in three SDs from the mean.</p><p>Looking at the latency dataset we started out with, one SD from the mean is <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image014.jpg"/></span>(<span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image016.jpg"/></span> range -9 to 89) containing 88 percent of all elements. Two SDs from the mean is <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image014.jpg"/></span> range -58 to 138) containing the same 88 percent of all elements. However, three SDs from the mean is(<span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image018.jpg"/></span>range -107 to 187) containing 100 percent of all elements. There is a mismatch between what the empirical rule states and the results we found, because the empirical rule applies generally to uniformly distributed datasets with a large number of elements.</p></div></div><div class="section" title="Understanding Criterium output"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec83"/>Understanding Criterium output</h2></div></div></div><p>In <a class="link" href="ch04.html" title="Chapter 4. Host Performance">Chapter 4</a>, <span class="emphasis"><em>Host Performance</em></span>, we <a class="indexterm" id="id431"/>introduced the Clojure library <span class="emphasis"><em>Criterium</em></span> to measure the latency of Clojure expressions. A sample benchmarking result is as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (require '[criterium.core :refer [bench]])
nil
user=&gt; (bench (reduce + (range 1000)))
Evaluation count : 162600 in 60 samples of 2710 calls.
             Execution time mean : 376.756518 us
    Execution time std-deviation : 3.083305 us
   Execution time lower quantile : 373.021354 us ( 2.5%)
   Execution time upper quantile : 381.687904 us (97.5%)

Found 3 outliers in 60 samples (5.0000 %)
low-severe 2 (3.3333 %)
low-mild 1 (1.6667 %)
 Variance from outliers : 1.6389 % Variance is slightly inflated by outliers</pre></div><p>We can see that the<a class="indexterm" id="id432"/> result has some familiar terms we discussed earlier in this section. A high mean and low standard deviation indicate that there is not a lot of variation in the execution times. Likewise, the lower (first) and upper (third) quartiles indicate that they are not too far away from the mean. This result implies that the body of code is more or less stable in terms of response time. Criterium repeats the execution many times to collect the latency numbers.</p><p>However, why does Criterium attempt to do a statistical analysis of the execution time? What would be amiss if we simply calculate the mean? It turns out that the response times of all executions are not always stable and there is often disparity in how the response time shows up. Only upon running sufficient times under correctly simulated load we can get complete data and other indicators about the latency. A statistical analysis gives insight into whether there is something wrong with the benchmark.</p></div><div class="section" title="Guided performance objectives"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec84"/>Guided performance objectives</h2></div></div></div><p>We only briefly discussed <a class="indexterm" id="id433"/>performance objectives in <a class="link" href="ch01.html" title="Chapter 1. Performance by Design">Chapter 1</a>, <span class="emphasis"><em>Performance by Design</em></span> because that discussion needs a reference to statistical concepts. Let's say we identified the functional scenarios and the corresponding response time. Should response time remain fixed? Can we constrain throughput in order to prefer a stipulated response time?</p><p>The performance objective should specify the worst-case response time, that is, maximum latency, the average response time and the maximum standard deviation. Similarly, the performance objective should also mention the worst-case throughput, maintenance window throughput, average throughput, and the maximum standard deviation.</p></div></div></div>
<div class="section" title="Performance testing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Performance testing</h1></div></div></div><p>Testing for performance <a class="indexterm" id="id434"/>requires us to know what we are going to test, how we want to test, and what environment to set up for the tests to execute. There are several pitfalls to be aware of, such as a lack of near-real hardware and resources of production use, similar OS and software environments, diversity of representative data for test cases, and so on. Lack of diversity in test inputs may lead to a monotonic branch prediction, hence introducing a "bias" in test results.</p><div class="section" title="The test environment"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec85"/>The test environment</h2></div></div></div><p>Concerns about the <a class="indexterm" id="id435"/>test environment begin with the hardware representative of the production environment. Traditionally, the test environment hardware has been a scaled-down version of the production environment. The performance analysis done on non-representative hardware is almost certain to skew the results. Fortunately, in recent times, thanks to the commodity hardware and cloud systems, provisioning test environment hardware that is similar to the production environment is not too difficult.</p><p>The network and storage bandwidth, operating system, and software used for performance testing should of course be the same as in production. What is also important is to have a "load" representative of the test scenarios. The load comes in different combinations including the concurrency of requests, the throughput and standard deviation of requests, the current population level in the database or in the message queue, CPU and heap usage, and so on. It is important to simulate a representative load.</p><p>Testing often requires quite some work on the part of the piece of code that carries out the test. Be sure to keep its overhead to a minimum so that it does not impact the benchmark results. Wherever possible, use a system other than the test target to generate requests.</p></div><div class="section" title="What to test"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec86"/>What to test</h2></div></div></div><p>Any implementation of a non-trivial system typically involves many hardware and software components. Performance testing a certain feature or a service in the entire system needs to account for the way it interacts with the various sub-systems. For example, a web service call may touch multiple layers such as the web server (request/response marshaling and unmarshaling), URI-based routing, service handler, application-database connector, the database layer, logger component, and so on. Testing only the service handler would be a terrible mistake, because that is not exactly the performance what the web client will experience. The performance test should test at the perimeter of a system to keep the results realistic, preferably with a third-party observer.</p><p>The performance objectives state the criteria for testing. It would be useful to test what is not required by the objective, especially when the tests are run concurrently. Running meaningful performance tests may require a certain level of isolation.</p></div><div class="section" title="Measuring latency"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec87"/>Measuring latency</h2></div></div></div><p>The latency obtained<a class="indexterm" id="id436"/> by executing a body of code may vary <a class="indexterm" id="id437"/>slightly on each run. This necessitates that we execute the code many times and collect samples. The latency numbers may be impacted by the JVM warm-up time, garbage collection and the JIT compiler kicking in. So, the test and sample collection should ensure that these conditions do not impact the results. Criterium follows such methods to produce the results. When we test a very small piece of code this way, it is called a <a class="indexterm" id="id438"/>
<span class="strong"><strong>Micro-benchmark</strong></span>.</p><p>As the latency of some operations may vary during runs, it is important to collect all samples and segregate them into periods and frequencies turning up into a histogram. The maximum latency is an important metric when measuring latency—it indicates the worst-case latency. Besides the maximum, the 99 percentile and 95 percentile latency numbers are also important to put things in perspective. It's important to actually collect the latency numbers instead of inferring them from standard deviation, as we noted earlier that the empirical rule works only for normal distributions without significant outliers.</p><p>The outliers are an important data point when measuring latency. A proportionately higher count of outliers indicates a possibility of degradation of service.</p><div class="section" title="Comparative latency measurement"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec27"/>Comparative latency measurement</h3></div></div></div><p>When evaluating libraries for<a class="indexterm" id="id439"/> use in projects, or when coming up with alternate solutions against some baseline, comparative latency benchmarks are useful to determine the performance trade-offs. We will inspect two comparative benchmarking tools based on Criterium, called Perforate and Citius. Both make it easy to run Criterium benchmarks grouped by context, and to easily view the benchmark results.</p><p>Perforate (<a class="ulink" href="https://github.com/davidsantiago/perforate">https://github.com/davidsantiago/perforate</a>) is <a class="indexterm" id="id440"/>a Leiningen <a class="indexterm" id="id441"/>plugin that lets one define goals; a goal (defined using <code class="literal">perforate.core/defgoal</code>) is a common task or context having one or more benchmarks. Each benchmark is defined using <code class="literal">perforate.core/defcase</code>. As of version 0.3.4, a sample benchmark code may look like the following code snippet:</p><div class="informalexample"><pre class="programlisting">(ns foo.bench
  (:require [perforate.core :as p]))

(p/defgoal str-concat "String concat")
(p/defcase str-concat :apply
  [] (apply str ["foo" "bar" "baz"]))
(p/defcase str-concat :reduce
  [] (reduce str ["foo" "bar" "baz"]))

(p/defgoal sum-numbers "Sum numbers")
(p/defcase sum-numbers :apply
  [] (apply + [1 2 3 4 5 6 7 8 9 0]))
(p/defcase sum-numbers :reduce
  [] (reduce + [1 2 3 4 5 6 7 8 9 0]))</pre></div><p>You can declare the test environments in <code class="literal">project.clj</code> and provide the setup/cleanup code when defining the goal. Perforate provides ways to run the benchmarks from the command-line.</p><p>Citius (<a class="ulink" href="https://github.com/kumarshantanu/citius">https://github.com/kumarshantanu/citius</a>) is<a class="indexterm" id="id442"/> a library that provides integration <a class="indexterm" id="id443"/>hooks for clojure.test and other forms of invocation. It<a class="indexterm" id="id444"/> imposes more rigid constraints than Perforate, and renders additional comparative information about the benchmarks. It presumes a fixed number of targets (cases) per test suite where there may be several goals. </p><p>As of version 0.2.1, a sample benchmark code may look like the following code snippet:</p><div class="informalexample"><pre class="programlisting">(ns foo.bench
  (:require [citius.core :as c]))

(c/with-bench-context ["Apply" "Reduce"]
  {:chart-title "Apply vs Reduce"
   :chart-filename "bench-simple.png"}
  (c/compare-perf
    "concat strs"
    (apply str ["foo" "bar" "baz"])
    (reduce str ["foo" "bar" "baz"]))
  (c/compare-perf
    "sum numbers"
    (apply + [1 2 3 4 5 6 7 8 9 0])
    (reduce + [1 2 3 4 5 6 7 8 9 0])))</pre></div><p>In the previous example, the code runs the benchmarks, reports the comparative summary, and draws a bar chart image of the mean latencies.</p></div><div class="section" title="Latency measurement under concurrency"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec28"/>Latency measurement under concurrency</h3></div></div></div><p>When we benchmark <a class="indexterm" id="id445"/>a piece of code with Criterium, it uses just a single thread to determine results. That gives us a fair output in terms of single-threaded result, but there are many benchmarking scenarios where single-threaded latency is far from what we need. Under concurrency, the latency often differs quite a bit from single-threaded latency. Especially when we deal with <span class="emphasis"><em>stateful</em></span> objects (e.g. drawing a connection from a JDBC connection pool, updating shared in-memory state etc.), the latency is likely to vary in proportion with the contention. In such scenarios it is useful to find out the latency patterns of the code under various concurrency levels.</p><p>The Citius library we discussed in the previous sub-section supports tunable concurrency levels. Consider the following benchmark of implementations of shared counters:</p><div class="informalexample"><pre class="programlisting">(ns foo.bench
  (:require
    [clojure.test :refer [deftest]]
    [citius.core :as c])
  (:import [java.util.concurrent.atomic AtomicLong]))

(def a (atom 0))
(def ^AtomicLong b (AtomicLong. 0))

(deftest test-counter
  (c/with-bench-context ["Atom" "AtomicLong"] {}
    (c/compare-perf "counter"
      (swap! a unchecked-inc) (.incrementAndGet b))))

;; Under Unix-like systems run the following command in terminal:
;; CITIUS_CONCURRENCY=4,4 lein test</pre></div><p>When I ran<a class="indexterm" id="id446"/> this benchmark on a 4th generation quad-core Intel Core i7 processor (Mac OSX 10.10), the mean latency at concurrency level 04 was 38 to 42 times the value of the mean latency at concurrency level 01. Since, in many cases, the JVM is used to run server-side applications, benchmarking under concurrency becomes all the more important.</p></div></div><div class="section" title="Measuring throughput"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec88"/>Measuring throughput</h2></div></div></div><p>Throughput is expressed per <a class="indexterm" id="id447"/>unit of time. Coarse-grained<a class="indexterm" id="id448"/> throughput, that is, the throughput number collected over a long period of time, may hide the fact when the throughput is actually delivered in bursts instead of a uniform distribution. Granularity of the throughput test is subject to the nature of the operation. A batch process may process bursts of data, whereas a web service may deliver uniformly distributed throughput.</p><div class="section" title="Average throughput test"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec29"/>Average throughput test</h3></div></div></div><p>Though Citius (as of version 0.2.1) shows <a class="indexterm" id="id449"/>extrapolated throughput (per second, per thread) in benchmark results, that throughput number may not represent the actual throughput very well for a variety of reasons. Let's construct a simple<a class="indexterm" id="id450"/> throughput benchmark harness as follows, beginning with the helper functions:</p><div class="informalexample"><pre class="programlisting">(import '[java.util.concurrent ExecutorService Executors Future])
(defn concurrently
  ([n f]
    (concurrently n f #(mapv deref %)))
  ([n f g]
    (let [^ExecutorService
          thread-pool (Executors/newFixedThreadPool n)
          future-vals (transient [])]
      (dotimes [i n]
        (let [^Callable task (if (coll? f) (nth f i) f)
              ^Future each-future (.submit thread-pool task)]
          (conj! future-vals each-future)))
      (try
        (g (persistent! future-vals))
        (finally
          (.shutdown thread-pool))))))

(defn call-count
  []
  (let [stats (atom 0)]
    (fn
      ([] (deref stats))
      ([k] (if (identical? :reset k)
             (reset! stats 0)
             (swap! stats unchecked-inc))))))

(defn wrap-call-stats
  [stats f]
  (fn [&amp; args]
    (try
      (let [result (apply f args)]
        (stats :count)
        result))))

(defn wait-until-millis
  ([^long timeout-millis]
    (wait-until-millis timeout-millis 100))
  ([^long timeout-millis ^long progress-millis]
    (while (&lt; (System/currentTimeMillis) timeout-millis)
      (let [millis (min progress-millis
                     (- timeout-millis (System/currentTimeMillis)))]
        (when (pos? millis)
          (try
            (Thread/sleep millis)
            (catch InterruptedException e
              (.interrupt ^Thread (Thread/currentThread))))
          (print \.)
          (flush))))))</pre></div><p>Now that we have<a class="indexterm" id="id451"/> the helper functions defined, let's see the benchmarking code:</p><div class="informalexample"><pre class="programlisting">(defn benchmark-throughput*
  [^long concurrency ^long warmup-millis ^long bench-millis f]
  (let [now        #(System/currentTimeMillis)
        exit?      (atom false)
        stats-coll (repeatedly concurrency call-count)
        g-coll     (-&gt;&gt; (repeat f)
                     (map wrap-call-stats stats-coll)
                     (map-indexed (fn [i g]
                                    (fn []
                                      (let [r (nth stats-coll i)]
                                        (while (not (deref exit?))
                                          (g))
                                        (r)))))
                     vec)
        call-count (-&gt;&gt; (fn [future-vals]
                          (print "\nWarming up")
                          (wait-until-millis (+ (now) warmup-millis))
                          (mapv #(% :reset) stats-coll) ; reset count
                          (print "\nBenchmarking")
                          (wait-until-millis (+ (now) bench-millis))
                          (println)
                          (swap! exit? not)
                          (mapv deref future-vals))
                     (concurrently concurrency g-coll)
                     (apply +))]
    {:concurrency concurrency
     :calls-count call-count
     :duration-millis bench-millis
     :calls-per-second (-&gt;&gt; (/ bench-millis 1000)
                         double
                         (/ ^long call-count)
                         long)}))

(defmacro benchmark-throughput
  "Benchmark a body of code for average throughput."
  [concurrency warmup-millis bench-millis &amp; body]
  `(benchmark-throughput*
    ~concurrency ~warmup-millis ~bench-millis (fn [] ~@body)))</pre></div><p>Let's now see how to test some code for throughput using the harness:</p><div class="informalexample"><pre class="programlisting">(def a (atom 0))
(println
  (benchmark-throughput
    4 20000 40000 (swap! a inc)))</pre></div><p>This harness <a class="indexterm" id="id452"/>provides only a simple throughput test. To inspect the throughput pattern you may want to bucket the throughput across rolling fixed-duration windows (e.g. per second throughput.) However, that topic is beyond the scope of this text, though we will touch upon it in the <span class="emphasis"><em>Performance monitoring</em></span> section later in this chapter.</p></div></div><div class="section" title="The load, stress, and endurance tests"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec89"/>The load, stress, and endurance tests</h2></div></div></div><p>One of the characteristics of tests is each run only represents the slice of time it is executed through. Repeated <a class="indexterm" id="id453"/>runs establish their general behavior. But how many runs should be enough? There may be several anticipated load scenarios for an operation. So, there is a need to repeat the tests at various load scenarios. Simple test runs may not always exhibit the long-term behavior and response of the operation. Running the tests under varying high load for longer duration allows us to observe them for any odd behavior that may not show up in a short test cycle.</p><p>When we test an <a class="indexterm" id="id454"/>operation at a load far beyond its anticipated latency and throughput objectives, that is<a class="indexterm" id="id455"/> <span class="strong"><strong>stress testing</strong></span>. The intent of a stress test is to ascertain a reasonable behavior exhibited by the operation beyond the maximum load it was developed for. Another way to observe the behavior of an operation is to see how it behaves when run for a very long duration, typically for several days or weeks. Such prolonged tests are called <a class="indexterm" id="id456"/>
<span class="strong"><strong>endurance tests</strong></span>. While <a class="indexterm" id="id457"/>a stress test checks the graceful behavior of the operation, an endurance test checks the consistent behavior of the operation over a long period.</p><p>There are several tools that may help with load and stress testing. Engulf (<a class="ulink" href="http://engulf-project.org/">http://engulf-project.org/</a>) is <a class="indexterm" id="id458"/>a distributed<a class="indexterm" id="id459"/> HTTP-based, load-generation tool written in Clojure. Apache JMeter and Grinder are Java-based load-generation tools. Grinder can be scripted using Clojure. Apache Bench is a load-testing tool for web systems. Tsung is an extensible, high-performance, load-testing tool written in Erlang.</p></div></div>
<div class="section" title="Performance monitoring"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Performance monitoring</h1></div></div></div><p>During prolonged testing or after <a class="indexterm" id="id460"/>the application has gone to production, we need to monitor its performance to make sure the application continues to meet the performance objectives. There may be infrastructure or operational issues impacting the performance or availability of the application, or occasional spikes in latency or dips in throughput. Generally, monitoring alleviates such risk by generating a continuous feedback stream.</p><p>Roughly there are three kinds of components used to build a monitoring stack. A <span class="strong"><strong>collector</strong></span> sends the numbers from each host that needs to be monitored. The collector gets host information and the performance numbers and sends them to an <span class="strong"><strong>aggregator</strong></span>. An aggregator receives the data sent by the collector and persists them until asked by a <span class="strong"><strong>visualizer</strong></span> on behalf of the user.</p><p>The<a class="indexterm" id="id461"/> project <span class="strong"><strong>metrics-clojure</strong></span> (<a class="ulink" href="https://github.com/sjl/metrics-clojure">https://github.com/sjl/metrics-clojure</a>) is a Clojure wrapper over the <span class="strong"><strong>Metrics</strong></span> (<a class="ulink" href="https://github.com/dropwizard/metrics">https://github.com/dropwizard/metrics</a>) Java<a class="indexterm" id="id462"/> framework, which acts as a collector. <span class="strong"><strong>Statsd</strong></span>
<a class="indexterm" id="id463"/> is a well-known aggregator that does not persist data by itself but passes it on to a variety of servers. One of the popular visualizer projects is <span class="strong"><strong>Graphite</strong></span>
<a class="indexterm" id="id464"/> that stores the data as well as produces graphs for requested periods. There are several other alternatives to these, notably <a class="indexterm" id="id465"/>
<span class="strong"><strong>Riemann</strong></span> (<a class="ulink" href="http://riemann.io/">http://riemann.io/</a>) that is written in Clojure and Ruby. Riemann is an event processing-based aggregator.</p><div class="section" title="Monitoring through logs"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec90"/>Monitoring through logs</h2></div></div></div><p>One of the popular <a class="indexterm" id="id466"/>performance monitoring approaches that has emerged in recent times is via logs. The idea is simple—the application emits metrics data as logs, which are shipped from the individual machine to a central log aggregation service. Then, those metrics data are aggregated for each time window and further moved for archival and visualization.</p><p>As a high-level example of such a monitoring system, you may like to use<a class="indexterm" id="id467"/> <span class="strong"><strong>Logstash-forwarder</strong></span> (<a class="ulink" href="https://github.com/elastic/logstash-forwarder">https://github.com/elastic/logstash-forwarder</a>) to grab the application logs from the local filesystem and ship to <a class="indexterm" id="id468"/>
<span class="strong"><strong>Logstash</strong></span> (<a class="ulink" href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a>), where it forwards the metrics logs to <span class="strong"><strong>StatsD</strong></span>
<a class="indexterm" id="id469"/> (<a class="ulink" href="https://github.com/etsy/statsd">https://github.com/etsy/statsd</a>) for metrics aggregation or to Riemann<a class="indexterm" id="id470"/> (<a class="ulink" href="http://riemann.io/">http://riemann.io/</a>) for events analysis and monitoring alerts. StatsD and/or Riemann can forward the metrics data to Graphite<a class="indexterm" id="id471"/> (<a class="ulink" href="http://graphite.wikidot.com/">http://graphite.wikidot.com/</a>) for archival and graphing of the time-series metrics data. Often, people want to plug in a non-default time-series data store (such as <span class="strong"><strong>InfluxDB</strong></span>: <a class="ulink" href="https://influxdb.com/">https://influxdb.com/</a>) or a visualization layer (such as <span class="strong"><strong>Grafana</strong></span>: <a class="ulink" href="http://grafana.org/">http://grafana.org/</a>) with Graphite.</p><p>A detailed<a class="indexterm" id="id472"/> discussion on this topic is out of the scope of this text, but I think exploring this area would serve you well.</p></div><div class="section" title="Ring (web) monitoring"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec91"/>Ring (web) monitoring</h2></div></div></div><p>If you develop web <a class="indexterm" id="id473"/>software using<a class="indexterm" id="id474"/> Ring (<a class="ulink" href="https://github.com/ring-clojure/ring">https://github.com/ring-clojure/ring</a>) then you may find the Ring extension of the metrics-clojure library<a class="indexterm" id="id475"/> useful: <a class="ulink" href="http://metrics-clojure.readthedocs.org/en/latest/ring.html">http://metrics-clojure.readthedocs.org/en/latest/ring.html</a> —this tracks a number of useful metrics that can be queried in JSON format and integrated with visualization via the web browser.</p><p>To emit a continuous stream of metrics data from the web layer, <span class="strong"><strong>Server-Sent Events</strong></span> (<span class="strong"><strong>SSE</strong></span>) may be a good idea due to its low overhead. Both <span class="strong"><strong>http-kit</strong></span>
<a class="indexterm" id="id476"/> (<a class="ulink" href="http://www.http-kit.org/">http://www.http-kit.org/</a>) and <span class="strong"><strong>Aleph</strong></span>
<a class="indexterm" id="id477"/> (<a class="ulink" href="http://aleph.io/">http://aleph.io/</a>), which work with Ring, support SSE today.</p></div><div class="section" title="Introspection"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec92"/>Introspection</h2></div></div></div><p>Both Oracle JDK and OpenJDK<a class="indexterm" id="id478"/> provide two GUI tools called <a class="indexterm" id="id479"/>
<span class="strong"><strong>JConsole</strong></span> (executable name <code class="literal">jconsole</code>) and <span class="strong"><strong>JVisualVM</strong></span> (executable name <code class="literal">jvisualvm</code>) that we can use to introspect into running JVMs for instrumentation data. There are also some command-line tools (<a class="ulink" href="http://docs.oracle.com/javase/8/docs/technotes/tools/">http://docs.oracle.com/javase/8/docs/technotes/tools/</a>) in the JDK to peek into the inner details of the running JVMs.</p><p>A common way to introspect a running <a class="indexterm" id="id480"/>Clojure application is to have an <span class="strong"><strong>nREPL</strong></span> (<a class="ulink" href="https://github.com/clojure/tools.nrepl">https://github.com/clojure/tools.nrepl</a>) service running so that we can connect <a class="indexterm" id="id481"/>to it later using an nREPL client. Interactive introspection over nREPL using the Emacs editor (embedded nREPL client) is popular among some, whereas others prefer to script an nREPL client to carry out tasks.</p><div class="section" title="JVM instrumentation via JMX"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec30"/>JVM instrumentation via JMX</h3></div></div></div><p>The JVM has a <a class="indexterm" id="id482"/>built-in mechanism to introspect managed resources via the extensible <span class="strong"><strong>Java Management Extensions</strong></span> (<span class="strong"><strong>JMX</strong></span>) <a class="indexterm" id="id483"/>API. It provides a way for application maintainers to expose manageable resources as "MBeans". Clojure has an easy-to-use <code class="literal">contrib</code> library <a class="indexterm" id="id484"/>called <code class="literal">java.jmx</code> (<a class="ulink" href="https://github.com/clojure/java.jmx">https://github.com/clojure/java.jmx</a>) to access JMX. There is a decent amount of open source tooling for visualization of JVM instrumentation data via JMX, such as <code class="literal">jmxtrans</code> and <code class="literal">jmxetric</code>, which integrate with Ganglia and Graphite.</p><p>Getting quick memory<a class="indexterm" id="id485"/> stats of the JVM is pretty easy using Clojure:</p><div class="informalexample"><pre class="programlisting">(let [^Runtime r (Runtime/getRuntime)]
  (println "Maximum memory" (.maxMemory r))
  (println "Total memory" (.totalMemory r))
  (println "Free memory" (.freeMemory r)))
Output:
Maximum memory 704643072
Total memory 291373056
Free memory 160529752</pre></div></div></div></div>
<div class="section" title="Profiling"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Profiling</h1></div></div></div><p>We briefly discussed<a class="indexterm" id="id486"/> profiler types in <a class="link" href="ch01.html" title="Chapter 1. Performance by Design">Chapter 1</a>, <span class="emphasis"><em>Performance by Design</em></span>. The JVisualVM tool we discussed with respect to introspection in the previous section is also a CPU and memory profiler that comes bundled with the JDK. Let's see them in action— consider the following two Clojure functions that stress the CPU and memory respectively:</p><div class="informalexample"><pre class="programlisting">(defn cpu-work []
  (reduce + (range 100000000)))

(defn mem-work []
  (-&gt;&gt; (range 1000000)
       (map str)
       vec
       (map keyword)
       count))</pre></div><p>Using JVisualVM is pretty easy—open the Clojure JVM process from the left pane. It has sampler and regular profiler styles of profiling. Start profiling for CPU or memory use when the code is running and wait for it to collect enough data to plot on the screen.</p><div class="mediaobject"><img alt="Profiling" src="graphics/3642_06_03.jpg"/></div><p>The following shows <a class="indexterm" id="id487"/>memory profiling in action:</p><div class="mediaobject"><img alt="Profiling" src="graphics/3642_06_04.jpg"/></div><p>Note that JVisualVM is a very simple, entry-level profiler. There are several commercial JVM profilers on the market for sophisticated needs.</p><div class="section" title="OS and CPU/cache-level profiling"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec93"/>OS and CPU/cache-level profiling</h2></div></div></div><p>Profiling only the JVM may not <a class="indexterm" id="id488"/>always tell the whole story. Getting down to OS and hardware-level profiling often provides better insight into what is going on with the application. On Unix-like operating systems, command-line tools such as <code class="literal">top</code>, <code class="literal">htop</code>, <code class="literal">perf</code>, <code class="literal">iota</code>, <code class="literal">netstat</code>, <code class="literal">vista</code>, <code class="literal">upstate</code>, <code class="literal">pidstat</code> etc can help. Profiling the CPU for cache misses and other information is a useful source to catch performance issues. Among open source tools for <a class="indexterm" id="id489"/>Linux, <span class="strong"><strong>Likwid </strong></span>(<a class="ulink" href="http://code.google.com/p/likwid/">http://code.google.com/p/likwid/</a> and <a class="ulink" href="https://github.com/rrze-likwid/likwid">https://github.com/rrze-likwid/likwid</a>) is small yet effective for Intel and AMD processors; <span class="strong"><strong>i7z</strong></span> (<a class="ulink" href="https://code.google.com/p/i7z/">https://code.google.com/p/i7z/</a> <a class="indexterm" id="id490"/>and <a class="ulink" href="https://github.com/ajaiantilal/i7z">https://github.com/ajaiantilal/i7z</a>) is specifically for Intel processors. There are also dedicated commercial tools such as <span class="strong"><strong>Intel VTune Analyzer</strong></span>
<a class="indexterm" id="id491"/> for more elaborate needs.</p></div><div class="section" title="I/O profiling"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec94"/>I/O profiling</h2></div></div></div><p>Profiling I/O may <a class="indexterm" id="id492"/>require special<a class="indexterm" id="id493"/> tools too. Besides <code class="literal">iota</code> and <code class="literal">blktrace</code>, <code class="literal">ioping</code> (<a class="ulink" href="https://code.google.com/p/ioping/">https://code.google.com/p/ioping/</a> and <a class="ulink" href="https://github.com/koct9i/ioping">https://github.com/koct9i/ioping</a>) is useful to measure real-time I/O latency on Linux/Unix systems. The <span class="strong"><strong>vnStat</strong></span> tool<a class="indexterm" id="id494"/> is useful to monitor and log network <a class="indexterm" id="id495"/>traffic on Linux. The IOPS of a storage device may not tell the whole truth unless it is accompanied by latency information for different operations, and how many reads and writes can simultaneously happen.</p><p>In an I/O bound workload one has to look for the read and write IOPS over time and set a threshold to achieve optimum performance. The application should throttle the I/O access so that the threshold is not crossed.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Summary</h1></div></div></div><p>Delivering high-performance applications not only requires care for performance but also systematic effort to measure, test, monitor, and optimize the performance of various components and subsystems. These activities often require the right skill and experience. Sometimes, performance considerations may even bring system design and architecture back to the drawing board. Early structured steps taken to achieve performance go a long way to ensuring that the performance objectives are being continuously met.</p><p>In the next chapter, we will look into performance optimization tools and techniques.</p></div></body></html>