<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Data Processing Using the Lambda Pattern</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data Processing Using the Lambda Pattern</h1>
                </header>
            
            <article>
                
<p><span>This chapter describes the Lambda pattern, which is not to be confused with AWS Lambda functions. The Lambda architecture consists of two layers, typically used in data analytics processing. The two layers include a speed layer to calculate data in near-real time and a batch layer that processes vast amounts of historical data in batches.</span></p>
<p>Because&#160;serverless platforms allow us to scale horizontally very quickly, and since it's simple to store large amounts of data, the Lambda pattern is well suited for a serverless implementation. Lambda architectures are relatively new, coming onto the scene with the advent of big data processing and the desire to see the results of processing sooner than was previously available using batch systems such as Hadoop. This type of architecture or pattern is especially interesting since there are so many components involved in making it work, which we'll walk through using an example application that will calculate average prices for the cryptocurrencies Bitcoin and Ethereum.</p>
<p>By the end of this chapter, you can expect to have learned the following:</p>
<ul>
<li>A thorough understanding of the Lambda architecture and when it may be appropriate to use</li>
<li><span><span>What tooling and options are available when designing a Lambda architecture in a serverless environment</span></span></li>
<li><span>How to create a speed layer for processing a stream of cryptocurrency prices</span></li>
<li><span>How to develop a batch layer for processing historical prices</span></li>
<li><span>Alternate implementations&#160;and tooling when building a serverless Lambda architecture</span></li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducing the lambda architecture</h1>
                </header>
            
            <article>
                
<p>To the best of my knowledge,&#160;<span>Nathan Martz, author of Apache Storm,&#160;</span><span>first introduced&#160;</span>the lambda architecture in a 2011 blog post. You can read the post yourself at&#160;<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html</a>. In this post, Nathan proposes a new type of system that can calculate historical views of large datasets alongside a real-time layer that can answer queries for real or near-real-time data. He labels these two layers the batch layer and the real-time layer.</p>
<p>The Lambda architecture was derived from trying to solve the problem of answering queries for data that is continuously updated. It's important to keep in mind the type of data we're dealing with here. Streaming data in this context are factual records. Some examples of streaming factual data are the following:</p>
<ul>
<li>The temperature at a given location at a given time</li>
<li>An HTTP log record from a web server</li>
<li>The price of Bitcoin from a given exchange at a given time</li>
</ul>
<p>You can imagine the case where a temperature sensor is taking measurements in a given location and sending those readings somewhere every 5 seconds. If the temperature reading on January 31, 2018, at 12:00:00 was 75.4 °F, that fact should never change. A reading 5 seconds later may be 75.5 °F, but that does not nullify the prior reading. In this and other cases, we are working with an append-only data stream of facts.</p>
<p>Using our temperature analogy, imagine that we need to answer questions about this data such as the following:</p>
<ul>
<li>What was the average weekly temperature since 2010?</li>
<li>What was the average monthly temperature since 2000?</li>
<li>What were the daily high and low temperatures over the past year?</li>
<li>What is the temperature trend over the past 2 hours?</li>
</ul>
<p>Not only are we working with immutable data, but there is a time domain to consider as well in the queries to which we need to respond. If we had a naive implementation, we could store each piece of data in a relational database and perform queries on demand. There are&#160;31,540,000 seconds in a year. If our system was uploading measurements every 5 seconds, that is&#160;6,308,000 each year. Now, assume that we need to keep track of 10,000 different sensors around the world. This means our system would be adding&#160;63,080,000,000 new records each year.</p>
<p>The initial challenge using a relational database would simply be finding a subset of those records for a particular location, for example, <kbd>SELECT * FROM temperatures where location_id = 1234</kbd>. Of course, we can undertake this type of query quickly using indexes, but there are significant limitations and trade-offs when dealing with billions or trillions of rows.</p>
<p>The second challenge would be performing calculations to get the right answer (that is, average temperature by week, high-low temperature each day). If our query was pulling data from 20 years ago until today, that would mean a lot of disk access and a significant load on the database, presuming the analytical query could be done in SQL.</p>
<p>Admittedly, there are systems that can deal with this level of scale, such as data warehouses or NoSQL data stores. However, data warehouses are not designed for real-time queries. NoSQL systems may be better at storing large amounts of data, but they lack flexibility or ability when it comes to running calculations on that data.</p>
<p>What is the solution when we have the level of scale of a data warehouse, a continually updated data stream and the requirement to serve queries in real time? This is where the Lambda pattern can help. Comprised of a batch layer and speed layer, the Lambda pattern is designed to solve this problem of responding to queries in real time, pulling data from both the batch layer and speed layer outputs.&#160;</p>
<div class="packt_infobox">Technically speaking, the view layer is a separate component of this architecture. As mentioned at the start of this chapter, there are many moving parts and components to a serverless implementation of the lambda pattern. For this reason, I won't be discussing the view layer in much detail so we can focus on the data portion of this pattern, which is more in tune with this chapter's theme. For a discussion of frontend applications in serverless systems, I'll refer you to <a href="svrls-dsnptn-bstprac_ch02.html" target="_blank">Chapter 2</a>, <em>A Three-Tier Web Application Using RE</em><em>ST</em>.</div>
<div><span>The&#160;architecture&#160;is shown in the following diagram:</span></div>
<div class="CDPAlignCenter CDPAlign"><img src="images/9358ca3b-b4c3-4c7e-9faf-8fd1b0e46415.jpg" style="width:52.00em;height:21.83em;"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Batch layer</h1>
                </header>
            
            <article>
                
<p>When the lambda architecture was proposed, Hadoop was already widely adopted and regarded as a proven technology.&#160;<span>Hadoop is a linearly scalable system and can easily churn through terabytes of data in a reasonable amount of time to calculate nearly anything from your dataset. Here, <em>reasonable</em>&#160;may mean a job that runs for a few hours in the middle of the night so that new views of your data are ready first thing in the morning.</span></p>
<p>Using our temperature monitoring analogy, a day's worth of data will require a new batch job run if we need to calculate the average temperature in the month or year. Also, imagine we wanted to calculate trends day by day, month by month, or year by year. Whenever a new batch of daily temperatures is completed, our system would need to perform some work to calculate the pre-materialized views. By pre-calculating all of this data, any query would just look up the answer on demand without needing to calculate anything.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Speed layer</h1>
                </header>
            
            <article>
                
<p>A batch layer by itself isn't anything new. The problem we're trying to solve here is answering queries quickly and up to date with a real-time view of our data stream. The magic with the Lambda pattern is the combination of the batch layer in conjunction with a speed layer.</p>
<p>The speed layer is a constantly updating system that processes new data from the data stream in real time. The calculations here will be the same as in the batch later, but it only works on a small subset of data as it arrives from the data stream. For example, to get the daily high temperature for a given location since 2015 in response to such a query, our system would do the following:</p>
<ol>
<li>Fetch daily high temperatures from January 1, 2015, until yesterday from the batch layer</li>
<li>Fetch the high temperature from today from the speed layer</li>
<li>Merge the two datasets into one to present back to the user</li>
</ol>
<p>Also note that in such a system, we could go even further. Our view layer could display the historical data in one area of the application and present the real-time information separately, which could be continually updated using a WebSocket connection or polling. By separating out these two layers, many options open up regarding application development and interaction.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Lambda serverless architecture</h1>
                </header>
            
            <article>
                
<p>While the overall design and theme of a lambda architecture remain the same as a traditional system, there are variations and adaptations that we need to make. Perhaps more importantly, there are many different ways to implement this pattern using serverless systems or, at the very least, managed services.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Streaming data producers</h1>
                </header>
            
            <article>
                
<p><span>A</span>ny system must start with data to process. On serverless platforms, there are multiple choices for streaming systems. Azure, Google Compute, and AWS all offer some form of streaming systems. I mentioned these in <a href="svrls-dsnptn-bstprac_ch06.html" target="_blank">Chapter 6</a>,&#160;<em>Asynchronous Processing with the Messaging Pattern,</em> when discussing the differences between queues and streams:</p>
<ul>
<li><span><strong>Azure</strong>: Event Hubs</span></li>
<li><span><strong>AWS</strong>: Kinesis</span></li>
<li><span><strong>Google Compute Cloud</strong>: Cloud Dataflow</span></li>
</ul>
<p>It's worth briefly touching on the topic of queues versus streams again. As mentioned in <a href="svrls-dsnptn-bstprac_ch06.html" target="_blank">Chapter 6</a>,&#160;<em>Asynchronous Processing with the Messaging Pattern</em>, one of the main differentiators is that queues are primarily designed for once-only processing. That is, once a message is pulled from a queue, no other consumer will see it. Data in a stream, on the other hand, has a given lifetime and cannot be removed by any consumer of that data. For example, a stream can set data to expire after 24 hours, or after 30 days. At any point in time, one or more readers can come along and begin reading data from the stream. It's up to the readers to keep track of where they are in the history of a stream. A new reader may start at the beginning of the stream, in the middle, or at the end.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data storage</h1>
                </header>
            
            <article>
                
<p>Since there are two distinct layers in this pattern, storage choices will likely be different since the two layers are drastically different in their data requirements. The batch layer requires extreme scalability and should perform well for a high number of concurrent reads during batch processing. The speed layer, on the other hand, doesn't need to store as much data but should be extremely fast for both reads and writes.</p>
<p>In many examples of this pattern, you'll see references to <strong>Hadoop Filesystem</strong> (<strong>HDFS</strong>) for storing historical data and NoSQL databases for real-time data. While it's near impossible to say what you should pick, it is possible to speak to some of your options.</p>
<p>Cloud storage systems such as AWS S3 or Google Cloud Storage were designed to fill a similar role as HDFS, that is, to store practically as much data as you need. The advantages of storing plain files on services such as this are that it's straightforward, requires almost no management, and is very durable. What I like about using flat or structured files is that it becomes possible to process the data later and store it in a different system such as a database. Also, there are a myriad of serverless or hosted systems that you can leverage that read data from these systems. Focusing only on AWS, the following systems can perform batch processing or analytical queries on S3 data:</p>
<ul>
<li><strong>Elastic MapReduce</strong> (<strong>EMR)</strong></li>
<li>Athena</li>
<li>Redshift</li>
</ul>
<p>DynamoDB or other NoSQL are also options for historical data. Azure Cosmos DB and Google Bigtable are other services that I cannot speak to directly but that are options if you're building on top of those cloud providers.&#160;</p>
<div class="packt_tip"><span>At least in the case of DynamoDB, special consideration should be made since</span> read <span>and write throughput needs to be carefully considered to maintain a workable system.</span></div>
<p>For the speed layer, there are also multiple tools you can use. DynamoDB is a viable choice since it's linearly scalable and you should have a fair idea of the read and write capacity needed. Managed Redis services such as AWS ElastiCache or Azure Redis Cache are also decent choices since Redis is exceptionally performant and the dataset is limited.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Computation in the speed layer</h1>
                </header>
            
            <article>
                
<p>It makes sense that, since we're using serverless systems, our serverless functions will perform any computations necessary. Serverless functions are a natural choice, for what should be obvious reasons at this point. Running functions on demand or in response to new data arriving on our streams is incredibly simple. Additionally, FaaS allows us to scale horizontally.</p>
<p><span>In a lambda architecture, we may need to calculate many different metrics at the same time, from the same data stream. Doing this from a <kbd>single</kbd> serverless function could be possible. However, in the case where the computation is heavier, we may need to split out the computation into multiple functions, each calculating their own set of metrics from the same stream. Numerous readers allow&#160;us to scale out horizontally and provide the flexibility needed&#160;when data changes or new metrics need to be calculated.</span></p>
<p>While serverless functions are a natural choice and easily understood, there are other options. On AWS, it's possible to use Spark Streaming from within the EMR system. Spark Streaming is purpose-built for this type of workload. In the case that your data stream outgrows the limitations of cloud functions such as Lambda, moving to Spark Streaming is a good alternative.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Computation in the batch layer</h1>
                </header>
            
            <article>
                
<p>Many lambda architecture systems will rely on Hadoop or Spark for the batch layer. Since we don't want to manage a cluster ourselves, we'll have to pick some other serverless system or, at the very least, a managed service. There are a variety of options here.</p>
<p>First, it's possible to implement our MapReduce system entirely using&#160;serverless technologies. You'll read about this in <a href="svrls-dsnptn-bstprac_ch08.html">Chapter 8,</a>&#160;The&#160;<em>MapReduce Pattern</em><a href="svrls-dsnptn-bstprac_ch08.html"></a>. If you'd rather not build a MapReduce system, there are other services that you can leverage. Both Spark and Hadoop are available within AWS EMR. HDInsight from Azure provides the same or similar functionality to EMR.</p>
<p>Batch processing is a solved problem nowadays, and you should have no problems finding a solution that works for your needs. Since there are so many options for batch processing, you may find it challenging to narrow down your choices.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Processing cryptocurrency prices using lambda architecture</h1>
                </header>
            
            <article>
                
<p>In this chapter, our example application will perform a single task of reading prices in real time for a variety of cryptocurrencies and calculating the average prices by the minute, hour, and day. Of course, this isn't all that useful in the real world because there is so much data on cryptocurrencies already. However, this presents an excellent scenario and dataset for an example application to illustrate this pattern. As is usual in this book, I'll build the application on top of AWS with Python. It's also important to note that none of the concepts are unique to AWS or Python and that this example application is portable to other languages and cloud providers.</p>
<div class="page">
<div class="section">
<div class="layoutArea">
<div class="column packt_infobox">
<p><span>You can find the code for this chapter at&#160;<a href="https://github.com/brianz/serverless-design-patterns/tree/master/ch7">https://github.com/brianz/serverless-design-patterns/tree/master/ch7</a>.<a href="https://github.com/brianz/serverless-design-patterns/tree/master/ch7"></a></span></p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">System architecture</h1>
                </header>
            
            <article>
                
<p>The architecture of this is perhaps the most complex in this book, even though the implementation itself is relatively simple. As mentioned earlier in this chapter, there are many moving parts in a lambda architecture since it is two distinct systems that work side by side, with each having its own unique set of services. I show a high-level architecture in the following diagram:</p>
<div class="packt_infobox"><span>This design is somewhat contrived and overly simplified. A system with true scale likely would not be able to get away with a serverless function that calculated historical prices by year, as I do in the batch layer. To fit this example application into a single chapter, I had to make the system relatively simple to demonstrate the pattern and techniques. While this may be an oversimplification of an actual big data system, it does show that the design works on a serverless platform and is even applicable to a decent-sized data problem.</span></div>
<div class="CDPAlignCenter CDPAlign"><img src="images/cd62972d-2a1f-42c0-90a2-9f48ff2c47f0.jpg"/></div>
<p>I'll break this down piece by piece and describe how each layer and component works. While there may be a lot going on in this design, each part is quite simple. Most of the complexity of this system comes from the system setup and concepts, instead&#160;of application code.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data producer</h1>
                </header>
            
            <article>
                
<p>To start, we need to have some data to process. In a real system where you are creating the data or wish to send some data <em>out</em> from your system, this isn't much of an issue. In our case, we need to pull data in real time from somewhere, and I've chosen the public API from GDAX (<a href="https://www.gdax.com">https://www.gdax.com</a>), which is a digital exchange for cryptocurrencies from Coinbase. This API is suitable for our example application mainly because there are many transactions and there is a WebSocket endpoint that we can subscribe to. A simple script that subscribes to the GDAX WebSocket API and publishes those messages to our Kinesis stream will serve as our data producer.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Speed layer</h1>
                </header>
            
            <article>
                
<p>The speed layer is also relatively simple. Each message published to the Kinesis stream will trigger a Lambda function. This Lambda function will just write the data to DynamoDB so that we can serve data in real time for any queries. With this design, we're set up for a decent amount of real-time load and concurrency. The data producer will deliver data at a reasonably fast rate, perhaps a few messages per second. If there was a burst of traffic and the speed layer started seeing tens or hundreds of messages per second, it would not be a problem.</p>
<p>Since serverless functions and databases such as DynamoDB scale linearly, the speed layer can absorb practically any amount of real-time traffic. There are, of course, provisioning and throughput concerns, as well as maximum concurrent limits to contend. However, these issues are just configuration settings that you can quickly change and increase as needed.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Batch layer</h1>
                </header>
            
            <article>
                
<p>Our batch layer gets a bit more interesting. Some of the details are AWS-centric, but you can carry the general idea across cloud providers. AWS Kinesis Firehose is another version of Kinesis that is designed to transport data in batches to various locations. In this architecture, we'll set up the Kinesis&#160;Firehose stream to ingest data from the primary Kinesis stream. &#160;I'll also configure the Firehose stream to deliver batches of messages to S3 every minute.</p>
<div class="packt_tip">If you have heard about AWS Kinesis but the term Kinesis Firehose is new to you, don't worry. Kinesis Firehose's specialty is loading data into various services, such as S3 and Redshift. A plain Kinesis stream captures data and makes it available for consumption, but that consumption is your responsibility. Kinesis Firehose is useful when you'd like to dump the streaming data to S3 or Redshift automatically.</div>
<p>With a new file being delivered to S3 every minute, we can set up a Lambda function to trigger on that event and perform some work. The work here will be reading the list of messages, calculating the average price per currency, and writing out a new file back to S3. If you follow the flow of time and data, you should be able to see that we can extend this pattern down at different time increments - minutes, hours, days, months, and even years. The general flow and set of triggers look like this:</p>
<ul>
<li>Every minute, a Lamba function reads data stored on S3 and calculates the average price for the last minute</li>
<li>Every hour,&#160;<span>a Lamba function reads data stored on S3 and calculates the average price for the last hour</span></li>
<li>Every day,&#160;<span>a Lamba function reads data stored on S3 and calculates the average price for the previous day</span></li>
</ul>
<p>Since data is stored on S3, nothing is stopping this system from evolving to create an entirely new batch layer using more powerful tools such as Spark and Athena.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AWS resources</h1>
                </header>
            
            <article>
                
<p>In my opinion, the most complicated part of this entire system is set up all of the various resources and the interplay between them all. If you count up the number of resources we need to make this system work, the list quickly grows:</p>
<ul>
<li>Two S3 buckets</li>
<li>One Kinesis stream</li>
<li>One Kinesis Firehose stream</li>
<li>One DynamoDB table</li>
<li>Four Lambda functions</li>
<li>Multiple IAM roles</li>
</ul>
<p>Not only do we need to create all of the preceding resources, but we need to ensure they can communicate with one another. As is often the case with building with AWS, much of the work involved in managing a stack such as this is getting permissions correct so that the Lambda function can read/write from/to the right S3 buckets, DynamoDB table, or Kinesis stream.</p>
<p>You can see proof of this if we use <kbd>cloc</kbd> to count the lines of code in this application. Looking at the following output, you'll see that the amount of configuration code in <kbd>serverless.yml</kbd> is higher than the application code, with 165 lines of YAML configuration to 128 lines of Python code:</p>
<pre><strong>$ cloc . </strong><br/><strong>       4 text files.</strong><br/><strong>       4 unique files. </strong><br/><strong>       2 files ignored.</strong><br/><br/><strong>github.com/AlDanial/cloc v 1.72 T=0.03 s (96.4 files/s, 12465.5 lines/s)</strong><br/><strong>---------------------------------------------------------------------------</strong><br/><strong>Language         files    blank     comment     code</strong><br/><strong>---------------------------------------------------------------------------</strong><br/><strong>YAML             1        6         8           157</strong><br/><strong>Python           2        59        22          128</strong><br/><strong>---------------------------------------------------------------------------</strong><br/><strong>SUM:             3        65        30          285</strong><br/><strong>---------------------------------------------------------------------------</strong></pre>
<p>I will walk through some interesting bits of the <kbd>serverless.yml</kbd> file here:</p>
<pre style="padding-left: 30px">service: gdax-lambda-arch<br/><br/>provider:<br/>  name: aws<br/>  runtime: python3.6<br/>  stage: ${env:ENV}<br/>  region: ${env:AWS_REGION}<br/>  iamRoleStatements:<br/>    - Effect: "Allow"<br/>      Action:<br/>        - "s3:GetObject"<br/>        - "s3:ListBucket"<br/>        - "s3:PutObject"<br/>      Resource:<br/>        - "arn:aws:s3:::brianz-gdax-${env:ENV}-firehose"<br/>        - "arn:aws:s3:::brianz-gdax-${env:ENV}-firehose/*"<br/>        - "arn:aws:s3:::brianz-gdax-${env:ENV}-results"<br/>        - "arn:aws:s3:::brianz-gdax-${env:ENV}-results/*"<br/>    - Effect: "Allow"<br/>      Action:<br/>        - "dynamodb:PutItem"<br/>      Resource: "arn:aws:dynamodb:${self:provider.region}:*:table/brianz-gdax-${env:ENV}-realtime"</pre>
<p>First, we need to ensure our Lambda functions will have access to the various resources. In the preceding&#160;<kbd>iamRoleStatements</kbd>, I'm giving various permissions for all Lambda functions in this stack to the two S3 buckets we'll use as well as DynamoDB. This shouldn't come as a big surprise. Our Lambda functions will be reading and writing data from and to S3. Likewise, our speed layer will be writing new records to DynamoDB.</p>
<p>Next, I'll walk through how to create a Kinesis stream that we can write to from our data producer, which, in turn, forwards messages on to a Kinesis Firehose delivery stream. Be warned; this is a bit raw and can seem complicated. I'll break this down bit by bit, hopefully in an order that is comprehensible.</p>
<p>The first step is creating a Kinesis stream. This part is straightforward using the <kbd>Resources</kbd> section of <kbd>serverless.yml</kbd>, which is straight-up CloudFormation. In this case, we only need a single shard since our application throughput is reasonably small. If the amount of data you're pushing is larger, you can add additional throughput by increasing the number of shards. The following code snippet is the <kbd>resources</kbd> section from <kbd>serverless.yml</kbd> and shows how I'm creating the Kinesis stream:</p>
<pre style="padding-left: 30px">resources:<br/>  Resources:<br/>    # This is the stream which the producer will write to. Any writes  <br/>     will trigger a lambda<br/>    # function. The Lambda function will need read access to this <br/>    stream.<br/>    GdaxKinesisStream:<br/>      Type: AWS::Kinesis::Stream<br/>      Properties:<br/>        Name: brianz-gdax-${env:ENV}-kinesis-stream<br/>        RetentionPeriodHours: 24<br/>        ShardCount: 1</pre>
<p>Next up is the bit which is a bit more complicated. Kinesis Firehose is still a Kinesis stream, that behaves a bit differently as I mentioned earlier. In a standard Kinesis stream, you are responsible for doing something with the messages that producers push onto the stream. Kinesis Firehose, on the other hand, will automatically deliver a batch of messages to some destination. Your choices for final destinations are as follows:</p>
<ul>
<li>AWS S3</li>
<li>AWS Redshift</li>
<li>AWS Elasticsearch service</li>
<li>Splunk</li>
</ul>
<p>We can create a Kinesis Firehose stream by adding some CloudFormation code in the <kbd>Resources</kbd> block. What we need to create via CloudFormation is the following:</p>
<ul>
<li>A Firehose stream that received data from the previous Kinesis stream and batch write data to S3 every 60 seconds</li>
<li>An IAM role that grants access to Firehose to read/write to/from our S3 buckets and also read the Kinesis stream</li>
</ul>
<p>This CloudFormation code is a bit verbose. Rather than putting the entire code block here, I'll refer you to the GitHub repository. You can read the full details of setting up the Kinesis Firehose stream at the following URL:&#160;<a href="https://github.com/brianz/serverless-design-patterns/blob/master/ch7/serverless/serverless.yml#L47-L113">https://github.com/brianz/serverless-design-patterns/blob/master/ch7/serverless/serverless.yml#L47-L113</a>.</p>
<p>The data source for this Firehose stream is the primary Kinesis stream, which I named <kbd>GdaxKinesisStream</kbd>. You can see the configuration to use this stream as a data source in the&#160;<kbd>KinesisStreamSourceConfiguration</kbd> and&#160;<kbd>DeliveryStreamType</kbd> keys. These two settings say that we're going to be using a Kinesis stream as a data source, as opposed to putting data directly on this Firehose stream via API calls. It also tells the Firehose stream where to find this source Kinesis stream via the <kbd>KinesisStreamSourceConfiguration</kbd>, which can be a bit confusing.</p>
<p>Here, <span><kbd>KinesisStreamSourceConfiguration</kbd> is comprised of two keys,&#160;<kbd>KinesisStreamARN</kbd> and&#160;</span><kbd>RoleARN</kbd>. The former, <kbd>KinesisStreamARN</kbd>, refers to the location of the Kinesis stream we're connecting to. <kbd>RoleARN</kbd>, on the other hand, has to do with permissions. This referenced role must permit for the reading of the source Kinesis stream. It's a bit too much to cover here, but if you look at the entirety of the configuration, it should make some amount of sense.</p>
<p>Now that we've taken care of the input source, we need to set up the S3 destination configuration in the&#160;<kbd>S3DestinationConfiguration</kbd> key. This configuration is analogous to that of the source stream; we need to give our Firehose stream the data on where to write data with the&#160;<span><kbd>BucketARN</kbd> and also give it a role with the necessary access.</span></p>
<p>The other interesting and important part of the <kbd><span>S3DestinationConfiguration</span></kbd> is that it's configured to write data to S3 every 60 seconds or every 5 MB, whichever comes first. Since the GDAX WebSocket feed isn't all that chatty, we can count on hitting the 60-second limit before the buffer in Firehose reaches 5 MB.</p>
<p>From here, we can turn our attention to the Lambda functions that will be running our application code. I've implemented four different Lambda functions, which will handle the following:</p>
<ul>
<li>Single events from the Kinesis stream</li>
<li>S3 objects created every 60 seconds from Kinesis Firehose</li>
<li>S3 objects created from the aggregated minute views of data</li>
<li>S3 objects created from the aggregated hour views of data</li>
</ul>
<p>The configuration of these four Lambda functions is shown as follows:</p>
<pre style="padding-left: 30px">functions:<br/>  ProcessPrice:<br/>    handler: handler.single<br/>    memorySize: 256<br/>    timeout: 3<br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn:<br/>            Fn::GetAtt:<br/>              - GdaxKinesisStream<br/>              - Arn<br/>    environment:<br/>      TABLE_NAME: brianz-gdax-${env:ENV}-realtime<br/>  CalculateMinuteView:<br/>    handler: handler.minute<br/>    memorySize: 256<br/>    timeout: 10<br/>    events:<br/>      - s3:<br/>          bucket: brianz-gdax-${env:ENV}-firehose<br/>          event: s3:ObjectCreated:*<br/>    environment:<br/>      DESTINATION_BUCKET: brianz-gdax-${env:ENV}-results<br/>  CalculateHourlyView:<br/>    handler: handler.hourly<br/>    memorySize: 512<br/>    timeout: 60<br/>    events:<br/>      - s3:<br/>          bucket: brianz-gdax-${env:ENV}-results<br/>          event: s3:ObjectCreated:*<br/>          rules:<br/>            - suffix: '59-minute.json'<br/>  CalculateDailyView:<br/>    handler: handler.daily<br/>    memorySize: 1024<br/>    timeout: 300<br/>    events:<br/>      - s3:<br/>          bucket: brianz-gdax-${env:ENV}-results<br/>          event: s3:ObjectCreated:*<br/>          rules:<br/>            - suffix: '23-hour.json'</pre>
<p>You can see that the first function, <kbd>ProcessPrice</kbd>, is triggered upon delivery of a message onto our Kinesis stream. Once this function executes, its job is done. There is no other interaction with this function and any other function.</p>
<p>The next three functions work in coordination. This process starts when <kbd>CalculateMinuteView</kbd> is triggered when the Firehose stream delivers a new batch of messages every 60 seconds to S3. This function will calculate the average prices using all of the delivered messages and upload a new file to S3 named <kbd>MM-minute.json</kbd>, where <kbd>MM</kbd> is a numerical representation of the minute calculated (<kbd>00, 01...59</kbd>).</p>
<p>Once we reach the end of an hour, this function will write a file named <kbd>59-minute.json</kbd>. Since that file signifies the end of an hour, we can trigger the <kbd>CalculateHourlyView</kbd> function to calculate the average prices for the past hour. This function produces files named <kbd>HH-hour.json</kbd>, where <kbd>HH</kbd> represents the 24 hours in a day <kbd>(00, 01...23)</kbd>. The same strategy holds true for hours and days. Once a file named <kbd>23-hour.json</kbd> arrives, it's time to calculate the daily average prices from&#160;<kbd>CalculateDailyView</kbd>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data producer</h1>
                </header>
            
            <article>
                
<p>The following example code shows what a simple client application looks like if you're interested in prices for Bitcoin and Ethereum. This code doesn't do anything other than deserializing the JSON payload from each message and printing it to the Terminal:</p>
<pre style="padding-left: 30px">from websocket import WebSocketApp<br/>from json import dumps, loads<br/><br/>URL = "wss://ws-feed.gdax.com"<br/><br/>def on_message(_, message):<br/>    json_message = loads(message)<br/>    print(json_message)<br/><br/><br/>def on_open(socket):<br/>    products = ["BTC-USD", "ETH-USD"]<br/>    channels = [<br/>        {<br/>            "name": "ticker",<br/>            "product_ids": products,<br/>        },<br/>    ]<br/>    params = {<br/>        "type": "subscribe",<br/>        "channels": channels,<br/>    }<br/>    socket.send(dumps(params))<br/><br/><br/>def main():<br/>    ws = WebSocketApp(URL, on_open=on_open, on_message=on_message)<br/>    ws.run_forever()<br/><br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Executing this code, I'll see payloads printing out whenever there is a buy or sell transaction for either of the two currencies to which I've subscribed:</p>
<pre style="padding-left: 30px">{'best_ask': '8150', <br/> 'best_bid': '8149.99', <br/> 'high_24h': '8302.73000000', <br/> 'last_size': '0.33846794', <br/> 'low_24h': '8150.00000000', <br/> 'open_24h': '8021.01000000', <br/> 'price': '8150.00000000', <br/> 'product_id': 'BTC-USD', <br/> 'sequence': 5434939366, <br/> 'side': 'buy', <br/> 'time': '2018-03-18T22:48:35.185000Z', <br/> 'trade_id': 39905775, <br/> 'type': 'ticker', <br/> 'volume_24h': '29375.86790154', <br/> 'volume_30d': '633413.03952202'}</pre>
<p>To get this data into our system, we need a few extra lines of code to publish data into an AWS Kinesis stream. The changes to the preceding client code are quite simple. Upon receiving a new message, I'll check whether the payload is of the correct type, merely by looking for the time key. Some messages that we get back are confirmations to our subscription and do not include the time key, indicating some other message other than a trade. The following code shows these changes and how I use the <kbd>boto3</kbd> library to publish data to a Kinesis stream:</p>
<pre style="padding-left: 30px">kinesis = boto3.client('kinesis')<br/><br/>def on_message(_, msg):<br/>    json_msg = json.loads(msg)<br/>    if 'time' in json_msg:<br/>        print('Publishing...')<br/>        response = kinesis.put_record(<br/>                StreamName='brianz-gdax-bz-kinesis-stream',<br/>                PartitionKey=json_msg['time'],<br/>                Data=msg + '|||',<br/>        ) <br/>        print(response)<br/>    else: <br/>        print(json_msg)</pre>
<p>This part of our application is completely standalone. I ran this small script on an EC2 instance inside of a <kbd>screen</kbd> session, so the code continued to run when I logged out. This&#160;<span>implementation&#160;</span>isn't suitable for a real production system, but it worked just fine for the few days that I ran it. If I were doing this for an actual production system, I'd run this code with some daemon management systems such as <kbd>supervisord</kbd>, <kbd>upstart</kbd>, or&#160;<kbd>runit</kbd>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Speed layer</h1>
                </header>
            
            <article>
                
<p>Our speed layer is the simplest part of the entire system. With the configuration in place<span>—</span>a <kbd>single</kbd> Lambda function to execute whenever a new stream message arrives<span>—</span>the only work we need to do is decode the data from the message payload, calculate the DynamoDB partition key, and write it to DynamoDB. The following code block shows all of this work in the <kbd>single</kbd> function, which is processing a single message from AWS Kinesis:</p>
<div class="packt_tip">This code is all located in a single <kbd>handler.py</kbd> function, which goes against a best practice of splitting up application code and decoupling application logic from the cloud-specific bits. However, this application code is for demonstration purposes, and I can get away with breaking some rules for the sake of clarity and brevity. If there were a real system rather than a demo, I would be much more deliberate and organized with this code. Some of the import statements will be used in the batch layer code.</div>
<pre style="padding-left: 30px">import json<br/>import os<br/>import os.path<br/><br/>from base64 import b64decode<br/>from datetime import datetime<br/><br/>from lambda_arch.aws import (<br/>        get_matching_s3_keys,<br/>        list_s3_bucket,<br/>        read_body_from_s3,<br/>        write_to_dynamo_table,<br/>        write_to_s3,<br/>)<br/><br/>def single(event, context):<br/>    """Process a single message from a kinesis stream and write it to <br/>       DynamoDB"""<br/>    record = event['Records'][0]<br/>    data = record['kinesis']['data']<br/><br/>    # Append on a delimiter since we need to unpack messages which are <br/>    concatenated together when<br/>    # receiving multiple messages from Firehose.<br/>    data = json.loads(b64decode(data).decode().rstrip('|||'))<br/><br/>    # Create our partition key<br/>    data['productTrade'] = '{product_id}|{time}|<br/>    {trade_id}'.format(**data)<br/><br/>    write_to_dynamo_table(os.environ['TABLE_NAME'], data)</pre>
<p>The result of this code is that every single GDAX transaction we're interested in ends up in our DynamoDB table. The following screenshot shows a subset of the data stored. With the data in DynamoDB, our view layer can quickly look up a set of rows for a particular time range:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/8cc2640e-7f08-4798-8e9d-a8ab7510e8b1.png"/></div>
<p>Prices for DynamoDB are more dependent on the reads and writes you need for this system. Even though there is no code to trim out data we no longer need (that is, data that is historical and handled the batch layer), it's not a huge concern. Still, if there were a production system, you'd want to consider some techniques on expiring data that is no longer needed.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Batch layer</h1>
                </header>
            
            <article>
                
<p>Whereas our speed layer is only interacting with DynamoDB, our batch layer will just be interacting with S3. There are two distinct types of functions in this layer—the functions that respond to S3 objects that arrive from Firehose, and functions that respond to S3 objects coming from Lambda functions. There isn't all that much difference, but it's important to point out the two different categories.</p>
<p>The following code block, taken from <kbd>handler.py</kbd>, shows the application code that comprises our batch layer:</p>
<pre style="padding-left: 30px">def _get_bucket_and_key_from_event(event): <br/>    record = event['Records'][0]<br/>    s3 = record['s3'] <br/>    bucket = s3['bucket']['name'] <br/>    key = s3['object']['key'] <br/>    return (bucket, key)<br/><br/>def minute(event, context):<br/>    """Process an S3 object uploaded to S3 from Kinesis Firehose.<br/><br/>    The data format from Firehose is all of the messages from the  <br/>    `single` function above,<br/>    concatenated together. In order to read thse messages, we need to <br/>    decode them and split the<br/>    string by our own delimiter.<br/><br/>    """<br/>    bucket, key = _get_bucket_and_key_from_event(event)<br/>    data = read_body_from_s3(bucket, key).decode()<br/><br/>    product_prices = {}<br/><br/>    lines = [json.loads(l) for l in data.split('|||') if l]<br/>    times = []<br/><br/>    for line in lines:<br/>        # Only keep track of buys for the average price, since sells <br/>        could be sell orders which are<br/>        # never executed.<br/>        if line.get('side') != 'buy':<br/>            continue<br/><br/>        product_id = line['product_id']<br/>        price = float(line['price'])<br/><br/>        times.append(line['time'])<br/>        if product_id not in product_prices:<br/>            product_prices[product_id] = {'prices': [price]}<br/>        else:<br/>            product_prices[product_id]['prices'].append(price)<br/><br/>    if not product_prices:<br/>        return<br/><br/>    # Calculate the average for each product<br/>    for key in product_prices:<br/>        prices = product_prices[key]['prices']<br/>        product_prices[key]['average'] = sum(prices) * 1.0 / <br/>        len(prices)<br/><br/>    # Determine the most recent timestamp from the list of buys so we <br/>    can determine the key to<br/>    # write.<br/>    times.sort()<br/>    latest_time = times[-1]<br/>    latest_dt = datetime.strptime(latest_time, DT_FORMAT)<br/><br/>    destination_bucket = os.environ['DESTINATION_BUCKET']<br/>    new_key = latest_dt.strftime('%Y/%m/%d/%H/%M-minute.json')<br/>    new_payload = json.dumps(product_prices, indent=2)<br/><br/>    print('Uploading to', destination_bucket, new_key)<br/><br/>    write_to_s3(destination_bucket, new_key, new_payload)<br/><br/><br/>def _aggregate_prices(event, period='hour'):<br/>    """Aggregate average prices for a particular time slice"""<br/>    bucket, key = _get_bucket_and_key_from_event(event)<br/>    key_root = os.path.dirname(key)<br/><br/>    product_prices = {}<br/><br/>    for key in get_matching_s3_keys(bucket, prefix=key_root + '/', <br/>    suffix='-minute.json'):<br/>        data = read_body_from_s3(bucket, key).decode()<br/>        minute_prices = json.loads(data)<br/><br/>        for product_id, payload in minute_prices.items():<br/>            prices = payload['prices']<br/>            if product_id not in product_prices:<br/>                product_prices[product_id] = {'prices': prices}<br/>            else:<br/>                product_prices[product_id]['prices'].extend(prices)<br/><br/>    for key in product_prices:<br/>        prices = product_prices[key]['prices']<br/>        average_price = sum(prices) * 1.0 / len(prices)<br/>        product_prices[key]['average'] = average_price<br/><br/>    new_key = '%s-%s.json' % (key_root, period)<br/>    new_payload = json.dumps(product_prices, indent=2)<br/><br/>    print('Uploading to', bucket, new_key)<br/><br/>    write_to_s3(bucket, new_key, new_payload)<br/><br/><br/>def hourly(event, context):<br/>    _aggregate_prices(event, period='hour')<br/><br/><br/>def daily(event, context):<br/>    _aggregate_prices(event, period='day')</pre>
<p>Batch processing starts with the <kbd>minute</kbd> function. Again, this function starts when a new file arrives in S3 from our Firehose stream. At this point, we can be assured that the <kbd>minute</kbd> function is invoked once a minute based on our configuration. We don't need to go through this line by line, but we can look at the final results as well as some small tricks.</p>
<p>You may notice the line that splits data using <kbd>split('|||')</kbd>. Firehose will concatenate records together before delivering to any data source. The AWS documentation on Kinesis Firehose explicitly states this, but it's still easy to overlook:</p>
<div class="packt_quote">"For data delivery to Amazon S3, Kinesis Firehose concatenates multiple incoming records based on buffering configuration of your delivery stream and then delivers them to Amazon S3 as an S3 object. You may want to add a record separator at the end of each record before you send it to Kinesis Firehose so that you can divide a delivered S3 object to individual records."</div>
<p>The preceding has been quoted from&#160;<a href="https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html">https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html</a>.<a href="https://docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html"></a></p>
<p><span>If you look back up at the data producer, you can see that we're appending the</span> <kbd>'|||'</kbd> <span>string to each message. With this delimiter, it's possible for us to break apart individual messages in this function.</span></p>
<p>The final results that the <kbd>minute</kbd> function uploads to S3 have the following form. For each currency, it includes the list of individual buy prices along with the average price:</p>
<pre style="padding-left: 30px">{<br/>  "BTC-USD": {<br/>    "prices": [<br/>      8173.59,<br/>      8173.59,<br/>      8173.59,<br/>      8173.59,<br/>      8173.59,<br/>      8173.59,<br/>      8173.59,<br/>      8173.59,<br/>      8174.99,<br/>      8176.17,<br/>      8176.17<br/>    ],<br/>    "average": 8174.186363636362<br/>  },<br/>  "ETH-USD": {<br/>    "prices": [<br/>      533.01,<br/>      533.01,<br/>      533.01,<br/>      533.01,<br/>      533.01,<br/>      533.01<br/>    ],<br/>    "average": 533.0100000000001<br/>  }<br/>}</pre>
<p>Our <kbd>CalculateHourlyView</kbd> executes once a file named <kbd>59-minute.json</kbd>&#160;arrives in S3. The arrival of this S3 object signifies the end of an hour, so it's time to calculate the average prices for the entire hour. That work for the hour calculation, as well as the calculation of the daily average, is all wrapped up in the&#160;<kbd>_aggregate_prices</kbd> function. By using the prefix of the S3 object that triggers the functions, we'll only scan through a subset of the S3 records that are used in the calculation of the averages. For example, when a new S3 object arrives with a key name of <kbd>$BUCKET/2018/03/19/04/59-minute.json</kbd>, our application code can pick apart the&#160;<span><kbd>$BUCKET/2018/03/19/04</kbd> prefix and only scan files in that location. When an S3 object arrives named&#160;<kbd>$BUCKET/2018/03/19/23-hour.json</kbd>, the daily function will scan through files with the&#160;<kbd>$BUCKET/2018/03/19</kbd> prefix.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p>After running this system for a few days, I was able to produce many S3 files as expected, as well as keeping track of each trade in DynamoDB. As I mentioned earlier in this chapter, implementing some view layer wasn't feasible for this example system. However, querying DynamoDB is quite easy, provided primary keys and sort keys are set up correctly.&#160;Any view layer that needed to get historical data could easily grab files from S3.&#160;</p>
<p>The following screenshot shows listings of S3 files for a given hour. Each file has the format shown earlier with individual prices, along with the pre-computed average for each currency:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/84eb37ec-4bcb-429d-9d44-11d7ae582ff8.png"/></div>
<p>Looking one level up in the S3 hierarchy, we can see the hour files that contain the average prices:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/ea568325-0b31-4753-9ead-2547ea41f0ec.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the Lambda pattern at a conceptual level as well as in detail. I walked through an example implementation with serverless technologies to calculate average cryptocurrency prices for different time slices. Our example application was fed by a simple script that receives data from the GDAX WebSocket API. This data producer script published data to a single AWS Kinesis stream, which, in turn, triggered a series of events, ultimately resulting in real-time updates to DynamoDB and triggering batch jobs to calculate historical views of the minute, hourly, and daily average prices for multiple cryptocurrencies.</p>
<p>I discussed when the Lambda pattern may be appropriate and the types of data for which it's well suited. We talked through various systems and services that one may leverage when building a lambda architecture using serverless technologies. I introduced AWS Kinesis and AWS Kinesis Firehose, which are streaming systems you may leverage for real-time applications.</p>
<p>While the details of a Lambda pattern implementation can be quite intricate, readers should have a decent understanding of its advantages, disadvantages, and when they should consider it.</p>
<p>In the next chapter, I'll cover the MapReduce pattern and work through an example where we will build our very own serverless implementation of this pattern.</p>


            </article>

            
        </section>
    </div>
</body>
</html>