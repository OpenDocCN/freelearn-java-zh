- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Enabling Data Security and Governance
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用数据安全和治理
- en: In the preceding chapters, we learned how to evaluate requirements and analyze
    and apply various architectural patterns to solve both real-time and batch-based
    problems. We learned how to choose the optimal technical stack and develop, deploy,
    and execute the proposed solution. We also discussed various popular architectural
    patterns for data ingestion. However, any discussion about data architecture is
    incomplete without mentioning data governance and data security. In this chapter,
    we will focus on understanding and applying data governance and security in the
    data layer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何评估需求，以及如何分析和应用各种架构模式来解决实时和基于批处理的问题。我们学习了如何选择最佳的技术栈，并开发、部署和执行所提出的解决方案。我们还讨论了数据摄取的各种流行架构模式。然而，任何关于数据架构的讨论如果没有提及数据治理和数据安全都是不完整的。在本章中，我们将重点关注理解和应用数据层中的数据治理和安全。
- en: In this chapter, we will first discuss what data governance is, and why it is
    so important. We will also briefly discuss a few open source data governance tools
    that are available on the market. Then, we will practically demonstrate a data
    governance implementation by adding a data governance layer to a data ingestion
    pipeline. The data ingestion pipeline ID will be developed using Apache NiFi and
    data governance will be achieved using DataHub. Then, we will discuss the need
    for data security and the types of solutions that help enable it. Finally, we
    will discuss a few open source tools available to enable data security.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先将讨论数据治理是什么，以及为什么它如此重要。我们还将简要讨论市场上可用的几个开源数据治理工具。然后，我们将通过向数据摄取管道添加数据治理层来实际演示数据治理的实施。数据摄取管道的
    ID 将使用 Apache NiFi 开发，数据治理将通过 DataHub 实现。然后，我们将讨论数据安全的需求以及有助于实现它的解决方案类型。最后，我们将讨论可用于启用数据安全的开源工具。
- en: By the end of the chapter, you will know the definition of and need for a data
    governance framework. You will also know when data governance is required and
    all about the data governance framework by the **Data Governance Institute** (**DGI**).
    In addition, you will know how to implement practical data governance using DataHub.
    Finally, you will know about various solutions and tools available to enable data
    security.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解数据治理框架的定义和需求。您还将了解何时需要数据治理，以及关于数据治理框架的所有内容，包括**数据治理研究所**（**DGI**）的内容。此外，您将了解如何使用
    DataHub 实施实际的数据治理。最后，您将了解可用于启用数据安全的各种解决方案和工具。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introducing data governance – what and why
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍数据治理——是什么以及为什么
- en: Practical data governance using DataHub and NiFi
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataHub 和 NiFi 的实际数据治理
- en: Understanding the need for data security
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据安全的需求
- en: Solutions and tools available for data security
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于数据安全性的解决方案和工具
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您将需要以下内容：
- en: OpenJDK 1.11 installed on your local machine
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您本地机器上安装的 OpenJDK 1.11
- en: Docker installed on your local machine
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您本地机器上安装的 Docker
- en: An AWS account
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 AWS 账户
- en: NiFi 1.12.0 installed on your local machine
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您本地机器上安装的 NiFi 1.12.0
- en: DataHub installed on your local machine
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您本地机器上安装的 DataHub
- en: Prior knowledge of YAML is preferred
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议您具备 YAML 的相关知识
- en: 'The code for this chapter can be downloaded from this book’s GitHub repository:
    [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以从本书的 GitHub 仓库下载：[https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08)。
- en: Introducing data governance – what and why
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍数据治理——是什么以及为什么
- en: First, let’s try to understand what data governance is and what it does. In
    layman’s terms, **data governance** is the process of assigning proper authority
    and taking decisions regarding data-related matters. According to the DGI, data
    governance is defined as “*a system of decision rights and accountabilities for
    information-related processes, executed according to agreed-upon models that describe
    who can take what actions with what information, and when, under what circumstances,
    using what methods.*”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们尝试了解数据治理是什么以及它做什么。用通俗易懂的话说，**数据治理**是分配适当的权限并就数据相关事务做出决策的过程。根据DGI的定义，数据治理被定义为“*一个关于信息相关流程的决策权和问责制系统，根据已同意的模型执行，这些模型描述了谁可以在什么情况下使用什么信息采取什么行动，以及何时、在何种情况下、使用什么方法。*”
- en: As evident from this definition, it is a practice of creating definitive strategies,
    policies, and rules that define who can make what decisions or perform actions
    related to data. It also lays out guidelines about how to make decisions related
    to data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如定义所示，它是一种创建明确的策略、政策和规则的做法，这些规则定义了谁可以做出什么决定或执行与数据相关的行动。它还规定了如何做出与数据相关的决策的指南。
- en: 'Data governance considers the following aspects:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理考虑以下方面：
- en: Rules
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则
- en: Enterprise-level organizations
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业级组织
- en: Decision rights and procedures
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策权和程序
- en: Accountability
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 责任制
- en: Monitoring and controlling data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和控制数据
- en: Now that we have a basic understanding of data governance, let’s explore what
    scenarios data governance is recommended in.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对数据治理有了基本的了解，让我们来探讨在哪些情况下推荐采用数据治理。
- en: When to consider data governance
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑数据治理的时机
- en: 'A formal data governance framework should be considered in any of the following
    scenarios:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下任何情况下，都应考虑采用正式的数据治理框架：
- en: The data in the organization has grown so much and become so complex that traditional
    data management tools are unable to solve cross-functional data needs.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织中的数据量增长如此之大，变得如此复杂，以至于传统的数据管理工具无法解决跨职能的数据需求。
- en: The horizontally focused business units and teams need cross-functional data
    generated or maintained by other focused groups. In this case, an enterprise-wide
    solution (instead of siloed) for data availability and management is required.
    For example, for B2B sales of Visa, the marketing and accounting departments maintain
    data in silos. However, the sales department needs both these silos’ data to create
    accurate sales predictions. So, the data across these different departments should
    be stored in an enterprise-wide central repository instead of silos. Hence, this
    enterprise-wide data requires data governance for proper access, use, and management.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平聚焦的业务单元和团队需要由其他聚焦团队生成或维护的跨职能数据。在这种情况下，需要企业级的数据可用性和管理解决方案（而不是孤岛式），例如，对于维萨（Visa）的B2B销售，市场营销和会计部门在孤岛中维护数据。然而，销售部门需要这两个孤岛的数据来创建准确的销售预测。因此，这些不同部门之间的数据应存储在企业级的中央存储库中，而不是孤岛中。因此，这个企业级的数据需要数据治理以确保适当的访问、使用和管理。
- en: Compliance, legal, and contractual obligations can also call for formal data
    governance. For example, the **Health Insurance Portability and Accountability
    Act** (**HIPAA**) enforces that all **protected health information** (**PHI**)
    data should be well governed and protected from theft or unauthorized access.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合规性、法律和合同义务也可能要求正式的数据治理。例如，**健康保险可携带性和问责法案**（HIPAA）强制要求所有**受保护的健康信息**（PHI）数据应得到良好的治理并受到盗窃或未经授权访问的保护。
- en: So far, we’ve discussed what data governance is and when we should have a formal
    data governance framework. Now we will learn about the DGI data governance framework.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了数据治理是什么以及何时应该有一个正式的数据治理框架。现在我们将学习关于DGI数据治理框架的内容。
- en: The DGI data governance framework
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DGI数据治理框架
- en: 'The DGI data governance framework is a logical structure for implementing proper
    data governance in an organization to enable better data-related decisions to
    be made. The following diagram depicts this data governance framework:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DGI数据治理框架是在组织内实施适当数据治理的逻辑结构，以使做出更好的数据相关决策成为可能。以下图表展示了这个数据治理框架：
- en: '![Figure 8.1 – Data governance framework ](img/B17084_08_001.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 数据治理框架](img/B17084_08_001.jpg)'
- en: Figure 8.1 – Data governance framework
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 数据治理框架
- en: 'In this section, we will discuss the various components of the DGI data governance
    framework. Let’s take a look at each of the components highlighted in the preceding
    diagram:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论DGI数据治理框架的各个组成部分。让我们看看前面图表中突出显示的每个组件：
- en: '**Mission**: The mission of the DGI framework can be attributed to three primary
    responsibilities:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使命**：DGI框架的使命可以归因于三个主要责任：'
- en: Define rules
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义规则
- en: Execute and implement the rules by providing ongoing protection and services
    to data stakeholders
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为数据利益相关者提供持续的保护和服务来执行和实施规则
- en: Deal with scenarios that arise due to non-compliance
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理因不遵守规则而出现的场景
- en: This form of mission is very similar to political governance. In the political
    governance model, the government makes the rules, the executive branch implements
    the rules, and the judiciary deals with non-compliance or rule breakers. Just
    like political governance, here, one set of data stakeholders defines the rules.
    Another set of stakeholders ensures that the rules are followed. Finally, a third
    set of stakeholders makes decisions related to non-compliance. Optionally, the
    organization can develop a vision statement around the mission, which may be used
    to inspire data stakeholders to envision possibilities and set data-related goals.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种使命形式与政治治理非常相似。在政治治理模型中，政府制定规则，行政部门执行规则，司法部门处理不遵守规则或违反规则的人。就像政治治理一样，在这里，一组数据利益相关者定义规则。另一组利益相关者确保规则得到遵守。最后，第三组利益相关者做出与不遵守规则相关的决策。组织可以选择在使命周围制定一个愿景声明，这可能被用来激发数据利益相关者展望可能性并设定数据相关目标。
- en: '**Focus areas**: Mission and vision lead us to the primary focus areas. We
    have two primary focus areas. They are as follows:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重点领域**：使命和愿景引导我们到主要关注领域。我们有两个主要关注领域。具体如下：'
- en: Goals should be **SMART** – that is, **Specific, Measurable, Actionable, Relevant,
    and Timely**. We should remember the principle of the four P’s while deciding
    on the goals we want to pursue – that is, programs, projects, professional disciplines,
    and people. We must ask how these efforts can help our organization in terms of
    revenue, cost, complexity, and ensuring survival (that is, security, compliance,
    and privacy).
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标应该是**SMART** – 即**具体、可衡量、可操作、相关和及时**。在决定我们想要追求的目标时，我们应该记住四个P的原则 – 即项目、项目、专业学科和人员。我们必须问这些努力如何从收入、成本、复杂性和确保生存（即安全、合规性和隐私）的角度帮助我们的组织。
- en: Metrics should also be SMART. Everyone in data governance should know how to
    quantify and measure success.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标也应该SMART。数据治理中的每个人都应该知道如何量化并衡量成功。
- en: 'This discussion leads to the question of where we can fund our data governance
    program. To do so, we must ask the following questions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这次讨论引出了一个问题：我们可以在哪里为我们的数据治理计划提供资金。为了做到这一点，我们必须提出以下问题：
- en: How can we fund the data governance office?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何为数据治理办公室提供资金？
- en: How can we fund the data architects/analysts to define rules and data?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何为数据架构师/分析师提供资金以定义规则和数据？
- en: How can we fund the data stewardship activities?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何为数据治理活动提供资金？
- en: Focus areas are very important when planning formal data governance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划正式数据治理时，重点领域非常重要。
- en: '**Data rules and definitions**: In this component, rules, policies, standards,
    and compliances are set around data. Typical activities may include creating new
    rules, exploring existing rules, and addressing gaps and overlaps.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据规则和定义**：在这个组件中，围绕数据设定了规则、政策、标准和合规性。典型活动可能包括创建新规则、探索现有规则以及解决差距和重叠。'
- en: '**Decision rights**: This component implores the question, *Who can make a
    data-related decision, when, and using what process?* The data governance program
    allows us to store decision rights as metadata for data-related decisions.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**决策权**：这个组件提出了问题，*谁可以在何时以及使用什么流程做出与数据相关的决策？* 数据治理计划允许我们将决策权作为与数据相关的决策的元数据存储。'
- en: '**Accountability**: This component creates accountability for the implementation
    of a rule or decision once it is made. The data governance program may be expected
    to integrate the accountabilities in a day-to-day **software development life
    cycle** (**SDLC**).'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**问责制**：这个组件在做出决策后，为规则的执行或决策创造了问责制。数据治理计划可能需要将问责制整合到日常的**软件开发生命周期**（**SDLC**）中。'
- en: '**Controls**: Data is the new gold, so there is a huge security risk involved
    if there are sensitive data breaches. How do we ensure that such risks are mitigated
    and handled properly? They can be handled using controls. Controls can be preventive
    or reactive. The data governance team is usually tasked with making recommendations
    for creating these controls at different levels of the control stack (networking/OS/database/application).
    Sometimes, the data governance team is also asked to modify the existing controls
    to ensure better data governance.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**控制措施**：数据是新的黄金，如果发生敏感数据泄露，将涉及巨大的安全风险。我们如何确保这些风险得到缓解和处理？它们可以通过控制措施来处理。控制措施可以是预防性的或反应性的。数据治理团队通常负责在不同控制层（网络/操作系统/数据库/应用程序）上制定创建这些控制措施的建议。有时，数据治理团队还被要求修改现有的控制措施，以确保更好的数据治理。'
- en: '**Data stakeholders**: Data stakeholders are individuals or groups who either
    could affect or are affected by the data. Since they have a direct correlation
    with the data, they are consulted while taking data decisions. Again, based on
    the scenario, they may want to be involved in some data-related decisions, should
    be consulted before decisions are finalized, or be informed once decisions are
    made.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据利益相关者**：数据利益相关者是那些可能影响或受数据影响的人或团体。由于他们与数据有直接关联，因此在做出数据决策时会咨询他们。同样，根据情景，他们可能希望参与某些与数据相关的决策，在决策最终确定之前应进行咨询，或者在决策做出后应被告知。'
- en: '**Data governance office**: This facilitates, supports, and runs the data governance
    program and data stewardship activities. It is the centralized governing body
    and usually consists of data architects, data analysts, and those working on creating
    the metadata. It collects and aligns policies, rules, and standards from different
    stakeholders across the organization and comes up with organizational-level rules
    and standards. It is responsible for providing centralized data governance-related
    communication. It is also responsible for collecting the data governance metrics
    and publishing reports and success measures to all the data stakeholders.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据治理办公室**：它促进、支持和运行数据治理计划以及数据管理员活动。它是中央治理机构，通常由数据架构师、数据分析师以及那些从事元数据创建工作的人组成。它收集并协调来自组织不同利益相关者的政策、规则和标准，并提出组织层面的规则和标准。它负责提供中央数据治理相关的沟通。它还负责收集数据治理指标，并向所有数据利益相关者发布报告和成功措施。'
- en: '**Data stewards**: They are part of the data stewardship council, which is
    responsible for making data-related decisions such as setting policies, specifying
    standards, or providing recommendations to the DGO. Based on the size and complexity
    of the organization, the data stewardship council(s) may have a hierarchy. In
    data quality-focused governance projects, there can be an optional data quality
    steward.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据管理员**：他们是数据治理委员会的一部分，负责做出与数据相关的决策，例如制定政策、指定标准或向DGO提供建议。根据组织的规模和复杂性，数据治理委员会（们）可能有一个等级结构。在以数据质量为重点的治理项目中，可能有一个可选的数据质量管理员。'
- en: '**Data processes**: These are the methods that are used to govern data. The
    processes should be documented, standardized, and repeatable. They are designed
    to follow regulatory and compliance requirements for data management, privacy,
    and security.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理**：这些是用于管理数据的手段。这些过程应该被记录、标准化和可重复。它们旨在遵循数据管理、隐私和安全方面的监管和合规要求。'
- en: All these components work in tandem and create a feedback loop, which ensures
    data governance is continuously improving and up to date.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些组件协同工作，形成一个反馈循环，确保数据治理持续改进并保持最新。
- en: In this section, we learned about what data governance is, when it should be
    implemented, and the DGI framework. In the next section, we will provide step-by-step
    guidelines for implementing data governance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了数据治理是什么，何时应该实施，以及DGI框架。在下一节中，我们将提供实施数据治理的逐步指南。
- en: Practical data governance using DataHub and NiFi
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataHub和NiFi进行实际数据治理
- en: In this section, we will discuss a tool called DataHub and how different data
    stakeholders and stewards can make use of it to enable better data governance.
    But first, we will understand the use case and what we are trying to achieve.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一个名为DataHub的工具，以及不同的数据利益相关者和管理员如何利用它来实现更好的数据治理。但首先，我们将了解用例以及我们试图实现的目标。
- en: In this section, we will build a data governance capability around a data ingestion
    pipeline. This data ingestion pipeline will fetch any new objects from an S3 location,
    enrich them, and store the data in a MySQL table. In this particular use case,
    we are getting telephone recharge or top-up events in an S3 bucket from various
    sources such as mobile or the web. We are enriching this data and storing it in
    a MySQL database using an Apache NiFi pipeline.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将围绕数据摄取管道构建数据治理能力。此数据摄取管道将从S3位置获取任何新对象，对其进行丰富，并将数据存储在MySQL表中。在这个特定的用例中，我们从移动或网络等不同来源的S3桶中获取电话充值或充值（套餐）事件。我们使用Apache
    NiFi管道丰富这些数据并将其存储在MySQL数据库中。
- en: 'Apache NiFi is a powerful and reliable drag-and-drop visual tool that allows
    you to easily process and distribute data. It creates directed graphs to create
    a workflow or a data pipeline. It consists of the following high-level components
    so that you can create reliable data routing and transformation capabilities:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Apache NiFi是一个强大且可靠的拖放式可视化工具，它允许您轻松处理和分发数据。它创建有向图以创建工作流程或数据管道。它由以下高级组件组成，以便您创建可靠的数据路由和转换能力：
- en: '**FlowFile**: Each data record is serialized and processed as a FlowFile object
    in the NiFi pipeline. A FlowFile object consists of flowfile content and attribute
    denoting data content and metadata respectively.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FlowFile**: 在NiFi管道中，每个数据记录都被序列化并作为FlowFile对象进行处理。FlowFile对象由表示数据内容和元数据的flowfile内容和属性组成。'
- en: '**Processor**: This is one of the basic units of NiFi. This component is primarily
    responsible for processing the data. It takes a FlowFile as input, processes it,
    and generates a new one. There are around 300 inbuilt processors. NiFi allows
    you to develop and deploy additional custom processors using Java.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器**: 这是NiFi的基本单元之一。该组件主要负责处理数据。它以FlowFile作为输入，对其进行处理，并生成一个新的FlowFile。大约有300个内置处理器。NiFi允许您使用Java开发和部署额外的自定义处理器。'
- en: '**Queue**: NiFi follows a staged event-driven architecture. This means that
    communication between processors is asynchronous. So, there needs to be a message
    bus that will hold a FlowFile generated by one processor, before it is picked
    by the next processor. This message bus is called the NiFi queue. Additionally,
    it supports setting up backpressure thresholds, load balancing, and prioritization
    policies.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**队列**: NiFi遵循分阶段事件驱动架构。这意味着处理器之间的通信是异步的。因此，需要一个消息总线来保存一个处理器生成的FlowFile，直到它被下一个处理器取走。这个消息总线被称为NiFi队列。此外，它支持设置背压阈值、负载均衡和优先级策略。'
- en: '**Controller service**: This allows you to share functionality and state across
    JVM cleanly and consistently. It is responsible for things such as creating and
    maintaining database connection pools or distributed caches.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器服务**: 这允许您在JVM中干净且一致地共享功能状态。它负责创建和维护数据库连接池或分布式缓存等任务。'
- en: '**Processor groups**: When a data flow becomes complex, it makes more sense
    to group a set of components, such as processors and queues, into an encapsulation
    called a processor group. A complex pipeline may have multiple processor groups
    connected. Each processor group, in turn, has a data flow inside it.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器组**: 当数据流变得复杂时，将一组组件（如处理器和队列）组合成一个称为处理器组的封装更有意义。一个复杂的管道可能连接多个处理器组。每个处理器组内部都有一个数据流。'
- en: '**Ports**: Processor groups can be connected using ports. To get the input
    from an external processor group, input ports are used. To send a processed FlowFile
    out of the processor group, output ports are used.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口**: 可以使用端口连接处理器组。要从外部处理器组获取输入，使用输入端口。要将处理后的FlowFile从处理器组发送出去，使用输出端口。'
- en: 'Now, let’s build the NiFi pipeline for our use case. Our source is an S3 bucket
    called `chapter8input`, while our output is a MySQL cluster. We will be receiving
    S3 objects in the `chapter8input` folder from different sources. Each S3 object
    will be in JSON format and will contain one phone recharge or top-up (bundle)
    event. Our data sink is a MySQL table called `bundle_events`. The **Data Definition
    Language** (**DDL**) of this table is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为我们的用例构建NiFi管道。我们的源是一个名为`chapter8input`的S3桶，而我们的输出是一个MySQL集群。我们将从不同的来源接收`chapter8input`文件夹中的S3对象。每个S3对象将以JSON格式存在，并包含一个电话充值或充值（套餐）事件。我们的数据汇是一个名为`bundle_events`的MySQL表。该表的**数据定义语言**（**DDL**）如下：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, the NiFi pipeline polls the S3 bucket and checks for any change events,
    such as a JSON file being created or updated. Once a file is uploaded to the S3
    bucket, NiFi should fetch the file, enrich it with the source type using the S3
    object name, and then write the enriched data to the MySQL `bundle_events` table.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，NiFi管道会轮询S3存储桶并检查任何更改事件，例如创建或更新JSON文件。一旦文件上传到S3存储桶，NiFi应该获取该文件，使用S3对象名称丰富其源类型，然后将丰富后的数据写入MySQL的`bundle_events`表。
- en: If you don’t have Apache NiFi installed in your system, download and install
    Apache NiFi-1.12.0\. You can download, install, and follow the startup instructions
    at [https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi](https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi)
    to do so. Alternatively, you can spin up a NiFi cluster/node on an AWS EC2 instance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您系统中没有安装Apache NiFi，请下载并安装Apache NiFi-1.12.0。您可以在[https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi](https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi)下载、安装并遵循启动说明进行安装。或者，您可以在AWS
    EC2实例上启动一个NiFi集群/节点。
- en: Creating the NiFi pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建NiFi管道
- en: 'Next, we will discuss how to build the NiFi pipeline. The NiFi pipeline that
    we will build is shown in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何构建NiFi管道。我们将构建的NiFi管道如下截图所示：
- en: '![Figure 8.2 – NiFi pipeline to read data from S3 and write to MySQL ](img/B17084_08_002.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 从S3读取数据并写入MySQL的NiFi管道](img/B17084_08_002.jpg)'
- en: Figure 8.2 – NiFi pipeline to read data from S3 and write to MySQL
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 从S3读取数据并写入MySQL的NiFi管道
- en: 'Let’s try to understand the overall NiFi pipeline and how we are building it:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解整个NiFi管道以及我们是如何构建它的：
- en: First, we have used a **ListS3** processor to capture whether any S3 objects
    have been inserted or updated in the configured S3 bucket. It lists all the change
    events.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用**ListS3**处理器来捕获配置的S3存储桶中是否有任何S3对象被插入或更新。它列出了所有更改事件。
- en: Then, using the **SplitRecord** processor, we split the records into individual
    events.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用**SplitRecord**处理器将记录拆分为单个事件。
- en: Using the `sourceKey` attribute for the FlowFile.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用FlowFile的`sourceKey`属性。
- en: Then, we use **FetchS3Object** to fetch the S3 object. The **FetchS3Oject**
    processor is responsible for reading the actual S3 object. If **FetchS3Object**
    successfully reads the file, it sends the S3 object content as a FlowFile to the
    **JoltTransformRecord** processor.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用**FetchS3Object**来获取S3对象。**FetchS3Object**处理器负责读取实际的S3对象。如果**FetchS3Object**成功读取文件，它将S3对象内容作为FlowFile发送到**JoltTransformRecord**处理器。
- en: '**JoltTransformRecord** is used to enrich the data before the enriched data
    is sent to be written to MySQL using the **PutDatabaseRecord** processor.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**JoltTransformRecord**用于在丰富数据被**PutDatabaseRecord**处理器写入MySQL之前丰富数据。'
- en: The success of the **PutDatabaseRecord** processor is sent to the **LogSuccessMessage**
    processor. As shown in the preceding screenshot, all FlowFiles in failure scenarios
    are sent to the **LogErrorMessage** processor.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PutDatabaseRecord**处理器的成功状态被发送到**LogSuccessMessage**处理器。如图中所示，所有在失败场景中的FlowFiles都被发送到**LogErrorMessage**处理器。'
- en: 'Now, let’s configure each of the NiFi processors present in the NiFi pipeline
    (shown in *Figure 8.2*):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们配置NiFi管道中存在的每个NiFi处理器（如图8.2所示）：
- en: '**ListS3 processor**: The configuration of the **ListS3** processor is shown
    in the following screenshot:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ListS3处理器**：以下截图显示了**ListS3**处理器的配置：'
- en: '![Figure 8.3 – Configuring the ListS3 processor ](img/B17084_08_003.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 配置ListS3处理器](img/B17084_08_003.jpg)'
- en: Figure 8.3 – Configuring the ListS3 processor
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 配置ListS3处理器
- en: As we can see, the **ListS3** processor is configured to poll and listen for
    changes in the **chapter8input** S3 bucket. Apart from the bucket name, we must
    configure the **Region**, **Access Key ID**, and **Secret Access Key** details
    for the NiFi instance to connect to the AWS S3 bucket. Finally, we have configured
    the Record Writer property, which is set as a controller service of the **JsonRecordSetWriter**
    type.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**ListS3**处理器被配置为轮询并监听**chapter8input** S3存储桶中的更改。除了存储桶名称外，我们还必须为NiFi实例配置**区域**、**访问密钥ID**和**秘密访问密钥**详细信息，以便连接到AWS
    S3存储桶。最后，我们已配置记录编写器属性，将其设置为**JsonRecordSetWriter**类型的控制器服务。
- en: '**SplitRecord processor**: Next, we will configure the **SplitRecord** processor,
    as shown in the following screenshot:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SplitRecord处理器**：接下来，我们将配置**SplitRecord**处理器，如下截图所示：'
- en: '![Figure 8.4 – Configuring the SplitRecord processor ](img/B17084_08_004.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 配置SplitRecord处理器](img/B17084_08_004.jpg)'
- en: Figure 8.4 – Configuring the SplitRecord processor
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 配置SplitRecord处理器
- en: '**SplitRecord** is responsible for splitting a single FlowFile containing multiple
    write events in S3 into individual events. Now, each event is metadata for one
    **S3Object**.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**SplitRecord**负责将包含多个写入事件的单个FlowFile在S3中分割成单独的事件。现在，每个事件都是单个**S3Object**的元数据。'
- en: '**EvaluateJsonPath processor**: We use the **EvaluateJsonPath** processor to
    extract the value of the **key** column from the FlowFile content and add it as
    an attribute to the FlowFile attributes. The configuration of the **EvaluateJsonPath**
    processor is shown in the following screenshot:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EvaluateJsonPath处理器**：我们使用**EvaluateJsonPath**处理器从FlowFile内容中提取**key**列的值，并将其作为属性添加到FlowFile属性中。**EvaluateJsonPath**处理器的配置如下截图所示：'
- en: '![Figure 8.5 – Configuring the EvaluateJsonPath processor ](img/B17084_08_005.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 配置EvaluateJsonPath处理器](img/B17084_08_005.jpg)'
- en: Figure 8.5 – Configuring the EvaluateJsonPath processor
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 配置EvaluateJsonPath处理器
- en: Here, we have configured the `flowfile-attribute` value. This indicates that
    a new key-value pair will be added to the FlowFile attributes. The attribute to
    be added should be provided as a dynamic property, where the name of the property
    will be the attribute key and the value of the property will be the attribute
    value. Here, we have added a property named `$.key`. This expression fetches the
    value of the `key` field from the FlowFile content (which is a JSON).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已配置了`flowfile-attribute`值。这表示将向FlowFile属性中添加一个新的键值对。要添加的属性应提供为动态属性，其中属性名称将是属性键，属性值将是属性值。在这里，我们添加了一个名为`$.key`的属性。此表达式从FlowFile内容（这是一个JSON）中获取`key`字段的值。
- en: '**FetchS3Object processor**: The following screenshot shows the configuration
    of the **FetchS3Object** processor:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FetchS3Object处理器**：以下截图显示了**FetchS3Object**处理器的配置：'
- en: '![Figure 8.6 – Configuring the FetchS3Object processor ](img/B17084_08_006.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 配置FetchS3Object处理器](img/B17084_08_006.jpg)'
- en: Figure 8.6 – Configuring the FetchS3Object processor
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 配置FetchS3Object处理器
- en: As we can see, the `${sourcekey}` is a NiFi expression that gets the value of
    the **sourcekey** attribute. Apart from this, the S3-related properties such as
    **Bucket**, **Access Key ID**, and **Secret Access Key** need to be set in this
    processor. The output of this processor is the content of our **S3Object** (which
    is in JSON format).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`${sourcekey}`是一个NiFi表达式，用于获取**sourcekey**属性值。除此之外，还需要在此处理器中设置与S3相关的属性，如**Bucket**、**Access
    Key ID**和**Secret Access Key**。此处理器的输出是我们**S3Object**的内容（以JSON格式）。
- en: '**JoltTransformRecord processor**: The **JoltTransform** processor is used
    to add a new key-value pair to the JSON. The configuration of the **JoltTransformRecord**
    processor is as follows:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JoltTransformRecord处理器**：**JoltTransform**处理器用于向JSON中添加新的键值对。**JoltTransformRecord**处理器的配置如下：'
- en: '![Figure 8.7 – Configuring the JoltTransformRecord processor ](img/B17084_08_007.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 配置JoltTransformRecord处理器](img/B17084_08_007.jpg)'
- en: Figure 8.7 – Configuring the JoltTransformRecord processor
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 配置JoltTransformRecord处理器
- en: The `shift`, `chain`, and `default`. For more information about Apache Jolt,
    go to [https://github.com/bazaarvoice/jolt#jolt](https://github.com/bazaarvoice/jolt#jolt).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`shift`、`chain`和`default`。有关Apache Jolt的更多信息，请访问[https://github.com/bazaarvoice/jolt#jolt](https://github.com/bazaarvoice/jolt#jolt)。'
- en: 'Here, we will discuss the Jolt transformation we are using to add a key-value
    pair to the JSON FlowFile content. We add the `source` key using the **Jolt Specification**
    property, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论我们用于向JSON FlowFile内容添加键值对的Jolt转换。我们使用**Jolt规范**属性添加`source`键，如下所示：
- en: '![Figure 8.8 – Using the Jolt Specification property to add the source key
    ](img/B17084_08_008.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 使用Jolt规范属性添加源键](img/B17084_08_008.jpg)'
- en: Figure 8.8 – Using the Jolt Specification property to add the source key
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 使用Jolt规范属性添加源键
- en: As you can see, by setting `operation` to `default` in `source` key with the
    dynamic value calculated using the NiFi expression language. `${sourcekey:substringBefore('_')}`
    is a NiFi expression. This expression returns a substring of the `FlowFile` attribute’s
    `'_'`).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过在 `source` 键中将 `operation` 设置为 `default` 并使用 NiFi 表达式语言计算出的动态值。`${sourcekey:substringBefore('_')}`
    是一个 NiFi 表达式。此表达式返回 `FlowFile` 属性 `'_'` 的子字符串）。
- en: '**PutDatabaseRecord processor**: Once this JSON is enriched with the newly
    added key-value pair, the record is written to the MySQL database using the **PutDatabaseRecord**
    processor. The following screenshot shows the configuration of the **PutDatabaseRecord**
    processor:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PutDatabaseRecord 处理器**：一旦此 JSON 被添加的新键值对丰富，记录就会使用 **PutDatabaseRecord**
    处理器写入 MySQL 数据库。以下屏幕截图显示了 **PutDatabaseRecord** 处理器的配置：'
- en: '![Figure 8.9 – Configuring the PutDatabaseRecord processor ](img/B17084_08_009.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 配置 PutDatabaseRecord 处理器](img/B17084_08_009.jpg)'
- en: Figure 8.9 – Configuring the PutDatabaseRecord processor
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 配置 PutDatabaseRecord 处理器
- en: 'As evident from this configuration, we need to configure two controller services
    – `MySQL`, `INSERT`, and `bundle_events`. Now, let’s see how the **DBCPConnectionPool**
    service, called **MysqlDBCPConnectionPool**, is configured. The following screenshot
    shows the configuration of the **MysqlDBCPConnectionPool** service:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如此配置所示，我们需要配置两个控制器服务 – `MySQL`、`INSERT` 和 `bundle_events`。现在，让我们看看名为 **DBCPConnectionPool**
    的服务（称为 **MysqlDBCPConnectionPool**）是如何配置的。以下屏幕截图显示了 **MysqlDBCPConnectionPool**
    服务的配置：
- en: '![Figure 8.10 – Configuring the DBCPConnectionPool service ](img/B17084_08_010.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 配置 DBCPConnectionPool 服务](img/B17084_08_010.jpg)'
- en: Figure 8.10 – Configuring the DBCPConnectionPool service
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 配置 DBCPConnectionPool 服务
- en: The **MysqlDBCPConnectionPool** controller service is used to create a JDBC
    connection pool. To do so, you must configure the **Database Connection URL**
    (the JDBC URL), **Database Driver Class Name**, and **Database Driver Location(s)**
    properties, as shown in the preceding screenshot.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**MysqlDBCPConnectionPool** 控制器服务用于创建 JDBC 连接池。为此，您必须配置 **数据库连接 URL**（JDBC
    URL）、**数据库驱动类名称** 和 **数据库驱动位置** 属性，如前一个屏幕截图所示。'
- en: With that, we have built the entire NiFi pipeline to extract data from S3, enrich
    it, and write it to a MySQL table. You can check out the entire NiFi pipeline
    by going to [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经构建了整个 NiFi 管道，用于从 S3 提取数据，对其进行丰富，并将其写入 MySQL 表。您可以通过访问 [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml)
    来查看整个 NiFi 管道。
- en: Setting up DataHub
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 DataHub
- en: Now, we will add a data governance layer to our solution. Although many data
    governance tools are available, most of them are paid tools. There are several
    data governance tools available for pay-as-you-go models in the cloud, such as
    AWS Glue Catalog. However, since our solution will be running on-premises, we
    will choose a platform-agnostic open source tool. DataHub, developed by LinkedIn,
    is one such open source tool that comes with a decent set of features. We will
    use it in this chapter to explain data governance practically. In this section,
    we will learn how to configure DataHub. In this pipeline, S3 is the source, MySQL
    is the target, and NiFi is the processing engine. To create data governance, we
    need to connect to all these systems and extract metadata from them. DataHub does
    this for us.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将向我们的解决方案添加一个数据治理层。尽管有许多数据治理工具可用，但其中大多数是付费工具。云中有几种按需付费的数据治理工具，例如 AWS Glue
    目录。然而，由于我们的解决方案将在本地运行，我们将选择一个平台无关的开源工具。LinkedIn 开发的 DataHub 就是这样的开源工具之一，它附带了一套相当不错的功能。我们将在此章中使用它来实际解释数据治理。在本节中，我们将学习如何配置
    DataHub。在此管道中，S3 是源，MySQL 是目标，NiFi 是处理引擎。要创建数据治理，我们需要连接到所有这些系统并从中提取元数据。DataHub
    为我们完成这项工作。
- en: Before we begin connecting these components (of the data pipeline) to DataHub,
    we need to download and install DataHub in our local Docker environment. The detailed
    installation instructions can be found at [https://datahubproject.io/docs/quickstart](https://datahubproject.io/docs/quickstart).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将这些组件（数据管道的组件）连接到 DataHub 之前，我们需要在我们的本地 Docker 环境中下载并安装 DataHub。详细的安装说明可以在
    [https://datahubproject.io/docs/quickstart](https://datahubproject.io/docs/quickstart)
    找到。
- en: In the next section, we will learn how to connect different data sources to
    DataHub.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何将不同的数据源连接到DataHub。
- en: Adding an ingestion source to DataHub
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向DataHub添加摄取源
- en: 'To connect a data source or pipeline component to DataHub, we must go to the
    **Ingestion** tab from the top-right menu bar, as shown in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据源或管道组件连接到DataHub，我们必须从右上角的菜单栏转到**摄取**标签，如图所示：
- en: '![Figure 8.11 – Connecting a new source for metadata ingestion in DataHub ](img/B17084_08_011.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11 – 在DataHub中连接新的元数据源](img/B17084_08_011.jpg)'
- en: Figure 8.11 – Connecting a new source for metadata ingestion in DataHub
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 在DataHub中连接新的元数据源
- en: 'Then, we must click the **Create new source** button to create a new connection
    to a data source. On clicking this button, we get the following popup:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须点击**创建新源**按钮来创建到数据源的新连接。点击此按钮后，我们会得到以下弹出窗口：
- en: '![Figure 8.12 – Choosing the type of data source to create a new connection
    ](img/B17084_08_012.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图8.12 – 选择要创建新连接的数据源类型](img/B17084_08_012.jpg)'
- en: Figure 8.12 – Choosing the type of data source to create a new connection
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 选择要创建新连接的数据源类型
- en: 'This popup is a multipage wizard where you can create a new data source connection.
    First, as shown in the preceding screenshot, you are asked to choose a data source
    type. For example, if the data is in a MySQL database, then we would select MySQL
    from the various thumbnails present in the wizard. For sources, which are not
    listed, such as NiFi and S3, we can select the **Custom** thumbnail. Once you’ve
    chosen a type, click **Next**; you will be taken to the second step called **Configure
    Recipe**, as shown in the following screenshot:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此弹出窗口是一个多页向导，您可以在其中创建新的数据源连接。首先，如图所示，您被要求选择数据源类型。例如，如果数据存储在MySQL数据库中，那么我们会从向导中提供的各种缩略图中选择MySQL。对于未列出的源，例如NiFi和S3，我们可以选择**自定义**缩略图。一旦选择了类型，点击**下一步**；您将被带到第二个步骤，称为**配置食谱**，如图所示：
- en: '![Figure 8.13 – Configuring a recipe for the data source ](img/B17084_08_013.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 配置数据源的食谱](img/B17084_08_013.jpg)'
- en: Figure 8.13 – Configuring a recipe for the data source
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 配置数据源的食谱
- en: In the `YAML` code in the space provided. Irrespective of the data source, the
    YAML has two top-level elements – `source` and `sink`. Again, both `source` and
    `sink` contain two elements – `type` and `config`. Here, `type` denotes the type
    of source or sink. For example, in the preceding screenshot, we are configuring
    NiFi, so the source type is `nifi`. If we had selected MySQL as the type of connection
    in the previous step, the source’s `type` in this step would be `mysql`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的空间中的`YAML`代码中。无论数据源如何，YAML都有两个顶级元素——`source`和`sink`。同样，`source`和`sink`都包含两个元素——`type`和`config`。在这里，`type`表示源或目的地的类型。例如，在上面的截图中，我们正在配置NiFi，因此源类型是`nifi`。如果我们上一步选择了MySQL作为连接类型，那么在此步骤中源的类型将是`mysql`。
- en: 'There are primarily three types of sinks that are currently available in DataHub,
    as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在DataHub中主要有三种类型的目的地，如下所示：
- en: '`stdout`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stdout`'
- en: '**DataHub**: The metadata is sent to DataHub via the GMS REST API'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataHub**：元数据通过GMS REST API发送到DataHub'
- en: '**File**: The metadata is sent to a configured file'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件**：元数据被发送到配置的文件'
- en: Here, we will be using DataHub as the sink since this allows us to do governance
    and monitoring using DataHub’s capabilities. In this case, our type of sink will
    be `datahub-rest`. We also need to specify the GMS REST API’s HTTP address as
    the base path. Since our DataHub installation is using Docker, we will use `localhost:9002`
    as the server IP address (as shown in *Figure 8.13*).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用DataHub作为目的地，因为这允许我们利用DataHub的功能进行治理和监控。在这种情况下，我们的目的地类型将是`datahub-rest`。我们还需要指定GMS
    REST API的HTTP地址作为基本路径。由于我们的DataHub安装使用Docker，我们将使用`localhost:9002`作为服务器IP地址（如图*8.13*所示）。
- en: 'Once everything has been added, click **Next** to go to the **Schedule Execution**
    step, as shown in the following screenshot:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一切添加完毕后，点击**下一步**进入**执行计划**步骤，如图所示：
- en: '![Figure 8.14 – Create an execution schedule ](img/B17084_08_014.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14 – 创建执行计划](img/B17084_08_014.jpg)'
- en: Figure 8.14 – Create an execution schedule
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 创建执行计划
- en: In this step, we set an execution schedule for DataHub to fetch metadata from
    the source. In this case, we configured a CRON expression, `0,30 * * * *`, which
    means it will run every 30 minutes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们为DataHub设置执行计划以从源获取元数据。在此情况下，我们配置了一个CRON表达式，`0,30 * * * *`，这意味着它将每30分钟运行一次。
- en: 'Click **Next** to go to the last step, as shown in the following screenshot:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**下一步**进入最后一步，如图下所示截图：
- en: '![Figure 8.15 – The final step of the New Ingestion Source wizard ](img/B17084_08_015.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – 新建数据源向导的最后一步](img/B17084_08_015.jpg)'
- en: Figure 8.15 – The final step of the New Ingestion Source wizard
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 新建数据源向导的最后一步
- en: 'In the final step of the wizard, we need to enter the name of the ingestion
    source. This name can be any arbitrary name that helps uniquely identify that
    source. Finally, we must click **Done** to add the new data source. The following
    screenshot shows the added sources:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在向导的最后一步，我们需要输入数据源名称。这个名称可以是任何有助于唯一识别该源的任意名称。最后，我们必须点击**完成**来添加新的数据源。以下截图显示了添加的源：
- en: '![Figure 8.16 – Sources added at a glance ](img/B17084_08_016.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图8.16 – 快速查看添加的源](img/B17084_08_016.jpg)'
- en: Figure 8.16 – Sources added at a glance
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 快速查看添加的源
- en: As shown in the preceding screenshot, we can monitor how many times a job has
    run to fetch metadata from each source and whether the last run was successful
    or not. All the YAML files that have been used for our use case can be found on
    GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述截图所示，我们可以监控作业运行次数以从每个源获取元数据，以及最后一次运行是否成功。我们用于用例的所有YAML文件都可以在GitHub上找到，网址为[https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08)。
- en: In the next section, we will discuss how we can perform different data governance
    tasks using this tool.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何使用此工具执行不同的数据治理任务。
- en: Governance activities
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 治理活动
- en: 'As discussed in the preceding section, once a data source or pipeline is connected
    with DataHub, it provides a lot of tools to enable a data governance model around
    those sources. The following are a few capabilities that we will explore in this
    chapter:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述章节讨论，一旦数据源或管道与DataHub连接，它将提供许多工具来支持围绕这些源的数据治理模型。以下是我们将在本章中探讨的一些功能：
- en: '**Adding a domain**: A domain in DataHub can be any logical grouping; it might
    be organization-specific. This helps us analyze the domain-wise resource utilization
    and other statistics. For example, an organization can use business unit names
    as domains. In our particular scenario, **DataServices** is a business unit in
    the organization, and we create a domain in the name of the business unit. To
    create a new domain, we can navigate to **Manage** | **Domains** and click the
    **New Domain** button. Once this button is clicked, a popup dialog will open,
    as shown in the following screenshot:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加域**：在DataHub中，域可以是任何逻辑分组；它可能是特定于组织的。这有助于我们分析按域的资源利用和其他统计数据。例如，组织可以使用业务单元名称作为域。在我们的特定场景中，**DataServices**是组织中的一个业务单元，我们创建了一个以业务单元命名的域。要创建新域，我们可以导航到**管理**
    | **域**并点击**新建域**按钮。一旦点击此按钮，将打开一个弹出对话框，如图下所示：'
- en: '![Figure 8.17 – Creating a new domain ](img/B17084_08_017.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17 – 创建新域](img/B17084_08_017.jpg)'
- en: Figure 8.17 – Creating a new domain
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 创建新域
- en: As we can see, we must provide a name and description for a domain. You cannot
    create two domains with the same name.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们必须为域提供一个名称和描述。不能创建两个具有相同名称的域。
- en: '**Adding a user group**: We can manage users and groups by going to **Settings**
    and selecting the **Groups** tab. Upon clicking the **Create group** button, a
    dialog similar to the following will appear:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加用户组**：我们可以通过进入**设置**并选择**组**选项卡来管理用户和组。点击**创建组**按钮后，将出现一个类似于以下对话框：'
- en: '![Figure 8.18 – Adding a new user group ](img/B17084_08_018.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18 – 添加新的用户组](img/B17084_08_018.jpg)'
- en: Figure 8.18 – Adding a new user group
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 添加新的用户组
- en: As we can see, we must provide a group name and an optional description. User
    groups help make any user part of a group, and accountability, ownership, access
    policies and rules can be assigned to a group.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们必须提供一个组名和可选的描述。用户组有助于使任何用户成为组的一部分，并将责任、所有权、访问策略和规则分配给组。
- en: '**Exploring metadata**: This activity in data governance comes under data definition.
    Most data governance tools support metadata management. Here, as shown in the
    following screenshot, the DataHub dashboard provides a summary of the metadata:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索元数据**：在数据治理中，这项活动属于数据定义。大多数数据治理工具支持元数据管理。在此，如图下所示，DataHub仪表板提供了元数据的摘要：'
- en: '![Figure 8.19 – Platform-wise and domain-wise resource summary ](img/B17084_08_019.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19 – 平台和域资源概览](img/B17084_08_019.jpg)'
- en: Figure 8.19 – Platform-wise and domain-wise resource summary
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 平台和域资源概览
- en: 'The preceding screenshot shows the various platforms and objects present on
    each platform. It also shows how many resources are in each domain. For enterprise
    data governance, it is important to monitor and understand how many resources
    are being used by different business units for auditing, tracking, and financial
    purposes. The following screenshot shows a MySQL table resource and its corresponding
    data definitions:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了每个平台上存在的各种平台和对象。它还显示了每个域中有多少资源。对于企业数据治理，监控和理解不同业务单元用于审计、跟踪和财务目的的资源数量非常重要。下面的截图显示了一个MySQL表资源和相应的数据定义：
- en: '![Figure 8.20 – Adding descriptions to metadata ](img/B17084_08_020.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图8.20 – 向元数据添加描述](img/B17084_08_020.jpg)'
- en: Figure 8.20 – Adding descriptions to metadata
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 – 向元数据添加描述
- en: 'As we can see, we can add a description to the metadata. On the other hand,
    the following screenshot shows how the schema of the table is loaded and seen
    in DataHub:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们可以向元数据添加描述。另一方面，下面的截图显示了如何在DataHub中加载和查看表的架构：
- en: '![Figure 8.21 – Schema of the table  ](img/B17084_08_021.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图8.21 – 表的架构](img/B17084_08_021.jpg)'
- en: Figure 8.21 – Schema of the table
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21 – 表的架构
- en: As we can see, descriptions, terms, and tags can be added and maintained in
    each column of the schema. This enables data definition activities for data governance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，描述、术语和标签可以添加并维护在模式的每一列中。这为数据治理的数据定义活动提供了可能。
- en: '**Exploring lineage**: DataHub allows us to explore the lineage of the data.
    In the data definition of data governance, apart from maintaining the metadata
    and descriptions, proper governance must know the origin of the data and how it
    is being used. This aspect is covered by **Lineage**, as shown in the following
    screenshot:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索血缘**：DataHub允许我们探索数据血缘。在数据治理的数据定义中，除了维护元数据和描述外，适当的治理还必须了解数据的来源以及它的使用方式。这一方面由**血缘**所涵盖，如下面的截图所示：'
- en: '![Figure 8.22 – Data lineage ](img/B17084_08_022.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图8.22 – 数据血缘](img/B17084_08_022.jpg)'
- en: Figure 8.22 – Data lineage
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22 – 数据血缘
- en: 'Here, we can see **1 Downstream** and no upstream events for the **chapter8input**
    S3 bucket. We can click on the **Visualize Lineage** button to see the lineage
    as a diagram, as shown in the following screenshot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到**1个下游**和**chapter8input** S3存储桶没有上游事件。我们可以点击**可视化血缘**按钮来以图表的形式查看血缘，如下面的截图所示：
- en: '![Figure 8.23 – Visualizing data lineage as a workflow ](img/B17084_08_023.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图8.23 – 将数据血缘可视化成工作流](img/B17084_08_023.jpg)'
- en: Figure 8.23 – Visualizing data lineage as a workflow
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 – 将数据血缘可视化成工作流
- en: Apart from visual lineage, you can explore the impact analysis of this resource.
    This impact analysis helps communicate with the impacted groups in case a change
    or maintenance event occurs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可视化血缘之外，您还可以探索此资源的影響分析。这种影响分析有助于在发生更改或维护事件时与受影响的群体进行沟通。
- en: '**Creating accountability by adding a domain and owners to a resource**: We
    can start creating accountability of resources by attaching a domain and one or
    multiple owners to a resource from the landing page of the resource, as shown
    in the following screenshot:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过添加域和所有者到资源来建立问责制**：我们可以从资源的着陆页开始，通过将域和一个或多个所有者附加到资源上来创建资源的问责制，如下面的截图所示：'
- en: '![Figure 8.24 – Assigning a domain and owners ](img/B17084_08_024.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图8.24 – 分配域和所有者](img/B17084_08_024.jpg)'
- en: Figure 8.24 – Assigning a domain and owners
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24 – 分配域和所有者
- en: 'An owner can be a group or a user. However, it is advisable to assign ownership
    to a group and add users to that group. When adding an owner, you can choose the
    type of ownership, as shown in the following screenshot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 所有者可以是组或用户。然而，建议将所有权分配给组，并将用户添加到该组中。在添加所有者时，您可以选择所有权的类型，如下面的截图所示：
- en: '![Figure 8.25 – Adding an owner ](img/B17084_08_025.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图8.25 – 添加所有者](img/B17084_08_025.jpg)'
- en: Figure 8.25 – Adding an owner
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25 – 添加所有者
- en: Here, while adding the **offer_analytics** owner, we set the type of owner to
    **Technical Owner**. An owner can be one of three types – **Technical Owner**,
    **Business Owner**, or **Data Steward**. **Technical Owner** is accountable for
    producing, maintaining, and distributing the asset. **Business Owner** is a domain
    expert associated with the asset. Finally, **Data Steward** is accountable for
    the governance of the asset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，当我们添加**offer_analytics**所有者时，我们将所有者类型设置为**技术所有者**。所有者可以是三种类型之一 – **技术所有者**、**业务所有者**或**数据管理员**。**技术所有者**负责生成、维护和分发资产。**业务所有者**是与资产相关的领域专家。最后，**数据管理员**负责资产的管理。
- en: '**Setting policies**: DataHub allows us to set policies. Policies are sets
    of rules that define privileges over an asset or the DataHub platform. There are
    two categories of policies – **Platform** and **Metadata**. The **Platform** policy
    allows us to assign DataHub platform-level privileges to users or groups, while
    the **Metadata** policy allows us to assign metadata privileges to users or groups.
    To create a new policy, navigate to **Settings** | **Privileges**. Upon clicking
    the **Create New Policy** button, a wizard appears, as shown in the following
    screenshot:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置策略**：DataHub 允许我们设置策略。策略是一组规则，用于定义对资产或 DataHub 平台的权限。策略分为两类 – **平台** 和
    **元数据**。**平台**策略允许我们将 DataHub 平台级别的权限分配给用户或组，而**元数据**策略允许我们将元数据权限分配给用户或组。要创建新策略，请转到
    **设置** | **权限**。点击 **创建新策略**按钮后，将出现一个向导，如下截图所示：'
- en: '![Figure 8.26 – Creating a View_Analytics policy ](img/B17084_08_026.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.26 – 创建 View_Analytics 策略](img/B17084_08_026.jpg)'
- en: Figure 8.26 – Creating a View_Analytics policy
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26 – 创建 View_Analytics 策略
- en: Here we are creating a new policy called **View_Analytics**. We have chosen
    **Platform** as the type of policy. Optionally, we can add a description of the
    policy. Here, we have added a description that states **This policy is for viewing
    analytics only**.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在创建一个名为 **View_Analytics** 的新策略。我们已选择 **平台** 作为策略类型。可选地，我们可以添加策略的描述。在这里，我们添加了一个描述，说明
    **此策略仅用于查看分析**。
- en: 'Click **Next** to go to the **Configure Privileges** section, as shown in the
    following screenshot:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**下一步**进入**配置权限**部分，如下截图所示：
- en: '![Figure 8.27 – Configuring policy privileges ](img/B17084_08_027.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.27 – 配置策略权限](img/B17084_08_027.jpg)'
- en: Figure 8.27 – Configuring policy privileges
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27 – 配置策略权限
- en: 'Here, we are configuring the platform privileges for the policy that we are
    defining. Finally, we must select/specify the users or groups that this policy
    will be applied to. The following screenshot shows how we can assign users or
    groups to a policy:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在配置我们定义的策略的平台权限。最后，我们必须选择/指定此策略将应用到的用户或组。以下截图显示了如何将用户或组分配给策略：
- en: '![Figure 8.28 – Assigning users/groups to a policy ](img/B17084_08_028.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.28 – 将用户/组分配给策略](img/B17084_08_028.jpg)'
- en: Figure 8.28 – Assigning users/groups to a policy
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28 – 将用户/组分配给策略
- en: As we can see, the **View_Analytics** policy is assigned to the **executives**
    group.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**View_Analytics**策略已分配给**executives**组。
- en: '**Visual analytics**: DataHub also allows users to create and save dashboards
    for the analytics required for the data governance of an organization. The following
    screenshot shows various metrics, such as the most viewed datasets and data governance
    completeness (how much of the documentation/lineage/schema is well defined) for
    different entity types:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化分析**：DataHub 还允许用户创建和保存用于组织数据治理所需分析的仪表板。以下截图显示了各种指标，例如最常查看的数据集和数据治理完整性（文档/血缘/模式的定义程度）：'
- en: '![Figure 8.29 – Various visual analytics metrics ](img/B17084_08_029.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.29 – 各种可视化分析指标](img/B17084_08_029.jpg)'
- en: Figure 8.29 – Various visual analytics metrics
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.29 – 各种可视化分析指标
- en: 'The following screenshot shows some other dashboard charts where we can find
    the platforms that were used per domain and their count. We can also see the number
    of entities (datasets or pipeline components) per platform:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了其他仪表板图表，其中我们可以找到按领域使用的平台及其计数。我们还可以看到每个平台上的实体数量（数据集或管道组件）：
- en: '![Figure 8.30 – Data Landscape Summary ](img/B17084_08_030.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.30 – 数据景观摘要](img/B17084_08_030.jpg)'
- en: Figure 8.30 – Data Landscape Summary
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.30 – 数据景观摘要
- en: Now that we have discussed and learned about the concepts of data governance
    and have practically implemented data governance around a use case, let’s learn
    about data security.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论并学习了数据治理的概念，并在一个用例中实际实施了数据治理，让我们来了解数据安全。
- en: Understanding the need for data security
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据安全的需求
- en: 'Before we understand the need for data security, let’s try to define what data
    security is. **Data security** is the process of protecting enterprise data and
    preventing any data loss from malicious or unauthorized access to data. Data security
    includes the following tasks:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解数据安全的需求之前，让我们尝试定义一下什么是数据安全。**数据安全**是保护企业数据并防止数据因恶意或未经授权的访问而丢失的过程。数据安全包括以下任务：
- en: Protecting sensitive data from attacks.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护敏感数据免受攻击。
- en: Protecting data and applications from any ransomware attacks.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护数据和应用程序免受任何勒索软件攻击。
- en: Protecting against any attacks that can delete, modify, or corrupt corporate
    data.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防御任何可能删除、修改或破坏企业数据的攻击。
- en: Allowing access and control of data to the necessary user within the organization.
    Again, read-only, write, and delete access is provided to the data based on the
    role and its use.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组织内部允许必要的用户访问和控制数据。再次强调，根据角色及其用途，数据提供只读、写入和删除访问权限。
- en: Some industries may have stringent data security requirements. For example,
    a US health insurance company needs to ensure PHI data is extremely well protected
    according to HIPAA standards. Another example is that a financial firm such as
    Bank Of America has to ensure card and account data is extremely secure because
    that can cause direct monetary loss. But even if there is no stringent data security
    requirement in your organization, data security is essential to prevent data loss
    and loss of trust of the organization’s customers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一些行业可能对数据安全有严格的要求。例如，一家美国健康保险公司需要确保根据HIPAA标准，PHI数据得到极好的保护。另一个例子是，像美国银行这样的金融机构必须确保卡和账户数据极其安全，因为这可能导致直接的货币损失。但即使在没有严格的数据安全要求的情况下，数据安全对于防止数据丢失和防止组织客户对组织的信任损失也是至关重要的。
- en: Although data privacy and data security are overlapping terms, it helps to understand
    the subtle difference between the two. Data privacy ensures that only an authenticated
    user can access the data, and even if the data is accessed somehow, it should
    be encrypted or tokenized so that unauthorized users cannot use the data. Data
    security includes data privacy concerns, but apart from that, it focuses on tackling
    any malicious activity concerning the data. One simple example is that, to protect
    PHI data, Cigna Health insurance has encrypted all its sensitive data. This ensures
    data privacy but not data security. Although hackers may not be able to decipher
    the encrypted data, they can still delete the data or double encrypt the data
    so that the data becomes unusable.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据隐私和数据安全是重叠的术语，但了解两者之间的细微差别是有帮助的。数据隐私确保只有经过身份验证的用户才能访问数据，即使数据被某种方式访问，它也应该被加密或标记化，以便未经授权的用户无法使用数据。数据安全包括数据隐私问题，但除此之外，它还专注于解决与数据相关的任何恶意活动。一个简单的例子是，为了保护PHI数据，Cigna健康保险公司已对其所有敏感数据进行加密。这确保了数据隐私，但并未确保数据安全。尽管黑客可能无法解密加密数据，但他们仍然可以删除数据或对数据进行双重加密，使数据变得无法使用。
- en: Now, let’s discuss why data security is needed.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下为什么数据安全是必要的。
- en: 'Every year, the damage that’s caused by data breaches is around $8 billion
    in the US alone and, on average, each incident causes approximately 25,000 accounts
    to be compromised. Let’s explore a few of the biggest data breaches that have
    happened over the last few years:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，仅在美国，数据泄露造成的损失就约为80亿美元，平均每次事件导致大约25,000个账户受到损害。让我们探讨一下过去几年发生的几起最大的数据泄露事件：
- en: In June 2013, Capital One reported a data breach that affected all 106 million
    of its accounts. Personal information such as credit history, social security
    numbers, and bank accounts was compromised.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2013年6月，Capital One报告了一起数据泄露事件，影响了其1.06亿个账户。个人信息，如信用记录、社会保障号码和银行账户信息遭到泄露。
- en: In June 2021, a massive data breach happened on LinkedIn, wherein 700 million
    accounts (92% of the total accounts at that time) were affected. User data was
    posted for sale on the dark web.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2021年6月，LinkedIn发生了一起大规模数据泄露事件，当时有7亿个账户（当时总账户数的92%）受到影响。用户数据被发布在暗网上出售。
- en: In April 2019, Facebook reported an attack that compromised 533 million accounts.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2019年4月，Facebook报告了一起攻击事件，影响了5.33亿个账户。
- en: 'As we can see, these breaches not only pose security risks to the compromised
    data but also damage the reputation and trust of the company. Now, let’s discuss
    a few common data security threats, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这些泄露不仅对受损害的数据构成安全风险，还损害了公司的声誉和信任。现在，让我们讨论一些常见的数据安全威胁，如下所示：
- en: '**Phishing and other social engineering attacks**: Social engineering is a
    common way to trick or manipulate individuals to gain unauthorized access to corporate
    data or gather confidential information. Phishing is a common form of social engineering
    attack. Here, messages or emails that appear to be from a trusted source are sent
    to individuals. They contain malicious links that, when clicked, can give unauthorized
    access to corporate networks and data.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**钓鱼和其他社会工程学攻击**：社会工程学是欺骗或操纵个人以获取对公司数据未授权访问或收集机密信息的常用方法。钓鱼是社会工程学攻击的一种常见形式。在这里，看似来自可信来源的消息或电子邮件被发送给个人。它们包含恶意链接，当点击时，可以未授权访问公司网络和数据。'
- en: '**Insider threats**: Insider threats are caused by an employee who inadvertently
    or intentionally threatens the security of corporate data. They can be of the
    following types:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部威胁**：内部威胁是由无意或有意威胁公司数据安全的员工引起的。它们可以是以下类型：'
- en: Malicious actors, who intentionally steal data or cause harm to the organization
    for personal gain.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意行为者，他们出于个人利益故意窃取数据或对组织造成损害。
- en: Non-malicious actors, who cause threats accidentally or because they are unaware
    of security standards.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非恶意行为者，他们意外地造成威胁或因为他们不了解安全标准。
- en: Compromised users, whose systems are compromised by external attackers without
    their knowledge. Then, attackers perform malicious activities, pretending to be
    legitimate users.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被入侵的用户，他们的系统在没有他们意识的情况下被外部攻击者入侵。然后，攻击者假装成合法用户执行恶意活动。
- en: '**Ransomware**: This is a form of malware that infects corporate systems and
    encrypts data so that it becomes useless without a decryption key. Attackers then
    display a ransom message for payment to release the decryption key.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**勒索软件**：这是一种感染企业系统并加密数据的恶意软件，使其在没有解密密钥的情况下变得无用。然后，攻击者显示勒索信息以支付解密密钥。'
- en: '**SQL injection**: Here, attackers gain access to databases and their data.
    Then, they inject unwanted code into seemingly innocent SQL queries to perform
    unwanted operations, thereby either deleting or corrupting the data.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQL注入**：在这里，攻击者获取对数据库及其数据的访问权限。然后，他们向看似无害的SQL查询中注入不需要的代码以执行不需要的操作，从而删除或损坏数据。'
- en: '**Distributed Denial of Service (DDoS) attacks**: A DDoS attack is a malicious
    attempt to disrupt a web resource such as a web service. It acts by sending a
    huge volume of dummy calls to the web server. Due to the extreme volume of service
    calls at a very short burst, the web server crashes or becomes unresponsive, causing
    web resource downtime.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式拒绝服务（DDoS）攻击**：DDoS攻击是一种恶意尝试破坏网络资源，如网络服务的攻击。它通过向网络服务器发送大量虚假调用来实现。由于在极短的时间内服务调用量极大，网络服务器崩溃或变得无响应，导致网络资源停机。'
- en: '**Compromised data in the cloud**: Ensuring security in the cloud is a big
    challenge. Security is required while sending data over the network and for data
    at rest in the cloud.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云中的数据泄露**：确保云中的安全是一个巨大的挑战。在通过网络发送数据以及在云中静止的数据都需要安全措施。'
- en: In this section, we discussed what data security is and the threats that can
    occur in absence of it by covering some real-world examples. Now, let’s discuss
    the solutions and tools available for data security.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过一些现实世界的例子讨论了数据安全是什么以及缺乏数据安全可能发生的威胁。现在，让我们讨论可用于数据安全性的解决方案和工具。
- en: Solution and tools available for data security
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用于数据安全性的解决方案和工具
- en: 'In the previous section, we briefly discussed what data security is and why
    it is needed. We also looked at a few common data security threats. The solutions
    and tools described here help mitigate or minimize the risk from the threats discussed
    in the previous section:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们简要讨论了数据安全是什么以及为什么需要它。我们还查看了一些常见的数据安全威胁。这里描述的解决方案和工具有助于减轻或最小化前一节中讨论的威胁的风险：
- en: '**Data discovery and classification**: To ensure data security, it is important
    to discover sensitive information. This technique uses data discovery to classify
    data into various security labels (such as confidential, public, and so on). Once
    classification is done, security policies can be applied to the various classifications
    of data according to the organization’s needs.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据发现和分类**：为确保数据安全，发现敏感信息非常重要。这项技术使用数据发现将数据分类到各种安全标签（如机密、公开等）。一旦完成分类，就可以根据组织的需要将安全策略应用于各种数据分类。'
- en: '**Firewalls**: This is the first line of defense against any network intrusions.
    They exclude any undesirable traffic from entering the network. They also help
    open specific ports to the external network, which gives hackers less of a chance
    to enter the network.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防火墙**：这是任何网络入侵的第一道防线。它们阻止任何不受欢迎的流量进入网络。它们还帮助打开到外部网络的特定端口，这给黑客进入网络的机会更少。'
- en: '**Backup and recovery**: This helps organizations protect themselves in case
    the data is deleted or destroyed.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备份和恢复**：这有助于组织在数据被删除或损坏的情况下保护自己。'
- en: '**Antivirus**: This is used to detect any viruses, Trojans, and rootkits that
    can steal, modify, or damage your data. They are widely used for personal and
    corporate data security.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杀毒软件**：这用于检测可能窃取、修改或损坏您数据的任何病毒、特洛伊木马和后门程序。它们被广泛用于个人和企业数据安全。'
- en: '**Intrusion Detection and Prevention Systems** (**IDS/IPS**): IDS/IPS performs
    a deep inspection of packets on network traffic and logs any malicious activities.
    These tools help stop DDoS attacks.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**入侵检测和预防系统**（**IDS/IPS**）：IDS/IPS 对网络流量中的数据包进行深度检查，并记录任何恶意活动。这些工具有助于阻止 DDoS
    攻击。'
- en: '**Security Information and Event Management** (**SIEM**): This analyzes recorded
    logs from various kinds of devices such as network devices, servers, and applications,
    and generates security alert events based on specific criteria and thresholds.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全信息和事件管理**（**SIEM**）：这分析来自各种设备（如网络设备、服务器和应用程序）的记录日志，并根据特定标准和阈值生成安全警报事件。'
- en: '**Data Loss Prevention** (**DLP**): This mechanism monitors different devices
    to make sure sensitive data is not copied, moved, or deleted without proper authorization.
    It also monitors and logs who is using the data.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据丢失预防**（**DLP**）：此机制监控不同设备，以确保敏感数据未经适当授权不会被复制、移动或删除。它还监控并记录谁在使用数据。'
- en: '**Access control**: This technique allows or denies read, write, or delete
    access to individual data resources. Access controls can be implemented using
    an **Access Control List** (**ACL**) or **Role-Based Access Control** (**RBAC**).
    Cloud security systems such are IAM are used to enforce access control on the
    cloud.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制**：这项技术允许或拒绝对单个数据资源的读取、写入或删除访问。访问控制可以通过使用**访问控制列表**（**ACL**）或**基于角色的访问控制**（**RBAC**）来实现。云安全系统，如
    IAM，用于在云上强制执行访问控制。'
- en: '**Security as a Service**: This is modeled on the lines of Software-as-a-Service.
    Here, service providers provide the security of corporate infrastructure on a
    subscription basis. It uses a pay-as-you-go model.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全即服务**：这是基于软件即服务的模式。在这里，服务提供商基于订阅模式提供企业基础设施的安全服务。它使用按使用付费的模式。'
- en: '**Data encryption**: Data such as PHI can be very sensitive. If it is lost
    or leaked, this can cause regulatory issues and heavy financial losses. To provide
    data protection on such data, we can either encrypt the data (where the masked
    data loses all the original data properties, such as size) or tokenize it (where
    the masked data retains the properties of the original data).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加密**：如 PHI 这样的数据可能非常敏感。如果丢失或泄露，这可能导致监管问题和严重的经济损失。为了保护此类数据，我们可以加密数据（其中被屏蔽的数据失去了所有原始数据属性，如大小）或对其进行标记（其中被屏蔽的数据保留了原始数据的属性）。'
- en: '**Physical security**: Finally, physical security is essential to stop unauthorized
    access to data. Physical security can be enabled by creating strong security policies
    and implementing them. Policies such as locking the system whenever you step out,
    no tailgating, and others can help prevent social engineering attacks and unauthorized
    access to data.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理安全**：最后，物理安全对于阻止对数据的未授权访问至关重要。可以通过创建强大的安全策略并实施它们来启用物理安全。例如，每次离开时锁定系统、不允许尾随他人等策略可以帮助防止社会工程攻击和对数据的未授权访问。'
- en: In this section, we learned about the data security solutions and tools that
    are available in the industry. Now, let’s summarize what we’ve learned in this
    chapter.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了行业中所提供的各种数据安全解决方案和工具。现在，让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned what data governance is and why is it needed. Then,
    we briefly discussed the data governance framework. After that, we discussed how
    to develop a practical data governance solution using DataHub. Next, we learned
    what data security is and why it is needed. Finally, we briefly discussed the
    various solutions and tools that are available for ensuring data security.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了数据治理是什么以及为什么需要它。然后，我们简要讨论了数据治理框架。之后，我们讨论了如何使用DataHub开发一个实用的数据治理解决方案。接下来，我们学习了数据安全是什么以及为什么需要它。最后，我们简要讨论了确保数据安全所提供的各种解决方案和工具。
- en: With that, we have learned how to ingest data using real-time and batch-based
    pipelines, popular architectural patterns for data ingestion, and data governance
    and data security. In the next chapter, we will discuss how to publish the data
    as a service for downstream systems.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些，我们已经学习了如何使用实时和基于批次的管道来摄取数据，以及数据摄取的流行架构模式，以及数据治理和数据安全。在下一章中，我们将讨论如何将数据作为服务发布给下游系统。
- en: Section 3 – Enabling Data as a Service
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3节 – 启用数据即服务
- en: This section of the book focuses on architecting solutions for Data as a Service.
    In this part, you will learn how to build various kinds of Enterprise grade **Data
    as a Service** (**DaaS**) solutions and secure and manage them properly.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本书本节主要关注构建数据即服务（Data as a Service，**DaaS**）解决方案的架构。在本部分，你将学习如何构建各种类型的企业级**数据即服务**（**DaaS**）解决方案，并正确地管理和保护它们。
- en: 'This section comprises the following chapters:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括以下章节：
- en: '[*Chapter 9*](B17084_09.xhtml#_idTextAnchor144)*, Exposing MongoDB Data as
    a Service*'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B17084_09.xhtml#_idTextAnchor144)*，将MongoDB数据作为服务暴露*'
- en: '[*Chapter 10*](B17084_10.xhtml#_idTextAnchor157)*, Federated and Scalable DaaS
    with GraphQL*'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B17084_10.xhtml#_idTextAnchor157)*，使用GraphQL的联邦和可扩展DaaS*'
