- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enabling Data Security and Governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we learned how to evaluate requirements and analyze
    and apply various architectural patterns to solve both real-time and batch-based
    problems. We learned how to choose the optimal technical stack and develop, deploy,
    and execute the proposed solution. We also discussed various popular architectural
    patterns for data ingestion. However, any discussion about data architecture is
    incomplete without mentioning data governance and data security. In this chapter,
    we will focus on understanding and applying data governance and security in the
    data layer.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first discuss what data governance is, and why it is
    so important. We will also briefly discuss a few open source data governance tools
    that are available on the market. Then, we will practically demonstrate a data
    governance implementation by adding a data governance layer to a data ingestion
    pipeline. The data ingestion pipeline ID will be developed using Apache NiFi and
    data governance will be achieved using DataHub. Then, we will discuss the need
    for data security and the types of solutions that help enable it. Finally, we
    will discuss a few open source tools available to enable data security.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will know the definition of and need for a data
    governance framework. You will also know when data governance is required and
    all about the data governance framework by the **Data Governance Institute** (**DGI**).
    In addition, you will know how to implement practical data governance using DataHub.
    Finally, you will know about various solutions and tools available to enable data
    security.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing data governance – what and why
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical data governance using DataHub and NiFi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for data security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solutions and tools available for data security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenJDK 1.11 installed on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker installed on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NiFi 1.12.0 installed on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataHub installed on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior knowledge of YAML is preferred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be downloaded from this book’s GitHub repository:
    [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing data governance – what and why
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s try to understand what data governance is and what it does. In
    layman’s terms, **data governance** is the process of assigning proper authority
    and taking decisions regarding data-related matters. According to the DGI, data
    governance is defined as “*a system of decision rights and accountabilities for
    information-related processes, executed according to agreed-upon models that describe
    who can take what actions with what information, and when, under what circumstances,
    using what methods.*”
  prefs: []
  type: TYPE_NORMAL
- en: As evident from this definition, it is a practice of creating definitive strategies,
    policies, and rules that define who can make what decisions or perform actions
    related to data. It also lays out guidelines about how to make decisions related
    to data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data governance considers the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enterprise-level organizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision rights and procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and controlling data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of data governance, let’s explore what
    scenarios data governance is recommended in.
  prefs: []
  type: TYPE_NORMAL
- en: When to consider data governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A formal data governance framework should be considered in any of the following
    scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The data in the organization has grown so much and become so complex that traditional
    data management tools are unable to solve cross-functional data needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The horizontally focused business units and teams need cross-functional data
    generated or maintained by other focused groups. In this case, an enterprise-wide
    solution (instead of siloed) for data availability and management is required.
    For example, for B2B sales of Visa, the marketing and accounting departments maintain
    data in silos. However, the sales department needs both these silos’ data to create
    accurate sales predictions. So, the data across these different departments should
    be stored in an enterprise-wide central repository instead of silos. Hence, this
    enterprise-wide data requires data governance for proper access, use, and management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance, legal, and contractual obligations can also call for formal data
    governance. For example, the **Health Insurance Portability and Accountability
    Act** (**HIPAA**) enforces that all **protected health information** (**PHI**)
    data should be well governed and protected from theft or unauthorized access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we’ve discussed what data governance is and when we should have a formal
    data governance framework. Now we will learn about the DGI data governance framework.
  prefs: []
  type: TYPE_NORMAL
- en: The DGI data governance framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DGI data governance framework is a logical structure for implementing proper
    data governance in an organization to enable better data-related decisions to
    be made. The following diagram depicts this data governance framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Data governance framework ](img/B17084_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Data governance framework
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the various components of the DGI data governance
    framework. Let’s take a look at each of the components highlighted in the preceding
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mission**: The mission of the DGI framework can be attributed to three primary
    responsibilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define rules
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute and implement the rules by providing ongoing protection and services
    to data stakeholders
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deal with scenarios that arise due to non-compliance
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This form of mission is very similar to political governance. In the political
    governance model, the government makes the rules, the executive branch implements
    the rules, and the judiciary deals with non-compliance or rule breakers. Just
    like political governance, here, one set of data stakeholders defines the rules.
    Another set of stakeholders ensures that the rules are followed. Finally, a third
    set of stakeholders makes decisions related to non-compliance. Optionally, the
    organization can develop a vision statement around the mission, which may be used
    to inspire data stakeholders to envision possibilities and set data-related goals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Focus areas**: Mission and vision lead us to the primary focus areas. We
    have two primary focus areas. They are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goals should be **SMART** – that is, **Specific, Measurable, Actionable, Relevant,
    and Timely**. We should remember the principle of the four P’s while deciding
    on the goals we want to pursue – that is, programs, projects, professional disciplines,
    and people. We must ask how these efforts can help our organization in terms of
    revenue, cost, complexity, and ensuring survival (that is, security, compliance,
    and privacy).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics should also be SMART. Everyone in data governance should know how to
    quantify and measure success.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This discussion leads to the question of where we can fund our data governance
    program. To do so, we must ask the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we fund the data governance office?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we fund the data architects/analysts to define rules and data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we fund the data stewardship activities?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus areas are very important when planning formal data governance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data rules and definitions**: In this component, rules, policies, standards,
    and compliances are set around data. Typical activities may include creating new
    rules, exploring existing rules, and addressing gaps and overlaps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decision rights**: This component implores the question, *Who can make a
    data-related decision, when, and using what process?* The data governance program
    allows us to store decision rights as metadata for data-related decisions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accountability**: This component creates accountability for the implementation
    of a rule or decision once it is made. The data governance program may be expected
    to integrate the accountabilities in a day-to-day **software development life
    cycle** (**SDLC**).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Controls**: Data is the new gold, so there is a huge security risk involved
    if there are sensitive data breaches. How do we ensure that such risks are mitigated
    and handled properly? They can be handled using controls. Controls can be preventive
    or reactive. The data governance team is usually tasked with making recommendations
    for creating these controls at different levels of the control stack (networking/OS/database/application).
    Sometimes, the data governance team is also asked to modify the existing controls
    to ensure better data governance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data stakeholders**: Data stakeholders are individuals or groups who either
    could affect or are affected by the data. Since they have a direct correlation
    with the data, they are consulted while taking data decisions. Again, based on
    the scenario, they may want to be involved in some data-related decisions, should
    be consulted before decisions are finalized, or be informed once decisions are
    made.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data governance office**: This facilitates, supports, and runs the data governance
    program and data stewardship activities. It is the centralized governing body
    and usually consists of data architects, data analysts, and those working on creating
    the metadata. It collects and aligns policies, rules, and standards from different
    stakeholders across the organization and comes up with organizational-level rules
    and standards. It is responsible for providing centralized data governance-related
    communication. It is also responsible for collecting the data governance metrics
    and publishing reports and success measures to all the data stakeholders.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data stewards**: They are part of the data stewardship council, which is
    responsible for making data-related decisions such as setting policies, specifying
    standards, or providing recommendations to the DGO. Based on the size and complexity
    of the organization, the data stewardship council(s) may have a hierarchy. In
    data quality-focused governance projects, there can be an optional data quality
    steward.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data processes**: These are the methods that are used to govern data. The
    processes should be documented, standardized, and repeatable. They are designed
    to follow regulatory and compliance requirements for data management, privacy,
    and security.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All these components work in tandem and create a feedback loop, which ensures
    data governance is continuously improving and up to date.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about what data governance is, when it should be
    implemented, and the DGI framework. In the next section, we will provide step-by-step
    guidelines for implementing data governance.
  prefs: []
  type: TYPE_NORMAL
- en: Practical data governance using DataHub and NiFi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss a tool called DataHub and how different data
    stakeholders and stewards can make use of it to enable better data governance.
    But first, we will understand the use case and what we are trying to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will build a data governance capability around a data ingestion
    pipeline. This data ingestion pipeline will fetch any new objects from an S3 location,
    enrich them, and store the data in a MySQL table. In this particular use case,
    we are getting telephone recharge or top-up events in an S3 bucket from various
    sources such as mobile or the web. We are enriching this data and storing it in
    a MySQL database using an Apache NiFi pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache NiFi is a powerful and reliable drag-and-drop visual tool that allows
    you to easily process and distribute data. It creates directed graphs to create
    a workflow or a data pipeline. It consists of the following high-level components
    so that you can create reliable data routing and transformation capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FlowFile**: Each data record is serialized and processed as a FlowFile object
    in the NiFi pipeline. A FlowFile object consists of flowfile content and attribute
    denoting data content and metadata respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processor**: This is one of the basic units of NiFi. This component is primarily
    responsible for processing the data. It takes a FlowFile as input, processes it,
    and generates a new one. There are around 300 inbuilt processors. NiFi allows
    you to develop and deploy additional custom processors using Java.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Queue**: NiFi follows a staged event-driven architecture. This means that
    communication between processors is asynchronous. So, there needs to be a message
    bus that will hold a FlowFile generated by one processor, before it is picked
    by the next processor. This message bus is called the NiFi queue. Additionally,
    it supports setting up backpressure thresholds, load balancing, and prioritization
    policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller service**: This allows you to share functionality and state across
    JVM cleanly and consistently. It is responsible for things such as creating and
    maintaining database connection pools or distributed caches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processor groups**: When a data flow becomes complex, it makes more sense
    to group a set of components, such as processors and queues, into an encapsulation
    called a processor group. A complex pipeline may have multiple processor groups
    connected. Each processor group, in turn, has a data flow inside it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ports**: Processor groups can be connected using ports. To get the input
    from an external processor group, input ports are used. To send a processed FlowFile
    out of the processor group, output ports are used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s build the NiFi pipeline for our use case. Our source is an S3 bucket
    called `chapter8input`, while our output is a MySQL cluster. We will be receiving
    S3 objects in the `chapter8input` folder from different sources. Each S3 object
    will be in JSON format and will contain one phone recharge or top-up (bundle)
    event. Our data sink is a MySQL table called `bundle_events`. The **Data Definition
    Language** (**DDL**) of this table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, the NiFi pipeline polls the S3 bucket and checks for any change events,
    such as a JSON file being created or updated. Once a file is uploaded to the S3
    bucket, NiFi should fetch the file, enrich it with the source type using the S3
    object name, and then write the enriched data to the MySQL `bundle_events` table.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have Apache NiFi installed in your system, download and install
    Apache NiFi-1.12.0\. You can download, install, and follow the startup instructions
    at [https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi](https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi)
    to do so. Alternatively, you can spin up a NiFi cluster/node on an AWS EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the NiFi pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we will discuss how to build the NiFi pipeline. The NiFi pipeline that
    we will build is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – NiFi pipeline to read data from S3 and write to MySQL ](img/B17084_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – NiFi pipeline to read data from S3 and write to MySQL
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to understand the overall NiFi pipeline and how we are building it:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we have used a **ListS3** processor to capture whether any S3 objects
    have been inserted or updated in the configured S3 bucket. It lists all the change
    events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, using the **SplitRecord** processor, we split the records into individual
    events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `sourceKey` attribute for the FlowFile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we use **FetchS3Object** to fetch the S3 object. The **FetchS3Oject**
    processor is responsible for reading the actual S3 object. If **FetchS3Object**
    successfully reads the file, it sends the S3 object content as a FlowFile to the
    **JoltTransformRecord** processor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**JoltTransformRecord** is used to enrich the data before the enriched data
    is sent to be written to MySQL using the **PutDatabaseRecord** processor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The success of the **PutDatabaseRecord** processor is sent to the **LogSuccessMessage**
    processor. As shown in the preceding screenshot, all FlowFiles in failure scenarios
    are sent to the **LogErrorMessage** processor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s configure each of the NiFi processors present in the NiFi pipeline
    (shown in *Figure 8.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '**ListS3 processor**: The configuration of the **ListS3** processor is shown
    in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Configuring the ListS3 processor ](img/B17084_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Configuring the ListS3 processor
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the **ListS3** processor is configured to poll and listen for
    changes in the **chapter8input** S3 bucket. Apart from the bucket name, we must
    configure the **Region**, **Access Key ID**, and **Secret Access Key** details
    for the NiFi instance to connect to the AWS S3 bucket. Finally, we have configured
    the Record Writer property, which is set as a controller service of the **JsonRecordSetWriter**
    type.
  prefs: []
  type: TYPE_NORMAL
- en: '**SplitRecord processor**: Next, we will configure the **SplitRecord** processor,
    as shown in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Configuring the SplitRecord processor ](img/B17084_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Configuring the SplitRecord processor
  prefs: []
  type: TYPE_NORMAL
- en: '**SplitRecord** is responsible for splitting a single FlowFile containing multiple
    write events in S3 into individual events. Now, each event is metadata for one
    **S3Object**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**EvaluateJsonPath processor**: We use the **EvaluateJsonPath** processor to
    extract the value of the **key** column from the FlowFile content and add it as
    an attribute to the FlowFile attributes. The configuration of the **EvaluateJsonPath**
    processor is shown in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Configuring the EvaluateJsonPath processor ](img/B17084_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Configuring the EvaluateJsonPath processor
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have configured the `flowfile-attribute` value. This indicates that
    a new key-value pair will be added to the FlowFile attributes. The attribute to
    be added should be provided as a dynamic property, where the name of the property
    will be the attribute key and the value of the property will be the attribute
    value. Here, we have added a property named `$.key`. This expression fetches the
    value of the `key` field from the FlowFile content (which is a JSON).
  prefs: []
  type: TYPE_NORMAL
- en: '**FetchS3Object processor**: The following screenshot shows the configuration
    of the **FetchS3Object** processor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Configuring the FetchS3Object processor ](img/B17084_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Configuring the FetchS3Object processor
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the `${sourcekey}` is a NiFi expression that gets the value of
    the **sourcekey** attribute. Apart from this, the S3-related properties such as
    **Bucket**, **Access Key ID**, and **Secret Access Key** need to be set in this
    processor. The output of this processor is the content of our **S3Object** (which
    is in JSON format).
  prefs: []
  type: TYPE_NORMAL
- en: '**JoltTransformRecord processor**: The **JoltTransform** processor is used
    to add a new key-value pair to the JSON. The configuration of the **JoltTransformRecord**
    processor is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Configuring the JoltTransformRecord processor ](img/B17084_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Configuring the JoltTransformRecord processor
  prefs: []
  type: TYPE_NORMAL
- en: The `shift`, `chain`, and `default`. For more information about Apache Jolt,
    go to [https://github.com/bazaarvoice/jolt#jolt](https://github.com/bazaarvoice/jolt#jolt).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will discuss the Jolt transformation we are using to add a key-value
    pair to the JSON FlowFile content. We add the `source` key using the **Jolt Specification**
    property, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Using the Jolt Specification property to add the source key
    ](img/B17084_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Using the Jolt Specification property to add the source key
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, by setting `operation` to `default` in `source` key with the
    dynamic value calculated using the NiFi expression language. `${sourcekey:substringBefore('_')}`
    is a NiFi expression. This expression returns a substring of the `FlowFile` attribute’s
    `'_'`).
  prefs: []
  type: TYPE_NORMAL
- en: '**PutDatabaseRecord processor**: Once this JSON is enriched with the newly
    added key-value pair, the record is written to the MySQL database using the **PutDatabaseRecord**
    processor. The following screenshot shows the configuration of the **PutDatabaseRecord**
    processor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Configuring the PutDatabaseRecord processor ](img/B17084_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Configuring the PutDatabaseRecord processor
  prefs: []
  type: TYPE_NORMAL
- en: 'As evident from this configuration, we need to configure two controller services
    – `MySQL`, `INSERT`, and `bundle_events`. Now, let’s see how the **DBCPConnectionPool**
    service, called **MysqlDBCPConnectionPool**, is configured. The following screenshot
    shows the configuration of the **MysqlDBCPConnectionPool** service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Configuring the DBCPConnectionPool service ](img/B17084_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Configuring the DBCPConnectionPool service
  prefs: []
  type: TYPE_NORMAL
- en: The **MysqlDBCPConnectionPool** controller service is used to create a JDBC
    connection pool. To do so, you must configure the **Database Connection URL**
    (the JDBC URL), **Database Driver Class Name**, and **Database Driver Location(s)**
    properties, as shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have built the entire NiFi pipeline to extract data from S3, enrich
    it, and write it to a MySQL table. You can check out the entire NiFi pipeline
    by going to [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up DataHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will add a data governance layer to our solution. Although many data
    governance tools are available, most of them are paid tools. There are several
    data governance tools available for pay-as-you-go models in the cloud, such as
    AWS Glue Catalog. However, since our solution will be running on-premises, we
    will choose a platform-agnostic open source tool. DataHub, developed by LinkedIn,
    is one such open source tool that comes with a decent set of features. We will
    use it in this chapter to explain data governance practically. In this section,
    we will learn how to configure DataHub. In this pipeline, S3 is the source, MySQL
    is the target, and NiFi is the processing engine. To create data governance, we
    need to connect to all these systems and extract metadata from them. DataHub does
    this for us.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin connecting these components (of the data pipeline) to DataHub,
    we need to download and install DataHub in our local Docker environment. The detailed
    installation instructions can be found at [https://datahubproject.io/docs/quickstart](https://datahubproject.io/docs/quickstart).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to connect different data sources to
    DataHub.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an ingestion source to DataHub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To connect a data source or pipeline component to DataHub, we must go to the
    **Ingestion** tab from the top-right menu bar, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Connecting a new source for metadata ingestion in DataHub ](img/B17084_08_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Connecting a new source for metadata ingestion in DataHub
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must click the **Create new source** button to create a new connection
    to a data source. On clicking this button, we get the following popup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Choosing the type of data source to create a new connection
    ](img/B17084_08_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Choosing the type of data source to create a new connection
  prefs: []
  type: TYPE_NORMAL
- en: 'This popup is a multipage wizard where you can create a new data source connection.
    First, as shown in the preceding screenshot, you are asked to choose a data source
    type. For example, if the data is in a MySQL database, then we would select MySQL
    from the various thumbnails present in the wizard. For sources, which are not
    listed, such as NiFi and S3, we can select the **Custom** thumbnail. Once you’ve
    chosen a type, click **Next**; you will be taken to the second step called **Configure
    Recipe**, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Configuring a recipe for the data source ](img/B17084_08_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Configuring a recipe for the data source
  prefs: []
  type: TYPE_NORMAL
- en: In the `YAML` code in the space provided. Irrespective of the data source, the
    YAML has two top-level elements – `source` and `sink`. Again, both `source` and
    `sink` contain two elements – `type` and `config`. Here, `type` denotes the type
    of source or sink. For example, in the preceding screenshot, we are configuring
    NiFi, so the source type is `nifi`. If we had selected MySQL as the type of connection
    in the previous step, the source’s `type` in this step would be `mysql`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are primarily three types of sinks that are currently available in DataHub,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stdout`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataHub**: The metadata is sent to DataHub via the GMS REST API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**File**: The metadata is sent to a configured file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we will be using DataHub as the sink since this allows us to do governance
    and monitoring using DataHub’s capabilities. In this case, our type of sink will
    be `datahub-rest`. We also need to specify the GMS REST API’s HTTP address as
    the base path. Since our DataHub installation is using Docker, we will use `localhost:9002`
    as the server IP address (as shown in *Figure 8.13*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once everything has been added, click **Next** to go to the **Schedule Execution**
    step, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Create an execution schedule ](img/B17084_08_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Create an execution schedule
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we set an execution schedule for DataHub to fetch metadata from
    the source. In this case, we configured a CRON expression, `0,30 * * * *`, which
    means it will run every 30 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Next** to go to the last step, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – The final step of the New Ingestion Source wizard ](img/B17084_08_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – The final step of the New Ingestion Source wizard
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final step of the wizard, we need to enter the name of the ingestion
    source. This name can be any arbitrary name that helps uniquely identify that
    source. Finally, we must click **Done** to add the new data source. The following
    screenshot shows the added sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Sources added at a glance ](img/B17084_08_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Sources added at a glance
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, we can monitor how many times a job has
    run to fetch metadata from each source and whether the last run was successful
    or not. All the YAML files that have been used for our use case can be found on
    GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how we can perform different data governance
    tasks using this tool.
  prefs: []
  type: TYPE_NORMAL
- en: Governance activities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in the preceding section, once a data source or pipeline is connected
    with DataHub, it provides a lot of tools to enable a data governance model around
    those sources. The following are a few capabilities that we will explore in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding a domain**: A domain in DataHub can be any logical grouping; it might
    be organization-specific. This helps us analyze the domain-wise resource utilization
    and other statistics. For example, an organization can use business unit names
    as domains. In our particular scenario, **DataServices** is a business unit in
    the organization, and we create a domain in the name of the business unit. To
    create a new domain, we can navigate to **Manage** | **Domains** and click the
    **New Domain** button. Once this button is clicked, a popup dialog will open,
    as shown in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Creating a new domain ](img/B17084_08_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Creating a new domain
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we must provide a name and description for a domain. You cannot
    create two domains with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding a user group**: We can manage users and groups by going to **Settings**
    and selecting the **Groups** tab. Upon clicking the **Create group** button, a
    dialog similar to the following will appear:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Adding a new user group ](img/B17084_08_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Adding a new user group
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we must provide a group name and an optional description. User
    groups help make any user part of a group, and accountability, ownership, access
    policies and rules can be assigned to a group.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring metadata**: This activity in data governance comes under data definition.
    Most data governance tools support metadata management. Here, as shown in the
    following screenshot, the DataHub dashboard provides a summary of the metadata:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Platform-wise and domain-wise resource summary ](img/B17084_08_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Platform-wise and domain-wise resource summary
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows the various platforms and objects present on
    each platform. It also shows how many resources are in each domain. For enterprise
    data governance, it is important to monitor and understand how many resources
    are being used by different business units for auditing, tracking, and financial
    purposes. The following screenshot shows a MySQL table resource and its corresponding
    data definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Adding descriptions to metadata ](img/B17084_08_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Adding descriptions to metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, we can add a description to the metadata. On the other hand,
    the following screenshot shows how the schema of the table is loaded and seen
    in DataHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Schema of the table  ](img/B17084_08_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – Schema of the table
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, descriptions, terms, and tags can be added and maintained in
    each column of the schema. This enables data definition activities for data governance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring lineage**: DataHub allows us to explore the lineage of the data.
    In the data definition of data governance, apart from maintaining the metadata
    and descriptions, proper governance must know the origin of the data and how it
    is being used. This aspect is covered by **Lineage**, as shown in the following
    screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Data lineage ](img/B17084_08_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – Data lineage
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see **1 Downstream** and no upstream events for the **chapter8input**
    S3 bucket. We can click on the **Visualize Lineage** button to see the lineage
    as a diagram, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Visualizing data lineage as a workflow ](img/B17084_08_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – Visualizing data lineage as a workflow
  prefs: []
  type: TYPE_NORMAL
- en: Apart from visual lineage, you can explore the impact analysis of this resource.
    This impact analysis helps communicate with the impacted groups in case a change
    or maintenance event occurs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating accountability by adding a domain and owners to a resource**: We
    can start creating accountability of resources by attaching a domain and one or
    multiple owners to a resource from the landing page of the resource, as shown
    in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Assigning a domain and owners ](img/B17084_08_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – Assigning a domain and owners
  prefs: []
  type: TYPE_NORMAL
- en: 'An owner can be a group or a user. However, it is advisable to assign ownership
    to a group and add users to that group. When adding an owner, you can choose the
    type of ownership, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Adding an owner ](img/B17084_08_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – Adding an owner
  prefs: []
  type: TYPE_NORMAL
- en: Here, while adding the **offer_analytics** owner, we set the type of owner to
    **Technical Owner**. An owner can be one of three types – **Technical Owner**,
    **Business Owner**, or **Data Steward**. **Technical Owner** is accountable for
    producing, maintaining, and distributing the asset. **Business Owner** is a domain
    expert associated with the asset. Finally, **Data Steward** is accountable for
    the governance of the asset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting policies**: DataHub allows us to set policies. Policies are sets
    of rules that define privileges over an asset or the DataHub platform. There are
    two categories of policies – **Platform** and **Metadata**. The **Platform** policy
    allows us to assign DataHub platform-level privileges to users or groups, while
    the **Metadata** policy allows us to assign metadata privileges to users or groups.
    To create a new policy, navigate to **Settings** | **Privileges**. Upon clicking
    the **Create New Policy** button, a wizard appears, as shown in the following
    screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Creating a View_Analytics policy ](img/B17084_08_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – Creating a View_Analytics policy
  prefs: []
  type: TYPE_NORMAL
- en: Here we are creating a new policy called **View_Analytics**. We have chosen
    **Platform** as the type of policy. Optionally, we can add a description of the
    policy. Here, we have added a description that states **This policy is for viewing
    analytics only**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Next** to go to the **Configure Privileges** section, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – Configuring policy privileges ](img/B17084_08_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.27 – Configuring policy privileges
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are configuring the platform privileges for the policy that we are
    defining. Finally, we must select/specify the users or groups that this policy
    will be applied to. The following screenshot shows how we can assign users or
    groups to a policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – Assigning users/groups to a policy ](img/B17084_08_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.28 – Assigning users/groups to a policy
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the **View_Analytics** policy is assigned to the **executives**
    group.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual analytics**: DataHub also allows users to create and save dashboards
    for the analytics required for the data governance of an organization. The following
    screenshot shows various metrics, such as the most viewed datasets and data governance
    completeness (how much of the documentation/lineage/schema is well defined) for
    different entity types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.29 – Various visual analytics metrics ](img/B17084_08_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.29 – Various visual analytics metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows some other dashboard charts where we can find
    the platforms that were used per domain and their count. We can also see the number
    of entities (datasets or pipeline components) per platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30 – Data Landscape Summary ](img/B17084_08_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.30 – Data Landscape Summary
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed and learned about the concepts of data governance
    and have practically implemented data governance around a use case, let’s learn
    about data security.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for data security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we understand the need for data security, let’s try to define what data
    security is. **Data security** is the process of protecting enterprise data and
    preventing any data loss from malicious or unauthorized access to data. Data security
    includes the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Protecting sensitive data from attacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting data and applications from any ransomware attacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting against any attacks that can delete, modify, or corrupt corporate
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing access and control of data to the necessary user within the organization.
    Again, read-only, write, and delete access is provided to the data based on the
    role and its use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some industries may have stringent data security requirements. For example,
    a US health insurance company needs to ensure PHI data is extremely well protected
    according to HIPAA standards. Another example is that a financial firm such as
    Bank Of America has to ensure card and account data is extremely secure because
    that can cause direct monetary loss. But even if there is no stringent data security
    requirement in your organization, data security is essential to prevent data loss
    and loss of trust of the organization’s customers.
  prefs: []
  type: TYPE_NORMAL
- en: Although data privacy and data security are overlapping terms, it helps to understand
    the subtle difference between the two. Data privacy ensures that only an authenticated
    user can access the data, and even if the data is accessed somehow, it should
    be encrypted or tokenized so that unauthorized users cannot use the data. Data
    security includes data privacy concerns, but apart from that, it focuses on tackling
    any malicious activity concerning the data. One simple example is that, to protect
    PHI data, Cigna Health insurance has encrypted all its sensitive data. This ensures
    data privacy but not data security. Although hackers may not be able to decipher
    the encrypted data, they can still delete the data or double encrypt the data
    so that the data becomes unusable.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss why data security is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every year, the damage that’s caused by data breaches is around $8 billion
    in the US alone and, on average, each incident causes approximately 25,000 accounts
    to be compromised. Let’s explore a few of the biggest data breaches that have
    happened over the last few years:'
  prefs: []
  type: TYPE_NORMAL
- en: In June 2013, Capital One reported a data breach that affected all 106 million
    of its accounts. Personal information such as credit history, social security
    numbers, and bank accounts was compromised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In June 2021, a massive data breach happened on LinkedIn, wherein 700 million
    accounts (92% of the total accounts at that time) were affected. User data was
    posted for sale on the dark web.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In April 2019, Facebook reported an attack that compromised 533 million accounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we can see, these breaches not only pose security risks to the compromised
    data but also damage the reputation and trust of the company. Now, let’s discuss
    a few common data security threats, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Phishing and other social engineering attacks**: Social engineering is a
    common way to trick or manipulate individuals to gain unauthorized access to corporate
    data or gather confidential information. Phishing is a common form of social engineering
    attack. Here, messages or emails that appear to be from a trusted source are sent
    to individuals. They contain malicious links that, when clicked, can give unauthorized
    access to corporate networks and data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insider threats**: Insider threats are caused by an employee who inadvertently
    or intentionally threatens the security of corporate data. They can be of the
    following types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malicious actors, who intentionally steal data or cause harm to the organization
    for personal gain.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-malicious actors, who cause threats accidentally or because they are unaware
    of security standards.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compromised users, whose systems are compromised by external attackers without
    their knowledge. Then, attackers perform malicious activities, pretending to be
    legitimate users.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ransomware**: This is a form of malware that infects corporate systems and
    encrypts data so that it becomes useless without a decryption key. Attackers then
    display a ransom message for payment to release the decryption key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQL injection**: Here, attackers gain access to databases and their data.
    Then, they inject unwanted code into seemingly innocent SQL queries to perform
    unwanted operations, thereby either deleting or corrupting the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed Denial of Service (DDoS) attacks**: A DDoS attack is a malicious
    attempt to disrupt a web resource such as a web service. It acts by sending a
    huge volume of dummy calls to the web server. Due to the extreme volume of service
    calls at a very short burst, the web server crashes or becomes unresponsive, causing
    web resource downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compromised data in the cloud**: Ensuring security in the cloud is a big
    challenge. Security is required while sending data over the network and for data
    at rest in the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed what data security is and the threats that can
    occur in absence of it by covering some real-world examples. Now, let’s discuss
    the solutions and tools available for data security.
  prefs: []
  type: TYPE_NORMAL
- en: Solution and tools available for data security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we briefly discussed what data security is and why
    it is needed. We also looked at a few common data security threats. The solutions
    and tools described here help mitigate or minimize the risk from the threats discussed
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data discovery and classification**: To ensure data security, it is important
    to discover sensitive information. This technique uses data discovery to classify
    data into various security labels (such as confidential, public, and so on). Once
    classification is done, security policies can be applied to the various classifications
    of data according to the organization’s needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Firewalls**: This is the first line of defense against any network intrusions.
    They exclude any undesirable traffic from entering the network. They also help
    open specific ports to the external network, which gives hackers less of a chance
    to enter the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backup and recovery**: This helps organizations protect themselves in case
    the data is deleted or destroyed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Antivirus**: This is used to detect any viruses, Trojans, and rootkits that
    can steal, modify, or damage your data. They are widely used for personal and
    corporate data security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intrusion Detection and Prevention Systems** (**IDS/IPS**): IDS/IPS performs
    a deep inspection of packets on network traffic and logs any malicious activities.
    These tools help stop DDoS attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security Information and Event Management** (**SIEM**): This analyzes recorded
    logs from various kinds of devices such as network devices, servers, and applications,
    and generates security alert events based on specific criteria and thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Loss Prevention** (**DLP**): This mechanism monitors different devices
    to make sure sensitive data is not copied, moved, or deleted without proper authorization.
    It also monitors and logs who is using the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access control**: This technique allows or denies read, write, or delete
    access to individual data resources. Access controls can be implemented using
    an **Access Control List** (**ACL**) or **Role-Based Access Control** (**RBAC**).
    Cloud security systems such are IAM are used to enforce access control on the
    cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security as a Service**: This is modeled on the lines of Software-as-a-Service.
    Here, service providers provide the security of corporate infrastructure on a
    subscription basis. It uses a pay-as-you-go model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data encryption**: Data such as PHI can be very sensitive. If it is lost
    or leaked, this can cause regulatory issues and heavy financial losses. To provide
    data protection on such data, we can either encrypt the data (where the masked
    data loses all the original data properties, such as size) or tokenize it (where
    the masked data retains the properties of the original data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Physical security**: Finally, physical security is essential to stop unauthorized
    access to data. Physical security can be enabled by creating strong security policies
    and implementing them. Policies such as locking the system whenever you step out,
    no tailgating, and others can help prevent social engineering attacks and unauthorized
    access to data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned about the data security solutions and tools that
    are available in the industry. Now, let’s summarize what we’ve learned in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what data governance is and why is it needed. Then,
    we briefly discussed the data governance framework. After that, we discussed how
    to develop a practical data governance solution using DataHub. Next, we learned
    what data security is and why it is needed. Finally, we briefly discussed the
    various solutions and tools that are available for ensuring data security.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have learned how to ingest data using real-time and batch-based
    pipelines, popular architectural patterns for data ingestion, and data governance
    and data security. In the next chapter, we will discuss how to publish the data
    as a service for downstream systems.
  prefs: []
  type: TYPE_NORMAL
- en: Section 3 – Enabling Data as a Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section of the book focuses on architecting solutions for Data as a Service.
    In this part, you will learn how to build various kinds of Enterprise grade **Data
    as a Service** (**DaaS**) solutions and secure and manage them properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17084_09.xhtml#_idTextAnchor144)*, Exposing MongoDB Data as
    a Service*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17084_10.xhtml#_idTextAnchor157)*, Federated and Scalable DaaS
    with GraphQL*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
