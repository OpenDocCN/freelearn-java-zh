- en: Lightweight Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lightweight Java EE. Is that even possible? In the past, J2EE applications and
    especially application servers have been considered a heavyweight and cumbersome
    technology. And up to some degree deservedly so. APIs were quite unwieldy to use.
    There was a lot of XML configuration required, which eventually led to the use
    of **XDoclet**, a tool used to generate XML based on meta information put into
    JavaDoc comments. Application servers were also cumbersome to work with, especially
    with regard to startup and deployment times.
  prefs: []
  type: TYPE_NORMAL
- en: However, since the name change to Java EE and especially since version 6, these
    assumptions are not true anymore. Annotations were introduced, which originally
    emerged from the XDoclet-motivated JavaDoc tags. And a lot has happened to improve
    the productivity and developer experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What makes a technology lightweight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Java EE standards help reducing work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to choose project dependencies and archive formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits of zero-dependency enterprise applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern Java EE application servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *one application per application server* approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight enterprise technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What makes a technology *lightweight*? And how lightweight, productive and relevant
    is Java EE in the age of containers and cloud?
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important aspects of a lightweight technology is the productivity
    and effectiveness it enables. The time the development team spends is precious
    and expensive and the less time spent on overhead the better. This includes developing
    glue code, building projects, writing and executing tests, and deploying software,
    on both local and remote environments. Ideally, engineers can spend as much time
    as possible on implementing revenue-generating business functionality.
  prefs: []
  type: TYPE_NORMAL
- en: A technology should therefore not add much overhead on top of the business use
    cases. Technical cross-cutting concerns are certainly necessary but should be
    kept to a minimum. In the previous chapter, we have seen how Java EE enables developers
    to implement business use cases in a productive way. Project artifact builds and
    deployments should in the same way aim to minimize the required time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: This and the following chapter will show how Java EE supports crafting productive
    development workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Why Java EE standards?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the principles of Java EE is to provide a productive enterprise API.
    As seen in the *Concepts and design principles of modern Java EE* section in the
    previous chapter, one of the biggest advantages is the ability to integrate different
    standards without developer-side configuration required. The Java EE umbrella
    requires the different standards to work well together. The enterprise container
    has to meet this requirement. The software engineers only develop against the
    APIs and let the application server do the *hard integration work*.
  prefs: []
  type: TYPE_NORMAL
- en: Following the convention over configuration approach, using different, integrated
    standards that are part of the umbrella specification doesn't require initial
    configuration. As seen in various examples previously, the technologies that have
    emerged from different standards within Java EE work well with each other. We
    have seen examples such as using JSON-B to automatically map objects to JSON in
    JAX-RS resources; integrating Bean Validation into JAX-RS and therefore HTTP responses
    by introducing a single annotation; injecting managed beans into instances defined
    by other standards, such as Bean Validation validators or JSON-B type adapters;
    or managing technical transactions that span JPA database operations in EJBs.
  prefs: []
  type: TYPE_NORMAL
- en: What is the alternative to using an umbrella standard that embraces various
    reusable technologies? Well, to introduce vendor-specific frameworks with third-party
    dependencies that need to be wired together with manual developer work involved.
    One of the biggest advantages of the Java EE API is having the whole variety of
    technology right at the developer's fingertips; providing productive integration
    and saving developers time for focusing on business use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Convention over configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pursuing the idea of convention over configuration, further, enterprise applications
    can be developed without any initial configuration required. The APIs provide
    default behavior that matches the majority of use cases. Engineer are only required
    to put extra effort in if that behavior is not sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: This implies that in today's world, enterprise projects can be set up with minimal
    configuration involved. The days of extensive XML configuration are over. Especially,
    applications that don't ship web frontend technology can keep XML files to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a simple example of an application that offers REST endpoints,
    and accesses databases and external systems. REST endpoints are integrated by
    JAX-RS that internally uses servlets to handle requests. Servlets traditionally
    are configured using the `web.xml` deployment descriptor file residing under `WEB-INF`.
    However, JAX-RS ships a shortcut by sub-classing `Application`, annotated with
    `@ApplicationPath`, as shown in the previous chapter. This registers a JAX-RS
    application servlet for the provided path. At startup time, the project will be
    scanned for JAX-RS related classes such as resources or providers. After the application
    has been started, the REST endpoints are available to handle requests even without
    a provided `web.xml` file.
  prefs: []
  type: TYPE_NORMAL
- en: Managed beans are traditionally configured using a `beans.xml` configuration
    file. In web archive applications this file also resides under `WEB-INF`. Nowadays,
    it is primarily used to specify the bean discovery mode, that is, which CDI beans
    are considered per default. It's advisable to configure the `bean-discovery-mode`
    of `all`, not just `annotated` beans. The `beans.xml` file can override CDI bean
    composition of all sorts, such as interceptors, alternatives, decorators, and
    so on. As the CDI specification states, for the simplest example it's sufficient
    for this file to be empty.
  prefs: []
  type: TYPE_NORMAL
- en: The JPA persistence units are configured using the `persistence.xml` file under
    `META-INF`. As previously shown, it comprises the datasource definitions that
    are used in the the application. Mapping JPA entities to database tables is configured
    via annotations in domain model classes. This approach keeps the concerns in a
    single place and minimizes XML usage.
  prefs: []
  type: TYPE_NORMAL
- en: For the majority of enterprise applications that don't include a web frontend,
    this amount of configuration is already sufficient. Frontend technologies such
    as JSF are usually configured via `web.xml` and `faces-config.xml` or if required,
    via additional, implementation-specific files.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, vendor-specific configuration files, such as `jboss-web.xml` or
    `glassfish-web.xml`, were quite common. In a modern Java EE world, the majority
    of applications don't require these workarounds anymore. In order to allow portability,
    it is highly advisable to implement features using standard APIs first and only
    if this is not possible within reasonable effort to go with vendor-specific features.
    Experience with legacy projects showed that this approach leads to better manageable
    situations. Unlike vendor-specific features, Java EE standards are guaranteed
    to continue to work in the future.
  prefs: []
  type: TYPE_NORMAL
- en: At application startup, the container scans the available classes for annotations
    and known types. Managed beans, resources, entities, extensions, and cross-cutting
    concerns are discovered and configured appropriately. This mechanism is a great
    benefit for developers. They don't need to explicitly specify required classes
    in configuration files but can rely on the server's discovery; inversion of control
    at its best.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency management of Java EE projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dependency management of an enterprise project targets the dependencies
    that are added on top of the JDK. This includes dependencies that are required
    during compilation, tests, and at runtime. In a Java enterprise project, the Java
    EE API is required with *provided* dependency scope. Since the APIs are available
    on the application server, they don't have to be included in the packaged archive.
    The provided Java EE API therefore doesn't have an implication on the package
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world enterprise projects usually include more dependencies than this.
    Typical examples for third-party dependencies include logging frameworks such
    as **Slf4j**, **Log4j**, or **Logback**, JSON mapping frameworks such as **Jackson**,
    or general purpose libraries such as **Apache Commons**. There are several issues
    with these dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, third-party dependencies are usually not provided, thus increasing
    the size of the artifact. This doesn't sound that harmful, but has some implications
    that we'll see later. The more dependencies are added to the resulting artifact,
    the longer the build will take. Build systems need to copy potentially big dependencies
    into the artifact each and every time the project is built. As we will see in
    [Chapter 6](599c6821-8971-4489-931c-9e11b5e23afd.xhtml), *Application Development
    Workflows*, project builds need to be as fast as possible. Every dependency added
    to the package increases the turnaround time.
  prefs: []
  type: TYPE_NORMAL
- en: Potential collisions of dependencies and their versions represent an even bigger
    challenge. This includes both packaged dependencies and transitive dependencies
    as well as libraries that already exist on the application server. For example,
    logging frameworks are often already present in the container's classpath, potentially
    in a different version. Different versions being used introduce potential issues
    with the aggregate of libraries being there. Experience shows that implicit dependencies
    that are added transitively represent the biggest challenge in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from technical reasons, there are some other aspects to consider before
    lightheadedly introducing dependencies to a software project. Dependency licenses,
    for example, can become an issue when developing a software product that is shipped
    to customers. It's not only required that the company is permitted to use certain
    dependencies, but also that involved licenses are compatible to each other, when
    shipped in a software package. The simplest way to meet licensing criteria is
    to avoid dependencies, at least, if they serve no business purpose. Engineers
    should make similar consideration in regard to security, especially for software
    being developed for sectors with high demands in security.
  prefs: []
  type: TYPE_NORMAL
- en: I was once involved in a *firefighter* job responsible for updating versions
    of used frameworks in an enterprise project. The project included a lot of build
    dependencies. With all its included third-party dependencies, the project runtime
    eventually contained *all* known logging frameworks. The same was true for JSON
    mapping frameworks, which introduced a lot of version conflicts and runtime dependency
    mismatches. This was before the advent of JSON-B and JSON-P. We spent most of
    the time configuring the project build, untangling and excluding the transitive
    dependencies from the project artifact. This is a typical issue when using third-party
    libraries. The price for saving project code is to spend time and effort configuring
    the project build and potentially untangling dependencies, especially if they
    introduce a lot of transitive functionality.
  prefs: []
  type: TYPE_NORMAL
- en: By managing build dependencies, engineers focus on aspects that are insignificant
    to the business use cases. The question to be asked is whether it pays off to
    save some lines of code, when at the same time we introduce dependencies. Experience
    shows that the trade-off of duplication versus lightweightness, such as in dependency-free
    projects, is too often in favor of avoiding duplication. A prime example for this
    are projects that introduce the whole Apache Commons library to use a functionality
    that could have been realized with a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas it's good practice to not reinvent the wheel by developing own versions
    of functionality that could be reused, the consequences also have to be considered.
    Experience shows that introduced dependencies are quite often neglected and only
    utilized marginally. Most of them serve little business value.
  prefs: []
  type: TYPE_NORMAL
- en: When engineers inspect code quality, for example using code analysis tools,
    what also should be considered is the ratio of dependencies and project code that
    target business use cases versus *plumbing*. There is a straightforward method
    that can be applied for dependencies. Before a third-party dependency is introduced,
    consider a few questions. Does adding the functionality add value to the business?
    How much project code does it save? How big is the impact on the resulting artifact?
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine part of the use case of the car manufacture application
    is to communicate with specific factory software using a proprietary Java API.
    Obviously, the communication is crucial to fulfill the business purpose and it
    makes a lot of sense to include this dependency in the project. On the contrary,
    adding a different logging framework hardly improves the application's business
    value. Furthermore, Chapter 9, *Monitoring, Performance, and Logging* will discuss
    the issues with traditional logging.
  prefs: []
  type: TYPE_NORMAL
- en: However, in order not to unnecessarily increase the build size, crucial dependencies
    can be installed on the application server and be declared as provided in the
    project's build.
  prefs: []
  type: TYPE_NORMAL
- en: In the first chapter, we saw the difficulties with shared business dependencies
    such as shared models. Ideally, applications are as self-sufficient as possible.
    Chapter 8, *Microservices and System Architecture* will deep-dive into self-contained
    systems and the motivation for architectures that share nothing.
  prefs: []
  type: TYPE_NORMAL
- en: In regard to technical dependencies, however, the Java EE API already includes
    the technology that the majority of enterprise applications need. Ideally, engineers
    develop zero-dependency Java EE applications that are packaged as thin deployment
    artifacts containing the application-relevant classes only. If some use cases
    require third-party dependencies, they can be installed in the container. The
    goal is to have a light footprint of deployment artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: For production code, this means that only provided dependencies are included,
    ideally, only the Java EE API. Test dependencies, however, are a different story;
    software tests require some additional technology. Chapter 7, *Testing* covers
    the required dependencies for the test scope.
  prefs: []
  type: TYPE_NORMAL
- en: Lightweight way of packaging applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach of zero-dependency applications simplifies many project build concerns.
    There is no need to manage third-party dependencies with regard to versions or
    collisions since there aren't any included.
  prefs: []
  type: TYPE_NORMAL
- en: What other aspects does this approach simplify? Project builds, no matter whether
    Gradle or Maven are being used, always show the best performance when nothing
    needs to be added to the artifact. The resulting size of the packages directly
    impacts the build time. In the case of zero-dependency applications, only the
    compiled classes are included, that is, only the actual business logic. Therefore,
    the resulting build times are as minimal as they will get. All build time is spent
    on compiling the project's classes, running the test cases, and packaging the
    classes into a thin deployment artifact. Building this approach should happen
    in seconds. Yes, seconds. As a general rule, every project build that takes more
    than 10 seconds should be reconsidered.
  prefs: []
  type: TYPE_NORMAL
- en: This rule, of course, puts certain pressure on project builds. It naturally
    requires to avoid including any larger dependency or implementation; these should
    be provided by the application server. Test run times are usually another aspect
    that prevents fast builds. Chapter 7, *Testing*, will shed light on how to develop
    tests in an effective way.
  prefs: []
  type: TYPE_NORMAL
- en: Fast builds are one benefit of crafting zero-dependency applications. Another
    implication is fast artifact transmission. Built artifacts, such as WAR or JAR
    files, are usually kept in an artifact repository for later use, for example,
    **Sonatype Nexus** or **JFrog Artifactory**. Transmitting these artifacts over
    the wire greatly speeds up if only a few kilobytes of data are involved. This
    applies to all sorts of artifact deployment. No matter where the built archives
    are shipped to, smaller footprints always pay off especially when workflows are
    executed often, as it is the case for Continuous Delivery.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of reconsidering practices and stripping everything which does not
    provide value also targets the way of packaging applications. Traditionally, enterprise
    applications have been shipped as EAR files. The structure of these included a
    web archive, a WAR file, and one or more enterprise JARs. Enterprise JAR archives
    contained the business logic, usually implemented in EJBs. The web archive contained
    the web services and frontend technology communicating with the business logic
    using local or remote EJBs. However, this separation is not necessary, as all
    components are shipped on a single server instance.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging several technical concerns in several sub-archives is not required
    anymore. All business logic as well as web services and cross-cutting concerns
    are packaged into a single WAR file. This greatly simplifies the project setup
    as well as the build process. Applications don't have to be zipped in multiple
    hierarchies just to be unzipped on a single server instance again. WAR files containing
    the required business code deployed in a container is the best implementation
    of thin artifacts. Because of this reason, deploying thin WAR files is faster
    than the corresponding EAR files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following demonstrates the contents of a thin web application artifact
    with typical components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb372acc-bd04-436a-9058-306fd1046ebe.png)'
  prefs: []
  type: TYPE_IMG
- en: The deployment artifact only contains classes that are required to implement
    the business use case, no technology-specific implementation, and only minimal
    configuration. Especially, there are no library JAR files included.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the Java EE platform encourages lightweight artifacts. This
    is due to the platform separating the API from the implementations. Developers
    only program against the API; the application server implements the API. This
    makes it possible to ship only the business logic, which includes certain aspects
    in lightweight artifacts. Besides the obvious benefits of avoiding dependency
    collisions and building vendor-independent solutions, this approach also enables
    fast deployments. The less content the artifacts include, the less unpacking needs
    to be done on the container side. Therefore, it's highly advisable to package
    enterprise applications into a single WAR file.
  prefs: []
  type: TYPE_NORMAL
- en: In the last year, we have seen more and more interest in shipping enterprise
    applications as *fat* JARs, that is, shipping the application together with the
    implementation. The approaches of fat deployment artifacts have usually been used
    in enterprise frameworks such as the Spring Framework. The motivation behind these
    approaches is that the versions of the required dependencies and frameworks are
    explicitly specified and shipped together with the business logic. Fat deployment
    artifacts can be created as fat WARs, which are deployed on a servlet container,
    or fat JARs started as standalone, executable JARs. A Java EE application packaged
    as fat JAR therefore ships the enterprise container together with the application
    in an executable JAR. However, as stated before, the build, shipping, and deployment
    times increase greatly if third-party dependencies are added to the artifact.
  prefs: []
  type: TYPE_NORMAL
- en: Experience shows that explicitly shipping the enterprise implementation together
    with the application is in most cases not technically but business-politically
    motivated. Operational environments within companies that are inflexible regarding
    application server and Java installations, especially regarding version upgrades,
    in some cases force developers to find workarounds. Enterprise applications that
    are built using newer technology cannot be deployed on older, existing server
    installations. Sometimes, the business-politically easier solution is to ignore
    the existing installations altogether and to directly execute the standalone JAR,
    which only requires a certain Java version. However, whereas these solutions are
    certainly justified, the technically more reasonable solutions would be to package
    applications into thin deployment artifacts. Interestingly, as we will see in
    the next chapter, shipping software in Linux containers holds the advantages of
    both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: There is another interesting approach that enables to ship the whole application
    as an executable package and to keep fast workflows of thin deployments. Several
    application server vendors provide the solution to ship a custom application container
    as executable JAR that deploys the thin application as additional argument at
    startup time. By doing so, the whole package of both artifacts includes the business
    logic as well as the implementation and is started as a standalone application.
    The application is still separated from its runtime and packaged as thin artifact,
    as a so-called *hollow* JAR or WAR file. This approach especially makes sense
    if the addressed flexibility is required without the use of Linux containers.
  prefs: []
  type: TYPE_NORMAL
- en: As a conclusion, it is highly advisable to build thin deployment artifacts,
    ideally thin WAR files. If this approach does not work for business-political
    reasons, hollow JARs can provide a reasonable workaround. However, as we will
    see in the next chapter, container technologies such as Docker don't require to
    make use of executable JAR approaches and provide the same benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE application servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What else makes an enterprise technology lightweight besides the productiveness
    of the API? What about the runtime, the enterprise container?
  prefs: []
  type: TYPE_NORMAL
- en: Developers often complained about J2EE application servers being too slow, too
    cumbersome, and unwieldy to use. Installation sizes and memory consumption were
    quite high. Typically, a lot of applications ran on a server instance in parallel
    with being redeployed individually. This approach sometimes introduced additional
    challenges, such as classloader hierarchy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Modern application servers are far from this negative image. Most of them have
    been heavily optimized for startup and deployment time. Especially, server-internal
    module approaches such as **Open Service Gateway Initiative** (**OSGi**) tackled
    the necessity of supporting the full Java EE API by loading required modules on
    demand and greatly speeding up operations. In terms of resource usage, application
    servers also greatly improved compared to the past. A modern container consumes
    less memory than a running browser instance on a desktop computer. For example,
    an **Apache TomEE** instance starts up in one second, consumes less than 40 megabytes
    on disk and less than 30 megabytes of memory.
  prefs: []
  type: TYPE_NORMAL
- en: The performance overhead of managed beans is equally negligible. In fact, compared
    to CDI managed beans and other frameworks such as Spring Framework, stateless
    EJBs show the best results. This is due to the fact that stateless session beans
    are pooled and reused after their business methods have been invoked.
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, application servers manage pools of connections and threads and
    they enable engineers to gather statistics and performance insights out of the
    box. The container is responsible for providing monitoring for these aspects.
    DevOps engineers have the possibility to directly use this data without introducing
    custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Besides these aspects, the application servers also manage the bean instances
    and life cycles, resources, and database transactions, as we have seen in the
    previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This is the point of having an application container. It does the required work
    to run an enterprise application in production; the software engineers are responsible
    for dealing with the business logic. The container provides and manages the required
    resources and is forced by the standards to provide insights into deployed applications.
    Due to the vendors that put a lot of effort into optimizing the required technologies,
    the resource overhead can be kept low.
  prefs: []
  type: TYPE_NORMAL
- en: Application servers' installation sizes are still somewhat bigger than other
    enterprise frameworks. As of writing this book, vendors are striving to provide
    smaller, on-demand runtimes tailored for the application's needs. The **MicroProfile**
    initiative includes several application server vendors that define additional
    enterprise profiles complementary to the Java EE umbrella. These profiles are
    assembled from Java EE standards as well. This is a very interesting approach
    for developers since it doesn't require any change on the application side. The
    runtime, that is, the set of standards included, will be fitted to what the application
    needs in order to fulfill its business logic.
  prefs: []
  type: TYPE_NORMAL
- en: One application per application server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, with big installation sizes and long startup times, application
    servers have been used to deploy several, if not dozens of, enterprise applications.
    A server instance was shared among several teams, sometimes a whole company. This
    comes with certain inflexibility, similar to shared application models. Teams
    cannot simply choose newer JDK or server versions, or restart or reconfigure the
    application server without coordinating with other teams. This naturally inhibits
    fast and productive processes, and complicates Continuous Delivery.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of team working methods, and project and application life cycles, the
    easiest approach therefore is to deploy an application on a dedicated application
    server. The DevOps team has full control over its version, configuration, and
    life cycle. This approach simplifies processes and avoids potential issues such
    as collisions with other teams and used technology. Issues with hierarchical classloading
    that deploying several applications could introduce are avoided as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'An application server certainly represents quite a construct for just a single
    application. However, as we have seen previously, the installation sizes of application
    servers have already decreased compared to the past. Apart from that, developers
    should care more about the sizes of the deployment artifacts, since these are
    the moving parts of the development workflow. In a Continuous Delivery approach,
    the application is potentially built and packaged many times a day. The longer
    the time spent on the project build and transmitting artifacts, the longer the
    turnaround times. This affects each and every build and can add up to a lot of
    overhead during a day. The application server is not installed and shipped that
    often. Therefore, it is advisable to deploy an application to a single, dedicated
    Java EE server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7fc2681-0118-4d06-ba47-a32640353b73.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next chapter, we will see how container technologies such as Docker support
    this approach. Shipping the application, including the whole stack down to the
    operating system as a container, encourages the approach of one application per
    application server.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The seamless integration of multiple Java EE standards with the convention over
    configuration approach minimizes the amount of boilerplate work developers have
    to do. The configuration of modern enterprise applications is thus kept to a minimum.
    Especially the default conventions, which work for the majority of enterprise
    applications and the possibility of overriding configuration only if required,
    increases the developer's productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise applications should minimize their dependencies ideally to only the
    provided Java EE API. The third-party dependencies should only be added if they
    are a business necessity and not a technical one.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE applications should be packaged as thin WAR files, following a zero-dependency
    approach. This has a positive impact on the time spend to build as well as to
    publish the application.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Java EE applications are far from the negative image of heavyweight J2EE
    runtimes. They start up and deploy fast and try to reduce the memory impact. Whereas
    application servers might not be the most lightweight runtime out there, they
    ship with enough benefits for enterprise applications such as integrating technology
    or managing life cycles, connections, transactions, or threads, that would have
    to be implemented otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: In order to simplify application life cycles and deployments it's advisable
    to deploy one application per application server. This gets rid of a few potential
    challenges and perfectly fits into a modern world of container technologies. The
    next chapter will show you this modern world in the age of cloud platforms, what
    container technologies are about and how Java EE fits into this picture.
  prefs: []
  type: TYPE_NORMAL
