- en: Chapter 8. More About Search – Search Trees and Hash Tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we had a look at both binary search and trees. In
    this chapter, we will see how they are related and how this helps us create some
    more flexible, searchable data structures. We will also look at a different kind
    of searchable structure called a hash table. The reason for using these structures
    is that they allow mutation and still remain searchable. Basically, we need to
    be able to insert and delete elements from the data structures with ease while
    still being able to conduct a search efficiently. These structures are relatively
    complicated, so we need to take a step-by-step approach toward understanding it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary search trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balanced binary search trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary search tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You already know what binary search is. Let''s go back to the sorted array
    from an earlier chapter and study it again. If you think about binary search,
    you know you need to start from the middle of the sorted array. Depending on the
    value to be searched, either we return if the middle element is the search item,
    or move to the left or right based on whether the search value is greater than
    or less than the middle value. After this, we continue the same process recursively.
    This means the landing points in each step are quite fixed; they are the middle
    values. We can draw all the search paths as in the next figure. In each step,
    the arrows connect to the mid points of both the right half and left half, considering
    the current position. In the bottom part, we disassemble the array and spread
    out the elements while keeping the sources and targets of the arrows similar.
    As one can see, this gives us a binary tree. Since each edge in this tree moves
    from the midpoint of one step to the midpoint of the next step in the binary search,
    the same search can be performed in the tree by simply following its edges. This
    tree is quite appropriately called a binary search tree. Each level of this tree
    represents a step in binary search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary search tree](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Binary search tree
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to search for item number **23**. We start from the original midpoint,
    which is the root of the tree. The root has the value **50**. **23** is less than
    **50**, so we must check the left-hand side; in the case of our tree, follow the
    left edge. We arrive at the value **17**. **23** is greater than **17**, so we
    must follow the right edge and arrive at the value **23**. We just found the element
    we have been searching for. This algorithm can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start at the root.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current element is equal to the search element, we are done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the search element is less than the current element, we follow the left edge
    and start again from 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the search element is greater than the current element, we follow the right
    edge and start again from 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To code this algorithm, we must first create a binary search tree. Create a
    `BinarySearchTree` class extending the `BinaryTree` class and then put your algorithm
    inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now wrap the method so that you don''t need to pass the root. This method also
    checks whether the tree is an empty tree and fails the search if that is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So what exactly is the point of modifying an array in a binary tree? After all,
    are we not doing the same exact search still? Well, the point is that when we
    have this in a tree form, we can easily insert new values in the tree or delete
    some values. In the case of an array, insertion and deletion have linear time
    complexity and cannot go beyond the preallocated array size.
  prefs: []
  type: TYPE_NORMAL
- en: Insertion in a binary search tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Insertion in a binary search tree is done by first searching for the value
    to be inserted. This either finds the element or ends the search unsuccessfully,
    where the new value is supposed to be if it were in that position. Once we reach
    this position, we can simply add the element to the tree. In the following code,
    we rewrite the search again because we need access to the parent node once we
    find the empty spot to insert our element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap this up into a method that does not need a starting node. It also
    makes sure that when we insert into an empty tree, we just add a root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose in our earlier tree, we want to insert the value **21**. The following
    figure shows the search path using arrows and how the new value is inserted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Insertion in a binary search tree](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Insertion of a new value into a binary tree
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the means to insert elements in the tree, we can build the
    tree simply by a successive insertion. The following code creates a random tree
    with 20 elements and then does an in-order traversal of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you run the preceding code, you will always find that the elements are sorted.
    Why is this the case? We will see this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: What to do if the element inserted is the same as the element already present
    in the search tree? It depends on that particular application. Generally, since
    we search by value, we don't want duplicate copies of the same value. For simplicity,
    we will not insert a value if it is already there.
  prefs: []
  type: TYPE_NORMAL
- en: Invariant of a binary search tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An invariant is a property that stays the same irrespective of the modifications
    made in the structure it is related to. An in-order traversal of a binary search
    tree will always result in the traversal of the elements in a sorted order. To
    understand why this happens, let''s consider another invariant of a binary tree:
    all descendants of the left child of a node have a value less than or equal to
    the value of the node, and all descendants of the right child of a node have a
    value greater than the value of the node. It is understandable why this is true
    if you think about how we formed the binary search tree using the binary search
    algorithm. This is why when we see an element bigger than our search value, we
    always move to the left child. This is because all the values that are descendants
    of the right child are bigger than the left child so there is no point investing
    time in checking them. We will use this to establish that an in-order traversal
    of a binary search tree will traverse elements in a sorted order of the values
    in the nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use induction to argue for this. Suppose we have a tree with only one
    node. In this case, any traversal could be easily sorted. Now let''s consider
    a tree with only three elements, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Invariant of a binary search tree](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A binary search tree with three nodes
  prefs: []
  type: TYPE_NORMAL
- en: An in-order traversal of this tree will first process the left child, then the
    parent, and finally, the right child. Since the search tree guarantees that the
    left child has a value that is less than or equal to the parent and the right
    child has a value greater than or equal to the value of the parent, the traversal
    is sorted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s consider our general case. Suppose this invariant we discussed is
    true for trees with maximum *h-levels*. We will prove that, in such a case, it
    is also true for trees with maximum *h+1* levels. We will consider a general search
    tree, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Invariant of a binary search tree](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A general binary search tree
  prefs: []
  type: TYPE_NORMAL
- en: The triangles represent subtrees with maximum *n* levels. We assume that the
    invariant holds true for subtrees. Now, an in-order traversal would first traverse
    the left subtree in sorted order, then the parent, and finally, the right subtree
    in the same order. The sorted order traversal of the subtrees is implied by the
    assumption that the invariant holds true for these subtrees. This will result
    in the order *[traversal of left descendants in sorted order][traversal of parents][traversal
    of right descendants in sorted order]*. Since the left descendants are all less
    than or equal to the parent and right descendants are all greater than or equal
    to the parent, the order mentioned is actually a sorted order. So a tree of the
    maximum level *h+1* can be drawn, as shown in the preceding figure, with each
    sub-tree having *n* levels maximum. If this the case and the invariant is true
    for all trees with level *h*, it must also be true for trees with level *h+1*.
  prefs: []
  type: TYPE_NORMAL
- en: We already know that the invariant is true for trees with maximum level 1 and
    2\. However, it must be true for trees with maximum level 3 as well. This implies
    it must be true for trees with maximum level 4 and so on up to infinity. This
    proves that the invariant is true for all *h* and is universally true.
  prefs: []
  type: TYPE_NORMAL
- en: Deletion of an element from a binary search tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are interested in all the modifications of a binary search tree where the
    resultant tree will remain a valid binary search tree. Other than insertion, we
    need to be able to carry out deletion as well. That is to say, we need to be able
    to remove an existing value from the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deletion of an element from a binary search tree](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Three simple cases of deletion of nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'The main concern is to know what to do with the children of the deleted node.
    We don''t want to lose those values from the tree, and we still want to make sure
    the tree remains a search tree. There are four different cases we need to consider.
    The relatively easier three cases are shown in the preceding figure. Here''s a
    brief description of these cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The first case is where there is no child. This is the easiest case; we just
    delete the node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second case is where there is only a right subtree. In this case, the subtree
    can take the place of the deleted node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third case is very similar to the second case, except it is about the left
    subtree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth case is, of course, when both the children are present for the node
    to be deleted. In this case, none of the children can take the place of the node
    that is to be deleted as the other one would also need to be attached somewhere.
    We resolve this by replacing the node that needs to be deleted by another node
    that can be a valid parent of both the children. This node is the least node of
    the right subtree. Why is this the case? It is because if we delete this node
    from the right subtree, the remaining nodes of the right subtree would be greater
    than or equal to this node. And this node is also, of course, greater than all
    the nodes of the left subtree. This makes this node a valid parent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question is this: what is the least node in the right subtree? Remember
    that when we move to the left child of a node, we always get a value that is less
    than or equal to the current node. Hence, we must keep traversing left until we
    find no more left child. If we do this, we will reach the least node eventually.
    The least node of any subtree cannot have any left child, so it can be deleted
    using the first case or the second case of deletion. The delete operation of the
    fourth case is thus used to:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy the value of the least node in the right subtree to the node to be deleted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete the least node in the right subtree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To write the deletion code, we need to first add a few methods to our `BinaryTree`
    class, which is meant for deleting nodes and rewriting node values. The method
    `deleteNodeWithSubtree` simply deletes a node along with all its descendants.
    It simply forgets about all the descendants. It also has certain checks to confirm
    the validity of the input. Deletion of a root, as usual, must be handled separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we add another method to the `BinaryTree` class for rewriting the value
    in a node. We don''t allow this class to use public methods in the `node` class
    to maintain encapsulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is self-explanatory. Finally, we write a method to replace
    a node''s child with another node from the same tree. This is useful for cases
    2 and 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add a method to `BinarySearchTree` to find the least node in the
    subtree. We walk keeping to the left until there is no more child on the left-hand
    side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can implement our deletion algorithm. First, we create a `deleteNode`
    method that deletes a node. We can then use this method to delete a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Case 1**: There are no children. In this case, we can simply delete the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Case 2**: There is only a right child. The right child can take the place
    of the deleted node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Case 3**: There is only a left child. The left child can take the place of
    the deleted node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Case 4**: Both left child and right child are present. In this case, first
    we copy the value of the leftmost child in the right subtree (or the successor)
    to the node that needs to be deleted. Once we do this, we delete the leftmost
    child in the right subtree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The process of deleting a node turned out to be a little more complicated, but
    it is not difficult. In the next section, we will discuss the complexity of the
    operations of a binary search tree.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of the binary search tree operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first operation we will consider is the search operation. It starts at the
    root and moves down one level every time we move from one node to either of its
    children. The maximum number of edges we have to traverse during the search operation
    must be equivalent to the maximum height of the tree—that is, the maximum distance
    between any node and root. If the height of the tree is *h*, then the complexity
    of search is *O(h)*.
  prefs: []
  type: TYPE_NORMAL
- en: Now what is the relation between the number of nodes *n* of a tree and the height
    *h* of a tree? It really depends on how the tree is built. Any level would require
    at least one node in it, so in the worst case scenario, *h = n* and the search
    complexity is *O(n)*. What is our best case? Or rather, what do we want *h* to
    be in relation to *n*? In other words, what is the minimum *h*, given a particular
    *n*. To answer this, we first ask, what is the maximum *n* we can fit in a tree
    with height *h*?
  prefs: []
  type: TYPE_NORMAL
- en: 'The root is just a single element. The children of the root make a complete
    level adding two more nodes for a tree of height 2\. In the next level, we will
    have two children for any node in this level. So the next level or level three
    has a total of *2X2=4* nodes. It can be easily seen that the level *h* of the
    tree has a total of *2^((h-1))* nodes. The total number of nodes that a tree of
    height *h* can then have is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is our ideal case where the complexity of the search is *O(lg n)*. This
    kind of a tree where all the levels are full is called a balanced binary tree.
    Our aim is to maintain the balanced nature of the tree even when insertion or
    deletion is carried out. However, in general, the tree would not remain balanced
    in the case of an arbitrary order of insertion of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Insertion simply requires searching the element; once this is done, adding a
    new node is just a constant time operation. Therefore, it has the same complexity
    as that of a search. Deletion actually requires a maximum of two searches (in
    the fourth case), so it also has the same complexity as that of a search.
  prefs: []
  type: TYPE_NORMAL
- en: Self-balancing binary search tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A binary search tree that remains balanced to some extent when insertion and
    deletion is carried out is called a self-balancing binary search tree. To create
    a balanced version of an unbalanced tree, we use a peculiar operation called **rotation**.
    We will discuss rotation in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-balancing binary search tree](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Rotation of a binary search tree
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows the rotation operation on nodes **A** and **B**. Left rotation
    on **A** creates the right image, and right rotation on **B** creates the left
    image. To visualize a rotation, first think about pulling out the subtree **D**.
    This subtree is somewhere in the middle. Now the nodes are rotated in either the
    left or right direction. In the case of the left rotation, the right child becomes
    the parent and the parent becomes the left child of the original child. Once this
    rotation is done, the **D** subtree is added to the right child's position of
    the original parent. The right rotation is exactly the same but in the opposite
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: How does it help balance a tree? Notice the left-hand side of the diagram. You'll
    realize that the right side looks heavier, however, once you perform left rotation,
    the left-hand side will appear heavier. Actually, a left rotation decreases the
    depth of the right subtree by one and increases that of the left subtree by one.
    Even if, originally, the right-hand side had a depth of 2 when compared to the
    left-hand side, you could fix it using left rotation. The only exception is the
    subtree **D** since the root of **D** remains at the same level; its maximum depth
    does not change. A similar argument will hold true for the right rotation as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rotation keeps the search-tree property of the tree unchanged. This is very
    important if we are going to use it to balance search trees. Let''s consider the
    left rotation. From the positions, we can conclude the following inequalities:'
  prefs: []
  type: TYPE_NORMAL
- en: Each node in **C** ≤ **A**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ≤ B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A** ≤ Each node in **D** ≤ **B**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B** ≤ Each node in **E**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After we perform the rotation, we check the inequalities the same way and we
    find they are exactly the same. This proves the fact that rotation keeps the search-tree
    property unchanged. A very similar argument can be made for the right rotation
    as well. The idea of the algorithm of a rotation is simple: first take the middle
    subtree out, do the rotation, and reattach the middle subtree. The following is
    the implementation in our `BinaryTree` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s do some parameter value checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The child and grandchild we want to move depend on the direction of the rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The root node needs to be treated differently as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We now can look at our first self-balancing binary tree called the AVL tree.
  prefs: []
  type: TYPE_NORMAL
- en: AVL tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AVL tree is our first self-binary search tree. The idea is simple: keep every
    subtree as balanced as possible. An ideal scenario would be for both the left
    and right subtrees, starting from every node, to have exactly the same height.
    However, since the number of nodes are not in the form of *2^p* *-1*, where *p*
    is a positive integer, we cannot always achieve this. Instead, we allow a little
    bit of wiggle room. It''s important that the difference between the height of
    the left subtree and the right subtree must not be greater than one. If, during
    any insert or delete operation, we happen to break this condition, we will apply
    rotations to fix this. We only have to worry about a difference of two between
    the heights as we are only thinking of insertion and deletion of one element at
    a time, and inserting one element or deleting it cannot change the height by more
    than one. Therefore, our worst case is that there was already a difference of
    one and the new addition or deletion created one more difference requiring a rotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest kind of rotation is shown in the following figure. The triangles
    represent subtrees of equal heights. Notice that the height of the left subtree
    is two less than the height of the right subtree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![AVL tree](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: AVL tree – simple rotation
  prefs: []
  type: TYPE_NORMAL
- en: So we do a left rotation to generate the subtree of the structure, as shown
    in the preceding diagram. You can see that the heights of the subtrees follow
    our condition. The simple right rotation case is exactly the same, just in the
    opposite direction. We must do this for all the ancestors of the node that were
    either inserted or deleted as the heights of subtrees rooted by these nodes were
    the only ones affected by it. Since rotations also cause heights to change, we
    must start from the bottom and walk our way up to the root while doing rotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more kind of case called a double rotation. Notice that the height
    of the subtree rooted by the middle grandchild does not change due to the rotation.
    So, if this is the reason for the imbalance, a simple rotation will not fix it.
    It is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![AVL tree](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Simple rotation does not fix this kind of imbalance
  prefs: []
  type: TYPE_NORMAL
- en: Here, the subtree that received an insertion is rooted by **D** or a node is
    deleted from the subtree **C**. In the case of an insertion, notice that there
    would be no rotation on **B** as the left subtree of **B** has a height of only
    one more than that of its right subtree. **A** is however unbalanced. The height
    of the left subtree of **A** is two less than that of its right subtree. However,
    if we do a rotation on **A**, as shown in the preceding figure, it does not fix
    the problem; only the left-heavy condition is transformed into a right-heavy condition.
    To resolve this, we need a double rotation, as shown in the next figure. First,
    we do an opposite direction rotation of the middle grandchild so that it is not
    unbalanced in the opposite direction. A simple rotation after this will fix the
    imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: '![AVL tree](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: AVL tree double rotation
  prefs: []
  type: TYPE_NORMAL
- en: 'So we create an AVL tree class, and we add an extra field to the `Node` class
    to store the height of the subtree rooted by it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We must override the `newNode` method to return our extended node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a utility method to retrieve the height of a subtree with a null check.
    The height of a null subtree is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we include a method to compute and update the height of the subtree
    rooted by a node. The height is one more than that of the maximum height of its
    children:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We also override the `rotate` method in `BinaryTree` to update the height of
    the subtrees after the rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of these methods, we implement the rebalancing of a node all
    the way up to the root, as described in the preceding code. The rebalancing bit
    is done by checking the difference in the height of the left and right subtrees.
    If the difference is 0, 1, or -1, nothing needs to be done. We simply move up
    the tree recursively. When the height difference is 2 or -2, this is when we need
    to rebalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the rotation is implemented, implementing the insert and delete operations
    is very simple. We first do a regular insertion or deletion, followed by rebalancing.
    A simple insertion operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The delete operation is also very similar. It only requires an additional check
    confirming that the node is actually found and deleted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Complexity of search, insert, and delete in an AVL tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The worst case of an AVL tree is when it has maximum imbalance. In other words,
    the tree is worst when it reaches its maximum height for a given number of nodes.
    To find out how much that is, we need to ask the question differently, given a
    height h: what is the minimum number of nodes (n) that can achieve this? Let the
    minimum number of nodes required to achieve this be *f(h)*. A tree of height *h*
    will have two subtrees, and without any loss of generality, we can assume that
    the left subtree is higher than the right subtree. We would like both these subtrees
    to also have a minimum number of nodes. So the height of the left subtree would
    be *f(h-1)*. We want the height of the right subtree to be minimum, as this would
    not affect the height of the entire tree. However, in an AVL tree, the difference
    between the heights of two subtrees at the same level can differ by a maximum
    of one. The height of this subtree is *h-2*. So the number of nodes in the right
    subtree is *f(h-2)*. The entire tree must also have a root, hence the total number
    of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It almost looks like the formula of the Fibonacci sequence, except for the
    *+1* part. Our starting values are 1 and 2 because *f(1) = 1* (only the root)
    and *f(2) = 2* (just one child). This is greater than the starting values of the
    Fibonacci sequence, which are 1 and 1\. One thing is of course clear that the
    number of nodes would be greater than the corresponding Fibonacci number. So,
    the following is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that for a large enough *h*, *F* [h] *≈ φF* [h-1] holds true; here
    *φ* is the golden ratio *(1 + √5)/2*. This means *F* [h] *= C φ* ^h, where C is
    some constant. So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This means even the worst height of an AVL tree is logarithmic, which is what
    we wanted. Since an insertion processes one node in each level until it reaches
    the insertion site, the complexity of an insertion is *O(lg n)*; it is the same
    for performing search and delete operations, and it holds true for the same reason.
  prefs: []
  type: TYPE_NORMAL
- en: Red-black tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An AVL tree guarantees logarithmic insertion, deletion, and search. But it
    makes a lot of rotations. In most applications, insertions are randomly ordered
    and so are deletions. So, the trees would sort of balance out eventually. However,
    since the AVL tree is too quick to rotate, it may make very frequent rotations
    in opposite directions even when it would be unnecessary, had it been waiting
    for the future values to be inserted. This can be avoided using a different approach:
    knowing when to rotate a subtree. This approach is called a red-black tree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a red-black tree, the nodes have a color, either black or red. The colors
    can be switched during the operations on the node, but they have to follow these
    conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The root has to be black
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A red node cannot have a black child
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The black height of any subtree rooted by any node is equal to the black height
    of the subtree rooted by the sibling node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now what is the black height of a subtree? It is the number of black nodes
    found from the root to the leaf. When we say *leaf*, we really mean null children,
    which are considered black and allow a parent to be red without violating rule
    2\. This is the same no matter which path we take. This is because of the third
    condition. So the third condition can also be restated as this: the number of
    black nodes in the path from the root of any subtree to any of its leaves is the
    same, irrespective of which leave we choose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ease of manipulation, the null children of the leaves are also considered
    sort of half nodes; null children are always considered black and are the only
    ones really considered as leaves as well. So leaves don''t contain any value.
    But they are different from the conventional leaves in other red-black trees.
    New nodes can be added to the leaves but not in a red-black tree; this is because
    the leaves here are null nodes. So we will not draw them out explicitly or put
    them in the code. They are only helpful to compute and match black heights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Red-black tree](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a red-black tree
  prefs: []
  type: TYPE_NORMAL
- en: In our example of the red-black tree of height 4, the null nodes are black,
    which are not shown (in print copy, the light-colored or gray nodes are red nodes
    and dark-colored nodes are black nodes).
  prefs: []
  type: TYPE_NORMAL
- en: Both insertion and deletion are more complicated than the AVL tree, as there
    are more cases that we need to handle. We will discuss this in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Insertion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Insertion is done in the same way we do it with BST. After an insertion is
    complete, the new node is colored red. This preserves the black height, but it
    can result in a red node being a child of another red node, which would violate
    condition 2\. So we do some manipulation to fix this. The following two figures
    show four cases of insertions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Insertion](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Case 1 and 2 of red-black tree insertion
  prefs: []
  type: TYPE_NORMAL
- en: '![Insertion](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Case 3 and 4 of red-black tree insertion
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss the insertions case by case. Notice that the trees in the diagram
    look black and unbalanced. But this is only because we have not drawn the entire
    tree; it''s just a part of the tree we are interested in. The important point
    is that the black height of none of the nodes change because of whatever we do.
    If the black height must be increased to fit the new node, it must be at the top
    level; so we simply move it up to the parent. The four cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The parent is black. In this case, nothing needs to be done as it does not violate
    any of the constraints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both parent and uncle are red. In this case, we repaint parent, uncle, and grandparent
    and the black heights are still unchanged. Notice now that no constraints are
    violated. If, however, the grandparent is the root, keep it black. This way, the
    entire tree's black height is increased by 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The parent is red and uncle is black. The newly added node is on the same side
    of the parent as the parent is of the grandparent. In this case, we make a rotation
    and repaint. We first repaint the parent and grandparent and then rotate the grandparent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the case that is similar to case 3, except the newly added node is on
    the opposite side of the parent as the parent is of the grandparent. Case 3 cannot
    be applied here because doing so will change the black height of the newly added
    node. In this case, we rotate the parent to make it the same as case 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that all the cases can happen in the opposite direction, that is, in mirror
    image. We will handle both the cases the same way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s create our `RedBlackTree` class extending the `BinarySearchTree` class.
    We have to again extend the `Node` class and include a flag to know whether the
    node is black:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We now add a utility method that returns whether a node is black. As explained
    earlier, a null node is considered black:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''re ready to define the method of rebalancing after we do an insertion.
    This method works as described in the four cases earlier. We maintain a `nodeLeftGrandChild`
    flag that stores whether the parent is the left child of the grand parent or its
    right child. This helps us find the uncle and also rotate in the correct direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The insertion is now done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Deletion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deletion starts with a normal binary search tree deletion. If you remember,
    this always involves deletion of a node with at most one child. Deletion of an
    internal node is done by first copying the value of the leftmost node of the right
    subtree and deleting it. So we will consider only this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deletion](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Case 1, 2, and 3 of deletion in a red-black tree
  prefs: []
  type: TYPE_NORMAL
- en: After the deletion is done, the parent of the deleted node either has no child
    or has one child, which was originally its grandchild. During the insertion, the
    problem we needed to solve was a red child of a red parent. In a deletion process,
    this cannot happen. But it can cause the black height to change.
  prefs: []
  type: TYPE_NORMAL
- en: One simple case is that if we delete a red node, the black height does not change
    anything, so we don't have to do anything. Another simple case is that if the
    deleted node were black and the child red, we can simply repaint the child black
    in order to restore the black height.
  prefs: []
  type: TYPE_NORMAL
- en: 'A black child cannot really happen because that would mean the original tree
    was black and unbalanced, as the deleted node had a single black child. But since
    recursion is involved, a black child can actually arise while moving up the path
    with recursive rebalancing. In the following discussion, we only look at cases
    where the deleted node was black and the child was also black (or null child,
    which is considered black). Deletion is done as per the following cases, as shown
    in the figures *Case 1 and 2 and 3 of deletion in a red-black tree* and *Case
    4, 5 and 6 of deletion from a red-black tree*:'
  prefs: []
  type: TYPE_NORMAL
- en: The first case we have is when the parent, sibling, and both the nephews are
    black. In this case, we can simply repaint the sibling to red, which will make
    the parent black and balanced. However, the black height of the whole subtree
    will reduce by one; hence, we must continue rebalancing from the parent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the case when the parent and sibling are black, but the away nephew
    is red. In this case, we cannot repaint the sibling as this would cause the red
    sibling to have a red child, violating constraint 2\. So we first repaint the
    red nephew to black and then rotate to fix the black height of the nephew while
    fixing the black height of the child.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the near nephew is red instead of the away nephew, the rotation does not
    restore the black height of the near nephew that has been repainted. So, we repaint
    NN but do a double rotation instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now consider what happens when the sibling is red. We first repaint the parent
    and sibling using opposite colors and rotating P. But this does not change the
    black height of any node; it reduces the case to case 5 or 6, which we will discuss
    now. So we simply call the rebalancing code again recursively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are now done with all the cases where the parent was black. This is a case
    where the parent is red. In this case, we consider the near nephew to be black.
    Simply rotating the parent fixes the black height.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our final case is when the parent is red and the near nephew is red. In this
    case, we recolor the parent and do a double rotation. Notice that the top node
    remains red. This is not a problem because the original top node, which is the
    parent node, was also red and hence its parent must be black.![Deletion](img/00059.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Case 4, 5, and 6 of deletion from a red-black tree
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we can define the `rebalanceForDelete` method coding all the preceding
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we override the `deleteValue` method to invoke rebalancing after the deletion.
    We only need to rebalance if the deleted node was black. We first check that.
    Then, we need to figure out whether the deleted child was a left child of the
    parent or the right child. After that, we can invoke the `rebalanceForDelete`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The worst case of a red-black tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What is the worst possible red-black tree? We try to find out the same way
    we did in the case of the AVL tree. This one is a little more complicated, though.
    To understand the worst tree, we must take into account the black height. To fit
    the minimum number of nodes n into height h, we need to first choose a black height.
    Now it is desirable to have as few black nodes as possible so that we don''t have
    to include black nodes for balancing the black height in the siblings of the nodes
    we are trying to stretch the height with. Since a red node cannot be the parent
    of another, we must have alternate black nodes. We consider height *h* and an
    even number so that the black height is *h/2 = l*. For simplicity, we don''t count
    the black null nodes for either the height or the black height. The next figure
    shows some examples of the worst trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The worst case of a red-black tree](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Worst red-black trees
  prefs: []
  type: TYPE_NORMAL
- en: The general idea is, of course, to have one path with the maximum possible height.
    This path should be stuffed with the maximum number of red nodes and the other
    paths filled with the least number of nodes, that is, with only black nodes. The
    general idea is shown in the next figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of nodes in a full black tree of height *l-1* is of course *2* ^(l-1)
    *– 1*. So, if the number of nodes for height *h = 2l* is *f(l)*, then we have
    the recursive formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, from the preceding figure, we can already see that *f(1) = 2*, *f(2) =
    6*, and *f(3) = 14*. It looks like the formula should be *f(l) = 2* ^(l-1) *-2*.
    We already have the base cases. If we can prove that if the formula is true for
    l, then it is also true for *l+1*, we would be able to prove the formula for all
    *l* by induction. This is what we will try to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The worst case of a red-black tree](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: General idea of the worst red-black tree
  prefs: []
  type: TYPE_NORMAL
- en: 'We already have *f(l+1) = f(l) + 2l+1* and we also assume *f(l) = 2l+1-2*.
    So this is the case: *f(l+1) = 2l+1-2 + 2l+1 = 2l+2-2*. Hence, if the formula
    holds true for *l*, it also holds true for *l+1*; therefore, it is proved by induction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So the minimum number of nodes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, a red-black tree has a guaranteed logarithmic height; from this,
    it is not hard to derive that the search, insertion, and deletion operations are
    all logarithmic.
  prefs: []
  type: TYPE_NORMAL
- en: Hash tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A hash table is a completely different kind of searchable structure. The idea
    starts from what is called a hash function. It is a function that gives an integer
    for any value of the desired type. For example, the hash function for strings
    must return an integer for every string. Java requires every class to have a `hashcode()`
    method. The object class has one method implemented by default, but we must override
    the default implementation whenever we override the `equals` method. The hash
    function holds the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Same values must always return the same hash value. This is called consistency
    of the hash. In Java, this means if `x` and `y` are two objects and `x.equals(y)`
    is `true`, then `x.hashcode() == y.hashcode()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different values may return the same hash, but it is preferred that they don't.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hash function is computable in constant time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A perfect hash function will always provide a different hash value for different
    values. However, such a hash function cannot be computed in constant time in general.
    So, we normally resort to generating hash values that look seemingly random but
    are really complicated functions of the value itself. For example, `hashcode`
    of the `String` class looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Notice that it is a complicated function that is computed from constituent characters.
  prefs: []
  type: TYPE_NORMAL
- en: A hash table keeps an array of buckets indexed by the hash code. The bucket
    can have many kinds of data structures, but here, we will use a linked list. This
    makes it possible to jump to a certain bucket in constant time and then the bucket
    is kept small enough so that the search within the bucket, even a linear search,
    will not cost that much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a skeleton class for our hash table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We accept two parameters. `InitialSize` is the initial number of buckets we
    want to start with, and our second parameter is the maximum load factor.
  prefs: []
  type: TYPE_NORMAL
- en: What is load factor? Load factor is the average number of values per bucket.
    If the number of buckets is *k* and the total number of values in it is *n*, then
    load factor is *n/k*.
  prefs: []
  type: TYPE_NORMAL
- en: Insertion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Insertion is done by first computing the hash and picking up the bucket in that
    index. Now firstly, the bucket is searched linearly for the value. If the value
    is found, insertion is not carried out; otherwise, the new value is added to the
    end of the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we create a function for inserting in a given array of buckets and then
    using it to perform the insertion. This would be useful when you are dynamically
    growing your hash table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note that effective hash code is computed by taking the remainder of the actual
    hash code divided by the number of buckets. This is done to limit the number of
    hash code.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more thing to be done here and that is rehashing. Rehashing is
    the process of dynamically growing the hash table as soon as it exceeds a predefined
    load factor (or in some cases due to other conditions, but we will use load factor
    in this text). Rehashing is done by creating a second array of buckets of a bigger
    size and copying each element to the new set of buckets. Now the old array of
    buckets is discarded. We create this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can have our completed `insert` function for a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The complexity of insertion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is easy to see that the insert operation is almost constant time unless
    we have to rehash it; in this case, it is *O(n)*. So how many times do we have
    to rehash it? Suppose the load factor is *l* and the number of buckets is *b*.
    Say we start from an `initialSize` *B*. Since we are doubling every time we rehash,
    the number of buckets will be *b = B.2* ^R; here *R* is the number of times we
    rehashed. Hence, the total number of elements can be represented as this: *n =
    bl = Bl. 2* *R*. Check this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: There must be about `lg n` number of rehashing operations, each with complexity
    of `O(n)`. So the average complexity for inserting `n` elements is `O(n lg n)`.
    Hence, the average complexity for inserting each element is `O(lg n)`. This, of
    course, would not work if the values are all clustered together in a single bucket
    that we are inserting into. Then, each insert would be `O(n)`, which is the worst
    case complexity of an insertion.
  prefs: []
  type: TYPE_NORMAL
- en: Deletion is very similar to insertion; it involves deletion of elements from
    the buckets after they are searched.
  prefs: []
  type: TYPE_NORMAL
- en: Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Search is simple. We compute the hash code, go to the appropriate bucket, and
    do a linear search in the bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Complexity of the search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complexity of the search operation is constant time if the values are evenly
    distributed. This is because in this case, the number of elements per bucket would
    be less than or equal to the load factor. However, if all the values are in the
    same bucket, search is reduced to a linear search and it is *O(n)*. So the worst
    case is linear. The average case of search is constant time in most cases, which
    is better than that of binary search trees.
  prefs: []
  type: TYPE_NORMAL
- en: Choice of load factor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the load factor is too big, each bucket would hold a lot of values that would
    output a bad linear search. But if the load factor is too small, there would be
    a huge number of unused buckets causing wastage of space. It is really a compromise
    between search time and space. It can be shown that for a uniformly distributed
    hash code, the fraction of buckets that are empty can be approximately expressed
    as e^(-l), where l is the load factor and e is the base of a natural logarithm.
    If we use a load factor of say 3, then the fraction of empty buckets would be
    approximately e^(-3) = 0.0497 or 5 percent, which is not bad. In the case of a
    non-uniformly distributed hash code (that is, with unequal probabilities for different
    ranges of values of the same width), the fraction of empty buckets would always
    be greater. Empty buckets take up space in the array, but they do not improve
    the search time. Therefore, they are undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we saw a collection of searchable and modifiable data structures.
    All of these allowed you to insert new elements or delete elements while still
    remaining searchable and that too quite optimally. We saw binary search trees
    in which a search follows the paths of the tree from the root. Binary search trees
    can be modified optimally while still remaining searchable if they are of the
    self-balancing type. We studied two different kinds of self-balancing trees: AVL
    trees and red-black trees. Red-black trees are less balanced than AVL trees, but
    they involve a fewer number of rotations than AVL trees. In the end, we went through
    the hash table, which is a different kind of searchable structure. Although the
    worst case complexity of search or insertion is *O(n)*, hash tables provide constant
    time search and average time insertion (*O(lg n)*) in most cases. If a hash table
    does not keep growing, the average insertion and deletion operations will also
    be constant time.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see some more important general purpose data structures.
  prefs: []
  type: TYPE_NORMAL
