- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FIRST Tests and the Test Pyramid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we’ve seen the value of writing unit tests that run quickly
    and give repeatable results. Called FIRST tests, these provide rapid feedback
    on our design. They are the gold standard of unit tests. We’ve also seen how the
    hexagonal architecture helps us design our code in a way that gets the maximum
    amount covered by FIRST unit tests. But we’ve also limited ourselves to testing
    only our domain model – the core of our application logic. We simply have no tests
    covering how our domain model behaves once it connects to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover all the other kinds of tests that we need. We
    will introduce the test pyramid, which is a way of thinking about the different
    kinds of tests needed, and how many of each we should have. We’ll discuss what
    each kind of test covers and useful techniques and tools to help. We’ll also bring
    the whole system together by introducing CI/CD pipelines and test environments,
    outlining the critical role they play in combining code components to create a
    system for our end users.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The test pyramid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests – FIRST tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end and user acceptance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI/CD pipelines and test environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wordz – integration test for our database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Test-Driven-Development-with-Java/tree/main/chapter10](https://github.com/PacktPublishing/Test-Driven-Development-with-Java/tree/main/chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: To run this code, we will need to install the open source Postgres database
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Postgres, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://www.postgresql.org/download/](https://www.postgresql.org/download/)
    in your browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the correct installer for your operating system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Postgres installer selection](img/Figure_10.1_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Postgres installer selection
  prefs: []
  type: TYPE_NORMAL
- en: Follow the instructions for your operating system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The test pyramid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very useful way of thinking about different kinds of tests is by using the
    **test pyramid**. It is a simple graphical representation of the different kinds
    of tests we need around our code and the relative numbers of each. This section
    introduces the key ideas behind the test pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test pyramid in graphic form looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The test pyramid](img/Figure_10.2_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – The test pyramid
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the previous graphic that tests are divided into four layers.
    We have unit tests at the bottom. Integration tests are layered on top of those.
    The pyramid is completed by end-to-end and user acceptance tests at the top. The
    graphic shows unit tests in our system are the highest in number, with fewer integration
    tests and the least number of acceptance tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of these kinds of tests are new to this book. Let’s define what they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are familiar. They are the FIRST tests we have been using up until now.
    One defining feature of these tests is that they do not require the presence of
    any external systems, such as databases or payment processors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration tests**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tests verify that a software component is correctly integrated with an
    external system, such as a database. These tests are slow and critically dependent
    on the external environment being available and correctly set up for our test.
  prefs: []
  type: TYPE_NORMAL
- en: '**End-to-end tests**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the broadest of all tests. An end-to-end test represents something
    very close to the end user experience. This test is performed against all the
    real components of the system – possibly in test environments with test data –
    using the same commands as a real user would use.
  prefs: []
  type: TYPE_NORMAL
- en: '**User** **acceptance tests**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where the real system is tested as a user would use it. Here, we can
    confirm that the final system is fit for purpose, according to the requirements
    the user has given us.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s not obvious at first why having fewer tests of any kind would be an advantage.
    After all, everything up until now in this book has positively praised the value
    of testing. Why do we not simply have *all the tests*? The answer is a pragmatic
    one: not all tests are created equal. They don’t all offer equal value to us as
    developers.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason for the shape of this pyramid is to reflect the practical value of
    each layer of testing. Unit tests written as FIRST tests are *fast* and *repeatable*.
    If we could build a system out of only these unit tests, we surely would. But
    unit tests do not exercise every part of our code base. Specifically, they do
    not exercise connections from our code to the outside world. Nor do they exercise
    our application in the same way as a user would use it. As we progress up through
    the layers of testing, we move away from testing the *internal* components of
    our software and move toward testing how it interacts with *external* systems
    and, ultimately, the end user of our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test pyramid is about *balance*. It aims to create layers of tests that
    achieve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Run as quickly as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cover as much code as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prevent as many defects as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimize duplication of the test effort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will look at a breakdown of the tests involved
    at each layer of the test pyramid. We’ll consider the strengths and weaknesses
    of each kind of test, allowing us to understand what the test pyramid is guiding
    us toward.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests – FIRST tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’re going to look at the base of the test pyramid, which
    consists of unit tests. We’ll examine why this layer is critical to success.
  prefs: []
  type: TYPE_NORMAL
- en: By now, we’re very familiar with FIRST unit tests. The preceding chapters have
    covered these in detail. They are the gold standard of unit tests. They are fast
    to run. They are repeatable and reliable. They run isolated from each other, so
    we can run one, or run many and run them in any order we choose. FIRST tests are
    the powerhouses of TDD, enabling us to work with a rapid feedback loop as we code.
    Ideally, all our code would fall under this feedback loop. It provides a fast,
    efficient way to work. At every step, we can execute code and prove to ourselves
    that it is working as we intended. As a helpful byproduct, by writing tests that
    exercise each possible desirable behavior in our code, we will end up exercising
    every possible code path. We will get 100% *meaningful* test coverage of code
    under unit tests when we work in this way.
  prefs: []
  type: TYPE_NORMAL
- en: Because of their advantages, unit tests form the bedrock of our testing strategy.
    They are represented as the base of the test pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit tests have advantages and limitations, as summarized in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Limitations** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| These are the fastest-running tests and provide the fastest possible feedback
    loop for our code. | The smaller scope of these tests means that having all unit
    tests pass is no guarantee that the system as a whole is working correctly. |'
  prefs: []
  type: TYPE_TB
- en: '| Stable and repeatable, having no dependencies on things outside of our control.
    | They can be written with too strong a tie to implementation details, making
    future additions and refactoring difficult. |'
  prefs: []
  type: TYPE_TB
- en: '| Can provide very detailed coverage of a specific set of logic. Locate defects
    accurately. | Not helpful for testing interactions with external systems. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – Unit test advantages and disadvantages
  prefs: []
  type: TYPE_NORMAL
- en: In any system, we expect to have the largest number of tests at the unit level.
    The test pyramid represents this graphically.
  prefs: []
  type: TYPE_NORMAL
- en: We can’t achieve full coverage by using unit tests alone in the real world but
    we can improve our situation. By applying the hexagonal architecture to our application,
    we can get the majority of code under unit tests. Our fast-running unit tests
    can cover a lot of ground like this and provide a lot of confidence in our application
    logic. We can get as far as knowing that if the external systems behave as we
    expect them to, our domain layer code will be able to correctly handle every use
    case we have thought about.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test position when using unit tests alone is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Unit tests cover the domain model](img/Figure_10.3_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Unit tests cover the domain model
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests only test components of our domain model. They do not test external
    systems, nor do they use external systems. They rely on test doubles to simulate
    our external systems for us. This gives us advantages in development cycle speed
    but has the drawback that our connections to those external systems remain untested.
    If we have a piece of unit-tested code that accesses a repository interface, we
    know that its logic works with a stub repository. Its internal logic will even
    have 100% test coverage and this will be valid. But we won’t know if it will work
    with the real repository yet.
  prefs: []
  type: TYPE_NORMAL
- en: The adapter layer code is responsible for those connections, and it is not tested
    at the unit test level. To test this layer, we’re going to need a different approach
    to testing. We will need to test what happens when our domain layer code is integrated
    with actual external systems.
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at how we test these external systems adapters using
    a kind of testing known as integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’re going to look at the next layer up in the test pyramid:
    integration testing. We’ll see why it’s important, review helpful tools, and understand
    the role of integration testing in the overall scheme of things.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration tests** exist to test that our code will successfully integrate
    with external systems. Our core application logic is tested by unit tests, which,
    by design, do not interact with external systems. This means that we need to test
    behavior with those external systems at some point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Integration tests are the second layer up in the test pyramid. They have advantages
    and limitations, as summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Limitations** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Test that software components interact correctly when connected | Require
    test environments to be set up and maintained |'
  prefs: []
  type: TYPE_TB
- en: '| Provide a closer simulation of the software system as it will be used live
    | Tests run more slowly than unit tests |'
  prefs: []
  type: TYPE_TB
- en: '|  | Susceptible to problems in the test environment, such as incorrect data
    or network connection failures |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – Integration test advantages and disadvantages
  prefs: []
  type: TYPE_NORMAL
- en: There should be fewer integration tests than unit tests. Ideally, far fewer.
    While unit tests avoided many problems of testing external systems by using test
    doubles, integration tests must now face those challenges. By nature, they are
    more difficult to set up. They *can* be less repeatable. They generally run more
    slowly than unit tests do, as they wait for responses from external systems.
  prefs: []
  type: TYPE_NORMAL
- en: To give a sense of this, a typical system might have thousands of unit tests
    and hundreds of acceptance tests. In between, we have several integration tests.
    Many integration tests point to a design opportunity. We can refactor the code
    so that our integration test is pushed down to being a unit test or promoted to
    being an acceptance test.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to have fewer integration tests is due to **flaky tests**. A
    flaky test is a nickname given to a test that sometimes passes and sometimes fails.
    When it fails, it is due to some problem interacting with the external system
    and not a defect in the code we are testing. Such a failure is called a **false
    negative** test result – a result that can mislead us.
  prefs: []
  type: TYPE_NORMAL
- en: Flaky tests are a nuisance precisely because we cannot immediately tell the
    root cause of the failure. Without diving into error logs, we only know that the
    test failed. This leads to developers learning to ignore these failed tests, often
    choosing to re-run the test suite several times until the flaky test passes. The
    problem here is that we are training developers to have less faith in their tests.
    We are training them to ignore test failures. This is not a good place to be.
  prefs: []
  type: TYPE_NORMAL
- en: What should an integration test cover?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our design so far, we have decoupled external systems from our domain code
    using the *Dependency Inversion Principle*. We have created an interface defining
    how we use that external system. There will be some implementation of this interface,
    which is what our integration test will be covering. In hexagonal architecture
    terms, this is an *adapter*.
  prefs: []
  type: TYPE_NORMAL
- en: This adapter should only contain the minimum amount of code necessary to interact
    with the external system in a way that satisfies our interface. It should have
    no application logic in it at all. That should be inside the domain layer and
    covered by unit tests. We call this a *thin adapter*, doing only enough work to
    adapt to the external system. This means our integration test is nicely limited
    in scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent the scope of an integration test like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Integration tests cover the adapter layer](img/Figure_10.4_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Integration tests cover the adapter layer
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests only test the adapter layer components, those pieces of code
    that directly interact with external systems, such as databases and web endpoints.
    The integration test will create an instance of the adapter under test and arrange
    for it to connect to *a version* of the external service. This is important. We’re
    still not connecting to the production services yet. Until the integration test
    passes, we’re not sure that our adapter code works correctly. So, we don’t want
    to access real services just yet. We also want to have that extra level of control
    over these services. We want to be able to safely and easily create test accounts
    and fake data to use with our adapter. That means we need a collection of live-like
    services and databases to use. That means they have to live and run somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '**Test environments** are the name given to the arrangement of external systems
    we use in integration tests. It is an environment for running web services and
    data sources, specifically for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: A test environment enables our code to connect to test versions of real external
    systems. It’s one step closer to production readiness, compared to the unit test
    level. There are some challenges involved in using test environments, however.
    Let’s look into the good practices for testing integrations with databases and
    web services.
  prefs: []
  type: TYPE_NORMAL
- en: Testing database adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic approach to testing a database adapter is to set up a database server
    in the test environment and get the code under test to connect to it. The integration
    test will preload a known dataset into the database as part of its Arrange step.
    The test then runs the code that interacts with the database in the Act step.
    The Assert step can inspect the database to see if expected database changes happened.
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest challenge in testing a database is that it remembers data. Now,
    this might seem a little obvious, as that is the entire point of using a database
    in the first place. But it conflicts with one of the goals of testing: to have
    isolated, repeatable tests. As an example, if our test created a new user account
    for user `testuser1` and that was stored in the database, we would have a problem
    running that test again. It would not be able to create `testuser1` and instead
    would receive a **user already** **exists** error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different approaches to overcoming this problem, each with trade-offs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Delete all data from the database before and after each** **test case**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach preserves the isolation of our tests, but it is slow. We have
    to recreate the test database schema before every test.
  prefs: []
  type: TYPE_NORMAL
- en: '**Delete all data before and after the full set of adapter** **tests run**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We delete data less often, allowing several related tests to run against the
    same database. This loses test isolation due to the stored data, as the database
    will not be in the state expected at the start of the next test. We have to run
    tests in a particular order, and they must all pass, to avoid spoiling the database
    state for the next test. This is not a good approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Use** **randomized data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of creating *testuser1* in our test, we randomize names. So, on one
    run, we might get `testuser-cfee-0a9b-931f`. On the next run, the randomly chosen
    username would be something else. The state stored in the database will not conflict
    with another run of the same test. This is another way to preserve test isolation.
    However, it does mean that tests can be harder to read. It requires periodic cleanup
    of the test database.
  prefs: []
  type: TYPE_NORMAL
- en: '**Rollback transactions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can add data required by our tests inside a database transaction. We can
    roll back the transaction at the end of the test.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ignore** **the problem**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, if we work with read-only databases, we can add test data that will
    never be accessed by the production code and leave it there. If this works, it
    is an attractive option requiring no extra effort.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as *database-rider*, available from [https://database-rider.github.io/getting-started/](https://database-rider.github.io/getting-started/),
    assist by providing library code to connect to databases and initialize them with
    test data.
  prefs: []
  type: TYPE_NORMAL
- en: Testing web services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A similar approach is used to test the integration with web services. A test
    version of the web service is set to run in the test environment. The adapter
    code is set to connect to this test version of the web service, instead of the
    real version. Our integration test can then examine how the adapter code behaves.
    There might be additional web APIs on the test service to allow inspection by
    the assertions in our test.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the disadvantages are a slower running test and the risk of flaky tests
    due to issues as trivial as network congestion.
  prefs: []
  type: TYPE_NORMAL
- en: Sandbox APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, hosting our own local service might be impossible, or at least undesirable.
    Third-party vendors are usually unwilling to release test versions of their service
    for us to use in our test environment. Instead, they typically offer a **sandbox
    API**. This is a version of their service that the third party hosts, not us.
    It is disconnected from their production systems. This sandbox allows us to create
    test accounts and test data, safe from affecting anything real in production.
    It will respond to our requests as their production versions will respond, but
    without taking any action such as taking payment. Consider them test simulators
    for real services.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer-driven contract testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A useful approach to testing interactions with web services is called **consumer-driven
    contract testing**. We consider our code as having a contract with the external
    service. We agree to call certain API functions on the external service, supplying
    data in the form required. We need the external service to respond to us predictably,
    with data in a known format and well-understood status codes. This forms a *contract*
    between the two parties – our code and the external service API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consumer-driven contract testing involves two components, based on that contract,
    often using code generated by tools. This is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Consumer-driven contract testing](img/Figure_10.5_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Consumer-driven contract testing
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows that we’ve captured the expected interactions with
    an external service as an API contract. Our adapter for that service will be coded
    to implement that API contract. When using consumer-driven contract testing, we
    end up with two tests, which test either side of that contract. If we consider
    a service to be a black box, we have a public interface presented by the black
    box, and an implementation, whose details are hidden inside that black box. A
    contract test is two tests. One test confirms that the outside interface is compatible
    with our code. The other test confirms that the implementation of that interface
    works and gives the expected results.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical contract test will need two pieces of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A stub of the external service**: A stub of the external service is generated.
    If we are calling a payment processor, this stub simulates the payment processor
    locally. This allows us to use it as a test double for the payment processor service
    as we write our adapter code. We can write an integration test against our adapter,
    configuring it to call this stub. This allows us to test our adapter code logic
    without accessing the external system. We can verify that the adapter sends the
    correct API calls to that external service and handles the expected responses
    correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A replay of a set of calls to the real external service**: The contract also
    allows us to run tests against the real external service – possibly in sandbox
    mode. We’re not testing the functionality of the external service here – we assume
    that the service provider has done that. Instead, we are verifying that what we
    believe about its API is true. Our adapter has been coded to make certain API
    calls in certain orders. This test verifies that this assumption is correct. If
    the test passes, we know that our understanding of the external service API was
    correct and also that it has not changed. If this test was previously working
    but now fails, that would be an early indication that the external service has
    changed its API. We would then need to update our adapter code to follow that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One recommended tool for doing this is called Pact, available at [https://docs.pact.io](https://docs.pact.io).
    Read the guides there for more details on this interesting technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen that integration tests get us one step nearer to production. In
    the next section, we look at the final level of testing in the test pyramid, which
    is the most live-like so far: user acceptance tests.'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end and user acceptance tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will progress to the top of the test pyramid. We’ll review
    what end-to-end and user acceptance tests are and what they add to unit and integration
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: At the top of the test pyramid lies two similar kinds of tests called **end-to-end
    tests** and **user acceptance tests**. Technically, they are the same kind of
    test. In each case, we start up the software fully configured to run in its most
    live-like test environment, or possibly in production. The idea is that the system
    is tested as a whole from one end to the other.
  prefs: []
  type: TYPE_NORMAL
- en: One specific use of an end-to-end test is for **user acceptance testing** (**UAT**).
    Here, several key end-to-end test scenarios are run. If they all pass, the software
    is declared fit for purpose and accepted by the users. This is often a contractual
    stage in commercial development, where the buyer of the software formally agrees
    that the development contract has been satisfied. It’s still end-to-end testing
    that is being used to determine that, with cherry-picked test cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'These tests have advantages and limitations, as summarized in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Limitations** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Most comprehensive testing of functionality available. We are testing at
    the same level that a user of our system – either person or machine – would experience
    our system. | Slowest tests to run. |'
  prefs: []
  type: TYPE_TB
- en: '| Tests at this level are concerned with pure behavior as observed from outside
    the system. We could refactor and rearchitect large parts of the system and still
    have these tests protect us. | Reliability issues – many problems in the setup
    and environment of our system can cause false negative test failures. This is
    termed “brittleness” – our tests are highly dependent on their environment working
    correctly. Environments can be broken due to circumstances beyond our control.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Contractually important – these tests are the essence of what the end user
    cares about. | These are the most challenging of all the tests to write, due to
    the extensive environment setup requirements. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.3 – End-to-end test advantages and disadvantages
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance tests having a spot at the top of the pyramid is a reflection that
    we don’t need many of them. The majority of our code should now be covered by
    unit and integration tests, assuring us that our application logic works, as well
    as our connections to external systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious question is *what’s left to test*? We don’t want to duplicate testing
    that has already been done at the unit and integration levels. But we do need
    some way to validate that the software *as a whole* is going to work as expected.
    This is the job of end-to-end testing. This is where we configure our software
    so that it connects to real databases and real external services. Our production
    code has passed all the unit tests with test doubles. These test passes suggest
    our code *should* work when we connect these real external services. But *should*
    is a wonderful weasel word in software development. Now, is the time to verify
    that it does, using an end-to-end test. We can represent the coverage of these
    tests using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – End-to-end/user acceptance tests cover the entire code base](img/Figure_10.6_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – End-to-end/user acceptance tests cover the entire code base
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests cover the entire code base, both the domain model and the adapter
    layer. As such, it repeats testing work already done by unit and integration tests.
    The main technical aspect we want to test in end-to-end testing is that our software
    is configured and wired up correctly. Throughout this book, we have used *dependency
    inversion and injection* to isolate us from external systems. We’ve created test
    doubles and injected those. Now, we must create actual production code, the real
    adapter layer components that connect to the production systems. We inject those
    into our system during its initialization and configuration. This sets the code
    up to work for real.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests will then duplicate a *small amount* of happy path testing
    already covered by unit and integration tests. The purpose here is *not* to verify
    the behaviors that we have already tested. Instead, these tests verify that we
    have injected the correct production objects, by confirming that the system as
    a whole behaves correctly when connected to production services.
  prefs: []
  type: TYPE_NORMAL
- en: 'A user acceptance test builds on this idea by running through key test scenarios
    considered critical to accepting the software as complete. These will be end-to-end
    tests at a technical level. But their purpose is broader than the technical goal
    of ensuring our system is correctly configured. They are more of a legal contractual
    nature: *Have we built what was asked of us?* By using the iterative approach
    in this book together with its technical practices, there’s a higher chance that
    we will have done so.'
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance testing tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Various testing libraries exist to help us write automated acceptance and end-to-end
    tests. Tasks such as connecting to a database or calling an HTTP web API are common
    to this kind of testing. We can leverage libraries for these tasks, instead of
    writing code ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: The main differentiator among these tools is the way they interact with our
    software. Some are intended to simulate a user clicking a desktop GUI, or a browser-based
    web UI. Others will make HTTP calls to our software, exercising a web endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few popular acceptance testing tools to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RestEasy**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A popular tool for testing REST APIs: [https://resteasy.dev/](https://resteasy.dev/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**RestAssured**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another popular tool for testing REST APIs that takes a fluent approach to
    inspecting JSON responses: [https://rest-assured.io/](https://rest-assured.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selenium**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A popular tool for testing web UIs through the browser: [https://www.selenium.dev/](https://www.selenium.dev/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cucumber**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available from [https://cucumber.io/](https://cucumber.io/). Cucumber allows
    English language-like descriptions of tests to be written by domain experts. At
    least, that’s the theory. I’ve never seen anybody other than a developer write
    Cucumber tests in any project I’ve been part of.
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance tests form the final piece of the test pyramid and allow our application
    to be tested under conditions that resemble the production environment. All that
    is needed is a way to automate running all those layers of testing. That’s where
    CI/CD pipelines come in, and they are the subject of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD pipelines and test environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CI/CD pipelines and test environments are an important part of software engineering.
    They are a part of the development workflow that takes us from writing code to
    having systems in the hands of users. In this section, we’re going to look at
    what the terms mean and how we can use these ideas in our projects.
  prefs: []
  type: TYPE_NORMAL
- en: What is a CI/CD pipeline?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with defining the terms:'
  prefs: []
  type: TYPE_NORMAL
- en: CI stands for **continuous integration**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration is where we take individual software components and join them together
    to make a whole. CI means we do this all the time as we write new code.
  prefs: []
  type: TYPE_NORMAL
- en: CD stands for either **continuous delivery** or **continuous deployment**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll cover the difference later, but in both cases, the idea is that we are
    taking the latest and greatest version of our integrated software and delivering
    it to a stakeholder. The goal of continuous delivery is that we could – if we
    wanted to – deploy every single code change to production with a single click
    of a button.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that CI/CD is an engineering *discipline* – not a set
    of tools. However we achieve it, CI/CD has the goal of growing a single system
    that is always in a usable state.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need continuous integration?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In terms of the test pyramid, the reason we need CI/CD is to pull all the testing
    together. We need a mechanism to build the whole of our software, using the latest
    code. We need to run all the tests and ensure they all pass before we can package
    and deploy the code. If any tests fail, we know the code is not suitable for deployment.
    To ensure we get fast feedback, we must run the tests in order of fastest to slowest.
    Our CI pipeline will run unit tests first, followed by integration tests, followed
    by end-to-end and acceptance tests. If any tests fail, the build will produce
    a report of test failures for that stage, then stop the build. If all the tests
    pass, we package our code up ready for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, the idea of **integration** is fundamental to building software,
    whether we work alone or in a development team. When working alone, following
    the practices in this book, we’re building software out of several building blocks.
    Some we have made ourselves, while for others, we’ve selected a suitable library
    component and used that. We’ve also written adapters – components that allow us
    to access external systems. All of that needs integrating – bringing together
    as a whole – to turn our lines of code into a working system.
  prefs: []
  type: TYPE_NORMAL
- en: When working in a team, integration is even more important. We need to not only
    bring together the pieces we have written but also all the other pieces written
    by the rest of our team. Integrating work in progress from colleagues is urgent.
    We end up building on what others have already written. As we work outside of
    the main integrated code base, there is a risk of not including the latest design
    decisions and pieces of reusable code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the goal of CI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Continuous integration](img/Figure_10.7_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Continuous integration
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind CI was to avoid the classic waterfall development trap,
    where a team wrote code as isolated individuals while following a plan and only
    integrated it at the end. Many times, that integration failed to produce working
    software. There was often some misunderstanding or missing piece that meant components
    did not fit together. At this late stage of a waterfall project, mistakes are
    expensive to fix.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not just big teams and big projects that suffer from this. My turning point
    was while writing a flight simulator game for Britain’s RAF Red Arrows display
    team. Two of us worked on that game to a common API we had agreed on. When we
    first attempted to integrate our parts – at 03:00 A.M., in front of the company
    managing director, of course – the game ran for about three frames and then crashed.
    Oops! Our lack of CI provided an embarrassing lesson. It would have been good
    to know that was going to happen a lot earlier, especially without the managing
    director watching.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need continuous delivery?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If CI is all about keeping our software components together as an ever-growing
    whole, then CD is about getting that whole into the hands of people who care about
    it. The following figure illustrates CD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Continuous delivery](img/Figure_10.8_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Continuous delivery
  prefs: []
  type: TYPE_NORMAL
- en: 'Delivering a stream of value to end users is a core tenet of agile development.
    No matter which flavor of agile methodology you use, getting features into the
    hands of users has always been the goal. We want to deliver usable features at
    regular, short intervals. Doing this provides three benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Users get the value** **they want**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End users don’t care about our development process. They only care about getting
    solutions to their problems. Whether that’s the problem of being entertained while
    waiting for an Uber ride, or the problem of paying everyone’s wages in a multinational
    business, our user just wants their problem gone. Getting valuable features to
    our users becomes a competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: '**We gain valuable** **user feedback**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yes, that’s what I asked for – but it isn’t what I meant!* That is extremely
    valuable user feedback that agile approaches deliver. Once an end user sees the
    feature as we have implemented it, sometimes, it becomes clear that it isn’t quite
    solving their problem. We can correct this quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aligns the code base and** **development team**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To pull off this feat, you do need to have your team and workflows together.
    You can’t effectively do this unless your workflow results in known working software
    being continuously available as a single whole.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery or continuous deployment?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exact definitions of these terms seem to vary, but we can think of them like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous delivery**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deliver software to internal stakeholders, such as product owners and QA
    engineers
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous deployment**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deliver software into production and to end users
  prefs: []
  type: TYPE_NORMAL
- en: Out of the two, continuous *deployment* sets a much higher bar. It requires
    that once we integrate code into our pipeline, that code is ready to go live –
    into production, to real users. This is, of course, hard. It needs top-class test
    automation to give us confidence that our code is ready to deploy. It also benefits
    from having a fast rollback system in production – some means of quickly reverting
    a deployment if we discover a defect not covered by our tests. Continuous deployment
    is the ultimate workflow. For all who achieve it, deploying new code last thing
    on Friday simply holds no fear. Well, maybe a little less fear.
  prefs: []
  type: TYPE_NORMAL
- en: Practical CI/CD pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most projects use a CI tool to handle the sequencing chores. Popular tools
    are provided by Jenkins, GitLab, CircleCI, Travis CI, and Azure DevOps. They all
    work similarly, executing separate build stages sequentially. That’s where the
    name pipeline comes from – it resembles a pipe being loaded at one end with the
    next build stage and coming out of the other end of the pipe, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Stages in a CI pipeline](img/Figure_10.9_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Stages in a CI pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'A CI pipeline comprises the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source control**: Having a common location in which to store the code is
    essential to CI/CD. It is the place where code gets integrated. The pipeline starts
    here, by pulling down the latest version of the source code and performing a clean
    build. This prevents errors caused by older versions of code being present on
    the computer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.jar` file, to run on the JVM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Static code analysis**: Linters and other analysis tools check the source
    code for stylistic violations, such as variable length and naming conventions.
    The development team can choose to fail the build when specific code issues are
    detected by static analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Unit tests**: All the unit tests are run against the built code. If any fail,
    the pipeline stops. Test failure messages are reported.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Integration tests**: All integration tests are run against the built code.
    If any fail, the pipeline is stopped and error messages are reported.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Acceptance tests**: All acceptance tests are run against the built code.
    If all tests pass, the code is considered to be working and ready for delivery/deployment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.jar` file containing an embedded web server.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens next depends on the needs of the project. The packaged code may
    be deployed to production automatically or it may simply be placed in some internal
    repository, for access by product owners and QA engineers. Formal deployment would
    then happen later, after quality gatekeeping.
  prefs: []
  type: TYPE_NORMAL
- en: Test environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One obvious problem caused by needing a CI pipeline to run integration tests
    is having a place to run those tests. Ordinarily, in production, our application
    integrates with external systems such as databases and payment providers. When
    we run our CI pipeline, we do not want our code to process payments or write to
    production databases. Yet we *do* want to test that the code *could* integrate
    with those things, once we configure it to connect to those real systems.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to create a **test environment**. These are collections of databases
    and simulated external systems that lie under our control. If our code needs to
    integrate with a database of user details, we can create a copy of that user database
    and run it locally. During testing, we can arrange for our code to connect to
    this local database, instead of the production version. External payment providers
    often provide a sandbox API. This is a version of their service that, again, does
    not connect to any of their real customers. It features simulated behavior for
    their service. In effect, it is an external test double.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of setup is called a **live-like** or **staging** environment. It
    allows our code to be tested with more realistic integration. Our unit tests use
    stubs and mocks. Our integration tests can now use these richer test environments.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and challenges of using test environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Test environments offer both advantages and disadvantages, as summarized in
    the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Advantages** | **Challenges** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **The environment** **is self-contained**We can create it and destroy it
    at will. It will not affect production systems. | **Not** **production environments**No
    matter how live-like we make them, these environments are simulations. The risk
    is that our fake environments give us false positives – tests that pass only because
    they are using fake data. This can give us false confidence, leading us to deploy
    code that will fail in production.The real test happens when we set our code live.
    Always. |'
  prefs: []
  type: TYPE_TB
- en: '| **More realistic** **than stubs**The environment gets us one step closer
    to testing under production loads and conditions. | **Extra effort to create**
    **and maintain**More development work is needed to set these environments up and
    keep them in step with the test code. |'
  prefs: []
  type: TYPE_TB
- en: '| **Check assumptions about** **external systems**Third-party sandbox environments
    allow us to confirm that our code uses the latest, correct API, as published by
    the supplier. | **Privacy concerns**Simply copying over a chunk of production
    data isn’t good enough for a test environment. If that data contains **personally
    identifiable information** (**PII**) as defined by GDPR or HIPAA, then we can’t
    legally use it directly. We have to create an extra step to anonymize that data
    or generate pseudo-realistic random test data. Neither is trivial. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.4 – Test environments advantages and challenges
  prefs: []
  type: TYPE_NORMAL
- en: Testing in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I can hear the gasps already! Running our tests in production is generally a
    terrible idea. Our tests might introduce fake orders that our production system
    treats as real ones. We may have to add test user accounts, which can present
    a security risk. Worse still, because we are in a testing phase, there is a very
    good chance that our code does not work yet. This can cause all sorts of problems
    – all while connected to production systems.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these concerns, sometimes, things must be tested in production. Big
    data companies such as Google and Meta both have things that can only be tested
    live due to the sheer scale of their data. There is no way a meaningful live-like
    test environment can be created; it will simply be too small. What can we do in
    cases like this?
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach is to mitigate the risks. Two techniques are valuable here: blue-green
    deployment and traffic partitioning.'
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Blue-green deployment** is a deployment technique designed for the rapid
    rollback of failed deployments. It works by dividing the production servers into
    two groups. They are referred to as *blue* and *green*, chosen as they are neutral
    colors that both denote success. Our production code will be running on one group
    of servers at any one time. Let’s say we are currently running on the blue group.
    Our next deployment will then be to the green group. This is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Blue-green deployment](img/Figure_10.10_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Blue-green deployment
  prefs: []
  type: TYPE_NORMAL
- en: Once the code has been deployed to the green group, we switch over the production
    configuration to connect to green group servers. We retain the previous working
    production code on the blue servers. If our testing goes well against the green
    group, then we’re done. Production is now working with the latest green group
    code. If the testing fails, we revert that configuration to connect to the blue
    servers once again. It’s a fast rollback system that enables our experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Traffic partitioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to blue-green deployment, we can limit the amount of traffic that
    we send to our test servers. Instead of flipping production to wholly use the
    new code under test, we can simply send a small percentage of user traffic there.
    So, 99% of users might be routed to our blue servers, which we know to work. 1%
    can be routed to our new code under test in the green servers, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Traffic partitioning](img/Figure_10.11_B18384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – Traffic partitioning
  prefs: []
  type: TYPE_NORMAL
- en: If defects are discovered, only 1% of users will be affected before we revert
    to 100% blue servers. This gives us a rapid rollback, mitigating problems in production
    caused by a failed deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now covered the roles of different kinds of tests and seen how they fit
    into a coherent system known as the test pyramid. In the next section, we’ll apply
    some of this knowledge to our Wordz application by writing an integration test.
  prefs: []
  type: TYPE_NORMAL
- en: Wordz – integration test for our database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll review an integration test for our Wordz application
    to get a feel for what they look like. We’ll cover the details of writing these
    tests and setting up the test tools in [*Chapter 14*](B18384_14.xhtml#_idTextAnchor293),
    *Driving the Database Layer*, and [*Chapter 15*](B18384_15.xhtml#_idTextAnchor306),
    *Driving the* *Web Layer*.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching a word from the database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As part of our earlier design work, we identified that Wordz would need a place
    to store the candidate words to be guessed. We defined an interface called `WordRepository`
    to isolate us from the details of storage. At that iteration, we had only got
    as far as defining one method on the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of this WordRepository interface will access the database
    and return a word given its `wordNumber`. We will defer implementing this to [*Chapter
    14*](B18384_14.xhtml#_idTextAnchor293), *Driving the Database Layer*. For now,
    let’s take an early look at what the integration test will look like, at a high
    level. The test uses open source libraries to help write the test, and to provide
    the database. We’ve chosen the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An open source library called `database-rider` (available from [https://database-rider.github.io/getting-started/](https://database-rider.github.io/getting-started/))
    as a test tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Postgres, a popular open source relational database, to store our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the test code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fetchesWord()` test method is marked by the `@DataSet` annotation. This
    annotation is provided by the *database-rider* test framework and it forms the
    Arrange step of our test. It specifies a file of known test data that the framework
    will load into the database before the test runs. The data file is located underneath
    the root folder of `src/test/resources`. The parameter in the annotation gives
    the rest of the path. In our case, the file will be located at `src/test/resources/adapters/data/wordTable.json`.
    Its content looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This JSON file tells the `database-rider` framework that we would like to insert
    a single row into a database table named `WORD`, with column values of `1`, `27`,
    and `ARISE`.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not going to write the adapter code to make this test pass just yet. There
    are several steps we would need to take to get this test to compile, including
    downloading various libraries and getting the Postgres database up and running.
    We’ll cover these steps in detail in [*Chapter 14*](B18384_14.xhtml#_idTextAnchor293),
    *Driving the* *Database Layer*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overview of this integration test code is that it is testing a new class
    called `WordRepositoryPostgres` that we will write. That class will contain the
    database access code. We can see the tell-tale JDBC object, `javax.sql.DataSource`,
    which represents a database instance. This is the clue that we are testing integration
    with a database. We can see new annotations from the database testing library:
    `@DBRider` and `@DataSet`. Finally, we can see something instantly recognizable
    – the Arrange, Act, and Assert steps of a test:'
  prefs: []
  type: TYPE_NORMAL
- en: The Arrange step creates a `WordRepositoryPostgres` object, which will contain
    our database code. It works with the `database-rider` library’s `@DataSet` annotation
    to put some known data into the database before the test runs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Act step calls the `fetchWordByNumber()` method, passing in the numeric
    `wordNumber` we want to test. This number aligns with the contents of the `wordTable.json`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Assert step confirms the expected word, `ARISE`, is returned from the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see, integration tests aren’t so different from unit tests in essence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve seen how the test pyramid is a system that organizes
    our testing efforts, keeping FIRST unit tests firmly as the foundation for all
    we do, but not neglecting other testing concerns. First, we introduced the ideas
    of integration and acceptance testing as ways of testing more of our system. Then,
    we looked at how the techniques of CI and CD keep our software components brought
    together and ready to release at frequent intervals. We’ve seen how to bring the
    whole build process together using CI pipelines, possibly going on to CD. We’ve
    made a little progress on Wordz by writing an integration test for the `WordRepositoryPostgres`
    adapter, setting us up to write the database code itself.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll take a look at the role of manual testing in our
    projects. It’s clear by now that we automate as much testing as we can, meaning
    that the role of manual testing no longer means following huge test plans. Yet,
    manual testing is still very valuable. How has the role changed? We’ll review
    that next.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some questions and their answers regarding this chapter’s
    material:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is the test pyramid represented as a pyramid shape?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shape depicts a broad foundation of many unit tests. It shows layers of
    testing above that exercise a closer approximation to the final, integrated system.
    It also shows that we expect fewer tests at those higher levels of integration.
  prefs: []
  type: TYPE_NORMAL
- en: What are the trade-offs between unit, integration, and acceptance tests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unit tests: Fast, repeatable. Don’t test connections to external systems.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integration tests: Slower, sometimes unrepeatable. They test the connection
    to the external system.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Acceptance tests: Slowest of all. They can be flaky but offer the most comprehensive
    tests of the whole system.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the test pyramid guarantee correctness?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. Testing can only ever reveal the presence of a defect, *never* the absence
    of one. The value of extensive testing is in how many defects we avoid putting
    into production.
  prefs: []
  type: TYPE_NORMAL
- en: Does the test pyramid only apply to object-oriented programming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. This strategy of test coverage applies to any programming paradigm. We can
    write code using any paradigm - object-oriented, functional, procedural, or declarative.
    The various kinds of tests only depend on whether our code accesses external systems
    or makes up purely internal components.
  prefs: []
  type: TYPE_NORMAL
- en: Why don’t we prefer end-to-end tests, given they test the whole system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: End-to-end tests run slowly. They depend directly on having either production
    databases and web services running, or a test environment running containing test
    versions of those things. The network connections required, and things such as
    database setup, can result in tests giving us false negative results. They fail
    because of the environment, not because the code was incorrect. Because of these
    reasons, we engineer our system to make maximum use of fast, repeatable unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to consumer-driven* *contract testing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pact.io produce a popular open source contract testing tool that’s available
    on their website, [https://docs.pact.io](https://docs.pact.io). The website features
    an explanatory video and a useful introduction to the benefits of contract-driven
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: '*Database-rider database* *testing library*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An open source database integration testing library that works with JUnit5\.
    It is available from [https://database-rider.github.io/getting-started/](https://database-rider.github.io/getting-started/).
  prefs: []
  type: TYPE_NORMAL
- en: '*Modern Software Engineering, Dave Farley,* *ISBN 978-0137314911*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book explains in detail the reasons behind CD and various technical practices
    such as trunk-based development to help us achieve that. Highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: '*Minimum CD*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Details on what is needed for CD: [https://minimumcd.org/minimumcd/](https://minimumcd.org/minimumcd/).'
  prefs: []
  type: TYPE_NORMAL
