["```java\napply plugin: 'java'\napply plugin: 'application'\n\nsourceCompatibility = '1.8'\n\nmainClassName = 'kioto.ProcessingEngine'\n\nrepositories {\n mavenCentral()\n maven { url 'https://packages.confluent.io/maven/' }\n}\n\nversion = '0.1.0'\n\ndependencies {\n  compile 'com.github.javafaker:javafaker:0.15'\n  compile 'com.fasterxml.jackson.core:jackson-core:2.9.7'\n  compile 'io.confluent:kafka-avro-serializer:5.0.0'\n  compile 'org.apache.kafka:kafka_2.12:2.0.0'\n  compile 'org.apache.kafka:kafka-streams:2.0.0'\n  compile 'io.confluent:kafka-streams-avro-serde:5.0.0'\n}\n\njar {\n  manifest {\n    attributes 'Main-Class': mainClassName\n  } from {\n    configurations.compile.collect {\n       it.isDirectory() ? it : zipTree(it)\n    }\n  }\n  exclude \"META-INF/*.SF\"\n  exclude \"META-INF/*.DSA\"\n  exclude \"META-INF/*.RSA\"\n}\n```", "```java\ncompile 'org.apache.kafka:kafka-streams:2.0.0'\n```", "```java\ncompile 'io.confluent:kafka-streams-avro-serde:5.0.0'\n```", "```java\nconfigurations.compile.collect {\n  it.isDirectory() ? it : zipTree(it)\n}\n```", "```java\nsrc\nmain\n--java\n----kioto\n------avro\n------custom\n------events\n------plain\n------serde\n--resources\ntest\n```", "```java\nimport ...\npublic final class PlainStreamsProcessor {\n  private final String brokers;\n  public PlainStreamsProcessor(String brokers) {\n    super();\n    this.brokers = brokers;\n  }\n  public final void process() {\n    // below we will see the contents of this method \n  }\n  public static void main(String[] args) {\n    (new PlainStreamsProcessor(\"localhost:9092\")).process();\n  }\n}\n```", "```java\nStreamsBuilder streamsBuilder = new StreamsBuilder();\n```", "```java\nKStream healthCheckJsonStream = \n  streamsBuilder.stream( Constants.getHealthChecksTopic(), \n    Consumed.with(Serdes.String(), Serdes.String()));\n```", "```java\nKStream healthCheckStream = healthCheckJsonStream.mapValues((v -> {\n  try {\n    return Constants.getJsonMapper().readValue(\n      (String) v, HealthCheck.class);\n  } catch (IOException e) {\n    // deal with the Exception\n  }\n }));\n```", "```java\nKStream uptimeStream = healthCheckStream.map(((KeyValueMapper)(k, v)-> {\n  HealthCheck healthCheck = (HealthCheck) v;\n  LocalDate startDateLocal = healthCheck.getLastStartedAt().toInstant()\n              .atZone(ZoneId.systemDefault()).toLocalDate();\n  int uptime = Period.between(startDateLocal, LocalDate.now()).getDays();\n  return new KeyValue<>(\n    healthCheck.getSerialNumber(), String.valueOf(uptime));\n }));\n```", "```java\nuptimeStream.to( Constants.getUptimesTopic(), \n  Produced.with(Serdes.String(), Serdes.String()));\n```", "```java\nTopology topology = streamsBuilder.build();\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", this.brokers);\nprops.put(\"application.id\", \"kioto\");\nKafkaStreams streams = new KafkaStreams(topology, props);\nstreams.start();\n```", "```java\n$ gradle build\n```", "```java\nBUILD SUCCESSFUL in 1s\n6 actionable task: 6 up-to-date\n```", "```java\n$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n--topic uptimes --property print.key=true\n```", "```java\nEW05-HV36 33\nBO58-SB28 20\nDV03-ZT93 46\n...\n```", "```java\n$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n--topic uptimes --property print.key=true\n```", "```java\n$ java -cp ./build/libs/kioto-0.1.0.jar \nkioto.plain.PlainStreamsProcessor\n```", "```java\n$ java -cp ./build/libs/kioto-0.1.0.jar \nkioto.plain.PlainStreamsProcessor\n```", "```java\n2017/07/05 15:03:18.045 INFO ... Setting newly assigned \npartitions [healthchecks-2, healthchecks -3]\n```", "```java\n...\n2017/07/05 15:03:18.045 INFO ... Revoking previously assigned partitions [healthchecks -0, healthchecks -1, healthchecks -2, healthchecks -3]\n2017/07/05 15:03:18.044 INFO ... State transition from RUNNING to PARTITIONS_REVOKED\n2017/07/05 15:03:18.044 INFO ... State transition from RUNNING to REBALANCING\n2017/07/05 15:03:18.044 INFO ... Setting newly assigned partitions [healthchecks-2, healthchecks -3]\n...\n```", "```java\nimport ...\npublic final class CustomStreamsProcessor {\n  private final String brokers;\n  public CustomStreamsProcessor(String brokers) {\n    super();\n    this.brokers = brokers;\n  }\n  public final void process() {\n    // below we will see the contents of this method\n  }\n  public static void main(String[] args) {\n    (new CustomStreamsProcessor(\"localhost:9092\")).process();\n  }\n}\n```", "```java\nStreamsBuilder streamsBuilder = new StreamsBuilder();\n```", "```java\nSerde customSerde = Serdes.serdeFrom(\n  new HealthCheckSerializer(), new HealthCheckDeserializer());\n```", "```java\nKStream healthCheckStream =\n  streamsBuilder.stream( Constants.getHealthChecksTopic(),\n    Consumed.with(Serdes.String(), customSerde));\n```", "```java\nKStream uptimeStream = healthCheckStream.map(((KeyValueMapper)(k, v)-> {\n  HealthCheck healthCheck = (HealthCheck) v;\n  LocalDate startDateLocal = healthCheck.getLastStartedAt().toInstant()\n               .atZone(ZoneId.systemDefault()).toLocalDate();\n  int uptime =\n      Period.between(startDateLocal, LocalDate.now()).getDays();\n  return new KeyValue<>(\n      healthCheck.getSerialNumber(), String.valueOf(uptime));\n}));\nuptimeStream.to( Constants.getUptimesTopic(),\n      Produced.with(Serdes.String(), Serdes.String()));\nTopology topology = streamsBuilder.build();\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", this.brokers);\nprops.put(\"application.id\", \"kioto\");\nKafkaStreams streams = new KafkaStreams(topology, props);\nstreams.start(); \n```", "```java\n$ gradle build\n```", "```java\nBUILD SUCCESSFUL in 1s\n6 actionable task: 6 up-to-date\n```", "```java\n$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n--topic uptimes --property print.key=true\n```", "```java\n EW05-HV36 33\n BO58-SB28 20\n DV03-ZT93 46\n ...\n```", "```java\ncompile 'io.confluent:kafka-streams-avro-serde:5.0.0'\n```", "```java\nimport ...\npublic final class AvroStreamsProcessor {\n  private final String brokers;\n  private final String schemaRegistryUrl;\n  public AvroStreamsProcessor(String brokers, String schemaRegistryUrl) {\n    super();\n    this.brokers = brokers;\n    this.schemaRegistryUrl = schemaRegistryUrl;\n  }\n  public final void process() {\n    // below we will see the contents of this method\n  }\n  public static void main(String[] args) {\n    (new AvroStreamsProcessor(\"localhost:9092\", \n        \"http://localhost:8081\")).process();\n  }\n}\n```", "```java\nStreamsBuilder streamsBuilder = new StreamsBuilder();\n```", "```java\nGenericAvroSerde avroSerde = new GenericAvroSerde();\n```", "```java\navroSerde.configure(\n  Collections.singletonMap(\"schema.registry.url\", schemaRegistryUrl), false);\n```", "```java\nKStream avroStream =\n  streamsBuilder.stream( Constants.getHealthChecksAvroTopic(),\n    Consumed.with(Serdes.String(), avroSerde));\n```", "```java\nKStream healthCheckStream = avroStream.mapValues((v -> {\n  GenericRecord healthCheckAvro = (GenericRecord) v;\n  HealthCheck healthCheck = new HealthCheck(\n    healthCheckAvro.get(\"event\").toString(),\n    healthCheckAvro.get(\"factory\").toString(),\n    healthCheckAvro.get(\"serialNumber\").toString(),\n    healthCheckAvro.get(\"type\").toString(),\n    healthCheckAvro.get(\"status\").toString(),\n    new Date((Long) healthCheckAvro.get(\"lastStartedAt\")),\n    Float.parseFloat(healthCheckAvro.get(\"temperature\").toString()),\n    healthCheckAvro.get(\"ipAddress\").toString());\n  return healthCheck;\n}));\n```", "```java\nKStream uptimeStream = healthCheckStream.map(((KeyValueMapper)(k, v)-> {\n  HealthCheck healthCheck = (HealthCheck) v;\n  LocalDate startDateLocal = healthCheck.getLastStartedAt().toInstant()\n               .atZone(ZoneId.systemDefault()).toLocalDate();\n  int uptime =\n     Period.between(startDateLocal, LocalDate.now()).getDays();\n  return new KeyValue<>(\n     healthCheck.getSerialNumber(), String.valueOf(uptime));\n}));\n\nuptimeStream.to( Constants.getUptimesTopic(),\n      Produced.with(Serdes.String(), Serdes.String()));\n\nTopology topology = streamsBuilder.build();\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", this.brokers);\nprops.put(\"application.id\", \"kioto\");\nKafkaStreams streams = new KafkaStreams(topology, props);\nstreams.start();\n```", "```java\n$ gradle build\n```", "```java\nBUILD SUCCESSFUL in 1s\n 6 actionable task: 6 up-to-date\n```", "```java\n $ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n      --topic uptimes --property print.key=true\n```", "```java\n EW05-HV36 33\n BO58-SB28 20\n DV03-ZT93 46\n ... \n```", "```java\npackage kioto.events;\nimport ...\npublic final class EventProducer {\n  private final Producer<String, String> producer;\n  private EventProducer(String brokers) {\n    Properties props = new Properties();\n    props.put(\"bootstrap.servers\", brokers);\n    props.put(\"key.serializer\", StringSerializer.class);\n    props.put(\"value.serializer\", StringSerializer.class);\n    producer = new KafkaProducer<>(props);\n  }\n  private void produce() {\n    // ...\n  }\n  private void sendMessage(long id, long ts, String info) {\n    // ...\n  }\n  public static void main(String[] args) {\n    (new EventProducer(\"localhost:9092\")).produce();\n  }\n}\n```", "```java\nprivate void produce() {\n  long now = System.currentTimeMillis();\n  long delay = 1300 - Math.floorMod(now, 1000);\n  Timer timer = new Timer();\n  timer.schedule(new TimerTask() {\n    public void run() {\n      long ts = System.currentTimeMillis();\n      long second = Math.floorMod(ts / 1000, 60);\n      if (second != 54) {\n        EventProducer.this.sendMessage(second, ts, \"on time\");\n      }\n      if (second == 6) {\n        EventProducer.this.sendMessage(54, ts - 12000, \"late\");\n      }\n    }\n  }, delay, 1000);\n}\n```", "```java\nprivate void sendMessage(long id, long ts, String info) {\n  long window = ts / 10000 * 10000;\n  String value = \"\" + window + ',' + id + ',' + info;\n  Future futureResult = this.producer.send(\n     new ProducerRecord<>(\n          \"events\", null, ts, String.valueOf(id), value));\n  try {\n    futureResult.get();\n  } catch (InterruptedException | ExecutionException e) {\n    // deal with the exception\n  }\n}\n```", "```java\n$. /bin/kafka-topics --zookeeper localhost:2181 --create --topic \nevents --replication-factor 1 --partitions 4\n```", "```java\n$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n--topic events\n```", "```java\n1532529060000,47, on time\n1532529060000,48, on time\n1532529060000,49, on time\n1532529070000,50, on time\n1532529070000,51, on time\n1532529070000,52, on time\n1532529070000,53, on time\n1532529070000,55, on time\n1532529070000,56, on time\n1532529070000,57, on time\n1532529070000,58, on time \n1532529070000,59, on time\n1532529080000,0, on time\n1532529080000,1, on time\n1532529080000,2, on time\n1532529080000,3, on time\n1532529080000,4, on time\n1532529080000,5, on time\n1532529080000,6, on time\n1532529070000,54, late\n1532529080000,7, on time\n...\n```", "```java\npackage kioto.events;\nimport ...\npublic final class EventProcessor {\n  private final String brokers;\n  private EventProcessor(String brokers) {\n    this.brokers = brokers;\n  }\n  private void process() {\n    // ...\n  }\n  public static void main(String[] args) {\n    (new EventProcessor(\"localhost:9092\")).process();\n  }\n}\n```", "```java\nStreamsBuilder streamsBuilder = new StreamsBuilder();\nKStream stream = streamsBuilder.stream(\n  \"events\", Consumed.with(Serdes.String(), Serdes.String()));\n```", "```java\nKTable aggregates = stream\n  .groupBy( (k, v) -> \"foo\", Serialized.with(Serdes.String(), Serdes.String()))\n  .windowedBy( TimeWindows.of(10000L) )\n  .count( Materialized.with( Serdes.String(), Serdes.Long() ) );\n```", "```java\nkey | value\n ----------------- |-------\n 1532529050000:foo | 10\n 1532529060000:foo | 10\n 1532529070000:foo | 9\n 1532529080000:foo | 3\n ...\n```", "```java\naggregates\n  .toStream()\n  .map( (ws, i) -> new KeyValue( \"\"+((Windowed)ws).window().start(), \"\"+i))\n  .to(\"aggregates\", Produced.with(Serdes.String(), Serdes.String()));\n```", "```java\nTopology topology = streamsBuilder.build();\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", this.brokers);\nprops.put(\"application.id\", \"kioto\");\nprops.put(\"auto.offset.reset\", \"latest\");\nprops.put(\"commit.interval.ms\", 30000);\nKafkaStreams streams = new KafkaStreams(topology, props);\nstreams.start();\n```", "```java\n$. /bin/kafka-topics --zookeeper localhost:2181 --create --topic \naggregates --replication-factor 1 --partitions 4\n```", "```java\n$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n--topic aggregates --property print.key=true\n```", "```java\n1532529050000 10\n1532529060000 10\n1532529070000 9\n1532529080000 3\n```", "```java\n1532529050000 10\n1532529060000 10\n1532529070000 10\n1532529080000 10\n1532529090000 10\n1532529100000 4\n```", "```java\nprops.put(\"commit.interval.ms\", 0);\n```", "```java\n1532529080000 6\n1532529080000 7\n1532529080000 8\n1532529080000 9\n1532529080000 10 <-- Window end\n1532529090000 1  <-- Window beginning\n1532529090000 2\n1532529090000 3\n1532529090000 5  <-- The 4th didn't arrive\n1532529090000 6\n1532529090000 7\n1532529090000 8\n1532529090000 9  <-- Window end\n1532529100000 1\n1532529100000 2\n1532529100000 3\n1532529100000 4\n1532529100000 5\n1532529100000 6\n1532529090000 10 <-- The 4th arrived, so the count value is updated\n1532529100000 7\n1532529100000 8\n...\n```"]