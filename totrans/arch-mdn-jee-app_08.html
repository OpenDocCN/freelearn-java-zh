<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Microservices and System Architecture</h1>
                </header>
            
            <article>
                
<p>The previous chapters covered how to develop a single enterprise application with Java EE. Modern applications contain infrastructure and configuration definitions as code, making it possible to create environments in automated ways, either on premises or in cloud platforms. Continuous Delivery pipelines together with sufficient, automated test cases make it possible to deliver enterprise applications with high quality and productivity. Modern zero-dependency Java EE approaches support these efforts.</p>
<p>Enterprise systems rarely come with single responsibilities that could be reasonably mapped into single enterprise applications. Traditionally, enterprise applications combined multiple aspects of the business into monolithic applications. The question is, whether this approach to crafting distributed systems is advisable.</p>
<p>This chapter will cover:</p>
<ul>
<li>The motivations behind distribution</li>
<li>Possibilities and challenges of distributed systems</li>
<li>How to design interdependent applications</li>
<li>Application boundaries, APIs, and documentation</li>
<li>Consistency, scalability, challenges, and solutions</li>
<li>Event sourcing, event-driven architectures, and CQRS</li>
<li>Microservice architectures</li>
<li>How Java EE fits the microservice world</li>
<li>How to realize resilient communication</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivations behind distributed systems</h1>
                </header>
            
            <article>
                
<p>One of the first questions should ask for the need for distribution. There are several technical motivations behind designing distributed systems.</p>
<p>Typical enterprise scenarios are in essence distributed. Users or other systems that are spread across locations need to communicate with a service. This needs to happen over the network.</p>
<p>Another reason is scalability. If a single application reaches the point where it cannot reliably serve the overall load of clients, the business logic needs to be distributed to multiple hosts.</p>
<p>A similar reasoning aims toward a system's fault tolerance. Single applications represent single points of failure; if the single application is unavailable, the service won't be usable by the clients. Distributing services to multiple locations increases availability and resilience.</p>
<p>There are also other less technology-driven motivations. An application represents certain business responsibilities. In Domain-Driven Design language they are contained in the application's <strong>bounded context</strong>. Bounded contexts include the business concerns, logic, and data of the application and differentiate it from external concerns.</p>
<p>In the same way as engineers cluster code responsibilities into packages and modules, it certainly makes sense to craft contexts on a system scale as well. Coherent business logic and functionality is grouped into separate services as part of separate applications. The data and schema is also part of a bounded context. It can therefore be encapsulated into several database instances, which are owned by the corresponding distributed applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges of distribution</h1>
                </header>
            
            <article>
                
<p>With all these motivations, especially technical ones such as scalability, why shouldn't engineers distribute everything then? Distribution comes with certain overheads.</p>
<p>In general, the overall overhead that comes on top of the system's distilled business logic will be multiplied by the number of applications involved. For example, a single, monolithic application requires a monitoring solution. Distributing this application will cause all resulting applications to be monitored as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Communication overhead</h1>
                </header>
            
            <article>
                
<p>In distribution, first of all, there is an overhead cost in communicating between systems.</p>
<p>Technology is very effective in communicating within a single process. There is effectively no overhead in calling functionality that is part of the application. As soon as inter-process or remote communication is required, engineers have to define interface abstractions. Communication protocols such as HTTP have to be defined and used in order to exchange information.</p>
<p>This requires certain time and effort. Communication between applications has to be defined, implemented, and maintained. Within a single application, the communication is reflected in method invocations.</p>
<p>The required communication also becomes a concern of the business use case. It can no longer be assumed that certain functionality or data can be just used without any overhead. Communicating with the distributed system becomes a responsibility of the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance overhead</h1>
                </header>
            
            <article>
                
<p>Distributing applications at first decreases performance of the overall systems. Computer networks are slower than communication within a single host. Therefore networking will always come with a certain performance overhead.</p>
<p>The overhead in performance is not only caused by the communication itself, but also the need to synchronize. Synchronization within a single process already consumes certain processing time, and this impact is much bigger when distribution is involved.</p>
<p>However, despite this overhead in performance, distribution eventually increases the overall performance of the system as its applications scale out. Scaling horizontally always comes with a certain performance overhead compared to a single instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Organizational overhead</h1>
                </header>
            
            <article>
                
<p>Distributed systems containing several applications certainly need more organizational effort than a single one.</p>
<p>Multiple applications imply multiple deployments that need to be managed. Deploying new versions may have an impact on dependent applications. Teams need to ensure that versions of deployed applications work together well. Single, monolithic applications are not affected by this since they are consistent within themselves.</p>
<p>Besides that, multiple applications are developed in several projects and repositories, usually by multiple development teams. In particular, having multiple teams requires communication, not necessarily technically, but human-related communication. In the same way as for deploying applications, responsibilities, system boundaries, and dependencies need to be agreed upon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to design systems landscapes</h1>
                </header>
            
            <article>
                
<p>With all of these challenges and overheads involved, a lot of scenarios still require distribution. It's important to mention, that there must be enough motivation behind distributing systems. Distribution comes with costs and risks. If it's not necessary to distribute, building monolithic applications is always to be preferred.</p>
<p>Now, let's look into how to design reasonable system landscapes, tailored for business requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Context maps and bounded contexts</h1>
                </header>
            
            <article>
                
<p><strong>Bounded contexts</strong> define the application's responsibilities in business logic, behavior, and data ownership. So-called <strong>context maps</strong>, as described in Domain-Driven Design, represent the entire system landscape. It shows the individual responsibilities, contexts, and belongings of its applications. Bounded contexts therefore fit within a context map to show how they exchange information among each other.</p>
<p>The following shows the context map of the <em>cars</em> domain, including two bounded contexts:</p>
<div class="mce-root CDPAlignLeft CDPAlign"><img src="assets/cab69c57-671a-4028-b544-a1a602df0f2b.png"/></div>
<p>It's advisable to consider the different responsibilities of the system before designing and carving out applications. Lack of clarity on an application's responsibilities usually emerges quickly as soon as the system's context map is recorded.</p>
<p>Context maps are not only helpful during the initial project definition, but also during revisiting and refining responsibilities once business functionality changes. In order to prevent the boundaries and belongings of distribution applications from drifting apart, it's advisable to reflect on them from time to time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Separation of concerns</h1>
                </header>
            
            <article>
                
<p>The application's responsibilities should be clearly defined and differentiated from other applications.</p>
<p>In the same way as with code level, the concerns of several applications should be separated. The single responsibility principle holds true here as well.</p>
<p>The application's concerns include all business concerns, application boundaries, and owned data. As the business logic evolves and changes over time, these concerns should be revisited from time to time. This may result in applications that split up or get merged into single ones. The responsibilities and concerns that emerge from the context map should be reflected in the system's applications.</p>
<p>Data and data ownership is an important aspect of distributed applications. The business processes, being part of the bounded context defines the data involved in the use cases. Owned data are a concern of the specific applications and are only shared via the defined boundaries. Use cases that require data that is under the responsibility of another, remote application need to retrieve the information by remotely invoking the corresponding use cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Teams</h1>
                </header>
            
            <article>
                
<p>Teams and organizational structure are other important aspects to consider when designing distributed systems, since, as of writing this book, software is developed by humans. Considering Conway's law, that the organization's communication structure will eventually leak into the constructed system, teams should be defined similarly to the applications in the system.</p>
<p>Or, in other words, it makes sense for a single application to be developed by only a single team. Depending on the responsibilities and sizes, a single team can potentially craft multiple applications.</p>
<p>Again, comparing to the project code structure, this is a similar approach as horizontal versus vertical module layering. Similar to business motivated module structures, teams are therefore organized vertically, representing the structure of the context map. For example, rather than having several expert teams on software architecture, development, or operations, there will be teams for <em>car manufacture</em>, <em>assembly line</em>, and <em>order management</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project life cycles</h1>
                </header>
            
            <article>
                
<p>With individual teams being involved in developing distributed systems, applications will have independent project life cycles. This includes the way teams operate, for example, how they organize their sprint cycles.</p>
<p>The deployment cycles and schedules also emerge from the project life cycle. For the overall system to stay consistent and functional, potential dependencies on deployments of other applications need to be defined. This does not only target the application's availability.</p>
<p>Deployed application versions need to be compatible. In order to ensure this, applications that are dependent need to be clearly represented in the context map. Teams will have to communicate when dependent services introduce changes.</p>
<p>Again, painting a clear context map containing the bounded contexts helps define the interdependent applications and their responsibilities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to design system interfaces</h1>
                </header>
            
            <article>
                
<p>After the responsibilities of the system landscape have been defined, the boundaries of dependent systems need to be specified.</p>
<p>In previous chapters, we have seen various communication protocols and how to implement them. Besides the actual implementation, the question is now: how to design the interfaces of applications? Which aspects need to be considered, especially in distributed systems?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">API considerations</h1>
                </header>
            
            <article>
                
<p>The applications within a system are carved out based on their business responsibilities.</p>
<p>Similarly, the application's API should represent that business logic as well. The exposed API represents the business use cases a certain application comprises. This implies that a business domain expert can, without any further technical knowledge, identify the exposed business use cases from an API.</p>
<p>The business use cases are ideally offered in clear, lean interfaces. Invoking a use case should not require more technically-motivated communication steps or details than being part of the business logic. For example, if the <em>create a car</em> use case could be invoked as a single operation, the API of the <em>car manufacture</em> application should not require multiple invocations providing technical details.</p>
<p>An API should abstract the business logic in a clear, lean interface.</p>
<p>The API should therefore be decoupled from the application's implementation. The interface implementation should be independent from the chosen technology. This also implies that a communication format is chosen that doesn't set many constraints on the used technology.</p>
<p>It therefore makes sense to prefer technology that sets on standard protocols such as HTTP. It's more likely that engineer have knowledge in commonly used protocols, as that are supported by various technologies and tools. Creating application interfaces in HTTP web services allows clients to be developed in every technology that supports HTTP.</p>
<p>Abstracting the business logic in clear, lean interfaces that use standard protocols also enables change in used implementations, technologies, and platforms. Java Enterprise applications that only expose HTTP services could replace their technology with other implementations, without requiring dependent clients to change.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interface management</h1>
                </header>
            
            <article>
                
<p>Application interfaces are often subject to change during the development process.</p>
<p>New business use cases are included and existing one refined. The question is, how are these changes reflected in the API?</p>
<p>It depends on the nature and environment of the enterprise application how stable the API needs to be. If the project team is both in charge of the service, all clients and their life cycles, the API can introduce arbitrary changes that are reflected in the clients at the same time. The case is the <span>same if</span> for some reason the life cycles of involved applications are identical.</p>
<p>Usually, life cycles of distributed systems aren't that tightly coupled. For any other client/server model, or applications that have different life cycles, the APIs must not break existing clients. This means that the APIs are fully backwards-compatible, not introducing breaking changes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Change-resilient APIs</h1>
                </header>
            
            <article>
                
<p>There are certain principles in designing interfaces that prevent unnecessary breaks. For example, introducing new, optional payload data should not break the contract. Technology should be resilient as far as it can continue to work if all necessary data is provided. This matches the idea of <em>being conservative in what you do and liberal in what you accept</em>.</p>
<p>Therefore adding new, optional functionality or data should be possible without breaking clients. But what if existing logic changes?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking the business logic</h1>
                </header>
            
            <article>
                
<p>The question to be asked here is what a breaking change in the API means for the business use case. Is the application's past behavior not valid anymore? Should the client have to stop working from now on?</p>
<p>This is equivalent to, for example, a vendor of a widely-used smartphone app that decides to break existing versions and to force the users to update the installations to its latest version. There is arguably no need in doing so for existing functionality to continue.</p>
<p>If for some reason the existing use cases can't be used <em>as is</em> anymore, some additional, compensating business logic should be considered.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hypermedia REST and versioning</h1>
                </header>
            
            <article>
                
<p>Hypermedia REST APIs can bring some relief with this issue. In particular, Hypermedia controls provide the ability to evolve the API by dynamically defining resource links and actions. The clients of the REST service will adapt to the changes in accessing the services and considerately ignore unknown functionality.</p>
<p>A quite often suggested possibility is to version the API. This means introducing different operations or resources, such as <kbd>/car-manufacture/v1/cars</kbd>, with the version as the identifying part of the API. Versioning APIs, however, contradicts the idea of clean interfaces. In particular, since REST APIs resources represent domain entities, introducing several <em>versions</em> of a car doesn't make sense in business terms. The car entity is identified by its URI. Changing the URI to reflect changes in the business functionality would imply a change to the car's identity.</p>
<p>Sometimes several, different representations, or versions, of the same domain entities are required, for example, JSON mappings containing different sets of properties. Via HTTP interface this is achievable via <strong>content negotiation</strong>, by defining content type parameters. For example, different JSON representations for the same car can be requested via content types such as <kbd>application/json;vnd.example.car+v2</kbd>, if supported by the corresponding service.</p>
<p>Managing interfaces is a relevant topic for distributed systems. It's advisable to carefully design APIs upfront, with backwards-compatibility in mind. Extra efforts, such as additional operations that prevent an API from breaking existing functionality, should be preferred over clean interfaces that disrupt clients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Documenting boundaries</h1>
                </header>
            
            <article>
                
<p>Application boundaries that define APIs to invoke the application's business logic need to be made public to its clients, for example, other applications within the system. The question is, what information needs to be documented?</p>
<p>The application's bounded context is part of the context map. Therefore, the domain responsibilities should be clear. The application fulfills certain business use cases within its context.</p>
<p>This domain information needs to be documented first. Clients should be aware of what the application offers. This includes the use cases as well as the exchanged information and data ownership.</p>
<p>The responsibility of the <em>car manufacture</em> application is to assemble cars due to provided, exact specifications. The status information of manufactured cars is owned by the application for the whole process of assembling, until the car reaches the end of the production line and is ready for delivery. The application can be polled to provide status updates about the creation process of a car.</p>
<p>The application's domain description should contain the information the clients require, be precise in responsibilities, but not too verbose, only exposing what clients <em>need to know</em>.</p>
<p>Besides the business domain, there are technical aspects that need to be documented. Client applications need to be programmed against a system's API. They require information about the communication protocols, as well as data formats.</p>
<p>We covered several communication protocols and how to implement them in the second chapter of this book. At the time of writing, one of the most used protocols is HTTP, together with JSON or XML content types. With the example of HTTP, what needs to be documented?</p>
<p>HTTP endpoints, especially those following the REST constraints, represent the domain entities as resources, locatable by URLs. The available URLs need to be documented first. Clients will connect against these URLs in order to perform some business use cases. For example, the <kbd>/car-manufacture/cars/&lt;car-id&gt;</kbd> URL will refer to a particular car specified by its identifier.</p>
<p>The content type with detailed mapping information needs to be documented as well. Clients need to be aware of the structure and properties within the used content type.</p>
<p>For example, a car specification that is provided in order to create a car contains an <em>identifier</em>, an <em>engine type,</em> and a <em>chassis color</em>. The JSON format will look as follows:</p>
<pre>{
    "identifier": "&lt;car-identifier&gt;",
    "engine-type": "&lt;engine-type&gt;",
    "color": "&lt;chassis-color&gt;"
}</pre>
<p>The types and available values need to be documented as well. They will point to the business domain knowledge, the semantics behind an engine type. This is important, that both the content types as well as the semantics of the information are documented.</p>
<p>In the case of HTTP there will be more aspects to be documented such as potentially required header information, status codes provided by the web service, and so on.</p>
<p>All this documentation certainly depends on the used technology and communication protocol. The business domain, however, should also be part of the documentation, providing as much context as required.</p>
<p>The application's API documentation is part of the software project. It needs to be shipped together with the application in a particular version.</p>
<p>In order to ensure that the documentation matches the application's version, it should be part of the project repository, residing under version control as well. Therefore, it's highly advisable to use text-based documentation formats instead of binary formats such as Word documents. Lightweight markup languages such as <strong>AsciiDoc</strong> or <strong>Markdown</strong> have proven themselves well in the past.</p>
<p>The benefit of maintaining the documentation directly in the project, next to the application's sources, is to ensure the creation of documentation versions that are consistent with the developed service. Engineers are able to perform both changes in one step. Doing so prevents the documentation and service version from diverging.</p>
<p>There is a lot of tool support in documenting application boundaries depending on the communication technology. For HTTP web services for example, the <strong>OpenAPI Specification</strong> together with <strong>Swagger</strong> as a documentation framework are widely used. Swagger outputs the API definition as browsable HTML, making it easy for developers to identify the offered resources together with their usages.</p>
<p>Using Hypermedia REST services, however, gets rids of the biggest necessity of service documentation. Providing the information of which resources are available in links removes the need for documenting URLs. In fact, the server gets back the control of how URLs are constructed. Clients only enter an entry point, for example <kbd>/car-manufacture/</kbd>, and follow the provided Hypermedia links based on their relations. The knowledge what a car URL consists of solely resides on the server side and is explicitly not documented.</p>
<p>This is especially true for Hypermedia controls, not only directing the client to resources, but providing information on how to consume it. The <em>car manufacture</em> service that tells a client how to perform the <kbd>create-car</kbd> action: A POST request to <kbd>/car-manufacture/cars</kbd> is needed, including a request body in JSON content type with properties <kbd>identifier</kbd>, <kbd>engine-type</kbd>, and <kbd>color</kbd>.</p>
<p>The client needs to know the semantics of all relations and action names as well as the properties and where they originate. This is certainly client logic. All information on how to consume the API becomes part of the API. Designing REST services then eliminates the need for a lot of documentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consistency versus scalability</h1>
                </header>
            
            <article>
                
<p>Certainly it's necessary for distributed system to communicate. Since computer networks cannot be considered as reliable, even not in company-internal networks, reliable communication is a necessity. Business use cases are required to communicate in a reliable way, in order to ensure correct behavior.</p>
<p>Earlier in this book, we introduced the so-called CAP theorem that claims that it's impossible for distributed data stores to guarantee at most two of the three specified constraints. Systems can effectively choose whether they want to guarantee consistency or horizontal scalability. This highly affects the communication in a distributed world.</p>
<p>In general, enterprise systems should be consistent in their use cases. Business logic should transform the overall system from one consistent state to another, different consistent state.</p>
<p>In distributed systems, an overall consistent state would imply that use cases that communicate to external concerns would have to ensure that the invoked external logic also adheres to consistency. This approach leads to distributed transactions. Use cases that are invoked on a system would execute in an <em>all-or-nothing</em> fashion, including all external systems. This implies a need for a lock on all involved, distributed functionality until every single distributed application successfully performed its responsibilities.</p>
<p>Naturally, this approach doesn't scale. The fact that the system is distributed requires this transaction orchestration to be performed over the potentially slow network. This introduces a bottleneck, which results in a locking situation, since involved applications have to block and wait a relatively large amount of time.</p>
<p>Generally speaking, synchronous, consistent communication is only advisable for applications that don't involve more than two applications at a time. Performance tests as well as production experience indicate whether a chosen communication scenario scales well enough for the given use case and environment.</p>
<p>Using asynchronous communication is motivated by scalability. Distributed systems that communicate asynchronously won't, by definition, be consistent at all times. Asynchronous communication can happen on a logical level, where synchronous calls only initiate business logic without awaiting a consistent result.</p>
<p>Let's have a look into the motivations and design behind asynchronous, eventually consistent communication in distributed applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event sourcing, event-driven architectures, and CQRS</h1>
                </header>
            
            <article>
                
<p>Traditionally, enterprise applications are built using a model approach that is based on the atomic <strong>Create Read Update Delete</strong> (<strong>CRUD</strong>).</p>
<p>The current state of the system, including the state of the domain entities, is reflected in a relational database. If a domain entity is updated, the new state of the entity including all of its properties is put into the database and the old state is gone.</p>
<p>The CRUD approach requires applications to maintain consistency. In order to ensure the state of the domain entity is reflected correctly, all use case invocations have to be executed in a consistent manner, synchronizing modifications to the entities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shortcomings of CRUD-based systems</h1>
                </header>
            
            <article>
                
<p>This synchronization is also one of the shortcomings of CRUD-based systems, the way that we typically build applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scalability</h1>
                </header>
            
            <article>
                
<p>The required synchronization prevents the system from scaling infinitely. All transactions are executed on the relational database instance, which eventually introduces a bottleneck if the system needs to scale out.</p>
<p>This ultimately becomes a challenge for situations with huge amounts of workloads or huge numbers of users. For the vast majority of enterprise applications, however, the scalability of relational databases is sufficient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Competing transactions</h1>
                </header>
            
            <article>
                
<p>Another challenge that comes with CRUD-based models is to handle competing transactions. Business use cases that include the same domain entities and operate simultaneously need to ensure that the resulting state of the entities is consistent.</p>
<p>Editing a user's name and at the same time updating its account credit limit should not result in lost updates. The implementation has to ensure that the overall result of both transactions is still consistent.</p>
<p>Competing transactions that rely on optimistic locking usually result in failing transactions. This is definitely not ideal from a user's perspective, but at least maintains consistency, rather than suppressing that a transaction has been lost in space.</p>
<p>Following this approach, however, potentially leads to unnecessary locking. From a business theory perspective it should be possible to simultaneously edit the user's name and account credit limit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reproducibility</h1>
                </header>
            
            <article>
                
<p>Since the application only stores its current state, all historical information about previous states is gone. The state is always overwritten by the new updates.</p>
<p>This makes it hard to reproduce how an application got into its current state. If a current state was miscalculated from its originating use case invocations, there is no possibility of fixing the situation later on.</p>
<p>Some scenarios explicitly require reproducibility for legal terms. Some applications therefore include audit logs that permanently write certain information as soon as they happen to the system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event sourcing</h1>
                </header>
            
            <article>
                
<p>Event sourcing is an approach that tackles reproducibility as a shortcoming of CRUD-based systems.</p>
<p>Event sourced systems calculate the current state of the systems from atomic events that happened in the past. The events represent the individual business use case invocations, including the information provided in the invocations.</p>
<p>The current state is not permanently persisted, but emerges by applying all events one after another. The events themselves happened in the past and are immutable.</p>
<p>To give an example, a user with its characteristics is calculated from all events related to it. Applying <kbd>UserCreated</kbd>, <kbd>UserApproved</kbd>, and <kbd>UserNameChanged</kbd> one after another creates the current representation of the user up to its recent event:</p>
<div class="CDPAlignCenter CDPAlign"><img height="260" width="335" src="assets/998e273c-5a6c-4907-9708-2acb5c58a123.png"/></div>
<p>The events contain self-sufficient information mostly concerning the corresponding use case. For example, a <kbd>UserNameChanged</kbd> event contains the time stamp and the name the user was changed to, not other, unrelated information about the user. The event's information is therefore atomic.</p>
<p>Events are never changed nor deleted. If a domain entity is removed from the application, there will be a corresponding deletion event such as <kbd>UserDeleted</kbd>. The current state of the system then won't contain this user anymore after applying all events.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits</h1>
                </header>
            
            <article>
                
<p>An event-sourced application contains all of its information in atomic events. Therefore, the full history and context, how it got into its current state, is available. In order to reproduce the current state for debugging purposes, all events and their individual modifications to the system can be regarded.</p>
<p>The fact that everything that happened to the system is stored atomically has a couple of benefits, not only for debugging purposes.</p>
<p>Tests can make use of this information to replay everything that happened to a production system in system tests. Tests are then able to re-execute the exact business use case invocations that happened in productions. This is an advantage especially for system and performance tests.</p>
<p>The same is true for statistics that use the atomic information to gather insights about the usage of the application. This enables use cases and insights that are designed after an application has been deployed.</p>
<p>Assuming a manager wants to know how many users were created on a Monday, after the application has been running for two years. With CRUD-based systems that information would have had to explicitly been persisted by the time the use case was invoked. Use cases that were not explicitly requested in the past can only be added as new features, and will add value in the future.</p>
<p>With event sourcing these functionalities are possible. Since information about whatever happened to the system is stored, use cases that are developed in the future are able to operate on data that happened in the past.</p>
<p>These benefits, however, are certainly possible without the need for distributed systems. A monolithic, independent application can base its model on event sourcing, gaining the same benefits from it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Eventually consistent real world</h1>
                </header>
            
            <article>
                
<p>Before we go further into distributed systems in regard to consistency and scalability, let's look at an example of how consistent the real world is. Enterprise applications are typically built with the aspiration to provide full consistency. The real world, however, is highly distributed and not consistent at all.</p>
<p>Imagine you're hungry and you want to eat a burger. So you go to a restaurant, sit at a table, and tell the waiter that you would like to have a burger. The waiter will accept your order. Now, although your order has been accepted this doesn't necessarily mean that you will receive your meal. The process of ordering a meal is not fully consistent.</p>
<p>A lot of things can go wrong at this point. For example, the chef may tell the waiter that unfortunately the last burger patty was just used and there won't be more burgers for the day. So although your order has transactionally been accepted, the waiter will come back and tell you that the order won't be possible.</p>
<p>Now, instead of asking you to leave, the waiter might suggest to you an alternative dish. And if you're hungry and fine with the substitute you might eventually receive a meal.</p>
<p>This is how the highly distributed real world handles business use case transactions.</p>
<p>If the restaurant would be modeled in a fully consistent way the scenario would look different. In order to guarantee that an order is only accepted if it will be possible to provide the prepared meal, the whole restaurant would need to be locked down. The customers would have to wait and hold the conversation while the waiter goes into the kitchen and orders the meal from the chef. Since many other things can go wrong after ordering, the whole order transaction would actually have to block until the meal is fully prepared.</p>
<p>Obviously, this approach would not work. Instead, the real world is all about collaboration, intentions, and eventually dealing with issues if the intentions can't be fulfilled.</p>
<p>This means that the real world operates in an eventually consistent way. Eventually, the restaurant system will be in a consistent state, but not necessarily at all times, which leads to initially accepting orders that are actually not possible.</p>
<p>Real-world processes are represented as intentions or <strong>commands</strong>, such as ordering a burger, and atomic outcomes or <strong>events</strong>, such as that the order has been accepted. Events will then cause new commands that result in new outcomes or failures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event-driven architectures</h1>
                </header>
            
            <article>
                
<p>Now back to the topic of distributed systems. In the same way as for a restaurant, distributed systems that communicate in a consistent way, via distributed transactions, won't be able to scale.</p>
<p>Event-driven architectures solve this issue. The communication in these architectures happens via asynchronous events that are published and consumed reliably.</p>
<p>By doing so, consistent use case transactions get split up into multiple, smaller-scaled transactions that are consistent in themselves. This leads the overall use case eventually being consistent.</p>
<p>Let's see an example of how the use case of ordering a burger is represented in an event-driven architecture. The restaurant system consists of at least two distributed applications, the <em>waiter</em> and the <em>chef</em>. The restaurant applications communicate by listening to each other's events. The client application will communicate with the waiter in order to initiate the use case:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0479e749-f675-490e-875d-cd1964440029.png"/></div>
<p>The client orders a meal at the waiter application, which results in the <kbd>OrderPlaced</kbd> event. Once the event has been published reliably, the <kbd>orderMeal()</kbd> method's invocation returns. The client therefore is able to perform other work in parallel.</p>
<p>The chef system receives the <kbd>OrderPlaced</kbd> event and validates whether the order is possible with the currently available ingredients. If the order wouldn't be possible, the chef would emit a different event, such as <kbd>OrderFailedInsufficientIngredients</kbd>. In that case, the waiter would update the order status to failed.</p>
<p>When initiating the meal preparation was successful, the waiter receives the <kbd>MealPreparationStarted</kbd> event and updates the status of the order, what results in <kbd>OrderStarted</kbd>. If the client would ask the waiter about the status of their order, it could respond appropriately.</p>
<p>At some point the meal preparation would have been finished, resulting in a <kbd>MealPrepared</kbd> event, which notifies the waiter to deliver the order.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Eventual consistency in event-driven architectures</h1>
                </header>
            
            <article>
                
<p>The use case of ordering a meal is eventually consistent. Publishing the events reliably still ensures that all clients <em>eventually</em> know about the status of their order.</p>
<p>It is somewhat fine if processing the order doesn't happen immediately or if the order will fail for some reason. However, it must not happen that an order gets lost in the system due to unavailable applications. This needs to be ensured when publishing the events.</p>
<p>There are still transactions involved here, but on a much smaller scale and not involving external systems. Doing so enables distributed systems to cover transactional use cases while still enabling horizontal scalability.</p>
<p>The fact that some reliability is required for approaches like event-driven architectures is an important aspect in distributed systems, and should be considered when designing solutions and choosing technology.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Enter CQRS</h1>
                </header>
            
            <article>
                
<p>Now let's combine the motivations behind event-driven architectures and event sourcing.</p>
<p>Event-driven architectures communicate by atomic events. It makes sense to piggyback on this approach and build the system using event sourcing, by using the events as the system's source of truth. Doing so combines the benefits of both approaches, enabling horizontally scalable, event-sourced systems.</p>
<p>The question is how to model event-driven applications that base their domain model on events? And how to efficiently calculate and return the current state of domain entities?</p>
<p>The <strong>Command Query Responsibility Segregation</strong> (<strong>CQRS</strong>) principle describes how to model these applications. It is a consequence of event-driven architectures and is based on event sourcing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principles</h1>
                </header>
            
            <article>
                
<p>As the name suggests, CQRS separates the responsibilities for commands and queries, namely writes and reads.</p>
<p>A command changes the state of the system by ultimately producing events. It is not allowed to return any data. Either the command succeeds, which results in zero or more events, or it fails with an error. The events are produced reliably.</p>
<p>A query retrieves and returns data, without side effects on the system. It is not allowed to modify state.</p>
<p>To give an example in Java code, a command acts like a <kbd>void doSomething()</kbd> method, which changes state. A query acts like a getter <kbd>String getSomething()</kbd>, which has no impact on the system's state. These principles sound simple, but have some implications on the system's architecture.</p>
<p>The responsibilities of the commands and queries are separated into several concerns, allowing CQRS applications to emerge in fully independent applications that either write or read. Now, how to design and implement this approach?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Design</h1>
                </header>
            
            <article>
                
<p>Following event-driven architectures, the write and read systems communicate solely via events. The events are distributed via an event store or event hub. There is no other coupling than the write systems that produce events and both write and read systems that consume for events to update their internal state.</p>
<p>The following snippet shows the architecture of a CQRS system:</p>
<div class="CDPAlignCenter CDPAlign"><img height="350" width="796" src="assets/32876ade-c2d7-4a57-b69f-40b88a6f40be.png"/></div>
<p>The command and query services consume events from the event store. This is the only way for communication between them.</p>
<p>All services maintain a current-state representation that reflects the state of the domain entities. Entities are, for example, <em>meal orders</em> or <em>cars</em>, including the latest state of their properties. This state is kept in memory or persisted in databases.</p>
<p>These representations just enable the systems to contain a current state. The golden source of truth is the atomic events contained in the event store.</p>
<p>All application instances individually update their state representations by consuming and applying the events from the event store.</p>
<p>The command services contain the business logic that initiates changes to the systems. They produce events via the event store after potential command verification using their state representations.</p>
<p>In order to make the flow of information clear, let's go through an example meal order:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fb0fbe4e-8d2c-4763-972a-bba3f416d84d.png"/></div>
<p>The client orders the meal at a command service instance. After a potential verification against its representation, the command service produces the <kbd>OrderPlaced</kbd> event to the event store. If publishing the event was successful, the <kbd>orderMeal()</kbd> method returns. The client can proceed with its execution.</p>
<p>The command service can create a meal identifier for later retrieval, for example, as a universally unique identifier:</p>
<div class="CDPAlignCenter CDPAlign"><img height="423" width="718" src="assets/93abf0cf-c901-4837-a8da-fd5b74122dcc.png"/></div>
<p>The event store publishes the event to all consumers, which updates their internal representation accordingly. The client can access the status of the meal at the query service using its identifier. The query service will respond with its latest representation of the order.</p>
<p>In order to proceed with the order processing, an authority that invokes potential subsequent commands will handle the event as well:</p>
<div class="CDPAlignCenter CDPAlign"><img height="392" width="426" src="assets/ba948444-0114-4543-8feb-8236d7179c48.png"/></div>
<p>An event handler will listen to the <kbd>OrderPlaced</kbd> event and invoke the <kbd>prepareMeal()</kbd> use case of the chef system. This subsequent command will then potentially result in new events.</p>
<p>The section <em>Implementing microservices with Java EE</em>, covers how to implement CQRS among other things.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits</h1>
                </header>
            
            <article>
                
<p>CQRS enables distributed applications to not only scale horizontally, but independently in their write and read concerns. The replicas of query service, for example, can be different from the number of command services.</p>
<p>The read and write load in enterprise applications is usually not evenly distributed. Typically the read operations highly outperform the number of writes. For these cases the number of read instances can be scaled out independently. This would not be possible in a CRUD-based system.</p>
<p>Another benefit is that each service can optimize their state representations accordingly. For example, persistently storing the domain entities in a relational database might not be the best approach for every situation. It's also possible to just store the representation in memory and to recalculate all events at application startup. The point is that both the write and read instances are free to choose and optimize their representations according to the circumstances.</p>
<p>A side effect of this approach is also that CQRS provides read-side failover availability. In case of the event store being unavailable no new events can be published, therefore no use cases that modify state can be invoked on the system. In CRUD-based systems this would correspond to the database being down. In CQRS systems, however, at least the query services can still provide the latest state from their representations.</p>
<p>The state representations of CQRS systems also solve the scalability issue of event-sourced systems. Event-sourced systems calculate the current application state from the atomic events. Executing this each and every time an operation is invoked will over time become slower and slower as more events arrive. The representations of the command and query services eliminate this need by continuously applying the recent events.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shortcomings</h1>
                </header>
            
            <article>
                
<p>Building CQRS systems not only has benefits, it also has shortcomings.</p>
<p>Probably one of the biggest shortcoming of constructing these systems is that the majority of developers are not familiar with the concept, design and implementations. This will introduce difficulties when this approach is chosen in enterprise projects. Unlike CRUD-based systems, CQRS would require additional training and know-how.</p>
<p>Like any distributed system, there are naturally more applications involved in a CQRS system compared to the CRUD approach. As previously described for distributed systems in general, this requires some extra effort. Additionally, an event store is required.</p>
<p>Unlike the figures demonstrated, it is not mandatory to have the command and query sides in two or more independent applications. As long as the functionalities only communicate via events published by the event store, both can reside within the same application. This would, for example, result in a single waiter and chef application that still scales out horizontally. This is a reasonable trade-off, if individually scaling the write and read sides is not required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Communication</h1>
                </header>
            
            <article>
                
<p>Building CQRS systems is one approach to realizing asynchronous, eventually consistent communication. As we have seen previously in this book, there are many forms of communication, synchronous as well as asynchronous.</p>
<p>In order to enable scalable applications, distributed systems should not rely on synchronous communication that involves several systems. This leads to distributed transactions.</p>
<p>One approach to realize scalability with technology-agnostic, synchronous communication protocols is to model logically asynchronous processes. For example, communication protocols such as HTTP can be used to trigger processing that happens asynchronously while the caller immediately returns. This introduces eventual consistency, but enables the system to scale.</p>
<p>This also involves the consideration of whether the applications that made the distributed system make a difference in system-internal, and external communication. CQRS uses this approach by offering external interfaces, for example, using HTTP, to the clients, whereas the services themselves communicate via the event store. Modeling asynchronous processes that are accessed via uniform protocol doesn't distinguish here.</p>
<p>In general, it's advisable to prefer availability, that is, scalability, over consistency when designing distributed systems. There are many approaches possible, CQRS is one of them, combining asynchronous communication with event sourcing.</p>
<p>The following section covers the necessity of self-sufficient applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microservice architectures</h1>
                </header>
            
            <article>
                
<p>We saw the motivations, challenges, and benefits of distributed systems, as well as some approaches to handle communication and consistency. Now we will focus on the architecture of distributed applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sharing data and technology in enterprises</h1>
                </header>
            
            <article>
                
<p>A common idea in enterprises is to share and reuse technology as well as commonly used data. Earlier we looked at sharing Java modules and the shortcomings with that. What about sharing common technology or data models in distributed systems?</p>
<p>Multiple applications that form an enterprise system are often implemented using similar technology. This comes naturally with applications that are built by a single team or teams that work closely together. Doing so very often raises the idea of sharing technology that is being reused in the applications.</p>
<p>Projects could use commonly used modules that remove duplication in the overall system. A typical approach for this is shared models. There could be only one module within the organization that is being reused in all projects.</p>
<p>Sharing models leads to the question whether potentially persisted domain entities or transfer objects are being reused. Domain entities that are persisted in a database could then even be directly retrieved from the database system, right?</p>
<p>Commonly used databases stand in total contradiction to distributed systems. They tightly couple the involved applications. Changes in schemas or technology welds the application and project life cycles together. Commonly used database instances prevent applications from being able to scale. This eliminates the motivations behind distributed systems.</p>
<p>The same is true for sharing technology in general. As shown in previous chapters, commonly used modules and dependencies introduce technical constraints in the implementations. They couple the applications and limit their independence in changes and life cycles. Teams will have to communicate and discuss modifications, even if they would not affect the application's boundaries.</p>
<p>Looking at the domain knowledge and the responsibilities in the context map of the system, sharing data and technology makes little sense. There are indeed points of contact between the systems that are subject to be shared in technology.</p>
<p>However, the point is to implement applications, which only depend on their business responsibilities on the one side and documented communication protocols on the other side. It's therefore advisable to choose potential duplication and independence rather than coupling in technology.</p>
<p>Sharing other concerns rather than points of contact in the system's context map should alert engineers. The application's different responsibilities should make it clear that commonly used models or data reside in different contexts. The individual applications are exclusively responsible for their concerns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shared-nothing architectures</h1>
                </header>
            
            <article>
                
<p>With these thoughts in mind it's advisable to craft applications that share no common technology or data. They fulfill the application boundary contract in communication and business responsibilities.</p>
<p><strong>Shared-nothing architectures</strong> are independent in technology, potentially used libraries, their data and schemas thereof. They are free to choose implementations and potential persistence technology.</p>
<p>Changing the implementation of an application within a distributed system from Python to Java should have no impact on the other applications, if the contract of its HTTP interface is still met.</p>
<p>If data is required within other applications, this needs to be defined explicitly in the context map, requiring the application to expose data via its business logic interfaces. Databases are not shared.</p>
<p>Shared-nothing architectures enable applications with independent life cycles that depend on nothing more than the explicitly defined contracts. Teams are free to choose technology and the project life cycles. The technology, as well as the data including databases, is owned by the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interdependent systems</h1>
                </header>
            
            <article>
                
<p>Shared-nothing architectures eventually have to collaborate with other applications. The defined contracts have to be met, documented, and communicated.</p>
<p>This is the point, that shared-nothing architectures are only dependent on the defined contracts and responsibilities. In case of changes in the business logic the contracts are redefined and communicated. Solely the application's team is responsible for how to implement the contracts.</p>
<p>Interdependent systems are made up of several shared-nothing applications with well-defined interfaces. The used interfaces should be technology-agnostic to not set constraints on the used implementation.</p>
<p>This is the idea behind microservice architectures. Microservices consist of several interdependent applications that realize their individual business responsibilities and, combined together, solve a problem.</p>
<p>The name microservice doesn't necessarily say anything about the size of the application. An application should be built by a single team of developers. For organizational reasons team sizes should not grow too big. There is an often-cited notion by Amazon that the whole team should be able to survive on two pizzas.</p>
<p>The motivations behind distributed systems should be considered before crafting microservices. If there is no actual need to distribute a system, it should be avoided. Sticking to monolithic applications with reasonable responsibilities is to be preferred.</p>
<p>Usually the approach to craft microservice architectures is to slice up monolithic applications that grow too large in responsibilities, or diverged in teams and life cycles. This is comparable with refactoring approaches. Refactoring a class that grew too big into multiple delegates works well more often than trying to introduce a perfect scenario from the beginning.</p>
<p>In general, it's always advisable to consider the business requirements, context map of the system with their development teams and life cycles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">12-factor and cloud native applications</h1>
                </header>
            
            <article>
                
<p><a href="a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml">Chapter 5</a>, <em>Container and Cloud Environments with Java EE</em><span>,</span> introduced the approaches of 12-factor and cloud native applications. They heavily support microservice architectures.</p>
<p>In particular, the shared-nothing approach of having interdependent, distributed applications is well realizable with the principles of containerized, stateless, and scalable enterprise applications.</p>
<p>The 12-factor principles and the effective nature of cloud and container environments support teams in developing microservices with manageable overhead and high productivity.</p>
<p>However, an enterprise system doesn't not have to be distributed in order to comply with the 12-factor or cloud native principles. The approaches are certainly advisable for building monolithic applications as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When to use and when not to use microservices</h1>
                </header>
            
            <article>
                
<p>In the recent years microservice architectures have seen some hype in the software industry.</p>
<p>As always with hypes, engineers should ask themselves what is behind certain buzzwords and whether implementing them makes sense. It's always advisable to look into new technology and methodologies. It's not necessarily advisable to apply them immediately.</p>
<p>The reasons for using microservices are the same as for using distributed systems in general. There are technical reasons, such as applications that need independent deployment life cycles.</p>
<p>There are also reasons that are driven by the business requirements and situations in teams and project working modes.</p>
<p>Scalability is an often-cited motivation behind microservice architectures. As we have seen in event-driven architectures, monolithic applications aren't able so scale infinitely. The question is whether scalability is effectively an issue.</p>
<p>There are big companies that handle business logic for huge amounts of users using monolithic applications. Before considering distribution as a relief for scaling issues, performance insights and statistics should be gathered.</p>
<p>Engineers should avoid to use microservice architectures solely because of believing in a <em>silver bullet</em> approach. It can easily happen as a result of <em>buzzword-driven</em> meetings and conversations, that solutions are chosen based on limited or no evidence supporting the requirement. Microservices certainly provide benefits, but also come with a price in time and effort. In any way, the requirements and motivations whether to split up responsibilities into multiple applications should be clear.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing microservices with Java EE</h1>
                </header>
            
            <article>
                
<p>Now on to the question of how to build microservices with Enterprise Java.</p>
<p>In various discussions and meetings, Java EE has been considered as <em>too heavyweight</em> and cumbersome for microservices. Whereas this is certainly the case for J2EE technology and approaches, Java EE offers modern, lean ways of developing enterprise applications. <a href="f0a49441-e411-49c4-a4b6-c6193ba36094.xhtml">Chapter 4</a>, <em>Lightweight Java EE</em> and <a href="a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml">Chapter 5</a>, <em>Container and Cloud Environments with Java EE</em> covered these aspects, especially in regard to modern environments.</p>
<p>Java EE is indeed well suited for writing microservice applications. Container technologies and orchestration support the platform, particularly since Java EE separates the business logic from the implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Zero-dependency applications</h1>
                </header>
            
            <article>
                
<p>Microservices with Java EE are ideally built as zero-dependency applications.</p>
<p>Thin WAR applications are deployed on modern enterprise containers that can be shipped in containers. This minimizes deployment time. Java EE deployment artifacts should only contain provided dependencies, if there is a reasonable need for adding third-party dependencies, they should be installed in the application server. Container technologies simplify this approach.</p>
<p>This also matches the idea of shared-nothing architectures. The team is responsible for the application-specific technology, in this case the application server installation including libraries. Infrastructure as code definitions such as Dockerfiles, enable the development team to accomplish this in effective ways.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application servers</h1>
                </header>
            
            <article>
                
<p>Following this approach, the application server is shipped in a container, containing only a single application. The <em>one application per application server</em> approach also matches the idea of shared-nothing architectures.</p>
<p>The question is whether application servers introduce too much overhead if a single server instance only contains a single application. In the past, the storage and memory footprint certainly was significant.</p>
<p>Modern application servers considerably improved in this area. There are container base images of servers such as <strong>TomEE</strong> that consume 150 MB and less, for the server including the Java runtime and operating system, mind you. The memory consumption also significantly improved due to dynamically loading functionality.</p>
<p>In enterprise projects installation sizes are usually not an issue to be concerned with, especially if they're not exceeding all bounds. What's much more important is the size of the built and shipped artifacts. The application artifact, which in some technologies contains megabytes of dependencies, is built and transmitted many times. The runtime is only installed once per environment.</p>
<p>Container technologies such as Docker make use of layered file systems that encourage the moving parts to be small. Zero-dependency applications support this approach.</p>
<p>Making each and every Continuous Delivery build only shipping kilobytes of data is far more advisable than saving a few megabytes in the base installation.</p>
<p>If the installation size still needs to be shrunk down, some application vendors offer possibilities to tailor the container to the required standards, especially the MicroProfile initiative, which includes several application server vendors, and defines slimmed profiles.</p>
<p>Java EE microservices don't need to be shipped as standalone JAR files. On the contrast, applications shipped in containers should leverage the use of layered file systems and be deployed on enterprise containers residing in the base image. Standalone JAR files oppose this principle.</p>
<p>There are possibilities to combine standalone JAR files with thin deployments, by so-called hollow JAR. This approach, however, is not required when using containers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing application boundaries</h1>
                </header>
            
            <article>
                
<p>Let's move on to the implementation of the application boundaries with Java EE. This is, in fact, a more system-architectural question than an implementational one.</p>
<p>Communication between microservices should use technology-agnostic protocols. As seen previously, Java EE heavily supports HTTP, for both HTTP and REST services use Hypermedia.</p>
<p>The next sub-chapter will cover asynchronous communication in CQRS systems, using publish/subscribe messaging implemented with Apache Kafka.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing CQRS</h1>
                </header>
            
            <article>
                
<p>Earlier in this chapter we have seen the motivations and concepts behind event sourcing, event-driven architectures, and CQRS. CQRS offers an interesting approach to creating distributed applications that implement scalable, eventually consistent business use cases.</p>
<p>At the time of writing, there is a lot of interest in CQRS, yet little knowledge within companies of how to use it. Some frameworks and technologies have emerged that aim to implement this approach. Yet CQRS is an architectural style, and specific frameworks are not necessary to develop CQRS systems.</p>
<p>Let's have a close look at an approach that uses Java EE.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System interfaces</h1>
                </header>
            
            <article>
                
<p>The CQRS system interfaces are used from outside the system to initiate business use cases. For example, a client accesses the waiter system to order a burger.</p>
<p>These interfaces are used externally and ideally implemented using a technology-agnostic protocol.</p>
<p>For REST-like HTTP services, this implies that the command services implement HTTP methods that modify resources, such as POST, DELETE, PATCH, or PUT. The query services usually only implement resources queried by GET.</p>
<p>In our example, this means that the client POSTs a new meal order to a command service resource. Similarly, meal orders are retrieved via GET resources from query services.</p>
<p>These HTTP interfaces concern the external communication. Internally the application communicates via events that are published using an event hub.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example scenario using Apache Kafka</h1>
                </header>
            
            <article>
                
<p>In this example, I will use Apache Kafka as a distributed message broker, offering high performance and throughput. It's one example of a messaging technology, supporting a publish/subscribe approach, among others.</p>
<p>At the time of writing, Apache Kafka doesn't implement all JMS semantics. The following examples will use the Kafka's vendor-specific Client API.</p>
<p>Apache Kafka's publish/subscribe approach organizes messages in topics. It can be configured to enable transactional event producers and in-order event consumption, which is what event-driven architectures need to ensure in order to create reliable use cases.</p>
<p>Kafka brokers are distributed and use so-called consumer groups to manage message topics and partitions. Examining Kafka's architecture is beyond the scope of this book and it's advised to go further into its documentation when choosing this technology.</p>
<p>In short, a message is published to a topic and consumed once per consumer group. Consumer groups contain one or more consumers and guarantee that exactly one consumer will process the messages that have been published using transactional producers.</p>
<p>A CQRS system needs to consume messages in multiple locations. The applications that are interested in a specific topic will consume the message and update their internal representations. Therefore, all these updating consumers will receive an event. There are also event handlers who use the event to process the business logic further. Exactly one event handler needs to process the event per topic, otherwise processes would run multiple times or not at all.</p>
<p>The concept of Kafka consumer groups is therefore used in such a way, where there is one update consumer group per application and one event handler group per topic. This enables all instances to receive the events, but reliably one command service to process the business logic. By doing so, the instances are able to scale without affecting the overall system's outcome:</p>
<div class="CDPAlignCenter CDPAlign"><img height="401" width="747" src="assets/d8c5b4a8-2c08-49e1-b66b-52bc48f16cb4.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating Java EE</h1>
                </header>
            
            <article>
                
<p>In order to integrate the Apache Kafka cluster into the application this example will use Kafka's Java API.</p>
<p>The applications connect to Kafka to consume messages in their updating consumers and event handlers. The same is true for publishing events.</p>
<p>The used technology should be encapsulated from the rest of the application. In order to integrate the events, developers can use a functionality that naturally fits this scenario: CDI events.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CDI events</h1>
                </header>
            
            <article>
                
<p>The domain events contain the event specific data, a timestamp, and identifiers that reference the domain entity.</p>
<p>The following snippet shows an example of an abstract <kbd>MealEvent</kbd> and the <kbd>OrderPlaced</kbd> event:</p>
<pre>public abstract class <strong>MealEvent</strong> {

    <strong>private final Instant instant;</strong>

    protected MealEvent() {
        instant = Instant.now();
    }

    protected MealEvent(Instant instant) {
        Objects.requireNonNull(instant);
        this.instant = instant;
    }

    ...
}

public class <strong>OrderPlaced</strong> extends <strong>MealEvent</strong> {

    <strong>private final OrderInfo orderInfo;</strong>

    public OrderPlaced(OrderInfo orderInfo) {
        this.orderInfo = orderInfo;
    }

    public OrderPlaced(OrderInfo orderInfo, Instant instant) {
        super(instant);
        this.orderInfo = orderInfo;
    }

    ...
}</pre>
<p>Domain events like these are the core of the application. The domain entity representations are calculated from these events.</p>
<p>The integration into Kafka ensures that these events are fired via CDI. They are observed in the corresponding functionality that updates the state representations, or invokes subsequent commands, respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event handlers</h1>
                </header>
            
            <article>
                
<p>The following snippet shows an event handler of the chef system, invoking functionality of a command service:</p>
<pre>@Singleton
public class OrderEventHandler {

    @Inject
    MealPreparationService mealService;

    public void handle(<strong>@Observes OrderPlaced event</strong>) {
        <strong>mealService.prepareMeal(event.getOrderInfo());</strong>
    }
}</pre>
<p>The event handler consumes the event and will invoke the boundary of the subsequent meal preparation use case. The <kbd>prepareMeal()</kbd> method itself will result in zero or more events, in this case either <kbd>MealPreparationStarted</kbd> or <kbd>OrderFailedInsufficientIngredients</kbd>:</p>
<pre>public class MealPreparationService {

    <strong>@Inject
    EventProducer eventProducer;</strong>

    @Inject
    IngredientStore ingredientStore;

    public void prepareMeal(OrderInfo orderInfo) {

        // use ingredientStore to check availability

        if (...)
            <strong>eventProducer.publish(new OrderFailedInsufficientIngredients());</strong>
        else
            <strong>eventProducer.publish(new MealPreparationStarted(orderInfo));</strong>
    }
}</pre>
<p>The event producer will reliably publish the events to the Kafka cluster. If the publication fails, the whole event processing has to fail, and will be retried later.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">State representation</h1>
                </header>
            
            <article>
                
<p>The consumers that update the state representation consume the CDI events as well. The following snippet shows the bean that contains the meal order state representations:</p>
<pre>@Stateless
public class MealOrders {

    @PersistenceContext
    EntityManager entityManager;

    public <strong>MealOrder get(UUID orderId)</strong> {
        return entityManager.find(MealOrder.class, orderId.toString());
    }

    public void apply(<strong>@Observes OrderPlaced event</strong>) {
        MealOrder order = new MealOrder(event.getOrderInfo());
        entityManager.persist(order);
    }

    public void apply(<strong>@Observes OrderStarted event</strong>) {
        apply(event.getOrderId(), MealOrder::start);
    }

    public void apply(<strong>@Observes MealDelivered event</strong>) {
        apply(event.getOrderId(), MealOrder::deliver);
    }

    private void apply(UUID orderId, Consumer&lt;MealOrder&gt; consumer) {
        MealOrder order = entityManager.find(MealOrder.class, orderId.toString());
        if (order != null)
            consumer.accept(order);
    }
}</pre>
<p>This simple example represents the state of the meal orders in a relational database. As soon as a new CDI event arrives, the state of the orders is updated. The current state can be retrieved by the <kbd>get()</kbd> method.</p>
<p>The meal order domain entity is persisted via JPA. It contains the status of the order that is updated via observed CDI events:</p>
<pre>@Entity
@Table("meal_orders")
public class MealOrder {

    @Id
    private String orderId;

    @Embedded
    private MealSpecification specification;

    @Enumerated(EnumType.STRING)
    <strong>private OrderState state;</strong>

    private MealOrder() {
        // required for JPA
    }

    public MealOrder(OrderInfo orderInfo) {
        orderId = orderInfo.getOrderId().toString();
        <strong>state = OrderState.PLACED;</strong>

        // define specifications
    }

    public <strong>void start()</strong> {
        <strong>state = OrderState.STARTED;</strong>
    }

    public <strong>void deliver()</strong> {
        <strong>state = OrderState.DELIVERED;</strong>
    }

    ...
}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consuming Kafka messages</h1>
                </header>
            
            <article>
                
<p>The part that consumes the messages encapsulates the message hub from the rest of the application. It is integrated by firing CDI events on arriving messages. This certainly is specific to the Kafka API and should be considered as an example solution.</p>
<p>The updating consumer connects to a specific topic via its consumer group. The startup singleton bean ensures the consumer will be initiated at application startup. A container-managed executor service runs the event consumer in its own thread:</p>
<pre>@Startup
@Singleton
public class OrderUpdateConsumer {

    <strong>private EventConsumer eventConsumer;</strong>

    @Resource
    ManagedExecutorService mes;

    @Inject
    Properties kafkaProperties;

    @Inject
    Event&lt;MealEvent&gt; events;

    @PostConstruct
    private void init() {
        String orders = kafkaProperties.getProperty("topic.orders");

        <strong>eventConsumer = new EventConsumer(kafkaProperties,<br/>                ev -&gt; events.fire(ev), orders);

        mes.execute(eventConsumer);</strong>
    }

    @PreDestroy
    public void close() {
        <strong>eventConsumer.stop();</strong>
    }
}</pre>
<p>The application-specific Kafka properties are exposed via a CDI producer. They contain the corresponding consumer groups.</p>
<p>The event consumer performs the actual consumption:</p>
<pre>import org.apache.kafka.clients.consumer.KafkaConsumer;
import java.util.function.Consumer;
import static java.util.Arrays.asList;

public class EventConsumer implements Runnable {

    private final KafkaConsumer&lt;String, MealEvent&gt; consumer;
    private final Consumer&lt;MealEvent&gt; eventConsumer;
    private final AtomicBoolean closed = new AtomicBoolean();

    public EventConsumer(Properties kafkaProperties,<br/>            Consumer&lt;MealEvent&gt; eventConsumer, String... topics) {
        this.eventConsumer = eventConsumer;
        consumer = new KafkaConsumer&lt;&gt;(kafkaProperties);
        consumer.subscribe(asList(topics));
    }

    @Override
    public void run() {
        try {
            while (!closed.get()) {
                consume();
            }
        } catch (WakeupException e) {
            // will wakeup for closing
        } finally {
            consumer.close();
        }
    }

    private void consume() {
        ConsumerRecords&lt;String, MealEvent&gt; records =<br/>                consumer.poll(Long.MAX_VALUE);
        for (ConsumerRecord&lt;String, MealEvent&gt; record : records) {
            eventConsumer.accept(record.value());
        }
        consumer.commitSync();
    }

    public void stop() {
        closed.set(true);
        consumer.wakeup();
    }
}</pre>
<p>Kafka records that are consumed result in new CDI events. The configured properties use JSON serializers and deserializers, respectively, to map the domain event classes.</p>
<p>Events that are fired via CDI and consumed successfully are committed to Kafka. The CDI events are fired synchronously, to ensure that all processes are finish reliably before committing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Producing Kafka messages</h1>
                </header>
            
            <article>
                
<p>The event producer publishes the domain events to the message hub. This happens synchronously to rely on the messages being in the system. Once the transmission is acknowledged, the <kbd>EventProducer#publish</kbd> method invocation returns:</p>
<pre>import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;

@ApplicationScoped
public class EventProducer {

    private Producer&lt;String, MealEvent&gt; producer;
    private String topic;

    @Inject
    Properties kafkaProperties;

    @PostConstruct
    private void init() {
        producer = new KafkaProducer&lt;&gt;(kafkaProperties);
        topic = kafkaProperties.getProperty("topics.order");
        producer.initTransactions();
    }

    public <strong>void publish(MealEvent event)</strong> {
        ProducerRecord&lt;String, MealEvent&gt; record = new ProducerRecord&lt;&gt;(topic, event);
        try {
            producer.beginTransaction();
            <strong>producer.send(record);</strong>
            producer.commitTransaction();
        } catch (ProducerFencedException e) {
            producer.close();
        } catch (KafkaException e) {
            producer.abortTransaction();
        }
    }

    @PreDestroy
    public void close() {
        producer.close();
    }
}</pre>
<p>Going into the details of the Kafka producer API is beyond the scope of this book. However, it needs to be ensured that the events are sent reliably. The event producer bean encapsulates this logic.</p>
<p>These examples demonstrate one possibility for integrating Kafka.</p>
<p>As mentioned earlier, the <strong>Java EE Connector Architecture</strong> (<strong>JCA</strong>) is another possibility for integrating external concerns into the application container. At the time of writing, there are vendor-specific container solutions that integrate messaging via JCA. Existing solutions for integrating message hubs such as Kafka are an interesting alternative. However, application developers are advised to encapsulate technology specifics into single points of responsibilities and use standard Java EE functionality within the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application boundaries</h1>
                </header>
            
            <article>
                
<p>The applications of a CQRS system communicate via events internally. Externally, other protocols such as HTTP can be provided.</p>
<p>The query and command functionality of, for example, the waiter system, is exposed via JAX-RS. The command service offers functionality to place meal orders. It uses the event producer to publish the resulting events:</p>
<pre>public class OrderService {

    @Inject
    EventProducer eventProducer;

    public <strong>void orderMeal(OrderInfo orderInfo)</strong> {
        <strong>eventProducer.publish(new OrderPlaced(orderInfo));</strong>
    }

    void cancelOrder(UUID orderId, String reason) {
        eventProducer.publish(new OrderCancelled(orderId, reason));
    }

    void startOrder(UUID orderId) {
        eventProducer.publish(new OrderStarted(orderId));
    }

    void deliverMeal(UUID orderId) {
        eventProducer.publish(new MealDelivered(orderId));
    }
}</pre>
<p>The <kbd>orderMeal()</kbd> method is called by the HTTP endpoint. The other methods are called by the waiter system's event handler. They will result in new events that are delivered by the event hub.</p>
<p>The reason for not directly firing events or calling functionality internally here is that this application resides in a distributed environment. There might be other instances of the waiter system consuming the event hub and updating their representation accordingly.</p>
<p>The command service contains a JAX-RS resource that is used to order meals:</p>
<pre>@Path("orders")
public class OrdersResource {

    @Inject
    OrderService orderService;

    @Context
    UriInfo uriInfo;

    @POST
    public <strong>Response orderMeal(JsonObject order)</strong> {
        OrderInfo orderInfo = createOrderInfo(order);
        <strong>orderService.orderMeal(orderInfo);</strong>

        URI uri = uriInfo...

        return Response.accepted().header(HttpHeaders.LOCATION, uri).build();
    }

    ...
}</pre>
<p>The query service exposes the meal order representations. It loads the current state of the domain entities from the database as seen in the <kbd>MealOrders</kbd>. The JAX-RS resources of the query service use this functionality.</p>
<p>If the waiter system is shipped as a single instance, containing both the command and query services, these resources can be combined. It needs to be ensured though that the services don't cross-communicate, except via the eventing mechanism. The following code snippet shows the query service endpoint:</p>
<pre>@Path("orders")
public class OrdersResource {

    @Inject
    MealOrders mealOrders;

    @GET
    @Path("{id}")
    public <strong>JsonObject getOrder(@PathParam("id") UUID orderId)</strong> {
        <strong>MealOrder order = mealOrders.get(orderId);</strong>

        if (order == null)
            throw new NotFoundException();

        // create JSON response
        return Json.createObjectBuilder()...
    }
}</pre>
<p>These examples are not exhaustive, but are meant to give the reader an idea of integrating CQRS concepts and message hubs into Java EE applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating further CQRS concepts</h1>
                </header>
            
            <article>
                
<p>One of the benefits of event-sourced systems is that it's possible to take the full set of atomic events and replay them, for example, in test scenarios. System tests verify against the actual use cases that happened in production. Audit logging comes for free as well, being part of the core of the application.</p>
<p>This approach also enables us to change business functionality and replay some events, either to fix bugs and to correct behavior, or to apply the event information to new functionality. This makes it possible to apply new features on events as if they were part of the application since day one.</p>
<p>If the chef system adds functionality to continuously calculate the average time of meal preparation, the events can be redelivered to re-calculate the representations. Therefore the database contents will be reset and the events redelivered, only to the updating consumer, which results in new representation being calculated and persisted. Kafka can explicitly redeliver events.</p>
<p>The events, however, are solely used to update the status representations, not triggering new commands during replays. Otherwise, the system would end up in an inconsistent state. The demonstrated example realizes this by defining a dedicated Kafka consumer group for event handlers, which is not reset to redistribute events to the event handlers. Only the updating consumers re-consume the events, to recalculate the internal state representations.</p>
<p>The point is, that CQRS systems enable many more use cases, due to event sourcing being used. The possibilities of capturing and replaying events, as well as the contained context and history information, enable extensive scenarios.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java EE in the age of distribution</h1>
                </header>
            
            <article>
                
<p>Microservice architectures and distributed systems naturally require communication that involves more than a single, monolithic application. There are many ways how to implement communication with Java EE, depending on the chosen protocols and communication technologies.</p>
<p>There are some aspects to be considered when realizing communication. External applications that take part of the microservice system, for example, require discovering the service instances. In order to not tightly couple applications and configuration, looking up services should be dynamic, rather than configuring static hosts or IP addresses.</p>
<p>The cloud native principle of being resilient also concerns communication. Since networks can potentially fail anytime, application health should not be impacted when connectivity decelerates or goes down. The application should guard itself from potential errors propagating into the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discovering services</h1>
                </header>
            
            <article>
                
<p>Service discovery can happen in various ways, from DNS lookup to more sophisticated scenarios where the lookup is part of business logic, providing different endpoints depending on the situation. It generally encapsulates addressing external systems from the application's concerns. Ideally, the application logic only names the logical service it needs to communicate with, and the actual lookup is performed externally.</p>
<p>It depends on the used environments and runtime which possibilities enterprise developers have. Container technologies offer functionality to link services by names, taking away work and responsibility from the application. The clients connect against the link or service names as hostnames, which are resolved by the container technology.</p>
<p>This approach works both for Docker containers and container orchestration such as Docker Compose, Kubernetes, or OpenShift. All communication concerns solely use logical service names and ports to establish connections. This matches the 12-factor principles as well.</p>
<p>Since the lookup work is performed in the environment, the applications will only specify the desired service names. This is true for all outward communication, such as HTTP connections, databases, or message hubs. <a href="a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml">Chapter 5</a>, <em>Container and Cloud Environments with Java EE</em> demonstrated examples for this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Communicating resiliently</h1>
                </header>
            
            <article>
                
<p>Network communication is not reliable and can potentially break in all sorts of ways. Connections may timeout, services may be unavailable, respond slowly, or deliver unexpected answers.</p>
<p>In order to not let errors propagate into the application, the communications need to be resilient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Validating responses</h1>
                </header>
            
            <article>
                
<p>First of all, this implies client-side validation and handling errors. Irrelevant to the communication technology in use, applications can't rely on external systems to provide responses that are not malformed or not simply wrong.</p>
<p>This doesn't mean that clients immediately have to reject all responses that are not perfect in the application's understanding. Responses that contain more information or slightly different formats than expected, but are still understandable, should not lead to immediate errors. Following the principle to <em>be conservative in what you do and liberal in what you accept</em>, messages that contain just enough for the application to do its job should be accepted. For example, additional, unknown properties in JSON responses should not lead to refusing to map the object.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking timeouts and circuits</h1>
                </header>
            
            <article>
                
<p>Clients that perform synchronous calls to external systems block the subsequent execution until the external system responds. Invocations may fail, slow down the execution, or in the worst case effectively bring the whole application down. It's crucial to keep this fact in mind when implementing clients.</p>
<p>First of all client connections should always set reasonable timeouts, as shown similarly in <a href="">Chapter 3</a>, <em>Implementing Modern Java Enterprise Applications</em>. Timeouts prevent the application from deadlock situations.</p>
<p>As seen before, Java EE interceptors can be used to prevent potential runtime exceptions from propagating into the business logic.</p>
<p>So-called <strong>circuit breakers</strong> take this approach of preventing cascading failure further. They secure client invocations by defining error or timeout thresholds and prevent further invocations in case of failure. The circuit breaker approach comes from the model of electrical engineering, circuit breakers built into buildings, that intercept the connection by opening their circuits to prevent further damage.</p>
<p>A client circuit breaker similarly opens its circuit, that is, preventing further invocations, to not <em>damage</em> the application or the external system. Circuit breakers usually allow errors and timeouts to happen up to a certain degree and then cutting the connections for a certain time, or until the circuit is manually closed again.</p>
<p>Java EE applications can implement circuit breakers via interceptors. They can add sophisticated logic on when and how to open and close their circuits, for example, measuring the number of failures and timeouts.</p>
<p>The following demonstrates one possible circuit breaker approach in pseudo code. The interceptor behavior is annotated to a client method, similarly to client interceptor examples demonstrated earlier in this book:</p>
<pre>@Interceptor
public class CircuitBreaker {

    ...

    @AroundInvoke
    public Object aroundInvoke(InvocationContext context) {

        <strong>// close circuit after recovery time</strong>

        <strong>if (circuit.isOpen())
            return null;</strong>

        try {
            <strong>return context.proceed();</strong>
        } catch (Exception e) {

            <strong>// record exception
            // increase failure counter
            // open circuit if failure exceeds threshold

            return null;</strong>
        }
    }
}</pre>
<p>Similarly, the circuit breaker could measure the service time and open its circuit if the service becomes too slow, additionally to HTTP client timeouts.</p>
<p>There are some open source Java EE libraries available for this purpose, for example <strong>Breakr</strong> by Java EE expert Adam Bien. It depends on the technical requirements and the complexity of the logic, when to open and close the circuit, and whether third-party dependencies are required.</p>
<p>In order to build zero-dependency applications, potential libraries should be installed into the container and not shipped with the application artifacts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bulkheads</h1>
                </header>
            
            <article>
                
<p>Ships contain bulkheads that divide the vessel into several areas. If the ship hull gets a leak in some locations, only a single area is filled with water and the whole ship is likely still able to float.</p>
<p>The <strong>bulkhead</strong> pattern takes this idea to enterprise applications. If some component of the application fails or is working to capacity due to workload, then the rest of the application should still be able to fulfill its purpose. This, of course, highly depends on the business use case.</p>
<p>One example is to separate the thread execution of business processes from HTTP endpoints. Application servers manage a single pool of request threads. If, for example, a single business component fails and blocks all incoming requests, all available request threads will eventually be occupied. The result is in no other business uses cases is being able to be invoked, due to unavailable request threads. This could be the case if used clients don't implement proper timeouts, connect against a system that is down, and block the execution.</p>
<p>Using asynchronous JAX-RS resources together with dedicated managed executor services can relieve this issue. As seen earlier in this book, JAX-RS resources can invoke the business functionality in separate, container-managed threads to prevent the overall execution utilizing a request thread. Multiple components can use independent thread pools, which prevent failures from spreading.</p>
<p>Since the application server is responsible for managing threads, this approach should be implemented following Java EE standards. The idea is to define dedicated executor services that are injectable at the required positions.</p>
<p>The open source library <strong>Porcupine</strong> by Adam Bien uses this approach to create dedicated executor services that use <kbd>ManagedThreadFactory</kbd> to define thread pools with container-managed threads. The dedicated executor services can be configured and instrumented appropriately.</p>
<p>The following snippet shows one example of the bulkheads pattern, combining asynchronous JAX-RS resources with dedicated executor services:</p>
<pre>import com.airhacks.porcupine.execution.boundary.Dedicated;<br/>import java.util.concurrent.ExecutorService;<br/><br/>@Path("users")
@Produces(MediaType.APPLICATION_JSON)
public class UsersResource {

    <strong>@Inject
    @Dedicated("custom-name")
    ExecutorService executor;</strong>

    @GET
    public CompletionStage&lt;Response&gt; get() {
        return CompletableFuture
                .supplyAsync(this::getUsers, <strong>executor</strong>)
                .thenApply(s -&gt; Response.ok(s).build());
    }

    ...
}</pre>
<p>The business use case is executed in a managed thread provided by the executor service, in order to allow the request thread to return and to handle other requests. This enables other functionality of the application to still function, even if this part is overloaded, and utilizes all threads of the <kbd>custom-name</kbd> executer.</p>
<p>The following examines how the custom executor service is configured.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shaking hands and pushing back</h1>
                </header>
            
            <article>
                
<p>Another approach of communicating in a resilient way is <strong>handshaking</strong> and <strong>backpressure</strong>. The idea is that the communication partner being under load notifies the other side, which then backs off and eases the load. Handshaking here means that the calling side has a way of asking the service whether it can handle more requests. Backpressure reduces the load on a system by notifying clients when the limit is reached or pushing back requests.</p>
<p>The two approaches combined form a resilient and effective form of communication.</p>
<p>Information about the current load state of the application can be provided in HTTP resources or via header fields. The clients then take this information into account.</p>
<p>A more direct way is to simply reject a client request when the server's resources are fully utilized. Developers are advised to pay attention to the behavior of pooling such as in executor services, and how they handle situations with full queues. E<span>xceptionally, it's</span> advisable to abort the client request to not unnecessarily run into timeouts.</p>
<p>The following example shows a scenario using the Porcupine library. A business functionality is executed using a dedicated executor service, which will be configured to abort rejected executions. The clients will immediately receive a <kbd>503 Service Unavailable</kbd> response, indicating that currently the service is not able to serve requests.</p>
<p>The JAX-RS resource is similar to the previous example. The <kbd>custom-name</kbd> executor is configured to abort rejected executions via a specialized configurator. The <kbd>ExecutorConfigurator</kbd> is part of the Porcupine library. The following shows the custom configuration:</p>
<pre>import com.airhacks.porcupine.configuration.control.ExecutorConfigurator;<br/>import com.airhacks.porcupine.execution.control.ExecutorConfiguration;<br/><br/><strong>@Specializes</strong>
public class CustomExecutorConfigurator <strong>extends ExecutorConfigurator</strong> {

    @Override
    public ExecutorConfiguration defaultConfigurator() {
        return super.defaultConfigurator();
    }

    @Override
    public ExecutorConfiguration forPipeline(String name) {
        if ("custom-name".equals(name)) {
            <strong>return new ExecutorConfiguration.Builder().
                    abortPolicy().
                    build();</strong>
        }
        return super.forPipeline(name);
    }
}</pre>
<p>Executions that are rejected due to full queues will then result in a <kbd>RejectedExecutionException</kbd>. This exception is mapped via JAX-RS functionality:</p>
<pre>import java.util.concurrent.RejectedExecutionException;<strong><br/><br/>@Provider</strong>
public class RejectedExecutionHandler<br/><strong>        implements ExceptionMapper&lt;RejectedExecutionException&gt;</strong> {

    @Override
    public Response toResponse(RejectedExecutionException exception) {
        return Response.status(Response.Status.SERVICE_UNAVAILABLE)
                .build();
    }
}</pre>
<p>Client requests that would exceed the server limits immediately result in an error response. The client invocation can take this into account and act appropriately. For example, a circuit breaker pattern-like functionality can prevent the client from immediate subsequent invocations.</p>
<p>Backpressure is helpful when crafting scenarios with multiple services that need to meet <strong>service level agreements</strong> (<strong>SLA</strong>). <a href="">Chapter 9</a>, <em>Monitoring, Performance, and Logging</em> will cover this topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More on being resilient</h1>
                </header>
            
            <article>
                
<p>Besides resilience in communication, microservices also aim to improve service quality and availability. Applications should be able to scale and self-heal in cases of failures.</p>
<p>The use of container orchestration technology such as Kubernetes supports this approach. Pods that back logical services can be scaled up to handle more workload. The services balance the load between the containers. There are possibilities to auto-scale instances up or down based on the current workload on the cluster.</p>
<p>Kubernetes aims to maximize service uptime. It manages liveness and readiness probes to detect failures and potentially start new containers. In case of errors during deployments, it will leave currently running services untouched, until the new versions are able to serve traffic.</p>
<p>These approaches are managed by the runtime environment, not part of the application. It's advisable to minimize the non-functional, cross-cutting concerns within the enterprise application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>There a multiple motivations behind distributing systems. Despite certain introduced challenges and overheads in communication, performance, and organization, distribution is often necessary.</p>
<p>In order to design the system landscape, the system's context map that represents the individual responsibilities needs to be taken into consideration. It's advisable to design application APIs in clear, lean interfaces, ideally implemented with standard communication protocols. Before introducing breaking changes, engineers as well as business experts need to ask themselves whether it is necessary to force client functionality to stop working. In the same way, APIs should be designed in a resilient way, preventing unnecessary breaks, in other words: <em>be conservative in what you do and liberal in what you accept</em>.</p>
<p>Engineers that build distributed applications need to be aware of the trade-off between consistency and scalability. The majority of applications that use synchronous communication involving an external system will likely scale well enough. Distributed transactions should be avoided.</p>
<p>In order to communicate asynchronously, application can be based on event-driven architectures. The CQRS principle combines the motivations behind event-driven architectures and event sourcing. Whereas CQRS certainly offers interesting solutions, it only makes sense if there is a need for distributing application.</p>
<p>Microservice architectures don't share common technology or data with each other. Shared-nothing architectures are free to choose implementations and persistence technology. Zero-dependency Java EE applications shipped in containers are a reasonable fit for microservices. The <em>one application per application server</em> approach matches the idea of shared-nothing architectures. There are many aspects in which Java EE applications running in container orchestration frameworks support developing microservice architectures, such as service discovery, resilient communication via timeout, circuit breakers or bulkheads.</p>
<p>The following chapter covers the topics of performance, monitoring and logging.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>