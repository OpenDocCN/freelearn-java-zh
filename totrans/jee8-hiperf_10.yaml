- en: Continuous Performance Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in the previous chapter that driving a benchmark requires work and is
    not a small operation. However, it doesn't solve all the needs of software development,
    since you only do it from time to time. Moreover, between two benchmarks, you
    can have huge performance regression not captured by the unit tests validating
    the features and behavior of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve that, it is a good idea to try to add dedicated tests for performance.
    This is what this chapter is about. Thus, we will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Which tools you can use to *test* the performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How you can set up some continuous integration for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to write tests for the performance in such a context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing performance tests – the pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tests have some challenges you need to take into account when you
    write your tests to avoid having a lot of false positive results (not passing
    tests where the performance is actually acceptable).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common issues you will encounter are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to manage the external systems: We know that external systems are very
    important in applications today, but it is not always trivial to have them during
    tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to be deterministic: Continuous integration/test platforms are often shared.
    How do you ensure you have the resources needed in order to have a deterministic
    execution time and to not be slown down, as another build is using all the available
    resources?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to handle the infrastructure: To do an end-to-end test, you need multiple
    injectors (clients) and probably multiple backend servers. How do you ensure you
    have them available without them being too expensive if you use cloud platforms
    such as **Amazon Web Services** (**AWS**)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see the setup of performance tests as a benchmark preparation phase.
    The main difference will be that you should not rely on external consulting in
    the long-term, and you will ensure the benchmark iterations are almost completely
    automatic—*almost* because you will still need to take some actions if the tests
    fail, otherwise there is no point having a continuous system.
  prefs: []
  type: TYPE_NORMAL
- en: Writing performance tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tests are present at all stages in a Java EE application, and so,
    there are several ways to write a performance test. In this section, we will go
    through a few main ones, starting with the simplest one (algorithm validation).
  prefs: []
  type: TYPE_NORMAL
- en: JMH – the OpenJDK tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Java Microbenchmark Harness** (**JMH**) is a small library developed by the
    OpenJDK team—yes, the same one doing the JVM—which enables you to easily develop
    microbenchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: A microbenchmark designs a benchmark on a very small part of an application.
    Most of the time, you can see it as a *unit benchmark*, using the analogy with
    unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, it is something important when setting up performance tests as it will
    allow you to quickly identify a critical performance regression introduced by
    a recent change. The idea is to associate each benchmark with a small part of
    code, whereas a benchmark will include several layers. So, if the benchmark fails,
    you will spend a lot of time identifying why, instead of simply checking the related
    code part.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a benchmark with JMH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before being able to write code with JMH, you need to add it as a dependency
    of your project. We will use Maven syntax in the following example, but it has
    equivalents for Gradle, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to add into your `pom.xml` the following dependencies;
    we will use the scope `test` since the dependencies are only needed for our performance
    tests and not the "main" code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As of writing this book, the `jmh.version` property can take the `1.19` value.
    The `jmh-core` dependency will bring you JMH itself with its annotation-based
    API, and the `jmh-generator-annprocess` brings an annotation processor backbone—used
    when compiling the test classes/benchmarks—which will generate some sort of index
    file(s) used by the runtime to execute the benchmarks and the benchmark classes
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you have the right dependencies, you can develop your benchmarks. The
    API is quite straightforward. It uses the notion of a *state* and a state has
    a life cycle and (volatile) storage associated with the benchmark. The life cycle
    of a state is defined by marking methods with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@Setup` to execute an initialization task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@Teardown` to release any created resource in the setup method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State classes can also contain fields decorated with `@Param` to make them contextualized
    and configurable (such as enabling them to get a different target URL depending
    on the execution for example).
  prefs: []
  type: TYPE_NORMAL
- en: 'A state class is marked with `@State`, which takes, as parameter, the scope
    of the state instance for a benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Benchmark` means the state will be a singleton in the scope of the JVM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Group` defines the state as a singleton per group. A group is a way to put
    multiple benchmark methods (scenarii) in the same thread bucket during the execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Thread` defines the state as a singleton per thread (a bit like `@RequestScoped`
    for CDI).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that you can change how the life cycle is managed a bit by passing the
    runtime to `@Setup` and `@Teardown` to a different `Level`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Trial`: This is the default, and as we just explained, the benchmark is seen
    as a whole'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Iteration`: The life cycle is executed per iteration (warmup or measurement)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Invocation`: The life cycle of the state instance is executed per method;
    this is not recommended in practice to avoid some measurement errors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, once you have your state object, you define the class containing your benchmarks
    which are nothing other than methods marked with `@Benchmark`. Then, you have
    several annotations you can add to the method to customize the benchmark execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@Measurement`: To customize the number of iterations or the duration of the
    benchmark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@Warmup` : The same as `@Measurement`, but for the warmup (which is a sort
    of pre-execution of the benchmark not taken into account for the measurement,
    the goal being to only measure metrics on a hot JVM).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@OuputTimeUnit`: To customize the unit used for the metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@Threads`: To customize the number of threads used for the benchmark. You
    can see it as the number of *users*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@Fork`: The benchmark will be executed in another JVM to avoid test side effects.
    This annotation allows you to add custom parameters to the forked JVM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@OperationsPerInvocation`: If you have a loop in your benchmark method, this
    option (a number) will normalize the measurement. For instance, if you execute
    five times the same operation in your benchmark and set this value to five, then
    the execution time will be divided by five.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@Timeout`: It lets you define a maximum duration for the benchmark execution.
    JMH will interrupt the threads if overpassed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@CompilerControl`: To customize the way the annotation processor generates
    the code. For our use case, you rarely need this option, but while tuning some
    code portions, it can be interesting to test it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your first JMH benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After all that theory, here is a basic benchmark developed with JMH:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we have a single benchmark scenario called `compute`. It doesn't
    use any states and didn't customize any thread or fork count.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, this will not be enough and you will often need a state to get
    a service. So, it will more look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created a nested `QuoteState` instance which will be responsible for
    getting us a service, and we injected it to the benchmark method (`compute`) and
    used it to get our service instance. This avoids creating an instance per iteration,
    and thus, avoids taking into account the container startup duration.
  prefs: []
  type: TYPE_NORMAL
- en: This sort of implementation works well until you need an actual container. But
    when you need a container, it will require some mocking which is absolutely something
    you should get rid of—even for unit testing—as it will not represent anything
    close to the real deployment until your code doesn't use Java EE at all (which
    also means you don't need to mock anything if that is the case).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use a CDI 2.0 implementation supporting standalone API—and your application
    doesn''t need more for what you want to test—then you can change the state to
    start/stop the CDI container and look up the services you need with the new CDI
    standalone API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, the setup starts the `container`—you can customize the classes to deploy
    before calling `initialize()`—and then looks up the `QuoteService` using the `container` instance
    API. The `tearDown` method just closes the container properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though, in GlassFish, you can''t use that new API. But there is the `EJBContainer`,
    coming from Java EE 6, allowing you to do the same thing combined with the `CDI`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This uses exactly the same logic as before, except the `container` is started
    based on the `EJBContainer` API. This looks great, but it will not always work
    with all containers. One pitfall is that if you don't have any EJB, some containers
    will just not even try to deploy the application.
  prefs: []
  type: TYPE_NORMAL
- en: You can find several workarounds, but the saner solution is to check if you
    really need the full container or just a subset—such as just CDI—and in this case,
    you just start this subset (Weld or OpenWebBeans only for a CDI using a previous
    state). If you really need a full container and your vendor doesn't support any
    of the two previous ways of starting a container, you can also use a vendor-specific
    API, mock the container (but take care, you will bypass some execution time *cost*),
    use another vendor if close enough to your final container, or implement it with
    a third-party container manager such as the `arquillian` container API.
  prefs: []
  type: TYPE_NORMAL
- en: BlackHole to be a star
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JMH provides a particular class—`org.openjdk.jmh.infra.Blackhole`—that can
    seem weird as it mainly only allows you to `consume` an instance. It is retrieved
    by injecting it into the parameters of the benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Why consume the returned value of a method if we do nothing about it? Remember
    that the Oracle JVM has what is called the **just in time compilation** (**JIT**)
    which optimizes the code at runtime depending on the statistics of the code paths.
    If you don't call that `consume` method, you can end up not measuring the actual
    code you want, but some very optimized flavors of this code, since, most of the
    time, in your benchmark methods, you will ignore part of the returned values which
    can, hence, be optimized with the *dead-code elimination* rule.
  prefs: []
  type: TYPE_NORMAL
- en: Running the benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JMH provides a main-friendly API to run the benchmarks. It is based on defining
    the execution options which are more or less the options you can associate with
    a benchmark method (the one we saw in previous section with annotations) and a
    list of includes/excludes to select the classes/methods to run. Then, you pass
    these options to a runner and call its `run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we build a simple `Options` instance from the `OptionsBuilder` by including
    a benchmark class we want to include. Finally, we run it through the method with
    the same name as the runner, and collect the benchmark's results in a collection.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating JMH with JUnit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is no official JUnit integration of JMH. In spite of that, it is not
    that hard to do it yourself. There are lots of possible designs, but in the context
    of this book we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Our integration will go through a JUnit runner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benchmark classes will be identified by extracting the nested classes of
    the test class which will avoid using any scanning to find the benchmark classes
    or explicit listing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will introduce an `@ExpectedPerformances` annotation to be able to add assertions
    based on the execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Structurally, a microbenchmark test using this structure will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have the overall test structure, you just need to add the benchmarks
    themselves in the right nested class. Concretely, a benchmark class can look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this test class, we have two benchmark classes using a different state (a
    different service in our EE application) and each of them can have a different
    benchmark method counts. The execution is handled by the runner set with `@RunWith`,
    using the standard JUnit (4) API. We will note the `@ExpectedPerformances` usage
    on all benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: If you have already migrated to JUnit 5, or are using TestNG, the same sort
    of integration is possible but you will use extensions for JUnit 5 and probably
    an abstract class or a listener for TestNG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before seeing how to implement that runner, we must have this `@ExpectedPerformances`
    annotation. It is the one allowing us to assert the performance of our benchmark.
    So, we at least need:'
  prefs: []
  type: TYPE_NORMAL
- en: A score (duration but without specifying the unit, since JMH supports customizing
    the report unit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tolerance on the score as there is no way you get exactly the same score twice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do that, we can use this simple definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we tell the user to define a score, but we use a default score
    tolerance of `0.1`. In our implementation, we will consider it as being a percentage
    (10%). This will avoid frequent failure if the machine load is not very stable
    when running your job. Don't hesitate to decrease that value or even make it configurable
    through a system property.
  prefs: []
  type: TYPE_NORMAL
- en: To make our previous snippet work, we need to implement a JUnit runner. This
    is a design choice, but you can also use a rule, exposing some programmatic API
    or not. To keep it simple, we will not do it here and just consider the whole
    benchmark setup as done through the annotations. However, for a real-life project,
    it can be handy to enable the environment (system properties) to customize the
    annotation values a bit. A common implementation is to use them as a template
    and multiple all numeric values by a configured ratio on the JVM to let the tests
    run on any machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our runner will have these four roles:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the benchmark classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the model of these classes—typically, validate that each benchmark
    has an `@ExpectedPerformances`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run each benchmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the expectations to make the test fail if we have a regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is simpler for us to extend `ParentRunner<Class<?>>`. We could use `BlockJUnit4ClassRunner`,
    but it is based on methods and JMH only supports filtering the executions per
    class. So, let's stick to it for now. If you put a single benchmark per nested
    class, then you can simulate a run per method behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is to find our benchmark classes. With the JUnit
    runner API, you can access the test class with `getTestClass()`. To find our benchmark,
    we just need to check the nested classes of that class through `getClasses()`
    and make sure the class has at least one `@Benchmark` on a method to validate
    that it is a JMH class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We go through all nested classes of the test class, then keep (or filter) only
    the ones with a benchmark method at the least. Note that we store the result as
    we will need multiple times in our runner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the validation is as simple as going over these classes and their benchmark
    methods to validate they have the `@ExpectedPerformances` annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, to list the errors of the JUnit validation, we add an exception per method
    annotated with `@Benchmark` but without an `@ExpectedPerformances`. We do it first
    by converting the classes to a stream of benchmark methods, then keeping only
    the ones without the `@ExpectedPerformances` annotation to keep the *set vision*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last key part of the runner code is to convert a class to an actual
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: First, we execute the benchmark holder class and collect the results. Then,
    we iterate over the benchmark methods and for each of them, we extract the result
    (primary result label is the method name). Finally, we extract our `@ExpectedPerformances`
    constraint and compare it with the primary result of the test. The little trick
    here is to catch the `AssertionError` the assert can throw, and collect them all
    in a list to convert them in another `AssertionError`. You can just format the
    message the way you want, but doing it this way keeps standard JUnit formatting
    of the errors. The other tip here is to put the benchmark class and method in
    the error message to make sure you can identify which benchmark failed. The alternative
    way can be to introduce another annotation to use a custom name for each benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have looked at all the small technical points, let''s put it all
    together. To start, we will define that our runner will use `Class` children,
    which will represent each of our nested classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code executed by the parent constructor (`super(klass)`), JUnit will
    trigger a test validation where we compute the children we previously saw to be
    able to return them in `getChildren()` and to let JUnit handle all our nested
    classes. We implement `describeChild` to let JUnit associate a `Description` with
    each nested class and to have smoother integration with IDE (the goal is to show
    them in a tree when you run the tests). To compute the children and validate them,
    we can use this JUnit implementation of `collectInitializationErrors`—using this
    hook avoids computing it multiple times per test class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to ensure we can run our children (benchmarks) properly. To do
    that, we extend another JUnit hook which is designed to run each child. The only
    thing we take care of is mainly ensuring JUnit `@Ignored` is supported for our
    children:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In `runChild()`, we delegate the execution to the JUnit engine by adding, as
    an implementation of our test execution, the code we saw just before based on
    JMH runner but wrapped in a JUnit `Statement` to let it be integrated with JUnit
    notifiers. Now, we just need this execution implementation (`benchmarkStatement`).
    This is done by completing the class with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This reuses everything we previously saw; the `buildOptions` method will force
    the JMH runner to use the annotations on the benchmark for the execution configuration
    and we just include a single test at a time. Finally, we implement the `assertResults`
    method as we explained before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, with this runner, you can execute the test in the main build with surefire
    or failsafe and ensure your build will not pass if you have huge performance regression.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple implementation and you can enrich it in several ways, such
    as by simulating one child per benchmark to gain a nicer report in the IDE and
    surefire report (just run it the first time you encounter the enclosing class,
    then store the result and do the assertion per method). You can also assert more
    results, such as the secondary results or the iteration results (for example,
    no iteration is slower than X). Finally, you can implement some `living documentation` features,
    adding an @Documentation annotation which will be used by the runner to create
    a report file (in asciidoctor, for instance).
  prefs: []
  type: TYPE_NORMAL
- en: ContiPerf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An alternative to JMH, which is a bit less advanced but much easier to use,
    is ContiPerf. You can add it to your project with this dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We will not detail it completely in this book as we spent a lot of time on JMH
    already. But in a few words, it is based on a JUnit 4 rule. Thus, it can be combined
    with other rules and sorted thanks to the JUnit `RuleChain`, which makes it very
    powerful, with all light EE containers having a JUnit-based testing stack such
    as TomEE or Meecrowave, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The very big advantage of ContiPerf is that it is aligned with the JUnit model:'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the standard JUnit `@Test` method marker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can reuse the JUnit standard life cycle (`@BeforeClass`, `@Before`, and
    so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can combine it with other JUnit features (runners, rules, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is what a test can look like in terms of structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We immediately identify a JUnit structure, with an `@BeforeClass` initializing
    the test (you can start a container here and close it in an `@AfterClass` if needed),
    and a `@Test` that is our benchmark/scenario. The only difference with a JUnit
    test is the `ContiPerfRule` and the `@Required` and `@PerfTest` annotations.
  prefs: []
  type: TYPE_NORMAL
- en: The `@PerfTest` describes the test environment—how many threads, how long, how
    many iterations, the duration for the warmup, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The `@Required`, on another side, describes the assertions (validations) to
    do. It is equivalent to our `@ExpectedPerformances` in our JMH integration. It
    supports most of the common validations such as the throughoutput, the average,
    the total time, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Arquillian and ContiPerf – the diabolic pair
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the JMH section, we got the issue that starting a container was sometimes
    hard and really not straightforward. As ContiPerf is a rule, it is compatible
    with `Arquillian`, which can do all that work for you.
  prefs: []
  type: TYPE_NORMAL
- en: '`Arquillian` is a project created by JBoss (Red Hat, now) for abstracting the
    containers behind a **service provider interface** (**SPI**) and integrating it
    with JUnit or TestNG. The idea is to run a test from your IDE as usual, without
    having to care about needing a container.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, it requires you to define what you deploy into the container
    and to use the `Arquillian` runner with JUnit (or an abstract class with TestNG).
    Thanks to a mechanism of extensions and enrichers, you can inject most of what
    you need into the test class, such as CDI beans, which is really handy for writing
    tests. Here is a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet illustrates `Arquillian` usage with its common characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Arquillian` runner is in use; it is the magic that will start the container
    (once), deploy the application (per test class by default), execute the tests
    (inherited from JUnit default behavior), undeploy the application when all tests
    of the class were executed, and shut down the container once the tests have been
    executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The static `@Deployment` method returns an `Archive<?>` describing what to deploy
    into the container (the application). You don't have to deploy the full application
    and you can change it per test if you want. For instance, in our sample, we didn't
    deploy our `DataSourceConfiguration`, which is pointing to MySQL, but instead,
    a `InMemoryDatabaseConfiguration`, which we can assume uses an embedded database
    such as derby or h2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a CDI injected into the test directly, our `QuoteService`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the test is a standard JUnit test with its life cycle (`@Before`/`@After`)
    and its test methods (`@Test`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you find the construction of the archive too complicated, there are some
    projects such as the TomEE `ziplock` module that simplify it by reusing the *current* project
    metadata (such as the `pom.xml` and compiled classes), which allows you to create
    the archive with a single method invocation: `Mvn.war()`. Some containers, including
    TomEE, allow you to deploy each archive a single time. But if your container doesn''t
    support it, you can use the `Arquillian` suite extension to achieve almost the
    same result. The overall goal is to group your tests to deploy a single time your
    application and save precious minutes on your tests execution duration.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Arquillian` also allows us to go further and execute the test either from
    inside the container—as the previous example—or from a client perspective using
    the `@RunAsClient` annotation. In this case, your test is no longer executed inside
    the container but in JUnit JVM (which can be the same or not, depending on whether
    your container uses another JVM or not). In any case, coupling `Arquillian` with
    ContiPerf allows you to validate the performance without many headaches. You just
    have to add the ContiPerf rule and annotations on the methods you want to validate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This is almost the same test as before, but we added more data to ensure the
    dataset size doesn't affect our performance much in the `@Before` method used
    to initialize the database. To integrate the test with ContiPerf, we added the
    ContiPerf annotations to our method and the ContiPerf rule. The last trick you
    can see is a system property in the archive creation, able to switch databases
    depending on the JVM configuration. It can be used to test against several databases
    or environments and validates you are complicant with all your target platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to run this example, you need to add these dependencies into your
    pom—considering that you will test it against GlassFish 5.0, you need to change
    the container dependency and `arquillian` container integration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: JUnit is the test framework we use and we import its Arquillian integration
    (`arquillian-junit-container`). Then, we import our `arquillian` container integration
    (`arquillian-glassfish-embedded-3.1`) and Java EE container, as we use an embedded
    mode (`glassfish-embedded-all`). Don't forget to add a ContiPerf dependency too
    if you plan to use it.
  prefs: []
  type: TYPE_NORMAL
- en: JMeter and build integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned in a previous section that JMeter can be used to build scenarii you
    execute against your application. It can also be programmatically executed—it
    is Java-based, after all—or executed through some of its Maven integrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use its Maven plugin from lazerycode ([https://github.com/jmeter-maven-plugin/jmeter-maven-plugin](https://github.com/jmeter-maven-plugin/jmeter-maven-plugin)),
    you can even configure the remote mode to have real stress testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This snippet defines the `jmeter` plugin to use `jmeter1.company.com` and `jmeter2.company.com`
    for the load testing. Servers will be initialized and destroyed before/after running
    the plans.
  prefs: []
  type: TYPE_NORMAL
- en: Without entering into the deep details of the plugin—you can find them on the
    related GitHub wiki—the plugin uses configurations stored in the project in `src/test/jmeter`
    by default. This is where you can put your scenarii (`.jmx` files).
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with this solution is to provide `jmeter[1,2].company.com` machines.
    Of course, you can create some machines and let them run, though, this is not
    a very good way to manage the AWS machines and it would be better to start them
    with the build  (allowing you to have concurrent builds, if needed, on multiple
    branches at the same time).
  prefs: []
  type: TYPE_NORMAL
- en: There are several solutions for that need, but the simplest is probably to install
    on the CI platform an AWS client (or plugin) and launch it before/after the Maven
    builds the corresponding commands to provision the machine, set the machine hosts
    in a build property, and pass it to the Maven build. It requires you to variabilize
    the plugin configuration, but nothing fancy, as Maven supports placeholders and
    system property input. Nonetheless, running the tests from your machine can be
    hard as you will need to provision yourself with the machine you will use. Thus,
    this reduces the shareable side of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget to ensure the task of shutting down the instance is always executed,
    even when the tests fail, or else you could leak some machines and have a billing
    surprise at the end of the month.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as JMeter is a mainstream solution, you can easily find platforms
    supporting it natively and handling the infrastructure for you. The main ones
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: BlazeMeter ([https://www.blazemeter.com/](https://www.blazemeter.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flood.IO ([https://flood.io/](https://flood.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redline.13 ([https://www.redline13.com/](https://www.redline13.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't hesitate to have a look at their websites, their prices, and compare them
    to what you can build yourself directly with AWS if you don't have dedicated machines
    on your CI. This can allow you to solve the environment and infrastructure issues
    performance tests often encounter.
  prefs: []
  type: TYPE_NORMAL
- en: Gatling and continuous integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like JMeter, Gatling has its own Maven plugin, but it also has some companion
    AWS integration ([https://github.com/electronicarts/gatling-aws-maven-plugin](https://github.com/electronicarts/gatling-aws-maven-plugin))
    natively integrated with Maven.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the official Gatling Maven plugin declaration can look like in
    your `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The first plugin (1) defines how to run Gatling. The default configuration will
    look for simulations in `src/test/scala`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This setup will locally run your simulations. So, you will likely migrate to
    the non-official plugin, yet integrated with AWS, to be able to control the injectors.
    Here is what the declaration can look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This plugin will integrate Gatling on AWS. It requires some AWS configuration
    (like your keys), but you will generally configure them outside the `pom.xml` so
    as not to put your credentials in a public place—properties in a profile in your
    `settings.xml` is a good place to start. Here are the properties you will need
    to define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Defining all these properties doesn't prevent you from changing the values through
    a system property. For instance, setting `-Dec2.instance.count=9` will allow you
    to change the number of injectors (nine instead of three). The first group of
    properties (ec2 ones) define how to create the AWS instances and how many to create.
    The second group (Gatling one) defines where Gatling is and how to run it. The
    third group defines the simulation to run. Finally, the last group (s3 one) defines
    where to upload the results of the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration is not yet functional, as it is not yet self-sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: It relies on Gatling distribution, which is not yet installed (`gatling.local.home`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It relies on a script we didn't create yet (`install-gatling.sh`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to not depend on a local Gatling installation, we can use Maven
    to download it. To do so, we just need the dependency plugin of Maven:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This configuration will extract the Gatling distribution into the `target/gatling/gatling-charts-highcharts-bundle-2.2.4` folder
    and let the plugin use it when it runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the script, you can use this one, which is for Fedora. However, it is easy
    to adapt to any distribution if you pick another image on EC2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This script does three main things:'
  prefs: []
  type: TYPE_NORMAL
- en: Increases `ulimit` to make sure the injector can use enough file handlers and
    not be limited by the OS configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installs Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloads Gatling from Maven centra, extracts the archive (like we did with
    the previous Maven plugin), but on the injector machines which don't have to use
    Maven, and finally cleans up the extracted archive (removing sample simulations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need any dependencies, you will need to create a shade (and use the default `jar-with-dependencies`
    as a classifier). You can do that using `maven-assembly-plugin` and the `single`
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your (benchmark) application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the two previous sections, we learned how to handle the injectors, but you
    will also need to deploy the application to test, if possible, on a dedicated
    instance for the test. This doesn't mean you need to forget what we looked at
    in the previous chapter, such as ensuring the machine is used in production and
    other potentially impacting services are running. Instead, you must ensure the
    machine(s) does what you think it does and not something unexpected that will
    impact the figures/tests.
  prefs: []
  type: TYPE_NORMAL
- en: Here, again, using cloud services to deploy your application can be the easiest
    solution. The simplest solution will likely rely on some cloud CLI (such as AWS
    CLI or `aws` command) or a small `main(String[])` you can write using the cloud
    provider client API or SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on if you code the deployment yourself (or not), it will be more or
    less easy to integrate with the Maven (or Gradle) build. As part of your project,
    an `exec-maven-plugin` can enable you to integrate it exactly where you need to
    in the Maven life cycle. Most of the time, this will be done before the performance
    test but after test compilation, or even after packaging the application (if you
    keep the performance test in the same module, which is feasible).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t code the deployment yourself, you will need to define the stages
    of your performance build:'
  prefs: []
  type: TYPE_NORMAL
- en: Compile/package the project and tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the application (and don't forget to reset the environment too if needed,
    including cleaning a database or JMS queue).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the performance test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Undeploy the application/shut down the created server (if relevant).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With Maven or Gradle, it is easy to skip some of the tasks, either with a flag
    or a profile, and consequently, you will end up with commands like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The first command, `(1)`, will build the full project but bypass the performance
    tests as we don't activate the `perf-tests` profile by default. The second command
    will deploy the application on the target environment using a custom implementation
    based on AWS SDK, for instance, potentially creating it from scratch. Don't forget
    to log what you do, even if you are waiting for something, or else you—or others
    —may think the process is hanging. Then, we run the performance test `(3)` and
    finally we undeploy the application with a command symmetric to `(2)`.
  prefs: []
  type: TYPE_NORMAL
- en: With such a solution, you need to ensure `(4)` is executed as soon as `(2)`
    is executed. In general, you will enforce it to always be executed and handle
    a quick exit condition if the expected environment to destroy doesnt exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'To orchestrate the steps, you can have a look at the Jenkins pipeline feature:
    it will give you a lot of flexibility to implement this type of logic in a straightforward
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: Going further overpasses the scope of this book, but to give you some pointers,
    the deployments can rely on Docker-based tools which makes it really easy to deploy
    on cloud platforms in general. Nevertheless, don't forget Docker is not a provisioning
    tool. And if your *recipe* (the steps for creating your instance) is not simple
    (installing software from the repository, copying/downloading your application,
    and starting it), then you can invest in a provisioning tool such as chef or puppet
    to be more flexible, powerful, and to avoid hacks.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration platform and performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jenkins is the most commonly used continuous integration platform. There are
    alternatives, such as Travis, but the ecosystem of Jenkins and the easiness to
    extend it makes it the clear leader for Java and enterprises applications.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we want to solve on the build/test execution platform for performance
    is the isolation of the builds, the goal being obviously to ensure the obtained
    figures are not affected by other builds.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, Jenkins provides several plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon EC2 Container Service Plugin ([https://wiki.jenkins.io/display/JENKINS/Amazon+EC2+Container+Service+Plugin](https://wiki.jenkins.io/display/JENKINS/Amazon+EC2+Container+Service+Plugin)):
    Allows you to run the builds (tests) in a dedicated machine created based on a
    Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throttle Concurrent Build Plugin ([https://github.com/jenkinsci/throttle-concurrent-builds-plugin/blob/master/README.md](https://github.com/jenkinsci/throttle-concurrent-builds-plugin/blob/master/README.md)):
    Allows you to control how many concurrent builds can be executed per project.
    Concretely, for performance, we want to ensure it is one per project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of configuration, you will need to make sure the performance tests
    are executed with an accurate configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: Regularly, using Jenkins scheduling, but probably not each time there is a commit
    or a pull-request. Depending on the criticity, the stability of your project,
    and the duration of your performance tests, it can be once a day or once a week.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The previous plugins—or equivalent—are in use and correctly configured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The build notifies the correct channels if they fail (mail, Slack, IRC, and
    so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It will also be important to ensure you store the history of the runs to be
    able to compare them, in particular if you don''t run the performance test with
    each commit, which would give you the exact commit, introducing a regression.
    To do that, you can use another Jenkins plugin which is exactly intended to store
    the history of common performance tools: the Performance Plugin ([https://wiki.jenkins.io/display/JENKINS/Performance+Plugin](https://wiki.jenkins.io/display/JENKINS/Performance+Plugin)).
    This plugin supports Gatling and JMeter, as well as a few other tools. It is a
    nice plugin, allowing you to visualize the reports directly from Jenkins, which
    is very handy when investigating some changes. What''s more, it is compatible
    with Jenkins pipeline scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through some common ways to ensure the performance
    of your application is under control and to limit the risk of getting unexpected
    bad surprises when you go into a benchmark phase, or worse, in production! Setting
    up simple tests or complete environments for a volatile (temporary) benchmark
    each week (or even each day) are very feasible steps, enabling a product to be
    delivered with a better quality level once the entry cost has been paid.
  prefs: []
  type: TYPE_NORMAL
- en: After having understood how Java EE instruments your application to let you
    focus on your business, how to monitor and instrument your application to optimize
    your application, and how to boost your application with some tuning or caching,
    we now know how to automatically control performance regressions to be able to
    fix them as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, you have now covered all the parts of product—or library—creation
    related to performance, and you are able to deliver highly performant software.
    You've got this!
  prefs: []
  type: TYPE_NORMAL
