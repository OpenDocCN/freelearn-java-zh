["```java\n$ cloc . \n 4 text files.\n 4 unique files. \n 2 files ignored.\n\ngithub.com/AlDanial/cloc v 1.72 T=0.03 s (96.4 files/s, 12465.5 lines/s)\n---------------------------------------------------------------------------\nLanguage         files    blank     comment     code\n---------------------------------------------------------------------------\nYAML             1        6         8           157\nPython           2        59        22          128\n---------------------------------------------------------------------------\nSUM:             3        65        30          285\n---------------------------------------------------------------------------\n```", "```java\nservice: gdax-lambda-arch\n\nprovider:\n  name: aws\n  runtime: python3.6\n  stage: ${env:ENV}\n  region: ${env:AWS_REGION}\n  iamRoleStatements:\n    - Effect: \"Allow\"\n      Action:\n        - \"s3:GetObject\"\n        - \"s3:ListBucket\"\n        - \"s3:PutObject\"\n      Resource:\n        - \"arn:aws:s3:::brianz-gdax-${env:ENV}-firehose\"\n        - \"arn:aws:s3:::brianz-gdax-${env:ENV}-firehose/*\"\n        - \"arn:aws:s3:::brianz-gdax-${env:ENV}-results\"\n        - \"arn:aws:s3:::brianz-gdax-${env:ENV}-results/*\"\n    - Effect: \"Allow\"\n      Action:\n        - \"dynamodb:PutItem\"\n      Resource: \"arn:aws:dynamodb:${self:provider.region}:*:table/brianz-gdax-${env:ENV}-realtime\"\n```", "```java\nresources:\n  Resources:\n    # This is the stream which the producer will write to. Any writes  \n     will trigger a lambda\n    # function. The Lambda function will need read access to this \n    stream.\n    GdaxKinesisStream:\n      Type: AWS::Kinesis::Stream\n      Properties:\n        Name: brianz-gdax-${env:ENV}-kinesis-stream\n        RetentionPeriodHours: 24\n        ShardCount: 1\n```", "```java\nfunctions:\n  ProcessPrice:\n    handler: handler.single\n    memorySize: 256\n    timeout: 3\n    events:\n      - stream:\n          type: kinesis\n          arn:\n            Fn::GetAtt:\n              - GdaxKinesisStream\n              - Arn\n    environment:\n      TABLE_NAME: brianz-gdax-${env:ENV}-realtime\n  CalculateMinuteView:\n    handler: handler.minute\n    memorySize: 256\n    timeout: 10\n    events:\n      - s3:\n          bucket: brianz-gdax-${env:ENV}-firehose\n          event: s3:ObjectCreated:*\n    environment:\n      DESTINATION_BUCKET: brianz-gdax-${env:ENV}-results\n  CalculateHourlyView:\n    handler: handler.hourly\n    memorySize: 512\n    timeout: 60\n    events:\n      - s3:\n          bucket: brianz-gdax-${env:ENV}-results\n          event: s3:ObjectCreated:*\n          rules:\n            - suffix: '59-minute.json'\n  CalculateDailyView:\n    handler: handler.daily\n    memorySize: 1024\n    timeout: 300\n    events:\n      - s3:\n          bucket: brianz-gdax-${env:ENV}-results\n          event: s3:ObjectCreated:*\n          rules:\n            - suffix: '23-hour.json'\n```", "```java\nfrom websocket import WebSocketApp\nfrom json import dumps, loads\n\nURL = \"wss://ws-feed.gdax.com\"\n\ndef on_message(_, message):\n    json_message = loads(message)\n    print(json_message)\n\ndef on_open(socket):\n    products = [\"BTC-USD\", \"ETH-USD\"]\n    channels = [\n        {\n            \"name\": \"ticker\",\n            \"product_ids\": products,\n        },\n    ]\n    params = {\n        \"type\": \"subscribe\",\n        \"channels\": channels,\n    }\n    socket.send(dumps(params))\n\ndef main():\n    ws = WebSocketApp(URL, on_open=on_open, on_message=on_message)\n    ws.run_forever()\n\nif __name__ == '__main__':\n    main()\n```", "```java\n{'best_ask': '8150', \n 'best_bid': '8149.99', \n 'high_24h': '8302.73000000', \n 'last_size': '0.33846794', \n 'low_24h': '8150.00000000', \n 'open_24h': '8021.01000000', \n 'price': '8150.00000000', \n 'product_id': 'BTC-USD', \n 'sequence': 5434939366, \n 'side': 'buy', \n 'time': '2018-03-18T22:48:35.185000Z', \n 'trade_id': 39905775, \n 'type': 'ticker', \n 'volume_24h': '29375.86790154', \n 'volume_30d': '633413.03952202'}\n```", "```java\nkinesis = boto3.client('kinesis')\n\ndef on_message(_, msg):\n    json_msg = json.loads(msg)\n    if 'time' in json_msg:\n        print('Publishing...')\n        response = kinesis.put_record(\n                StreamName='brianz-gdax-bz-kinesis-stream',\n                PartitionKey=json_msg['time'],\n                Data=msg + '|||',\n        ) \n        print(response)\n    else: \n        print(json_msg)\n```", "```java\nimport json\nimport os\nimport os.path\n\nfrom base64 import b64decode\nfrom datetime import datetime\n\nfrom lambda_arch.aws import (\n        get_matching_s3_keys,\n        list_s3_bucket,\n        read_body_from_s3,\n        write_to_dynamo_table,\n        write_to_s3,\n)\n\ndef single(event, context):\n    \"\"\"Process a single message from a kinesis stream and write it to \n       DynamoDB\"\"\"\n    record = event['Records'][0]\n    data = record['kinesis']['data']\n\n    # Append on a delimiter since we need to unpack messages which are \n    concatenated together when\n    # receiving multiple messages from Firehose.\n    data = json.loads(b64decode(data).decode().rstrip('|||'))\n\n    # Create our partition key\n    data['productTrade'] = '{product_id}|{time}|\n    {trade_id}'.format(**data)\n\n    write_to_dynamo_table(os.environ['TABLE_NAME'], data)\n```", "```java\ndef _get_bucket_and_key_from_event(event): \n    record = event['Records'][0]\n    s3 = record['s3'] \n    bucket = s3['bucket']['name'] \n    key = s3['object']['key'] \n    return (bucket, key)\n\ndef minute(event, context):\n    \"\"\"Process an S3 object uploaded to S3 from Kinesis Firehose.\n\n    The data format from Firehose is all of the messages from the  \n    `single` function above,\n    concatenated together. In order to read thse messages, we need to \n    decode them and split the\n    string by our own delimiter.\n\n    \"\"\"\n    bucket, key = _get_bucket_and_key_from_event(event)\n    data = read_body_from_s3(bucket, key).decode()\n\n    product_prices = {}\n\n    lines = [json.loads(l) for l in data.split('|||') if l]\n    times = []\n\n    for line in lines:\n        # Only keep track of buys for the average price, since sells \n        could be sell orders which are\n        # never executed.\n        if line.get('side') != 'buy':\n            continue\n\n        product_id = line['product_id']\n        price = float(line['price'])\n\n        times.append(line['time'])\n        if product_id not in product_prices:\n            product_prices[product_id] = {'prices': [price]}\n        else:\n            product_prices[product_id]['prices'].append(price)\n\n    if not product_prices:\n        return\n\n    # Calculate the average for each product\n    for key in product_prices:\n        prices = product_prices[key]['prices']\n        product_prices[key]['average'] = sum(prices) * 1.0 / \n        len(prices)\n\n    # Determine the most recent timestamp from the list of buys so we \n    can determine the key to\n    # write.\n    times.sort()\n    latest_time = times[-1]\n    latest_dt = datetime.strptime(latest_time, DT_FORMAT)\n\n    destination_bucket = os.environ['DESTINATION_BUCKET']\n    new_key = latest_dt.strftime('%Y/%m/%d/%H/%M-minute.json')\n    new_payload = json.dumps(product_prices, indent=2)\n\n    print('Uploading to', destination_bucket, new_key)\n\n    write_to_s3(destination_bucket, new_key, new_payload)\n\ndef _aggregate_prices(event, period='hour'):\n    \"\"\"Aggregate average prices for a particular time slice\"\"\"\n    bucket, key = _get_bucket_and_key_from_event(event)\n    key_root = os.path.dirname(key)\n\n    product_prices = {}\n\n    for key in get_matching_s3_keys(bucket, prefix=key_root + '/', \n    suffix='-minute.json'):\n        data = read_body_from_s3(bucket, key).decode()\n        minute_prices = json.loads(data)\n\n        for product_id, payload in minute_prices.items():\n            prices = payload['prices']\n            if product_id not in product_prices:\n                product_prices[product_id] = {'prices': prices}\n            else:\n                product_prices[product_id]['prices'].extend(prices)\n\n    for key in product_prices:\n        prices = product_prices[key]['prices']\n        average_price = sum(prices) * 1.0 / len(prices)\n        product_prices[key]['average'] = average_price\n\n    new_key = '%s-%s.json' % (key_root, period)\n    new_payload = json.dumps(product_prices, indent=2)\n\n    print('Uploading to', bucket, new_key)\n\n    write_to_s3(bucket, new_key, new_payload)\n\ndef hourly(event, context):\n    _aggregate_prices(event, period='hour')\n\ndef daily(event, context):\n    _aggregate_prices(event, period='day')\n```", "```java\n{\n  \"BTC-USD\": {\n    \"prices\": [\n      8173.59,\n      8173.59,\n      8173.59,\n      8173.59,\n      8173.59,\n      8173.59,\n      8173.59,\n      8173.59,\n      8174.99,\n      8176.17,\n      8176.17\n    ],\n    \"average\": 8174.186363636362\n  },\n  \"ETH-USD\": {\n    \"prices\": [\n      533.01,\n      533.01,\n      533.01,\n      533.01,\n      533.01,\n      533.01\n    ],\n    \"average\": 533.0100000000001\n  }\n}\n```"]