<html><head></head><body>
      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Continuous Performance Evaluation</h1>
            
         </header>
         
         
         <article>
            
            
            <p class="mce-root">We saw in the previous chapter that driving a benchmark requires work and is not a
               small operation. However, it doesn't solve all the needs of software development,
               since you only do it from time to time. Moreover, between two benchmarks, you can
               have huge performance regression not captured by the unit tests validating the features
               and behavior of your application.
            </p>
            
            <p>To solve that, it is a good idea to try to add dedicated tests for performance. This
               is what this chapter is about. Thus, we will learn:
            </p>
            
            <ul>
               
               <li>Which tools you can use to <em>test</em> the performance
               </li>
               
               <li>How you can set up some continuous integration for performance</li>
               
               <li>How to write tests for the performance in such a context</li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Writing performance tests – the pitfalls</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Performance tests have some challenges you need to take into account when you write
               your tests to avoid having a lot of false positive results (not passing tests where
               the performance is actually acceptable).
            </p>
            
            <p>The most common issues you will encounter are:</p>
            
            <ul>
               
               <li>How to manage the external systems: We know that external systems are very important
                  in applications today, but it is not always trivial to have them during tests.
               </li>
               
               <li>How to be deterministic: Continuous integration/test platforms are often shared. How
                  do you ensure you have the resources needed in order to have a deterministic execution
                  time and to not be slown down, as another build is using all the available resources?
               </li>
               
               <li>How to handle the infrastructure: To do an end-to-end test, you need multiple injectors
                  (clients) and probably multiple backend servers. How do you ensure you have them available
                  without them being too expensive if you use cloud platforms such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>)?
               </li>
               
            </ul>
            
            <p>You can see the setup of performance tests as a benchmark preparation phase. The main
               difference will be that you should not rely on external consulting in the long-term,
               and you will ensure the benchmark iterations are almost completely automatic—<em>almost</em> because you will still need to take some actions if the tests fail, otherwise there
               is no point having a continuous system.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Writing performance tests</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Performance tests are present at all stages in a Java EE application, and so, there
               are several ways to write a performance test. In this section, we will go through
               a few main ones, starting with the simplest one (algorithm validation).
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">JMH – the OpenJDK tool</h1>
            
         </header>
         
         
         <article>
            
            
            <p><strong>Java Microbenchmark Harness</strong> (<strong>JMH</strong>) is a small library developed by the OpenJDK team—yes, the same one doing the JVM—which
               enables you to easily develop microbenchmarks.
            </p>
            
            <p>A microbenchmark designs a benchmark on a very small part of an application. Most
               of the time, you can see it as a <em>unit benchmark</em>, using the analogy with unit tests.
            </p>
            
            <p>Yet, it is something important when setting up performance tests as it will allow
               you to quickly identify a critical performance regression introduced by a recent change.
               The idea is to associate each benchmark with a small part of code, whereas a benchmark
               will include several layers. So, if the benchmark fails, you will spend a lot of time
               identifying why, instead of simply checking the related code part.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Writing a benchmark with JMH</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Before being able to write code with JMH, you need to add it as a dependency of your
               project. We will use Maven syntax in the following example, but it has equivalents
               for Gradle, and so on.
            </p>
            
            <p>The first thing to do is to add into your <kbd>pom.xml</kbd> the following dependencies; we will use the scope <kbd>test</kbd> since the dependencies are only needed for our performance tests and not the "main"
               code:
            </p>
            <pre><span>&lt;</span><span>dependencies</span><span>&gt;</span><br/>  <span>&lt;!-- your other dependencies --</span><span>&gt;<br/></span><br/>  <span>&lt;</span><span>dependency</span><span>&gt;</span><br/>    <span>&lt;</span><span>groupId</span><span>&gt;</span>org.openjdk.jmh<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>    <span>&lt;</span><span>artifactId</span><span>&gt;</span>jmh-core<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>    <span>&lt;</span><span>version</span><span>&gt;</span>${jmh.version}<span>&lt;/</span><span>version</span><span>&gt;</span><br/>    <span>&lt;</span><span>scope</span><span>&gt;</span>test<span>&lt;/</span><span>scope</span><span>&gt;</span><br/>  <span>&lt;/</span><span>dependency</span><span>&gt;</span><br/>  <span>&lt;</span><span>dependency</span><span>&gt;</span><br/>    <span>&lt;</span><span>groupId</span><span>&gt;</span>org.openjdk.jmh<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>    <span>&lt;</span><span>artifactId</span><span>&gt;</span>jmh-generator-annprocess<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>    <span>&lt;</span><span>version</span><span>&gt;</span>${jmh.version}<span>&lt;/</span><span>version</span><span>&gt;</span><br/>    <span>&lt;</span><span>scope</span><span>&gt;</span>test<span>&lt;/</span><span>scope</span><span>&gt;</span> <span>&lt;!-- should be provided but for tests only this<br/>    is more accurate --&gt;<br/></span><span>  </span><span>&lt;/</span><span>dependency</span><span>&gt;</span><br/><span>&lt;/</span><span>dependencies</span><span>&gt;</span></pre>
            <p>As of writing this book, the <kbd>jmh.version</kbd> property can take the <kbd>1.19</kbd> value. The <kbd>jmh-core</kbd> dependency will bring you JMH itself with its annotation-based API, and the <kbd>jmh-generator-annprocess</kbd> brings an annotation processor backbone—used when compiling the test classes/benchmarks—which
               will generate some sort of index file(s) used by the runtime to execute the benchmarks
               and the benchmark classes themselves.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The state</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Once you have the right dependencies, you can develop your benchmarks. The API is
               quite straightforward. It uses the notion of a <em>state</em> and a state has a life cycle and (volatile) storage associated with the benchmark.
               The life cycle of a state is defined by marking methods with:
            </p>
            
            <ul>
               
               <li><kbd>@Setup</kbd> to execute an initialization task
               </li>
               
               <li><kbd>@Teardown</kbd> to release any created resource in the setup method
               </li>
               
            </ul>
            
            <p>State classes can also contain fields decorated with <kbd>@Param</kbd> to make them contextualized and configurable (such as enabling them to get a different
               target URL depending on the execution for example).
            </p>
            
            <p>A state class is marked with <kbd>@State</kbd>, which takes, as parameter, the scope of the state instance for a benchmark:
            </p>
            
            <ul>
               
               <li><kbd>Benchmark</kbd> means the state will be a singleton in the scope of the JVM.
               </li>
               
               <li><kbd>Group</kbd> defines the state as a singleton per group. A group is a way to put multiple benchmark
                  methods (scenarii) in the same thread bucket during the execution.
               </li>
               
               <li><kbd>Thread</kbd> defines the state as a singleton per thread (a bit like <kbd>@RequestScoped</kbd> for CDI).
               </li>
               
            </ul>
            
            <p>Note that you can change how the life cycle is managed a bit by passing t<span>he runtime</span> to <kbd>@Setup</kbd> and <kbd>@Teardown</kbd> to a different <kbd>Level</kbd>:
            </p>
            
            <ul>
               
               <li><kbd>Trial</kbd>: This is the default, and as we just explained, the benchmark is seen as a whole
               </li>
               
               <li><kbd>Iteration</kbd>: The life cycle is executed per iteration (warmup or measurement)
               </li>
               
               <li><kbd>Invocation</kbd>: The life cycle of the state instance is executed per method; this is not recommended
                  in practice to avoid some measurement errors
               </li>
               
            </ul>
            
            <p>So, once you have your state object, you define the class containing your benchmarks
               which are nothing other than methods marked with <kbd>@Benchmark</kbd>. Then, you have several annotations you can add to the method to customize the benchmark
               execution:
            </p>
            
            <ul>
               
               <li><kbd>@Measurement</kbd>: To customize the number of iterations or the duration of the benchmark.
               </li>
               
               <li><kbd>@Warmup</kbd> : The same as <kbd>@Measurement</kbd>, but for the warmup (which is a sort of pre-execution of the benchmark not taken
                  into account for the measurement, the goal being to only measure metrics on a hot
                  JVM).
               </li>
               
               <li><kbd>@OuputTimeUnit</kbd>: To customize the unit used for the metrics.
               </li>
               
               <li><kbd>@Threads</kbd>: To customize the number of threads used for the benchmark. You can see it as the
                  number of <em>users</em>.
               </li>
               
               <li><kbd>@Fork</kbd>: The benchmark will be executed in another JVM to avoid test side effects. This annotation
                  allows you to add custom parameters to the forked JVM.
               </li>
               
               <li><kbd>@OperationsPerInvocation</kbd>: If you have a loop in your benchmark method, this option (a number) will normalize
                  the measurement. For instance, if you execute five times the same operation in your
                  benchmark and set this value to five, then the execution time will be divided by five.
               </li>
               
               <li><kbd>@Timeout</kbd>: It lets you define a maximum duration for the benchmark execution. JMH will interrupt
                  the threads if overpassed.
               </li>
               
               <li><kbd>@CompilerControl</kbd>: To customize the way the annotation processor generates the code. For our use case,
                  you rarely need this option, but while tuning some code portions, it can be interesting
                  to test it.
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Creating your first JMH benchmark</h1>
            
         </header>
         
         
         <article>
            
            
            <p>After all that theory, here is a basic benchmark developed with JMH:</p>
            <pre><span>public class </span>QuoteMicrobenchmark {<br/>    <span>@Benchmark<br/></span><span>    </span><span>public void </span>compute() {<br/>        <span>//<br/></span><span>    </span>}<br/>}</pre>
            <p>In this case, we have a single benchmark scenario called <kbd>compute</kbd>. It doesn't use any states and didn't customize any thread or fork count.
            </p>
            
            <p>In practice, this will not be enough and you will often need a state to get a service.
               So, it will more look like this:
            </p>
            <pre><span>public class </span>QuoteMicrobenchmark {<br/>    <span>@Benchmark<br/></span><span>    </span><span>public void </span>compute(<span>final </span>QuoteState quoteState) {<br/>        quoteState.<span>service</span>.findByName(<span>"test"</span>);<br/>    }<br/><br/>    <span>@State</span>(Scope.<span>Benchmark</span>)<br/>    <span>public static class </span>QuoteState {<br/>        <span>private </span>QuoteService <span>service</span>;<br/><br/>        <span>@Setup<br/></span><span>        </span><span>public void </span>setup() {<br/>            <span>service </span>= <span>new </span>QuoteService();<br/>        }<br/>    }<br/>}</pre>
            <p>Here, we created a nested <kbd>QuoteState</kbd> instance which will be responsible for getting us a service, and we injected it to
               the benchmark method (<kbd>compute</kbd>) and used it to get our service instance. This avoids creating an instance per iteration,
               and thus, avoids taking into account the container startup duration.
            </p>
            
            <p>This sort of implementation works well until you need an actual container. But when
               you need a container, it will require some mocking which is absolutely something you
               should get rid of—even for unit testing—as it will not represent anything close to
               the real deployment until your code doesn't use Java EE at all (which also means you
               don't need to mock anything if that is the case).
            </p>
            
            <p>If you use a CDI 2.0 implementation supporting standalone API—and your application
               doesn't need more for what you want to test—then you can change the state to start/stop
               the CDI container and look up the services you need with the new CDI standalone API:
            </p>
            <pre><span>@State</span>(Scope.<span>Benchmark</span>)<br/><span>public class </span>QuoteState {<br/>    <span>private </span>QuoteService <span>service</span>;<br/>    <span>private </span>SeContainer <span>container</span>;<br/><br/>    <span>@Setup<br/></span><span>    </span><span>public void </span>setup() {<br/>        <span>container </span>= SeContainerInitializer.<span>newInstance</span>().initialize();<br/>        <span>service </span>= <span>container</span>.select(QuoteService.<span>class</span>).get();<br/>    }<br/><br/>    <span>@TearDown<br/></span><span>    </span><span>public void </span>tearDown() {<br/>        <span>container</span>.close();<br/>    }<br/>}</pre>
            <p>Here, the setup starts the <kbd>container</kbd>—you can customize the classes to deploy before calling <kbd>initialize()</kbd>—and then looks up the <kbd>QuoteService</kbd> using the <kbd>container</kbd> instance API. The <kbd>tearDown</kbd> method just closes the container properly.
            </p>
            
            <p>Though, in GlassFish, you can't use that new API. But there is the <kbd>EJBContainer</kbd>, coming from Java EE 6, allowing you to do the same thing combined with the <kbd>CDI</kbd> class:
            </p>
            <pre><span>@State</span>(Scope.<span>Benchmark</span>)<br/><span>public class </span>QuoteState {<br/>    <span>private </span>QuoteService <span>service</span>;<br/>    <span>private </span>EJBContainer <span>container</span>;<br/><br/>    <span>@Setup<br/></span><span>    </span><span>public void </span>setup() {<br/>        <span>container </span>= EJBContainer.<span>createEJBContainer</span>(<span>new </span>HashMap&lt;&gt;());<br/>        <span>service </span>= CDI.<span>current</span>().select(QuoteService.<span>class</span>).get();<br/>    }<br/><br/>    <span>@TearDown<br/></span><span>    </span><span>public void </span>tearDown() {<br/>        <span>container</span>.close();<br/>    }<br/>}</pre>
            <p>This uses exactly the same logic as before, except the <kbd>container</kbd> is started based on the <kbd>EJBContainer</kbd> API. This looks great, but it will not always work with all containers. One pitfall
               is that if you don't have any EJB, some containers will just not even try to deploy
               the application.
            </p>
            
            <p>You can find several workarounds, but the saner solution is to check if you really
               need the full container or just a subset—such as just CDI—and in this case, you just
               start this subset (Weld or OpenWebBeans only for a CDI using a previous state). If
               you really need a full container and your vendor doesn't support any of the two previous
               ways of starting a container, you can also use a vendor-specific API, mock the container
               (but take care, you will bypass some execution time <em>cost</em>), use another vendor if close enough to your final container, or implement it with
               a third-party container manager such as the <kbd>arquillian</kbd> container API.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">BlackHole to be a star</h1>
            
         </header>
         
         
         <article>
            
            
            <p>JMH provides a particular class—<kbd>org.openjdk.jmh.infra.Blackhole</kbd>—that can seem weird as it mainly only allows you to <kbd>consume</kbd> an instance. It is retrieved by injecting it into the parameters of the benchmark:
            </p>
            <pre><span>public void </span>compute(<span>final </span>QuoteState quoteState, <span>final </span>Blackhole blackhole) {<br/>    blackhole.consume(quoteState.<span>service</span>.findByName(<span>"test"</span>));<br/>}</pre>
            <p>Why consume the returned value of a method if we do nothing about it? Remember that
               the Oracle JVM has what is called the <strong>just in time compilation</strong> (<strong>JIT</strong>) which optimizes the code at runtime depending on the statistics of the code paths.
               If you don't call that <kbd>consume</kbd> method, you can end up not measuring the actual code you want, but some very optimized
               flavors of this code, since, most of the time, in your benchmark methods, you will
               ignore part of the returned values which can, hence, be optimized with the <em>dead-code elimination</em> rule.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Running the benchmarks</h1>
            
         </header>
         
         
         <article>
            
            
            <p>JMH provides a main-friendly API to run the benchmarks. It is based on defining the
               execution options which are more or less the options you can associate with a benchmark
               method (the one we saw in previous section with annotations) and a list of includes/excludes
               to select the classes/methods to run. Then, you pass these options to a runner and
               call its <kbd>run()</kbd> method:
            </p>
            <pre><span>final </span>Collection&lt;RunResult&gt; results = <span>new </span>Runner(<br/>    new OptionsBuilder()<br/>      .include("com.packt.MyBenchmark")<br/>      .build())<br/> .run();</pre>
            <p>Here, we build a simple <kbd>Options</kbd> instance from the <kbd>OptionsBuilder</kbd> by including a benchmark class we want to include. Finally, we run it through the
               method with the same name as the runner, and collect the benchmark's results in a
               collection.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Integrating JMH with JUnit</h1>
            
         </header>
         
         
         <article>
            
            
            <p>There is no official JUnit integration of JMH. In spite of that, it is not that hard
               to do it yourself. There are lots of possible designs, but in the context of this
               book we will do the following:
            </p>
            
            <ul>
               
               <li>Our integration will go through a JUnit runner</li>
               
               <li>The benchmark classes will be identified by extracting the nested classes of the test
                  class which will avoid using any scanning to find the benchmark classes or explicit
                  listing
               </li>
               
               <li>We will introduce an <kbd>@ExpectedPerformances</kbd> annotation to be able to add assertions based on the execution
               </li>
               
            </ul>
            
            <p>Structurally, a microbenchmark test using this structure will look like this:</p>
            <pre><span>@RunWith</span>(JMHRunner.<span>class</span>)<br/><span>public class </span>QuoteMicrobenchmarkTest {<br/>    <span>@ExpectedPerformances</span>(score = <span>2668993660.</span>)<br/>    <span>public static class </span>QuoteMicrobenchmark {<br/>        <span>@Benchmark<br/></span><span>        @Fork</span>(<span>1</span>) <span>@Threads</span>(<span>2</span>)<br/>        <span>@Warmup</span>(iterations = <span>5</span>) <span>@Measurement</span>(iterations = <span>50</span>)<br/>        @ExpectedPerformances(score = 350000.)<br/>        <span>public void </span>findById(<span>final </span>QuoteState quoteState, <span>final<br/>        </span>Blackhole blackhole) {<br/>            blackhole.consume(quoteState.<span>service</span>.findById(<span>"test"</span>));<br/>        }<br/><br/>        // other benchmark methods<br/>    }<br/><br/>    <span>public static class </span>CustomerMicrobenchmark {<br/>        // benchmark methods<br/>    }<br/>}</pre>
            <p>Once you have the overall test structure, you just need to add the benchmarks themselves
               in the right nested class. Concretely, a benchmark class can look like this:
            </p>
            <pre>public static/*it is a nested class*/ class QuoteMicrobenchmark {<br/>    @Benchmark<br/>    @Fork(1) @Threads(2)<br/>    @Warmup(iterations = 5) @Measurement(iterations = 50)<br/>    @ExpectedPerformances(score = 350000.)<br/>    public void findById(final QuoteState quoteState, final Blackhole<br/>    blackhole) {<br/>        blackhole.consume(quoteState.service.findById("test"));<br/>    }<br/><br/>    @Benchmark<br/>    @Fork(1) @Threads(2) @Warmup(iterations = 10)<br/>    @Measurement(iterations = 100)<br/>    @ExpectedPerformances(score = 2660000.)<br/>    public void findByName(final QuoteState quoteState, final Blackhole<br/>    blackhole) {<br/>        blackhole.consume(quoteState.service.findByName("test"));<br/>    }<br/><br/>    @State(Scope.Benchmark)<br/>    public static class QuoteState {<br/>        private QuoteService service;<br/><br/>        @Setup public void setup() { service = new QuoteService(); }<br/>    }<br/>}</pre>
            <p>In this test class, we have two benchmark classes using a different state (a different
               service in our EE application) and each of them can have a different benchmark method
               counts. The execution is handled by the runner set with <kbd>@RunWith</kbd>, using the standard JUnit (4) API. We will note the <kbd>@ExpectedPerformances</kbd> usage on all benchmarks.
            </p>
            
            <div class="packt_tip">If you have already migrated to JUnit 5, or are using TestNG, the same sort of integration
               is possible but you will use extensions for JUnit 5 and probably an abstract class
               or a listener for TestNG.
            </div>
            
            <p>Before seeing how to implement that runner, we must have this <kbd>@ExpectedPerformances</kbd> annotation. It is the one allowing us to assert the performance of our benchmark.
               So, we at least need:
            </p>
            
            <ul>
               
               <li>A score (duration but without specifying the unit, since JMH supports customizing
                  the report unit)
               </li>
               
               <li>A tolerance on the score as there is no way you get exactly the same score twice</li>
               
            </ul>
            
            <p>To do that, we can use this simple definition:</p>
            <pre><span>@Target</span>(<span>METHOD</span>)<br/><span>@Retention</span>(<span>RUNTIME</span>)<br/><span>public </span>@<span>interface </span><span>ExpectedPerformances </span>{<br/>    <span>double </span>score();<br/>    <span>double </span>scoreTolerance() <span>default </span><span>0.1</span>;<br/>}</pre>
            <p>As you can see, we tell the user to define a score, but we use a default score tolerance
               of <kbd>0.1</kbd>. In our implementation, we will consider it as being a percentage (10%). This will
               avoid frequent failure if the machine load is not very stable when running your job.
               Don't hesitate to decrease that value or even make it configurable through a system
               property.
            </p>
            
            <p>To make our previous snippet work, we need to implement a JUnit runner. This is a
               design choice, but you can also use a rule, exposing some programmatic API or not.
               To keep it simple, we will not do it here and just consider the whole benchmark setup
               as done through the annotations. However, for a real-life project, it can be handy
               to enable the environment (system properties) to customize the annotation values a
               bit. A common implementation is to use them as a template and multiple all numeric
               values by a configured ratio on the JVM to let the tests run on any machine.
            </p>
            
            <p>Our runner will have these four roles:</p>
            
            <ul>
               
               <li>Find the benchmark classes</li>
               
               <li>Validate the model of these classes—typically, validate that each benchmark has an <kbd>@ExpectedPerformances</kbd></li>
               
               <li>Run each benchmark</li>
               
               <li>Validate the expectations to make the test fail if we have a regression</li>
               
            </ul>
            
            <p>It is simpler for us to extend <kbd>ParentRunner&lt;Class&lt;?&gt;&gt;</kbd>. We could use <kbd>BlockJUnit4ClassRunner</kbd>, but it is based on methods and JMH only supports filtering the executions per class.
               So, let's stick to it for now. If you put a single benchmark per nested class, then
               you can simulate a run per method behavior.
            </p>
            
            <p>The first thing we need to do is to find our benchmark classes. With the JUnit runner
               API, you can access the test class with <kbd>getTestClass()</kbd>. To find our benchmark, we just need to check the nested classes of that class through <kbd>getClasses()</kbd> and make sure the class has at least one <kbd>@Benchmark</kbd> on a method to validate that it is a JMH class:
            </p>
            <pre><span>children </span>= Stream.<span>of</span>(getTestClass().getJavaClass().getClasses())<br/>        .filter(benchmarkClass -&gt;<br/>        Stream.<span>of</span>(benchmarkClass.getMethods())<br/>        .anyMatch(m -&gt; m.isAnnotationPresent(<span>Benchmark</span>.<span>class</span>)))<br/>        .collect(<span>toList</span>());</pre>
            <p>We go through all nested classes of the test class, then keep (or filter) only the
               ones with a benchmark method at the least. Note that we store the result as we will
               need multiple times in our runner.
            </p>
            
            <p>Then, the validation is as simple as going over these classes and their benchmark
               methods to validate they have the <kbd>@ExpectedPerformances</kbd> annotation:
            </p>
            <pre>errors.addAll(<span>children</span>.stream()<br/>        .flatMap(c -&gt; Stream.<span>of</span>(c.getMethods()).filter(m -&gt;<br/>        m.isAnnotationPresent(<span>Benchmark</span>.<span>class</span>)))<br/>        .filter(m -&gt;<br/>        !m.isAnnotationPresent(<span>ExpectedPerformances</span>.<span>class</span>))<br/>        .map(m -&gt; <span>new </span>IllegalArgumentException(<span>"No<br/>        @ExpectedPerformances on " </span>+ m))<br/>        .collect(<span>toList</span>()));</pre>
            <p class="mce-root">Here, to list the errors of the JUnit validation, we add an exception per method annotated
               with <kbd>@Benchmark</kbd> but without an <kbd>@ExpectedPerformances</kbd>. We do it first by converting the classes to a stream of benchmark methods, then
               keeping only the ones without the <kbd>@ExpectedPerformances</kbd> annotation to keep the <em>set vision</em>.
            </p>
            
            <p>Finally, the last key part of the runner code is to convert a class to an actual execution:</p>
            <pre><span>final </span>Collection&lt;RunResult&gt; results;<br/><span>try </span>{<br/>    results = <span>new </span>Runner(buildOptions(<span>benchmarkClass</span>)).run();<br/>} <span>catch </span>(<span>final </span>RunnerException e) {<br/>    <span>throw new </span>IllegalStateException(e);<br/>}<br/><br/><span>// for all benchmarks assert the performances from the results<br/></span><span>final </span>List&lt;AssertionError&gt; errors = Stream.<span>of</span>(<span>benchmarkClass</span>.getMethods())<br/>        .filter(m -&gt; m.isAnnotationPresent(<span>Benchmark</span>.<span>class</span>))<br/>        .map(m -&gt; {<br/>            <span>final </span>Optional&lt;RunResult&gt; methodResult = <span>results</span>.stream()<br/>                  .filter(r -&gt;<br/>                  <span>m</span>.getName().equals(r.getPrimaryResult().getLabel()))<br/>                  .findFirst();<br/>                  <span>assertTrue</span>(m + <span>" didn't get any result"</span>,<br/>                  methodResult.isPresent());<br/><br/>            <span>final </span><span>ExpectedPerformances </span>expectations =<br/>            m.getAnnotation(<span>ExpectedPerformances</span>.<span>class</span>);<br/>            <span>final </span>RunResult result = <span>results</span>.iterator().next();<br/>            <span>final </span>BenchmarkResult aggregatedResult =<br/>            result.getAggregatedResult();<br/><br/>            <span>final double </span>actualScore =<br/>            aggregatedResult.getPrimaryResult().getScore();<br/>            <span>final double </span>expectedScore = expectations.score();<br/>            <span>final double </span>acceptedError = expectedScore *<br/>            expectations.scoreTolerance();<br/>            <span>try </span>{ <span>// use assert to get a common formatting for errors<br/></span><span>                </span><span>assertEquals</span>(m.getDeclaringClass().getSimpleName() +<br/>                <span>"#" </span>+ m.getName(), expectedScore, actualScore,<br/>                acceptedError);<br/>                <span>return null</span>;<br/>            } <span>catch </span>(<span>final </span>AssertionError ae) {<br/>                <span>return </span>ae;<br/>            }<br/>        }).filter(Objects::<span>nonNull</span>).collect(<span>toList</span>());<br/><span>if </span>(!errors.isEmpty()) {<br/>    <span>throw new </span>AssertionError(errors.stream()<br/>            .map(Throwable::getMessage)<br/>            .collect(Collectors.<span>joining</span>(<span>"</span><span>\n</span><span>"</span>)));<br/>}</pre>
            <p>First, we execute the benchmark holder class and collect the results. Then, we iterate
               over the benchmark methods and for each of them, we extract the result (primary result
               label is the method name). Finally, we extract our <kbd>@ExpectedPerformances</kbd> constraint and compare it with the primary result of the test. The little trick here
               is to catch the <kbd>AssertionError</kbd> the assert can throw, and collect them all in a list to convert them in another <kbd>AssertionError</kbd>. You can just format the message the way you want, but doing it this way keeps standard
               JUnit formatting of the errors. The other tip here is to put the benchmark class and
               method in the error message to make sure you can identify which benchmark failed.
               The alternative way can be to introduce another annotation to use a custom name for
               each benchmark.
            </p>
            
            <p>Now that we have looked at all the small technical points, let's put it all together.
               To start, we will define that our runner will use <kbd>Class</kbd> children, which will represent each of our nested classes:
            </p>
            <pre>public class JMHRunner extends ParentRunner&lt;Class&lt;?&gt;&gt; {<br/>  private List&lt;Class&lt;?&gt;&gt; children;<br/><br/>  public JMHRunner(final Class&lt;?&gt; klass) throws InitializationError {<br/>    super(klass);<br/>  }<br/><br/>  @Override<br/>  protected List&lt;Class&lt;?&gt;&gt; getChildren() {<br/>    return children;<br/>  }<br/><br/>  @Override<br/>  protected Description describeChild(final Class&lt;?&gt; child) {<br/>    return Description.createTestDescription(getTestClass().getJavaClass(), child.getSimpleName());<br/>  }</pre>
            <p><span>In the code executed by the parent constructor (<kbd>super(klass)</kbd>), JUnit will trigger a test validation where we compute the children we previously
                  saw to be able to return them in </span><kbd>getChildren()</kbd><span> and to let JUnit handle all our nested classes. We implement </span><kbd>describeChild</kbd><span> to let JUnit associate a </span><kbd>Description</kbd><span> with each nested class and to have smoother integration with IDE (the goal is to show
                  them in a tree when you run the tests). To compute the children and validate them,
                  we can use this JUnit</span> implementation of <kbd>collectInitializationErrors</kbd>—using this hook avoids computing it multiple times per test class:
            </p>
            <pre>  @Override<br/>  protected void collectInitializationErrors(final List&lt;Throwable&gt;<br/>  errors) {<br/>    super.collectInitializationErrors(errors);<br/><br/>    children = Stream.of(getTestClass().getJavaClass().getClasses())<br/>      .filter(benchmarkClass -&gt; Stream.of(benchmarkClass.getMethods())<br/>        .anyMatch(m -&gt; m.isAnnotationPresent(Benchmark.class)))<br/>      .collect(toList());<br/><br/>    errors.addAll(children.stream()<br/>        .flatMap(c -&gt; Stream.of(c.getMethods())<br/>        .filter(m -&gt; m.isAnnotationPresent(Benchmark.class)))<br/>        .filter(m -&gt;<br/>        !m.isAnnotationPresent(ExpectedPerformances.class))<br/>        .map(m -&gt; new IllegalArgumentException("No<br/>        @ExpectedPerformances on " + m))<br/>        .collect(toList()));<br/>  }</pre>
            <p>Then, we need to ensure we can run our children (benchmarks) properly. To do that,
               we extend another JUnit hook which is designed to run each child. The only thing we
               take care of is mainly ensuring JUnit <kbd>@Ignored</kbd> is supported for our children:
            </p>
            <pre>  @Override<br/>  protected boolean isIgnored(final Class&lt;?&gt; child) {<br/>    return child.isAnnotationPresent(Ignore.class);<br/>  }<br/><br/>  @Override<br/>  protected void runChild(final Class&lt;?&gt; child, final RunNotifier <br/>  notifier) {<br/>    final Description description = describeChild(child);<br/>    if (isIgnored(child)) {<br/>      notifier.fireTestIgnored(description);<br/>    } else {<br/>      runLeaf(benchmarkStatement(child), description, notifier);<br/>    }<br/>  }</pre>
            <p><span> In </span><kbd>runChild()</kbd>,<span> we delegate the execution to the JUnit engine by adding, as an implementation of
                  our test execution, the code we saw just before based on JMH runner but wrapped in
                  a JUnit </span><kbd>Statement</kbd><span> to let it be integrated with JUnit notifiers. </span>Now, we just need this execution implementation (<kbd>benchmarkStatement</kbd>). This is done by completing the class with the following code:
            </p>
            <pre>  private Statement benchmarkStatement(final Class&lt;?&gt; benchmarkClass) {<br/>    return new Statement() {<br/>      @Override<br/>      public void evaluate() throws Throwable {<br/>        final Collection&lt;RunResult&gt; results;<br/>        try {<br/>          results = new Runner(buildOptions(benchmarkClass)).run();<br/>        } catch (final RunnerException e) {<br/>          throw new IllegalStateException(e);<br/>        }<br/><br/>        assertResults(benchmarkClass, results);<br/>      }<br/>    };<br/>  }<br/><br/>  // all options will use JMH annotations so just<br/>  include the class to run<br/>  private Options buildOptions(final Class&lt;?&gt; test) {<br/>    return new OptionsBuilder()<br/>        .include(test.getName().replace('$', '.'))<br/>        .build();<br/>  }<br/><br/></pre>
            <p>This reuses everything we previously saw; the <kbd>buildOptions</kbd> method will force the JMH runner to use the annotations on the benchmark for the
               execution configuration and we just include a single test at a time. Finally, we implement
               the <kbd>assertResults</kbd> method as we explained before:
            </p>
            <pre>  public void assertResults(final Class&lt;?&gt; benchmarkClass, final<br/>  Collection&lt;RunResult&gt; results) {<br/>    // for all benchmarks assert the performances from the results<br/>    final List&lt;AssertionError&gt; errors =<br/>    Stream.of(benchmarkClass.getMethods())<br/>        .filter(m -&gt; m.isAnnotationPresent(Benchmark.class))<br/>        .map(m -&gt; {<br/>          final Optional&lt;RunResult&gt; methodResult = results.stream()<br/>              .filter(r -&gt;<br/>              m.getName().equals(r.getPrimaryResult().getLabel()))<br/>              .findFirst();<br/>              assertTrue(m + " didn't get any result",<br/>              methodResult.isPresent());<br/><br/>          final ExpectedPerformances expectations =<br/>          m.getAnnotation(ExpectedPerformances.class);<br/>          final RunResult result = results.iterator().next();<br/>          final BenchmarkResult aggregatedResult =<br/>          result.getAggregatedResult();<br/><br/>          final double actualScore =<br/>          aggregatedResult.getPrimaryResult().getScore();<br/>          final double expectedScore = expectations.score();<br/>          final double acceptedError = expectedScore *<br/>          expectations.scoreTolerance();<br/>          try { // use assert to get a common formatting for errors<br/>            assertEquals(m.getDeclaringClass().getSimpleName() + "#" +<br/>            m.getName(), expectedScore, actualScore, acceptedError);<br/>            return null;<br/>          } catch (final AssertionError ae) {<br/>            return ae;<br/>          }<br/>        }).filter(Objects::nonNull).collect(toList());<br/>    if (!errors.isEmpty()) {<br/>      throw new AssertionError(errors.stream()<br/>          .map(Throwable::getMessage)<br/>          .collect(Collectors.joining("\n")));<br/>    }<br/>  }<br/>}</pre>
            <p>Now, with this runner, you can execute the test in the main build with surefire or
               failsafe and ensure your build will not pass if you have huge performance regression.
            </p>
            
            <div class="packt_tip">This is a simple implementation and you can enrich it in several ways, such as by
               simulating one child per benchmark to gain a nicer report in the IDE and surefire
               report (just run it the first time you encounter the enclosing class, then store the
               result and do the assertion per method). You can also assert more results, such as
               the secondary results or the iteration results (for example, no iteration is slower
               than X). Finally, you can implement some <kbd>living documentation</kbd> features, adding an @Documentation annotation which will be used by the runner to
               create a report file (in asciidoctor, for instance).
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">ContiPerf</h1>
            
         </header>
         
         
         <article>
            
            
            <p>An alternative to JMH, which is a bit less advanced but much easier to use, is ContiPerf.
               You can add it to your project with this dependency:
            </p>
            <pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.databene&lt;/groupId&gt;<br/>    &lt;artifactId&gt;contiperf&lt;/artifactId&gt;<br/>    &lt;version&gt;2.3.4&lt;/version&gt;<br/>    &lt;scope&gt;test&lt;/scope&gt;<br/>&lt;/dependency&gt;</pre>
            <p>We will not detail it completely in this book as we spent a lot of time on JMH already.
               But in a few words, it is based on a JUnit 4 rule. Thus, it can be combined with other
               rules and sorted thanks to the JUnit <kbd>RuleChain</kbd>, which makes it very powerful, with all light EE containers having a JUnit-based
               testing stack such as TomEE or Meecrowave, for instance.
            </p>
            
            <p>The very big advantage of ContiPerf is that it is aligned with the JUnit model:</p>
            
            <ul>
               
               <li>It is based on the standard JUnit <kbd>@Test</kbd> method marker
               </li>
               
               <li>You can reuse the JUnit standard life cycle (<kbd>@BeforeClass</kbd>, <kbd>@Before</kbd>, and so on)
               </li>
               
               <li>You can combine it with other JUnit features (runners, rules, and so on)</li>
               
            </ul>
            
            <p>Here is what a test can look like in terms of structure:</p>
            <pre><span>public class </span>QuoteMicrobenchmarkTest {<br/>    <span>private static </span>QuoteService <span>service</span>;<br/><br/>    <span>@Rule<br/></span><span>    </span><span>public final </span>ContiPerfRule <span>rule </span>= <span>new </span>ContiPerfRule();<br/><br/>    <span>@BeforeClass<br/></span><span>    </span><span>public static void </span>init() {<br/>        <span>service </span>= <span>new </span>QuoteService();<br/>    }<br/><br/>    <span>@Test<br/></span><span>    @Required</span>(throughput = <span>7000000</span>)<br/>    <span>@PerfTest</span>(rampUp = <span>100</span>, duration = <span>10000</span>, threads = <span>10</span>, warmUp =<br/>    <span>10000</span>)<br/>    <span>public void </span>test() {<br/>        <span>service</span>.findByName(<span>"test"</span>).orElse(<span>null</span>);<br/>    }<span><br/></span>}</pre>
            <p>We immediately identify a JUnit structure, with an <kbd>@BeforeClass</kbd> initializing the test (you can start a container here and close it in an <kbd>@AfterClass</kbd> if needed), and a <kbd>@Test</kbd> that is our benchmark/scenario. The only difference with a JUnit test is the <kbd>ContiPerfRule</kbd> and the <kbd>@Required</kbd> and <kbd>@PerfTest</kbd> annotations.
            </p>
            
            <p>The <kbd>@PerfTest</kbd> describes the test environment—how many threads, how long, how many iterations, the
               duration for the warmup, and so on.
            </p>
            
            <p>The <kbd>@Required</kbd>, on another side, describes the assertions (validations) to do. It is equivalent
               to our <kbd>@ExpectedPerformances</kbd> in our JMH integration. It supports most of the common validations such as the throughoutput,
               the average, the total time, and so on.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Arquillian and ContiPerf – the diabolic pair</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In the JMH section, we got the issue that starting a container was sometimes hard
               and really not straightforward. As ContiPerf is a rule, it is compatible with <kbd>Arquillian</kbd>, which can do all that work for you.
            </p>
            
            <p><kbd>Arquillian</kbd> is a project created by JBoss (Red Hat, now) for abstracting the containers behind
               a <strong>service provider interface</strong> (<strong>SPI</strong>) and integrating it with JUnit or TestNG. The idea is to run a test from your IDE
               as usual, without having to care about needing a container.
            </p>
            
            <p>At a high level, it requires you to define what you deploy into the container and
               to use the <kbd>Arquillian</kbd> runner with JUnit (or an abstract class with TestNG). Thanks to a mechanism of extensions
               and enrichers, you can inject most of what you need into the test class, such as CDI
               beans, which is really handy for writing tests. Here is a sample:
            </p>
            <pre><span>@RunWith</span>(Arquillian.<span>class</span>)<br/><span>public class </span>QuoteServicePerformanceTest {<br/>    <span>@Deployment<br/></span><span>    </span><span>public static </span>Archive&lt;?&gt; quoteServiceApp() {<br/>        return ShrinkWrap.<span>create</span>(WebArchive.<span>class</span>, <span>"quote-manager.war"</span>)<br/>                .addClasses(QuoteService.<span>class,<br/>                InMemoryTestDatabaseConfiguration.class</span>)<br/>                .addAsWebResource(<span>new </span>ClassLoaderAsset(<span>"META<br/>                -INF/beans.xml"</span>), <span>"WEB-INF/classes/META-INF/beans.xml"</span>)<br/>                .addAsWebResource(<span>new </span>ClassLoaderAsset(<span>"META-<br/>                INF/persistence.xml"</span>), <span>"WEB-INF/classes/META-<br/>                INF/persistence.xml"</span>);<br/>    }<br/><br/>    <span>@Inject<br/></span><span>    </span><span>private </span>QuoteService <span>service</span>;<br/><br/>    <span>@Before<br/></span><span>    </span><span>public void </span>before() {<br/>        final Quote quote = new Quote();<br/>        quote.setName("TEST");<br/>        quote.setValue(10.);<br/>        service.create(quote);<br/>    }<br/><br/>    <span>@After<br/></span><span>    </span><span>public void </span>after() {<br/>        service.deleteByName("TEST");<br/>    }<br/><br/>    <span>@Test</span><br/>    <span>public void </span>findByName() {<br/>        assertTrues(<span>service</span>.findByName(<span>"TEST"</span>).orElse(false));<br/>    }<br/>}</pre>
            <p>This snippet illustrates <kbd>Arquillian</kbd> usage with its common characteristics:
            </p>
            
            <ul>
               
               <li>The <kbd>Arquillian</kbd> runner is in use; it is the magic that will start the container (once), deploy the
                  application (per test class by default), execute the tests (inherited from JUnit default
                  behavior), undeploy the application when all tests of the class were executed, and
                  shut down the container once the tests have been executed.
               </li>
               
               <li>The static <kbd>@Deployment</kbd> method returns an <kbd>Archive&lt;?&gt;</kbd> describing what to deploy into the container (the application). You don't have to
                  deploy the full application and you can change it per test if you want. For instance,
                  in our sample, we didn't deploy our <kbd>DataSourceConfiguration</kbd>, which is pointing to MySQL, but instead, a <kbd>InMemoryDatabaseConfiguration</kbd>, which we can assume uses an embedded database such as derby or h2.
               </li>
               
               <li>We have a CDI injected into the test directly, our <kbd>QuoteService</kbd>.
               </li>
               
               <li>The rest of the test is a standard JUnit test with its life cycle (<kbd>@Before</kbd>/<kbd>@After</kbd>) and its test methods (<kbd>@Test</kbd>).
               </li>
               
            </ul>
            
            <div class="packt_tip">If you find the construction of the archive too complicated, there are some projects
               such as the TomEE <kbd>ziplock</kbd> module that simplify it by reusing the <em>current</em> project metadata (such as the <kbd>pom.xml</kbd> and compiled classes), which allows you to create the archive with a single method
               invocation: <kbd>Mvn.war()</kbd>. Some containers, including TomEE, allow you to deploy each archive a single time.
               But if your container doesn't support it, you can use the <kbd>Arquillian</kbd> suite extension to achieve almost the same result. The overall goal is to group your
               tests to deploy a single time your application and save precious minutes on your tests
               execution duration.
            </div>
            
            <p><kbd>Arquillian</kbd> also allows us to go further and execute the test either from inside the container—as
               the previous example—or from a client perspective using the <kbd>@RunAsClient</kbd> annotation. In this case, your test is no longer executed inside the container but
               in JUnit JVM (which can be the same or not, depending on whether your container uses
               another JVM or not). In any case, coupling <kbd>Arquillian</kbd> with ContiPerf allows you to validate the performance without many headaches. You
               just have to add the ContiPerf rule and annotations on the methods you want to validate:
            </p>
            <pre><span>@RunWith</span>(Arquillian.<span>class</span>)<br/><span>public class </span>QuoteServicePerformanceTest {<br/>    <span>@Deployment<br/></span><span>    </span><span>public static </span>Archive&lt;?&gt; quoteServiceApp() {<br/>        <span>final </span>WebArchive baseArchive =<br/>        ShrinkWrap.<span>create</span>(WebArchive.<span>class</span>, <span>"quote-manager.war"</span>)<br/>              .addClasses(QuoteService.<span>class</span>)<br/>              .addAsWebResource(<span>new </span>ClassLoaderAsset(<span>"META<br/>              -INF/beans.xml"</span>), <span>"WEB-INF/classes/META-INF/beans.xml"</span>)<br/>              .addAsWebResource(<span>new </span>ClassLoaderAsset(<span>"META<br/>              -INF/persistence.xml"</span>), <span>"WEB-INF/classes/META<br/>              -INF/persistence.xml"</span>);<br/>        <span>if </span>(Boolean.<span>getBoolean</span>(<span>"test.performance." </span>+<br/>        QuoteService.<span>class</span>.getSimpleName() + <span>".database.mysql"</span>)) {<br/>            baseArchive.addClasses(DataSourceConfiguration.<span>class</span>);<br/>        } <span>else </span>{<br/>            baseArchive.addClasses(InMemoryDatabase.<span>class</span>);<br/>        }<br/>        <span>return </span>baseArchive;<br/>    }<br/><br/>    <span>@Rule<br/></span><span>    </span><span>public final </span>ContiPerfRule <span>rule </span>= <span>new </span>ContiPerfRule();<br/><br/>    <span>@Inject<br/></span><span>    </span><span>private </span>QuoteService <span>service</span>;<br/><br/>    <span>private final </span>Collection&lt;Long&gt; <span>id </span>= <span>new </span>ArrayList&lt;Long&gt;();<br/><br/>    <span>@Before<br/></span><span>    </span><span>public void </span>before() {<br/>        IntStream.<span>range</span>(<span>0</span>, <span>1000000</span>).forEach(i -&gt; insertQuote(<span>"Q" </span>+ i));<br/>        insertQuote(<span>"TEST"</span>);<br/>    }<br/><br/>    <span>@After<br/></span><span>    </span><span>public void </span>after() {<br/>        <span>id</span>.forEach(<span>service</span>::delete);<br/>    }<br/><br/>    <span>@Test<br/></span><span>    @Required</span>(max = <span>40</span>)<br/>    <span>@PerfTest</span>(duration = <span>500000</span>, warmUp = <span>200000</span>)<br/>    <span>public void </span>findByName() {<br/>        <span>service</span>.findByName(<span>"TEST"</span>);<br/>    }<br/><br/>    <span>private void </span>insertQuote(<span>final </span>String name) {<br/>        <span>final </span>Quote entity = <span>new </span>Quote();<br/>        entity.setName(name);<br/>        entity.setValue(Math.<span>random</span>() * <span>100</span>);<br/>        <span>id</span>.add(<span>service</span>.create(entity).getId());<br/>    }<br/>}</pre>
            <p>This is almost the same test as before, but we added more data to ensure the dataset
               size doesn't affect our performance much in the <kbd>@Before</kbd> method used to initialize the database. To integrate the test with ContiPerf, we
               added the ContiPerf annotations to our method and the ContiPerf rule. The last trick
               you can see is a system property in the archive creation, able to switch databases
               depending on the JVM configuration. It can be used to test against several databases
               or environments and validates you are complicant with all your target platforms.
            </p>
            
            <p>To be able to run this example, you need to add these dependencies into your pom—considering
               that you will test it against GlassFish 5.0, you need to change the container dependency
               and <kbd>arquillian</kbd> container integration:
            </p>
            <pre><span>&lt;</span><span>dependency</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>junit<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>junit<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>4.12<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>scope</span><span>&gt;</span>test<span>&lt;/</span><span>scope</span><span>&gt;</span><br/><span>&lt;/</span><span>dependency</span><span>&gt;</span><br/><span>&lt;</span><span>dependency</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>org.jboss.arquillian.junit<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>arquillian-junit-container<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>1.1.13.Final<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>scope</span><span>&gt;</span>test<span>&lt;/</span><span>scope</span><span>&gt;</span><br/><span>&lt;/</span><span>dependency</span><span>&gt;</span><br/><span>&lt;</span><span>dependency</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>org.jboss.arquillian.container<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>arquillian-glassfish-embedded-3.1<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>1.0.1<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>scope</span><span>&gt;</span>test<span>&lt;/</span><span>scope</span><span>&gt;</span><br/><span>&lt;/</span><span>dependency</span><span>&gt;</span><br/><span>&lt;</span><span>dependency</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>org.glassfish.main.extras<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>glassfish-embedded-all<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>5.0<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>scope</span><span>&gt;</span>test<span>&lt;/</span><span>scope</span><span>&gt;</span><br/><span>&lt;/</span><span>dependency</span><span>&gt;</span></pre>
            <p>JUnit is the test framework we use and we import its Arquillian integration (<kbd>arquillian-junit-container</kbd>). Then, we import our <kbd>arquillian</kbd> container integration (<kbd>arquillian-glassfish-embedded-3.1</kbd>) and Java EE container, as we use an embedded mode (<kbd>glassfish-embedded-all</kbd>). Don't forget to add a ContiPerf dependency too if you plan to use it.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">JMeter and build integration</h1>
            
         </header>
         
         
         <article>
            
            
            <p>We learned in a previous section that JMeter can be used to build scenarii you execute
               against your application. It can also be programmatically executed—it is Java-based,
               after all—or executed through some of its Maven integrations.
            </p>
            
            <p>If you use its Maven plugin from lazerycode (<a href="https://github.com/jmeter-maven-plugin/jmeter-maven-plugin">https://github.com/jmeter-maven-plugin/jmeter-maven-plugin</a>), you can even configure the remote mode to have real stress testing:
            </p>
            <pre><span>&lt;</span><span>plugin</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>com.lazerycode.jmeter<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>jmeter-maven-plugin<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>2.2.0<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>executions</span><span>&gt;</span><br/>    <span>&lt;</span><span>execution</span><span>&gt;</span><br/>      <span>&lt;</span><span>id</span><span>&gt;</span>jmeter-tests<span>&lt;/</span><span>id</span><span>&gt;</span><br/>      <span>&lt;</span><span>goals</span><span>&gt;</span><br/>        <span>&lt;</span><span>goal</span><span>&gt;</span>jmeter<span>&lt;/</span><span>goal</span><span>&gt;</span><br/>      <span>&lt;/</span><span>goals</span><span>&gt;</span><br/>    <span>&lt;/</span><span>execution</span><span>&gt;</span><br/>  <span>&lt;/</span><span>executions</span><span>&gt;</span><br/>  <span>&lt;</span><span>configuration</span><span>&gt;</span><br/>    <span>&lt;</span><span>remoteConfig</span><span>&gt;</span><br/>      <span>&lt;</span><span>startServersBeforeTests</span><span>&gt;</span>true<span>&lt;/</span><span>startServersBeforeTests</span><span>&gt;</span><br/>      <span>&lt;</span><span>serverList</span><span>&gt;</span>jmeter1.company.com,jmeter2.company.com<span>&lt;/</span><span>serverList</span><span>&gt;</span><br/>      <span>&lt;</span><span>stopServersAfterTests</span><span>&gt;</span>true<span>&lt;/</span><span>stopServersAfterTests</span><span>&gt;</span><br/>    <span>&lt;/</span><span>remoteConfig</span><span>&gt;</span><br/>  <span>&lt;/</span><span>configuration</span><span>&gt;</span><br/><span>&lt;/</span><span>plugin</span><span>&gt;</span></pre>
            <p>This snippet defines the <kbd>jmeter</kbd> plugin to use <kbd>jmeter1.company.com</kbd> and <kbd>jmeter2.company.com</kbd> for the load testing. Servers will be initialized and destroyed before/after running
               the plans.
            </p>
            
            <p>Without entering into the deep details of the plugin—you can find them on the related
               GitHub wiki—the plugin uses configurations stored in the project in <kbd><span>src/test/jmeter</span></kbd> by default. This is where you can put your scenarii (<kbd>.jmx</kbd> files).
            </p>
            
            <p>The challenge with this solution is to provide <kbd>jmeter[1,2].company.com</kbd> machines. Of course, you can create some machines and let them run, though, this
               is not a very good way to manage the AWS machines and it would be better to start
               them with the build  (allowing you to have concurrent builds, if needed, on multiple
               branches at the same time).
            </p>
            
            <p>There are several solutions for that need, but the simplest is probably to i<span>nstall on the CI platform an AWS client (or plugin) and launch it before/after the
                  Maven builds the corresponding commands to provision the machine, set the machine
                  hosts in a build property, and pass it to the Maven build. It requires you to variabilize
                  the plugin configuration, but nothing fancy, as Maven supports placeholders and system
                  property input. Nonetheless, running the tests from your machine can be hard as you
                  will need to provision yourself with the machine you will use. Thus, this reduces
                  the shareable side of the project.</span></p>
            
            <div class="packt_infobox">Don't forget to ensure the task of shutting down the instance is always executed,
               even when the tests fail, or else you could leak some machines and have a billing
               surprise at the end of the month.
            </div>
            
            <p>Finally, as JMeter is a mainstream solution, you can easily find platforms supporting
               it natively and handling the infrastructure for you. The main ones are:
            </p>
            
            <ul>
               
               <li>BlazeMeter (<a href="https://www.blazemeter.com/">https://www.blazemeter.com/</a>)
               </li>
               
               <li>Flood.IO (<a href="https://flood.io/">https://flood.io/</a>)
               </li>
               
               <li>Redline.13 (<a href="https://www.redline13.com/">https://www.redline13.com/</a>)
               </li>
               
            </ul>
            
            <p>Don't hesitate to have a look at their websites, their prices, and compare them to
               what you can build yourself directly with AWS if you don't have dedicated machines
               on your CI. This can allow you to solve the environment and infrastructure issues
               performance tests often encounter.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Gatling and continuous integration</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Like JMeter, Gatling has its own Maven plugin, but it also has some companion AWS integration
               (<a href="https://github.com/electronicarts/gatling-aws-maven-plugin">https://github.com/electronicarts/gatling-aws-maven-plugin</a>) natively integrated with Maven.
            </p>
            
            <p>Here is what the official Gatling Maven plugin declaration can look like in your <kbd>pom.xml</kbd>:
            </p>
            <pre><span>&lt;</span><span>plugin</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>io.gatling<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>gatling-maven-plugin<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>${gatling-plugin.version}<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>executions</span><span>&gt;</span><br/>    <span>&lt;</span><span>execution</span><span>&gt;</span><br/>      <span>&lt;</span><span>phase</span><span>&gt;</span>test<span>&lt;/</span><span>phase</span><span>&gt;</span><br/>      <span>&lt;</span><span>goals</span><span>&gt;</span><br/>        <span>&lt;</span><span>goal</span><span>&gt;</span>execute<span>&lt;/</span><span>goal</span><span>&gt;</span><br/>      <span>&lt;/</span><span>goals</span><span>&gt;</span><br/>    <span>&lt;/</span><span>execution</span><span>&gt;</span><br/>  <span>&lt;/</span><span>executions</span><span>&gt;</span><br/><span>&lt;/</span><span>plugin</span><span>&gt;</span></pre>
            <p>The first plugin (1) defines how to run Gatling. The default configuration will look
               for simulations in <kbd>src/test/scala</kbd>.
            </p>
            
            <p>This setup will locally run your simulations. So, you will likely migrate to the non-official
               plugin, yet integrated with AWS, to be able to control the injectors. Here is what
               the declaration can look like:
            </p>
            <pre>&lt;plugin&gt; <br/>  &lt;groupId&gt;com.ea.gatling&lt;/groupId&gt;<br/>  &lt;artifactId&gt;gatling-aws-maven-plugin&lt;/artifactId&gt;<br/>  &lt;version&gt;1.0.11&lt;/version&gt;<br/>  &lt;executions&gt;<br/>    &lt;execution&gt;<br/>      &lt;phase&gt;test&lt;/phase&gt;<br/>      &lt;goals&gt;<br/>        &lt;goal&gt;execute&lt;/goal&gt;<br/>      &lt;/goals&gt;<br/>    &lt;/execution&gt;<br/>  &lt;/executions&gt;<br/>&lt;/plugin&gt;</pre>
            <p>This plugin will integrate Gatling on AWS. It requires some AWS configuration (like
               your keys), but you will generally configure them outside the <kbd>pom.xml</kbd> so as not to put your credentials in a public place—properties in a profile in your
               <kbd>settings.xml</kbd> is a good place to start. Here are the properties you will need to define:
            </p>
            <pre>&lt;<span class="pl-ent">properties</span>&gt;
  &lt;<span class="pl-ent">ssh</span>.private.key&gt;${gatling.ssh.private.key}&lt;/<span class="pl-ent">ssh</span>.private.key&gt;
  &lt;<span class="pl-ent">ec2</span>.key.pair.name&gt;loadtest-keypair&lt;/<span class="pl-ent">ec2</span>.key.pair.name&gt;
  &lt;<span class="pl-ent">ec2</span>.security.group&gt;default&lt;/<span class="pl-ent">ec2</span>.security.group&gt;
  &lt;<span class="pl-ent">ec2</span>.instance.count&gt;3&lt;/<span class="pl-ent">ec2</span>.instance.count&gt;

  &lt;<span class="pl-ent">gatling</span>.local.home&gt;${project.build.directory}/gatling-charts<br/>  -highcharts-bundle-2.2.4/bin/gatling.sh&lt;/<span class="pl-ent">gatling</span>.local.home&gt;
   &lt;<span class="pl-ent">gatling</span>.install.script&gt;${project.basedir}/src/test/resources/install-<br/>  gatling.sh&lt;/<span class="pl-ent">gatling</span>.install.script&gt;
  &lt;<span class="pl-ent">gatling</span>.root&gt;gatling-charts-highcharts-bundle-2.2.4&lt;/<span class="pl-ent">gatling</span>.root&gt;
  &lt;<span class="pl-ent">gatling</span>.java.opts&gt;-Xms1g -Xmx16g -Xss4M <br/>  -XX:+CMSClassUnloadingEnabled -<br/>  XX:MaxPermSize=512M&lt;/<span class="pl-ent">gatling</span>.java.opts&gt;

  <span class="pl-c">&lt;!-- Fully qualified name of the Gatling simulation and a name<br/>  describing the test --&gt;</span>
  &lt;<span class="pl-ent">gatling</span>.simulation&gt;com.FooTest&lt;/<span class="pl-ent">gatling</span>.simulation&gt;
  &lt;<span class="pl-ent">gatling</span>.test.name&gt;LoadTest&lt;/<span class="pl-ent">gatling</span>.test.name&gt;
<br/>  &lt;!-- (3) --&gt;
  &lt;<span class="pl-ent">s3</span>.upload.enabled&gt;true&lt;/<span class="pl-ent">s3</span>.upload.enabled&gt;
  &lt;<span class="pl-ent">s3</span>.bucket&gt;loadtest-results&lt;/<span class="pl-ent">s3</span>.bucket&gt;
  &lt;<span class="pl-ent">s3</span>.subfolder&gt;my-loadtest&lt;/<span class="pl-ent">s3</span>.subfolder&gt;
&lt;/<span class="pl-ent">properties</span>&gt;</pre>
            <p>Defining all these properties doesn't prevent you from changing the values through
               a system property. For instance, setting <kbd>-Dec2.instance.count=9</kbd> will allow you to change the number of injectors (nine instead of three). The first
               group of properties (ec2 ones) define how to create the AWS instances and how many
               to create. The second group (Gatling one) defines where Gatling is and how to run
               it. The third group defines the simulation to run. Finally, the last group (s3 one)
               defines where to upload the results of the test.
            </p>
            
            <p>This configuration is not yet functional, as it is not yet self-sufficient:</p>
            
            <ul>
               
               <li>It relies on Gatling distribution, which is not yet installed (<kbd>gatling.local.home</kbd>)
               </li>
               
               <li>It relies on a script we didn't create yet (<kbd>install-gatling.sh</kbd>)
               </li>
               
            </ul>
            
            <p>To be able to not depend on a local Gatling installation, we can use Maven to download
               it. To do so, we just need the dependency plugin of Maven:
            </p>
            <pre><span>&lt;</span><span>plugin</span><span>&gt;</span><br/>  <span>&lt;</span><span>groupId</span><span>&gt;</span>org.apache.maven.plugins<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>  <span>&lt;</span><span>artifactId</span><span>&gt;</span>maven-dependency-plugin<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>  <span>&lt;</span><span>version</span><span>&gt;</span>3.0.2<span>&lt;/</span><span>version</span><span>&gt;</span><br/>  <span>&lt;</span><span>executions</span><span>&gt;</span><br/>    <span>&lt;</span><span>execution</span><span>&gt;</span><br/>      <span>&lt;</span><span>id</span><span>&gt;</span>download-gatling-distribution<span>&lt;/</span><span>id</span><span>&gt;</span><br/>      <span>&lt;</span><span>phase</span><span>&gt;</span>generate-test-resources<span>&lt;/</span><span>phase</span><span>&gt;</span><br/>      <span>&lt;</span><span>goals</span><span>&gt;</span><br/>        <span>&lt;</span><span>goal</span><span>&gt;</span>unpack<span>&lt;/</span><span>goal</span><span>&gt;</span><br/>      <span>&lt;/</span><span>goals</span><span>&gt;</span><br/>      <span>&lt;</span><span>configuration</span><span>&gt;</span><br/>        <span>&lt;</span><span>artifactItems</span><span>&gt;</span><br/>          <span>&lt;</span><span>artifactItem</span><span>&gt;</span><br/>            <span>&lt;</span><span>groupId</span><span>&gt;</span>io.gatling.highcharts<span>&lt;/</span><span>groupId</span><span>&gt;</span><br/>            <span>&lt;</span><span>artifactId</span><span>&gt;</span>gatling-charts-highcharts-bundle<span>&lt;/</span><span>artifactId</span><span>&gt;</span><br/>            <span>&lt;</span><span>version</span><span>&gt;</span>2.2.4<span>&lt;/</span><span>version</span><span>&gt;</span><br/>            <span>&lt;</span><span>classifier</span><span>&gt;</span>bundle<span>&lt;/</span><span>classifier</span><span>&gt;</span><br/>            <span>&lt;</span><span>type</span><span>&gt;</span>zip<span>&lt;/</span><span>type</span><span>&gt;</span><br/>            <span>&lt;</span><span>overWrite</span><span>&gt;</span>false<span>&lt;/</span><span>overWrite</span><span>&gt;</span><br/>            <span>&lt;</span><span>outputDirectory</span><span>&gt;</span>${project.build.directory}/gatling<span>&lt;/</span><span>outputDirectory</span><span>&gt;</span><br/>            <span>&lt;</span><span>destFileName</span><span>&gt;</span>gatling-charts-highcharts-bundle<br/>            -2.2.4.jar<span>&lt;/</span><span>destFileName</span><span>&gt;</span><br/>          <span>&lt;/</span><span>artifactItem</span><span>&gt;</span><br/>        <span>&lt;/</span><span>artifactItems</span><span>&gt;</span><br/>        <span>&lt;</span><span>outputDirectory</span><span>&gt;</span>${project.build.directory}/wars<span>&lt;/</span><span>outputDirectory</span><span>&gt;</span><br/>        <span>&lt;</span><span>overWriteReleases</span><span>&gt;</span>false<span>&lt;/</span><span>overWriteReleases</span><span>&gt;</span><br/>        <span>&lt;</span><span>overWriteSnapshots</span><span>&gt;</span>true<span>&lt;/</span><span>overWriteSnapshots</span><span>&gt;</span><br/>      <span>&lt;/</span><span>configuration</span><span>&gt;</span><br/>    <span>&lt;/</span><span>execution</span><span>&gt;</span><br/>  <span>&lt;/</span><span>executions</span><span>&gt;</span><br/><span>&lt;/</span><span>plugin</span><span>&gt;</span></pre>
            <p>This configuration will extract the Gatling distribution into the <kbd>target/gatling/gatling-charts-highcharts-bundle-2.2.4</kbd> folder and let the plugin use it when it runs.
            </p>
            
            <p>For the script, you can use this one, which is for Fedora. However, it is easy to
               adapt to any distribution if you pick another image on EC2:
            </p>
            <pre>#!/bin/sh<br/># Increase the maximum number of open files<br/>sudo ulimit -n 65536<br/>echo "* soft nofile 65535" | sudo tee --append /etc/security/limits.conf<br/>echo "* hard nofile 65535" | sudo tee --append /etc/security/limits.conf<br/><br/>sudo yum install --quiet --assumeyes java-1.8.0-openjdk-devel.x86_64 htop screen<br/><br/># Install Gatling<br/>GATLING_VERSION=2.2.4<br/>URL=https://repo.maven.apache.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/${GATLING_VERSION}/gatling-charts-highcharts-bundle-${GATLING_VERSION}-bundle.zip<br/>GATLING_ARCHIVE=gatling-charts-highcharts-bundle-${GATLING_VERSION}-bundle.zip<br/><br/>wget --quiet ${URL} -O ${GATLING_ARCHIVE}<br/>unzip -q -o ${GATLING_ARCHIVE}<br/><br/># Remove example code to reduce Scala compilation time at the beginning of load test<br/>rm -rf gatling-charts-highcharts-bundle-${GATLING_VERSION}/user-files/simulations/computerdatabase/</pre>
            <p>This script does three main things:</p>
            
            <ul>
               
               <li>Increases <kbd>ulimit</kbd> to make sure the injector can use enough file handlers and not be limited by the
                  OS configuration
               </li>
               
               <li>Installs Java</li>
               
               <li>Downloads Gatling from Maven centra, extracts the archive (like we did with the previous
                  Maven plugin), but on the injector machines which don't have to use Maven, and finally
                  cleans up the extracted archive (removing sample simulations)
               </li>
               
            </ul>
            
            <div class="packt_tip">If you need any dependencies, you will need to create a shade (and use the default <kbd>jar-with-dependencies</kbd> as a classifier). You can do that using <kbd>maven-assembly-plugin</kbd> and the <kbd>single</kbd> goal.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Deploying your (benchmark) application</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In the two previous sections, we learned how to handle the injectors, but you will
               also need to deploy the application to test, if possible, on a dedicated instance
               for the test. This doesn't mean you need to forget what we looked at in the previous
               chapter, such as ensuring the machine is used in production and other potentially
               impacting services are running. Instead, you must ensure the machine(s) does what
               you think it does and not something unexpected that will impact the figures/tests.
            </p>
            
            <p>Here, again, using cloud services to deploy your application can be the easiest solution.
               The simplest solution will likely rely on some cloud CLI (such as AWS CLI or <kbd>aws</kbd> command) or a small <kbd>main(String[])</kbd> you can write using the cloud provider client API or SDK.
            </p>
            
            <p>Depending on if you code the deployment <span>yourself (or not)</span>, it will be more or less easy to integrate with the Maven (or Gradle) build. As part
               of your project, an <kbd>exec-maven-plugin</kbd> can enable you to integrate it exactly where you need to in the Maven life cycle.
               Most of the time, this will be done before the performance test but after test compilation,
               or even after packaging the application (if you keep the performance test in the same
               module, which is feasible).
            </p>
            
            <p>If you don't code the deployment yourself, you will need to define the stages of your
               performance build:
            </p>
            
            <ol>
               
               <li>Compile/package the project and tests.</li>
               
               <li>Deploy the application (and don't forget to reset the environment too if needed, including
                  cleaning a database or JMS queue).
               </li>
               
               <li>Start the performance test.</li>
               
               <li>Undeploy the application/shut down the created server (if relevant).</li>
               
            </ol>
            
            <p>With Maven or Gradle, it is easy to skip some of the tasks, either with a flag or
               a profile, and consequently, you will end up with commands like this:
            </p>
            <pre>mvn clean install # (1)<br/>mvn exec:java@deploy-application # (2)<br/>mvn test -Pperf-tests # (3)<br/>mvn exec:java@undeploy-application #(4)</pre>
            <p>The first command, <kbd>(1)</kbd>, will build the full project but bypass the performance tests as we don't activate
               the <kbd>perf-tests</kbd> profile by default. The second command will deploy the application on the target
               environment using a custom implementation based on AWS SDK, for instance, potentially
               creating it from scratch. Don't forget to log what you do, even if you are waiting
               for something, or else you—or others —may think the process is hanging. Then, we run
               the performance test <kbd>(3)</kbd> and finally we undeploy the application with a command symmetric to <kbd>(2)</kbd>.
            </p>
            
            <p>With such a solution, you need to ensure <kbd>(4)</kbd> is executed as soon as <kbd>(2)</kbd> is executed. In general, you will enforce it to always be executed and handle a quick
               exit condition if the expected environment to destroy doesnt exist.
            </p>
            
            <p>To orchestrate the steps, you can have a look at the Jenkins pipeline feature: it
               will give you a lot of flexibility to implement this type of logic in a straightforward
               manner.
            </p>
            
            <p>Going further overpasses the scope of this book, but to give you some pointers, the
               deployments can rely on Docker-based tools which makes it really easy to deploy on
               cloud platforms in general. Nevertheless, don't forget Docker is not a provisioning
               tool. And if your <em>recipe</em> (the steps for creating your instance) is not simple (installing software from the
               repository, copying/downloading your application, and starting it), then you can invest
               in a provisioning tool such as chef or puppet to be more flexible, powerful, and to
               avoid hacks.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Continuous integration platform and performance</h1>
            
         </header>
         
         
         <article>
            
            
            <p class="mce-root">Jenkins is the most commonly used continuous integration platform. There are alternatives,
               such as Travis, but the ecosystem of Jenkins and the easiness to extend it makes it
               the clear leader for Java and enterprises applications.
            </p>
            
            <p>The first thing we want to solve on the build/test execution platform for performance
               is the isolation of the builds, the goal being obviously to ensure the obtained figures
               are not affected by other builds.
            </p>
            
            <p>To do that, Jenkins provides several plugins:</p>
            
            <ul>
               
               <li>Amazon EC2 Container Service Plugin (<a href="https://wiki.jenkins.io/display/JENKINS/Amazon+EC2+Container+Service+Plugin">https://wiki.jenkins.io/display/JENKINS/Amazon+EC2+Container+Service+Plugin</a>): Allows you to run the builds (tests) in a dedicated machine created based on a
                  Docker image.
               </li>
               
               <li>Throttle Concurrent Build Plugin (<a href="https://github.com/jenkinsci/throttle-concurrent-builds-plugin/blob/master/README.md">https://github.com/jenkinsci/throttle-concurrent-builds-plugin/blob/master/README.md</a>): Allows you to control how many concurrent builds can be executed per project. Concretely,
                  for performance, we want to ensure it is one per project.
               </li>
               
            </ul>
            
            <p>In terms of configuration, you will need to make sure the performance tests are executed
               with an accurate configuration:
            </p>
            
            <ul>
               
               <li>Regularly, using Jenkins scheduling, but probably not each time there is a commit
                  or a pull-request. Depending on the criticity, the stability of your project, and
                  the duration of your performance tests, it can be once a day or once a week.
               </li>
               
               <li>The previous plugins—or equivalent—are in use and correctly configured.</li>
               
               <li>The build notifies the correct channels if they fail (mail, Slack, IRC, and so on).</li>
               
            </ul>
            
            <p>It will also be important to ensure you store the history of the runs to be able to
               compare them, in particular if you don't run the performance test with each commit,
               which would give you the exact commit, introducing a regression. To do that, you can
               use another Jenkins plugin which is exactly intended to store the history of common
               performance tools: the Performance Plugin (<a href="https://wiki.jenkins.io/display/JENKINS/Performance+Plugin">https://wiki.jenkins.io/display/JENKINS/Performance+Plugin</a>). This plugin supports Gatling and JMeter, as well as a few other tools. It is a
               nice plugin, allowing you to visualize the reports directly from Jenkins, which is
               very handy when investigating some changes. What's more, it is compatible with Jenkins
               pipeline scripts.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Summary</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In this chapter, we went through some common ways to ensure the performance of your
               application is under control and to limit the risk of getting unexpected bad surprises
               when you go into a benchmark phase, or worse, in production! Setting up simple tests
               or complete environments for a volatile (temporary) benchmark each week (or even each
               day) are very feasible steps, enabling a product to be delivered with a better quality
               level once the entry cost has been paid.
            </p>
            
            <p>After having understood how Java EE instruments your application to let you focus
               on your business, how to monitor and instrument your application to optimize your
               application, and how to boost your application with some tuning or caching, we now
               know how to automatically control performance regressions to be able to fix them as
               soon as possible.
            </p>
            
            <p>As a result, you have now covered all the parts of product—or library—creation related
               to performance, and you are able to deliver highly performant software. You've got
               this!
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   </body></html>