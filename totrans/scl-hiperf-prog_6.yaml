- en: Chapter 6. Concurrency in Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will switch our focus from collections to a different topic:
    concurrency. Being able to take advantage of all the CPU resources that your hardware
    provides is critical to writing performant software. Unfortunately, writing concurrent
    code is not an easy task because it is easy to write unsafe programs. If you come
    from Java, you may still have nightmares involving `synchronized` blocks and locks!
    The `java.util.concurrent` package provides numerous tools that make writing concurrent
    code simpler. However, designing stable and reliable concurrent applications can
    still be a daunting challenge. In this chapter, we will explore the tools that
    are provided by the Scala standard library to take advantage of concurrency. After
    a short presentation of the main abstraction, `Future`, we will study its behavior
    and usage pitfalls that we should avoid. We will end this chapter by exploring
    a possible alternative to `Future` named `Task`, which is provided by the Scalaz
    library. In this chapter, we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future usage considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking calls and callbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalaz Task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing backtesting strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data scientists are off and running with the data analysis tools that you
    built for them to research trading strategies. However, they have hit a wall because
    backtesting strategies is becoming too expensive. As they have built more sophisticated
    strategies that require more historical data, and employ more stateful algorithms,
    backtesting has taken longer. Once again, you are being called upon to help out
    at MVT by leveraging Scala and the functional paradigm to deliver performant software.
  prefs: []
  type: TYPE_NORMAL
- en: The data scientists have incrementally built out a backtesting tool that allows
    the team to determine a strategy's performance by replaying historical data. This
    works by providing a preset strategy to run, the ticker to test against, and the
    time interval of historical data to replay. The backtester loads market data and
    applies the strategy to generate trading decisions. Once the backtester finishes
    replaying historical data, it summarizes and displays strategy performance results.
    The backtester is heavily depended on to determine the efficacy of proposed trading
    strategies before putting them into production for live trading.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin familiarizing yourself with the backtester, you look into the code,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snapshot from the data analysis repository, you see the primary
    method that drives backtesting. Given a `Strategy`, `Ticker`, and `Interval`,
    it can produce `BacktestPerformanceSummary`. Scanning the repository, you find
    a file named `CrazyIdeas.scala` that shows Dave as the only commit author. In
    here, you see example invocations of the backtester:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The usage of the backtester gives you a clue to a possible performance improvement.
    It looks like when Dave has a new idea, he wants to evaluate its performance on
    multiple symbols and compare it against other strategies. In its current form,
    backtests are performed sequentially. One way to improve the execution speed of
    the backtester is to parallelize the execution of all backtesting runs. If each
    invocation of the backtester is parallelized and if there are spare hardware resources,
    then backtesting multiple strategies and symbols will finish faster. To understand
    how to parallelize the backtester, we first need to dive into the topic of asynchronous
    programming and then see how Scala supports concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before diving into the code, we need to enrich our vocabulary to discuss the
    properties of asynchronous programming. Concurrency and parallelism are often
    used interchangeably, but there is an important distinction between these two
    terms. Concurrency involves two (or more) tasks that are started and executed
    in overlapping time periods. Both tasks are in-progress (that is, they are running)
    at the same time, but only one task may be performing actual work at any instant
    in time. This is the case when you write concurrent code on a single-core machine.
    Only one task can progress at a time, but multiple tasks are ongoing concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism exists only when both tasks are truly running at the same time.
    With a dual-core machine, you can execute two tasks at the same time. From this
    definition, we see that parallelism depends on the hardware that is available
    for use. This means that the property of concurrency can be added to a program,
    but parallelism is outside the control of the software.
  prefs: []
  type: TYPE_NORMAL
- en: To better illustrate these concepts, consider the example of painting a room.
    If there is only one painter, the painter can paint the first coat on a wall,
    move on to the next wall, go back to the first wall for the second coat and then
    finish the second wall. The painter is painting both walls concurrently, but can
    only spend time on one wall at any given time. If two painters are on the job,
    they can each focus on one wall and paint them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary construct in Scala to drive concurrent programming is `Future`.
    Found in the `scala.concurrent` package, `Future` can be seen as a container for
    a value that may not yet exist. Let''s look at a simple example to illustrate
    usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example shows a short method, creating a `Future` value simulates
    an expensive computation and prints a couple of lines to make it easier for us
    to understand the flow of the application. When running `example`, we see the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can see that `Future` was executed after the end of the `example` method.
    This is because when a `Future` is created, it starts its computation concurrently.
    You may be wondering, "What is this `context` object of the `ExecutionContext`
    type that is used when creating the `Future`?" We will explore `ExecutionContext`
    in-depth shortly, but for now, we treat it as the the object that is responsible
    for the execution of the `Future`. We import `scala.concurrent.ExecutionContext.global`,
    which is a default object that is created by the standard library to be able to
    execute the `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: A `Future` object is a stateful object. It is either not yet complete when the
    computation is underway or completed once the computation finishes. Furthermore,
    a completed `Future` can be either a success when the computation was able to
    complete, or it can be a failure if an exception was thrown during the computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Future` API provides combinators to compose the `Future` instances and
    manipulate the result that they contain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This snippet from the Scala console shows construction of a `Future` data type
    that wraps a constant integer value. We see that the integer contained in the `Future`
    data type is transformed using functions that are similar to the ones that we
    expect to find on `Option` and collection data types. These transforms are applied
    once the preceding `Future` completes, and return a new `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As promised, we now look into `ExecutionContext`. The `ExecutionContext` can
    be thought of as the machinery behind `Future` that provides runtime asynchrony.
    In the previous snippet, a `Future` was created to perform simple addition and
    modulo division without explicitly providing an `ExecutionContext` instance at
    the call site. Instead, only an import of the `global` object was provided. The
    snippet executes because `global` is an implicit value and the signature of `map`
    accepts an implicit `ExecutionContext`. Let''s look at the following signature
    of `map` to deepen our understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'From the signature of `map`, we see that unlike the `map` transformation on `List`,
    the `Future` requires a curried, implicit `ExecutionContext` argument. To understand
    how an `ExecutionContext` provides runtime asynchrony, we need to first understand
    its operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `execute` is a side-effecting method that operates on a `java.lang.Runnable`.
    For those familiar with concurrency in Java, you most likely recall that `Runnable`
    is the commonly-used interface to allow threads and other `java.util.concurrent`
    abstractions to execute code concurrently. Although we do not know how `Future`
    achieves runtime asynchrony yet, we do know there is a link between `Future` execution
    and creation of a `Runnable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question we will answer is, "How do I create an `ExecutionContext`?
    " By studying the companion object, we discover the following signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The standard library provides convenient ways to create an `ExecutionContext`
    from either a `java.util.concurrent.Executor` or `java.util.concurrent.ExecutorService`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are unfamiliar with the machinery that is provided by the `java.util.concurrent`
    package and you are looking for a deeper treatment than that provided by the API
    documentation, we encourage you to read *Java Concurrency in Practice* by Brian
    Goetz ([http://jcip.net/](http://jcip.net/)). Although *Java Concurrency in Practice*
    was written around the release of JDK 6, it contains numerous principles that
    continue to apply today. Reading this book will provide you with a deep understanding
    of the JDK-provided concurrency primitives that are utilized by the Scala standard
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The return type of the factory methods is a more specialized version of `ExecutionContext`.
    The standard library defines the following inheritance chain for `ExecutionContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, in the `ExecutionContext` companion object, we find the implicit context
    used in our first example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The documentation for the definition of `Implicits.global` indicates that this `ExecutionContext`
    is backed by a thread pool with a thread count that is equal to the available
    processor count. Our dive into `ExecutionContext` shows us how the simple `Future`
    example runs. We can illustrate how a `Future` applies its `ExecutionContext`
    to execute on multiple threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We extend the original snippet to print the name of the thread performing each
    transformation. When run on a machine with multiple cores, this snippet yields
    variable output, depending on which threads pick up the transformations. Here
    is an example output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This example shows that one `worker-3` thread performed the `map` transformation
    while another `worker-5` thread performed the `filter` transformation. There are
    two key insights to draw from our simple example about how `Future` affects control
    flow. First, `Future` is a data type for concurrency that enables us to break
    the control flow of a program into multiple logical threads of processing. Second,
    our example shows that `Future` begins execution immediately upon creation. This
    means that transformations are applied immediately in a different flow of the
    program. We can use these insights to improve the runtime performance of Dave's
    crazy ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Future and crazy ideas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We apply `Future` to Dave''s set of backtests to improve performance. We believe
    there is an opportunity for a performance improvement because Dave''s laptop has
    four CPU cores. This means that by adding concurrency to our program, we will
    be able to benefit from runtime parallelism. Our first attempt utilizes a for-comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Each backtest invocation is wrapped with the creation of a `Future` instance
    by calling `Future.apply`. This companion object method uses a by-name parameter
    to defer evaluation of the argument, which, in this case, is the invocation of `backtest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the new version of `CrazyIdeas.scala`, you are disappointed to
    see the runtime execution has not improved. You quickly double-check the number
    of CPUs on your Linux box, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Having confirmed there are eight cores available on your laptop, you wonder
    why the execution time matches the original serial execution time. The solution
    here is to consider how the for-comprehension is compiled. The for-comprehension
    is equivalent to the following simpler example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In this desugared representation of the for-comprehension, we see that the
    second `Future` is created and evaluated within the `flatMap` transformation of
    the first `Future`. Any transformation applied to a `Future` (for example, `flatMap`)
    is only invoked once the value provided to the transform has been computed. This
    means that the `Future` in the preceding example and the for-comprehension are
    executed sequentially. To achieve the concurrency that we are looking for, we
    must instead modify `CrazyIdeas.scala` to look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, four backtests are kicked off concurrently and the results
    are transformed into a `Future` of a `Tuple4` consisting of four `BacktestPerformanceSummary`
    values. Seeing is believing, and after showing Dave the faster runtime of his
    backtests, he is excited to iterate quickly on new backtest ideas. Dave never
    misses a chance to throw around a pun, exclaiming, "Using all my cores is making
    my laptop fans really whiz. Not sure I'm a fan of the noise, but I sure do like
    the performance!"
  prefs: []
  type: TYPE_NORMAL
- en: Future usage considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we illustrated the ease of use of the `Future` API
    by investigating how to introduce concurrency to the backtester. Like any powerful
    tool, your usage of `Future` must be disciplined to ensure correctness and performance.
    This section evaluates topics that commonly cause confusion and error when using
    the `Future` to add concurrency to your program. We will detail performing side-effects,
    blocking execution, handling failures, choosing an appropriate execution context,
    and performance considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Performing side-effects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When programming with `Future`, it is important to remember that `Future` is
    inherently a side-effecting construct. Unless the `success` or `failure` factory
    methods are used to lift a value into a `Future`, work is scheduled to be executed
    on a different thread (part of the `ExecutionContext` that is used to create the `Future`).
    More importantly, once executed, a `Future` cannot be executed again. Consider
    the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `Future` is computed and prints `FOO` as expected. We can then access the
    value wrapped in the `Future`. Note that when accessing the value, nothing is
    printing on the console. Once completed, the `Future` is merely a wrapper for
    a realized value. If you want to perform the computation again, you need to create
    a new instance of `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that the preceding example uses `Future.value` to extract the result of
    the computation. This is for the sake of simplicity. Production code should rarely,
    if ever, use this method. Its return type is defined as `Option[Try[A]]`. An `Option`
    is used to represent the case of a completed `Future` with a `Some`, and an unrealized `Future`
    with a `None`. Furthermore, remember that a realized `Future` can have two states:
    success or failure. This is the purpose of the inner `Try`. Like `Option.get`,
    it is almost never a good idea to use `Future.value`. To extract a value from
    a `Future`, refer to the additional techniques described next.'
  prefs: []
  type: TYPE_NORMAL
- en: Blocking execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we added concurrency to the backtester, we wrote a for-comprehension returning
    `Future[(BacktestPerformanceSummary, BacktestPerformanceSummary, BacktestPerformanceSummary,
    BacktestPerformanceSummary)]`, which may leave you wondering how you access the
    value wrapped in the `Future`. Another way of asking the question is, "Given `Future[T]`,
    how do I return `T`?" The short answer is, "You don't!" Programming with many `Future`
    requires a shift in thinking away from synchronous execution towards asynchronous
    execution. When programming with an asynchronous model, the goal is to avoid working
    with `T` directly because it implies a synchronous contract.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, there are situations where it is useful to have the `Future[T]
    => T` function. For example, consider the backtester snippet. If the code from
    the snippet is used to create a program by defining an `object` extending `App`,
    the program will terminate before backtesting completes. As the threads in the `ExecutionContext`
    global are daemon threads, the JVM terminates immediately after creating the `Future`.
    In this scenario, we need a synchronization mechanism to pause execution until
    the result is ready. By extending the `Awaitable` trait, `Future` is able to provide
    such facilities. The `Await` module exposes two methods that achieve this goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As `Future` extends `Awaitable`, a `Future` can be supplied as an argument
    to either method. The `ready` halts program flow until `T` is available and returns
    the completed `Future[T]`. In practice, `ready` is rarely used because it is conceptually
    strange to return a `Future[T]` from a synchronous call instead of `T`. You are
    more likely to commonly use `result`, which provides the desired transformation
    returning `T` given `Future[T]`. For example, `CrazyIdeas.scala` can be modified
    to look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we see the blocking, synchronous invocation of `Await.result`
    to return the `Tuple` of `Future[BacktestPerformanceSummary]`. This blocking call
    is parameterized with a timeout to defend against the scenario where the `Future`
    is not computed within a certain amount of time. Using a blocking call to retrieve
    the backtest results means that the JVM will only exit after the backtest completes
    or when the timeout expires. When the timeout expires and the backtest is incomplete, `result`
    and `ready` throw a `TimeoutException`.
  prefs: []
  type: TYPE_NORMAL
- en: Blocking execution of your program is potentially detrimental to your program's
    performance, and it should be used with caution. Using the methods on the `Await`
    companion object make blocking calls easy to recognize. As `ready` and `result`
    throw an exception when timing out, rather than returning a different data type,
    you must take extra caution to handle this scenario. You should treat any synchronous
    call involving asynchrony (that either does not provide a timeout or does not
    handle the timeout) as a bug.
  prefs: []
  type: TYPE_NORMAL
- en: Programming asynchronously requires a mindset shift to write a program that
    describes what to do when the value appears rather than writing programs that
    require a value to exist before acting on it. You should be suspicious of any
    use of `Await` that interrupts transformation of a to-be-computed value. A set
    of transformations should be composed by acting upon `Future[T]` instead of `T`.
    Usage of `Await` should be restricted to scenarios where a program has no other
    work to perform and requires the result of the transformation, as we saw with
    the backtester.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As the standard library models timeout with an exception instead of a different
    return type, it is hard to enforce that a timeout is always handled One way to
    improve safety is to write a utility method that returns `Option[T]` instead of `T`
    to account for the timeout scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: With this new method, an entire error class is eliminated. As you encounter
    other unsafe transformations, consider defining methods that return a data type
    that encodes expected errors to avoid accidentally mishandling the transformation
    result. What other examples of unsafe transformations come to mind?
  prefs: []
  type: TYPE_NORMAL
- en: Handling failures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Working with `Future` requires disciplined handling of error scenarios to avoid
    writing a set of transformations that are difficult to reason about. When an exception
    is thrown inside a `Future` transformation, it bubbles up within the transformation''s
    thread of computation and interrupts downstream transformations. Consider the
    following motivating example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: What do you expect to occur after the first `map` transformation? It is clear
    that the transformation will fail because the provided input cannot be cast to
    an integer. In this scenario, the `Future` is considered to be a failed `Future`
    and downstream transformations operating on the wrapped `Int` value, in this example,
    will not occur. In this simple example, it is obvious that the transformation
    cannot continue. Imagine a larger code base operating on data more complicated
    than a single integer with multiple failure scenarios across multiple namespaces
    and source files. In a real-world setting, it is more challenging to identify
    where an asynchronous computation broke down.
  prefs: []
  type: TYPE_NORMAL
- en: '`Future` provides facilities for handling failures. It provides `recover` and `recoverWith`
    in order to continue downstream transformations. The signatures are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference between these two recovery methods is that the partial function
    provided to `recover` returns `U`, while `recoverWith` returns `Future[U]`. In
    our previous example, we can use `recover` to supply a default value to continue
    a transformation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this snippet produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This approach allows you to continue a pipeline of transformations when one
    transform fails, but it suffers from the same shortcoming as the methods on `Await`.
    The returned `Future[T]` data type does not reflect the possibility of failure.
    Using recovery methods is error-prone because it is impossible to know whether
    the error conditions have been handled without reading through the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The error handling that we investigated is appropriate to handle failures during
    a computation. It is likely that after a series of transformations complete, you
    will wish to perform special logic. Imagine you are building a web service that
    submits trading orders to exchanges. Order submission is successful if the order
    was submitted to the exchange; otherwise, it is considered a failed submission.
    As order submission involves communication with an external system, the exchange,
    you modeled this action with a `Future`. Here is what the method handling order
    submission looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: An `ExecutionContext`, a way to submit orders, and a way to update a customer's
    trading positions after the trade is submitted allow a customer provided `RawOrder`
    to be submitted to the exchange. In the first processing step, the `RawOrder`
    is converted into a `ValidatedOrder`, and then lifted into a `Future`. `Future.failure`
    and `Future.successful` are convenient ways to lift or to wrap a computed value
    into a `Future`. The value is lifted into a `Future` to allow the entire sequence
    of steps to be written as a single for-comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: Following the completion of all processing steps, `onComplete` is invoked to
    asynchronously handle completion of request processing. You can imagine in this
    context that completing request processing implies creating a serialized version
    of a response and transmitting this to the caller. Previously, the only mechanism
    at our disposal to perform work once a value is computed is to block using `Await`. `onComplete`
    is an asynchronously invoked callback that registers a function to be invoked
    when the value is completed. As shown in the example, `onComplete` supports handling
    success and failure cases, which makes it a general-purpose tool to handle the
    outcome of a `Future` transformation. In addition to `onComplete`, `Future` provides `onFailure`
    specifically for failure cases and `onSuccess` and `foreach` specifically for
    success cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'These callback methods expose a method signature that returns `Unit`. As a
    functional programmer, you should be leery of invoking these methods because they
    are side-effecting. The `onComplete` invocations should only happen at the absolute
    end of a computation when a side-effect can no longer be deferred. In the web
    service example, the side-effect is transmission of the response to the caller.
    Another common use case for using these side-effecting callbacks is to handle
    cross-cutting concerns, such as application metrics. Coming back to the web service,
    here is one way to increment an error counter when order submission to the exchange
    fails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, a side-effect is performed when submission to the exchange
    fails via the `onFailure` callback. In this isolated example, it is straightforward
    to track where the side-effect is happening. However, in a larger system it can
    be a challenge to identify when and where callbacks were registered. Additionally,
    from the `Future` API documentation, we learn that callback execution is unordered,
    which indicates that all callbacks must be treated independently. This is why
    you must be disciplined about when and where you apply these side-effects.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach to error handling is to use a data type that can encode
    errors. We have seen this approach applied with `Await` when `Option` was the
    returned data type. `Option` makes it clear that the computation might fail while
    remaining convenient to use because its transformations (for example, `map`) operate
    on the wrapped value. Unfortunately, `Option` does not allow us to encode the
    error. In this case, it is helpful to use another tool from the Scalaz library
    called disjunction. Disjunction is conceptually similar to `Either`, which can
    be used to represent one of two possible types. Disjunction is different from `Either`
    because its operations are right-biased. Let''s take a look at a simple example
    to illustrate this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `\/` is the shorthand symbol used by Scalaz to represent a disjunction.
    In this example, a right disjunction is created by wrapping the one integer literal.
    This disjunction either has the `Throwable` value or the `Int` value, and it is
    analogous to `Either[Throwable, Int]`. In contrast to `Either`, the `map` transformation
    operates on the right side of the disjunction. In this example, `map` accepts
    an `Int` value as input because the right side of the disjunction is an `Int`
    value. As disjunction is right-biased, it is a natural fit to represent failure
    and success values. Using the infix notation, it is common to define error handling
    with `Future` as `Future[Throwable \/ T]`. In place of `Throwable`, one can define
    an ADT of possible error types to make error handling explicit. This approach
    is favorable because it enforces handling of failure cases without relying on
    the author to invoke a recovery method. If you are interested to learn more about
    how to use disjunction as a tool for error handling, review Eugene Yokota's excellent
    Scalaz tutorial at [http://eed3si9n.com/learning-scalaz/Either.html](http://eed3si9n.com/learning-scalaz/Either.html).
  prefs: []
  type: TYPE_NORMAL
- en: Hampering performance through executor submissions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As `Future` provides an expressive and easy-to-use API, it is common to perform
    numerous transforms to complete a computation in a large-scale system. Reflecting
    on the order submission web service mentioned in the previous section, you can
    imagine multiple application layers operating on a `Future`. A production-ready
    web service typically composes together multiple layers to service a single request.
    An example request flow may contain the following stages: request deserialization,
    authorization, application service invocation, database lookups and/or third-party
    service callouts, and response translation to a JSON format. If each of these
    stages in the workflow is modeled with a `Future`, then it is common to have five
    or more transformations to handle a single request.'
  prefs: []
  type: TYPE_NORMAL
- en: Decomposing your software system into small areas of responsibility in a way
    that is similar to the preceding example is a good engineering practice to support
    testing in isolation and improving maintainability. However, this approach to
    software design comes with a performance cost when working with `Future`. As we
    have seen through our example usage, nearly all transforms on a `Future` require
    submitting work to an `Executor`. In our example workflow, most stages in the
    transformation are small. In this scenario, the overhead of submitting work to
    the executor dominates the execution time of the computation. If the order submission
    web service services numerous customers with stringent throughput and latency
    requirements, then it is possible that engineering practices focusing on testability
    and maintainability will result in poorly performing software.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hampering performance through executor submissions](img/B05368_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you consider the preceding diagram, you can see a thread pool with four threads
    being used to apply transforms to a `Future`. Each transform is submitted to the
    pool and there is a chance that a different thread is picked for the computation.
    This diagram visualizes how multiple small transforms may hamper performance due
    to the overhead of `Executor` submissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just how large is the overhead of `Executor` submissions? This is the motivating
    question to write a benchmark to quantify the overhead of submitting work to an `Executor`.
    The benchmark focuses on adding 1 to an integer N-times in two ways. One approach
    is to perform the addition operation within a single `Future`, while the second
    approach is to perform each addition operation with a new `Future` transformation.
    The latter approach is a proxy for the stages of order submission request processing
    that uses multiple `Future` transformations in a larger software system. Performing
    integer addition is the proxy operation because it is an extremely cheap computation,
    which means that the execution time will be dominated by `Executor` submissions.
    The benchmarks look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`TransformFutureState` allows the number of operations to be parameterized.
    `manyTransforms` represents each addition operation using a `map` transformation
    that involves submitting work to an `Executor`. `oneTransform` performs all addition
    operations using a single `Executor` submission via `Future.apply`. In this controlled
    test, `Await.result` is used as a blocking mechanism to await the completion of
    the computation. The results of running this test on a two-core machine with five
    transformations and ten transformations can be seen in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Map count** | **Throughput (ops per second)** | **Error
    as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `manyTransforms` | 5 | 463,614.88 | ± 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| `oneTransform` | 5 | 412,675.70 | ± 0.81 |'
  prefs: []
  type: TYPE_TB
- en: '| `manyTransforms` | 10 | 118,743.55 | ± 2.34 |'
  prefs: []
  type: TYPE_TB
- en: '| `oneTransform` | 10 | 316,175.79 | ± 1.79 |'
  prefs: []
  type: TYPE_TB
- en: While both scenarios yield comparable results with five transformations, we
    can see a clear difference with ten transforms being applied. This benchmark makes
    it clear that `Executor` submissions can dominate performance. Although the cost
    can be high, our advice to you is to model your system without considering this
    cost up-front. In our experience, it is easier to rework a well-modeled system
    for performance improvements than it is to extend or to rework a poorly-modeled
    but performant system. For this reason, we advise against going to great lengths
    to group `Executor` submissions when attempting to put together the initial version
    of a complex system.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a good design in place, the first step is to benchmark and to
    profile in order to identify whether `Executor` submissions are the bottleneck.
    In the event that you discover that your style of `Future` usage is causing performance
    bottlenecks, there are several courses of action you should consider.
  prefs: []
  type: TYPE_NORMAL
- en: The lowest cost development option is to replace unnecessarily costly `Future`
    creation with the use of `Future.success` or `Future.failure`. The order submission
    web service took advantage of these factory methods to lift values into a `Future`.
    As the value is already computed, these factory methods avoid submitting any tasks
    to the `Executor` that are referenced by the provided `ExecutionContext`. Replacing
    usages of `Future.apply` with either `Future.successful` or `Future.failure` when
    the value is already computed can yield cost savings.
  prefs: []
  type: TYPE_NORMAL
- en: A more expensive alternative in terms of development effort is to rework your
    implementation to group together `Future` transformations in a way similar to `manyTransforms`.
    This tactic involves reviewing each application layer to determine whether transforms
    within a single layer can be combined. If possible, we recommend that you avoid
    merging transformations across application layers (for example, between request
    deserialization or authorization and application service processing) because this
    weakens your model and increases maintenance cost.
  prefs: []
  type: TYPE_NORMAL
- en: If neither of these options produces acceptable performance, then it may be
    worthwhile to discuss with the product owners the option of addressing the performance
    issue with hardware. As your system's design has not been compromised and it reflects
    solid engineering practices, then it likely can be horizontally scaled or clustered.
    Depending on the state tracked by your system, this option might be possible without
    additional development work. Perhaps product owners value a system that can be
    easily maintained and extended more than performance. If this is the case, adding
    scale to your system may be a viable way forward.
  prefs: []
  type: TYPE_NORMAL
- en: Provided that you are unable to buy your way out of the performance challenge,
    then there are three additional possibilities. One option is to investigate an
    alternative to `Future`, named `Task`. This construct, which is provided by the
    Scalaz library, allows computations to be performed with fewer `Executor` submissions.
    This option involves significant development because the `Future` data type will
    need to be replaced throughout the application with `Task`. We will explore `Task`
    at the end of this chapter and investigate the performance benefits that it can
    provide.
  prefs: []
  type: TYPE_NORMAL
- en: Independent of using `Task`, it can be useful to review your application's model
    to critically question whether or not there is unnecessary work being done on
    the critical path. As we saw with MVT's reporting infrastructure and the introduction
    of stream processing, it is sometimes possible to rethink a design to improve
    performance. Like the introduction of `Task`, reconsidering your system's architecture
    is a large-scale change. The last resort option is to merge application layers
    in order to support grouping `Future` transformations. We advise against exercising
    this option, unless all other suggestions have failed. This option results in
    a code base that is more difficult to reason about because concerns are no longer
    separated. In the short-run, you may reap performance benefits, but in our experience,
    these benefits are outweighed in the long-run by the cost of maintaining and extending
    such a system.
  prefs: []
  type: TYPE_NORMAL
- en: Handling blocking calls and callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described in the first part of this chapter, the `Future` API provides an
    elegant way to write concurrent programs. As it is considered a bad practice to
    block on a `Future`, it is not unusual to see `Future` being widely used across
    an entire code base. However, it is unlikely that your system is only composed
    of your own code. Most real-world applications leverage existing libraries and
    third-party software to avoid re-implementing existing solutions to some common
    problems (such as data encoding and decoding, communication over HTTP, database
    drivers, and so on). Unfortunately, not all libraries use the future API, and
    it may become a challenge to gracefully integrate them into your system. In this
    section, we will examine some common pitfalls that you may encounter and mention
    possible workarounds.
  prefs: []
  type: TYPE_NORMAL
- en: ExecutionContext and blocking calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While working on the backtester, you noticed that one module of the code is
    used to load some historical buy orders from a relational database. Since you
    started rewriting the application to take advantage of `Future`, the module API
    is fully asynchronous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'However, after profiling the application, you noticed that this part of the
    code performs quite poorly. You attempted to increase the database connection
    count, first doubling it, then tripling it, both without success. Attempting to
    understand the cause of the problem, you look at all the locations where the method
    is called, and you noticed the following pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: All the callers are importing the global `ExecutionContext` to be implicitly
    used by the method. The default thread pool is backed by a `ForkJoinPool`, and
    it is sized based on the available cores on the machine. As such, it is CPU-bound
    and designed to handle nonblocking, CPU intensive operations. This is a good choice
    for applications that do not perform blocking calls. However, if your application
    runs blocking calls asynchronously (that is, in a `Future` execution), relying
    on the default `ExecutionContext` will most likely quickly degrade performance.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous versus nonblocking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before going further, we want to clarify some of the terms used in this section.
    Nonblocking can be a confusing term in the context of concurrency. When using
    `Future`, we perform asynchronous operations, meaning that we start a computation so it
    can proceed with the flow of the program. The computation is executed in the background
    and will eventually yield a result. This behavior is sometimes called nonblocking,
    meaning that the API call returns immediately. However, blocking and nonblocking
    most often refer to I/O operations and how they are performed, especially how
    the thread that is performing the operation is used. For example, writing a sequence
    of bytes to a local file can be a blocking operation because the thread calling `write`
    will have to wait (block) until the I/O operation is completed. When using nonblocking
    constructs, such as the ones provided in the `java.nio` package, it is possible
    to perform I/O operations that will be executed without blocking a thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to implement an API with a combination of the following behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **API characteristics** | **Returns** | **Blocks a thread?** |'
  prefs: []
  type: TYPE_TB
- en: '| Synchronous/blocking | At the end of the computation | Yes, the calling thread
    executes the operation |'
  prefs: []
  type: TYPE_TB
- en: '| Asynchronous/blocking | Immediately | Yes, this blocks a thread from a dedicated
    pool |'
  prefs: []
  type: TYPE_TB
- en: '| Asynchronous/nonblocking | Immediately | No, the thread is freed-up while
    the blocking operation is performed |'
  prefs: []
  type: TYPE_TB
- en: Using a dedicated ExecutionContext to block calls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Clearly, our problem is that we are using the `ExecutionContext` global to
    perform blocking calls. We are querying a relational database, and most JDBC drivers
    are implemented to perform blocking calls. The pooled threads call the driver
    and block while waiting for the query and the response to travel over the network,
    making them unusable by other computations. An option is to create a dedicated `ExecutionContext`
    to execute the `Future`, including blocking operations. This `ExecutionContext`
    is sized with more threads in the anticipation that they will be blocked when
    performing their computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The first benefit is that we have more threads available, meaning that we can
    initiate more queries concurrently. The second benefit is that the other asynchronous
    computations performed in our system are done on a separate pool (for example,
    the global context), and they will avoid starvation since no threads are blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'We write a short benchmark to evaluate the performance of our new system. In
    this example, we use a mock implementation of `findBuyOrders` to simulate querying
    the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass the `ExecutionContext` as a parameter. Our benchmark compares the throughput
    of an application relying on the default `ExecutionContext` and one using an `ExecutionContext`,
    which is dedicated to blocking operations; the latter is initialized with twenty
    times more threads. The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Operation count** | **Throughput (ops per second)** | **Error
    as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `withDefaultContext` | 10 | 3.21 | ± 0.65 |'
  prefs: []
  type: TYPE_TB
- en: '| `withDedicatedContext` | 10 | 9.34 | ± 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `withDefaultContext` | 1,000 | 0.04 | ± 2.56 |'
  prefs: []
  type: TYPE_TB
- en: '| `withDedicatedContext` | 1,000 | 0.73 | ± 0.41 |'
  prefs: []
  type: TYPE_TB
- en: The results confirm our intuition. The dedicated pool is bigger than the default
    context in anticipation of threads being blocked waiting for a blocking operation
    to finish. Having more threads available, it is able to start more blocking operations
    concurrently, thus achieving a better throughput. Creating a dedicated `ExecutionContext`
    is a good way to isolate blocking operations and make sure that they do not slow
    down CPU-bound computations. When designing your dedicated thread pool, make sure
    that you understand how the underlying resources (for example, connections, file
    handles, and so on) are used. For example, when dealing with a relational database,
    we know that one connection can only be used to perform one query at a time. A
    good rule of thumb is to create a thread pool with as many threads as the amount
    of connections that you want to open with your database server. If the number
    of connections is less than the thread count, some threads may be waiting for
    a connection and remain unused. If you have more connections than threads, the
    opposite situation may occur and some connections may remain unused.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good strategy is to rely on the type system and the compiler to ensure that
    you are not mixing up different `ExecutionContext` instances. Unless the type
    is differentiated, you may accidentally use a CPU-bound context when performing
    blocking operations. You can create your own `DatabaseOperationsExecutionContext`
    type wrapping an `ExecutionContext`, and accept this type when creating your database
    access module. Another idea is to use tagged types that are provided by Scalaz.
    Refer to [Chapter 3](ch03.html "Chapter 3. Unleashing Scala Performance"), *Unleashing
    Scala Performance*, for a refresher on tagged types. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Using a tagged types for our `ExecutionContext` gives us additional safety.
    It is easy to make a mistake in the `main` method while wiring up your application,
    and inadvertently use the wrong `ExecutionContext` when creating your modules.
  prefs: []
  type: TYPE_NORMAL
- en: Using the blocking construct
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The standard library provides a `blocking` construct that can be used to signal
    blocking operations executed inside a `Future`. We can modify our previous example
    to leverage `blocking` instead of a dedicated `ExecutionContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the preceding implementation, we use the default `ExecutionContext`
    to execute the `Future`. The `blocking` construct is used to notify the `ExecutionContext`
    that a computation is blocking. This allows the `ExecutionContext` to adapt its
    execution strategy. For example, the default global `ExecutionContext` will temporarily
    increase the number of threads in the pool when it performs a computation wrapped
    with `blocking`. A dedicated thread is created in the pool to execute the blocking
    computation, making sure that the rest of the pool remains available for CPU-bound
    computations.
  prefs: []
  type: TYPE_NORMAL
- en: You should use `blocking` cautiously. The `blocking` construct is merely used
    to notify `ExecutionContext` that the wrapped operation is blocking. It is the
    responsibility of the `ExecutionContext` to implement a specific behavior or ignore
    the notification. The only implementation that actually takes it into account
    and implements special behavior is the default `ExecutionContext` global.
  prefs: []
  type: TYPE_NORMAL
- en: Translating callbacks with Promise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While `Future` is the main construct of the `scala.concurrent` API, another
    useful abstraction is `Promise`. `Promise` is another way to create and complete
    a `Future`. The `Future` is a read-only container for a result that will eventually
    be computed. `Promise` is a handle that allows you to explicitly set the value
    contained in a `Future`. A `Promise` is always associated with only one `Future`,
    and this `Future` is specific to the `Promise`. It is possible to complete the
    `Future` of a `Promise` with a successful result, or an exception (which will
    fail the `Future`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a short example to understand how `Promise` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: A `Promise` can only be used once to complete its associated `Future`, either
    with a success or a failure. Attempting to complete an already realized `Promise`
    will throw an exception, unless you use `trySuccess`, `tryFailure`, or `tryComplete`.
    These three methods will attempt to complete the `Future` that is linked to the `Promise`
    and return `true` if the `Future` was completed or `false` if it was already previously
    completed.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be wondering in what circumstances you would really take
    advantage of `Promise`. Especially considering the previous example, would it
    be simpler to return the internal `Future` instead of relying on a `Promise`?
    Keep in mind that the preceding snippet is meant to demonstrate a simple workflow
    that illustrates the `Promise` API. However, we understand your question. In practice,
    we see two common use cases for `Promise`.
  prefs: []
  type: TYPE_NORMAL
- en: From callbacks to a Future-based API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first use case is to turn a callback-based API into a `Future`-based API.
    Imagine having to integrate with a third-party product, such as the proprietary
    database that MVT obtained recently by purchasing usage licenses. This is a great
    product that is used to store historical quotes per timestamp and ticker. It comes
    with a library to be used by a client application. Unfortunately, this library,
    while fully asynchronous and nonblocking, is callback-oriented, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'There is no doubt that the client works fine; after all, MVT paid a lot of
    money for it! However, it will not be easy to integrate it with your own application.
    Your program relies heavily on `Future`. This is where `Promise` can help us,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to the `Promise` abstraction, we are able to return a `Future`. We simply
    use `success` and `failure` in the respective callbacks to call the proprietary
    client. This use case often arises in production when you have to integrate with
    a Java library. Even though Java 8 introduced a significant improvement to the
    Java concurrent package, most Java libraries still rely on callbacks to implement
    asynchronous behavior. Using `Promise`, you can fully leverage the existing Java
    ecosystem in your program without giving up on Scala support for concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Future with Promise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Promise` can also be used to combine instances of `Future`. For example, let''s
    add a timeout capability to `Future`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Our method takes a by-name `Future` (that is, a `Future` that has not started
    its execution yet) as well as the timeout value to apply. In the method, we use
    a `Promise` as a container for the result. We start an internal `Future` that
    will block for the timeout duration before failing the `Promise` with an `Exception`.
    We also start the main `Future` and register a callback to complete the `Promise`
    with the result of the computation. The first of the two `Futures` that terminates
    will effectively complete the `Promise` with its result. Note that in this example,
    we use `tryFailure` and `tryCompleteWith`. It is likely that both `Futures` will
    eventually terminate and try to complete the `Promise`. We are only interested
    in the result of the first one that completes, but we also want to avoid throwing
    an `Exception` when attempting to complete an already realized `Promise`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding example is a naive implementation of a timeout. It is mostly a
    prototype used to demonstrate how `Promise` can be leveraged to enriched `Future`
    and implement complex behavior. A more realistic implementation would probably
    involve a `ScheduledExecutorService`. A `ScheduledExecutorService` allows you
    to schedule the execution of a computation after a certain delay. It allows us
    to schedule the call to `tryFailure` without blocking a thread with a call to `Thread.sleep`.
    We made the choice to keep this example simple and not introduce a new type, but
    we encourage you to research this implementation of `ScheduledExecutorService`.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you may occasionally have to write your own custom combinators
    for `Future`. `Promise` is a useful abstraction in your toolbox if you need to
    do this. However, `Future` and its companion object already provide a number of
    built-in combinators and methods that you should try to leverage as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Tasked with more backtest performance improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discovering `Future` and adopting an asynchronous mindset helped you better
    utilize your computing resources to test multiple strategies and tickers faster.
    You improved performance by treating the backtest as a black box. Without changing
    the implementation of the backtest, there were straightforward performance wins.
    Identifying logical sequences of transformations as candidates for concurrency
    is a good strategy to apply when considering how to speed up your software.
  prefs: []
  type: TYPE_NORMAL
- en: Let's extend this idea to a smaller logical unit of processing within the backtester.
    A backtest exercises a strategy for a ticker across a time period. After speaking
    with Dave, you discover that MVT does not maintain positions overnight. At the
    end of each trading day, MVT trading systems mitigate risk by ensuring that all
    stock positions are liquidated. This is done to defend against volatile overnight
    price moves after the market closes, which the company is unable to react to by
    trading. As positions are not held overnight, each trading day can be simulated
    independently of the previous trading day. Returning to our asynchronous mindset,
    this insight implies that trading day simulations can be performed concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into the implementation using `Future`, we will share an alternative
    abstraction, named `Task`, which is provided by the Scalaz library. `Task` provides
    compelling usage reasons for our proposed backtest modifications. We introduce `Task`
    next, provided that you are up to the task!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Scalaz Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scalaz `Task` provides a different approach to achieve concurrency. Although `Task`
    can be used in a way that mimics `Future`, there are important conceptual differences
    between these two abstractions. `Task` allows fine-grained control over asynchronous
    execution, which provides performance benefits. `Task` maintains referential transparency
    as well, which provides stronger reasoning abilities. Referential transparency
    is a property of expressions that are side-effect free. To better understand this
    principle, consider the following simple `sum` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Imagine that we are performing two summations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As `sum` is side-effect free, we can replace `sum(2, 3)` with its result, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This expression will always evaluate to 9, which satisfies referential transparency.
    Now imagine a twist in the implementation of `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, `sum` includes a side-effect of writing to a database that breaks referential
    transparency. We can no longer perform the replacement of `sum(2, 3)` with the
    value 9 because then the database will not be updated. Referential transparency
    is a concept at the heart of the functional programming paradigm because it provides
    strong reasoning guarantees. The Haskell wiki provides additional commentary and
    examples worth reviewing at [https://wiki.haskell.org/Referential_transparency](https://wiki.haskell.org/Referential_transparency).
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at common `Task` API usage to better understand how `Task`
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and executing Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The methods provided by the `Task` companion object are the main entry points
    to the API, and the best ways to create an instance of `Task`. The `Task.apply`
    is the first method to inspect. It takes a computation returning an instance of `A`
    (that is, a by-name parameter of the `A` type) and an implicit `ExecutorService`
    to run the computation. Contrary to `Future`, which uses `ExecutionContext` as
    an abstraction for a thread pool, `Task` uses the `ExecutorService`, which is
    defined in the Java standard library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing that you may have noticed is that, even though we instantiated
    a new `Task`, nothing is printed on the screen. This is an important difference
    when comparing `Task` and `Future`; while `Future` is eagerly evaluated, a `Task`
    is not computed until you explicitly ask for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example calls the `unsafePerformSync` instance method to execute
    the task. We can see the `println` as well as the returned result `42`. Note that `unsafePerformSync`
    is an unsafe call. If the computation throws an exception, the exception is re-thrown
    by `unsafePerformSync`. To avoid this side-effect, calling `unsafePerformSyncAttempt`
    is preferred. The `unsafePerformSyncAttempt` instance catches the exception and
    has a return type of `Throwable \/ A`, which allows you to cleanly handle the
    failure case. Note that when creating the task `t`, we did not provide an `ExecutorService`.
    By default, `apply` creates a `Task` to be run on `DefaultExecutorService`, a
    fixed thread pool for which the size is based on the count of available processors
    on the machine using a default parameter. The `DefaultExecutorService` is analogous
    to the global `ExecutionContext` that we explored with `Future`. It is CPU-bound
    and sized based on the available cores on the machine. We can also supply a different `ExecutorService`
    at creation time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the `Task` is executed on the supplied `ExecutorService`,
    not on the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of `Task` execution, let''s perform a little experiment. We will create
    an instance of `Task` and call `unsafePerformSync` twice in a row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We observe that `Starting task` prints after each call to `unsafePerformSync`.
    This indicates that the full computation is executed each time we call `unsafePerformSync`.
    That is another difference with `Future`. While a `Future` memorizes its result
    after the computation, a `Task` performs its computation each time we call `unsafePerformSync`.
    In other words, `Task` is referentially transparent and, therefore, closer to
    the functional programming paradigm than `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like `Future`, it is possible (and even recommended) to use `Task` in an asynchronous
    way. An instance of `Task` can be executed asynchronously by calling `unsafePerformAsync`.
    This method takes a callback of type`(Throwable \/ A) => Unit` that is called
    at the end of the computation. Observe the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We create our`Task`, and add a `Thread.sleep` to simulate an expensive computation.
    We call `unsafePerformAsync` and use a simple callback to print the answer (or
    an exception, if the computation fails). We call `createAndRunTask` and observe
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can see that our last statement, "Waiting for the answer" was printed first.
    This is because `unsafePerformAsync` returns immediately. We can see the statement
    from our computation, as well as the answer printed in our callback. This method
    is a rough equivalent to `onComplete`, which is defined on Scala's `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful method provided by the companion object of `Task` is `async`.
    Remember how we previously used `Promise` to turn a callback-based API into an
    API returning an instance of `Future`? It is possible to achieve the same goal
    with `Task`; that is, we can turn a callback-based API into a more monadic API
    returning a `Task`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluating this method in the REPL yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `doCoolThingsToTask` method uses `Task.async` to create a `Task` instance
    from a callback-based API that is defined in `CallbackAPI`. The `Task.async` can
    even be used to turn a Scala `Future` into a Scalaz `Task`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have to supply an `ExecutionContext` to be able to call `onComplete`
    on `Future`. This is due to `Future` eager evaluation. Almost all methods that
    are defined on `Future` will submit a computation to a thread pool immediately.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is also possible to convert a `Task` to a `Future`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The execution model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Understanding the `Task` execution model requires understanding the Scalaz `Future`
    execution model because `Task` composes a Scalaz `Future` and adds error handling.
    This is visible from the definition of `Task`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In this definition, `Future` is the not the Scala standard library version,
    but instead this is an alternative version that is provided by Scalaz. The Scalaz `Future`
    decouples defining transformations from execution strategy, providing us with
    fine-grained control over `Executor` submissions. Scalaz `Future` accomplishes
    this by defining itself as a trampolining computation. Trampolining is a technique
    that describes a computation as a discrete series of chunks that are run using
    constant space. To dive into the details of how a trampoline works, we recommend
    reading Runar Bjarnason's paper, *Stackless Scala With Free Monads*, available
    at [http://blog.higher-order.com/assets/trampolines.pdf](http://blog.higher-order.com/assets/trampolines.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '`Task` builds on Scalaz `Future` by providing error handling with the Scalaz `\/`
    disjunction. `Task` is the description of a computation. Transformations add to
    the description of the computation that will eventually be executed by a thread
    pool. To begin evaluation, a `Task` must be explicitly started. This behavior
    is interesting because when a `Task` is finally executed, we can limit computation
    execution to a single thread. This improves thread reuse and reduces context switching.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The execution model](img/B05368_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we see various calls to `apply` and `map`. These calls
    are merely modifying the definition of the task to be performed. It is only when
    we call `unsafePerformAsync` that the computation is realized in a different thread.
    Note that all the transforms are applied by the same thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can exercise `Future` and `Task` performance in a short microbenchmark comparing
    their throughput based on the transform (for example, `map` and `flatMap`), and
    the count of transformations applied. A snippet of the benchmark can be found,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Both scenarios run similar computations. We create an initial instance of `Future`
    or `Task` containing 0, and we apply several consecutive `map` operations to add
    1 to the accumulator. Two other scenarios performed the same computation but with `flatMap`
    instead. The results for `flatMap` are displayed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Operation count** | **Throughput (ops per second)** | **Error
    as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMapWithFuture` | 5 | 41,602.33 | ± 0.69 |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMapWithTask` | 5 | 59,478.50 | ± 2.14 |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMapWithFuture` | 10 | 31,738.80 | ± 0.52 |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMapWithTask` | 10 | 43,811.15 | ± 0.47 |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMapWithFuture` | 100 | 4,390.11 | ± 1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMapWithTask` | 100 | 13,415.30 | ± 0.60 |'
  prefs: []
  type: TYPE_TB
- en: 'The results for `map` operations can be found in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Operation count** | **Throughput (ops per second)** | **Error
    as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `mapWithFuture` | 5 | 45,710.02 | ± 1.30 |'
  prefs: []
  type: TYPE_TB
- en: '| `mapWithTask` | 5 | 93,666.73 | ± 0.57 |'
  prefs: []
  type: TYPE_TB
- en: '| `mapWithFuture` | 10 | 44,860.44 | ± 1.80 |'
  prefs: []
  type: TYPE_TB
- en: '| `mapWithTask` | 10 | 91,932.14 | ± 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| `mapWithFuture` | 100 | 19,974.24 | ± 0.55 |'
  prefs: []
  type: TYPE_TB
- en: '| `mapWithTask` | 100 | 46,288.17 | ± 0.46 |'
  prefs: []
  type: TYPE_TB
- en: This benchmark highlights the performance gain due to the different execution
    model of `Task`. Even for a small number of transforms, the throughput is better
    with a deferred evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling trading day simulations with Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Equipped with our understanding of `Task`, we now have the knowledge necessary
    to add concurrency to the execution of a single backtest run. You may recall that
    we discovered from Dave that MVT closes its positions at the end of each trading
    day. This insight allows us to model each trading day independently. Let''s familiarize
    ourselves with the current implementation by beginning with the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The profit-and-loss is the output of each simulated trading day. `PnL` provides
    a convenient method to add together two `PnL` instances, which can be used to
    sum the simulation `PnL` across multiple trading days. Once all the trading days
    are simulated, a `BacktestPerformanceSummary` is created to capture the simulation
    profit-and-loss. For our work on the backtester, we will use a `Thread.sleep`
    to simulate computationally expensive work in place of an actual decisioning strategy.
    The length of the sleep is parameterized by `DecisionDelayMillis`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We show a simplified version of the backtester that shows how `DecisionDelayMillis`
    is used to simulate a trading day, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The original backtest displays how a list of days is simulated in a synchronous
    fashion. For reproducibility, we substitute a constant profit-and-loss of $10
    in place of a dynamic value. This backtest ignores the application of a ticker
    and a strategy to focus on the core of our dilemma: How can we use `Task` to add
    concurrency to a backtest?'
  prefs: []
  type: TYPE_NORMAL
- en: 'From our examples, we saw that `Task` introduces concurrency through submission
    of multiple `Task`s to an `ExecutorService` and by performing the side-effect
    of running a `Task` with `unsafePerformAsync` to avoid a blocking wait for the
    result. As a first step, let''s implement a version of the backtest that uses `Task`
    without introducing concurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation changes the return type to `Task[BacktestPerformanceSummary]`.
    Since the `Task` is not run, referential transparency is maintained within this
    method. Each trading day is simulated using `Task.delay`. `delay` is a lazy variant
    of `Task.now` that defers evaluation of the provided value. Let''s look at the
    following signature to confirm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had instead used `Task.now` in place of `Task.delay`, the sleep (that
    is, the simulation) would have taken effect before running `Task`. We also see
    the use of another new capability, `Task.gatherUnordered`. `gatherUnordered` is
    useful when you wish to make the following transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Although `List` is used here, this relationship exists for any `Seq`. `gatherUnordered`
    provides a way to take a collection of `Task` and instead operate on a single `Task`
    that wraps a collection of the underlying type. Let''s look at the following signature
    to make our understanding more concrete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: This signature closely matches the previous function that we defined with the
    addition of an optional Boolean parameter. When `exceptionCancels` is set to `true`,
    any pending `Task` will not be evaluated. `gatherUnordered` allows us to merge
    together the results of each trading day's profit-and-loss and return a single `Task`
    wrapping `BacktestPerformanceSummary`. The Scala `Future` companion object provides
    an analogous method, named `sequence`, that performs the same operation on a sequence
    of `Future`s.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a functioning implementation of the backtest, but it does not add concurrency
    to the simulation of historical trading days. For our next iteration, we take
    advantage of a new part of the `Task` API, `Task.fork`. Let''s see how it is used,
    and then we will explain how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation gathers trading day `PnL` in the same way as before, but
    instead this uses a combination of `Task.fork` and `Task.now` to simulate the
    trading day. Let''s look at the signature of `Task.fork` to understand how runtime
    behavior changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`fork` accepts a `Task` as a by-name parameter and an implicit `ExecutorService`
    that defaults to the CPU-bound executor. The signature shows that `fork` submits
    the provided `Task` to `pool` in order to fork the computation into a different
    thread. `fork` is an explicit way to control concurrency with `Task`. Conceptually, `fork`
    is analogous to any `Future` transformation (for example, `map`) that involves
    submission to an executor. As `fork` lazily evaluates its argument, `Task.now`
    can be used to lift the trading day''s profit-and-loss into a `Task`. With this
    implementation, the `Task` that represents each trading day is submitted to an
    executor. If we assume 30 trading days are being backtested and the computer used
    has two cores, then this implementation allows each core to simulate 15 trading
    days instead of a single core simulating 30 days.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in earlier benchmarks, submitting a high volume of small computations
    to an executor is expensive. As we have explicit control over concurrency with
    `Task` using `fork`, we can improve our performance by optimizing the frequency
    of executor submissions. In our third attempt, we take advantage of knowing the
    number of trading days to be simulated to control executor submissions. The implementation
    now looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This implementation returns to representing the simulation of each trading day
    without any concurrency using `Task.delay`. In contrast to the previous implementations,
    the list of trading day simulation `Task`s is divided into chunks of 30 using `sliding`.
    Each chunk of 30 `Task`s is wrapped with an invocation of `Task.fork` to execute
    concurrently. This approach allows us to balance the benefits of concurrency with
    the overhead of executor submissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of these three implementations, which is most performant? The answer is not
    straightforward because it depends on the number of simulation trading days and
    the computational cost of simulating a trading day. To better understand the tradeoffs,
    we write a microbenchmark that tests each of the three backtest implementations.
    We show the state required to run the benchmark, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This benchmark allows us to sweep different backtest interval and decision
    delay combinations. Using a `daysWithin` method, which is omitted from the snippet,
    a count representing the number of months is converted into the list of simulation
    trading days. We display the implementation of only one benchmark because the
    other two are identical, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'To accurately time how long it takes to complete the `Task` computation, we
    start the computation with the blocking `unsafePerformSync` method. This is a
    rare example where it is acceptable to make a blocking call without a timeout.
    In this controlled test, we are confident that all invocations will return. For
    this test, we sweep the the month count, leaving the decision delay fixed at 1
    ms. Running this benchmark on a machine with four cores produces the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Months** | **Decision delay milliseconds** | **Throughput
    (ops per second)** | **Error as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `withoutConcurrency` | 1 | 1 | 25.96 | ± 0.46 |'
  prefs: []
  type: TYPE_TB
- en: '| `withAllForked` | 1 | 1 | 104.89 | ± 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| `withBatchedForking` | 1 | 1 | 27.71 | ± 0.70 |'
  prefs: []
  type: TYPE_TB
- en: '| `withoutConcurrency` | 12 | 1 | 1.96 | ± 0.41 |'
  prefs: []
  type: TYPE_TB
- en: '| `withAllForked` | 12 | 1 | 7.25 | ± 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| `withBatchedForking` | 12 | 1 | 8.60 | ± 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| `withoutConcurrency` | 24 | 1 | 0.76 | ± 2.09 |'
  prefs: []
  type: TYPE_TB
- en: '| `withAllForked` | 24 | 1 | 1.98 | ± 1.46 |'
  prefs: []
  type: TYPE_TB
- en: '| `WithBatchedForking` | 24 | 1 | 4.32 | ± 0.88 |'
  prefs: []
  type: TYPE_TB
- en: The results make the tradeoff between the overhead and the benefits of batching
    clearer. Batching is a clear win as the number of months increase with a short
    1 ms computational delay. Consider the scenario of backtesting 24 months with
    a 1 ms decision delay. Assuming 30-day months, there are 720 trading days to simulate.
    Split into batches of 30, there are 24 invocations of `fork` instead of 720\.
    The overhead for splitting the `Task` into batches, and gathering each batch's
    results, is overshadowed by the order of magnitude of fewer executor submissions.
    Our explicit control over forking yielded a doubling of throughput in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of months decreases, the overhead of creating `Task` batches becomes
    a dominating factor. In a 12-month backtest, there are 360 trading days, yielding
    12 batches. Here, batching yields about a 20% throughput improvement over forking
    all `Task`. Cutting the number of trading days in half from the 24-month test
    reduced the performance advantage by more than half. In the worst-case scenario,
    when there is one month to simulate, the batching strategy fails to take to advantage
    of all the cores on the machine. In this scenario, one batch is created, leaving
    CPU resources underutilized.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up the backtester
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen, there are a number of variables at play here. Accounting for
    computational costs, the number of available cores, the expected number of `Task`
    executor submissions, and batching overhead can be challenging. To extend our
    work, we can investigate a more dynamic batching strategy that takes better advantage
    of CPU resources with smaller backtest intervals. Using this benchmark, we got
    a taste for the additional tools that `Task` provides, and how explicit control
    of executor submissions can affect throughput.
  prefs: []
  type: TYPE_NORMAL
- en: The insights that we gleaned by working on the backtester can be applied to
    larger-scale software systems as well. We focused on analyzing results with a
    short 1 ms decision delay. As the cost of executing each `Task` increases (for
    example, 10 ms decision delay), diminishing marginal performance improvements
    are gained from batching. This is because the cost of executor submissions becomes
    overshadowed by the cost of the computation. While 1 ms appears to be a small
    amount of time, there are a potentially surprising number of computations that
    can be completed in this time frame. Consider that a throughput of 1,000 operations
    per second translates to 1 operation per millisecond. Reflecting on benchmarks
    that we have performed in our earlier efforts and through your own work, you can
    find numerous examples where we worked with operations that have a throughput
    higher than 1 operation per millisecond. The takeaway from this thought experiment
    is a large number of use cases fit within the definition of a short computation
    (that is, 1 ms), which means that there are a significant number of opportunities
    to optimize concurrency through the judicious use of `fork`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The backtester is a prime candidate for batching because the amount of work,
    namely the number of days to simulate, is known at the start of processing. In
    a stream processing environment, the amount of work is unknown. For example, consider
    the order book receiving events on-the-fly. How can you implement batching in
    a streaming environment?
  prefs: []
  type: TYPE_NORMAL
- en: We hope that backtester provided an illustrative example to give you a feeling
    for `Task`. There are additional tools that are provided by `Task` that we did
    not explore. We invite you to read the documentation for `Task` in the Scalaz
    library. In the book entitled, *Functional Programming in Scala*, written by two
    Scalaz contributors, Rúnar Bjarnason and Paul Chiusano, there is an excellent
    chapter describing the implementation of a simplified version of Scalaz `Task`.
    This is a great resource to understand the design of the API.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discovered how to harness the power of asynchronous programming
    with the Scala standard library using `Future` and `Promise`. We improved MVT
    backtesting performance by introducing concurrency to improve runtime performance
    and discovered how `Promise` can be used to extend `Future`. Along the way, we
    investigated the shortcomings of `Future` along with the techniques to mitigate
    these shortcomings. We also explored an alternative to `Future` with Scalaz `Task`,
    which provides compelling performance benefits while retaining referential transparency.
    Using what you have learned in this chapter, you can take full advantage of multicore
    hardware using Scala to scale your software systems and deliver higher throughput.
    In our final chapter, [Chapter 7](ch07.html "Chapter 7. Architecting for Performance"), *Architecting
    for Performance*, we explore a set of advanced functional programming techniques
    and concepts to enrich your functional programming toolbox.
  prefs: []
  type: TYPE_NORMAL
