<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Exploring the Collection API"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Exploring the Collection API</h1></div></div></div><p>In this chapter, we return to MVT in order to take on challenges that span multiple MVT teams. The market data team requires improved critical path order book performance to handle increased cancel request volume. The data science team wants better ad hoc data analysis tools to research trading strategies. Everyone has a problem that had to be solved yesterday. That's the start-up lifestyle!</p><p>We use the functional paradigm, our existing knowledge, and the Scala collections API to our advantage to solve these challenges. The power of the Scala language and its collections API allow you to approach problems in ways that you may not have thought possible before. As we work through these challenges and encounter new Scala collection usage, we detail collection implementation and tradeoffs to consider. We will consider the following collections in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">List</li><li class="listitem" style="list-style-type: disc">TreeMap</li><li class="listitem" style="list-style-type: disc">Queue</li><li class="listitem" style="list-style-type: disc">Set</li><li class="listitem" style="list-style-type: disc">Vector</li><li class="listitem" style="list-style-type: disc">Array</li></ul></div><div class="section" title="High-throughput systems – improving the order book"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec26"/>High-throughput systems – improving the order book</h1></div></div></div><p>In <a class="link" href="ch01.html" title="Chapter 1.  The Road to Performance">Chapter 1</a>, <span class="emphasis"><em>The Road to Performance</em></span>, you met MVT's head trader, Dave, under tense circumstances. The financial markets underwent a period of extreme volatility that exposed a weakness in the order book design. After speaking to Dave, you learned that in volatile markets, order volume is dominated by cancels because traders are reacting to quickly changing market conditions. Through order book benchmarking and profiling, you confirmed the suspicion that under high volume, cancel performance causes high order book response latency.</p><p>Although the market volatility that caused trading losses has passed, Dave recognizes the risk that future volatility poses for MVT's returns. Dave wants to invest engineering effort into making the order book more performant when cancelations frequently occur. By working with the data science team, Dave analyzed historical order book activity over a three month period and discovered interesting market characteristics. He shares with you that in the three months analyzed, on a per trading day basis, cancels comprised, on average, 70% of order book commands. The analysis also revealed that on the most volatile market days, cancel activity represents about 85% of order book activity. Known for his puns, Dave concludes with, "Now, you know everything I know. Like the order book, we are counting on you to execute!"</p><div class="section" title="Understanding historical trade-offs – list implementation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec33"/>Understanding historical trade-offs – list implementation</h2></div></div></div><p>Excited to improve order book performance, your first step is to familiarize yourself with the order book implementation. As you open up the order book repository, you ping Gary, a fellow engineer who has prior order book development experience. As Gary knows the history of order book development, he tells you to check out <code class="literal">ListOrderBook</code>. "This was our first attempt at modeling the order book. I think you can learn from our design by seeing its first incarnation," he adds, "Once you understand the implementation, check out <code class="literal">QueueOrderBook</code>. That's the next version of the order book. You profiled an older iteration of this implementation when we had the volatility wave. Let me know if you have any questions!" After thanking him, you dig into the repository to find <code class="literal">ListOrderBook</code>.</p><p>The <code class="literal">ListOrderBook</code> class defines the following state to manage buys (bids) and sells (offers):</p><pre class="programlisting">case class ListOrderBook( &#13;
  bids: TreeMap[Price, List[BuyLimitOrder]], &#13;
  offers: TreeMap[Price, List[SellLimitOrder]]) { &#13;
  def bestBid: Option[BuyLimitOrder] =  &#13;
    ??? // hidden for brevity &#13;
  def bestOffer: Option[SellLimitOrder] =  &#13;
    ??? // hidden for brevity &#13;
} &#13;
</pre><p>To refresh our memory, here are definitions of <code class="literal">Price</code>, <code class="literal">BuyLimitOrder</code>, and <code class="literal">SellLimitOrder</code>:</p><pre class="programlisting">sealed trait LimitOrder { &#13;
  def id: OrderId &#13;
  def price: Price &#13;
} &#13;
case class BuyLimitOrder(id: OrderId, price: Price) extends LimitOrder &#13;
case class SellLimitOrder(id: OrderId, price: Price) extends LimitOrder &#13;
case class Price(value: BigDecimal) &#13;
</pre><p>The <code class="literal">LimitOrder</code> is an <span class="strong"><strong>algebraic data type</strong></span> (<span class="strong"><strong>ADT</strong></span>) that represents the two possible order sides. The <code class="literal">Price</code> class is a strongly-typed wrapper for <code class="literal">BigDecimal</code>. Recalling the performance boost that value classes provide, you modify the definition of <code class="literal">Price</code>, as follows:</p><pre class="programlisting">case class Price(value: BigDecimal) extends AnyVal &#13;
</pre><p>The <code class="literal">ListOrderBook</code> class uses two Scala collection types to maintain its state: <code class="literal">List</code> and <code class="literal">TreeMap</code>. Let's have a deeper look at these data structures to understand the tradeoffs that they present.</p><div class="section" title="List"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec13"/>List</h3></div></div></div><p>Scala implements <code class="literal">List</code> as an immutable singly-linked list. A <code class="literal">List</code> is an ordered collection of elements of the same type. A <code class="literal">List</code> is a sealed abstract class with two implementations: <code class="literal">Nil</code>, which represents the empty list, and <code class="literal">::</code> (often called cons), which is used to represent an element and a tail. To make things more concrete, let's look at some pseudocode, which is close to the actual implementation:</p><pre class="programlisting">sealed trait List[+A] &#13;
case object Nil extends List[Nothing] &#13;
case class ::[A](head: A, tail: List[A]) extends List[A] &#13;
</pre><p>A <code class="literal">List</code> of three integers can be constructed using the following notation:</p><pre class="programlisting">val list = ::(1, ::(2, ::(3, Nil)))&#13;
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>Note the plus sign in the definition of the <code class="literal">List</code> trait. The plus (<code class="literal">+</code>) sign indicates that <code class="literal">List</code> is covariant on its type parameter, <code class="literal">A</code>. Covariance allows you to express polymorphic constraints with generic types. To make this more concrete, consider the following definitions:</p><p><code class="literal">sealed trait Base</code></p><p><code class="literal">case class Impl(value: Int) extends Base</code></p><p>Here, a relationship is expressed between <code class="literal">Base</code> and <code class="literal">Impl</code>. The <code class="literal">Impl</code> class is a subtype of <code class="literal">Base</code>. When used with <code class="literal">List</code>, covariance allows us to express that <code class="literal">List[Impl]</code> is a subtype of <code class="literal">List[Base]</code>. Expressed with an example, covariance is what allows the following snippet to compile:</p><p><code class="literal">val bases: List[Base] = List[Impl](Impl(1))</code></p><p>Covariance belongs to the broader topic of variances. If you wish to learn more about variances in Scala, refer to this excellent blog post by Andreas Schroeder at <a class="ulink" href="https://blog.codecentric.de/en/2015/03/scala-type-system-parameterized-types-variances-part-1/">https://blog.codecentric.de/en/2015/03/scala-type-system-parameterized-types-variances-part-1/</a>.</p></div></div><p>Unlike most other Scala collections, <code class="literal">List</code> supports pattern matching on its content. This is a powerful way to write expressive code that handles multiple scenarios while retaining compile-time safety that all possible cases are handled. Consider the following snippet:</p><pre class="programlisting">List(1,2,3,4) match { &#13;
  case 1 :: x :: rest =&gt; println(s"second element: $x, rest: $rest") &#13;
} &#13;
</pre><p>In this simple pattern match, we are able to express several concerns. Here, <code class="literal">1</code> is <code class="literal">1</code>, <code class="literal">x</code> is <code class="literal">2</code>, and <code class="literal">rest</code> is <code class="literal">List(3,4)</code>. When compiled, this snippet elicits a compiler warning because the Scala compiler infers that there are possible <code class="literal">List</code> patterns that were unmatched (for example, empty <code class="literal">List</code>). Compiler-provided warnings minimize the chance of your forgetting to handle a valid input.</p><p>A <code class="literal">List</code> is optimized for prepend operations. Adding 0 to the previous list is as easy as doing this:</p><pre class="programlisting">val list = ::(1, ::(2, ::(3, Nil))) &#13;
val listWithZero = ::(0, list) &#13;
</pre><p>This is a constant-time operation, and it has almost no memory cost, as <code class="literal">List</code> implements data sharing. In other words, the new list, <code class="literal">listWithZero</code>, is not a deep copy of <code class="literal">list</code>. Instead, it re-uses all its allocated elements and allocates only one new element, the cell containing <code class="literal">0</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" alt="List"/></div><p>
</p><p>In contrast to prepend operations, append operations (that is, adding an element to the end of the list) are computationally expensive because the entire <code class="literal">List</code> must be copied:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_002.jpg" alt="List"/></div><p>
</p><p>Given the poor append performance of List, you may wonder whether it is safe to use a <code class="literal">map</code> transform. A <code class="literal">map</code> transform occurs by applying a function to successive elements in the <code class="literal">List</code>, which can be logically represented by appending transformed values to a new <code class="literal">List</code>. To avoid this performance pitfall, <code class="literal">List.map</code> overrides the default implementation provided by the trait <code class="literal">TraversableOnce</code> to apply the transform using prepend operations. This provides improved <code class="literal">List.map</code> performance while retaining the same API. Overriding default behavior to provide a specialized implementation is a common Scala collections pattern. Constant time head operations make <code class="literal">List</code> ideal for algorithms involving last-in, first-out (LIFO) operations. For random access and first-in, first-out (FIFO) behaviors, you should employ <code class="literal">List </code>selectively.</p><p>In the next section, we investigate <code class="literal">TreeMap</code>. The <code class="literal">TreeMap</code> class is the implementation of the <code class="literal">SortedMap</code> trait that is used to maintain bids and offers.</p></div><div class="section" title="TreeMap"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec14"/>TreeMap</h3></div></div></div><p>The <code class="literal">TreeMap</code> class is a map that orders keys according to a provided ordering strategy. The following snippet of its class definition makes the ordering requirement clear:</p><pre class="programlisting">class TreeMap[A, +B] private (tree: RB.Tree[A, B])(implicit val ordering: Ordering[A]) &#13;
</pre><p>The <code class="literal">Ordering</code> class is a type class that defines a contract for the natural ordering of elements of the <code class="literal">A</code> type.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>If type classes are a concept that is new to you, we encourage you to read Daniel Westheide's well-written blog post on the topic at <a class="ulink" href="http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html">http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html</a>.</p></div></div><p>In <code class="literal">ListOrderBook</code>, we see that <code class="literal">Price</code> is the key. Looking at the companion object of <code class="literal">Price</code>, we see that the ordering is defined by delegating to the underlying<code class="literal"> BigDecimal</code> type's ordering definition:</p><pre class="programlisting">object Price { &#13;
  implicit val ordering: Ordering[Price] = new Ordering[Price] { &#13;
    def compare(x: Price, y: Price): Int = &#13;
      Ordering.BigDecimal.compare(x.value, y.value) &#13;
  } &#13;
} &#13;
</pre><p>The <code class="literal">TreeMap </code>class referenced by <code class="literal">ListOrderBook</code>, like <code class="literal">List</code>, is immutable. Immutability provides strong reasoning guarantees. We can be certain that there are no side effects because the effect of adding or removing a value from the map is always reflected as a new map.</p><p>The <code class="literal">TreeMap</code> class implementation is a special type of binary search tree, the red-black tree. This tree implementation provides logarithmic operation time for lookups, additions, and removals. You might be surprised to see <code class="literal">TreeMap</code> in place of <code class="literal">HashMap</code>. As documented in the Scala collections performance overview (<a class="ulink" href="http://docs.scala-lang.org/overviews/collections/performance-characteristics.html">http://docs.scala-lang.org/overviews/collections/performance-characteristics.html</a>), <code class="literal">HashMap</code> provides constant time lookups, additions, and removals, which is faster than <code class="literal">TreeMap</code>. However, <code class="literal">TreeMap</code> offers superior performance when performing ordered traversals. For example, finding the largest key in the map can be done in logarithmic time with <code class="literal">TreeMap</code>, while this is done in linear time for <code class="literal">HashMap</code>. This difference is an indicator that the order book implementation requires efficient ordered <code class="literal">Price</code> traversals.</p></div><div class="section" title="Adding limit orders"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec15"/>Adding limit orders</h3></div></div></div><p>Coming back to the <code class="literal">ListOrderBook</code> implementation, we see the following partial method definition reflects the heart of the order book:</p><pre class="programlisting">  def handle( &#13;
    currentTime: () =&gt; EventInstant, &#13;
    ob: ListOrderBook, &#13;
    c: Command): (ListOrderBook, Event) = c match { &#13;
    case AddLimitOrder(_, o) =&gt; ??? // hidden for brevity &#13;
    case CancelOrder(_, id) =&gt; ??? // hidden for brevity &#13;
  } &#13;
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>It might seem curious that a function is supplied as an argument to retrieve the current time. A potentially simpler way to achieve the same effect is to invoke <code class="literal">System.currentTimeMillis()</code>. The shortcoming of this approach is that accessing the system clock is a side-effect, which means that the function is no longer referentially transparent. By providing a function to retrieve the current time, we are able to control how this side-effect happens and produce repeatable test cases.</p></div></div><p>Given a <code class="literal">Command</code>, an order book instance, and a way to obtain the current time for event timestamps, an <code class="literal">Event</code> and a new state are produced. To refresh our memory, here are the commands the order book can process:</p><pre class="programlisting">  sealed trait Command &#13;
  case class AddLimitOrder(i: CommandInstant, o: LimitOrder) extends Command &#13;
  case class CancelOrder(i: CommandInstant, id: OrderId) extends Command &#13;
</pre><p>The following are the possible events created by processing commands:</p><pre class="programlisting">  sealed trait Event &#13;
  case class OrderExecuted(i: EventInstant, buy: Execution,  &#13;
    sell: Execution) extends Event &#13;
  case class LimitOrderAdded(i: EventInstant) extends Event &#13;
  case class OrderCancelRejected(i: EventInstant,  &#13;
    id: OrderId) extends Event &#13;
  case class OrderCanceled(i: EventInstant,  &#13;
    id: OrderId) extends Event &#13;
</pre><p>Let's focus on supporting the <code class="literal">AddLimitOrder</code> command to better understand the algorithmic properties of historical design choices. When adding a limit order, one of two outcomes is possible:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The incoming order price crosses the book resulting in <code class="literal">OrderExecuted</code></li><li class="listitem" style="list-style-type: disc">The oncoming order rests on the book resulting in <code class="literal">LimitOrderAdded</code></li></ul></div><p>Deducing whether or not the order crosses the book requires looking at the best price on the opposing side. Returning to the definition of <code class="literal">LimitOrderBook</code> with complete implementation of <code class="literal">bestBid</code> and <code class="literal">bestOffer</code>, we see the following:</p><pre class="programlisting">case class ListOrderBook( &#13;
  bids: TreeMap[Price, List[BuyLimitOrder]], &#13;
  offers: TreeMap[Price, List[SellLimitOrder]]) { &#13;
  def bestBid: Option[BuyLimitOrder] = &#13;
    bids.lastOption.flatMap(_._2.headOption) &#13;
   &#13;
  def bestOffer: Option[SellLimitOrder] = &#13;
    offers.headOption.flatMap(_._2.headOption) &#13;
} &#13;
</pre><p>The implementation shows that we are taking advantage of the logarithmic ordered search property of <code class="literal">TreeMap</code>. The best bid is the key with the highest price, which is the last value in the tree because the ordering is ascending. The best offer is the key with the lowest price, which is the first value in the tree.</p><p>Focusing specifically on the addition of a buy limit order and given the best offer, the following comparison occurs to determine whether the incoming buy order crosses the book or rests on the book:</p><pre class="programlisting">orderBook.bestOffer.exists(buyOrder.price.value &gt;= _.price.value)  &#13;
  match { &#13;
          case true =&gt; ??? // cross the book &#13;
          case false =&gt; ??? // rest on the book &#13;
  } &#13;
</pre><p>Let's first assume that the incoming buy order's price is lower than the best offer, which means the order is added to the book (that is, rests on the book). The question we are trying to answer is, "where in the book should the order be added?" The order book performs a logarithmic search to find the price level associated with the order price. From the definition of <code class="literal">ListOrderBook</code>, you know that each value in the map (the price level) is represented as a <code class="literal">List</code> of orders. Recalling a discussion with the head trader, Dave, you remember that orders within a price level are executed based on time priority. The first order added to a price level is the first order to be executed. Conceptually, a price level is a first-in, first-out (FIFO) queue. The implication is that adding an order to a price level is a linear time operation because the order is appended to the end. The following snippet confirms your hypothesis:</p><pre class="programlisting">val orders = orderBook.bids.getOrElse(buyOrder.price, Nil) &#13;
          orderBook.copy(bids = orderBook.bids + (buyOrder.price -&gt; orders.:+(buyOrder))) -&gt; &#13;
            LimitOrderAdded(currentTime()) &#13;
</pre><p>The snippet shows that adding a resting limit order to the book involves a linear time append operation to <code class="literal">List</code> of <code class="literal">BuyLimitOrder</code>. In your mind, you are beginning to wonder how MVT was able to trade profitably at all with this order book. Before leaping to this harsh judgment, you consider how crossing the book is handled.</p><p>Assuming that the incoming buy order's price is greater than or equal to the best offer price, then the buy order crosses the book, causing an execution. Time priority dictates that the first sell order received is executed against the incoming buy order, which translates to taking the first sell order in the price level. When generating an execution, you realize that modeling a price level with a <code class="literal">List</code> provides constant time performance. The following snippet shows how a price level is modified on a buy execution:</p><pre class="programlisting">      case (priceLevel, (sell :: Nil)) =&gt; (orderBook.copy(offers = orderBook.offers - sell.price), &#13;
        OrderExecuted(currentTime(), Execution(buy.id, sell.price), &#13;
          Execution(sell.id, sell.price))) &#13;
      case (_, (sell :: remainingSells)) =&gt; (orderBook.copy(offers = orderBook.offers + (sell.price -&gt; remainingSells)), &#13;
        OrderExecuted(currentTime(), &#13;
          Execution(buy.id, sell.price), Execution(sell.id, sell.price))) &#13;
</pre><p>The <code class="literal">ListOrderBook</code> takes advantage of the <code class="literal">List</code> pattern matching to handle the two possible cross scenarios:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The executed sell order is the only order available in the price level</li><li class="listitem" style="list-style-type: disc">Additional sell orders remain at the price level</li></ul></div><p>In the former scenario, the price level is removed from the book by removing the key from the offers <code class="literal">TreeMap</code>. In the latter scenario, the remaining orders form the new price level. Clearly, the order book is optimized for executions over adding resting orders. You wonder why this bias exists in the order book implementation. You wonder to yourself, "perhaps, executions are more much more prevalent than resting orders?" You are unsure and make a mental note to chat with Dave.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>Pause for a moment to consider biases in systems that you have designed. Did you optimize operations proportional to usage or latency constraints? Looking back, did your design choices lead you towards the best possible performance for the most important operations? Of course, hindsight makes it easy to call out suboptimal design choices. By reflecting on how you made these choices, you might be better able to avoid similar deficiencies in future systems.</p></div></div></div><div class="section" title="Canceling orders"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec16"/>Canceling orders</h3></div></div></div><p>The <code class="literal">ListOrderBook</code> also supports the <code class="literal">CancelOrder</code> command to remove an existing order by ID. Cancel requests pose an algorithmic challenge to <code class="literal">ListOrderBook</code>. As only the order ID is provided, <code class="literal">ListOrderBook</code> cannot efficiently determine which side the order rests on (that is, buy or sell). To determine the side, the buy and sell price levels are swept to find the order ID. This is an operation that is proportional to the number of price levels per side and the length of each price level. The worst case scenario is submitting an order ID that does not exist in the order book. The entire book must be swept to identify the absence of the provided order ID. A malicious trader could slow down MVT order book operations by submitting a constant stream of nonexistent order IDs. You make a note to talk with Dave about malicious trading activities and what MVT can do to defend against them.</p><p>Assuming that the order referenced by the cancel request exists in the book and its price level is discovered, the act of removing the cancelled order from the book is also expensive. Canceling is a linear time operation that requires traversing the linked list of orders and removing the node with the matching order ID. The following snippet implements canceling a sell order in <code class="literal">ListOrderBook</code>:</p><pre class="programlisting">orderBook.offers.find { case (price, priceLevel) =&gt; priceLevel.exists(_.id == idToCancel) } &#13;
        .fold[(ListOrderBook, Event)](orderBook -&gt; &#13;
        OrderCancelRejected(currentTime(), idToCancel)) { &#13;
        case (price, priceLevel) =&gt; &#13;
          val updatedPriceLevel = priceLevel.filter(_.id != idToCancel) &#13;
          orderBook.copy(offers = updatedPriceLevel.nonEmpty match { &#13;
            case true =&gt; orderBook.offers + (price -&gt; updatedPriceLevel) &#13;
            case false =&gt; orderBook.offers - price &#13;
          }) -&gt; OrderCanceled(currentTime(), idToCancel) &#13;
</pre><p>Studying this snippet, it is unsurprising to you that cancelation performance is the least performant order book operation. There are two linear time passes performed per price level to cancel the order. First, <code class="literal">exists</code> traverses the list of price level orders to determine whether the ID to be canceled exists in the price level. Once the price level containing the ID is found, there is a second traversal via <code class="literal">filter</code> to update the state of the order book.</p><p>The cancelation implementation in <code class="literal">ListOrderBook</code> is an illustration of the double-edged sword of Scala's expressive collection API. By virtue of being expressive, the cancelation logic is simple to understand and to maintain. However, its expressiveness also makes it easy to hide that the runtime performance of removing an order from a price level is <span class="emphasis"><em>2 * N</em></span>, where <span class="emphasis"><em>N</em></span> is the number of orders in a price level. This simple example makes it clear that in a performance-sensitive environment, it is important to take a step back from the code to consider the runtime overhead of the data structure that is being used.</p></div></div><div class="section" title="The current order book – queue implementation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec34"/>The current order book – queue implementation</h2></div></div></div><p>You refrain from judging <code class="literal">ListOrderBook</code> too harshly because you know from your prior software development experiences that there were likely extenuating circumstances that led to this implementation. You turn your attention to the current order book implementation, which is in <code class="literal">QueueOrderBook</code>. Looking over the source code, you are surprised to discover the implementation appears to match <code class="literal">ListOrderBook</code> except for the price level data structure:</p><pre class="programlisting">case class QueueOrderBook( &#13;
  bids: TreeMap[Price, Queue[BuyLimitOrder]], &#13;
  offers: TreeMap[Price, Queue[SellLimitOrder]]) &#13;
</pre><p>The only difference between the two implementations is the use of <code class="literal">scala.collection.immutable.Queue</code> in place of <code class="literal">List</code> to represent a price level. From a modeling perspective, using a FIFO queue makes sense. As time priority dictates execution order, a FIFO queue is a natural fit to store resting orders. You begin wondering whether switching out <code class="literal">List</code> for <code class="literal">Queue </code>was done purely for modeling purposes. The question on your mind is, "how does replacing <code class="literal">List</code> with <code class="literal">Queue</code> improve order book performance?" Understanding this change requires digging deeper into Scala's <code class="literal">Queue</code> implementation.</p><div class="section" title="Queue"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec17"/>Queue</h3></div></div></div><p>This snippet of a <code class="literal">Queue</code> definition reveals an interesting insight:</p><pre class="programlisting">class Queue[+A] protected(protected val in: List[A], protected val out: List[A]) &#13;
</pre><p>Without reading deeply into the <code class="literal">Queue</code> implementation, we see that it uses two <code class="literal">Lists</code> to manage state. Given the usage of <code class="literal">List</code> to model a FIFO queue in <code class="literal">ListOrderBook</code>, it should not be surprising to see the usage of <code class="literal">List</code> to build an immutable FIFO queue data structure. Let's look at the enqueue and dequeue operations to understand how in and out impact <code class="literal">Queue</code> performance. The following snippet shows the implementation of enqueue:</p><pre class="programlisting">def enqueue[B &gt;: A](elem: B) = new Queue(elem :: in, out) &#13;
</pre><p>As the element is prepended to <code class="literal">in</code>, enqueueing is a constant time operation. Recall that the analogous <code class="literal">ListOrderBook</code> operation is adding a resting order, which has linear runtime performance. This is a clear performance win for <code class="literal">QueueOrderBook</code>. Next, we consider dequeue implementation:</p><pre class="programlisting">def dequeue: (A, Queue[A]) = out match { &#13;
    case Nil if !in.isEmpty =&gt; val rev = in.reverse ; (rev.head, new Queue(Nil, rev.tail)) &#13;
    case x :: xs            =&gt; (x, new Queue(in, xs)) &#13;
    case _                  =&gt; throw new NoSuchElementException("dequeue on empty queue") &#13;
  } &#13;
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note35"/>Note</h3><p>As the implementation shows, dequeue throws an exception when invoked with an empty <code class="literal">Queue</code>. The exception is an unexpected outcome to invoking <code class="literal">dequeue</code> and feels out of place in the functional programming paradigm. For this reason, <code class="literal">Queue</code> also provides <code class="literal">dequeueOption</code> that returns an <code class="literal">Option</code>. This makes the handling of an empty <code class="literal">Queue</code> explicit and easier to reason about. We recommend using <code class="literal">dequeueOption</code> in any situation where you cannot guarantee that <code class="literal">dequeue</code> will always be called on a nonempty <code class="literal">Queue</code>.</p></div></div><p>The <code class="literal">dequeue</code> operation is more involved than <code class="literal">enqueue</code> due to the interaction between <code class="literal">in</code> and <code class="literal">out</code>. To understand how the <code class="literal">Queue</code> state is managed with the <code class="literal">dequeue</code> operations, review the following table. This table walks through a series of the <code class="literal">enqueue</code> and <code class="literal">dequeue</code> operations, listing the state of <code class="literal">in</code> and <code class="literal">out</code> at each step. As you review the table, consider which  <code class="literal">dequeue</code> patterns match statements that are invoked:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Operation</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>In</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Out</strong></span>
</p>
</td></tr><tr><td>
<p>enqueue(1)</p>
</td><td>
<p>List(1)</p>
</td><td>
<p>Nil</p>
</td></tr><tr><td>
<p>enqueue(2)</p>
</td><td>
<p>List(1, 2)</p>
</td><td>
<p>Nil</p>
</td></tr><tr><td>
<p>enqueue(3)</p>
</td><td>
<p>List(1, 2, 3)</p>
</td><td>
<p>Nil</p>
</td></tr><tr><td>
<p>dequeue</p>
</td><td>
<p>Nil</p>
</td><td>
<p>List(2, 3)</p>
</td></tr><tr><td>
<p>dequeue</p>
</td><td>
<p>Nil</p>
</td><td>
<p>List(3)</p>
</td></tr><tr><td>
<p>enqueue(4)</p>
</td><td>
<p>List(4)</p>
</td><td>
<p>List(3)</p>
</td></tr><tr><td>
<p>dequeue</p>
</td><td>
<p>List(4)</p>
</td><td>
<p>Nil</p>
</td></tr><tr><td>
<p>dequeue</p>
</td><td>
<p>Nil</p>
</td><td>
<p>Nil</p>
</td></tr></tbody></table></div><p>As the <code class="literal">enqueue</code> and <code class="literal">dequeue</code> invocations are intermingled, both <code class="literal">in</code> and <code class="literal">out</code> retain state. In the final sequence displayed, the queue returns to its initial state (that is, both <code class="literal">in</code> and <code class="literal">out</code> empty). The key insight from this implementation is that <code class="literal">Queue</code> amortizes the cost of <code class="literal">dequeue</code> to be constant time by deferring transfers from <code class="literal">in</code> and <code class="literal">out</code>. Each element transfer from <code class="literal">in</code> and <code class="literal">out</code> is a linear time <code class="literal">reverse</code> operation to maintain first-in, first-out ordering. Deferring the cost of this expensive operation until <code class="literal">out</code> is empty is a form of lazy evaluation. This is an illustrative example of how lazy evaluation can be used to improve runtime performance.</p><p>Now that you have an understanding of how <code class="literal">Queue</code> is implemented, you can reason about the performance improvements delivered by <code class="literal">QueueOrderBook</code>. The following table itemizes the runtime performance of each scenario to modify a price level:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Scenario</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>ListOrderBook</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>QueueOrderBook</strong></span>
</p>
</td></tr><tr><td>
<p>Add resting limit order</p>
</td><td>
<p>Linear</p>
</td><td>
<p>Constant</p>
</td></tr><tr><td>
<p>Generate execution</p>
</td><td>
<p>Constant</p>
</td><td>
<p>Amortized constant</p>
</td></tr><tr><td>
<p>Cancel order</p>
</td><td>
<p>Linear</p>
</td><td>
<p>Linear</p>
</td></tr></tbody></table></div><p>This table illustrates how understanding the runtime characteristics of the Scala collection API can result in tangible performance wins with small changes to your implementation. Recall that when <code class="literal">QueueOrderBook</code> was introduced, it was noted that its implementation is identical to <code class="literal">ListOrderBook</code>, the module changes to replace <code class="literal">List</code> operations with analogous <code class="literal">Queue</code> operations. This is a comparatively simple change for the performance boost shown previously.</p><p>You are excited to see the performance win to handle limit orders with <code class="literal">QueueOrderBook</code>, but you are left wondering about what can be done about cancelation performance. It remains unsettling to you that <code class="literal">QueueOrderBook</code> retains the same cancelation performance. In particular, because of the recent market volatility that exposed order book cancelation performance's weakness that caused MVT to trade unprofitably. Lazy evaluation was a big performance win to handle limit orders. Can this principle also be applied to cancel requests?</p></div></div><div class="section" title="Improved cancellation performance through lazy evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec35"/>Improved cancellation performance through lazy evaluation</h2></div></div></div><p>Queue provides high-performance <code class="literal">enqueue</code> and <code class="literal">dequeue</code> operations using the additional state, the second <code class="literal">List</code>, to defer and to batch expensive operations. This principle can be applied to the order book. When canceling an order, there are two expensive operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Identifying the price level containing the order-to-be-canceled</li><li class="listitem" style="list-style-type: disc">Traversing a <code class="literal">Queue</code> or <code class="literal">List</code> to remove the canceled order</li></ul></div><p>Focusing on the second operation, the motivating question is, "how can the order book defer the cost of linear traversal to modify internal state?" To answer this question, it is often helpful to consider the strengths of your implementation. With either order book implementation, we know there is excellent execution performance. One strategy that takes advantage of this insight is to defer cancellation until order execution occurs. The approach is to use additional state to maintain the intent to cancel without removing the order from order book state until it is performant to do so. This approach could look like the following:</p><pre class="programlisting">case class LazyCancelOrderBook( &#13;
  pendingCancelIds: Set[OrderId], &#13;
  bids: TreeMap[Price, Queue[BuyLimitOrder]], &#13;
  offers: TreeMap[Price, Queue[SellLimitOrder]]) &#13;
</pre><p>The <code class="literal">LazyCancelOrderBook</code> class adds additional state in the form of a <code class="literal">scala.collection.immutable.Set</code> to manage the IDs of canceled requests that have not been reflected into the the state of <code class="literal">bids</code> and <code class="literal">offers</code>. Before diving into how <code class="literal">pendingCancelIds</code> is used, let's investigate the Scala implementation of <code class="literal">Set</code>.</p><div class="section" title="Set"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec18"/>Set</h3></div></div></div><p>Scala's implementation of <code class="literal">Set</code> is neither an ADT, such as <code class="literal">List</code>, nor a concrete implementation, such as <code class="literal">TreeMap</code>. Instead, it is a trait, as shown in this snippet of its definition:</p><pre class="programlisting">trait Set[A] &#13;
</pre><p>The reason the standard library defines it is as a trait is to support specific implementations depending upon the element count. The <code class="literal">Set</code> companion object defines five implementations for sizes zero to four. Each implementation contains a fixed number of elements, as shown in <code class="literal">Set3</code>, as follows:</p><pre class="programlisting">class Set3[A] private[collection] (elem1: A, elem2: A, elem3: A) &#13;
</pre><p>When the number of elements is small, the runtime performance is faster with hand-rolled <code class="literal">Set</code> implementations. With this technique, additions and removals point to the next or previous hand-rolled implementation. For example, consider <code class="literal">+</code> and <code class="literal">-</code> from <code class="literal">Set3</code>:</p><pre class="programlisting">    def + (elem: A): Set[A] = &#13;
      if (contains(elem)) this &#13;
      else new Set4(elem1, elem2, elem3, elem) &#13;
 &#13;
    def - (elem: A): Set[A] = &#13;
      if (elem == elem1) new Set2(elem2, elem3) &#13;
      else if (elem == elem2) new Set2(elem1, elem3) &#13;
      else if (elem == elem3) new Set2(elem1, elem2) &#13;
      else this &#13;
</pre><p>After <code class="literal">Set4</code>, the standard library uses an implementation named <code class="literal">HashSet</code>. This is visible when adding an element to <code class="literal">Set4</code>:</p><pre class="programlisting"> def + (elem: A): Set[A] = &#13;
      if (contains(elem)) this &#13;
      else new HashSet[A] + (elem1, elem2, elem3, elem4, elem) &#13;
</pre><p>The <code class="literal">HashSet</code> is analogous to <code class="literal">TreeMap</code> because it is backed by an efficient data structure to manage internal state. For <code class="literal">HashSet</code>, the backing data structure is a hash trie. The hash trie provides amortized constant time performance for additions, removals, and contains operations as per the Scala collections performance overview (<a class="ulink" href="http://docs.scala-lang.org/overviews/collections/performance-characteristics.html">http://docs.scala-lang.org/overviews/collections/performance-characteristics.html</a>). If you want to dig deeper into how a hash trie works, the Scala hash trie overview (<a class="ulink" href="http://docs.scala-lang.org/overviews/collections/concrete-immutable-collection-classes.html#hash-tries">http://docs.scala-lang.org/overviews/collections/concrete-immutable-collection-classes.html#hash-tries</a>) is a good starting point.</p><p>Returning to the <code class="literal">LazyCancelOrderBook</code>, we now know that common set operations with <code class="literal">pendingCancelIds</code> are completed in amortized constant time. Provided that we focus on additions and removals, and contains operations, this suggests there will be minimal overhead as the size of the set increases. We can use <code class="literal">pendingCancelIds</code> to represent the intent to remove an order from the order book without paying the cost of performing the removal. This simplifies the handling of a cancel order to be a constant time addition to <code class="literal">pendingCancelIds</code>:</p><pre class="programlisting">def handleCancelOrder( &#13;
    currentTime: () =&gt; EventInstant, &#13;
    ob: LazyCancelOrderBook, &#13;
    id: OrderId): (LazyCancelOrderBook, Event) = &#13;
    ob.copy(pendingCancelIds = ob.pendingCancelIds + id) -&gt; &#13;
      OrderCanceled(currentTime(), id) &#13;
</pre><p>The implementation of <code class="literal">handleCancelOrder</code> becomes trivial because the work to remove the order from the book is deferred. While this is a performance win, this implementation suffers from a serious deficiency. This implementation is no longer able to identify order IDs that are absent from the order book, which result in <code class="literal">OrderCancelRejected</code>. One way to account for this requirement is to maintain an additional <code class="literal">Set</code> containing order IDs actively resting on the book. Now, the <code class="literal">LazyCancelOrderBook</code> state looks like the following:</p><pre class="programlisting">case class LazyCancelOrderBook( &#13;
  activeIds: Set[OrderId], &#13;
  pendingCancelIds: Set[OrderId], &#13;
  bids: TreeMap[Price, Queue[BuyLimitOrder]], &#13;
  offers: TreeMap[Price, Queue[SellLimitOrder]]) &#13;
</pre><p>With this definition, we can rewrite <code class="literal">handleCancelOrder</code> to account for nonexistent order IDs:</p><pre class="programlisting">def handleCancelOrder( &#13;
    currentTime: () =&gt; EventInstant, &#13;
    ob: LazyCancelOrderBook, &#13;
    id: OrderId): (LazyCancelOrderBook, Event) = &#13;
    ob.activeIds.contains(id) match { &#13;
      case true =&gt; ob.copy(activeIds = ob.activeIds - id, &#13;
        pendingCancelIds = ob.pendingCancelIds + id) -&gt; &#13;
        OrderCanceled(currentTime(), id) &#13;
      case false =&gt; ob -&gt; OrderCancelRejected(currentTime(), id) &#13;
    } &#13;
</pre><p>This implementation involves three amortized, constant time operations when the order ID exists in the book. First, there is an operation to identify whether or not the order ID exists in the order book. Then, the provided order ID is removed from the active ID set and added to the pending cancel set. Previously, this scenario required two linear runtime operations. The degenerate scenario of handling a nonexistent order ID now shrinks to a single amortized constant time operation.</p><p>Before celebrating performance wins, bear in mind that we still need to remove canceled orders from the book. To reduce the cost of cancelations, two potentially large sets were added to the order book, which increases the size of the memory footprint and garbage collection pressure. Additionally, benchmarking is needed to prove that theoretical performance improvements translate to real-world performance.</p><p>To complete <code class="literal">LazyCancelOrderBook</code> implementation, we need to account for <code class="literal">activeIds</code> when handling a limit order and <code class="literal">pendingCancelIds</code> when generating an execution. As you may recall, handling a limit order involved two scenarios:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Adding a resting limit order</li><li class="listitem" style="list-style-type: disc">Crossing the book to generate an execution</li></ul></div><p>Here is a partially implemented snippet that prepares us to handle these two scenarios for a <code class="literal">BuyLimitOrder</code>:</p><pre class="programlisting">orderBook.bestOffer.exists(_.price.value &lt;= buy.price.value) match { &#13;
        case true =&gt; ??? // crossing order &#13;
        case false =&gt; ???  // resting order &#13;
</pre><p>To support resting buy orders, the provided buy order must be enqueued and additionally, the buy order ID must be added to the <code class="literal">activeOrderIds</code> set:</p><pre class="programlisting">      def restLimitOrder: (LazyCancelOrderBook, Event) = { &#13;
        val orders = orderBook.bids.getOrElse(buy.price, Queue.empty) &#13;
        orderBook.copy(bids = orderBook.bids + (buy.price -&gt; orders.enqueue(buy)), &#13;
          activeIds = orderBook.activeIds + buy.id) -&gt; LimitOrderAdded(currentTime()) &#13;
      } &#13;
 &#13;
orderBook.bestOffer.exists(_.price.value &lt;= buy.price.value) match { &#13;
        case true =&gt; ??? // crossing order &#13;
        case false =&gt; restLimitOrder &#13;
</pre><p>The logic to add a resting limit order is shown in the preceding code and extracted into a method named <code class="literal">restLimitOrder</code>. This logic resembles the analogous scenario for <code class="literal">ListOrderBook</code> with the added amortized constant time active order ID addition operation. This change is straightforward and adds little processing time overhead. Finally, we consider the more complicated order crossing scenario. This scenario is analogous to <code class="literal">Queue.dequeue</code> in that this implementation pays the cost of the deferred action. The first dilemma to solve is identifying which order can be executed and which orders must be removed because they are canceled. <code class="literal">findActiveOrder</code> supplies this functionality and is shown with the assumption that <code class="literal">orderBook</code> is lexically in scope, as follows:</p><pre class="programlisting">      @tailrec &#13;
      def findActiveOrder( &#13;
        q: Queue[SellLimitOrder], &#13;
        idsToRemove: Set[OrderId]): (Option[SellLimitOrder], Option[Queue[SellLimitOrder]], Set[OrderId]) = &#13;
        q.dequeueOption match { &#13;
          case Some((o, qq)) =&gt; orderBook.pendingCancelIds.contains(o.id) match { &#13;
            case true =&gt; &#13;
              findActiveOrder(qq, idsToRemove + o.id) &#13;
            case false =&gt; &#13;
              (Some(o), if (qq.nonEmpty) Some(qq) else None, idsToRemove + o.id) &#13;
          } &#13;
          case None =&gt; (None, None, idsToRemove) &#13;
        } &#13;
</pre><p>
<code class="literal">findActiveOrder</code> recursively inspects a sell price level until an executable order is found or the price level is empty. In addition to optionally resolving a sell order that can be executed, the method returns the remaining price level. These order IDs have been canceled and must be removed from <code class="literal">pendingCancelIds</code>. Here, we see the bulk of the canceled work deferred when the cancel request was handled. Execution is now amortized to be a constant time operation when executions occur repeatedly without a cancelation in-between. The worst case scenario is a linear runtime that is proportional to the number of canceled orders in the price level. Let's look at how <code class="literal">findActiveOrder</code> is used to update the state of the order book:</p><pre class="programlisting">orderBook.offers.headOption.fold(restLimitOrder) { &#13;
        case (price, offers) =&gt; findActiveOrder(offers, Set.empty) match { &#13;
          case (Some(o), Some(qq), rms) =&gt; (orderBook.copy( &#13;
            offers = orderBook.offers + (o.price -&gt; qq), activeIds = orderBook.activeIds -- rms), &#13;
            OrderExecuted(currentTime(), &#13;
              Execution(buy.id, o.price), Execution(o.id, o.price))) &#13;
          case (Some(o), None, rms) =&gt; (orderBook.copy( &#13;
            offers = orderBook.offers - o.price, activeIds = orderBook.activeIds -- rms), &#13;
            OrderExecuted(currentTime(), &#13;
              Execution(buy.id, o.price), Execution(o.id, o.price))) &#13;
          case (None, _, rms) =&gt; &#13;
            val bs = orderBook.bids.getOrElse(buy.price, Queue.empty).enqueue(buy) &#13;
            (orderBook.copy(bids = orderBook.bids + (buy.price -&gt; bs), &#13;
              offers = orderBook.offers - price, &#13;
              activeIds = orderBook.activeIds -- rms + buy.id), &#13;
              LimitOrderAdded(currentTime())) &#13;
        } &#13;
      } &#13;
</pre><p>Order crossing implementation is now arguably more complicated than in <code class="literal">ListOrderBook</code> or <code class="literal">QueueOrderBook</code> due to the work to remove canceled orders and to remove the removed order IDs from <code class="literal">pendingCancelIds</code>. In all three pattern match statements, the set of returned order IDs returned as the final tuple member is removed from <code class="literal">pendingCancelIds</code> to indicate that the order is now removed from the book. The first two pattern match statements handle the distinction between finding an active order with one or more remaining orders in the price level and finding an active order with zero remaining orders in the price level. In the latter scenario, the price level is removed from the book. The third pattern match statement accounts for the scenario where an active order is not found. If an active order is not found because all orders were pending cancelation, then, by definition, the entire price level was searched, and it is, therefore, now empty.</p></div><div class="section" title="Benchmarking LazyCancelOrderBook"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec19"/>Benchmarking LazyCancelOrderBook</h3></div></div></div><p>As a rigorous performance engineer, you realize that although your code compiles and your tests pass, your work is not yet complete. You begin pondering how to benchmark <code class="literal">LazyCancelOrderBook</code> to determine whether or not your changes have improved real-world performance. Your first idea is to test cancelation in isolation to confirm that this operation has indeed been optimized. To do this, you rework <code class="literal">CancelBenchmarks</code>, which was introduced in <a class="link" href="ch02.html" title="Chapter 2.  Measuring Performance on the JVM">Chapter 2</a>, <span class="emphasis"><em>Measuring Performance on the JVM</em></span>, to work with <code class="literal">QueueOrderBook</code> and <code class="literal">LazyCancelOrderBook</code>. This benchmark sweeps different price level sizes canceling the first order, the last order, and a nonexistent order. We omit the source code because it is identical to the previous implementation and instead consider the results. These results were produced by running the following:</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter4' 'jmh:run CancelBenchmarks -foe true'</strong></span>
</pre><p>The benchmark provides us with the following results:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Benchmark</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Enqueued order count</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Throughput (ops per second)</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Error as percentage of throughput</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerCancelFirstOrderInLine</code>
</p>
</td><td>
<p>1</p>
</td><td>
<p>6,912,696.09</p>
</td><td>
<p>± 0.44</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyCancelFirstOrderInLine</code>
</p>
</td><td>
<p>1</p>
</td><td>
<p>25,676,031.5</p>
</td><td>
<p>± 0.22</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerCancelFirstOrderInLine</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>2,332,046.09</p>
</td><td>
<p>± 0.96</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyCancelFirstOrderInLine</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>12,656,750.43</p>
</td><td>
<p>± 0.31</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerCancelFirstOrderInLine</code>
</p>
</td><td>
<p>1</p>
</td><td>
<p>5,641,784.63</p>
</td><td>
<p>± 0.49</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyCancelFirstOrderInLine</code>
</p>
</td><td>
<p>1</p>
</td><td>
<p>25,619,665.34</p>
</td><td>
<p>± 0.48</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerCancelFirstOrderInLine</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>1,788,885.62</p>
</td><td>
<p>± 0.39</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyCancelFirstOrderInLine</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>13,269,215.32</p>
</td><td>
<p>± 0.30</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerCancelFirstOrderInLine</code>
</p>
</td><td>
<p>1</p>
</td><td>
<p>9,351,630.96</p>
</td><td>
<p>± 0.19</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyCancelFirstOrderInLine</code>
</p>
</td><td>
<p>1</p>
</td><td>
<p>31,742,147.67</p>
</td><td>
<p>± 0.65</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerCancelFirstOrderInLine</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>6,897,164.11</p>
</td><td>
<p>± 0.25</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyCancelFirstOrderInLine</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>24,102,925.78</p>
</td><td>
<p>± 0.24</p>
</td></tr></tbody></table></div><p>This test demonstrates that <code class="literal">LazyCancelOrderBook</code> consistently outperforms <code class="literal">QueueOrderBook</code> when canceling the first order, the last order, and a nonexistent order across order queue sizes of one and ten. This is exactly as expected because <code class="literal">LazyCancelOrderBook</code> defers the most expensive work until an order is executed. We see constant performance independent of the position of the order-to-be-canceled, which is further proof that the removal work is deferred. Also as expected, we see that canceling a nonexistent order results in improved performance because a linear traversal is no longer required to ascertain the absence of an order. However, we notice the performance hit as the enqueued order count increases from one to ten for <code class="literal">LazyCancelOrderBook</code>. We can hypothesize that the nearly 50% throughput reduction is due to the overhead of managing the state of active and pending cancel order IDs.</p><p>This result is a promising sign that your changes are indeed improving the real-world performance. As the new implementation passed the initial litmus test, you think about how to representatively simulate a combination of executions and cancelations. You decide to focus on creating a microbenchmark that combines executions and cancelations to exercise <code class="literal">LazyCancelOrderBook</code> in scenarios that more closely resemble production. You think back to a recent lunch conversation you had with Dave about market trading flows and recall that he said it is common to see about two cancelations per execution. Running with this idea, you create a benchmark that interleaves trades and cancelations. For both order book implementations, you want to test performance when during the following scenarios:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Two trades per cancelation</li><li class="listitem" style="list-style-type: disc">One trade per cancelation</li><li class="listitem" style="list-style-type: disc">Two cancelations per trade</li></ul></div><p>These three scenarios will help reveal shortcomings in <code class="literal">LazyCancelOrderBook</code> by focusing on production-like order book activities. The benchmark requires initializing each order book with a set of resting orders to be canceled or executed against. The following snippet demonstrates how to initialize the order books in a JMH test:</p><pre class="programlisting">@State(Scope.Benchmark) &#13;
  class InterleavedOrderState { &#13;
    var lazyBook: LazyCancelOrderBook = LazyCancelOrderBook.empty &#13;
    var eagerBook: QueueOrderBook = QueueOrderBook.empty &#13;
 &#13;
    @Setup &#13;
    def setup(): Unit = { &#13;
      lazyBook = (1 to maxOrderCount).foldLeft(LazyCancelOrderBook.empty) { &#13;
        case (b, i) =&gt; LazyCancelOrderBook.handle( &#13;
          () =&gt; EventInstant.now(), b, AddLimitOrder( &#13;
            CommandInstant.now(), BuyLimitOrder(OrderId(i), bidPrice)))._1 &#13;
      } &#13;
      eagerBook = (1 to maxOrderCount).foldLeft(QueueOrderBook.empty) { &#13;
        case (b, i) =&gt; QueueOrderBook.handle( &#13;
          () =&gt; EventInstant.now(), b, AddLimitOrder( &#13;
            CommandInstant.now(), BuyLimitOrder(OrderId(i), bidPrice)))._1 &#13;
      } &#13;
    } &#13;
  } &#13;
</pre><p>Before each trial, both order books will be filled with <code class="literal">maxOrderCount</code> (defined to be 30) resting bids. As there are three scenarios to test and two order books, there are six benchmarks defined for this test. Each set of three scenarios is the same per order book implementation. To avoid duplication, the following snippet shows the three benchmarks implemented for <code class="literal">LazyCancelOrderBook</code>:</p><pre class="programlisting">@Benchmark &#13;
  def lazyOneToOneCT(state: InterleavedOrderState): LazyCancelOrderBook = { &#13;
    val b1 = LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      state.lazyBook, firstCancel)._1 &#13;
    LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      b1, firstCrossSell)._1 &#13;
  } &#13;
 &#13;
@Benchmark &#13;
  def lazyTwoToOneCT(state: InterleavedOrderState): LazyCancelOrderBook = { &#13;
    val b1 = LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      state.lazyBook, firstCancel)._1 &#13;
    val b2 = LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      b1, secondCancel)._1 &#13;
    LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      b2, firstCrossSell)._1 &#13;
  } &#13;
 &#13;
@Benchmark &#13;
  def lazyOneToTwoCT(state: InterleavedOrderState): LazyCancelOrderBook = { &#13;
    val b1 = LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      state.lazyBook, firstCancel)._1 &#13;
    val b2 = LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      b1, firstCrossSell)._1 &#13;
    LazyCancelOrderBook.handle(() =&gt; EventInstant.now(), &#13;
      b2, secondCrossSell)._1 &#13;
  } &#13;
</pre><p>These benchmarks follow the convention of denoting the cancelation frequency ("C") first and the trade frequency ("T") second. For example, the final benchmark implements the scenario that represents one cancelation for every two trades. The commands are defined as values out-of-scope to avoid generating garbage during benchmark invocation. The benchmark invocation looks like the following:</p><pre class="programlisting">
<span class="strong"><strong>sbt 'project chapter4' 'jmh:run InterleavedOrderBenchmarks -foe true'</strong></span>
</pre><p>This invocation produces the following results:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Benchmark</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Throughput (ops per second)</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Error as percentage of throughput</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerOneToTwoCT</code>
</p>
</td><td>
<p>797,339.08</p>
</td><td>
<p>± 2.63</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyOneToTwoCT</code>
</p>
</td><td>
<p>1,123,157.94</p>
</td><td>
<p>± 1.26</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerOneToOneCT</code>
</p>
</td><td>
<p>854,635.26</p>
</td><td>
<p>± 2.48</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyOneToOneCT</code>
</p>
</td><td>
<p>1,469,338.46</p>
</td><td>
<p>± 1.85</p>
</td></tr><tr><td>
<p>
<code class="literal">eagerTwoToOneCT</code>
</p>
</td><td>
<p>497,368.11</p>
</td><td>
<p>± 0.72</p>
</td></tr><tr><td>
<p>
<code class="literal">lazyTwoToOneCT</code>
</p>
</td><td>
<p>1,208,671.60</p>
</td><td>
<p>± 1.69</p>
</td></tr></tbody></table></div><p>Across the board, <code class="literal">LazyCancelOrderBook</code> outperforms <code class="literal">QueueOrderBook</code>. The relative difference between lazy and eager performance shows an interesting relationship. The following table captures the relative performance difference:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Benchmark</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>LazyCancelOrderBook percentage performance improvement</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">OneToTwoCT</code>
</p>
</td><td>
<p>141.00%</p>
</td></tr><tr><td>
<p>
<code class="literal">OneToOneCT</code>
</p>
</td><td>
<p>172.00%</p>
</td></tr><tr><td>
<p>
<code class="literal">TwoToOneCT</code>
</p>
</td><td>
<p>243.00%</p>
</td></tr></tbody></table></div><p>Studying the preceding table, we observe that <code class="literal">LazyCancelOrderBook</code> shows the greatest performance win when there are two cancelations per trade. This result demonstrates the benefit of deferring the cost of processing a cancelation request. The next trend that we see is that as the frequency of trades increases and the frequency of cancelations decreases, <code class="literal">QueueOrderBook</code> performance improves relative to <code class="literal">LazyCancelOrderBook</code>. This result makes sense because <code class="literal">LazyCancelOrderBook</code> incurs extra costs when performing a trade. In addition to searching for canceled orders, <code class="literal">LazyCancelOrderBook</code> must update <code class="literal">activeIds</code>. The <code class="literal">QueueOrderBook</code> avoids these costs, but we see the overwhelming cost of cancelation processing continues to overshadow <code class="literal">QueueOrderBook</code> performance. Summarizing these results, we have more confidence that <code class="literal">LazyCancelOrderBook</code> is a stand-in replacement for <code class="literal">QueueOrderBook</code>. In scenarios involving heavy volumes of cancelations, it appears to be a clear winner, and in other scenarios, it appears to maintain parity with <code class="literal">QueueOrderBook</code>.</p></div><div class="section" title="Lessons learned"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec20"/>Lessons learned</h3></div></div></div><p>In this section, we leveraged Scala collections, in conjunction with the judicious use of lazy evaluation, to improve the performance of a critical component in MVT's infrastructure. By working through several order book implementations, you learned first-hand how a well-suited data structure can improve performance while a less optimal choice can derail performance. This exercise also exposed you to how Scala implements several of its collections, which you can now use to your advantage when working on a performance problem.</p><p>
<code class="literal">LazyCancelOrderBook</code> illustrates how valuable deferred evaluation can be in a performance-sensitive environment. When faced with a performance challenge, ask yourself the following questions to see whether it is possible to defer work (CPU work, not your actual work!). The following table lists each question and how it was answered with the order book:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Question</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Application to order book example</strong></span>
</p>
</td></tr><tr><td>
<p>How can I decompose into smaller discrete chunks?</p>
</td><td>
<p>The act of canceling was decomposed into identifying the event that was sent to the requester and removing the canceled order from the book state.</p>
</td></tr><tr><td>
<p>Why am I performing all of these steps now?</p>
</td><td>
<p>Originally, order removal happened eagerly because it was the most logical way to model the process.</p>
</td></tr><tr><td>
<p>Can I change any constraints to allow me to model the problem differently?</p>
</td><td>
<p>Ideally, we would have liked to remove the constraint requiring rejection of nonexistent orders. Unfortunately, this was out of our control.</p>
</td></tr><tr><td>
<p>What operations in my system are most performant?</p>
</td><td>
<p>Executing an order and resting an order on the book are the most performant operations. We leveraged fast execution time to perform removals of canceled orders from the book.</p>
</td></tr></tbody></table></div><p>Like any approach, deferred evaluation is not a panacea. Diligent benchmarking and profiling are necessary to validate the benefit delivered by the change. Arguably the implementation of <code class="literal">LazyCancelOrderBook</code> is more complicated than <code class="literal">QueueOrderBook</code>, which will increase the cost to maintain the system. In addition to making implementation more complicated, it is now more difficult to reason about runtime performance due to the variable cost of order execution. For the scenarios that we tested, <code class="literal">LazyCancelOrderBook</code> remained at parity with or better than <code class="literal">QueueOrderBook</code>. However, we only exercised a few of the many possible scenarios, and we did so with only a single price level in the order book. In a real-world environment, additional benchmarking and profiling are needed to build enough confidence that this new implementation delivers better performance.</p></div></div></div></div>
<div class="section" title="Historical data analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"/>Historical data analysis</h1></div></div></div><p>You have done great work with the order book, and we hope, have learned valuable skills along the way! It is now time to explore a new facet of MVT's activities. A group of expert traders and data scientists are constantly studying historical market data to design performant trading strategies. Until now, the company has not had the luxury of allocating technical resources to this team. As a result, this group has been using clunky, unreliable, and under-performing tools to analyze market data and build elaborate trading strategies. With a performant order book, the top priority is to focus on improving the strategies implemented by the company. Your new best friend, Dave, has explicitly asked for you to join the team and help them modernize their infrastructure.</p><div class="section" title="Lagged time series returns"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec36"/>Lagged time series returns</h2></div></div></div><p>The main tool used by the team is a simple program designed to compute lagged time series returns from historical trade execution data. So far, this tool has been a big disappointment. Not only does it return mostly invalid results, it is also slow and fragile. Before diving into the code, Dave gives you a short presentation of the business rules involved. Return time series are derived from midpoint time series. A midpoint is calculated on each minute, and it is based on the bid and ask prices of each trade execution. Consider the following table as a simple example:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Execution time</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Bid price</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Ask price</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Midpoint</strong></span>
</p>
</td></tr><tr><td>
<p>01/29/16 07:45</p>
</td><td>
<p>2.3</p>
</td><td>
<p>2.5</p>
</td><td>
<p>2.55</p>
</td></tr><tr><td>
<p>01/29/16 07:46</p>
</td><td>
<p>2.1</p>
</td><td>
<p>2.4</p>
</td><td>
<p>2.25</p>
</td></tr><tr><td>
<p>01/29/16 07:47</p>
</td><td>
<p>2.9</p>
</td><td>
<p>3.4</p>
</td><td>
<p>3.15</p>
</td></tr><tr><td>
<p>01/29/16 07:48</p>
</td><td>
<p>3.2</p>
</td><td>
<p>3.4</p>
</td><td>
<p>3.3</p>
</td></tr><tr><td>
<p>01/29/16 07:49</p>
</td><td>
<p>3.1</p>
</td><td>
<p>3.3</p>
</td><td>
<p>3.2</p>
</td></tr></tbody></table></div><p>The formula to calculate a midpoint is <span class="emphasis"><em>(bid_price + ask_price) / 2</em></span>. For example, the midpoint at 01/29/16 07:47 is <span class="emphasis"><em>(2.9 + 3.4) / 2</em></span>, that is, 3.15.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>In the real world, a midpoint would be weighed by the volume of the transaction, and the time series would use a more fine-grained time unit, such as seconds or even milliseconds. To keep the example simple, we disregard the volume dimension by assuming a volume of 1 for all executions. We also focus on calculating one data point per minute instead of a more granular time series that would use seconds or even milliseconds.</p></div></div><p>A series of midpoints is used to compute a series of returns. A series of returns is defined for a certain rollup value in minutes. To calculate the three minute return at time t<sub>3</sub>, the formula is: (midpoint_at_t<sub>3</sub> - midpoint_at_t<sub>0</sub>) / midpoint_at_t<sub>0</sub>. We also multiply the result by 100 to use percentages. If we use the previous midpoint series to calculate a three minute return series, we obtain the following table:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Time</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Midpoint</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>3 minute return</strong></span>
</p>
</td></tr><tr><td>
<p>01/29/16 07:45</p>
</td><td>
<p>2.55</p>
</td><td>
<p>N/A</p>
</td></tr><tr><td>
<p>01/29/16 07:46</p>
</td><td>
<p>2.25</p>
</td><td>
<p>N/A</p>
</td></tr><tr><td>
<p>01/29/16 07:47</p>
</td><td>
<p>3.15</p>
</td><td>
<p>N/A</p>
</td></tr><tr><td>
<p>01/29/16 07:48</p>
</td><td>
<p>3.3</p>
</td><td>
<p>22.73</p>
</td></tr><tr><td>
<p>01/29/16 07:49</p>
</td><td>
<p>3.2</p>
</td><td>
<p>29.69</p>
</td></tr></tbody></table></div><p>Note that the first three midpoints do not have a corresponding three minute return as there is no midpoint that is old enough to be used.</p><p>You are now familiar with the domain and can have a look at the existing code. Starting with this model:</p><pre class="programlisting">case class TimestampMinutes(value: Int) extends AnyVal { &#13;
  def next: TimestampMinutes = TimestampMinutes(value + 1) &#13;
} &#13;
 &#13;
case class AskPrice(value: Int) extends AnyVal &#13;
case class BidPrice(value: Int) extends AnyVal &#13;
case class Execution(time: TimestampMinutes, ask: AskPrice, bid: BidPrice) &#13;
 &#13;
case class Midpoint(time: TimestampMinutes, value: Double) &#13;
object Midpoint { &#13;
  def fromAskAndBid(time: TimestampMinutes,askPrice: AskPrice, &#13;
   bidPrice: BidPrice): Midpoint = &#13;
   Midpoint(time, (bidPrice.value + askPrice.value) / 2D) &#13;
} &#13;
 &#13;
case class MinuteRollUp(value: Int) extends AnyVal &#13;
case class Return(value: Double) extends AnyVal &#13;
 &#13;
object Return { &#13;
  def fromMidpoint(start: Midpoint, end: Midpoint): Return = &#13;
    Return((end.value - start.value) / start.value * 100) &#13;
} &#13;
</pre><p>Everything looks straightforward. Note that prices, midpoints, and returns are represented as <code class="literal">Int</code> and <code class="literal">Double</code>. We assume that our system is able to normalize the prices as integers instead of decimals. This simplifies our code, and also improves the performance of the program since we use primitive <code class="literal">Double</code> instead of, for example, <code class="literal">BigDecimal</code> instances. <code class="literal">TimestampMinutes</code> is similar to the more commonly used Epoch timestamp, but only down to the minute (see <a class="ulink" href="https://en.wikipedia.org/wiki/Unix_time">https://en.wikipedia.org/wiki/Unix_time</a>).</p><p>After studying the model, we look at the existing implementation of the <code class="literal">computeReturnsWithList</code> method:</p><pre class="programlisting">def computeReturnsWithList( &#13;
  rollUp: MinuteRollUp, &#13;
  data: List[Midpoint]): List[Return] = { &#13;
  for { i &lt;- (rollUp.value until data.size).toList} yield       Return.fromMidpoint(data(i - rollUp.value), data(i)) &#13;
} &#13;
</pre><p>This method assumes that the list of midpoint received as input is already sorted by execution time. This randomly accesses various indices of the list to read the midpoints that are required to compute each return. To compute the second return value (index 1 in the returned list) with a rollup value of three minutes, we access elements at index 4 and 1 in the input list. The following diagram provides a visual reference for how returns are computed:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_003.jpg" alt="Lagged time series returns"/></div><p>
</p><p>You have been warned that this method is slow, but it is also incorrect. Dave has verified many times that it returns incorrect results. Before tackling the performance issue, you have to handle the correctness problem. Optimizing an incorrect approach would not be a good use of your time and, therefore, of the company's money! Rapidly, you realize that this method puts too much trust in the data that it is fed. For this algorithm to work, the input list of midpoints has to do the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">This has to be properly sorted by execution time, from the oldest to the newest execution</li><li class="listitem" style="list-style-type: disc">This has to have no more than one midpoint per minute</li><li class="listitem" style="list-style-type: disc">This has to not contain any minutes without a midpoint, that is, it has no missing data points</li></ul></div><p>You bring this up to Dave to better understand how the midpoint series is generated. He explains that it is loaded from sequential logs that are recorded by the order book. It is certain that the list is sorted by execution time. Also, he assures you that considering the large volume of trades handled by the order book, it is impossible to have a minute without a single execution. However, he acknowledges that it is more than likely that more than one midpoint is computed for the same execution time. It looks like you have found the problem causing invalid returns. Fixing it should not be too complicated, and you think that it is now time to reflect on the performance issue.</p><p>We spent time studying the structure of a singly-linked list in the previous section. You know that it is optimized for operations involving the head and the tail of the list. On the contrary, randomly accessing an element by its index is an expensive operation requiring linear time. To improve midpoint execution performance, we turn to a data structure with improved random access performance: <code class="literal">Vector</code>.</p><div class="section" title="Vector"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec21"/>Vector</h3></div></div></div><p>To improve the performance of our system, we should reconsider the data structure that stores <code class="literal">Midpoint</code> values. A good option is to replace <code class="literal">List</code> with <code class="literal">Vector</code>, another Scala collection provided by the standard library. The <code class="literal">Vector</code> is an efficient collection that provides effectively constant time random access. The cost of random access operations depends on various assumptions, such as, the maximum length of the <code class="literal">Vector</code>. The <code class="literal">Vector</code> is implemented as an ordered tree data structure called a trie. In a trie, the keys are the indices of the values stored in the <code class="literal">Vector</code> (to learn more about tries and their use cases, see <a class="ulink" href="https://en.wikipedia.org/wiki/Trie">https://en.wikipedia.org/wiki/Trie</a>). As <code class="literal">Vector</code> implements the <code class="literal">Seq</code> trait, just like <code class="literal">List</code>, modifying the existing method is straightforward:</p><pre class="programlisting">def computeReturnsWithVector( &#13;
  rollUp: MinuteRollUp, &#13;
  data: Vector[Midpoint]): Vector[Return] = { &#13;
  for { &#13;
    i &lt;- (rollUp.value until data.size).toVector &#13;
  } yield Return.fromMidpoint(data(i - rollUp.value), data(i)) &#13;
} &#13;
</pre><p>Changing the type of the collection is enough to switch to a more performant implementation. To make sure that we actually improved the performance, we devise a simple benchmark that is designed to use a few hours of historical trade executions and measure the throughput of each implementation. The results are as follows:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Benchmark</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Return rollup in minutes</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Throughput (ops per second)</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Error as percentage of throughput</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">computeReturnsWithList</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>534.12</p>
</td><td>
<p>± 1.69</p>
</td></tr><tr><td>
<p>
<code class="literal">computeReturnsWithVector</code>
</p>
</td><td>
<p>10</p>
</td><td>
<p>49,016.77</p>
</td><td>
<p>± 0.98</p>
</td></tr><tr><td>
<p>
<code class="literal">computeReturnsWithList</code>
</p>
</td><td>
<p>60</p>
</td><td>
<p>621.28</p>
</td><td>
<p>± 0.64</p>
</td></tr><tr><td>
<p>
<code class="literal">computeReturnsWithVector</code>
</p>
</td><td>
<p>60</p>
</td><td>
<p>51,666.50</p>
</td><td>
<p>± 1.64</p>
</td></tr><tr><td>
<p>
<code class="literal">computeReturnsWithList</code>
</p>
</td><td>
<p>120</p>
</td><td>
<p>657.44</p>
</td><td>
<p>± 1.07</p>
</td></tr><tr><td>
<p>
<code class="literal">computeReturnsWithVector</code>
</p>
</td><td>
<p>120</p>
</td><td>
<p>43,297.88</p>
</td><td>
<p>± 0.99</p>
</td></tr></tbody></table></div><p>Not only does <code class="literal">Vector</code> yield significantly better performance, it delivers the same throughput regardless of the size of the rollup. As a general rule, it is better to use <code class="literal">Vector</code> as a default implementation for immutable indexed sequences. Vector effectively provides constant time complexity not only for element random access but also for head and tail operations, as well as to append and prepend elements to an existing <code class="literal">Vector</code>.</p><p>The implementation of <code class="literal">Vector</code> is a tree structure of parity 32. Each node is implemented as an array of size 32, and it can store either up to 32 references to child nodes or up to 32 values. This 32-ary tree structure explains why the complexity of <code class="literal">Vector</code> is "effectively constant" instead of "constant". The real complexity of the implementation is log(32, N), where N is the size of the vector. This is considered close enough to actual constant time. This collection is a good choice to store very large sequences because the memory is allocated in chunks of 32 elements. These chunks are not preallocated for all levels of the tree, but only allocated as needed.</p><p>Until Scala 2.10, one downside of <code class="literal">Vector</code> as compared to <code class="literal">List</code> was the lack of pattern matching support. This is now fixed and you can pattern-match an instance of <code class="literal">Vector</code> in the same way you pattern match a <code class="literal">List</code>. Consider this short example of a method pattern matching a <code class="literal">Vector</code> to access and return its third element or return <code class="literal">None</code> if it contains fewer than three elements:</p><pre class="programlisting">def returnThirdElement[A](v: Vector[A]): Option[A] = v match { &#13;
 case _ +: _ +: x +: _ =&gt; Some(x) &#13;
  case _ =&gt; None &#13;
} &#13;
</pre><p>Invoking this method in the REPL demonstrates that pattern matching can be applied, as follows:</p><pre class="programlisting"><span class="strong"><strong>    scala&gt; returnThirdElement(Vector(1,2,3,4,5))&#13;
    res1: Option[Int] = Some(3)&#13;
</strong></span></pre></div><div class="section" title="Data clean up"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec22"/>Data clean up</h3></div></div></div><p>The return algorithm is now blazingly fast. That is, blazingly fast to return incorrect results! Remember that we still have to handle some edge cases and clean up the input data. Our algorithm only works if there is exactly one midpoint per minute, and Dave informed us that we are likely to see more than one midpoint computed for the same minute.</p><p>To handle this problem, we create a dedicated <code class="literal">MidpointSeries</code> module and make sure that an instance of <code class="literal">MidpointSeries</code>, wrapping a series of <code class="literal">Midpoint</code> instances, is properly created without duplicates:</p><pre class="programlisting">class MidpointSeries private(val points: Vector[Midpoint]) extends AnyVal &#13;
object MidpointSeries { &#13;
 &#13;
 private def removeDuplicates(v: Vector[Midpoint]): Vector[Midpoint] = { &#13;
   @tailrec &#13;
   def loop( &#13;
     current: Midpoint, &#13;
     rest: Vector[Midpoint], &#13;
     result: Vector[Midpoint]): Vector[Midpoint] = { &#13;
     val sameTime = current +: rest.takeWhile(_.time == current.time) &#13;
     val average = sameTime.map(_.value).sum / sameTime.size &#13;
 &#13;
     val newResult = result :+ Midpoint(current.time, average) &#13;
     rest.drop(sameTime.size - 1) match { &#13;
       case h +: r =&gt; loop(h, r, newResult) &#13;
       case _ =&gt; newResult &#13;
     } &#13;
   } &#13;
 &#13;
   v match { &#13;
     case h +: rest =&gt; loop(h, rest, Vector.empty) &#13;
     case _ =&gt; Vector.empty &#13;
   } &#13;
 } &#13;
 &#13;
 def fromExecution(executions: Vector[Execution]): MidpointSeries = { &#13;
   new MidpointSeries(removeDuplicates( &#13;
     executions.map(Midpoint.fromExecution))) &#13;
 } &#13;
</pre><p>Our <code class="literal">removeDuplicates</code> method uses a tail recursive method (Refer to <a class="link" href="ch03.html" title="Chapter 3. Unleashing Scala Performance">Chapter 3</a>, <span class="emphasis"><em>Unleashing Scala Performance</em></span>). This groups all the midpoints with the same execution time, calculates the average value of these data points, and builds a new series with these average values. Our module provides a <code class="literal">fromExecution</code> factory method to build an instance of <code class="literal">MidpointSeries</code> from a <code class="literal">Vector</code> of <code class="literal">Execution</code>. This factory method calls <code class="literal">removeDuplicates</code> to clean up the data.</p><p>To improve our module, we add our previous <code class="literal">computeReturns</code> method to the <code class="literal">MidpointSeries</code> class. That way, once constructed, an instance of <code class="literal">MidpointSeries</code> can be used to compute any return series:</p><pre class="programlisting">class MidpointSeries private(val points: Vector[Midpoint]) extends AnyVal { &#13;
 &#13;
 def returns(rollUp: MinuteRollUp): Vector[Return] = { &#13;
   for { &#13;
     i &lt;- (rollUp.value until points.size).toVector &#13;
   } yield Return.fromMidpoint(points(i - rollUp.value), points(i)) &#13;
 } &#13;
} &#13;
</pre><p>This is the same code that we previously wrote, but this time, we are confident that <code class="literal">points</code> does not contain duplicates. Note that the constructor is marked <code class="literal">private</code>, so the only way to instantiate an instance of <code class="literal">MidpointSeries</code> is via our factory method. This guarantees that it is impossible to create an instance of <code class="literal">MidpointSeries</code> with a "dirty" <code class="literal">Vector</code>. You release this new version of the program, wish good luck to Dave and his team, and leave for a well deserved lunch break.</p><p>As you return, you are surprised to find Vanessa, one of the data scientists, waiting at your desk. "The return series code still doesn't work", she says. The team was so excited to finally be given a working algorithm that they decided to skip lunch to play with it. Unfortunately, they discovered some inconsistencies with the results. You try to collect as much data as possible, and spend an hour looking at the invalid results that Vanessa is talking about. You noticed that they all involved trade executions for two specific symbols: FOO and BAR. A surprisingly small amount of trades is recorded for these symbols, and it is not unusual for several minutes to elapse between trade executions. You questioned Dave about these symbols. He explains that these are thinly traded tickers, and it is expected to see a lower trading volume for them. The problem is now clear to you. The midpoint series recorded for these symbols do not fulfill one of the prerequisite of your algorithm: at least one execution per minute. You refrain from reminding Dave that he assured you this situation was impossible and start working on a fix. The trader is always right!</p><p>You are not confident that you can rework the algorithm to make it more robust while preserving the current throughput. A better option would be to find a way to clean up the data to generate the missing data points. You seek advice from Vanessa. She explains that it would not disturb the trading algorithm to perform a linear extrapolation of the missing data points, based on the surrounding existing points. You write a short method to extrapolate a midpoint at a certain time using the previous and following points (respectively, <code class="literal">a</code> and <code class="literal">b</code> in the following snippet):</p><pre class="programlisting">private def extrapolate(a: Midpoint,b: Midpoint, time: TimestampMinutes): Midpoint = { &#13;
 val price = a.value + &#13;
   ((time.value - a.time.value) / (b.time.value - a.time.value)) * &#13;
     (b.value - a.value) &#13;
 Midpoint(time, price) &#13;
} &#13;
</pre><p>With this method, we can write a clean up method that follows the model of the previously mentioned <code class="literal">removeDuplicates</code> function to preprocess the data:</p><pre class="programlisting">private def addMissingDataPoints( &#13;
  v: Vector[Midpoint]): Vector[Midpoint] = { &#13;
 @tailrec &#13;
 def loop( &#13;
   previous: Midpoint, &#13;
   rest: Vector[Midpoint], &#13;
   result: Vector[Midpoint]): Vector[Midpoint] = rest match { &#13;
   case current +: mPoints if previous.time.value == current.time.value - 1 =&gt; &#13;
     // Nothing to extrapolate, the data points are consecutive &#13;
     loop(current, mPoints, result :+ previous) &#13;
 &#13;
   case current +: mPoints if previous.time.value &lt; current.time.value - 1 =&gt; &#13;
     //Need to generate a data point &#13;
     val newPoint = extrapolate(previous, current, previous.time.next) &#13;
     loop(newPoint, rest, result :+ previous) &#13;
 &#13;
   case _ =&gt; result :+ previous &#13;
 } &#13;
 &#13;
 v match { &#13;
   case h +: rest =&gt; loop(h, rest, Vector.empty) &#13;
   case _ =&gt; Vector.empty &#13;
 } &#13;
} &#13;
</pre><p>Our internal tail-recursive method handles the case where two points are already consecutive, and the case where a point is missing. In the latter case, we create a new point with our <code class="literal">extrapolate</code> method and insert it in the result <code class="literal">Vector</code>. Note that we use this new point to extrapolate consecutive missing points. We update our factory method to perform this additional clean up after removing possible duplicates:</p><pre class="programlisting">def fromExecution(executions: Vector[Execution]): MidpointSeries = { &#13;
 new MidpointSeries( &#13;
   addMissingDataPoints( &#13;
     removeDuplicates( &#13;
       executions.map(Midpoint.fromExecution)))) &#13;
} &#13;
</pre><p>We now have the assurance that our input data is clean and ready to be used by our return series algorithm.</p></div></div><div class="section" title="Handling multiple return series"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec37"/>Handling multiple return series</h2></div></div></div><p>The team is impressed by the improvements that you implemented, and by how quickly you were able to fix the existing code. They mention a project they have had in mind for a while without knowing how to approach it. A couple of weeks ago, Vanessa designed a machine learning algorithm to evaluate trading strategies over several tickers, based on their return series. This algorithm requires that all the return series involved contain the same amount of data points. Your previous changes already took care of this requirement. However, another condition is that the return values must be normalized or scaled. A feature is a machine learning term for an individual measurable property. In our example, each return data point is a feature. Feature scaling is used to standardize the range of possible values to ensure that broad ranges of values do not distort a learning algorithm. Vanessa explains that scaling features will help her algorithm to deliver better results. Our program will handle a set of return series, compute a scaling vector, and calculate a new set of normalized return series.</p><div class="section" title="Array"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec23"/>Array</h3></div></div></div><p>For this system, we consider switching from <code class="literal">Vector</code> to <code class="literal">Array</code>. <code class="literal">Array</code> is a mutable, indexed collection of values. It provides real constant complexity for random access, as opposed to <code class="literal">Vector</code>, which implements this operation in effectively constant time. However, contrary to <code class="literal">Vector</code>,  <code class="literal">Array</code> is allocated once as a single and contiguous chunk of memory. Furthermore, it does not permit append and prepend operations. A Scala <code class="literal">Array</code> is implemented with a Java <code class="literal">Array</code>, which is memory optimized. A Scala <code class="literal">Array</code> is more user-friendly than the native Java <code class="literal">Array</code>. Most methods that are available on other Scala collections are made available when using <code class="literal">Array</code>. Implicit conversions are used to augment <code class="literal">Array</code> with <code class="literal">ArrayOps</code> and <code class="literal">WrappedArray</code>. <code class="literal">ArrayOps</code> is a simple wrapper for <code class="literal">Array</code> to temporarily enrich <code class="literal">Array</code> with all the operations found in indexed sequences. Methods called on <code class="literal">ArrayOps</code> will yield an <code class="literal">Array</code>. On the contrary, a conversion from <code class="literal">Array</code> to <code class="literal">WrappedArray</code> is permanent. Transformer methods called on <code class="literal">WrappedArray </code>yield another <code class="literal">WrappedArray</code>. We see this in the standard library documentation, as follows:</p><pre class="programlisting">val arr = Array(1, 2, 3) &#13;
val arrReversed = arr.reverse   // arrReversed is an Array[Int] &#13;
val seqReversed: Seq[Int] = arr.reverse   &#13;
// seqReversed is a WrappedArray &#13;
</pre><p>Having decided to use <code class="literal">Array</code> for our new module, we start working on the code to scale the features of each return series:</p><pre class="programlisting">class ReturnSeriesFrame(val series: Array[Array[Return]]) { &#13;
  val scalingVector: Array[Double] = { &#13;
    val v = new Array[Double](series.length) &#13;
    for (i &lt;- series.indices) { &#13;
      v(i) = series(i).max.value &#13;
    } &#13;
      v &#13;
  } &#13;
} &#13;
</pre><p>A scaling vector is computed for a set of series. The first value of the vector is used to scale the first series, the second value for the second series, and so on. The scaling value is simply the greatest value in the series. We can now write the code to use the scaling vector and compute the normalized version of the frame:</p><pre class="programlisting">object ReturnSeriesFrame { &#13;
def scaleWithMap(frame: ReturnSeriesFrame): ReturnSeriesFrame = { &#13;
 new ReturnSeriesFrame( &#13;
   frame.series.zip(frame.scalingVector).map {  &#13;
case (series, scaling) =&gt; series.map(point =&gt; Return(point.value / scaling)) &#13;
   }) &#13;
} &#13;
} &#13;
</pre><p>We zip each series with its scaling value, and create a new scaled return series. We can compare the presented version of the code using <code class="literal">Array</code> with another, almost identical, implementation using <code class="literal">Vector</code> (this code is omitted here for brevity, but it can be found in the source code attached to the book):</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Benchmark</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Series Size</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Throughput in operations per second</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Error as percentage of throughput</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithVector</code>
</p>
</td><td>
<p>60</p>
</td><td>
<p>101,116.50</p>
</td><td>
<p>± 0.85</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithArray</code>
</p>
</td><td>
<p>60</p>
</td><td>
<p>176,260.52</p>
</td><td>
<p>± 0.68</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithVector</code>
</p>
</td><td>
<p>1,440</p>
</td><td>
<p>4,077.74</p>
</td><td>
<p>± 0.71</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithArray</code>
</p>
</td><td>
<p>1,440</p>
</td><td>
<p>7,865.85</p>
</td><td>
<p>± 1.39</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithVector</code>
</p>
</td><td>
<p>28,800</p>
</td><td>
<p>282.90</p>
</td><td>
<p>± 1.06</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithArray</code>
</p>
</td><td>
<p>28,800</p>
</td><td>
<p>270.36</p>
</td><td>
<p>± 1.85</p>
</td></tr></tbody></table></div><p>These results show that <code class="literal">Array</code> performs better than <code class="literal">Vector </code>for shorter series. As the size of the series increases, their respective performances are on-par. We can even see that the throughput is identical for a series containing 20 days of data (28,800 minutes). For larger sequences, the locality of <code class="literal">Vector </code>and its memory allocation model alleviate the difference with <code class="literal">Array</code>.</p><p>Our implementation is idiomatic: it uses higher-order functions and immutable structures. However, using transform functions, such as <code class="literal">zip</code> and <code class="literal">map</code>, creates new instances of <code class="literal">Array</code>. An alternative is to leverage the mutable nature of <code class="literal">Array</code> to limit the amount of garbage generated by our program.</p></div><div class="section" title="Looping with the Spire cfor macro"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec24"/>Looping with the Spire cfor macro</h3></div></div></div><p>Scala supports two loop constructs: the <code class="literal">for</code> loop and the <code class="literal">while</code> loop. The latter, in spite of its good performance characteristics, is usually avoided in functional programming. It requires the usage of mutable state and <code class="literal">var</code> to keep track of the looping condition. In this section, we will show you a technique to take advantage of <code class="literal">while</code> loop performance that prevents mutable references from leaking into application code.</p><p>Spire is a numeric library written for Scala that allows developers to write efficient numeric code. Spire leverages patterns, such as, type classes, macros, and specialization (remember specialization from <a class="link" href="ch03.html" title="Chapter 3. Unleashing Scala Performance">Chapter 3</a>, <span class="emphasis"><em>Unleashing Scala Performance</em></span>). You can learn more about Spire at <a class="ulink" href="https://github.com/non/spire">https://github.com/non/spire</a>.</p><p>One of the macros made available by Spire is <code class="literal">cfor</code>. Its syntax is inspired from the more traditional for loop that is encountered in Java. In the following implementation of feature scaling, we use the <code class="literal">cfor</code> macro to iterate over our series and normalize the values:</p><pre class="programlisting">def scaleWithSpire(frame: ReturnSeriesFrame): ReturnSeriesFrame = { &#13;
 import spire.syntax.cfor._ &#13;
 &#13;
 val result = new Array[Array[Return]](frame.series.length) &#13;
 &#13;
 cfor(0)(_ &lt; frame.series.length, _ + 1) { i =&gt; &#13;
   val s = frame.series(i) &#13;
   val scaled = new Array[Return](s.length) &#13;
   cfor(0)(_ &lt; s.length, _ + 1) { j =&gt; &#13;
     val point = s(j) &#13;
     scaled(j) = Return(point.value / frame.scalingVector(i)) &#13;
   } &#13;
   result(i) = scaled &#13;
 } &#13;
 &#13;
 new ReturnSeriesFrame(result) &#13;
} &#13;
</pre><p>This example highlights that <code class="literal">cfor</code> macros can be nested. The macro is essentially syntactic sugar that compiles to a Scala <code class="literal">while</code> loop. We can examine the following generated bytecode to prove this:</p><pre class="programlisting">public highperfscala.dataanalysis.ArrayBasedReturnSeriesFrame scaleWithSpire(highperfscala.dataanalysis.ArrayBasedReturnSeriesFrame); &#13;
    Code: &#13;
       0: aload_1 &#13;
       1: invokevirtual #121                // Method highperfscala/dataanalysis/ArrayBasedReturnSeriesFrame.series:()[[Lhighperfscala/dataanalysis/Return; &#13;
       4: arraylength &#13;
       5: anewarray     #170                // class "[Lhighperfscala/dataanalysis/Return;" &#13;
       8: astore_2 &#13;
       9: iconst_0 &#13;
      10: istore_3 &#13;
      11: iload_3 &#13;
     [... omitted for brevity] &#13;
      39: iload         6 &#13;
      [... omitted for brevity] &#13;
      82: istore        6 &#13;
      84: goto          39 &#13;
      [... omitted for brevity] &#13;
      95: istore_3 &#13;
      96: goto          11 &#13;
      99: new           #16                 // class highperfscala/dataanalysis/ArrayBasedReturnSeriesFrame &#13;
     102: dup &#13;
     103: aload_2 &#13;
     104: invokespecial #19                 // Method highperfscala/dataanalysis/ArrayBasedReturnSeriesFrame."&lt;init&gt;":([[Lhighperfscala/dataanalysis/Return;)V &#13;
     107: areturn &#13;
</pre><p>We notice the two <code class="literal">goto</code> statements, instructions <code class="literal">96</code> and <code class="literal">84</code>, which are used to loop back respectively to the beginning of the outer loop and the inner loop (which respectively begin with instructions <code class="literal">11</code> and <code class="literal">39</code>). We can run a benchmark of this new implementation to confirm the performance gain:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Benchmark</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Series size</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Throughput (ops per second)</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Error as percentage of throughput</strong></span>
</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithArray</code>
</p>
</td><td>
<p>60</p>
</td><td>
<p>176,260.52</p>
</td><td>
<p>± 0.68</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithCfor</code>
</p>
</td><td>
<p>60</p>
</td><td>
<p>256,303.49</p>
</td><td>
<p>± 1.33</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithArray</code>
</p>
</td><td>
<p>1,440</p>
</td><td>
<p>7,865.85</p>
</td><td>
<p>± 1.39</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithCfor</code>
</p>
</td><td>
<p>1,440</p>
</td><td>
<p>11,446.47</p>
</td><td>
<p>± 0.89</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithArray</code>
</p>
</td><td>
<p>28,800</p>
</td><td>
<p>270.36</p>
</td><td>
<p>± 1.85</p>
</td></tr><tr><td>
<p>
<code class="literal">normalizeWithCfor</code>
</p>
</td><td>
<p>28,800</p>
</td><td>
<p>463.56</p>
</td><td>
<p>± 1.51</p>
</td></tr></tbody></table></div><p>The macro, which is compiled to a while loop, is able to deliver better performance. Using the <code class="literal">cfor</code> construct, we are able to retain performance while avoiding the introduction of multiple vars. Although this approach sacrifices immutability, the scope of mutability is limited and less error-prone than an equivalent implementation using an imperative <code class="literal">while</code> or <code class="literal">for</code> loop.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Summary</h1></div></div></div><p>In this chapter, we explored and experimented with various collection implementations. We discussed the underlying representation, complexity, and use cases of each data structure. We also introduced a third-party library, Spire, to improve the performance of our programs. Some of the implementations presented drifted away from typical functional programming practices, but we were able to restrict the use of mutable state to internal modules, while still exposing functional public APIs. We expect that you are eager to learn more, but in the next chapter, we will become lazy! In contrast to this chapter, which focused on eager collections, we turn our attention to lazy collections in the next chapter.</p></div></body></html>