- en: '*Chapter 10*: Exporting, Batching, Bulking, and Loading'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Manipulating large amounts of data requires serious skills (know-how and programming
    skills) in exporting, batching, bulking, and loading data. Each of these areas
    requires a significant amount of code and a lot of time to be implemented and
    tested against real datasets. Fortunately, jOOQ provides comprehensive APIs that
    cover all these operations and expose them in a fluent style, while hiding the
    implementation details. In this context, our agenda includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exporting data in text, JSON, XML, CSV, charts, and `INSERT` statements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batching `INSERT`, `UPDATE`, `DELETE`, `MERGE`, and `Record`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bulking queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading JSON, CSV, arrays, and `Record`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/jOOQ-Masterclass/tree/master/Chapter10](https://github.com/PacktPublishing/jOOQ-Masterclass/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Exporting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exporting (or formatting) data is achievable via the `org.jooq.Formattable`
    API. jOOQ exposes a suite of `format()` and `formatFoo()` methods that can be
    used to format `Result` and `Cursor` (remember `fetchLazy()` from [*Chapter 8*](B16833_08.xhtml#_idTextAnchor128),
    *Fetching and Mapping*) as text, JSON, XML, CSV, XML, charts, and `INSERT` statements.
    As you can see in the documentation, all these methods come in different flavors
    capable of exporting data into a string or a file via the Java `OutputStream`
    or `Writer` APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting as text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I''m sure that you have already seen in your console output something similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Tabular text data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16833_Figure_10.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Tabular text data
  prefs: []
  type: TYPE_NORMAL
- en: 'This textual tabular representation can be achieved via the `format()` method.
    A flavor of this method takes an integer argument representing the maximum number
    of records to include in the formatted result (by default, jOOQ logs just the
    first five records of the result formatted via jOOQ''s text export, but we can
    easily format and log all the `fetch` records as `result.format(result.size()`).
    But, if you need a fine-tuning of this output, then jOOQ has a dedicated immutable
    class named `TXTFormat` with a lot of intuitive options available in the documentation.
    Using this class in conjunction with exporting the resulting text into a file
    named `result.txt` via `format``(Writer writer, TXTFormat format)` can be done
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can see this example in the bundled code, `Format` (available for MySQL
    and PostgreSQL), next to other examples.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exporting `Result`/`Cursor` as JSON can be done via `formatJSON()` and its
    overloads. Without arguments, `formatJSON()` produces a JSON containing two main
    arrays: an array named `"fields"`, representing a header (as you''ll see later,
    this can be useful for importing the JSON into the database), and an array named
    `"records"`, which wraps the fetched data. Here is such an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this JSON can be obtained via the `formatJSON()` method without arguments,
    or via `formatJSON(JSONFormat.DEFAULT_FOR_RESULTS)`. If we want to render only
    the `"records"` array and avoid rendering the header represented by the `"fields"`
    array, then we can rely on `formatJSON(JSONFormat.DEFAULT_FOR_RECORDS)`. This
    produces something as shown here (as you''ll see later, this can also be imported
    back into the database):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`DEFAULT_FOR_RESULTS` and `DEFAULT_FOR_RECORDS` are two statics of the immutable
    `org.jooq.JSONFormat` used to fine-tune JSON imports/exports. When these statics
    are not enough, we can instantiate `JSONFormat` and fluently append a suite of
    intuitive options such as the ones from this example (check all the available
    options in the jOOQ documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, let''s use `jsonFormat` in the context of exporting a JSON into a
    file via `formatJSON(Writer writer, JSONFormat format)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting JSON looks like this (also importable into the database):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'If we fetch a single `Record` (so, not `Result`/`Cursor`, via `fetchAny()`,
    for instance), then `formatJSON()` will return an array containing only the data,
    as in this sample of fetching `Record3<String, Long, String>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'But, if we explicitly mention `JSONFormat.RecordFormat.OBJECT`, then this becomes
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: You can check out this example in the bundled code, *Format* (available for
    MySQL and PostgreSQL), next to other examples including formatting a UDT, an array
    type, and an embeddable type as JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Export XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exporting `Result`/`Cursor` as XML can be done via `formatXML()` and its overloads.
    Without arguments, `formatXML()` produces an XML containing two main elements:
    an element named `<fields/>`, representing a header, and an element named `<records/>`,
    which wraps the fetched data. Here is such an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The jOOQ code that produced this output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this XML can be obtained via the `formatXML()` method without arguments
    or via `formatXML(XMLFormat.DEFAULT_FOR_RESULTS)`. If we want to keep only the
    `<records/>` element and avoid rendering the `<fields/>` element, then use `formatJXML(XMLFormat.DEFAULT_FOR_RECORDS)`.
    This is an output sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '`DEFAULT_FOR_RESULTS` and `DEFAULT_FOR_RECORDS` are two statics of the immutable
    `org.jooq.XMLFormat`, used to fine-tune XML imports/exports. Besides these, we
    can instantiate `XMLFormat` and fluently append a suite of intuitive options.
    For instance, the previous snippets of XML are rendered based on the default record
    format, `XMLFormat.RecordFormat.VALUE_ELEMENTS_WITH_FIELD_ATTRIBUTE`; notice the
    `<value/>` element and the `field` attribute. But, using `XMLFormat`, we can go
    for two other options: `VALUE_ELEMENTS` and `COLUMN_NAME_ELEMENTS`. The former
    formats the records using just the `<value/>` element as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '`COLUMN_NAME_ELEMENTS` uses the column names as elements. Let''s use this setting
    next to `header(false)` to format the `MANAGER.MANAGER_EVALUATION` UDT (available
    in the PostgreSQL schema):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting XML looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'If we fetch a single `Record` (so, no `Result`/`Cursor` via `fetchAny()`, for
    instance) then `formatXML()` will return an XML containing only the data, as in
    this sample of fetching `Record3<String, Long, String>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, you can alter this default output via `XMLFormat`. For instance,
    let''s consider that we have this record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'And, let''s apply `RecordFormat.COLUMN_NAME_ELEMENTS`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'The rendered XML is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: Consider this example next to others (including exporting XML into a file) in
    the bundled code, `Format` (available for MySQL and PostgreSQL).
  prefs: []
  type: TYPE_NORMAL
- en: Exporting HTML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exporting `Result`/`Cursor` as HTML can be done via `formatHTML()` and its
    overloads. By default, jOOQ attempts to wrap the fetched data in a simple HTML
    table, therefore, expect to see tags such as `<table/>`, `<th/>`, and `<td/>`
    in the resultant HTML. For instance, formatting the `MANAGER.MANAGER_EVALUATION`
    UDT (available in the PostgreSQL schema) can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'The resultant HTML looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the value of `MANAGER_EVALUATION`, (*67, 34, 33, 66*), is wrapped
    in a `<td/>` tag. But, maybe you''d like to obtain something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: 'We can obtain this HTML by decorating our query as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: Check out more examples in the bundled code, *Format* (available for MySQL and
    PostgreSQL).
  prefs: []
  type: TYPE_NORMAL
- en: Exporting CSV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exporting `Result`/`Cursor` as CSV can be done via `formatCSV()` and its overloads.
    By default, jOOQ renders a CSV file as the one here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: 'Among the handy overloads, we have `formatCSV``(boolean header, char delimiter,
    String nullString)`. Via this method, we can specify whether the CSV header should
    be rendered (by default, `true`), the record''s delimiter (by default, a *comma*),
    and a string for representing `NULL` values (by default, `""`). Next to this method,
    we also have a suite of combinations of these arguments such as `formatCSV(char
    delimiter, String nullString)`, `formatCSV(char delimiter)`, and `formatCSV(boolean
    header, char delimiter)`. Here is an example that renders the header (default)
    and uses `TAB` as a delimiter and `"N/A"` for representing `NULL` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting CSV looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever we need more options, we can rely on the immutable `CSVFormat`. Here
    is an example of using `CSVFormat` and exporting the result in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: The complete code next to other examples is available in the bundled code, *Format*
    (available for MySQL and PostgreSQL).
  prefs: []
  type: TYPE_NORMAL
- en: Exporting a chart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exporting `Result`/`Cursor` as a chart may result in something as observed
    in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – jOOQ chart sample'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16833_Figure_10.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – jOOQ chart sample
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an area chart containing three graphs: *a*, *b*, and *c*. Graph `a`
    represents `PRODUCT.BUY_PRICE`, graph *b* represents `PRODUCT.MSRP`, and graph
    *c* represents `avg(ORDERDETAIL.PRICE_EACH)`. While this chart can be displayed
    on the console, it can be exported to a file as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, the chart is obtained via the `formatChart()` method. More precisely,
    in this example, via `formatChart``(Writer writer, ChartFormat format)`. The `ChartFormat`
    class is immutable and contains a suite of options for customizing the chart.
    While you can check all of them in the jOOQ documentation, here is the `cf` used
    in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: The complete code next to other examples is available in the bundled code in
    the application named *Format* (available for MySQL and PostgreSQL).
  prefs: []
  type: TYPE_NORMAL
- en: Exporting INSERT statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: jOOQ can export `Result`/`Cursor` as `INSERT` statements via the `formatInsert()`
    method and its overloads. By default, if the first record is `TableRecord`, then
    `formatInsert()` uses the first record's `TableRecord.getTable()` method to generate
    `INSERT` statements into this table, otherwise, it generates `INSERT` statements
    into `UNKNOWN_TABLE`. In both cases, jOOQ calls the `Result.fields()` method to
    determine the column names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example that exports the generated `INSERT` statements into a file
    on disk. The `INSERT` statements are generated into a database table named `product_stats`
    specified via `formatInsert(Writer writer, Table<?> table, Field<?>… fields)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: 'A generated `INSERT` statement looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: The complete code next to other examples, including exporting `INSERT` statements
    for UDT, JSON, array, and embeddable types, is available in the bundled code,
    `Format` (available for MySQL and PostgreSQL). Next, let's talk about batching.
  prefs: []
  type: TYPE_NORMAL
- en: Batching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batching can be the perfect solution for avoiding performance penalties caused
    by a significant number of separate database/network round trips representing
    inserts, deletes, updates, merges, and so on. For instance, without batching,
    having 1,000 inserts requires 1,000 separate round trips, while employing batching
    with a batch size of 30 will result in 34 separate round trips. The more inserts
    (statements) we have, the more helpful batching is.
  prefs: []
  type: TYPE_NORMAL
- en: Batching via DSLContext.batch()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `DSLContext` class exposes a suite of `batch()` methods that allow us to
    execute a set of queries in batch mode. So, we have the following `batch()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes, jOOQ implements these methods via JDBC's `addBatch()`. Each
    query is accumulated in the batch via `addBatch()`, and in the end, it calls the
    JDBC `executeBatch()` method to send the batch to the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s assume that we need to batch a set of `INSERT` statements
    into the `SALE` table. If you have a Hibernate (JPA) background, then you know
    that this kind of batch will not work because the `SALE` table has an auto-incremented
    primary key, and Hibernate will automatically disable/prevent insert batching.
    But, jOOQ doesn''t have such issues, so batching a set of inserts into a table
    having an auto-incremented primary key can be done via `batch(Query... queries)`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned array contains the number of affected rows per `INSERT` statement
    (in this case, `[1, 1, 1, …]`). While executing several queries without bind values
    can be done as you just saw, jOOQ allows us to execute one query several times
    with bind values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: Notice that you will have to provide *dummy* bind values for the original query,
    and this is commonly achieved via `null` values, as in this example. jOOQ generates
    a single query (`PreparedStatement`) with placeholders (`?`) and will loop the
    bind values to populate the batch. Whenever you see that `int[]` contains a negative
    value (for instance, `-2`) it means that the affected row count value couldn't
    be determined by JDBC.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, JDBC prepared statements are better, so, whenever possible,
    jOOQ relies on `PreparedStatement` ([www.jooq.org/doc/latest/manual/sql-execution/statement-type/](http://www.jooq.org/doc/latest/manual/sql-execution/statement-type/)).
    But, we can easily switch to static statements (`java.sql.Statement`) via `setStatementType()`
    or `withStatementType()` as in the following example (you can also apply this
    globally via `@Bean`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: This time, the bind values will be automatically inlined into a static batch
    query. This is the same as the first examples from this section, which use `batch(Query...
    queries)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, using binding values is also useful for inserting (updating, deleting,
    and so on) a collection of objects. For instance, consider the following list
    of `SimpleSale` (POJO):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we define the proper `BatchBindStep` containing one `INSERT` (it could
    be `UPDATE`, `DELETE`, and so on, as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, we bind the values and execute the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find these examples in the bundled code, `BatchInserts`, next to examples
    for batching updates, `BatchUpdates`, and deletes, `BatchDeletes`, as well. But,
    we can also combine all these kinds of statements in a single `batch()` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE299]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE300]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE301]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE303]'
  prefs: []
  type: TYPE_PRE
- en: While using `batch()` methods, jOOQ will always preserve your order of statements
    and will send all these statements in a single batch (round trip) to the database.
    This example is available in an application named *CombineBatchStatements*.
  prefs: []
  type: TYPE_NORMAL
- en: During the batch preparation, the statements are accumulated in memory, so you
    have to pay attention to avoid memory issues such as OOMs. You can easily emulate
    a batch size by calling the jOOQ batch in a `for` loop that limits the number
    of statements to a certain value. You can execute all batches in a single transaction
    (in case of an issue, roll back all batches) or execute each batch in a separate
    transaction (in case of an issue, roll back only the last batch). You can see
    these approaches in the bundled code, `EmulateBatchSize`.
  prefs: []
  type: TYPE_NORMAL
- en: While a synchronous batch ends up with an `execute()` call, an asynchronous
    batch ends up with an `executeAsync()` call. For example, consider the application
    named *AsyncBatch*. Next, let's talk about batching records.
  prefs: []
  type: TYPE_NORMAL
- en: Batching records
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Batching records is another story. The jOOQ API for batching records relies
    on a set of dedicated methods per statement type as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`INSERT`: `batchInsert()` follows `TableRecord.insert()` semantics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UPDATE`: `batchUpdate()` follows `UpdatableRecord.update()` semantics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DELETE`: `batchDelete()` follows `UpdatableRecord.delete()` semantics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MERGE`: `batchMerge()` follows `UpdatableRecord.merge()` semantics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INSERT`/`UPDATE`: `batchStore()` follows `UpdatableRecord.store()` semantics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll cover each of these statements but before that, let's point out
    an important aspect. By default, all these methods create batch operations for
    executing a certain type of query with bind values. jOOQ preserves the order of
    the records as long as the records generate the same SQL with bind variables,
    otherwise, the order is changed to group together the records that share the same
    SQL with bind variables. So, in the best-case scenario, when all records generate
    the same SQL with bind variables, there will be a single batch operation, while
    in the worst-case scenario, the number of records will be equal to the number
    of batch operations. In short, the number of batch operations that will be executed
    is equal to the number of distinct rendered SQL statements.
  prefs: []
  type: TYPE_NORMAL
- en: If we switch from the default `PreparedStatement` to a static `Statement` (`StatementType.STATIC_STATEMENT`),
    then the record values are inlined. This time, there will be just one batch operation
    and the order of records is preserved exactly. Obviously, this is preferable when
    the order of records must be preserved and/or the batch is very large, and rearranging
    the records can be time-consuming and results in a significant number of batch
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Batch records insert, update, and delete
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s consider the following set of `Record`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE304]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE305]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE306]'
  prefs: []
  type: TYPE_PRE
- en: 'Inserting these records in batch can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, these records are inserted in a single batch operation since
    the generated SQL with bind variables is the same for `sr1` to `sr3`. Moreover,
    the batch preserves the order of records as given (`sr3`, `sr1`, and `sr2`). If
    we want to update, and respectively to delete these records, then we replace `batchInsert()`
    with `batchUpdate()`, and, respectively, `batchDelete()`. You can also have these
    records in a collection and pass that collection to `batchInsert()`, as in this
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s consider a mix of records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE310]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE311]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE312]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE313]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE314]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE315]'
  prefs: []
  type: TYPE_PRE
- en: Calling `batchInsert(bt1, sr1, sr2, bt2, sr4, sr3)` is executed in two batch
    operations, one for `SaleRecord` and one for `BankTransactionRecord`. jOOQ will
    group `SaleRecord` (`sr1`, `sr2`, `sr3`, and `sr4`) in one batch operation and
    `BankTransactionRecord` (`bt1` and `bt2`) in another batch operation, so the order
    of records in not preserved (or, is partially preserved) since (`bt1`, `sr1`,
    `sr2`, `bt2`, `sr4`, and `sr3`) may become ((`bt1` and `bt2`), (`sr1`, `sr2`,
    `sr4`, and `sr3`)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s consider these records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE316]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE317]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE318]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE319]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE320]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE321]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE322]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE323]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE324]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE325]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE326]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE327]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE328]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE329]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE330]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE331]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE332]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE333]'
  prefs: []
  type: TYPE_PRE
- en: If we execute `batchInsert(sr3, sr2, sr1)`, then there will be three batch operations,
    since `sr1`, `sr2`, and `sr3` produce three SQLs that will different bind variables.
    The order of records is preserved as `sr3`, `sr2`, and `sr1`. The same flow applies
    for `batchUpdate()` and `batchDelete()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any of these examples can take advantage of JDBC static statements by simply
    adding the `STATIC_STATEMENT` setting as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE334]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE335]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE336]'
  prefs: []
  type: TYPE_PRE
- en: You can practice these examples in *BatchInserts*, *BatchUpdates*, and *BatchDeletes*.
  prefs: []
  type: TYPE_NORMAL
- en: Batch merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you already know from the bullet list from the *Batching records* section,
    `batchMerge()` is useful for executing batches of `MERGE` statements. Mainly,
    `batchMerge()` conforms to the `UpdatableRecord.merge()` semantics covered in
    [*Chapter 9*](B16833_09.xhtml#_idTextAnchor162), *CRUD, Transactions, and Locking*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, `batchMerge()` renders the synthetic `INSERT ... ON DUPLICATE
    KEY UPDATE` statement emulated depending on dialect; in MySQL, via `INSERT ...
    ON DUPLICATE KEY UPDATE`, in PostgreSQL, via `INSERT ... ON CONFLICT`, and in
    SQL Server and Oracle, via `MERGE INTO`. Practically, `batchMerge()` renders an
    `INSERT ... ON DUPLICATE KEY UPDATE` statement independent of the fact that the
    record has been previously fetched from the database or is created now. The number
    of distinct rendered SQL statements gives us the number of batches. So, by default
    (which means default settings, default changed flags, and no optimistic locking),
    jOOQ renders a query that delegates to the database the decision between insert
    and update based on the primary key uniqueness. Let''s consider the following
    records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE338]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE339]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute a merge in batch as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE340]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, in PostgreSQL, the render SQL is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE341]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE342]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE343]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE344]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE345]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE346]'
  prefs: []
  type: TYPE_PRE
- en: Because `sr1` (having primary key *1*) and `sr2` (having primary key *2*) already
    exist in the `SALE` table, the database will decide to update them, while `sr3`
    (having primary key *9999*) will be inserted, since it doesn't exist in the database.
    There will be just one batch since the generated SQL with bind variables is the
    same for all `SaleRecord`. The order of records is preserved. More examples are
    available in `BatchMerges`.
  prefs: []
  type: TYPE_NORMAL
- en: Batch store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`batchStore()` is useful for executing `INSERT` or `UPDATE` statements in the
    batch. Mainly, `batchStore()` conforms to `UpdatableRecord.store()`, which was
    covered in the previous chapter. So, unlike `batchMerge()`, which delegates the
    decision of choosing between update or insert to the database, `batchStore()`
    allows jOOQ to decide whether INSERT or UPDATE should be rendered by analyzing
    the state of the primary key''s value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s rely on defaults (which means default settings, default
    changed flags, and no optimistic locking), the following two records are used
    for executing in a batch store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE347]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE348]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE349]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE350]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE351]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE352]'
  prefs: []
  type: TYPE_PRE
- en: Since `sr1` is a brand-new `SaleRecord`, it will result in `INSERT`. On the
    other hand, `sr2` was fetched from the database and it was updated, so it will
    result in `UPDATE`. Obviously, the generated SQL statements are not the same,
    therefore, there will be two batch operations and the order will be preserved
    as `sr1` and `sr2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example that updates `SaleRecord` and adds a few more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE353]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE354]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE355]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE356]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE357]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE358]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE359]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE360]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE361]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two batch operations: a batch that contains all updates needed to update
    the fetched `SaleRecord` and a batch that contains all inserts needed to insert
    the new `SaleRecord`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the bundled code, you can find more examples that couldn't be listed here
    because they are large, so take your time to practice examples from `BatchStores`.
    This was the last topic of this section. Next, let's talk about the *batched connection*
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Batched connection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides the batching capabilities covered so far, jOOQ also comes with an API
    named `org.jooq.tools.jdbc.BatchedConnection`. Its main purpose is to buffer already
    existing jOOQ/JDBC statements and execute them in batches without requiring us
    to change the SQL strings or the order of execution. We can use `BatchedConnection`
    explicitly or indirectly via `DSLContext.batched``(BatchedRunnable runnable)`
    or `DSLContext.batchedResult``(BatchedCallable<T> callable)`. The difference between
    them consists of the fact that the former returns `void` and the latter returns
    `T`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s assume that we have a method (service) that produces a
    lot of `INSERT` and `UPDATE` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE362]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE363]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE364]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE365]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE366]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE367]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE368]'
  prefs: []
  type: TYPE_PRE
- en: 'To improve the performance of this method, we can simply add batch-collecting
    code via `DSLContext.batched()`, as here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE369]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE370]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE371]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, if `INSERT` statements are produced by an `inserts(Configuration
    c)` method and `UPDATE` statements by another method, `updates(Configuration c)`,
    then both of them should be collected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE372]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE373]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE374]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE375]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE376]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE377]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, this API can be used for batching jOOQ records as well. Here is a
    sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE378]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE379]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE380]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE381]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE382]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE383]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE384]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE385]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE386]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, here is another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE387]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE388]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE389]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE390]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE391]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE392]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE393]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE394]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE395]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE396]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE397]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE398]'
  prefs: []
  type: TYPE_PRE
- en: Notice that jOOQ will preserve exactly your order of statements and this order
    may affect the number of batch operations. Read carefully the following note,
    since it is very important to have it in your mind while working with this API.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'jOOQ automatically creates a new batch every time it detects that:'
  prefs: []
  type: TYPE_NORMAL
- en: '- The SQL string changes (even whitespace is considered a change).'
  prefs: []
  type: TYPE_NORMAL
- en: '- A query produces results (for instance, `SELECT`); such queries are not part
    of the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: '- A static statement occurs after a prepared statement (or vice versa).'
  prefs: []
  type: TYPE_NORMAL
- en: '- A JDBC interaction is invoked (transaction committed, connection closed,
    and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: '- The batch size threshold is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: As an important limitation, notice that the affected row count value will be
    reported always by the JDBC `PreparedStatement.executeUpdate()` as 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the last bullet from the previous note refers to a batch size threshold.
    Well, this API can take advantage of `Settings.batchSize()`, which sets the maximum
    batch statement size as here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE399]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE400]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE401]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE402]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, if we rely on `BatchedConnection` explicitly, then we can wrap the
    JDBC connection and specify the batch size as an argument via the `BatchedConnection(Connection
    delegate, int batchSize)` constructor as follows (here, the batch size is set
    to 2; consider reading the comments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE403]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE404]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE405]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE406]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE407]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE408]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE409]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE410]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE411]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE412]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE413]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE414]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE415]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE416]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE417]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE418]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE419]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE420]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE421]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE422]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE423]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE424]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE425]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE426]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE427]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE428]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE429]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE430]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE431]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE432]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE433]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE434]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE435]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE436]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE437]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE438]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE439]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE440]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE441]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE442]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE443]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE444]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE445]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE446]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE447]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE448]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE449]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE450]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE451]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE452]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE453]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE454]'
  prefs: []
  type: TYPE_PRE
- en: Moreover, `BatchedConnection` implements `java.sql.Connection`, so you can use
    the entire arsenal of `Connection` methods, including methods for shaping the
    behavior of transactions. More examples are available in `Batched`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's tackle two special cases encountered in PostgreSQL and SQL Server.
  prefs: []
  type: TYPE_NORMAL
- en: Batching and fetching sequences in PostgreSQL/Oracle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you know, PostgreSQL/Oracle can rely on sequences for providing primary
    keys (and other unique values). For instance, our PostgreSQL `employee` table
    uses the following sequence for producing sequence values for `employee_number`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE455]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE456]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE457]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE458]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE459]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE460]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE461]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE462]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE463]'
  prefs: []
  type: TYPE_PRE
- en: 'But, in the context of batching, fetching the `employee` primary keys from
    the application requires a database round trip (`SELECT`) for each primary key.
    Obviously, it is a performance penalty to have a batch of *n* `INSERT` statements
    and execute *n* round trips (`SELECT` statements) just to fetch their primary
    keys. Fortunately, jOOQ leverages at least two solutions. One of them is to inline
    sequence references in SQL statements (the `EMPLOYEE_SEQ.nextval()` call):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE464]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE465]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE466]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE467]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE468]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE469]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE470]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE471]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE472]'
  prefs: []
  type: TYPE_PRE
- en: 'Another approach is to pre-fetch a number of *n* primary keys via `SELECT`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE473]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use these primary keys in batch (notice the `ids.get(n).value1()` call):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE474]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE475]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE476]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE477]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE478]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE479]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE480]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE481]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE482]'
  prefs: []
  type: TYPE_PRE
- en: Both of these examples rely on the `public static final EMPLOYEE_SEQ` field
    or, more precisely, on `jooq.generated.Sequences.EMPLOYEE_SEQ`. Mainly, the jOOQ
    Code Generator will generate a sequence object per database sequence and each
    such object has access to methods such as `nextval()`, `currval()`, `nextvals(int
    n)`, and others, which will be covered in [*Chapter 11*](B16833_11.xhtml#_idTextAnchor209),
    *jOOQ Keys*.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you rely on an auto-generated sequence from (`BIG`)`SERIAL` or
    on a sequence associated as default (for example, in the `sale` table, we have
    a sequence associated to `sale_id` as `DEFAULT NEXTVAL ('sale_seq')`), then the
    simplest way to batch is to omit the primary key field in statements, and the
    database will do the rest. The previous examples, along with many more, are available
    in *BatchInserts* for PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: SQL Server IDENTITY columns and explicit values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inserting explicit values for the SQL Server `IDENTITY` columns results in
    the error *Cannot insert explicit value for identity column in table ''table_name''
    when* `IDENTITY_INSERT` *is set to* `OFF`. Bypassing this error can be done by
    setting `IDENTITY_INSERT` to `ON` before `INSERT`. In the context of batching,
    this can be done as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE483]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE484]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE485]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE486]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE487]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE488]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE489]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE490]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE491]'
  prefs: []
  type: TYPE_PRE
- en: You can find this example in `BatchInserts` for SQL Server. Next, let's talk
    about bulking.
  prefs: []
  type: TYPE_NORMAL
- en: Bulking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Writing bulk queries in jOOQ is just a matter of using the jOOQ DSL API. For
    instance, a bulk insert SQL looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE492]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE493]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE494]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE495]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE496]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be expressed in jOOQ by chaining the `values()` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE497]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE498]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE499]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE500]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE501]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE502]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE503]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE504]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE505]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE506]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE507]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE508]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE509]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE510]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, you can use a bulk update SQL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE511]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE512]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE513]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE514]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE515]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE516]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE517]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE518]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE519]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE520]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE521]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE522]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be expressed in jOOQ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE523]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE524]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE525]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE526]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE527]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE528]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE529]'
  prefs: []
  type: TYPE_PRE
- en: More examples are available in `Bulk` for MySQL. Next, let's talk about the
    *Loader* API, which has built-in bulk support.
  prefs: []
  type: TYPE_NORMAL
- en: Loading (the Loader API)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we need to load (import) our database tables with data coming from
    different sources (CSV, JSON, and so on), we can rely on the jOOQ Loader API (`org.jooq.Loader`).
    This is a fluent API that allows us to smoothly tackle the most important challenges,
    such as handling duplicate keys, bulking, batching, committing, and error handling.
  prefs: []
  type: TYPE_NORMAL
- en: The Loader API syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, we have a file containing the data to be imported in a common format
    such as CSV or JSON, and we customize the Loader API general syntax to fit our
    needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE530]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE531]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE532]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE533]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE534]'
  prefs: []
  type: TYPE_PRE
- en: While `TARGET_TABLE` is obviously the table in which the data should be imported,
    let's see what options we have.
  prefs: []
  type: TYPE_NORMAL
- en: Options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can mainly distinguish between three types of options that can be used for
    customizing the import process: options for handling duplicate keys, throttling
    options, and options for handling failures (errors). The following diagram highlights
    each category of options and the valid paths that can be used for chaining these
    options fluently:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The Loader API options'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16833_Figure_10.3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – The Loader API options
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore each of these categories, starting with the one for tackling duplicate
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate keys options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A duplicate key occurs when a unique key exists in the table and we attempt
    to import a record having the same key. By unique key, jOOQ means any unique key,
    not only primary keys.
  prefs: []
  type: TYPE_NORMAL
- en: So, handling duplicate keys can be done via `onDuplicateKeyError()`, which is
    the default, or via `onDuplicateKeyIgnore()` or `onDuplicateKeyUpdate()`. The
    default behavior throws an exception if there are any duplicate keys.
  prefs: []
  type: TYPE_NORMAL
- en: By explicitly using `onDuplicateKeyIgnore()`, we instruct jOOQ to skip any duplicate
    key without throwing an exception (this is the synthetic `ON DUPLICATE KEY IGNORE`
    clause, which can be emulated by jOOQ depending on dialect). We can instruct jOOQ
    to execute `UPDATE` instead of `INSERT` via `onDuplicateKeyUpdate()` (this is
    the synthetic `ON DUPLICATE KEY UPDATE` clause, which can be emulated by jOOQ
    depending on dialect).
  prefs: []
  type: TYPE_NORMAL
- en: Throttling options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three throttling options that can be used to fine-tune the import.
    These options refer to bulking, batching, and committing. jOOQ allows us to explicitly
    use any combination of these options or to rely on the following defaults: no
    bulking, batching, and committing.'
  prefs: []
  type: TYPE_NORMAL
- en: Bulking can be set via `bulkNone()` (which is the default and means that no
    bulking will be used), `bulkAfter(int rows)` (which allows us to specify how many
    rows will be inserted in one bulk via a multi-row `INSERT` (`insert into ... (...)
    values (?, ?, ?,...), (?, ?, ?,...), (?, ?, ?,...), ...`), and `bulkAll()` (which
    attempts to create one bulk from the entire source of data).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 10.3*, `bulkNone()` is the only one that can be
    chained after all options used for handling duplicate values. The `bulkAfter()`
    and `bulkAll()` methods can be chained only after `onDuplicateKeyError()`. Moreover,
    `bulkNone()`, `bulkAfter()`, and `bulkAll()` are mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Batching can be avoided via the default `batchNone()`, or it can be explicitly
    set via `batchAfter(int bulk)` or `batchAll()`. Explicitly specifying the number
    of bulk statements that should be sent to the server as a single JDBC batch statement
    can be accomplished via `batchAfter(int bulk)`. On the other hand, sending a single
    batch containing all bulks can be done via `batchAll()`. If bulking is not used
    (`bulkNone()`) then it is as if each row represents a bulk, so, for instance,
    `batchAfter(3)` means to create batches of three rows each.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 10.3*, `batchNone()`, `batchAfter()`, and `batchAll()`
    are mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, committing data to the database can be controlled via four dedicated
    methods. By default, `commitNone()` leaves committing and rolling back operations
    up to client code (for instance, via `commitNone()`, we can allow Spring Boot
    to handle commit and rollback). But, if we want to commit after a certain number
    of batches, then we have to use `commitAfter(int batches)` or the handy `commitEach()`
    method, which is equivalent to `commitAfter(1)`. And, if we decide to commit all
    batches at once, then we need `commitAll()`. If batching is not used (relying
    on `batchNone()`), then it is as if each batch is a bulk, (for instance, `commitAfter(3)`
    means to commit after every three bulks). If bulking is not used either (relying
    on `bulkNone()`), then it is as if each bulk is a row (for instance, `commitAfter(3)`
    means to commit after every three rows).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 10.3*, `commitNone()`, `commitAfter()`, `commitEach()`,
    and `commitAll()` are mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Error options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attempting to manipulate (import) large amounts of data is a process quite prone
    to errors. While some of the errors are fatal and should stop the importing process,
    others can be safely ignored or postponed to be resolved after import. In the
    case of fatal errors, the Loader API relies on a method named `onErrorAbort()`.
    If an error occurs, then the Loader API stops the import process. On the other
    hand, we have `onErrorIgnore()`, which instructs the Loader API to skip any insert
    that caused an error and try to execute the next one.
  prefs: []
  type: TYPE_NORMAL
- en: Special cases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While finding the optimal combination of these options is a matter of benchmarking,
    there are several things that you should know, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If there are no unique keys in our table then `onDuplicateKeyUpdate()` acts
    exactly as `onDuplicateKeyIgnore()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `bulkAll()` + `commitEach()` or `bulkAll()` + `commitAfter()` is used, then
    jOOQ forces the usage of `commitAll()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `batchAll()` + `commitEach()` or `batchAll()` + `commitAfter()` is used,
    then jOOQ forces the usage of `commitAll()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's quickly cover the supported sources of data.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Providing the source of data can be accomplished via dedicated methods that
    are specific to the supported different data types. For instance, if the data
    source is a CSV file, then we rely on the `loadCSV()` method; if it is a JSON
    file, then we rely on the `loadJSON()` method; and if it is an XML file, then
    we rely on `loadXML()`. Moreover, we can import arrays via `loadArrays()` and
    jOOQ `Record`s via `loadRecords()`.
  prefs: []
  type: TYPE_NORMAL
- en: The `loadCSV()`, `loadJSON()`, and `loadXML()` methods come in 10+ flavors that
    allow us to load data from `String`, `File`, `InputStream`, and `Reader`. On the
    other hand, `loadArrays()` and `loadRecords()` allow us to load data from an array,
    `Iterable`, `Iterator`, or `Stream`.
  prefs: []
  type: TYPE_NORMAL
- en: Listeners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Loader API comes with import listeners to be chained for keeping track of
    import progress. We mainly have `onRowStart(LoaderRowListener listener)` and `onRowEnd``(LoaderRowListener
    listener)`. The former specifies a listener to be invoked before processing the
    current row, while the latter specifies a listener to be invoked after processing
    the current row. `LoaderRowListener` is a functional interface.
  prefs: []
  type: TYPE_NORMAL
- en: Execution and error handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the Loader API is executed, we have access to meaningful feedback that
    is available through the returned `org.jooq.Loader`. For instance, we can find
    out the number of executed bulks/batches via the `executed()` method, the number
    of processed rows via the `processed()` method, the number of stored rows (`INSERT`/`UPDATE`)
    via the `stored()` method, the number of ignored rows (caused by errors or duplicate
    keys) via the `ignored()` method, and the potential errors via the `errors()`
    method as `List<LoaderError>`. As you'll see in the next section of examples,
    `LoaderError` contains details about the errors (if any).
  prefs: []
  type: TYPE_NORMAL
- en: Examples of using the Loader API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, after all this theory, it is time to see some examples of loading
    CSV, JSON, `Record`, and arrays. All these examples are executed and dissected
    in the context of Spring Boot `@Transactional`. Feel free to practice them under
    the jOOQ transactional context by simply removing `@Transactional` and wrapping
    the code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE535]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE536]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE537]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE538]'
  prefs: []
  type: TYPE_PRE
- en: So, let's start by loading some CSV.
  prefs: []
  type: TYPE_NORMAL
- en: Loading CSV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading CSV is accomplished via the `loadCSV()` method. Let''s start with a
    simple example based on the following typical CSV file (`in.csv`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE539]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE540]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE541]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE542]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE543]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, this data should be imported in the `sale` table, so `TARGET_TABLE`
    (`Table<R>`) that should be passed to `loadInto()` is `SALE`. Pointing jOOQ to
    this file is accomplished via the `loadCSV()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE544]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE545]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE546]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE547]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE548]'
  prefs: []
  type: TYPE_PRE
- en: This code relies on the default options. Notice the call of the `fieldsCorresponding()`
    method. This method signals to jOOQ that all input fields having a corresponding
    field in `SALE` (with the same name) should be loaded. Practically, in this case,
    all fields from the CSV file have a correspondent in the `SALE` table, so all
    of them will be imported.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, obviously, this is not always the case. Maybe we want to load only a subset
    of scattered fields. In such cases, simply pass *dummy* nulls for the field indexes
    (positions) that shouldn''t be loaded (this is an index/position-based field mapping).
    This time, let''s collect the number of processed rows as well via `processed()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE549]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE550]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE551]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE552]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE553]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE554]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE555]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE556]'
  prefs: []
  type: TYPE_PRE
- en: 'This code loads from CSV only `SALE.FISCAL_YEAR`, `SALE.SALE_`, `SALE.FISCAL_MONTH`,
    `SALE.REVENUE_GROWTH`, and `SALE.TREND`. Notice that we''ve used the `fields()`
    method instead of `fieldsCorresponding()`, since `fields()` allows us to keep
    only the desired fields and skip the rest. A sample of the resultant `INSERT`
    (in MySQL dialect) looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE557]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE558]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE559]'
  prefs: []
  type: TYPE_PRE
- en: 'While this CSV file is a typical one (first line header, data separated by
    a comma, and so on), sometimes we have to deal with CSV files that are quite customized,
    such as the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE560]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE561]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE562]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE563]'
  prefs: []
  type: TYPE_PRE
- en: 'This CSV file contains the same data as the previous one expect that there
    is no header line, the data separator is `|`, the quote mark is `*`, and the `null`
    values are represented as `{null}`. Loading this CSV file into `SALE` requires
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE564]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE565]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE566]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE567]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE568]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE569]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE570]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE571]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE572]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE573]'
  prefs: []
  type: TYPE_PRE
- en: First of all, since there is no header, we rely on `fields()` to explicitly
    specify the list of fields (`SALE_ID` is mapped to index `1` in CSV, `FISCAL_YEAR`
    to index `2`, and so on). Next, we call `ignoreRows(0)`; by default, jOOQ skips
    the first line, which is considered the header of the CSV file, but since there
    is no header in this case, we have to instruct jOOQ to take into account the first
    line as a line containing data. Obviously, this method is useful for skipping
    *n* rows as well. Taking it a step further, we call `separator()`, `nullString()`,
    and `quote()` to override the defaults. Finally, we call `errors()` and collect
    potential errors in `List<LoaderError>`. This is an optional step and is not related
    to this particular example. In the bundled code (*LoadCSV* for MySQL), you can
    see how to loop this list and extract valuable information about what happened
    during the loading process. Moreover, you'll see more examples of loading CSV
    files. Next, let's explore more examples for loading a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: Loading JSON
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading JSON is done via the `loadJSON()` method. Let''s start with a JSON
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE574]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE575]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE576]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE577]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE578]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE579]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE580]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE581]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE582]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE583]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE584]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE585]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE586]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE587]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE588]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE589]'
  prefs: []
  type: TYPE_PRE
- en: 'This JSON file was previously exported via `formatJSON()`. Notice the `"fields"`
    header, which is useful for loading this file into the `SALE` table via the mapping
    provided by the `fieldsCorresponding()` method. Without a header, the `fieldsCorresponding()`
    method cannot produce the expected results since the input fields are missing.
    But, if we rely on the `fields()` method, then we can list the desired fields
    (all or a subset of them) and count on index-based mapping without worrying about
    the presence or absence of the `"fields"` header. Moreover, this time, let''s
    add an `onRowEnd()` listener as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE590]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE591]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE592]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE593]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE594]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE595]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE596]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE597]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE598]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE599]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE600]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE601]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE602]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE603]'
  prefs: []
  type: TYPE_PRE
- en: 'After each row is processed you''ll see an output in the log as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE604]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE605]'
  prefs: []
  type: TYPE_PRE
- en: 'But, let''s look at a JSON file without the `"fields"` header, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE606]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE607]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE608]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE609]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE610]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE611]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE612]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE613]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE614]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE615]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE616]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE617]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE618]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE619]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE620]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE621]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE622]'
  prefs: []
  type: TYPE_PRE
- en: 'This kind of JSON can be loaded via `fieldsCorresponding()` or via `fields()`.
    Since the field names are available as JSON keys, the `fieldsCorresponding()`
    method maps them correctly. Using `fields()` should be done by keeping in mind
    the order of keys in this JSON. So, `"fiscal_month"` is on index `1`, `"revenue_growth"`
    on index `2`, and so on. Here is an example that loads only `"fiscal_month"`,
    `"revenue_growth"`, `"sale"`, `"fiscal_year"`, and `"employee_number"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE623]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE624]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE625]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE626]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE627]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE628]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE629]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE630]'
  prefs: []
  type: TYPE_PRE
- en: 'But, sometimes, the missing data is in JSON itself, as here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE631]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE632]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE633]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE634]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE635]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE636]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE637]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE638]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE639]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE640]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE641]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE642]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE643]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE644]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE645]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE646]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE647]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE648]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, in both cases, we must rely on `fields()`, as here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE649]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE650]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE651]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE652]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE653]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE654]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s assume that we have a JSON file that should be imported into the
    database using batches of size `2` (rows), so we need `batchAfter(2)`. The commit
    (as in all the previous examples) will be accomplished by Spring Boot via `@Transactional`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE655]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE656]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE657]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE658]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE659]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE660]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE661]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE662]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE663]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE664]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE665]'
  prefs: []
  type: TYPE_PRE
- en: Since `commitNone()` is the default behavior, it could be omitted. Essentially,
    `commitNone()` allows `@Transactional` to handle the commit/rollback actions.
    By default, `@Transactional` commits the transaction at the end of the annotated
    method. If something goes wrong, the entire payload (all batches) is rolled back.
    But, if you remove `@Transactional`, then `auto-commit` `=true` goes into action.
    This commits after each batch (so, after every two rows). If something goes wrong,
    then there is no rollback action, but the loading process is aborted immediately
    since we rely on the default settings, `onDuplicateKeyError()` and `onErrorAbort()`.
    If we remove `@Transactional` and set `auto-commit` to `false` (`spring.datasource.hikari.auto-commit=false`),
    then nothing commits.
  prefs: []
  type: TYPE_NORMAL
- en: This example returns the number of executed batches via `executed()`. For instance,
    if there are 36 rows processed with `batchAfter(2)`, then `executed()` returns
    18\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s consider a JSON file that contains duplicate keys. Every time
    a duplicate key is found, the Loader API should skip it, and, in the end, it should
    report the number of ignored rows. Moreover, the Loader API should commit after
    each batch of three rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE666]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE667]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE668]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE669]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE670]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE671]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE672]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE673]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE674]'
  prefs: []
  type: TYPE_PRE
- en: If you want to execute `UPDATE` instead of ignoring duplicate keys, just replace
    `onDuplicateKeyIgnore()` with `onDuplicateKeyUpdate()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s import a JSON using `bulkAfter(2)`, `batchAfter(3)`, and `commitAfter(3)`.
    In other words, each bulk has two rows, and each batch has three bulks. Therefore,
    six rows commit after three batches, that is nine bulks, so after 18 rows, you
    get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE675]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE676]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE677]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE678]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE679]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE680]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE681]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE682]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE683]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE684]'
  prefs: []
  type: TYPE_PRE
- en: If something goes wrong, the last uncommitted batch is rolled back without affecting
    the already committed batches. More examples are available in the bundled code,
    `LoadJSON`, for MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: Loading records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading jOOQ `Record` via the Loader API is a straightforward process accomplished
    via the `loadRecords()` method. Let''s consider the following set of records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE685]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE686]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE687]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE688]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE689]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE690]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading them can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE691]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE692]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE693]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE694]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE695]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE696]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE697]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE698]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE699]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE700]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE701]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE702]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at loading the following map of `Record`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE703]'
  prefs: []
  type: TYPE_PRE
- en: 'So, `CustomerRecord` should be loaded in `CUSTOMER`, and `CustomerdetailRecord`
    should be loaded in `CUSTOMERDETAIL`. For this, we can use `Map.keySet()` and
    `Map.values()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE704]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE705]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE706]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE707]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE708]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE709]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE710]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE711]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE712]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE713]'
  prefs: []
  type: TYPE_PRE
- en: More examples are available in the bundled code, `LoadRecords`, for MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: Loading arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading arrays is accomplished via the `loadArrays()` method. Let''s consider
    the following array containing data that should be loaded into the `SALE` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE714]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading this array can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE715]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE716]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE717]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE718]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE719]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE720]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is another example that relies on `loadArrays(Object[]... os)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE721]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE722]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE723]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE724]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE725]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE726]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE727]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE728]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE729]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE730]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE731]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE732]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE733]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE734]'
  prefs: []
  type: TYPE_PRE
- en: You can check out these examples next to others not listed here in the bundled
    code, `LoadArrays`, for MySQL. It is time to summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve covered four important topics: exporting, batching,
    bulking, and loading. As you saw, jOOQ comes with dedicated APIs for accomplishing
    each of these tasks that require a lot of complex code under the hood. Frequently,
    jOOQ simplifies the complexity (as usual) and allows us to focus on what we have
    to do and less on how we do it. For instance, it is amazing to see that it takes
    seconds to write a snippet of code for loading a CSV or a JSON file into the database
    while having fluent and smooth support for error handling control, diagnosis output,
    bulking, batching, and committing control.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the jOOQ keys.
  prefs: []
  type: TYPE_NORMAL
