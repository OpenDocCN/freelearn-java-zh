["```java\nY = f(x) \n```", "```java\nC:\\Users\\Ilango\\Documents\\Packt-Book-Writing-Project\\DevProjects\\Chapter1>echo %JAVA_HOME%\nC:\\Program Files\\Java\\jdk1.8.0_102\n```", "```java\nC:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter1>java -version\njava version \"1.8.0_131\"\nJava(TM) SE Runtime Environment (build 1.8.0_131-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)\n\nC:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter1>javac -version\njavac 1.8.0_102\n```", "```java\nC:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter1>scala -version\nScala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL\n```", "```java\nC:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter1>sbt sbtVersion\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0\n[info] Loading project definition from C:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter1\\project\n[info] Set current project to Chapter1 (in build file:/C:/Users/Ilango/Documents/Packt/DevProjects/Chapter1/)\n[info] 0.13.17\n```", "```java\nspark-shell --master local[2]\n```", "```java\nwinutils.exe chmod -R 777 C:\\tmp\\hive\n```", "```java\nC:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter1>spark-shell --master local[2]\nSpark context Web UI available at http://192.168.56.1:4040\nSpark context available as 'sc' (master = local[2], app id = local-1520484594646).\nSpark session available as 'spark'.\nWelcome to\n ____ __\n / __/__ ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/ '_/\n /___/ .__/\\_,_/_/ /_/\\_\\ version 2.2.1\n/_/\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_102)\nType in expressions to have them evaluated.\nType :help for more information.\nscala>\n```", "```java\nscala> import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.SparkSession\n\nscala> lazy val session: SparkSession = SparkSession.builder().getOrCreate()\nres7: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f68756d\n```", "```java\nscala> spark\nres21: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f68756d\n```", "```java\nscala> sc\nres5: org.apache.spark.SparkContext = org.apache.spark.SparkContext@553ce348\n```", "```java\nscala> sc.version\nres2: String = 2.2.1\nscala> spark\nres3: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f68756d\n```", "```java\nscala> sc.getConf.getAll\nres17: Array[(String, String)] = Array((spark.driver.port,51576), (spark.debug.maxToStringFields,25), (spark.jars,\"\"), (spark.repl.class.outputDir,C:\\Users\\Ilango\\AppData\\Local\\Temp\\spark-47fee33b-4c60-49d0-93aa-3e3242bee7a3\\repl-e5a1acbd-6eb9-4183-8c10-656ac22f71c2), (spark.executor.id,driver), (spark.submit.deployMode,client), (spark.driver.host,192.168.56.1), (spark.app.id,local-1520484594646), (spark.master,local[2]), (spark.home,C:\\spark-2.2.1-bin-hadoop2.7\\bin\\..))\n```", "```java\nscala> sc.getConf.setAppName(\"Iris-Pipeline\")\nres22: org.apache.spark.SparkConf = org.apache.spark.SparkConf@e8ce5b1\n```", "```java\nscala> sc.conf.getAll\nres20: Array[(String, String)] = Array((spark.driver.port,51576), (spark.app.name,Spark shell), (spark.sql.catalogImplementation,hive), (spark.repl.class.uri,spark://192.168.56.1:51576/classes), (spark.debug.maxToStringFields,150), (spark.jars,\"\"), (spark.repl.class.outputDir,C:\\Users\\Ilango\\AppData\\Local\\Temp\\spark-47fee33b-4c60-49d0-93aa-3e3242bee7a3\\repl-e5a1acbd-6eb9-4183-8c10-656ac22f71c2), (spark.executor.id,driver), (spark.submit.deployMode,client), (spark.driver.host,192.168.56.1), (spark.app.id,local-1520484594646), (spark.master,local[2]), (spark.home,C:\\spark-2.2.1-bin-hadoop2.7\\bin\\..))\n```", "```java\nscala> val dfReader1 = spark.read\ndfReader1: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c\n```", "```java\nscala> val dfReader2 = dfReader1.format(\"com.databricks.spark.csv\")\ndfReader2: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c\n```", "```java\nscala> val dfReader3 = dfReader2.option(\"header\", true)\ndfReader3: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c\n```", "```java\nscala> val dfReader4 = dfReader3.option(\"inferSchema\",true)\ndfReader4: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c\n```", "```java\nscala> val dFrame = dfReader4.load(\"iris.csv\")\ndFrame: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields]\n```", "```java\nscala> dFrame.count\nres1: Long = 150\n```", "```java\nscala> val irisDataFrame = spark.read.format(\"com.databricks.spark.csv\").option(\"header\",true).option(\"inferSchema\", true).load(\"iris.csv\").show\n\n```", "```java\nspark-shell --master local[2]\n```", "```java\nval dfReader1 = spark.read\ndfReader1: org.apache.spark.sql.DataFrameReader=org.apache.spark.sql.DataFrameReader@6980d3b3\n```", "```java\nval dfReader2 = dfReader1.format(\"com.databricks.spark.csv\")\ndfReader2: org.apache.spark.sql.DataFrameReader=org.apache.spark.sql.DataFrameReader@6980d3b3\n```", "```java\nval dfReader3 = dfReader2.option(\"header\", true)\n```", "```java\nval dfReader4 = dfReader3.option(\"inferSchema\", true)\n```", "```java\nval dFrame1 = dfReader4.load(\"iris.csv\")\ndFrame1: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields]\n```", "```java\ndFrame1.describe(\"Id\",\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Species\")\nWARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\nres16: org.apache.spark.sql.DataFrame = [summary: string, Id: string ... 5 more fields]\n```", "```java\nsc.getConf.getAll\nres4: Array[(String, String)] = Array((spark.repl.class.outputDir,C:\\Users\\Ilango\\AppData\\Local\\Temp\\spark-10e24781-9aa8-495c-a8cc-afe121f8252a\\repl-c8ccc3f3-62ee-46c7-a1f8-d458019fa05f), (spark.app.name,Spark shell), (spark.sql.catalogImplementation,hive), (spark.driver.port,58009), (spark.debug.maxToStringFields,150),\n```", "```java\nval dFrame2 =  dFrame1.describe(\"Id\",\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Species\"\n)\ndFrame2: org.apache.spark.sql.DataFrame = [summary: string, Id: string ... 5 more fields]\n```", "```java\nval dFrame2Display= = dfReader2.show\n```", "```java\nval dfReader = spark.read.format(\"com.databricks.spark.csv\").option(\"header\",true).option(\"inferSchema\",true).load(\"iris.csv\")\ndfReader: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields] \n```", "```java\nimport org.apache.spark.sql.SparkSession\n```", "```java\nlazy val session: SparkSession = SparkSession.builder().getOrCreate()\n```", "```java\nval dataSetPath = \"<<path to folder containing your iris.csv file>>\\\\iris.csv\"\n```", "```java\ndef buildDataFrame(dataSet: String): DataFrame = {\n/*\n The following is an example of a dataSet parameter string: \"C:\\\\Your\\\\Path\\\\To\\\\iris.csv\"\n*/\n```", "```java\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n```", "```java\nval result1: Array[String] = session.sparkContext.textFile(<<path to iris.csv represented by the dataSetPath variable>>)\n```", "```java\n//Each line in the RDD is a row in the Dataset represented by a String, which we can 'split' along the new //line character\nval result2: RDD[String] = result1.flatMap { partition => partition.split(\"\\n\").toList }\n\n//the second transformation operation involves a split inside of each line in the dataset where there is a //comma separating each element of that line\nval result3: RDD[Array[String]] = result2.map(_.split(\",\"))\n```", "```java\nval result4: Array[Array[String]] = result3.collect.drop(1)\n```", "```java\nimport org.apache.spark.ml.linalg.Vectors\n```", "```java\nval result5 = result4.map(row => (Vectors.dense(row(1).toDouble, row(2).toDouble, row(3).toDouble, row(4).toDouble),row(5)))\n```", "```java\nval dataFrame = spark.createDataFrame(result5).toDF(featureVector, speciesLabel)\n```", "```java\ndataFrame.show\n+--------------------+-------------------------+\n|iris-features-column|iris-species-label-column|\n+--------------------+-------------------------+\n| [5.1,3.5,1.4,0.2]| Iris-setosa|\n| [4.9,3.0,1.4,0.2]| Iris-setosa|\n| [4.7,3.2,1.3,0.2]| Iris-setosa|\n.....................\n.....................\n+--------------------+-------------------------+\nonly showing top 20 rows\n```", "```java\nobject IrisPipeline extends IrisWrapper { \n```", "```java\nimport org.apache.spark.ml.feature.StringIndexer\n```", "```java\nval indexer = new StringIndexer().setInputCol\n(irisFeatures_CategoryOrSpecies_IndexedLabel._2).setOutputCol(irisFeatures_CategoryOrSpecies_IndexedLabel._3)\n```", "```java\nval splitDataSet: Array[org.apache.spark.sql.Dataset\n[org.apache.spark.sql.Row]] = dataSet.randomSplit(Array(0.85, 0.15), 98765L)\n```", "```java\nsplitDataset.size\nres48: Int = 2\n```", "```java\nval trainDataSet = splitDataSet(0)\ntrainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iris-features-column: vector, iris-species-label-column: string]\n```", "```java\nval testDataSet = splitDataSet(1)\ntestSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iris-features-column: vector, iris-species-label-column: string]\n```", "```java\ntrainSet.count\nres12: Long = 14\n```", "```java\ntestSet.count\nres9: Long = 136\n```", "```java\nval randomForestClassifier = new RandomForestClassifier()\n  .setFeaturesCol(irisFeatures_CategoryOrSpecies_IndexedLabel._1)\n  .setFeatureSubsetStrategy(\"sqrt\")\n```", "```java\nval irisPipeline = new Pipeline().setStages(Array[PipelineStage](indexer) ++  Array[PipelineStage](randomForestClassifier))\n```", "```java\nval finalParamGrid: Array[ParamMap] = gridBuilder3.build()\n```", "```java\nval validatedTestResults: DataFrame = new TrainValidationSplit()\n```", "```java\nval validatedTestResults: DataFrame = new TrainValidationSplit().setSeed(1234567L).setEstimator(irisPipeline)\n```", "```java\nval validatedTestResults.setEvaluator(new MulticlassClassificationEvaluator())\n```", "```java\n--------+\n |iris-features-column|iris-species-column|label| rawPrediction| probability|prediction|\n +--------------------+-------------------+-----+--------------------+\n | [4.4,3.2,1.3,0.2]| Iris-setosa| 0.0| [40.0,0.0,0.0]| [1.0,0.0,0.0]| 0.0|\n | [5.4,3.9,1.3,0.4]| Iris-setosa| 0.0| [40.0,0.0,0.0]| [1.0,0.0,0.0]| 0.0|\n | [5.4,3.9,1.7,0.4]| Iris-setosa| 0.0| [40.0,0.0,0.0]| [1.0,0.0,0.0]| 0.0|\n```", "```java\nval validatedTestResultsDataset:DataFrame = validatedTestResults.select(\"prediction\", \"label\")\n```", "```java\nval modelOutputAccuracy: Double = new MulticlassClassificationEvaluator()\n```", "```java\nval multiClassMetrics = new MulticlassMetrics(validatedRDD2)\n```", "```java\nval accuracyMetrics = (multiClassMetrics.accuracy, multiClassMetrics.weightedPrecision)\n```", "```java\nval accuracy = accuracyMetrics._1\n```", "```java\n\nval weightedPrecsion = accuracyMetrics._2\n```", "```java\nsbt console\nscala>\nimport com.packt.modern.chapter1.IrisPipeline\nIrisPipeline.main(Array(\"iris\")\nAccuracy (precision) is 0.9285714285714286 Weighted Precision is: 0.9428571428571428\n```", "```java\nsbt package\n```"]