- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book does not blindly praise microservices. Instead, it’s about how we
    can use their benefits while being able to handle the challenges of building scalable,
    resilient, and manageable microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an introduction to this book, the following topics will be covered in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: My way into microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a microservice-based architecture?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges with microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design patterns for handling challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software enablers that can help us handle these challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other important considerations that aren’t covered in this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No installations are required for this chapter. However, you may be interested
    in taking a look at the C4 model conventions, [https://c4model.com](https://c4model.com),
    since the illustrations in this chapter are inspired by the C4 model.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter does not contain any source code.
  prefs: []
  type: TYPE_NORMAL
- en: My way into microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I first learned about the concept of microservices back in 2014, I realized
    that I had been developing microservices (well, kind of) for a number of years
    without knowing it was microservices I was dealing with. I was involved in a project
    that started in 2009 where we developed a platform based on a set of separated
    features. The platform was delivered to a number of customers that deployed it
    on-premises. To make it easy for customers to pick and choose what features they
    wanted to use from the platform, each feature was developed as an **autonomous
    software component**; that is, it had its own persistent data and only communicated
    with other components using well-defined APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since I can’t discuss specific features in this project’s platform, I have
    generalized the names of the components, which are labeled from **Component A**
    to **Component F**. The **composition** of the platform as a set of components
    is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: The composition of the platform'
  prefs: []
  type: TYPE_NORMAL
- en: From the illustration, we can also see that each component has its own storage
    for persistent data, and is not sharing databases with other components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each component is developed using Java and the Spring Framework, packaged as
    a WAR file, and deployed as a web app in a Java EE web container, for example,
    Apache Tomcat. Depending on the customer’s specific requirements, the platform
    can be deployed on single or multiple servers. A two-node deployment may look
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: A two-node deployment scenario'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of autonomous software components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From this project, I learned that decomposing the platform’s functionality
    into a set of autonomous software components provides a number of benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: A customer can deploy parts of the platform in its own system landscape, integrating
    it with its existing systems using its well-defined APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example where one customer decided to deploy **Component
    A**, **Component B**, **Component D**, and **Component E** from the platform and
    integrate them with two existing systems in the customer’s system landscape, **System
    A** and **System B**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Partial deployment of the platform'
  prefs: []
  type: TYPE_NORMAL
- en: Another customer could choose to replace parts of the platform’s functionality
    with implementations that already exist in the customer’s system landscape, potentially
    requiring some adoption of the existing functionality in the platform’s APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example where a customer has replaced **Component C** and
    **Component F** in the platform with their own implementation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Replacing parts of the platform'
  prefs: []
  type: TYPE_NORMAL
- en: Each component in the platform can be delivered and upgraded separately. Thanks
    to the use of well-defined APIs, one component can be upgraded to a new version
    without being dependent on the life cycle of the other components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example where **Component A** has been upgraded from version
    **v1.1** to **v1.2**. **Component B**, which calls **Component A**, does not need
    to be upgraded since it uses a well-defined API; that is, it’s still the same
    after the upgrade (or it’s at least backward-compatible):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B19825_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Upgrading a specific component'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the use of well-defined APIs, each component in the platform can
    also be scaled out to multiple servers independently of the other components.
    Scaling can be done either to meet high availability requirements or to handle
    higher volumes of requests. In this specific project, it was achieved by *manually*
    setting up load balancers in front of a number of servers, each running a Java
    EE web container. An example where **Component A** has been scaled out to three
    instances looks as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19825_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Scaling out the platform'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with autonomous software components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'My team also learned that decomposing the platform introduced a number of new
    challenges that we were not exposed to (at least not to the same degree) when
    developing more traditional, monolithic applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding new instances to a component required manually configuring load balancers
    and manually setting up new nodes. This work was both time-consuming and error-prone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The platform was initially prone to errors caused by the other systems it was
    communicating with. If a system stopped responding to requests that were sent
    from the platform in a timely fashion, the platform quickly ran out of crucial
    resources, for example, OS threads, specifically when exposed to a large number
    of concurrent requests. This caused components in the platform to hang or even
    crash. Since most of the communication in the platform is based on synchronous
    communication, one component crashing can lead to cascading failures; that is,
    clients of the crashing components could also crash after a while. This is known
    as a **chain of failures**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping the configuration in all the instances of the components consistent
    and up to date quickly became a problem, causing a lot of manual and repetitive
    work. This led to quality problems from time to time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the state of the platform in terms of latency issues and hardware
    usage (for example, usage of CPU, memory, disks, and the network) was more complicated
    compared to monitoring a single instance of a monolithic application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting log files from a number of distributed components and correlating
    related log events from the components was also difficult, but feasible since
    the number of components was fixed and known in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over time, we addressed most of the challenges that were mentioned in the preceding
    list with a mix of in-house-developed tools and well-documented instructions for
    handling these challenges manually. The scale of the operation was, in general,
    at a level where manual procedures for releasing new versions of the components
    and handling runtime issues were acceptable, even though they were not desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Enter microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning about microservice-based architectures in 2014 made me realize that
    other projects had also been struggling with similar challenges (partly for other
    reasons than the ones I described earlier, for example, the large cloud service
    providers meeting web-scale requirements). Many microservice pioneers had published
    details of lessons they’d learned. It was very interesting to learn from these
    lessons.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the pioneers initially developed monolithic applications that made them
    very successful from a business perspective. But over time, these monolithic applications
    became more and more difficult to maintain and evolve. They also became challenging
    to scale beyond the capabilities of the largest machines available (also known
    as **vertical scaling**). Eventually, the pioneers started to find ways to split
    monolithic applications into smaller components that could be released and scaled
    independently of each other. Scaling small components can be done using **horizontal
    scaling**, that is, deploying a component on a number of smaller servers and placing
    a load balancer in front of it. If done in the cloud, the scaling capability is
    potentially endless – it is just a matter of how many virtual servers you bring
    in (given that your component can scale out on a huge number of instances, but
    more on that later on).
  prefs: []
  type: TYPE_NORMAL
- en: In 2014, I also learned about a number of new open source projects that delivered
    tools and frameworks that simplified the development of microservices and could
    be used to handle the challenges that come with a microservice-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of these are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pivotal released **Spring Cloud**, which wraps parts of the **Netflix OSS**
    in order to provide capabilities such as dynamic service discovery, configuration
    management, distributed tracing, circuit breaking, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I also learned about **Docker** and the container revolution, which is great
    for minimizing the gap between development and production. Being able to package
    a component not only as a deployable runtime artifact (for example, a Java `war`
    or `jar` file) but as a complete image, ready to be launched as a container on
    a server running Docker, was a great step forward for development and testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For now, think of a container as an isolated process. We will learn more about
    containers in *Chapter 4*, *Deploying Our Microservices Using Docker*.
  prefs: []
  type: TYPE_NORMAL
- en: A container engine, such as Docker, is not enough to be able to use containers
    in a production environment. Something is needed that can ensure that all the
    containers are up and running and that can scale out containers on a number of
    servers, thereby providing high availability and increased compute resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These types of products became known as **container orchestrators**. A number
    of products have evolved over the last few years, such as Apache Mesos, Docker
    in Swarm mode, Amazon ECS, HashiCorp Nomad, and **Kubernetes**. Kubernetes was
    initially developed by Google. When Google released v1.0 in 2015, it also donated
    Kubernetes to **CNCF** ([https://www.cncf.io/](https://www.cncf.io/)). During
    2018, Kubernetes became kind of a de facto standard, available both pre-packaged
    for on-premises use and as a service from most of the major cloud providers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As explained in [https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/),
    Kubernetes is actually an open source-based rewrite of an internal container orchestrator,
    named **Borg**, used by Google for more than a decade before the Kubernetes project
    was founded.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, I started to learn about the concept of a **service mesh** and how
    a service mesh can complement a container orchestrator to further offload microservices
    from responsibilities to make them manageable and resilient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample microservice landscape
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this book can’t cover all aspects of the technologies I just mentioned,
    I will focus on the parts that have proven to be useful in customer projects I
    have been involved in since 2014\. I will describe how they can be used together
    to create cooperating microservices that are manageable, scalable, and resilient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each chapter in this book will address a specific concern. To demonstrate how
    things fit together, I will use a small set of cooperating microservices that
    we will evolve throughout this book. The microservice landscape will be described
    in *Chapter 3*, *Creating a Set of Cooperating Microservices*; for now, it is
    sufficient to know that it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: The microservice-based system landscape used in the book'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a very small system landscape of cooperating microservices.
    The surrounding support services that we will add in the coming chapters might
    look overwhelmingly complex for these few microservices. But keep in mind that
    the solutions presented in this book aim to support a much larger system landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have been introduced to the potential benefits and challenges of
    microservices, let’s start to look into how a microservice can be defined.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A microservice architecture is about splitting up monolithic applications into
    smaller components, which achieves two major goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Faster development, enabling continuous deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easier to scale, manually or automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A microservice is essentially an autonomous software component that is independently
    upgradeable, replaceable, and scalable. To be able to act as an autonomous component,
    it must fulfill certain criteria, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It must conform to a shared-nothing architecture; that is, microservices don’t
    share data in databases with each other!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must only communicate through well-defined interfaces, either using APIs
    and synchronous services or preferably by sending messages asynchronously. The
    APIs and message formats used must be stable, well documented, and evolve by following
    a defined versioning strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be deployed as separate runtime processes. Each instance of a microservice
    runs in a separate runtime process, for example, a Docker container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservice instances are stateless so that incoming requests to a microservice
    can be handled by any of its instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a set of cooperating microservices, we can deploy to a number of smaller
    servers instead of being forced to deploy to a single big server, like we have
    to do when deploying a monolithic application.
  prefs: []
  type: TYPE_NORMAL
- en: Given that the preceding criteria have been fulfilled, it is easier to scale
    up a single microservice into more instances (for example, using more virtual
    servers) compared to scaling up a big monolithic application.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing autoscaling capabilities that are available in the cloud is also a
    possibility, but is not typically feasible for a big monolithic application. It’s
    also easier to upgrade or even replace a single microservice compared to upgrading
    a big monolithic application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated by the following diagram, where a monolithic application
    has been divided into six microservices, all of which have been deployed into
    separate servers. Some of the microservices have also been scaled up independently
    of the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with medium confidence](img/B19825_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Dividing a monolith into microservices'
  prefs: []
  type: TYPE_NORMAL
- en: 'A very frequent question I receive from customers is:'
  prefs: []
  type: TYPE_NORMAL
- en: How big should a microservice be?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I try to use the following rules of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: Small enough to fit in the head of a developer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big enough to not jeopardize performance (that is, latency) and/or data consistency
    (SQL foreign keys between data that’s stored in different microservices are no
    longer something you can take for granted)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, to summarize, microservice architecture is, in essence, an architectural
    style where we decompose a monolithic application into a group of cooperating
    autonomous software components. The motivation is to enable faster development
    and to make it easier to scale the application.
  prefs: []
  type: TYPE_NORMAL
- en: With a better understanding of how to define a microservice, we can move on
    and detail the challenges that come with a system landscape of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Challenges with autonomous software components* section, we have already
    seen some of the challenges that autonomous software components can bring (and
    they all apply to microservices as well), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Many small components that use synchronous communication can cause *a chain
    of failure* problem, especially under high load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping the configuration up to date for many small components can be challenging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s hard to track a request that’s being processed and involves many components,
    for example, when performing root cause analysis, where each component stores
    log records locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the usage of hardware resources on a component level can be challenging
    as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual configuration and management of many small components can become costly
    and error-prone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another downside (but not always obvious initially) of decomposing an application
    into a group of autonomous components is that they form a **distributed system**.
    Distributed systems are known to be, by their nature, very hard to deal with.
    This has been known for many years (but in many cases was neglected until proven
    differently). My favorite quote to establish this fact is from Peter Deutsch who,
    back in 1994, stated the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The 8 fallacies of distributed computing**: Essentially everyone, when they
    first build a distributed application, makes the following eight assumptions.
    All prove to be false in the long run and all cause big trouble and painful learning
    experiences:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. The network is reliable
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Latency is zero
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Bandwidth is infinite
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. The network is secure
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5\. The topology doesn’t change
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6\. There is one administrator
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7\. The transport cost is zero
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 8\. The network is homogeneous
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Peter Deutsch, 1994
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In general, building microservices based on these false assumptions leads to
    solutions that are prone to both temporary network glitches and problems that
    occur in other microservice instances. When the number of microservices in a system
    landscape increases, the likelihood of problems also goes up. A good rule of thumb
    is to design your microservice architecture based on the assumption that there
    is always something going wrong in the system landscape. The microservice architecture
    needs to be designed to handle this, in terms of detecting problems and restarting
    failed components. Also, on the client side, ensure that requests are not sent
    to failed microservice instances. When problems are corrected, requests to the
    previously failing microservice should be resumed; that is, microservice clients
    need to be resilient. All of this needs, of course, to be fully automated. With
    a large number of microservices, it is not feasible for operators to handle this
    manually!
  prefs: []
  type: TYPE_NORMAL
- en: The scope of this is large, but we will limit ourselves for now and move on
    to learn about design patterns for microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Design patterns for microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This topic will cover the use of design patterns to mitigate challenges with
    microservices, as described in the preceding section. Later in this book, we will
    see how we can implement these design patterns using Spring Boot, Spring Cloud,
    Kubernetes, and Istio.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of design patterns is actually quite old; it was invented by Christopher
    Alexander back in 1977\. In essence, a design pattern is about describing a reusable
    solution to a problem when given a specific context. Using a tried and tested
    solution from a design pattern can save a lot of time and increase the quality
    of the implementation compared to spending time inventing the solution ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The design patterns we will cover are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reactive microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Central configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized log analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized monitoring and alarms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list is not intended to be comprehensive; instead, it’s a minimal list
    of design patterns that are required to handle the challenges we described previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a lightweight approach to describing design patterns, and focus
    on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requirements for the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book, we will delve more deeply into how to apply these design
    patterns. The context for these design patterns is a system landscape of cooperating
    microservices where the microservices communicate with each other using either
    synchronous requests (for example, using HTTP) or by sending asynchronous messages
    (for example, using a message broker).
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The service discovery pattern has the following problem, solution, and solution
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How can clients find microservices and their instances?
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservices instances are typically assigned dynamically allocated IP addresses
    when they start up, for example, when running in containers. This makes it difficult
    for a client to make a request to a microservice that, for example, exposes a
    REST API over HTTP. Consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, diagram  Description automatically generated](img/B19825_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: The service discovery issue'
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Add a new component – a **service discovery** service – to the system landscape,
    which keeps track of currently available microservices and the IP addresses of
    its instances.
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically register/unregister microservices and their instances as they
    come and go.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client must be able to make a request to a logical endpoint for the microservice.
    The request will be routed to one of the available microservice instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests to a microservice must be load-balanced over the available instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must be able to detect instances that currently are unhealthy so that requests
    will not be routed to them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation notes**: As we will see in *Chapter 9*, *Adding Service Discovery
    Using Netflix Eureka*, *Chapter 15*, *Introduction to Kubernetes*, and *Chapter
    16*, *Deploying Our Microservices to Kubernetes*, this design pattern can be implemented
    using two different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client-side routing**: The client uses a library that communicates with the
    service discovery service to find out the proper instances to send the requests
    to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-side routing**: The infrastructure of the service discovery service
    also exposes a reverse proxy that all requests are sent to. The reverse proxy
    forwards the requests to a proper microservice instance on behalf of the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The edge server pattern has the following problem, solution, and solution requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a system landscape of microservices, it is in many cases desirable to expose
    some of the microservices to the outside of the system landscape and hide the
    remaining microservices from external access. The exposed microservices must be
    protected against requests from malicious clients.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Add a new component, an **edge server**, to the system landscape that all incoming
    requests will go through:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: The edge server design pattern'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation notes**: An edge server typically behaves like a reverse proxy
    and can be integrated with a discovery service to provide dynamic load-balancing
    capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Hide internal services that should not be exposed outside their context; that
    is, only route requests to microservices that are configured to allow external
    requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expose external services and protect them from malicious requests; that is,
    use standard protocols and best practices such as OAuth, OIDC, JWT tokens, and
    API keys to ensure that the clients are trustworthy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reactive microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reactive microservices pattern has the following problem, solution, and
    solution requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, as Java developers, we are used to implementing synchronous communication
    using blocking I/O, for example, a RESTful JSON API over HTTP. Using a blocking
    I/O means that a thread is allocated from the operating system for the length
    of the request.
  prefs: []
  type: TYPE_NORMAL
- en: If the number of concurrent requests goes up, a server might run out of available
    threads in the operating system, causing problems ranging from longer response
    times to crashing servers. Using a microservice architecture typically makes this
    problem even worse, where typically a chain of cooperating microservices is used
    to serve a request. The more microservices involved in serving a request, the
    faster the available threads will be drained.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use non-blocking I/O to ensure that no threads are allocated while waiting for
    processing to occur in another service, that is, a database or another microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever feasible, use an asynchronous programming model, sending messages without
    waiting for the receiver to process them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a synchronous programming model is preferred, use reactive frameworks that
    can execute synchronous requests using non-blocking I/O, without allocating a
    thread while waiting for a response. This will make the microservices easier to
    scale in order to handle an increased workload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices must also be designed to be resilient and self-healing. Resilient
    meaning being capable of producing a response even if one of the services it depends
    on fails; self-healing meaning that once the failing service is operational again,
    the microservice must be able to resume using it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2013, key principles for designing reactive systems were established in **The
    Reactive Manifesto** ([https://www.reactivemanifesto.org/](https://www.reactivemanifesto.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: According to the manifesto, the foundation for reactive systems is that they
    are message-driven; they use asynchronous communication. This allows them to be
    elastic, that is, scalable, and resilient, that is, tolerant to failures. Elasticity
    and resilience together enable a reactive system to always respond in a timely
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Central configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central configuration pattern has the following problem, solution, and solution
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An application is, traditionally, deployed together with its configuration,
    for example, a set of environment variables and/or files containing configuration
    information. Given a system landscape based on a microservice architecture, that
    is, with a large number of deployed microservice instances, some queries arise:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I get a complete picture of the configuration that is in place for all
    the running microservice instances?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I update the configuration and make sure that all the affected microservice
    instances are updated correctly?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Add a new component, a **configuration server**, to the system landscape to
    store the configuration of all the microservices, as illustrated by the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: The central configuration design pattern'
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Make it possible to store configuration information for a group of microservices
    in one place, with different settings for different environments (for example,
    **dev**, **test**, **qa**, and **prod**).
  prefs: []
  type: TYPE_NORMAL
- en: Centralized log analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Centralized log analysis has the following problem, solution, and solution requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditionally, an application writes log events to log files that are stored
    in the local filesystem of the server that the application runs on. Given a system
    landscape based on a microservice architecture, that is, with a large number of
    deployed microservice instances on a large number of smaller servers, we can ask
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I get an overview of what is going on in the system landscape when each
    microservice instance writes to its own local log file?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I find out if any of the microservice instances get into trouble and
    start writing error messages to their log files?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If end users start to report problems, how can I find related log messages;
    that is, how can I identify which microservice instance is the root cause of the
    problem? The following diagram illustrates the problem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: Microservices write log files to their local file system'
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Add a new component that can manage **centralized logging** and is capable
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting new microservice instances and collecting log events from them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting and storing log events in a structured and searchable way in a
    central database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing APIs and graphical tools for querying and analyzing log events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Microservices stream log events to standard system output, `stdout`. This makes
    it easier for a log collector to find the log events compared to when log events
    are written to microservice-specific log files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices tag the log events with the correlation ID described in the next
    section regarding the *Distributed tracing* design pattern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A canonical log format is defined, so that log collectors can transform log
    events collected from the microservices to a canonical log format before log events
    are stored in the central database. Storing log events in a canonical log format
    is required to be able to query and analyze the collected log events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed tracing has the following problem, solution, and solution requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It must be possible to track requests and messages that flow between microservices
    while processing an external request to the system landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of fault scenarios are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If end users start to file support cases regarding a specific failure, how can
    we identify the microservice that caused the problem, that is, the root cause?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one support case mentions problems related to a specific entity, for example,
    a specific order number, how can we find log messages related to processing this
    specific order – for example, log messages from all microservices that were involved
    in processing it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If end users start to file support cases regarding an unacceptably long response
    time, how can we identify which microservice in a call chain is causing the delay?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: The distributed tracing issue'
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To track the processing between cooperating microservices, we need to ensure
    that all related requests and messages are marked with a common **correlation
    ID** and that the correlation ID is part of all log events. Based on a correlation
    ID, we can use the centralized logging service to find all related log events.
    If one of the log events also includes information about a business-related identifier,
    for example, the ID of a customer, product, or order, we can find all related
    log events for that business identifier using the correlation ID.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to analyze delays in a call chain of cooperating microservices, we
    must be able to collect timestamps for when requests, responses, and messages
    enter and exit each microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign unique correlation IDs to all incoming or new requests and events in
    a well-known place, such as a header with a standardized name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a microservice makes an outgoing request or sends a message, it must add
    the correlation ID to the request and message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All log events must include the correlation ID in a predefined format so that
    the centralized logging service can extract the correlation ID from the log event
    and make it searchable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trace records must be created for when requests, responses, and messages both
    enter and exit a microservice instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The circuit breaker pattern has the following problem, solution, and solution
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A system landscape of microservices that uses synchronous intercommunication
    can be exposed to a **chain of failures**. If one microservice stops responding,
    its clients might get into problems as well and stop responding to requests from
    their clients. The problem can propagate recursively throughout a system landscape
    and take out major parts of it.
  prefs: []
  type: TYPE_NORMAL
- en: This is especially common in cases where synchronous requests are executed using
    blocking I/O, that is, blocking a thread from the underlying operating system
    while a request is being processed. Combined with a large number of concurrent
    requests and a service that starts to respond unexpectedly slowly, thread pools
    can quickly become drained, causing the caller to hang and/or crash. This failure
    can spread unpleasantly quickly to the caller’s caller, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Add a **circuit breaker** that prevents new outgoing requests from a caller
    if it detects a problem with the service it calls.
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open** the circuit and fail fast (without waiting for a timeout) if problems
    with the service are detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probe for failure correction (also known as a **half-open** circuit); that is,
    allow a single request to go through on a regular basis to see whether the service
    is operating normally again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close** the circuit if the probe detects that the service is operating normally
    again. This capability is very important since it makes the system landscape resilient
    to these kinds of problems; in other words, it self-heals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates a scenario where all synchronous communication
    within the system landscape of microservices goes through circuit breakers. All
    the circuit breakers are closed; they allow traffic, except for one circuit breaker
    (for **Microservice E**) that has detected problems in the service the requests
    go to. Therefore, this circuit breaker is open and utilizes fast-fail logic; that
    is, it does not call the failing service and waits for a timeout to occur. Instead,
    **Microservice E** can immediately return a response, optionally applying some
    fallback logic before responding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: The circuit breaker design pattern'
  prefs: []
  type: TYPE_NORMAL
- en: Control loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The control loop pattern has the following problem, solution, and solution requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a system landscape with a large number of microservice instances spread out
    over a number of servers, it is very difficult to manually detect and correct
    problems such as crashed or hung microservice instances.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Add a new component, a **control loop**, to the system landscape. This process
    is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.15: The control loop design pattern'
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The control loop will constantly observe the **actual state** of the system
    landscape, comparing it with a **desired state**, as specified by the operators.
    If the two states differ, it will take action to make the actual state equal to
    the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation notes**: In the world of containers, a *container orchestrator*
    such as Kubernetes is typically used to implement this pattern. We will learn
    more about Kubernetes in *Chapter 15*, *Introduction to Kubernetes*.'
  prefs: []
  type: TYPE_NORMAL
- en: Centralized monitoring and alarms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this pattern, we have the following problem, solution, and solution requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If observed response times and/or the usage of hardware resources become unacceptably
    high, it can be very hard to discover the root cause of the problem. For example,
    we need to be able to analyze hardware resource consumption per microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To curb this, we add a new component, a **monitor service**, to the system landscape,
    which is capable of collecting metrics about hardware resource usage for each
    microservice instance level.
  prefs: []
  type: TYPE_NORMAL
- en: Solution requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The solution requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It must be able to collect metrics from all the servers that are used by the
    system landscape, which includes autoscaling servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be able to detect new microservice instances as they are launched on
    the available servers and start to collect metrics from them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be able to provide APIs and graphical tools for querying and analyzing
    the collected metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be possible to define alerts that are triggered when a specified metric
    exceeds a specified threshold value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows Grafana, which visualizes metrics from Prometheus,
    a monitoring tool that we will look at in *Chapter 20*, *Monitoring Microservices*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.16: Monitoring with Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: That was an extensive list! I am sure these design patterns helped you to understand
    the challenges with microservices better. Next, we will move on to learning about
    software enablers.
  prefs: []
  type: TYPE_NORMAL
- en: Software enablers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we’ve already mentioned, we have a number of very good open source tools
    that can help us both meet our expectations of microservices and, most importantly,
    handle the new challenges that come with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spring Boot**, an application framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spring Cloud/Netflix OSS**, a mix of application framework and ready-to-use
    services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker**, a tool for running containers on a single server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes**, a container orchestrator that manages a cluster of servers
    that run containers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Istio**, a service mesh implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table maps the design patterns we will need to handle these challenges,
    along with the corresponding open source tool that will be used in this book to
    implement the design patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Design Pattern** | **Spring Boot** | **Spring Cloud** | **Kubernetes**
    | **Istio** |'
  prefs: []
  type: TYPE_TB
- en: '| **Service discovery** |  | Netflix Eureka and Spring Cloud LoadBalancer |
    Kubernetes `kube-proxy` and service resources |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Edge server** |  | Spring Cloud Gateway and Spring Security OAuth | Kubernetes
    Ingress controller | Istio ingress gateway |'
  prefs: []
  type: TYPE_TB
- en: '| **Reactive microservices** | Project Reactor and Spring WebFlux |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Central configuration** |  | Spring Config Server | Kubernetes `ConfigMaps`
    and Secrets |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Centralized log analysis** |  |  | Elasticsearch, Fluentd, and Kibana.
    Note: Actually not part of Kubernetes, but can easily be deployed and configured
    together with Kubernetes |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Distributed tracing** | Micrometer Tracing and Zipkin |  |  | Jaeger |'
  prefs: []
  type: TYPE_TB
- en: '| **Circuit breaker** |  | Resilience4j |  | Outlier detection |'
  prefs: []
  type: TYPE_TB
- en: '| **Control loop** |  |  | Kubernetes controller managers |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Centralized monitoring and alarms** |  |  |  | Kiali, Grafana, and Prometheus
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1.17: Mapping design patterns to open source tools'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that any of Spring Cloud, Kubernetes, or Istio can be used to implement
    some design patterns, such as service discovery, edge server, and central configuration.
    We will discuss the pros and cons of using these alternatives later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: With the design patterns and tools that we will use in the book introduced,
    we will wrap up this chapter by going through some related areas that are also
    important, but not covered in this text.
  prefs: []
  type: TYPE_NORMAL
- en: Other important considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be successful when it comes to implementing a microservice architecture,
    there are a number of related areas to consider as well. I will not cover these
    areas in this book; instead, I’ll just briefly mention them here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importance of DevOps**: One of the benefits of a microservice architecture
    is that it enables shorter delivery times and, in extreme cases, allows *continuous
    delivery* of new versions. To be able to deliver that fast, you need to establish
    an organization where dev and ops work together under the mantra *you built it,
    you run it*. This means that developers are no longer allowed to simply pass new
    versions of the software over to the operations team. Instead, the dev and ops
    organizations need to work much more closely together, organized into teams that
    have full responsibility for the end-to-end life cycle of one microservice (or
    a group of related microservices). Besides the organizational part of dev/ops,
    the teams also need to automate the delivery chain, that is, the steps for building,
    testing, packaging, and deploying the microservices to the various deployment
    environments. This is known as setting up a *delivery pipeline*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organizational aspects and Conway’s law**: Another interesting aspect of
    how a microservice architecture might affect the organization is *Conway’s law*,
    which states the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ”Any organization that designs a system (defined broadly) will produce a design
    whose structure is a copy of the organization’s communication structure.”
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Melvyn Conway, 1967
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: This means that the traditional approach of organizing IT teams for large applications
    based on their technology expertise (for example, UX, business logic, and database
    teams) will lead to a big three-tier application – typically, a big monolithic
    application with a separately deployable unit for the UI, one for processing the
    business logic, and one for the big database.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To successfully deliver an application based on a microservice architecture,
    the organization needs to be changed into teams that work with one or a group
    of related microservices. The team must have the skills that are required for
    those microservices, for example, languages and frameworks for the business logic
    and database technologies for persisting its data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Decomposing a monolithic application into microservices**: One of the most
    difficult decisions (and expensive if done wrong) is how to decompose a monolithic
    application into a set of cooperating microservices. If this is done in the wrong
    way, you will end up with problems such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow delivery**: Changes in the business requirements will affect too many
    of the microservices, resulting in extra work.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bad performance**: To be able to perform a specific business function, a
    lot of requests have to be passed between various microservices, resulting in
    long response times.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistent data**: Since related data is separated into different microservices,
    inconsistencies can appear over time in data that’s managed by different microservices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good approach to finding proper boundaries for microservices is to apply
    **domain-driven design** and its concept of **bounded contexts**. According to
    Eric Evans, a *bounded context* is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ”A description of a boundary (typically a subsystem, or the work of a particular
    team) within which a particular model is defined and applicable.”
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: This means that a microservice defined by a bounded context will have a well-defined
    model of its own data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Importance of API design**: If a group of microservices exposes a common,
    externally available API, it is important that the API is easy to understand and
    adheres to the following guidelines:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the same concept is used in multiple APIs, it should have the same description
    in terms of the naming and data types used.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is of great importance that APIs are allowed to evolve in an independent
    but controlled manner. This typically requires applying a proper versioning schema
    for the APIs, for example, [https://semver.org/](https://semver.org/). This implies
    supporting multiple major versions of an API over a specific period of time, allowing
    clients of the API to migrate to new major versions at their own pace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Migration paths from on-premises to the cloud**: Many companies today run
    their workload on-premises, but are searching for ways to move parts of their
    workload to the cloud. Since most cloud providers today offer *Kubernetes as a
    Service*, an appealing migration approach can be to first move the workload into
    Kubernetes on-premises (as microservices or not) and then redeploy it on a Kubernetes
    as a Service offering provided by a preferred cloud provider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Good design principles for microservices, the 12-factor app**: The 12-factor
    app ([https://12factor.net](https://12factor.net)) is a set of design principles
    for building software that can be deployed in the cloud. Most of these design
    principles are applicable to building microservices independently of where and
    how they will be deployed, that is, in the cloud or on-premises. Some of these
    principles will be covered in this book, such as config, processes, and logs,
    but not all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it for the first chapter! I hope it gave you a good basic idea of microservices
    and the challenges that come with them, as well as an overview of what we will
    cover in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this introductory chapter, I described my own way into microservices and
    delved into a bit of their history. We defined what a microservice is – a kind
    of autonomous distributed component with some specific requirements. We also went
    through both the good and challenging aspects of microservice-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: To handle these challenges, we defined a set of design patterns and briefly
    mapped the capabilities of open source products such as Spring Boot, Spring Cloud,
    Kubernetes, and Istio to the design patterns.
  prefs: []
  type: TYPE_NORMAL
- en: You’re eager to develop your first microservice now, right? In the next chapter,
    you will be introduced to Spring Boot and complementary open source tools that
    we will use to develop our first microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/SpringBoot3e](https://packt.link/SpringBoot3e)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1849216352344398875.png)'
  prefs: []
  type: TYPE_IMG
