["```java\n//y on the Left-Hand is the predicted variable, and PX1, PX2, PX3... are predictor varibles (X)\ny = LR0 + (LR1 * PX1) + (LR2 * PX2) + (LR3 * PX3) + (LR4 * PX4) + ......\n```", "```java\n//Natural Logarithm of the probability that Y equals one of two values, perhaps 0 and 1, each taken to //represent one of two mutually exclusive states\nLn[p/(1-p)] = LR0 + (LR1 * PX1) + (LR2 * PX2) + (LR3 * PX3) + (LR4 * PX4)\n```", "```java\nfLogistic(X1,X2,X3,..) = LR0 + (LR1 * PX1) + (LR2 * PX2) + (LR3 * PX3) +  ...\n```", "```java\nscala> val dfReader1 = spark.read \ndfReader1: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@3d9dc84d\n```", "```java\nscala> val dfReader2 = dfReader1.format(\"com.databricks.spark.csv\")\ndfReader2: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@3d9dc84d\n```", "```java\nscala> val dfReader3 = dfReader2.option(\"header\", true)\ndfReader3: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@3d9dc84d\n```", "```java\nscala> val dfReader4 = dfReader3.option(\"inferSchema\", true)\ndfReader4: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@3d9dc84d\n```", "```java\nscala> val dataFrame = dfReader4.load(\"\\\\bcw.csv\")\ndataFrame: org.apache.spark.sql.DataFrame = [id: int, clump_thickness: int ... 9 more fields]\n```", "```java\nscala> val dataSetPath = \"C:\\\\Users\\\\Ilango\\\\Documents\\\\Packt\\\\DevProjects\\\\Chapter2\\\\\"\ndataSetPath: String = C:\\Users\\Ilango\\Documents\\Packt\\DevProjects\\Chapter2\\\n```", "```java\nscala> val firstRDD = spark.sparkContext.textFile(dataSetPath + \"\\\\bcw.csv\")\nfirstRDD: org.apache.spark.rdd.RDD[String] = C:\\<<path to your dataset file>>\n MapPartitionsRDD[1] at textFile at <console>:25\n```", "```java\nscala> firstRDD.getNumPartitions\nres7: Int = 2\n```", "```java\nscala> val secondRDD = firstRDD.flatMap{ row => row.split(\"\\n\").toList }\nsecondRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[33] at flatMap at <console>:27\n```", "```java\nscala> val rddArrayString = secondRDD.map(_.split(\",\"))\nrddArrayString: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[34] at map at <console>:29\n```", "```java\nscala> rddArrayString.count\nres9: Long = 700\n```", "```java\nscala> val purgedRDD = rddArrayString.filter(_(6) != \"?\")\npurgedRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[35] at filter at <console>:31\n```", "```java\nscala> purgedRDD.count\nres12: Long = 684\n```", "```java\n//Drop the Array with the headers in it\nscala> val headerRemoved = cleanedRDD.collect.drop(1)\nheaderRemoved: Array[Array[String]] \n```", "```java\n//Step 1\nscala> import org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.linalg.Vectors\n\n//Step 2\nscala> val featureVectorArray = headerRemoved.map(row => (Vectors.dense(row(1).toDouble,..,row(10)))\nfeatureVectorArray: Array[(org.apache.spark.ml.linalg.Vector, String)] \n```", "```java\nscala> val dataFrame = spark.createDataFrame(featureVectorArray)\ndataFrame: org.apache.spark.sql.DataFrame = [_1: vector, _2: string]\n\n//display the first 20 rows of the new DataFrame 'dataFrame'\n//Readers are requested to run the show command and see what the contents are, as an exercise\nscala> dataFrame.show\n+--------------------+---+\n| _1| _2|\n+--------------------+---+\n|------------------------\n|-----------------------\n|----------------------\n|-----------------------\nDisplaying 20 rows..\n```", "```java\n//Features column\nscala> def bcFeatures = \"bc-diagnosis-label-column\"\nbcFeatures: String\n\n//unindexed label column\nscala> def bcDiagCategory = \"bc-indexed-category-column\"\nbcDiagCategory: String\n```", "```java\n;scala> val dataFrame2 = dataFrame.toDF(bcFeatures, bcDiagCategory)\ndataFrame2: org.apache.spark.sql.DataFrame = [bc-diagnosis-label-column: vector, bc-indexed-category-column: string]\n```", "```java\nscala> dataFrame2.show\n+-------------------------+--------------------------+\n|bc-diagnosis-label-column|bc-indexed-category-column|\n+-------------------------+--------------------------+\n| --------------------| 2|\n| --------------------| 2|\n| --------------------| 2|\n| --------------------| 2|\n| --------------------| 2|\n| --------------------| 4|\n| --------------------| 2|\n| --------------------| 2|\n| --------------------| 2|\n| --------------------| 2|\n| -----------------------\n| ------------------------\n| ------------------------\n| ------------------------\n| ------------------------\n| ------------------------\n| ------------------------\n| ------------------------\n| ------------------------\n| -----------------------\n+-------------------------+--------------------------+\nonly showing top 20 rows\n```", "```java\nimport org.apache.spark.sql.SparkSession\n```", "```java\nlazy val session: SparkSession = { SparkSession .builder() .master(\"local\") .appName(\"breast-cancer-pipeline\") .getOrCreate()\n```", "```java\nval dataSetPath = \"<<path to folder containing your Breast Cancer Dataset file>>\\\\bcw.csv\"\n```", "```java\ndef buildDataFrame(dataSet: String): DataFrame\n```", "```java\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n```", "```java\nval result1: Array[String] = session.sparkContext.textFile(<<path to bcw.csv represented by the dataSetPath variable>>)\n```", "```java\nval result2: RDD[String] = result1.flatMap { partition => partition.split(\"\\n\").toList }\nval result3: RDD[Array[String]] = result2.map(_.split(\",\"))\n```", "```java\nval result4: Array[Array[String]] = result3.collect.drop(1)\n```", "```java\nimport org.apache.spark.ml.linalg.Vectors\n```", "```java\nval result5 = result4.map(row => (Vectors.dense(row(1).toDouble,..toDouble),row(5)))\n```", "```java\nval dataFrame = spark.createDataFrame(result5).toDF(featureVector, speciesLabel)\n```", "```java\nobject BreastCancerRfPipeline extends WisconsinWrapper { }\n```", "```java\nimport org.apache.spark.ml.feature.StringIndexer\n```", "```java\nval indexer = new StringIndexer().setInputCol(bcwCategory).setOutputCol(\"indexedSpeciesLabel\")\n```", "```java\nval splitDataSet: Array[org.apache.spark.sql.\nDataset[org.apache.spark.sql.Row]] = indexedDataFrame.randomSplit(Array(0.75, 0.25), 98765L)\n```", "```java\nsplitDataset.size\nres48: Int = 2\n```", "```java\nval trainDataSet = splitDataSet(0)\ntrainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [bcw-features-column: vector, bcw-species-label-column: string]\n```", "```java\nval testDataSet = splitDataSet(1)\ntestSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [bcw-features-column: vector, bcw-species-label-column: string]\n```", "```java\nval randomForestClassifier = new RandomForestClassifier()\n .setFeaturesCol(bcwFeatures_CategoryOrSpecies_IndexedLabel._1)\n .setFeatureSubsetStrategy(\"sqrt\")\n```", "```java\nval irisPipeline = new Pipeline().setStages(Array[PipelineStage](indexer) ++ Array[PipelineStage](randomForestClassifier)\n```", "```java\nval validatedTestResults: DataFrame = new TrainValidationSplit() .setSeed(1234567L) .setEstimatorParamMaps(finalParamGrid) .setEstimator(irisPipeline)\n```", "```java\nvalidatedTestResults.setEvaluator(new MulticlassClassificationEvaluator())\n```", "```java\nval validatedTestResultsDataset:DataFrame = validatedTestResults.select(\"prediction\", \"label\")\n```", "```java\nval modelOutputAccuracy: Double = new MulticlassClassificationEvaluator()\n```", "```java\nval multiClassMetrics = new MulticlassMetrics(validatedRDD2)\n```", "```java\nsbt console\nscala>\nimport com.packt.modern.chapter2.BreastCancerRfPipeline\nBreastCancerRfPipeline.main(\"bcw\")\nAccuracy (precision) is 0.9285714285714286 Weighted Precision is: 0.9428571428571428\n```", "```java\nsbt package\n```", "```java\npackage com.packt.modern.chapter2\n\nimport com.packt.modern.chapter2.WisconsinWrapper\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\nimport org.apache.spark.ml.{Pipeline, PipelineStage}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{DataFrame, Row}\n```", "```java\nobject BreastCancerLrPipeline extends WisconsinWrapper { }\n```", "```java\nimport org.apache.spark.ml.feature.StringIndexer\n```", "```java\nval indexer = new StringIndexer().setInputCol(bcwFeatures_IndexedLabel._2).setOutputCol(bcwFeatures_IndexedLabel._3)\n```", "```java\nval indexerModel = indexer.fit(dataSet)\nval indexedDataFrame = indexerModel.transform(dataSet)\n\n```", "```java\nindexedDataFrame.show\n```", "```java\nval splitDataSet: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = indexedDataFrame.randomSplit(Array(0.75, 0.25), 98765L)\n\n//create two vals to hold TrainingData and TestingData respectively\nval trainDataFrame = splitDataSet(0)\nval testDataFrame = splitDataSet(1)\n```", "```java\nval logitModel = new LogisticRegression() .setElasticNetParam(0.75) .setFamily(\"auto\") .setFeaturesCol(bcwFeatures_IndexedLabel._1) .setLabelCol(bcwFeatures_IndexedLabel._3).fit(trainDataSet)\n```", "```java\n//Next run the model on the test dataset and obtain predictions\nval testDataPredictions = logitModel.transform(testDataSet)\n```", "```java\ntestDataPredictions.show(25)\n```", "```java\n//Start building a Pipeline that has 2 stages, an Indexer and a Classifier\nval wbcPipeline = new Pipeline().setStages( \n                                  Array[PipelineStage](indexer) ++ Array[PipelineStage](logitModel)\n                                 )\n```", "```java\nval pipelineModel = wbcPipeline.fit(trainDataSet)\n```", "```java\nval predictions = pipelineModel.transform(testDataSet)\n```", "```java\nval modelOutputAccuracy: Double = new BinaryClassificationEvaluator() .setLabelCol(\"label\") .setMetricName(\"areaUnderROC\") //Area under Receiver Operating Characteristic Curve .setRawPredictionCol(\"prediction\") .setRawPredictionCol(\"rawPrediction\")\n.evaluate(testDataPredictions)\n```", "```java\nval predAndLabelsDFrame:DataFrame = predictions.select(\"prediction\", \"label\")\nprintln(\"Validated TestSet Results Dataset is:  \" + validatedTestResultsDataset.take(10))\n```", "```java\nval validatedRDD2: RDD[(Double, Double)] = predictionAndLabels.rdd.collect { case Row(predictionValue: Double, labelValue: Double) => (predictionValue,labelValue)\n}\n```", "```java\nval classifierMetrics = new BinaryClassificationMetrics(validatedRDD2)\n```", "```java\nval accuracyMetrics = (classifierMetrics.areaUnderROC(), classifierMetrics.areaUnderPR())\n\n//Area under ROC\nval aUROC = accuracyMetrics._1\nprintln(s\"Area under Receiver Operating Characteristic (ROC) curve: ${aUROC} \")\nval aPR = accuracyMetrics._2\nprintln(s\"Area under Precision Recall (PR) curve: ${aPR} \")\n```", "```java\nArea under Receiver Operating Characteristic (ROC) curve: 0.958521384053299\nArea under Precision Recall (PR) curve: 0.9447563927932\n```"]