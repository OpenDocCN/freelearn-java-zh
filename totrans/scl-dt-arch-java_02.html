<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer027">
<h1 class="chapter-number" id="_idParaDest-26"><a id="_idTextAnchor025"/>2</h1>
<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Data Storage and Databases</h1>
<p>In the previous chapter, we understood the foundations of modern data engineering and what architects are supposed to do. We also covered how data is growing at an exponential rate. However, to make use of that data, we need to understand how to store it efficiently and effectively.</p>
<p>In this chapter, we will focus on learning how to store data. We will start by learning about various types of data and the various formats of the available data. We will briefly discuss encoding and compression and how well they work with various data types. Then, we will learn about file and object storage and compare these data storage techniques. After that, we will cover the various kinds of databases that are available in modern data engineering. We will briefly discuss the techniques and tricks to choose the correct database for a particular use case. However, choosing the correct database doesn’t guarantee a well-built solution. As a data architect, it is important to know how to best design a solution around the database so that we can make the most of the technology we have chosen and have an effective, robust, and scalable data engineering solution in place. </p>
<p>We will end this chapter by discussing how to design data models for different kinds of databases. To help you understand these critical concepts, we will provide hands-on real-world scenarios wherever possible.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Understanding data types, formats, and encodings</li>
<li>Understanding file, block, and object storage</li>
<li>The data lake, data warehouse, and data mart</li>
<li>Databases and their types</li>
<li>Data model design considerations</li>
</ul>
<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Understanding data types, formats, and encodings</h1>
<p>In this section, you will learn about the various data types and data formats. We will also cover compression and how compression and formats go together. After that, we will briefly discuss data encodings. This section will prepare you to understand these basic features of data, which will be of use when we discuss data storage and databases in the upcoming sections.</p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Data types</h2>
<p>All datasets that are used in modern-day data engineering can be broadly classified into one of three categories, as follows:</p>
<ul>
<li><strong class="bold">Structured data</strong>: This is a type of dataset that can easily<a id="_idIndexMarker110"/> be mapped to a predefined structure<a id="_idIndexMarker111"/> or schema. It usually refers to the relational data model, where each data element<a id="_idIndexMarker112"/> can be mapped to a predefined field. In a structured dataset, usually, the number of fields, their data type, and the order of the fields are well defined. The most common example of this is a relational data structure where we model the data structure in terms of an <em class="italic">entity</em> and a <em class="italic">relationship</em>. Such a relational data structure can be denoted by crows-foot notation. If you are interested in learning<a id="_idIndexMarker113"/> the basics of crows-foot notation, please refer to <a href="https://vertabelo.com/blog/crow-s-foot-notation">https://vertabelo.com/blog/crow-s-foot-notation</a>.The following diagram shows an example of structured data in crows-feet notation:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 2.1 – Structured data representation " height="341" src="image/B17084_02_001.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Structured data representation</p>
<p>In the preceding diagram, we can see<a id="_idIndexMarker114"/> a system consisting of three structured<a id="_idIndexMarker115"/> datasets called <strong class="source-inline">Customer</strong>, <strong class="source-inline">Order</strong>, and <strong class="source-inline">Product</strong>. Each of these datasets has a fixed number of fields and their corresponding data types. We can also see the relationship between the datasets. For example, here, <strong class="source-inline">Order</strong> is related to <strong class="source-inline">Customer</strong> via <strong class="source-inline">customer_id</strong> and <strong class="source-inline">Order</strong> is related to <strong class="source-inline">Product</strong> via <strong class="source-inline">product_id</strong>. Since structured datasets<a id="_idIndexMarker116"/> have relationships between them, they are also called relational data models.</p>
<ul>
<li><strong class="bold">Unstructured data</strong>: This is a type of data or a dataset<a id="_idIndexMarker117"/> that doesn’t conform<a id="_idIndexMarker118"/> to any predefined data model. Due<a id="_idIndexMarker119"/> to the lack of any internal structure, they cannot be stored by any relational data stores such as <strong class="bold">Relational Database Management Systems</strong> (<strong class="bold">RDBMSs</strong>). Also, since there is no schema attached to it, querying and searching is not as easy as in a structured data model.  </li>
</ul>
<p>Around 70% of the data that’s generated by systems is unstructured. They can either be generated by humans or by machines: </p>
<ul>
<li><strong class="bold">Human-generated unstructured data</strong>: A few examples of human-generated unstructured datasets<a id="_idIndexMarker120"/> are media files such as audio and video files, chat, instant messaging, phone call transcriptions, and text messages</li>
<li><strong class="bold">Machine-generated unstructured data</strong>: A few examples of machine-generated unstructured data<a id="_idIndexMarker121"/> are scientific data such as seismic imagery, digital surveillance, and satellite imagery</li>
</ul>
<ul>
<li><strong class="bold">Semi-structured data</strong>: This is a type of dataset that, unlike<a id="_idIndexMarker122"/> a relational data model, doesn’t<a id="_idIndexMarker123"/> contain a tabular structure, but still contains markers or tags to define the hierarchy and field names of the data model. Semi-structured data is hierarchical. Semi-structured data is especially useful for platforms and programming language-agnostic communication between different systems. Before we discuss a few types of semi-structured data, let’s look at a real-world example. </li>
</ul>
<p>Mastercard, Visa, and <strong class="bold">American Express</strong> (<strong class="bold">Amex</strong>) are card networks that connect payment processors with issuers. Usually, there are a lot of <strong class="bold">business-to-business</strong> (<strong class="bold">B2B</strong>) sales on card networks, where a merchant<a id="_idIndexMarker124"/> buys subscription plans to accept a card network, thus increasing the card networks’ revenue stream. For example, my dentist accepts only Mastercard and Amex, while Costco stores now only accept Visa all over the US. Each of these huge card networks has many Salesforce orgs or business units such as accounting, sales, and marketing. </p>
<p>Suppose Visa wants to generate a saleability score and the best time to reach a B2B customer. Information gathered from marketing<a id="_idIndexMarker125"/> and accounting via Salesforce will be used by a real-time <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>)-based application, which will generate and attach the saleability score and best time to reach the customer. The enriched record, along with this extra information, must flow to the Salesforce org for sales. Salesforce usually uses APEX as a language on the Salesforce cloud (which may be hosted in a different OS), while the AI application that generates the score and best call time is written in Java and Python and sits over an on-premises Kubernetes cluster. For messages to communicate easily between these disparate systems (with different OSs and different languages), we would use a form of semi-structured data (JSON) that is independent of the OS or the language of the different applications involved in this use case.</p>
<p>Now, let’s look at a few of the most popular types of semi-structured data:</p>
<ul>
<li><strong class="bold">JSON</strong> is the short form of <strong class="bold">JavaScript Object Notation</strong>. According to<a id="_idIndexMarker126"/> Wikipedia, it is “<em class="italic">an open standard format that uses human-readable text to transmit data objects consisting of attribute-value pairs</em>.” The following<a id="_idIndexMarker127"/> example consists of key-value pairs, where<a id="_idIndexMarker128"/> the value can be another JSON as well. This JSON has at least one key-value pair<a id="_idIndexMarker129"/> and is a value; this is known as a nested JSON. An array<a id="_idIndexMarker130"/> of JSON objects is called a <strong class="bold">JSON array</strong>.</li>
</ul>
<p>The following is an example of a JSON:</p>
<p class="source-code">{</p>
<p class="source-code">  "customerId": 47,</p>
<p class="source-code">  "firstname": "Lilith",</p>
<p class="source-code">  "lastname": "Wolfgram",</p>
<p class="source-code">  "address": "324 Spring Ln",</p>
<p class="source-code">  "city": "Hanover",</p>
<p class="source-code">  "country": "Micronesia, Federated States of",</p>
<p class="source-code">  "countryCode": "FO",</p>
<p class="source-code">  "email": "Lilith.Wolfgram@gmail.com",</p>
<p class="source-code">  "bills": [</p>
<p class="source-code">    {</p>
<p class="source-code">      "billId": 0,</p>
<p class="source-code">      "billAmount": 4801.98,</p>
<p class="source-code">      "paymentStatus": false,</p>
<p class="source-code">      "dueDt": "2020-12-20"</p>
<p class="source-code">    },</p>
<p class="source-code">    {</p>
<p class="source-code">      "billId": 1,</p>
<p class="source-code">      "billAmount": 668.71,</p>
<p class="source-code">      "paymentStatus": false,</p>
<p class="source-code">      "dueDt": "2020-12-27"</p>
<p class="source-code">    },</p>
<p class="source-code">    {</p>
<p class="source-code">      "billId": 2,</p>
<p class="source-code">      "billAmount": 977.94,</p>
<p class="source-code">      "paymentStatus": true,</p>
<p class="source-code">      "dueDt": "2020-11-24"</p>
<p class="source-code">    }</p>
<p class="source-code">  ]</p>
<p class="source-code">}</p>
<p>As you can see, there are tags<a id="_idIndexMarker131"/> denoting field names<a id="_idIndexMarker132"/> such as <strong class="source-inline">‘customerId’</strong>. The <strong class="source-inline">‘bills’</strong> tag’s value is an array of JSON objects, so, its value is a JSON array. Also, since <strong class="source-inline">‘bills’</strong> is not a primitive data type but instead another JSON, the preceding JSON is a nested JSON<a id="_idIndexMarker133"/> object that shows how JSON has a hierarchical structure.</p>
<ul>
<li><strong class="bold">XML</strong> denotes <strong class="bold">Extensible Markup Language</strong>. As is evident from the name, it is an open data format<a id="_idIndexMarker134"/> that is both human<a id="_idIndexMarker135"/> and machine-readable, in which each data value is tagged or marked by a tag that denotes the name of the field. XML is very similar to JSON in passing information between disparate systems in a platform and language-agnostic way. Like JSON, XML is also a hierarchical data structure. XML is the de facto standard for wsdl SOAP APIs. The following is the XML structure for the JSON described earlier:<p class="source-code">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;</p><p class="source-code">&lt;root&gt;</p><p class="source-code">    &lt;customerId&gt;47&lt;/customerId&gt;</p><p class="source-code">    &lt;firstname&gt;Lilith&lt;/firstname&gt;</p><p class="source-code">    &lt;lastname&gt;Wolfgram&lt;/lastname&gt;</p><p class="source-code">    &lt;address&gt;324 Spring Ln&lt;/address&gt;</p><p class="source-code">    &lt;city&gt;Hanover&lt;/city&gt;</p><p class="source-code">    &lt;country&gt;Micronesia, Federated States of&lt;/country&gt;</p><p class="source-code">    &lt;countryCode&gt;FO&lt;/countryCode&gt;</p><p class="source-code">    &lt;email&gt;Lilith.Wolfgram@gmail.com&lt;/email&gt;</p><p class="source-code">    &lt;bills&gt;</p><p class="source-code">        &lt;bill&gt;</p><p class="source-code">            &lt;billId&gt;0&lt;/billId&gt;</p><p class="source-code">            &lt;billAmount&gt;4801.98&lt;/billAmount&gt;</p><p class="source-code">            &lt;paymentStatus&gt;false&lt;/paymentStatus&gt;</p><p class="source-code">            &lt;dueDt&gt;2020-12-20&lt;/dueDt&gt;</p><p class="source-code">        &lt;/bill&gt;</p><p class="source-code">        ...</p><p class="source-code">    &lt;/bills&gt;</p><p class="source-code">&lt;/root&gt;</p></li>
</ul>
<p>The full source code for the preceding code snippet is available on GitHub at <a href="https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/sample.xml">https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/sample.xml</a>.</p>
<p>As you can see, each XML<a id="_idIndexMarker136"/> starts with a <strong class="source-inline">root</strong> tag, and each value<a id="_idIndexMarker137"/> is encapsulated by the tag names. We explored various data types in this section, but we need to understand how these types of data are formatted. So, in the next section, we will discuss various data formats.</p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Data formats</h2>
<p>In data engineering, a dataset<a id="_idIndexMarker138"/> can be stored in different kinds of file formats. Each format has its pros and cons. However, it is important to know which data format is more suitable for certain types of use cases over others. In this section, we will discuss the various characteristics of data formats, a few popular data formats, and their suitable use cases.</p>
<h3>Characteristics of data formats</h3>
<p>First, let’s review the various characteristics<a id="_idIndexMarker139"/> of the data format that makes them different. These characteristics also determine when a particular data type should be selected over others to solve a business problem. The main characteristics of data formats are as follows:</p>
<ul>
<li><strong class="bold">Text versus binary</strong>: Any data file is either stored as a text<a id="_idIndexMarker140"/> or binary file. While both text and binary files<a id="_idIndexMarker141"/> store the data as a series of bits (0s and 1s), the bits in a text file can only represent characters. However, in a binary file, it can either be a character or any custom data. A binary file is a file that contains binary data as a series of sequential bytes in a specific format, which can be viewed or interpreted by a specific kind<a id="_idIndexMarker142"/> of application. A binary file will contain<a id="_idIndexMarker143"/> all the information required by the reader application to view/edit the data. For example, a <strong class="source-inline">.jpg</strong> file can only be opened by applications such as Photos. Text files, on the other hand, can only contain characters, can be opened in any text editor, and are human-readable. For example, any <strong class="source-inline">.txt</strong> file that can be opened by text editors such as Notepad is a text file.</li>
<li><strong class="bold">Schema support</strong>: A schema is an outline, diagram, or model that defines the structure of various types of data. The schema stores field-level information such as the <em class="italic">data type</em>, <em class="italic">max size</em>, or <em class="italic">default value</em>. A schema can be associated with the data, which helps with the following: <ul><li>Data validation</li>
<li>Data serialization and compression</li>
<li>A way to communicate the data to all its consumers to easily understand and interpret it </li>
</ul></li>
</ul>
<p>Data formats may<a id="_idIndexMarker144"/> or may not support schema enforcement. Also, a schema can be included along with the data, or it can be shared separately with the schema registry. The schema registry is a centralized registry of schemas so that different applications can add or remove fields independently, enabling better decoupling. This makes it suitable for schema evolution and validation.</p>
<ul>
<li><strong class="bold">Schema evolution</strong>: As a business grows, more columns get added or changes are made to the column data type, which results in the schema changing over time. Even as the schema evolves, it is important to have backward compatibility for your old data. Schema evolution provides a mechanism to update the schema while maintaining backward compatibility. A few data formats, such as <strong class="bold">Avro</strong>, support schema evolution, which is helpful in an agile business; as a dataset’s schema can change over time.</li>
<li><strong class="bold">Row versus columnar storage</strong>: To understand row versus columnar<a id="_idIndexMarker145"/> storage, let’s take a look<a id="_idIndexMarker146"/> at the following diagram:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 2.2 – Row versus columnar storage " height="622" src="image/B17084_02_002.jpg" width="827"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Row versus columnar storage</p>
<p>The preceding diagram shows<a id="_idIndexMarker147"/> how the same data will be stored in row-based storage versus column-based storage. This example shows how sales data is stored in a columnar format versus a row-based data storage format. In a row-based format, all data is stored row-wise – that is, the columns of a specific row are stored adjacent to each other. Since the data is stored row-wise, it is ideal for scenarios<a id="_idIndexMarker148"/> where it is preferable to read or write data row-wise, such as in <strong class="bold">Online Transaction Processing</strong> (<strong class="bold">OLTP</strong>).</p>
<p>On the other hand, as evident in the preceding diagram, columnar storage stores the values of the same<a id="_idIndexMarker149"/> column in an adjacent memory block. So, columns<a id="_idIndexMarker150"/> are stored together. Since it stores<a id="_idIndexMarker151"/> data in a columnar fashion, it can optimize storage space by storing repetitive column values once and pointers for each row (this is indicated by a striking of the repetitive <strong class="screen-inline">Clothes</strong> value in the <strong class="screen-inline">Columnar Storage</strong> part of the preceding diagram). This kind of storage is very useful for scenarios where only a subset of columns is read repetitively<a id="_idIndexMarker152"/> and doesn’t expect transactional writes. Two typical use cases that are widely used are <strong class="bold">Online Analytical Processing</strong> (<strong class="bold">OLAP</strong>) and big data processing. Both are mainly used for analytical queries on huge datasets. In big data, the columnar format gives the added advantage of being splittable and enables partition creation, which helps process the data faster.</p>
<ul>
<li><strong class="bold">Splittable</strong>: Another important factor is whether the file can be partitioned or split into multiple files. This factor plays a role when the data volume is huge or the velocity is too high, as in the case of big data. Big data files<a id="_idIndexMarker153"/> can be stored in a distributed filesystem such as <strong class="bold">HDFS</strong> if the underlying data format is splittable. By doing so, processing such partitioned big data becomes much faster.</li>
<li><strong class="bold">Compression</strong>: Data processing performance<a id="_idIndexMarker154"/> often depends on the data’s size. Compression reduces the size of the data on disk, which increases network I/O performance (however, it might take more time to decompress it while processing). It also reduces the data packet size when this data flows over the network, and hence the data transfer rates as well. The following table shows<a id="_idIndexMarker155"/> a few popular data compression algorithms and their features:</li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table001">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Name</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Lossless Compress</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Compression Ratio</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Splitable</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Compression Speed</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Decompress Speed</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Gzip</strong></p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
<td class="No-Table-Style">
<p>2.7x‒3x</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
<td class="No-Table-Style">
<p>100 MBps</p>
</td>
<td class="No-Table-Style">
<p>440 MBps</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Snappy</strong></p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
<td class="No-Table-Style">
<p>2x</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
<td class="No-Table-Style">
<p>580 MBps</p>
</td>
<td class="No-Table-Style">
<p>2020 MBps</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">LZ4</strong></p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
<td class="No-Table-Style">
<p>2.5x</p>
</td>
<td class="No-Table-Style">
<p>No</p>
</td>
<td class="No-Table-Style">
<p>800 MBps</p>
</td>
<td class="No-Table-Style">
<p>4220 MBps</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Zstd</strong></p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
<td class="No-Table-Style">
<p>2.8x</p>
</td>
<td class="No-Table-Style">
<p>Yes</p>
</td>
<td class="No-Table-Style">
<p>530 MBps</p>
</td>
<td class="No-Table-Style">
<p>1360 MBps</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1 – Different compression techniques</p>
<ul>
<li><strong class="bold">Companion technologies</strong>: Sometimes, the choice of data format is dependent on a companion technology. For example, in a Hadoop environment, if we are planning to process the data using a Hive MapReduce job, it might be a good idea to use ORC format over Parquet format. But on the other hand, if all our transformations are done using Apache Spark, Parquet may be a better choice.</li>
</ul>
<p>In this section, we learned<a id="_idIndexMarker156"/> about the features and characteristics of various data formats and how they affect the storage and processing of data elements. However, it is important for an architect to be aware of the popular data formats and how they can be used judiciously.</p>
<h3>Popular data formats</h3>
<p>In this section, we will discuss a few popular data formats that are worth knowing about. You will encounter these when you try to develop a data engineering solution. They are as follows (we covered two popular data formats, JSON and XML, in the <em class="italic">Semi-structured data</em> section, in the <em class="italic">Data types</em> subsection):</p>
<ul>
<li><strong class="bold">Delimiter Separated Format</strong>: This is a text data format where newline<a id="_idIndexMarker157"/> is used as a record delimiter<a id="_idIndexMarker158"/> and there can be specific field delimiters based on which type of delimiter-separated file we are dealing with. Two of the most popular<a id="_idIndexMarker159"/> delimiter-separated formats are <strong class="bold">Comma Separated Value</strong> (<strong class="bold">CSV</strong>) and <strong class="bold">Tab Separated Value</strong> (<strong class="bold">TSV</strong>). While in CSV, the field delimiter<a id="_idIndexMarker160"/> is a comma, for TSV it is a tab. Optionally, they can have a header record. Although it doesn’t support splitting, it provides a very good compression ratio. This format doesn’t support null values or schema evolution. </li>
</ul>
<p>Due to the simplicity<a id="_idIndexMarker161"/> of the format, it is quite popular in batch processing<a id="_idIndexMarker162"/> scenarios as well as real-time stream processing. However, the lack of schema evolution, partitioning capabilities, and non-standardized formatting makes its usage limited and not recommended for many use cases.</p>
<ul>
<li><strong class="bold">Avro</strong>: This is a row-based data storage format<a id="_idIndexMarker163"/> that is known for its serialization<a id="_idIndexMarker164"/> capabilities. It stores its data in binary format compactly and efficiently. Avro has great support for schema enforcement and schema evolution. An Avro schema is defined in JSON format. Avro files are not human-readable from a text editor. However, Avro data can be read in a human-readable format such as JSON using <strong class="source-inline">avro-tools-&lt;version&gt;.jar</strong>. The command to convert Avro into a human-readable JSON format is as follows: <p class="source-code"><strong class="bold">java -jar ~/avro-tools-1.7.4.jar tojson filename.avro</strong></p></li>
</ul>
<p><strong class="source-inline">avro</strong> data is always accompanied by its schema, which can be read using <strong class="source-inline">avro-tools-&lt;version&gt;.jar</strong>, like so:  </p>
<p class="source-code"><strong class="bold">java -jar ~/avro-tools-1.7.4.jar getschema filename.avro</strong></p>
<p>If we had a binary Avro file equivalent to that of the JSON described while explaining Semi-structured data in the <em class="italic">Data types</em> section, the <strong class="source-inline">avro</strong> schema would look as follows:</p>
<p class="source-code">{</p>
<p class="source-code">  "name": "MyClass",</p>
<p class="source-code">  "type": "record",</p>
<p class="source-code">  "namespace": "com.sample.avro",</p>
<p class="source-code">  "fields": [</p>
<p class="source-code">    {</p>
<p class="source-code">      "name": "customerId",</p>
<p class="source-code">      "type": "int"</p>
<p class="source-code">    },</p>
<p class="source-code">    {</p>
<p class="source-code">      "name": "firstname",</p>
<p class="source-code">      "type": "string"</p>
<p class="source-code">    },</p>
<p class="source-code">    {</p>
<p class="source-code">      "name": "lastname",</p>
<p class="source-code">      "type": "string"</p>
<p class="source-code">    }</p>
<p class="source-code">   ...  ]</p>
<p class="source-code">}</p>
<p>The full source code<a id="_idIndexMarker165"/> for the preceding <a id="_idIndexMarker166"/>code snippet is available on GitHub at <a href="https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/avroschema.json">https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/avroschema.json</a>.</p>
<ul>
<li><strong class="bold">Parquet</strong> is an open source column-based data storage<a id="_idIndexMarker167"/> that’s ideal for analytical<a id="_idIndexMarker168"/> loads. It was created by Cloudera in collaboration with Twitter. Parquet is very popular in <em class="italic">big data engineering</em> because it provides a lot of storage optimization options, as well as provides great columnar compression and optimization. Like Avro, it also supports splittable files and schema <a id="_idIndexMarker169"/>evolution. It is quite flexible and has very good support for nested data structures. Parquet gives great read performance; it especially works very well with Apache Spark </li>
<li>Let’s try to understand how a Parquet file is structured. The following diagram shows the Parquet file structure:<div class="IMG---Figure" id="_idContainer018"><img alt="Figure 2.3 – Parquet file format " height="521" src="image/B17084_02_004.jpg" width="376"/></div></li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Parquet file format</p>
<p>A Parquet file<a id="_idIndexMarker170"/> contains a header and a footer. The header only consists of a marker called <strong class="bold">PAR1</strong>, denoting that it’s a parquet file. Then, the file is divided into row groups, where each row group denotes a set of rows contained in a chunk of data. Each chunk of data is equal to the block size of the Parquet file (128 MB by default). Each row group contains a chunk for every column. Again, each column chunk consists of one or more pages. Each page in a column consists of <em class="italic">n</em> number of rows whose size is less than or equal to a configured page size. Each column chunk stores metadata as well (min/max value, number of nulls, and so on). The footer<a id="_idIndexMarker171"/> contains metadata for all row<a id="_idIndexMarker172"/> groups, as well as the schema of the data.</p>
<ul>
<li><strong class="bold">Optimized Row Columnar</strong> (<strong class="bold">ORC</strong>): This is yet another open source file format developed in the Hadoop ecosystem<a id="_idIndexMarker173"/> by Hortonworks in collaboration<a id="_idIndexMarker174"/> with Facebook. ORC is another form of columnar data storage that supports excellent compression and column optimization. Let’s look at the ORC file structure to understand how it is different from the Parquet format. The following diagram shows the structure of an ORC file format:</li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><img alt="Figure 2.4 – ORC file structure " height="837" src="image/B17084_02_005.png" width="844"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – ORC file structure</p>
<p>In ORC format, each file consists of multiple strips. The default stripe size is 250 MB. Each stripe is subdivided into index data, row data, and a stripe footer. The index data contains indexes and the row data consists of actual data, but both of them are stored in columnar format. The stripe footer contains column encodings and their location. The file footer contains information about the list of stripes, the number of rows in each stripe, and the data type of each column. Apart from that, it contains stripe-level statistical information such as min, max, and sum. Finally, the postscript<a id="_idIndexMarker175"/> contains information regarding the length<a id="_idIndexMarker176"/> of the file’s footer, metadata section, and compression-related information.</p>
<p>Let’s now understand how to choose from the different data formats.</p>
<h4>How to choose between Avro, Parquet, and ORC</h4>
<p>To choose the correct data format, we must consider the following:</p>
<ul>
<li><strong class="bold">Read or write-intensive query pattern</strong>: For a write-intensive use case, a row-based format<a id="_idIndexMarker177"/> works better as appending new records becomes easier. So, Avro would be a better choice for write-intensive use cases. On the other hand, if a read-intensive use case needs to read a subset of columns more frequently, a columnar data format such as Parquet or ORC is a suitable choice.</li>
<li><strong class="bold">Compression</strong>: This is a very important aspect when choosing<a id="_idIndexMarker178"/> a data format because compression reduces both the time and storage required to store or transmit data. For big data use cases, compression involves a huge role. Row-based storage is not suitable for such scenarios. So, for big data analytical use cases, columnar storage such as Parquet or ORC is preferred. Also, more compression is required if transforming/processing big data creates a lot of intermediate reads and writes. In such a scenario, ORC is preferred because it gives a better compression ratio<a id="_idIndexMarker179"/> than Parquet. For example, if you are running a MapReduce job or a <strong class="bold">Hive Query Language</strong> (<strong class="bold">HQL</strong>) query on Hive with the MapReduce engine, ORC will perform better than Parquet. </li>
<li><strong class="bold">Schema Evolution</strong>: In many data engineering use cases, schemas<a id="_idIndexMarker180"/> change frequently over time as new columns get added or dropped as business requirement changes. If there are frequent changes in the data schema and you need backward schema compatibility, Avro is the best choice. Avro supports<a id="_idIndexMarker181"/> very advanced schema evolution, compatibility, and versioning while keeping the schema definition simple in JSON format.</li>
<li><strong class="bold">Nested Columns</strong>: If your use case is suitable for a row-based<a id="_idIndexMarker182"/> format, Avro works great with the nested column structure. Otherwise, if the use case is suitable for a columnar data format and you have a lot of nested complex columns, then Parquet is the ideal data format for such use cases.</li>
<li><strong class="bold">Platform Support</strong>: Finally, the platform or framework<a id="_idIndexMarker183"/> plays a very important role. Hive works best with ORC, while Apache Spark and Delta Lake have great support for Parquet. Avro or JSON is often a good choice for Kafka.</li>
</ul>
<p>In this section, we learned about various data formats such as text, Parquet, Avro, and others. In the next section, we will learn how data (which can be in text, Parquet, or any other format) can be stored using different data storage formats.</p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Understanding file, block, and object storage</h1>
<p>In this section, we will cover the various data storage formats<a id="_idIndexMarker184"/> that are essential for an architect who is planning to store their data. Data storage formats organize, keep, and present data in different ways, each with its pros and cons. The available data storage formats are file, block, and object.</p>
<p>File storage<a id="_idIndexMarker185"/> organizes and exposes data as a hierarchy of files and folders, whereas block storage<a id="_idIndexMarker186"/> divides the data into chunks and stores them in organized, evenly sized volumes. Finally, object storage<a id="_idIndexMarker187"/> manages the data in a space-optimized fashion and links it to its associated metadata.</p>
<p>Now, let’s dive deeper by discussing their basic concepts, pros and cons, and the use cases where they are applied. Let’s begin by discussing the simplest and the oldest of them all: file storage.</p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>File storage</h2>
<p>In file-level storage, data<a id="_idIndexMarker188"/> is stored as a single piece of information inside a file. This file is given a name, can contain metadata, and resides inside a directory or subdirectory. When you need to find a file, the computer needs to know the absolute path of the file to search and read the file.</p>
<p>The pros and cons of file storage are as follows:</p>
<ul>
<li><strong class="bold">Pros</strong>: It’s simple, has broad<a id="_idIndexMarker189"/> capabilities, and can store anything</li>
<li><strong class="bold">Cons</strong>: Not ideal for storing<a id="_idIndexMarker190"/> huge volumes as there is no option to scale up, only to scale out</li>
</ul>
<p>A few typical use cases<a id="_idIndexMarker191"/> are as follows:</p>
<ul>
<li>File storage is ideal for file sharing in offices and other environments for its sheer simplicity; for example, NAS.</li>
<li>Local archiving. NAS provides excellent support for storing archival data.</li>
<li>Data protection and security. File-level storage is an old technology, but due to the test of time and a broad variety of uses, its policy, standard, and protection capabilities are all advanced. This makes it a great candidate for data protection use cases.</li>
</ul>
<p>Let’s now take a look at block-level storage.</p>
<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Block storage</h2>
<p>In block-level storage, the data is divided into small chunks<a id="_idIndexMarker192"/> of data and assigned unique chunk identifiers. Since the chunk of data is small and has a unique identifier, it can be stored anywhere. Also, a group<a id="_idIndexMarker193"/> of data chunks consists of a logical unit called a volume. In block-level storage, you can add a volume of data easily to scale up the infrastructure by adding blocks. </p>
<p>One of the interesting<a id="_idIndexMarker194"/> things is how it handles metadata. Unlike a file-based architecture, there are no additional details associated with block storage other than its address. Here, the operating system controls the storage management, which makes it ideal storage for high-performance use cases.</p>
<p>The pros and cons of block-level storage are as follows:</p>
<ul>
<li><strong class="bold">Pros</strong>: It provides metadata<a id="_idIndexMarker195"/> handling by controlling the OS or database, making it highly performant. You can also easily scale storage up and down.</li>
<li><strong class="bold">Cons</strong>: It can be expensive. Also, externalizing<a id="_idIndexMarker196"/> metadata handling in the application layer means more headaches when managing metadata.</li>
</ul>
<p>A few typical use cases<a id="_idIndexMarker197"/> are as follows:</p>
<ul>
<li><strong class="bold">Databases</strong>: Databases usually<a id="_idIndexMarker198"/> use block storage. For example, AWS Relational Data Service uses AWS Elastic Block Storage volumes as its storage to store data.</li>
<li><strong class="bold">Virtualization</strong>: Virtualization software such as <strong class="bold">VMware</strong>, <strong class="bold">Hyper-V</strong>, and <strong class="bold">Oracle VirtualBox</strong> use block storage<a id="_idIndexMarker199"/> as their filesystems<a id="_idIndexMarker200"/> for the virtual operating<a id="_idIndexMarker201"/> system.</li>
<li><strong class="bold">Cloud-based instances</strong>: Cloud-based instances such as AWS EC2 use block storage (AWS Elastic Block Storage) as their hard disk storage. </li>
<li><strong class="bold">Email servers</strong>: Microsoft’s email server, Exchange, uses block storage as its standard storage system.</li>
</ul>
<p>Let’s look at object-level storage next.</p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Object storage</h2>
<p>Object-level storage stores data<a id="_idIndexMarker202"/> in isolated containers called objects, which have unique identifiers and flat structures. This makes data retrieval super easy as you can retrieve an object by using the unique identifier, irrespective of the location it is stored.</p>
<p>The pros and cons of object-level storage are as follows:</p>
<ul>
<li><strong class="bold">Pros</strong>: Object storage provides<a id="_idIndexMarker203"/> great <em class="italic">metadata flexibility</em>. For example, you can customize metadata so that the application is associated with an object or you can set the priority of an application to an object. You can pretty much do any customization. This flexibility makes object storage strong and super easy to manage.</li>
</ul>
<p>Apart from metadata flexibility, object storage is known for its <em class="italic">accessibility</em> as it has a REST API to access, which makes it accessible from any platform or language.</p>
<p>Object storage is extremely <em class="italic">scalable</em>. Scaling out object architecture is as simple as adding nodes to an existing storage cluster. With the rapid growth of data and the cloud’s pay-as-you-go model, this feature has helped object storage become the most sought-after storage for data engineering needs of the present and future.</p>
<ul>
<li><strong class="bold">Cons</strong>: With so many positives, there<a id="_idIndexMarker204"/> are certain drawbacks to object storage. The biggest and most notable one is that objects can’t be modified. However, you can create a newer version of the object. In some use cases such as big data processing, this is a boon instead of a headache.</li>
</ul>
<p>A few typical use cases<a id="_idIndexMarker205"/> are as follows:</p>
<ul>
<li><strong class="bold">Big data</strong>: Due to scalability and metadata<a id="_idIndexMarker206"/> flexibility, huge volumes, as well as unstructured data, can be stored and read easily from object storage. This makes it suitable for big data storage.</li>
<li><strong class="bold">Cloud</strong>: Again, due to scalability, object storage is a perfect<a id="_idIndexMarker207"/> candidate for cloud systems. Amazon S3 is Amazon’s object storage solution and is very popular. Also, customizable metadata helps Amazon S3 objects have a life cycle defined through the AWS console or its SDKs.</li>
<li><strong class="bold">Web Apps</strong>: Object storage’s easy accessibility using<a id="_idIndexMarker208"/> a REST API makes it a perfect candidate to be used as a backend for web apps. For example, AWS S3 alone is used as a cheap and quick backend for static websites.</li>
</ul>
<p>With that, we have covered the various kinds of data storage. In the next section, we will learn how enterprise data (stored in any of the aforementioned storage formats) is organized into different kinds of data repositories, which enables other applications to retrieve, analyze, and query that data.</p>
<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>The data lake, data warehouse, and data mart</h1>
<p>To build a data <a id="_idIndexMarker209"/>architecture, an architect<a id="_idIndexMarker210"/> needs to understand the basic concept<a id="_idIndexMarker211"/> and differences between a data lake, data warehouse, and data mart. In this section, we will cover the modern data architectural ecosystem and where the data lake, data warehouse, and data mart fit into that landscape.</p>
<p>The following diagram depicts the landscape<a id="_idIndexMarker212"/> of a modern data architecture:</p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 2.5 – Landscape of a modern data architecture " height="681" src="image/B17084_02_006.jpg" width="1085"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Landscape of a modern data architecture</p>
<p>As we can see, various types of data get ingested into the data lake, where it lands in the raw zone. The data lake consists of structured, semi-structured, and unstructured data ingested directly from data sources. Data lakes have a zone consisting of cleansed, transformed, and sorted datasets that serve various downstream data processing activities such as data analytics, advanced analytics, publishing as Data-as-a-Service, AI, ML, and many more. This is called the <strong class="bold">curated zone</strong>. The data lake acts as a source<a id="_idIndexMarker213"/> for creating a data warehouse, which is a structured data repository built for a specific line of business.</p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Data lake</h2>
<p>In a modern data architecture, data<a id="_idIndexMarker214"/> from various sources is ingested into a data lake. A data lake is a data storage repository that contains structured, semi-structured, and unstructured data. In most cases, the usage of the data in a data lake is not predefined. Usually, once data is ingested and stored in a data lake, various teams use that data for analytics, reports, business intelligence, and other usages.</p>
<p>However, internally, a data lake contains different data zones. The following are the different data zones that are available in a data lake:</p>
<ul>
<li><strong class="bold">Raw data zone</strong>: The raw data from various<a id="_idIndexMarker215"/> data sources is loaded into this zone. Here, the data that’s loaded is in raw form. This data can be unstructured, uncleaned, and<a id="_idIndexMarker216"/> unformatted. This is also known as the landing zone.</li>
<li><strong class="bold">Master data zone</strong>: This data zone usually contains<a id="_idIndexMarker217"/> reference data that augments the analytical or transformational activities of data present in the raw zone or curated zone.</li>
<li><strong class="bold">User data zone</strong>: Sometimes, in certain data<a id="_idIndexMarker218"/> lakes, the user can manually drop certain data. They are usually static. This portion<a id="_idIndexMarker219"/> of the data lake is called the user data zone.</li>
<li><strong class="bold">Curated data zone</strong>: This is the data publishing layer<a id="_idIndexMarker220"/> of the data lake. It contains cleansed, transformed, and sorted data. The data present in this layer is usually structured. Data may be stored in large flat files, as key-value stores, as data documents, in a star schema, or in a denormalized<a id="_idIndexMarker221"/> format. All data governance, data management, and security policies apply to this layer as this is the main consumption layer of the data lake.</li>
<li><strong class="bold">Archived data zone</strong>: The archive zone<a id="_idIndexMarker222"/> consists of data that has been offloaded by other systems such as a data warehouse or the curated zone due to aging. Data in this zone can’t usually be modified but can be appended. This kind of data is used for historical analysis or auditing purposes. Usually, a cheaper data storage technology is used to store archived data. Technologies such as Amazon S3 provide more advanced capabilities to progressively move data to cheaper solutions automatically as time progresses using an S3 bucket’s life cycle policy.</li>
</ul>
<p>Let’s move on to data warehouses next.</p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Data warehouse</h2>
<p>A data warehouse is a sorted central repository<a id="_idIndexMarker223"/> that contains information that has been curated from multiple data sources in a structured user-friendly fashion for data analytics. A good amount of discovery, analysis, planning, and data modeling is required before ingesting the data in a data warehouse. It is highly cleansed, transformed, and structured. As evident from <em class="italic">Figure 2.6</em>, the data warehouse is built from a data lake in modern data engineering pipelines. While data lakes are usually centralized raw data zones for the enterprise or organization, data warehouses are usually built per business unit or department. Each data warehouse is structured such that it caters to the need of that particular department. A deep dive into data warehouses and their schema types will be discussed in <a href="B17084_04.xhtml#_idTextAnchor062"><em class="italic">Chapter 4</em></a>, <em class="italic">ETL Data Load – A Batch-Based Solution to Ingest Data in a Data Warehouse</em>.</p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Data marts</h2>
<p>Data marts are usually<a id="_idIndexMarker224"/> a subset of a data warehouse that focuses on a single line of business. While a data warehouse is typically few 100 GBs to TBs in size, data marts are usually less than 100 GB in size. Data marts provide great read performance as it contains data which is analyzed, designed, and stored for a very specific line of business. For example, from a centralized company data warehouse, there can be a specific data mart for the HR department, one for the finance department, and another for the sales department.</p>
<p>The following table captures the difference<a id="_idIndexMarker225"/> between a data lake and a data warehouse:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Characteristics</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Data Lake</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Data warehouse</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Load Pattern</strong></p>
</td>
<td class="No-Table-Style">
<p>ETL</p>
<p>(Extract, Load, and Transform)</p>
</td>
<td class="No-Table-Style">
<p>ETL</p>
<p>(Extract, Transform, and Load)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Type Of Data Stored</strong></p>
</td>
<td class="No-Table-Style">
<p>Structured, semi-structured and unstructured</p>
</td>
<td class="No-Table-Style">
<p>Structured</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Analysis Pattern</strong></p>
</td>
<td class="No-Table-Style">
<p>Acquire, analyze, and then determine structure of curated data</p>
</td>
<td class="No-Table-Style">
<p>Create the structure first and then acquire the data for insights</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Data Ingestion Pattern</strong></p>
</td>
<td class="No-Table-Style">
<p>Batch processing, real-time, Batch processing near real-time processing</p>
</td>
<td class="No-Table-Style">
<p>Batch processing</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Schema Application Time</strong></p>
</td>
<td class="No-Table-Style">
<p>Schema-on-read i.e., schema is applied while reading the data</p>
</td>
<td class="No-Table-Style">
<p>Schema-on-write i.e., schema is determined and is available when data is written</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.2 – Data lake versus a data warehouse</p>
<p>So far, we have learned how various<a id="_idIndexMarker226"/> data repositories are used and how they enable enterprise data platforms. Data in these repositories can be stored as files or objects, but they can be stored in an organized data collection called a database, which can retrieve, manage, and search data easily. In the next section, we will discuss databases in detail.</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Databases and their types</h1>
<p>In this section, we will cover the various types of databases that are commonly used to create modern data engineering solutions. We will also try to explore the possible scenario when a specific type of database will be used.</p>
<p>A database<a id="_idIndexMarker227"/> is a systematic collection of data or information that’s stored in such a way that it can easily be accessed, retrieved, and managed. In modern-day data engineering, primarily, databases can be broadly classified into two categories, as follows:</p>
<ul>
<li><strong class="bold">Relational database</strong>: This is a kind of database<a id="_idIndexMarker228"/> known for storing structured datasets. Each type<a id="_idIndexMarker229"/> of dataset is related to another, and relational databases provide an easy way to establish a relationship between different kinds of datasets. We will discuss relational databases in detail later in this chapter.</li>
<li><strong class="bold">NoSQL databases</strong> or <strong class="bold">non-relational databases</strong>: NoSQL databases are<a id="_idIndexMarker230"/> non-relational databases, where data<a id="_idIndexMarker231"/> can be stored in some<a id="_idIndexMarker232"/> form other than a tabular format. NoSQL supports unstructured, semi-structured, and structured data. No wonder NoSQL stands for <em class="italic">Not only SQL</em>!</li>
</ul>
<p>The following diagram depicts the types of databases employed in a modern data engineering context:</p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 2.6 – Types of databases  " height="476" src="image/B17084_02_008.jpg" width="708"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Types of databases </p>
<p>Now, let’s discuss the various database types in detail.</p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Relational database </h2>
<p>A relational database, as<a id="_idIndexMarker233"/> discussed earlier, stores structural data. Each type of data<a id="_idIndexMarker234"/> in a relational database is stored<a id="_idIndexMarker235"/> in a container called a database table or, simply, a table. Each table<a id="_idIndexMarker236"/> needs to be defined first before data is loaded into that table. The table definition contains the column names or field names, their data type, and their size (optionally). Relational databases are further subdivided into two types: hierarchical databases and RDBMSs.</p>
<h3>Hierarchical database </h3>
<p>These are databases where data<a id="_idIndexMarker237"/> is stored in a tree-like structure. The databases<a id="_idIndexMarker238"/> consist of a series of data records. Each record contains<a id="_idIndexMarker239"/> a set of fields that are determined by the type of records (this is also called a segment). Each segment can be related to another segment<a id="_idIndexMarker240"/> by relationships called <em class="italic">links</em>. These types of databases are known for <em class="italic">parent-child relationships</em>. The model is simple but can only support one-to-many<a id="_idIndexMarker241"/> relationships. The following diagram shows an example of a hierarchical database model:</p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 2.7 – An example of a hierarchical data model " height="381" src="image/B17084_02_009.jpg" width="742"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – An example of a hierarchical data model</p>
<p>As shown in the preceding diagram, <strong class="screen-inline">Member</strong> is the root segment. One record of the <strong class="screen-inline">Member</strong> segment contains the ID <strong class="screen-inline">001</strong>. There are two child segments to the root segment called <strong class="screen-inline">Address</strong> and <strong class="screen-inline">Language</strong>. In the <strong class="screen-inline">Address Segment</strong> part, we can see three record instances – that is, <strong class="screen-inline">Address Mail</strong>, <strong class="screen-inline">Address Home</strong>, and <strong class="screen-inline">Address Work</strong>. The <strong class="screen-inline">Language Segment</strong> part<a id="_idIndexMarker242"/> also has instances such as <strong class="screen-inline">Spoken</strong> and <strong class="screen-inline">Written</strong>.</p>
<p>Examples<a id="_idIndexMarker243"/> of hierarchical databases<a id="_idIndexMarker244"/> include IBM <strong class="bold">Information Management System</strong> (<strong class="bold">IMS</strong>) and RDM Mobile.</p>
<h3>RDBMS</h3>
<p>RDBMS is a relational database management system<a id="_idIndexMarker245"/> that uses SQL as its programming and querying interface. It is the most popular<a id="_idIndexMarker246"/> and established kind of database across the industry. Data is stored in tables, which represent specific entities. Tables have a clearly defined set of columns, along with their data type. Each row in a table is called a record. Each table <a id="_idIndexMarker247"/>can contain primary keys that uniquely identify a record. Each table supports a variety of indexes. One table can be linked to another table by a foreign key index. RDBMS can support one-to-many and many-to-many relationships. They are very powerful and have been the powerhouses behind most modern applications for many decades.</p>
<p>Examples of RDBMSs include MySQL, Oracle, PostgreSQL, Amazon RDS, and Azure SQL.</p>
<p><em class="italic">When to use</em>: RDBMS is pretty<a id="_idIndexMarker248"/> much used everywhere you need multi-row ACID<a id="_idIndexMarker249"/> transactions and where you require complex joins. Web applications, employee management systems, and financial organization's online transactions are a few examples of where RDBMS is used.</p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>NoSQL database </h2>
<p>NoSQL, as discussed earlier<a id="_idIndexMarker250"/> in this section, supports unstructured<a id="_idIndexMarker251"/> as well as semi-structured data. This is possible because it supports flexible schema. Also, NoSQL databases store and process data in a distributed manner and hence can scale out infinitely. The usage of distributed computing in NoSQL database architectures helps them support a tremendous volume of data and makes them a great choice for big data processing. The different ways a relational database and a NoSQL database handle scaling can be seen in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 2.8 – Scale-up versus scale-out " height="386" src="image/B17084_02_010.jpg" width="671"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Scale-up versus scale-out</p>
<p>As we can see, a relational database<a id="_idIndexMarker252"/> scales up the same instance. However, that creates a limitation of scaling. Also, scaling up is a costlier operation. On the other hand, NoSQL uses commodity hardware, which is cheap, and the architecture is such that to scale it, it needs to be scaled out. This means that NoSQL can scale infinitely and is cheaper to scale.</p>
<p>NoSQL databases can be further categorized into specific kinds of databases. We will discuss each of them briefly while providing examples and usages.</p>
<h3>Key-value store</h3>
<p>Key-value stores are the simplest kind<a id="_idIndexMarker253"/> of NoSQL databases. The data<a id="_idIndexMarker254"/> that’s stored is in a key and value format. The attribute name is stored in the <em class="italic">key</em>, while the value of the attribute is stored in the <em class="italic">value</em>. Here, the key needs to be a string, but the value can be an object of any type. This means that the value can be a JSON, an XML, or some custom serialized object.</p>
<p>A few examples of key-value stores are Redis, Memcached, and RocksDB.</p>
<p><em class="italic">When to use</em>: </p>
<ul>
<li>In a microservice<a id="_idIndexMarker255"/> or an application. If you need a lookup table that needs to be read fast, then an in-memory key-value store such as Redis and Memcached is a good choice. Again, while Memcached supports concurrent reads, it doesn’t support values that are complex like Redis does. Cloud services such as AWS ElastiCache support both of these databases. If you are interested, you<a id="_idIndexMarker256"/> can find a more detailed comparison between Redis and Memcached at <a href="https://aws.amazon.com/elasticache/redis-vs-memcached/">https://aws.amazon.com/elasticache/redis-vs-memcached/</a>.</li>
<li>In real-time event stream processing, the state needs to be maintained if the current event processing is dependent<a id="_idIndexMarker257"/> on the state of an older event. This kind of real-time processing is called stateful stream processing. In stateful stream processing, RocksDB is a great choice to maintain the state as a key-value pair. Kafka Streams uses RocksDB internally to maintain the state for stateful<a id="_idIndexMarker258"/> stream<a id="_idIndexMarker259"/> processing. </li>
</ul>
<p>Next, let’s take a look at document-based databases.</p>
<h3>Document-based database </h3>
<p>Document databases are NoSQL databases<a id="_idIndexMarker260"/> that give you an easy way to store<a id="_idIndexMarker261"/> and query data from a document. A document is defined as a semi-structured data format such as JSON or XML. Document databases support nested elements as well, such as nested JSON and JSON arrays. Each document in a document database is stored in a key-value pair, where the key is a unique ID for the document and the value is the document that is stored. Document databases support indexing on any field of the document, even if it is a nested field.</p>
<p>A few examples of document-based databases are MongoDB, Apache CouchDB, Azure Cosmos DB, AWS DocumentDB, and ElasticSearch.</p>
<p><em class="italic">When to use</em>: </p>
<ul>
<li>When you want to publish<a id="_idIndexMarker262"/> curated data from a data lake or a data mart to web applications by using a microservice or REST API. Since web applications run on JavaScript, they can easily parse a JSON document. Storing a well-designed JSON document in a document database such as MongoDB or AWS DocumentDB gives the web application amazing performance.</li>
<li>If you are receiving feeds from multiple dynamic data feeds, such as social media feeds from Twitter, LinkedIn, and Facebook, and the schema of these feeds is evolving, you must process and publish this data together by extracting certain data points or running some kind of aggregation over them, then Apache CouchDB may be an excellent choice. Simply put, if you are consuming document data and have no control over the inbound schema, a document-based data store is a great choice.</li>
<li>If your lookup needs are not catered by a key-value store. If the value is a document that has a very complex schema or that the cost of the storage in the key-value store is becoming too high because of the volume of the data, then a document-based database is the next most obvious choice.</li>
<li>If you are creating<a id="_idIndexMarker263"/> a search repository for a business, then you<a id="_idIndexMarker264"/> might want to store the data in a search engine storage such as Elasticsearch, a document-based database. It creates a reverse text index (called the Lucene index) while storing the<a id="_idIndexMarker265"/> data. This is a special document-based database where each record is stored as a document, along with a unique key. Elasticsearch provides amazing search performance. However, data should only be stored in Elasticsearch if you want to perform a high-performance text-based search over the data or to create some visualization out of the data. </li>
</ul>
<p>Let’s now explore columnar databases.</p>
<h3>Columnar database</h3>
<p>A columnar database<a id="_idIndexMarker266"/> stores data in a columnar format. Columnar databases<a id="_idIndexMarker267"/> are created using Big table. According to a paper published by Google that introduced Bigtable, it is <em class="italic">a sparse, distributed, persistent multidimensional sorted map</em>. At its core, each columnar database is a map. Here, each data record is associated with a key called the row key. These keys<a id="_idIndexMarker268"/> are unique and lexicographically sorted. The data that’s stored in a columnar database is persisted<a id="_idIndexMarker269"/> in a distributed filesystem that provides high availability of the data. Instead of columns, we define column families in a columnar database. Each column family can consist of any number of columns. The columns inside a column family are not fixed for all records and can be added dynamically. This means that in most data records, one or more columns may be empty or non-existent, so this data structure is sparse.  This allows you to dynamically add columns to a record. This makes columnar databases a great choice for storing unstructured data. The following diagram tries to capture the essence of a columnar database:</p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 2.9 – Columnar database structure " height="819" src="image/B17084_02_11.jpg" width="1532"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Columnar database structure</p>
<p>As shown in the preceding<a id="_idIndexMarker270"/> diagram, the records are divided into regions. One or more regions reside on a node in a distributed filesystem such as HDFS or GFS. Each column family inside a region is stored as a separate file. Again, each column inside a column family can support versioning, which makes columnar storage truly multi-dimensional.</p>
<p>Examples include Apache HBase, Cassandra, and Apache Kudu.</p>
<p><em class="italic">When to use</em>:</p>
<ul>
<li>In ad agency and marketing<a id="_idIndexMarker271"/> campaigns, a columnar data store is used to store the events of user clicks and user choices in real time. These real-time events are used on the fly to optimize the ads shown to a user or offers to send to a customer.</li>
<li>Another example is data received from Kafka as a stream of events that are small in size. These need to be stored in HDFS so that they can be analyzed or processed periodically using some form of batch application. Here, a columnar database is preferred since storing the data directly in HDFS or a warehouse such as Hive will create too many small files, which, in turn, will create too much metadata, thus slowing down the Hadoop cluster’s overall performance. Columnar storage is written to disk when the region size is reached and is usually placed in sequential files, so they are ideal for this kind of storage. </li>
<li>They are great databases<a id="_idIndexMarker272"/> for massive dynamic spikes in data. For example, they<a id="_idIndexMarker273"/> are great for handling massive data surges when sales are on during the holiday season.</li>
</ul>
<p>Next, let’s take a look at the graph database.</p>
<h3>Graph database </h3>
<p>A graph database is a database<a id="_idIndexMarker274"/> where data is stored in a graph structure. Essentially, this means<a id="_idIndexMarker275"/> that graph databases not only store the data but also the relationships between it. With the advent of social networking and since the data of every domain has become more connected, there is a growing need to not only query data but query the connections between the data. Also, in social networks, it is necessary to explore neighboring data points (for example, LinkedIn needs to explore data adjacency to show whether a person is connected to your profile as a 1st level, 2nd level, or 3rd level connection). Although relational databases can be used to get relationships using joins, a database can store, process, and query connections efficiently only if it natively supports relationships.</p>
<p>Most graph databases<a id="_idIndexMarker276"/> use a popular modeling approach called the <strong class="bold">property graph model</strong>. Here, data is organized into nodes, relationships, and properties. The following diagram shows an example of data stored using the property graph model:</p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<img alt="Figure 2.10 – Example of a property graph model " height="316" src="image/B17084_02_012.jpg" width="1013"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Example of a property graph model</p>
<p>In the property graph model, there are nodes<a id="_idIndexMarker277"/> and relationships. <strong class="bold">Nodes</strong> are the entities in the model such as <strong class="bold">Employee</strong>, <strong class="bold">Company</strong>, and <strong class="bold">City</strong>. These entities have attributes (key-value pairs), which are called the properties of that entity. In the preceding example, <strong class="bold">Employee</strong> has properties such as <strong class="source-inline">name</strong>, <strong class="source-inline">data_of_birth</strong>, and <strong class="source-inline">employee_ID</strong>. </p>
<p>Relationships are directed<a id="_idIndexMarker278"/> and use named connections between two named entities<a id="_idIndexMarker279"/> or nodes. For example, as shown in the preceding diagram, <strong class="source-inline">HAS_CEO</strong> is a relationship between <strong class="bold">Company</strong> and <strong class="bold">Employee</strong>. Relationships always have a direction, a type, a source node, and a target node. Relationships can also have properties. In the preceding example, the <strong class="source-inline">HAS_CEO</strong> relationship has a property called <strong class="source-inline">start_date</strong>.</p>
<p>Just like SQL standards, which are used to query RDBMS, graph databases can be queried using GQL. GQL is a newly announced ISO standard that helps query graph databases. One of the more popular<a id="_idIndexMarker280"/> open source GQLs available is openCypher. (You can learn more about openCypher at <a href="https://opencypher.org/">https://opencypher.org/</a>.) Other popular graph database query languages include Cypher, TinkerPop3, and SPARQL.</p>
<p>Some examples of graph databases<a id="_idIndexMarker281"/> are Neo4J, ArangoDB, RedisGraph, Amazon Neptune, and GraphDB.</p>
<p><em class="italic">When to use</em>:</p>
<ul>
<li>Fraud call detection.</li>
<li>Recommendation engines.</li>
<li>Customer engagement on travel websites.</li>
<li>Referral relationships. For example, using a graph database, a healthcare provider can identify the various other providers they can get a referral from. This helps target specific clients and build a relationship that can be beneficial for both providers.</li>
<li>Helps in a marketing campaign to identify influencers in a connected network by querying the number of incoming connections to a particular node.</li>
</ul>
<p>In this section, we discussed<a id="_idIndexMarker282"/> various types of databases and when they should be used. We covered a few examples and sample use cases where a particular database should be chosen and why. In the next section, we will look at a few considerations a data architect should keep in mind while designing data models for various databases.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Data model design considerations</h1>
<p>In this section, we will briefly discuss various design considerations you should consider while designing a data model for the various databases discussed in the previous section. The following aspects need to be considered while designing a data model:</p>
<ul>
<li><strong class="bold">Normalized versus denormalized</strong>: Normalization is a data organization technique. It is used<a id="_idIndexMarker283"/> to reduce redundancy in a relationship<a id="_idIndexMarker284"/> or set of relationships. This is highly used in RDBMS, and it is always a best<a id="_idIndexMarker285"/> practice in RDBMS to create a normalized data model. In a normalized data model, you store a column in one of the tables (which is most suitable), rather than storing the same column in multiple tables. When fetching data, if you need the data of that column, you can join the tables to fetch that column. The following diagram shows an example of normalized data modeling using the crows-feet notation:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt=" Figure 2.11 – Normalized data modeling " height="461" src="image/B17084_02_013.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 2.11 – Normalized data modeling</p>
<p>In the preceding diagram, none<a id="_idIndexMarker286"/> of the columns are repeated or redundant. Now, suppose<a id="_idIndexMarker287"/> we need to show an order<a id="_idIndexMarker288"/> that should display <strong class="source-inline">customer name</strong>, <strong class="source-inline">Customer ID</strong>, <strong class="source-inline">Order ID</strong>, <strong class="source-inline">item name</strong>, and <strong class="source-inline">Order date</strong>. To fetch this information, we can write a join query, like this:</p>
<p class="source-code"><strong class="bold">SELECT cust.CustomerID, orders.OrderID, items.ItemName,orders.OrderDate FROM Orders orders JOIN Customers cust ON orders.CustomerID=cust.CustomerID JOIN Order_items orderitem on orderitem.OrderID = orders.OrderID JOIN Items items ON items.ItemID = orderitem.ItemID</strong></p>
<p>On the other hand, if we are designing the same thing for a NoSQL database, the focus should not be on reducing redundancy or normalizing the data. Instead, we should focus on the read speed. Such a change in design mindset is triggered by two important factors. First, NoSQL works with a huge volume of data and stores the data in distributed commodity hardware. So, data storage is not costly, and joins may not work efficiently if the volume of data is in hundreds of TBs or PBs. Second, NoSQL doesn’t have the <strong class="source-inline">JOIN</strong> kind of queries, because NoSQL databases are non-relational. The following is an example of a document data model storing the same information that needs to be fetched:</p>
<p class="source-code">{</p>
<p class="source-code">  OrderId: Int, //documentKey</p>
<p class="source-code">  CustomerID: Int,</p>
<p class="source-code">  OrderDes: String</p>
<p class="source-code">  Items: [{</p>
<p class="source-code">    itemId: Int,</p>
<p class="source-code">    itemName: String,</p>
<p class="source-code">    OrderDate: Date </p>
<p class="source-code">    }]</p>
<p class="source-code">}</p>
<p>As we can see, a single document<a id="_idIndexMarker289"/> contains all the necessary information, so this means<a id="_idIndexMarker290"/> a lot of redundant<a id="_idIndexMarker291"/> data. However, for big data scenarios, NoSQL works perfectly fine and provides great performance.</p>
<ul>
<li><strong class="bold">Query-first versus schema-first pattern</strong>: While designing a data model in NoSQL, you must ask<a id="_idIndexMarker292"/> yourself what queries<a id="_idIndexMarker293"/> are going to run<a id="_idIndexMarker294"/> on this data model. The data model design in NoSQL usually starts with the kind of analytical query that will run on the model. This helps design a correct key for a document or a record in a NoSQL database. Also, in the case of a columnar database, it helps group columns in a column family based on the query that will run on the data. Such optimization helps NoSQL databases run queries with awesome performance on big data or unstructured data. </li>
</ul>
<p>On the other hand, RDBMS<a id="_idIndexMarker295"/> is designed to store data <a id="_idIndexMarker296"/>in a predefined schema that has been normalized and the relationships are very well defined. Since SQL is a declarative language and can query tables along with any related table during runtime, queries are not considered while designing an RDBMS data model. </p>
<ul>
<li><strong class="bold">Cost versus speed optimization</strong>: With the advent of cloud databases and cloud-based<a id="_idIndexMarker297"/> solutions, understanding the cost considerations<a id="_idIndexMarker298"/> is a very important factor for a modern data architect. For example, when it comes to storage versus <strong class="bold">Input/Output Operations Per Second</strong> (<strong class="bold">IOPS</strong>), IOPS are always costlier than storage<a id="_idIndexMarker299"/> in cloud-based models. However, understanding<a id="_idIndexMarker300"/> the difference in how IOPS is calculated for an RDBMS or document store can help you save costs and effort in the longer term. An RDBMS IOPS is based on the page or the block size. So, RDBMS IOPS is determined by the number of pages it has accessed. However, in a document database, IOPS is based on the number of DB read/writes that happen in that database.</li>
</ul>
<p>Another example is that if, in an AWS DocumentDB, you give a greater number of indexes, you might<a id="_idIndexMarker301"/> get a better speed, but too many indexes will increase IOPS considerably, so it might cost you more. A safe limit of indexes per collection is five.</p>
<ul>
<li><strong class="bold">Indexes</strong>: If you have a database, where<a id="_idIndexMarker302"/> you have a huge number of reads and you need to have great read performance, then you should consider having indexes in your database. Indexes help improve your read and update performance in your database. On the other hand, if you have a write-heavy application, indexes can slow down your insert performance.</li>
<li><strong class="bold">Data distributions</strong>: NoSQL databases are based<a id="_idIndexMarker303"/> on the scale-out architecture, where the data is stored and distributed across commodity nodes. One of the reasons that NoSQL databases have great performance for huge data volumes is that they can read or write data in parallel in the distributed nodes. However, if not designed properly, the data can be stored unevenly, which can cause a huge volume of data<a id="_idIndexMarker304"/> to be present in one node. This kind of uneven distribution of data in a distributed database is called <strong class="bold">data skew</strong>. </li>
</ul>
<p>Often, the problem of a node containing unusually high amounts of data, which can cause read<a id="_idIndexMarker305"/> or write bottlenecks for the database, is called <strong class="bold">hotspotting</strong>. Often, this happens due to a lack of understanding of the design principles of NoSQL databases and poor key design. In columnar databases, choosing an incremental sequence number as a key often leads to hotspotting. Instead, in both document and columnar databases, unique keys should be chosen and a combination of a few key column values should be concatenated in a particular order, preferably at least one of them being a text value. Techniques such as salting and MD5 encryption<a id="_idIndexMarker306"/> are used while designing the keys to help avoid hotspotting.</p>
<p>In this section, we covered the most obvious design considerations you should look at after you have chosen a database. While these considerations are basic for any data model design, there are other finer data model design techniques that are specific to the database you are choosing. We strongly recommend that you go over the official documentation of the database you’ve chosen for your solutions before you design your data model.</p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Summary</h1>
<p>In this chapter, we covered the various data types and data formats that are available. We also discussed the various popular data formats that are used in modern data engineering and the compression techniques that are compatible with each. Once we understood the data types and formats, we explored various data storage formats – file, block, and object storage – we can use to store the data. Then, we discussed various kinds of enterprise data repositories in detail – data lake, data warehouse, and data marts. Once we covered the basics of data, including the different types and their storage, we briefly discussed databases and their types. We discussed various examples of databases, the USP of each kind of database, and when a particular kind of database should be chosen over another. We explored possible use cases when a database should be used.</p>
<p>Finally, we briefly covered the basic design considerations that a data architect should keep in mind while designing their data model using any chosen database. </p>
<p>Now that you know about data types, formats, databases, and when to use what, in the next chapter, we will explore the various platforms where data engineering solutions can be deployed and run.</p>
</div>
</div>
</body></html>