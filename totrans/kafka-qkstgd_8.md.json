["```java\n{\"event\":\"HEALTH_CHECK\",\"factory\":\"Lake Anyaport\",\"serialNumber\":\"EW05-HV36\",\"type\":\"WIND\",\"status\":\"STARTING\",\"lastStartedAt\":\"2018-09-17T11:05:26.094+0000\",\"temperature\":62.0,\"ipAddress\":\"15.185.195.90\"}\n{\"event\":\"HEALTH_CHECK\",\"factory\":\"Candelariohaven\",\"serialNumber\":\"BO58-SB28\",\"type\":\"SOLAR\",\"status\":\"STARTING\",\"lastStartedAt\":\"2018-08-16T04:00:00.179+0000\",\"temperature\":75.0,\"ipAddress\":\"151.157.164.162\"}{\"event\":\"HEALTH_CHECK\",\"factory\":\"Ramonaview\",\"serialNumber\":\"DV03-ZT93\",\"type\":\"SOLAR\",\"status\":\"RUNNING\",\"lastStartedAt\":\"2018-07-12T10:16:39.091+0000\",\"temperature\":70.0,\"ipAddress\":\"173.141.90.85\"}\n...\n```", "```java\nEW05-HV36   33\nBO58-SB28   20\nDV03-ZT93   46\n...\n```", "```java\napply plugin: 'java'\napply plugin: 'application'\nsourceCompatibility = '1.8'\nmainClassName = 'kioto.ProcessingEngine'\nrepositories {\n    mavenCentral()\n    maven { url 'https://packages.confluent.io/maven/' }\n}\nversion = '0.1.0'\ndependencies {\n    compile 'com.github.javafaker:javafaker:0.15'\n    compile 'com.fasterxml.jackson.core:jackson-core:2.9.7'\n    compile 'io.confluent:kafka-avro-serializer:5.0.0'\n    compile 'org.apache.kafka:kafka_2.12:2.0.0'\n    compile 'org.apache.kafka:kafka-streams:2.0.0'\n    compile 'io.confluent:kafka-streams-avro-serde:5.0.0'\n    compile 'org.apache.spark:spark-sql_2.11:2.2.2'\n    compile 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.2'\n}\njar {\n    manifest {\n        attributes 'Main-Class': mainClassName\n    } from {\n        configurations.compile.collect {\n            it.isDirectory() ? it : zipTree(it)\n        }\n    }\n    exclude \"META-INF/*.SF\"\n    exclude \"META-INF/*.DSA\"\n    exclude \"META-INF/*.RSA\"\n}\n```", "```java\ncompile 'org.apache.spark:spark-sql_2.11:2.2.2'\n```", "```java\ncompile 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.2'\n```", "```java\npackage kioto.spark;\nimport kioto.Constants;\nimport org.apache.spark.sql.*;\nimport org.apache.spark.sql.streaming.*;\nimport org.apache.spark.sql.types.*;\nimport java.sql.Timestamp;\nimport java.time.LocalDate;\nimport java.time.Period;\n\npublic class SparkProcessor {\n  private String brokers;\n  public SparkProcessor(String brokers) {\n    this.brokers = brokers;\n  }\n  public final void process() {\n    //below is the content of this method\n  }\n  public static void main(String[] args) {\n    (new SparkProcessor(\"localhost:9092\")).process();\n  }\n}\n```", "```java\nSparkSession spark = SparkSession.builder()\n    .appName(\"kioto\")\n    .master(\"local[*]\")\n    .getOrCreate();\n```", "```java\n Dataset<Row> inputDataset = spark\n    .readStream()\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", brokers)\n    .option(\"subscribe\", Constants.getHealthChecksTopic())\n    .load();\n```", "```java\nStreamingQuery consoleOutput =\n    streamToPrint.writeStream()\n    .outputMode(\"append\")\n    .format(\"console\")\n    .start();\n```", "```java\nDataset<Row> healthCheckJsonDf =\n    inputDataset.selectExpr(\"CAST(value AS STRING)\");\n```", "```java\n+--------------------------+\n|                     value|\n+--------------------------+\n| {\"event\":\"HEALTH_CHECK...|\n+--------------------------+\n```", "```java\nStructType struct = new StructType()\n    .add(\"event\", DataTypes.StringType)\n    .add(\"factory\", DataTypes.StringType)\n    .add(\"serialNumber\", DataTypes.StringType)\n    .add(\"type\", DataTypes.StringType)\n    .add(\"status\", DataTypes.StringType)\n    .add(\"lastStartedAt\", DataTypes.StringType)\n    .add(\"temperature\", DataTypes.FloatType)\n    .add(\"ipAddress\", DataTypes.StringType);\n```", "```java\nDataset<Row> healthCheckNestedDs =\n    healthCheckJsonDf.select(\n        functions.from_json(\n            new Column(\"value\"), struct).as(\"healthCheck\"));\n```", "```java\nroot\n |-- healthcheck: struct (nullable = true)\n |    |-- event: string (nullable = true)\n |    |-- factory: string (nullable = true)\n |    |-- serialNumber: string (nullable = true)\n |    |-- type: string (nullable = true)\n |    |-- status: string (nullable = true)\n |    |-- lastStartedAt: string (nullable = true)\n |    |-- temperature: float (nullable = true)\n |    |-- ipAddress: string (nullable = true)\n```", "```java\nDataset<Row> healthCheckFlattenedDs = healthCheckNestedDs\n   .selectExpr(\"healthCheck.serialNumber\", \"healthCheck.lastStartedAt\");\n```", "```java\nroot\n |-- serialNumber: string (nullable = true)\n |-- lastStartedAt: string (nullable = true)\n```", "```java\nDataset<Row> healthCheckDs = healthCheckFlattenedDs\n    .withColumn(\"lastStartedAt\", functions.to_timestamp(\n        new Column (\"lastStartedAt\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"));\n```", "```java\nprivate final int uptimeFunc(Timestamp date) {\n    LocalDate localDate = date.toLocalDateTime().toLocalDate();\n    return Period.between(localDate, LocalDate.now()).getDays();\n}\n```", "```java\nDataset<Row> processedDs = healthCheckDs\n    .withColumn( \"lastStartedAt\", new Column(\"uptime\"));\n```", "```java\nDataset<Row> resDf = processedDs.select(\n    (new Column(\"serialNumber\")).as(\"key\"),\n    processedDs.col(\"uptime\").cast(DataTypes.StringType).as(\"value\"));\n```", "```java\n//StreamingQuery kafkaOutput =\nresDf.writeStream()\n   .format(\"kafka\")\n   .option(\"kafka.bootstrap.servers\", brokers)\n   .option(\"topic\", \"uptimes\")\n   .option(\"checkpointLocation\", \"/temp\")\n   .start();\n```", "```java\ntry {\n  spark.streams().awaitAnyTermination();\n} catch (StreamingQueryException e) {\n  // deal with the Exception\n}\n```", "```java\nfirstOutput = someDataSet.writeStream\n...\n    .start()\n...\n secondOutput = anotherDataSet.writeStream\n...\n    .start()\nfirstOutput.awaitTermination()\nanotherOutput.awaitTermination()\n```", "```java\n$ gradle jar\n```", "```java\nBUILD SUCCESSFUL in 3s\n1 actionable task: 1 executed\n```", "```java\n $ ./bin/confluent start\n```", "```java\n $ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 \n      --topic uptimes\n```", "```java\n{\"event\":\"HEALTH_CHECK\",\"factory\":\"Lake Anyaport\",\"serialNumber\":\"EW05-HV36\",\"type\":\"WIND\",\"status\":\"STARTING\",\"lastStartedAt\":\"2017-09-17T11:05:26.094+0000\",\"temperature\":62.0,\"ipAddress\":\"15.185.195.90\"}\n{\"event\":\"HEALTH_CHECK\",\"factory\":\"Candelariohaven\",\"serialNumber\":\"BO58-SB28\",\"type\":\"SOLAR\",\"status\":\"STARTING\",\"lastStartedAt\":\"2017-08-16T04:00:00.179+0000\",\"temperature\":75.0,\"ipAddress\":\"151.157.164.162\"}\n{\"event\":\"HEALTH_CHECK\",\"factory\":\"Ramonaview\",\"serialNumber\":\"DV03-ZT93\",\"type\":\"SOLAR\",\"status\":\"RUNNING\",\"lastStartedAt\":\"2017-07-12T10:16:39.091+0000\",\"temperature\":70.0,\"ipAddress\":\"173.141.90.85\"}\n...\n```", "```java\n EW05-HV36   33\n BO58-SB28   20\n DV03-ZT93   46\n ...\n```"]