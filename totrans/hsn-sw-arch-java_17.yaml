- en: 'Chapter 14: Monitoring and Tracing Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a risk, as developers and architects, of overlooking what happens to
    our applications and services after production release. We may be tempted to think
    that it's just a problem for sysadmins and whoever oversees service operations.
    As is easy to understand, this is the wrong point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how our application behaves in production gives us a lot of insight
    into what is and is not working—from both a code and an architecture perspective.
    As we learned in the previous chapter, maintenance of our application is crucial
    for the success of each software project, and looking closely at how the application
    is going in production is the perfect way to understand whether there is something
    that can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in modern DevOps teams, as we learned in [*Chapter 5*](B16354_05_Final_JM_ePUB.xhtml#_idTextAnchor109),
    *Exploring the Most Common Development Models*, the separation of concerns must
    be overcome, and the development and architectural teams are responsible for operating
    services as well. In this chapter, we will have an overview of the common topics
    regarding the visibility of what happens to our application during production.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Log management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting application metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining application health checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application Performance Management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is not only to have an overview of these topics and what they are useful
    for, but also to understand the impact that a correct design and implementation
    may have on these topics. With that said, let's start discussing log management.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the source code used in this chapter here: [https://github.com/PacktPublishing/Hands-On-Software-Architecture-with-Java/tree/master/Chapter14](https://github.com/PacktPublishing/Hands-On-Software-Architecture-with-Java/tree/master/Chapter14).'
  prefs: []
  type: TYPE_NORMAL
- en: Log management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Log management** has been taken for granted so far in the book. Let''s just
    take a quick glance over some basic concepts related to producing logs in our
    Java applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Logging in Java has had a troubled history. At the very beginning, no standard
    was provided as part of the Java platform. When a standard (such as **Java Util
    Logging** (**JUL**)) was added to the platform (in release **1.4**), other alternative
    frameworks were available, such as **Apache Commons Logging** and **Log4j**.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Log4j has been deprecated and replaced by **Log4j2**
    and **logback**.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the JUL standard has been a part of the platform for many years
    now, the usage of alternative frameworks such as logback and Log4j2 is still very
    widespread, due to their features and performance. Regardless of which implementation
    we choose, there are some common concepts to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Common concepts in logging frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As described previously, no matter what kind of preferred log implementation
    is used in your projects, there are two main concepts that are common to every
    one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`INFO` level is commonly present, and defines the *average* level of verbosity,
    reporting log entries that record basic information useful to understand what''s
    going on, but that can be discarded if you want to reduce the amount of logging
    or if you are familiar and confident enough with your app''s behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Above the `INFO` level, there are `WARNING` and other similar levels (such as
    `ERROR` and `FATAL`) that are used for reporting unusual or incorrect behavior.
    For this reason, these levels are almost always kept active. Finally, below `INFO`
    there are other levels, such as `DEBUG` and `TRACE`, which are used for getting
    details on what is happening in our application and are usually reported only
    for a limited amount of time. They are also used to gather data on whether there
    is something wrong in our application that needs troubleshooting, as well as collect
    data in non-production environments for development purposes. These levels of
    logging are discouraged in production as they will produce a lot of entries and
    may impact performance, which leads to our next point.
  prefs: []
  type: TYPE_NORMAL
- en: '`CONSOLE` and `FILE` are two common ones used to report log entries in the
    console or in a file. Other alternatives may include appenders to a database and
    to other external systems (such as logging to a socket).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As described in the previous point, appenders may impact the overall performance.
    Writing to a file may be slower than writing to the console. For this reason,
    appenders often offer asynchronous alternatives that buffer the log entries in
    memory before writing them to the target system. However, this of course increases
    the risk of losing data should our application crash before the entries are forwarded
    to the relevant system (such as the file or the database).
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some very basic concepts of logging in Java (similar concepts can
    be found in other languages). But there are also some recommendations about logging
    that I can provide, from personal experience:'
  prefs: []
  type: TYPE_NORMAL
- en: It's a good practice to avoid string concatenation (such as `"My log is " +
    variable + " ! "`). Other than being ugly, it can have a performance impact since
    string concatenation operations happen even if the log is not used (because it's
    related to a disabled level). Most logging frameworks offer alternatives based
    on placeholders (such as `"My log is {} !", variable`). Most recent Java versions
    automatically mitigate string concatenation by replacing it at compilation time
    with more efficient alternatives (such as `StringBuilder`), but it's still a good
    idea to avoid it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider differentiating log destinations by content type. You may want to have
    different appenders (almost every framework allows it) to log different information.
    So, business information (related to how our application is performing from a
    business perspective) such as user information or the products used can go to
    a specific database table (and maybe then can be aggregated and reported), while
    logs containing technical information (useful for troubleshooting or checking
    the application's health) may go in a file to be easily accessed by SysOps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can also be done by log severity by sending the `INFO` level on a certain
    appender, and other levels on other appenders. This may also allow for different
    *quality of service* for logs: you could log business information that is logged
    on an asynchronous appender (because you may lose some data in the event of an
    application issue—this is not a problem), while technical logs should go on synchronous
    appenders (because you cannot afford to lose anything if you intend to understand
    the issues behind a misbehaving application).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log rotation** is an essential concept, but it''s still sometimes forgotten,
    especially in older applications. Log rotation can be implemented using the logging
    framework itself or by external scripts and utilities. It''s basically related
    to file appenders, and defines the way logs are archived by renaming them, moving
    them, and optionally compressing them. A log rotation policy allows the current
    logs to be small enough (for easy reading and searching) and makes it easier to
    find information from previous dates and save space on disk. This will help SysOps,
    who sometimes have to deal with misconfigured applications that fill the disk
    because of a misconfigured log rotation policy. Hopefully, this should be less
    common nowadays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every message should provide meaningful information. As trivial as it may sound,
    it's very easy to write just basic things in logs, assuming that whoever reads
    the log will have enough context. *I highly recommend not doing this!* Log messages
    could be read by people not knowing much about the application (such as first-line
    support staff) in emergency situations (such as production troubleshooting). When
    in doubt, be as clear as possible. This doesn't necessarily mean being verbose,
    but make sure to provide a decent amount of content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging levels should be defined in a standard way. Especially in big projects
    composed of many microservices or applications, it should be well documented what
    is supposed to be logged as INFO, what should be higher, and what should be lower.
    In this way, logging levels can be set in a uniform way, expecting the same kind
    of information across all modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same is true for log format. Almost every logging library supports defining
    a pattern, which means setting which information (apart from the log message itself)
    should be written, including date, time, the log level, and more. It's better
    if this kind of format is uniform across all the components to be easy to read
    and parse using tools (such as the very basic `grep` utility). Also, I strongly
    suggest configuring the logging library to provide information about the class
    that is generating the log. It's usually a bit expensive from a computational
    perspective (often negligible) but is worth it for sure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should have a discussion with security and legal advisors as soon as possible
    (if present) about what can, must, and should not be present in logs. This varies
    from application to application, but there may be information (such as personal
    information or credit card data) that is prohibited from being present in logs
    (or needs to be anonymized), and other information that is required to be present
    by law (such as audit information). You need to know about this and implement
    the requirements accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a follow-up from the previous point, most applications have legal (or other
    kinds of) requirements for log storage and archiving. You may need to store logs
    for many years, sometimes in an immutable way. Hence, log rotation and specialized
    hardware and software may be needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a final consideration about logging, we cannot avoid having a chat about
    log aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: Log aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 9*](B16354_09_Final_JM_ePUB.xhtml#_idTextAnchor230), *Designing
    Cloud-Native Architectures*, when discussing **Twelve-Factor Applications**, we
    saw how logs can be seen as an event stream and must be handled by a supporting
    platform, capable of capturing the event stream, storing it, and making it usable
    and searchable across different applications. We even mentioned **Fluentd** as
    a commonly used solution for this. This is exactly what log aggregation is about.
    A typical log aggregation architecture features the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An agent for collecting logs from the console (or a file) in the form of event
    streams. Fluentd is a common choice (even though it has some known limitations
    in terms of performance and logs that can potentially be lost in corner cases).
    **Filebeat** and **collectd** are some alternatives to this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence for log entries. **Elasticsearch** is practically the standard in
    this area, providing storing, indexing, and searching capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A frontend for navigating and monitoring log entries. Software commonly used
    for this goal are **Kibana** and **Grafana**. Here is a screenshot of the Kibana
    UI:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Kibana UI home'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.1_B16354_new.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – Kibana UI home
  prefs: []
  type: TYPE_NORMAL
- en: Log aggregation should be considered a must in cloud-native architecture, because
    having a heavily distributed application based on microservices will mean having
    a lot of systems to monitor and troubleshoot in the event of issues.
  prefs: []
  type: TYPE_NORMAL
- en: With a log aggregation strategy, you will have a centralized way to access logs,
    hence everything will become a bit easier.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we had a quick overview of logging in Java, and then we highlighted
    the characteristics and the advantages of log aggregation. In the next section,
    we are going to have a look at another key topic about monitoring, which is metrics
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting application metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Metrics** are a way to instrument your source code to provide real-time insights
    into what''s happening. Metrics are also known as **telemetry**. Instead of logs,
    which represent information pushed into a file, a console, or another appender,
    metrics are values exposed by the application and are supposed to be pulled by
    whoever is interested in them.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, while a log contains what's happening in our application, collected
    in a sequential way, metrics expose a snapshot of how the application was behaving
    in that instant, summarized into some well-known values (such as the number of
    threads, the memory allocated, and so on). It's also possible to define some custom
    metrics, which can be useful to define figures that are specific to our particular
    use case (such as the number of payments, transactions, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: There are many widespread frameworks useful for exposing metrics in Java. `Micrometer`
    is an open source façade implementation, while other commercial solutions exist,
    such as **New Relic** and **Datadog**. However, I think that one of the most interesting
    efforts in this area is one part of the MicroProfile standard. We looked at MicroProfile
    in [*Chapter 7*](B16354_07_Final_JM_ePUB.xhtml#_idTextAnchor164), *Exploring Middleware
    and Frameworks*, when discussing Quarkus as an implementation of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'I think that a quick example (MicroProfile compliant) will be useful here to
    better explain what metrics look like. Let''s see a simple hello world REST API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `hello` method is annotated with two metric-related annotations
    (`Counted` and `Timed`), which declare the kind of metrics we want to collect.
    The annotations also provide some documentation (the `name` and `description`
    of the metric). Now, if we query the application via REST, we can see all the
    metrics values exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A number of other metrics (such as minimum, maximum, and average) are omitted
    in the preceding output and calculated automatically by the framework.
  prefs: []
  type: TYPE_NORMAL
- en: These kinds of metrics are exposed under the `/metrics/application` endpoint
    (`/q/metrics/application`, in the case of the Quarkus framework).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MicroProfile specification also defines the `/metrics/vendor` (vendor-specific),
    `/metrics/base` (a meaningful predefined subset), and `/metrics` (all the metrics
    available) endpoints. In these endpoints, you may find a lot of useful insights
    into the application, such as virtual machine stats and similar things. This is
    a small subset of what can be retrieved from such endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The metrics exposed in this way can then be collected by external systems, which
    can store them and provide alerts in the event of something going wrong. A widely
    used framework to do so is **Prometheus**.
  prefs: []
  type: TYPE_NORMAL
- en: Being a part of the **Cloud-Native Computing Foundation** (**CNCF**) effort,
    Prometheus is able to collect the metrics from various systems (including OpenMetrics-compliant
    endpoints, similar to the ones exposed by the example we saw previously), store
    them in a so-called **Time Series Database** (**TSDB**) (which is basically a
    database optimized for storing events on a temporal scale), and provide capabilities
    for querying the metrics and providing alerts. It also offers a built-in graphical
    interface and integration with Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: But metrics are just one of the aspects of application monitoring. Another similar
    and important one is health checks.
  prefs: []
  type: TYPE_NORMAL
- en: Defining application health checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Health checks** are a kind of special case for metrics collection. Instead
    of exposing figures useful for evaluating the trends of application performance,
    health checks provide simple *on/off* information about the application being
    healthy or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Such information is particularly useful in cloud and PaaS environments (such
    as Kubernetes) because it can allow self-healing (such as a restart) in the event
    of an application not working.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenMetrics currently defines three kinds of health checks: **live**, **ready**,
    and **started**. These checks come from concepts in the Kubernetes world:'
  prefs: []
  type: TYPE_NORMAL
- en: By using a live (health) check, Kubernetes knows whether an application is up
    and running, and restarts it if it's not healthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a readiness check, Kubernetes will be aware of whether the application
    is ready to take requests and will forward connections to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Startup checks identify the successful completion of the startup phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that **ready** and **started** are very similar but **started** has to
    do with the first startup of the application (which may be slow), while **ready**
    may involve a temporary inability to process requests (as an example, a traffic
    spike or other temporary slowdowns).
  prefs: []
  type: TYPE_NORMAL
- en: Quarkus provides such checks with the `smallrye-health` extension. The probes
    are exposed, by default, at the `/q/health/live`, `/q/health/ready`, and `/q/health/started`
    endpoints and the results are formatted as JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement the checks, Quarkus provides an infrastructure based
    on annotations. This is how a basic `Liveness` probe is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding method is annotated with `@Liveness` and returns
    a message using the `up` method of the `HealthCheckResponse` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, a `Readiness` check will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Also, in this case, the preceding method is annotated (in this case, with `@Readiness`)
    and returns a message using the `up` method of the `HealthCheckResponse` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a `Startup` check will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For startup checks, the preceding method is annotated (with `@Startup`) and
    returns a message using the `up` method of the `HealthCheckResponse` object.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the API is pretty simple. The objects providing the functionalities
    are singletons by default. Of course, in a real-world application, you may want
    to do some complex checks such as testing the database connection or something
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: You can of course return a negative response (such as with the `down()` method)
    if you detect any failure. Other useful features include the chaining of multiple
    checks (where the cumulative answer is `up`, only if every check is up) and the
    ability to include some metadata in the response.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing OpenTracing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tracing is a crucial monitoring technique when you have a long chain of calls
    (for example, a microservice calling other microservices, and so on), as you compose
    your answer by calling a huge number of internal or external services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, it''s a very common use case in microservices applications: you have
    a call coming into your application (such as from a REST web service or an operation
    on a web user interface, which in turn translates into one or more REST calls).
    This kind of call will then be served by a number of different microservices,
    ultimately being assembled into a unique answer.'
  prefs: []
  type: TYPE_NORMAL
- en: The issue with this is that you may end up losing trace of whatever happened.
    It becomes very hard to correlate the incoming call with every specific sub-call.
    And that may be a big problem, in terms of troubleshooting issues and even simply
    understanding what's happening.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing allows a way to identify the path made by each request by propagating
    an identifier code used in each subsystem, hence helping to document and reconstruct
    the tree of calls used to implement our use case, both for troubleshooting and
    for other purposes, such as audit logging.
  prefs: []
  type: TYPE_NORMAL
- en: '`smallrye-opentracing`.'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting feature is that OpenTracing also supports computing the time
    spent on each sub-call.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see a very simple example to understand how tracing works in Quarkus.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a simple REST resource, as we have seen many other times
    in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it's a simple REST method listening on the `/trace` endpoint.
    It uses a service (`NameGuessService`) that has been injected.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s worth noticing that there is no specific code related to tracing: indeed,
    tracing in REST endpoints is basically automatically provided by the framework.
    It''s enough to have the `smallrye-opentracing` extension in the project itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at the `NameGuessService` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, there is nothing special here: there''s just a simple mocked
    service returning a string, which is chosen randomly. The only notable thing is
    that the method is annotated with `@Traced`, because the framework needs to know
    explicitly whether the method must be traced.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Where do we go from here?* The most common and useful way to use tracing is
    with a Jaeger server. Jaeger basically exposes some services that collect and
    graphically display what''s happening in our application. The basic concept is
    a **span**, which is an end-to-end method call. In our case, one **span** is made
    out of our REST call, and another one is the sub-call in our injected service.'
  prefs: []
  type: TYPE_NORMAL
- en: A quick way to test our service locally is to use a ready-made Jaeger server
    containerized.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a laptop with a container engine (such as Docker) installed, it''s enough
    to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will run a `jaegertracing` *all-in-one* image, specifying the ports used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then run our application, hooking it into a Jaeger server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: These parameters are provided as command-line arguments but could also be provided
    as part of the properties file. In this case, we are specifying how this service
    is called and which kind of sampling should be done (it's okay to use the default
    parameters for the purposes of this test).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can invoke our REST service a couple of times at `http://127.0.0.1:8080/trace`,
    just to generate some traffic to display. If we then navigate to the Jaeger UI,
    available by default at `http://localhost:16686/`, we will see something similar
    (click on the **Find Traces** button and select the **test-opentracing** service,
    if necessary):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Jaeger UI home'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.2_B16354_new.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – Jaeger UI home
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, each of the calls made to our service is displayed with the
    overall time to respond (a couple of milliseconds, in our example).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we click on one of those calls, we can see the two spans:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Jaeger UI spans'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.3_B16354_new.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – Jaeger UI spans
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the main span concerns the REST call with a smaller span on
    the sub-call of the injected service. It's easy to imagine how useful it is to
    have this kind of information on an application running in production.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in this example, we have just one microservice with two methods.
    However, the same concept can be easily extended to more than one microservice
    talking to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing and metrics are part of a bigger concept called **Application Performance
    Management** (**APM**).
  prefs: []
  type: TYPE_NORMAL
- en: Application Performance Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: APM is a broad and very important aspect of running an application in production.
    It involves a lot of different technologies, and sometimes it has some unknowns
    around log aggregation, metrics collection, and overall monitoring, among other
    things.
  prefs: []
  type: TYPE_NORMAL
- en: Each vendor or stack of monitoring technologies has slightly different comprehensions
    of what APM is about, and somewhat different implementations of it as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'I think that it''s good to start from the goal: the goal of APM is to have
    insights into how a set of applications is performing, and what impact the underlying
    parameters (such as memory usage, database metrics, and more) have on the end
    user experience (such as user interface responsiveness, response times, and so
    on).'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to understand that to implement such a useful (and broad) goal, you
    may need to stack a number of different tools and frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen some of this in the previous section: you may want to have information
    coming from logs (to understand what the application is doing), together with
    metrics (to understand the resource consumption and collect other KPIs such as
    the number of calls), with health checks (to have a quick view over which service
    is up), and with tracing (to understand how each specific call is performed, including
    all the sub-calls).'
  prefs: []
  type: TYPE_NORMAL
- en: And there are a number of other tools that you can use. As an example, JVM provides
    some useful parameters (we saw some when discussing metrics) such as memory and
    CPU consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, for code that is not natively instrumented (such as legacy
    code that is not providing metrics using frameworks similar to the one seen previously),
    it is possible to collect some metrics using some more invasive approaches, such
    as Java agents, which are low-level configurations that act on the JVM to understand
    how and when each method of our code is called.
  prefs: []
  type: TYPE_NORMAL
- en: With that said, you can imagine how hard it can be to provide a unified, easy-to-read,
    overall vision of what's happening with our application. You will need to install
    and maintain a lot of different tools and glue them together in order to display
    meaningful and uniform information.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, aside from open source standards and tools, commercial solutions
    have emerged (such as **Dynatrace**, **Datadog**, and **Splunk**), which allow
    us to use ready-made stacks to provide such information.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now that it is clear how important and useful it is to have this kind of
    information, let''s look at some topics to be aware of when talking about APM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It may impact performance**: Many of the approaches seen so far have been
    designed to have as little impact as possible by using asynchronous and non-blocking
    techniques. However, especially if we use older approaches such as Java agents,
    the impact can be significant. And if you think that an APM system might be useful
    when your application is slow, it''s easy to understand that APM must be as lightweight
    as possible, to avoid putting any further pressure on systems that are already
    requested.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It requires nontrivial maintenance**: The data collected can simply be huge
    in quantity. Think about every transaction generating a bunch of metrics (timing,
    error codes, resources consumed), plus a number of lines of logs and tracing information.
    When all these metrics are multiplied by hundreds or thousands of transactions,
    it may become difficult to maintain them. Plus, as said, each specific type of
    information you might want to look for (logs, metrics, and checks) is managed
    by a different stack, hence we may end up using different servers, storage, and
    configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The information collected may be hard to correlate**: Especially in the event
    of an issue, you may want to understand whether a specific transaction caused
    the issue and how the system behaved. While tracing makes it easy to correlate
    a transaction with each sub-call and subsystem, correlating tracing information
    with logging information plus metrics and health checks will still be trouble.
    Moreover, comparing different kinds of data (such as timespans with KPIs and messages)
    can be hard, especially in user interfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, it's crucial to correlate the platform information with
    the related features implemented. In the next section, we are going to look a
    bit more into what kind of information is worth collecting, and how to categorize
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Service monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very important consideration is what to monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it's very important to collect as much data as possible, in terms of
    metrics and KPIs, as they may reveal interesting trends, and can be very useful
    if something unpredicted happens. But at the same time, business users are mostly
    interested in different kinds of metrics and information, such as the number of
    transactions per second (or per hour, or per day), the amount of money that passes
    through the platform, the number of concurrent users, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, there are two different kinds of KPIs to look for, sometimes with a
    blurred boundary between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Technical information**: Things such as the memory used, the number of threads,
    the number of connections, and so on. These things are useful for sizing and scaling
    systems and trying to forecast whether our system will perform well or some interventions
    are needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business information**: Defining what information is business information
    heavily depends on the application realm, but usually includes the average transaction
    time, the number of concurrent users, the number of new users, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a technical standpoint, you can use the same frameworks (especially ones
    for collecting metrics) in order to collect both technical and business information.
  prefs: []
  type: TYPE_NORMAL
- en: But it's very important (and not so easy to do) to try to correlate one kind
    of metric with another.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it could be useful to have a map (even simple documentation
    such as a web page can be enough) that documents where each feature is hosted,
    and how specific business information is related to a set of technical information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example: if we have a business KPI about the transaction
    time of a specific functionality, it is important to understand which servers
    provide that functionality, and which specific set of microservices (or applications)
    implements it.'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, you can link a business metric (such as the transaction time) to
    a set of technical metrics (such as the memory used by a number of JVMs, used
    threads, CPU consumption on the servers that are running such JVMs, and more).
  prefs: []
  type: TYPE_NORMAL
- en: By doing that, you can better correlate a change in performance in that particular
    feature (in our case, transactions going slower) to a specific subset of technical
    information (such as an increase in CPU usage on one particular server).
  prefs: []
  type: TYPE_NORMAL
- en: This will help in troubleshooting and quickly fixing production issues (by scaling
    the resources on impacted systems).
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than this, business metrics are simply valuable for some users: they
    may be used for forecasting the economic performance of the platform, the expected
    growth, and similar parameters. For this reason, it''s common to store such information
    on specific data stores (such as big data or data lakes), where they can be correlated
    with other information, which is analyzed and further studied.'
  prefs: []
  type: TYPE_NORMAL
- en: This completes the topics that were planned for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at some interesting considerations about monitoring
    and tracing our applications.
  prefs: []
  type: TYPE_NORMAL
- en: We started by reviewing some basic concepts about logging in Java, and why log
    aggregation is a good thing to do in microservices and cloud-native applications.
    We then moved on to the concept of metrics and health checks, and how applications
    can provide data in real time on the performance and health of our modules.
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed tracing, which is very important when it comes to troubleshooting
    and managing distributed applications (such as microservices applications). APM
    was the next topic and is about putting all the information together (such as
    metrics, health checks, and logs) to create an overview of the application insights.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we saw how service monitoring involves linking business
    information with the technical KPIs behind it, to support troubleshooting and
    draw more insights from the collected data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to see what's new in the latest version of
    the Java technology.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hanif Jetha, *How To Set Up an Elasticsearch, Fluentd and Kibana (EFK) Logging
    Stack on Kubernetes* ([https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes](https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Himanshu Shukla, *#Microservices : Observability Patterns* ([https://medium.com/@greekykhs/microservices-observability-patterns-eff92365e2a8](mailto:https://medium.com/@greekykhs/microservices-observability-patterns-eff92365e2a8))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MicroProfile Metrics* ([https://download.eclipse.org/microprofile/microprofile-metrics-2.3/microprofile-metrics-spec-2.3.html](https://download.eclipse.org/microprofile/microprofile-metrics-2.3/microprofile-metrics-spec-2.3.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The OpenTracing project* ([https://opentracing.io/](https://opentracing.io/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
