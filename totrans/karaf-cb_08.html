<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Providing a Big Data Integration Layer with Apache Cassandra"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Providing a Big Data Integration Layer with Apache Cassandra</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing Cassandra client bundles in Apache Karaf</li><li class="listitem" style="list-style-type: disc">Modeling data with Apache Cassandra</li><li class="listitem" style="list-style-type: disc">Building a project with a persistence layer for deployment in Karaf</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec68"/>Introduction</h1></div></div></div><p>As illustrated in the previous chapters, persistence is a large part of most deployments and applications. So far, we've focused on relational databases. Let's start off with some history.</p><p>In 1970, IBM published a paper named <span class="emphasis"><em>A Relational Model of Data for Large Shared Data Banks</em></span>. This paper became the foundation for RDBMS and modern relational databases in that it described joins and relationships between entities. From this work, followed SQL (1986), ACID (Atomic, Consistent, Isolated, and Durable), schema design, and sharding for scalability.</p><p>Let's fast <a id="id590" class="indexterm"/>forward to the advent of social networks; a term called <span class="strong"><strong>WebScale</strong></span> was coined based on Reed's law that states:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"The utility of large networks, particularly social networks, can scale exponentially with the size of the network."</em></span></p></blockquote></div><p>Does this mean that RDBMS cannot be scaled? No, but it led to the development of NoSQL. NoSQL is<a id="id591" class="indexterm"/> usually based on the following definitions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It was originally coined by Carlo Strozzi who developed the Strozzi NoSQL database in 1998</li><li class="listitem" style="list-style-type: disc">It typically has the key/value style of storage in columns/tables</li><li class="listitem" style="list-style-type: disc">It is generally schema-less, or each row can contain a different structure</li><li class="listitem" style="list-style-type: disc">It does not require SQL as a language; thus the name <span class="emphasis"><em>NoSQL</em></span></li><li class="listitem" style="list-style-type: disc">Many support BASE consistency</li><li class="listitem" style="list-style-type: disc">Most are <a id="id592" class="indexterm"/>distributed and fault-tolerant in nature</li></ul></div><p>Apache Cassandra<a id="id593" class="indexterm"/> was originally developed at Facebook, then released as open source in 2008, incubated at Apache in 2009, and became a top-level Apache project in 2010. Through rapid adaptation and features desirable in many use cases, Apache Cassandra has rapidly gained traction and wide distribution. Today's versions of Cassandra have a slightly stricter schema orientation with<a id="id594" class="indexterm"/> the introduction of <span class="strong"><strong>Cassandra Query Language</strong></span> (<span class="strong"><strong>CQL</strong></span>), a way of helping to drive transition from traditional RDBMS models to a more unstructured key/value pair storage model while retaining a structure and data model familiar to <a id="id595" class="indexterm"/>users in general. For an annotated history of the transition of Apache Cassandra, see <a class="ulink" href="http://www.datastax.com/documentation/articles/cassandra/cassandrathenandnow.html">http://www.datastax.com/documentation/articles/cassandra/cassandrathenandnow.html</a>.</p><p>CQL is the default and primary interface into the Cassandra DBMS. Using CQL is similar to using SQL. CQL and SQL share the same abstract idea of a table constructed of columns and rows. The main difference is that Cassandra does not support joins or subqueries, except for batch analysis through Apache Hive. Instead, Cassandra emphasizes denormalization through CQL features like collections and clustering specified at the schema level.</p><p>What this basically means is that there are other client APIs—they are, as of Cassandra release 2.x, actively discouraged from use by the Cassandra community. A healthy debate over the usage of schema modeling versus column family is still quite active on mailing lists and in user communities.</p></div></div>
<div class="section" title="Installing Cassandra client bundles in Apache Karaf"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec69"/>Installing Cassandra client bundles in Apache Karaf</h1></div></div></div><p>Before <a id="id596" class="indexterm"/>we begin to explore how <a id="id597" class="indexterm"/>to build Cassandra-backed applications, we must first install all the required client modules into the Karaf container.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec209"/>Getting ready</h2></div></div></div><p>The official <span class="emphasis"><em>GettingStarted</em></span> document from the Cassandra community can be found at <a class="ulink" href="http://wiki.apache.org/cassandra/GettingStarted">http://wiki.apache.org/cassandra/GettingStarted</a>.</p><p>The ingredients of this recipe include the Apache Karaf distribution kit, access to JDK, and Internet connectivity. We also assume that an Apache Cassandra database is downloaded and installed. Apache Cassandra can be downloaded and installed as RPMs, Debian <code class="literal">.deb</code> packages, or <code class="literal">.tar</code> archives. In a binary <code class="literal">.tar</code> archive, you'll have to open and change two configuration files: the <code class="literal">cassandra.yaml</code> and the <code class="literal">log4-server.properties</code> files of the <code class="literal">conf</code> folder. The changes pertain to where you store data, which is by default in the <code class="literal">/var/lib/cassandra/*</code> folder for the data backend and in the <code class="literal">/var/log/cassandra/*</code> folder for the system log. Once these changes are done, you can start Cassandra using the <code class="literal">bin/cassandra –F</code> command.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec210"/>How to do it…</h2></div></div></div><p>Cassandra's drivers aren't part of the standard Karaf feature library; so, we either have to write <a id="id598" class="indexterm"/>our own feature or manually<a id="id599" class="indexterm"/> install the necessary bundles for the client to run.</p><p>We use the following commands to install Apache Cassandra's driver and supplemental bundles into Karaf:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt;install -s mvn:io.netty/netty/3.9.0.Final</strong></span>
<span class="strong"><strong>karaf@root()&gt;install -s mvn:com.google.guava/guava/16.0.1</strong></span>
<span class="strong"><strong>karaf@root()&gt;install -s mvn:com.codahale.metrics/metrics-core/3.0.2</strong></span>
<span class="strong"><strong>karaf@root()&gt;install -s mvn:com.datastax.cassandra/cassandra-driver-core/2.0.2</strong></span>
</pre></div><p>We can verify the installation by executing the <code class="literal">list -t 0 | grep -i cass</code> command, which will list the DataStax driver bundle.</p><p>With this, we have access to the Cassandra driver from our own bundles.</p></div></div>
<div class="section" title="Modeling data with Apache Cassandra"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec70"/>Modeling data with Apache Cassandra</h1></div></div></div><p>Before<a id="id600" class="indexterm"/> we start writing a bundle using Apache<a id="id601" class="indexterm"/> Cassandra, let's look a little at how we model data in Cassandra using CQL 3.x.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec211"/>Getting ready</h2></div></div></div><p>Let's define a very simple schema, and as we are using CQL, Cassandra isn't schema-less from a client perspective even if the data storage internally works slightly differently.</p><p>We can reuse the <code class="literal">RecipeService</code> class from the previous chapter. We will just modify it slightly for our Cassandra integration. The original entity (and by virtue of using JPA) provides a basic table definition, which is as follows:</p><div class="informalexample"><pre class="programlisting">@Id
@Column(nullable = false)
private String title;

@Column(length=10000)
private String ingredients;</pre></div><p>So, we have two fields in this table: an ID field named <code class="literal">title</code> and a data field we call <code class="literal">ingredients</code> for consistency and simplicity.</p><p>First, we<a id="id602" class="indexterm"/> need a place to store this. Cassandra <a id="id603" class="indexterm"/>partitions data in keyspaces at the top level. Think of a keyspace as a map containing tables and their rules.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec212"/>How to do it…</h2></div></div></div><p>We'll need to perform the following two steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is starting the Cassandra client. The basic creation command in Cassandra's client, <code class="literal">cqlsh</code>, is shown as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./bin/cqlsh</strong></span>
<span class="strong"><strong>Connected to Cluster at localhost:9160.</strong></span>
<span class="strong"><strong>[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0]</strong></span>
<span class="strong"><strong>Use HELP for help.</strong></span>
</pre></div></li><li class="listitem">The next step is creating our data store. Now that we have started the interactive client session, we can create a keyspace as shown in the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; CREATE KEYSPACE karaf_demo WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};</strong></span>
</pre></div><p>The previous command-line prompt returns no response indicating that we have successfully created a new keyspace where we can store data. To use this keyspace, we need to tell Cassandra that this is where we'll be working right now.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip36"/>Tip</h3><p>We rely on SimpleStrategy as we only have one Cassandra node, we don't have a real cluster defined, nor do we have multiple data centers. If this was the case, we could change the strategy class for replication. We also set the <code class="literal">replication_factor</code> value to <code class="literal">1</code>; this can be set to more replicas and certainly should be done in security-related contexts where you, for instance, store account information.</p></div></div><p>To switch the keyspace, we issue a <code class="literal">USE</code> command as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; USE karaf_demo;</strong></span>
<span class="strong"><strong>cqlsh:karaf_demo&gt;</strong></span>
</pre></div><p>The preceding command prompt indicates that we are in the <code class="literal">karaf_demo</code> keyspace. Consider the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh:karaf_demo&gt; DESCRIBE tables;</strong></span>

<span class="strong"><strong>&lt;empty&gt;</strong></span>

<span class="strong"><strong>cqlsh:karaf_demo&gt;</strong></span>
</pre></div><p>As the preceding command indicates, we have nothing defined schema-wise in this keyspace, and so we need to define a table. This can be done as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh:karaf_demo&gt; CREATE TABLE RECIPES (title text PRIMARY KEY,ingredients text);</strong></span>
<span class="strong"><strong>cqlsh:karaf_demo&gt; DESCRIBE TABLES;</strong></span>

<span class="strong"><strong>recipes</strong></span>

<span class="strong"><strong>cqlsh:karaf_demo&gt;</strong></span>
</pre></div><p>We now have a defined table and the columns in this table are defined as the storage type <code class="literal">text</code> and the primary key and retrieval token as the <code class="literal">title</code>.</p></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec213"/>How it works…</h2></div></div></div><p>It is<a id="id604" class="indexterm"/> beyond the scope of this book to explore <a id="id605" class="indexterm"/>the under-the-hood functioning of Apache Cassandra. However, you can examine its source code at <a class="ulink" href="http://git-wip-us.apache.org/repos/asf/cassandra.git">http://git-wip-us.apache.org/repos/asf/cassandra.git</a>.</p><p>In terms of our data store, we can let Cassandra describe exactly what is being stored. Consider the following command-line snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh:karaf_demo&gt; DESCRIBE TABLE recipes;</strong></span>

<span class="strong"><strong>CREATE TABLE recipes (</strong></span>
<span class="strong"><strong>  title text,</strong></span>
<span class="strong"><strong>  ingredients text,</strong></span>
<span class="strong"><strong>  PRIMARY KEY (title)</strong></span>
<span class="strong"><strong>) WITH</strong></span>
<span class="strong"><strong>  bloom_filter_fp_chance=0.010000 AND</strong></span>
<span class="strong"><strong>  caching='KEYS_ONLY' AND</strong></span>
<span class="strong"><strong>  comment='' AND</strong></span>
<span class="strong"><strong>  dclocal_read_repair_chance=0.000000 AND</strong></span>
<span class="strong"><strong>  gc_grace_seconds=864000 AND</strong></span>
<span class="strong"><strong>  index_interval=128 AND</strong></span>
<span class="strong"><strong>  read_repair_chance=0.100000 AND</strong></span>
<span class="strong"><strong>  replicate_on_write='true' AND</strong></span>
<span class="strong"><strong>  populate_io_cache_on_flush='false' AND</strong></span>
<span class="strong"><strong>  default_time_to_live=0 AND</strong></span>
<span class="strong"><strong>  speculative_retry='NONE' AND</strong></span>
<span class="strong"><strong>  memtable_flush_period_in_ms=0 AND</strong></span>
<span class="strong"><strong>  compaction={'class': 'SizeTieredCompactionStrategy'} AND</strong></span>
<span class="strong"><strong>  compression={'sstable_compression': 'LZ4Compressor'};</strong></span>
</pre></div><p>As <a id="id606" class="indexterm"/>you can see, there are quite a few options <a id="id607" class="indexterm"/>you, as a data modeler, can work with. These will affect replication, caching, lifetime, compression, and several other factors.</p></div></div>
<div class="section" title="Building a project with a persistence layer for deployment in Karaf"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec71"/>Building a project with a persistence layer for deployment in Karaf</h1></div></div></div><p>Application <a id="id608" class="indexterm"/>developers often need to make use <a id="id609" class="indexterm"/>of a persistence layer in their projects; one of the preferred methodologies to perform this in Karaf is to make use of the Java Persistence API. As we are building on the existing JPA project, we will try and set up a new service layer and reuse (copy) the same project model while moving over the storage backend to Apache Cassandra. This is not a complete refactoring nor a reuse of code. Technically, we could have moved the API parts from <a class="link" href="ch07.html" title="Chapter 7. Providing a Persistence Layer with Apache Aries and OpenJPA">Chapter 7</a>, <span class="emphasis"><em>Providing a Persistence Layer with Apache Aries and OpenJPA</em></span>, into a new module and then refactored the chapter to have the Cassandra-related dependencies and a slightly different set of imports. This isn't really in the scope of a Cookbook, hence the copied structure.</p><p>In the <span class="emphasis"><em>Installing Cassandra client bundles in Apache Karaf</em></span> recipe, we learned how to install the necessary JAR files into Karaf. Continuing with this recipe, we'll make use of the drivers to build a simple application that persists recipes to a database using the <code class="literal">RecipeBookService</code> class, which will hide the complexities of data storage and retrieval from its users.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec214"/>Getting ready</h2></div></div></div><p>The ingredients of this recipe include the Apache Karaf distribution kit, access to JDK, and Internet connectivity. The sample code for this recipe is available at <a class="ulink" href="https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter8/chapter8-recipe1">https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter8/chapter8-recipe1</a>. Remember, you need both the drivers installed and Apache Cassandra running for these recipes to work!</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec215"/>How to do it…</h2></div></div></div><p>Building<a id="id610" class="indexterm"/> a project with a JPA persistence <a id="id611" class="indexterm"/>layer will require the following eight steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is generating a Maven-based bundle project. Create an empty Maven-based project. A <code class="literal">pom.xml</code> file containing the essential Maven coordinate information and bundle packaging directives will suffice.</li><li class="listitem">The next step is adding dependencies to the POM file. This is shown in the following code:<div class="informalexample"><pre class="programlisting">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.osgi&lt;/groupId&gt;
    &lt;artifactId&gt;org.osgi.core&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.osgi&lt;/groupId&gt;
    &lt;artifactId&gt;org.osgi.compendium&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.osgi&lt;/groupId&gt;
    &lt;artifactId&gt;org.osgi.enterprise&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;!-- Cassandra Driver --&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
    &lt;version&gt;2.0.1&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;!-- custom felix gogo command --&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.karaf.shell&lt;/groupId&gt;
    &lt;artifactId&gt;org.apache.karaf.shell.console&lt;/artifactId&gt;
    &lt;version&gt;3.0.0&lt;/version&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;</pre></div><p>For Karaf 3.0.0, we use the OSGi Version 5.0.0. The Cassandra driver only depends on three external projects. We are in luck as all of these are available as bundles—there isn't going to be much difficulty in deploying this in any version of Karaf. The majority of our dependencies are now related to our commands.</p></li><li class="listitem">The <a id="id612" class="indexterm"/>next step is adding build <a id="id613" class="indexterm"/>plugins. Our recipe requires only one build plugin to be configured, which is the bundle plugin. We configure the <code class="literal">maven-bundle-plugin</code> to assemble our project code into an OSGi bundle. We add the following plugin configuration to our POM file:<div class="informalexample"><pre class="programlisting">&lt;plugin&gt;
  &lt;groupId&gt;org.apache.felix&lt;/groupId&gt;
  &lt;artifactId&gt;maven-bundle-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.4.0&lt;/version&gt;
  &lt;extensions&gt;true&lt;/extensions&gt;
  &lt;configuration&gt;
    &lt;instructions&gt;
      &lt;Bundle-SymbolicName&gt;
              ${project.artifactId}
      &lt;/Bundle-SymbolicName&gt;
      &lt;Bundle-Activator&gt;
              com.packt.cassandra.demo.Activator
      &lt;/Bundle-Activator&gt;
      &lt;Export-Package&gt;
              com.packt.cassandra.demo.api.*
      &lt;/Export-Package&gt;
      &lt;Import-Package&gt;
        org.osgi.service.blueprint;resolution:=optional,
        org.apache.felix.service.command,
        org.apache.felix.gogo.commands,
        org.apache.karaf.shell.console,
        *
      &lt;/Import-Package&gt;
    &lt;/instructions&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;</pre></div><p>The Felix and Karaf imports are required by the optional Karaf commands.</p><p>Once again, compared to the JPA project, we have less complexity in the <code class="literal">pom.xml</code> file as we are relying on fewer external resources. We only make sure that we have the correct Karaf and Felix imports to activate our commands.</p></li><li class="listitem">The <a id="id614" class="indexterm"/>next<a id="id615" class="indexterm"/> step is creating a Blueprint descriptor file. Create the directory tree as <code class="literal">src/main/resources/OSGI-INF</code> in your project. Then, create a file named <code class="literal">blueprint.xml</code> in this folder. Consider the following code:<div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;
&lt;blueprint default-activation="eager"
&gt;
  &lt;!-- Define RecipeBookService Services, and expose them. --&gt;
  &lt;bean id="recipeBookService" class="com.packt.cassandra.demo.dao.RecipeBookServiceDAOImpl" init-method="init" destroy-method="destroy"/&gt;

  &lt;service ref="recipeBookService" interface="com.packt.cassandra.demo.api.RecipeBookService"/&gt;

  &lt;!-- Apache Karaf Commands --&gt;
  &lt;command-bundle &gt;
    &lt;command&gt;
      &lt;action class="com.packt.cassandra.demo.commands.AddRecipe"&gt;
        &lt;property name="recipeBookService" ref="recipeBookService"/&gt;
      &lt;/action&gt;
    &lt;/command&gt;
    &lt;command&gt;
      &lt;action class="com.packt.cassandra.demo.commands.RemoveRecipe"&gt;
        &lt;property name="recipeBookService" ref="recipeBookService"/&gt;
      &lt;/action&gt;
    &lt;/command&gt;
    &lt;command&gt;
      &lt;action class="com.packt.cassandra.demo.commands.ListRecipes"&gt;
        &lt;property name="recipeBookService" ref="recipeBookService"/&gt;
      &lt;/action&gt;
    &lt;/command&gt;
  &lt;/command-bundle&gt;
&lt;/blueprint&gt;</pre></div><p>In the preceding Blueprint structure, we ended up removing the transaction manager. Remember, Cassandra doesn't work in the same way as a traditional relational database.</p></li><li class="listitem">The <a id="id616" class="indexterm"/>next step is developing<a id="id617" class="indexterm"/> an OSGi service with a new Cassandra backend. We've created the basic project structure and plumbed in configurations for Blueprint descriptors. Now, we'll focus on the underlying Java code of our Cassandra-backed application. We break down this process into three steps: defining a service interface, implementing the service DAO, and implementing a very simple class that we will use as an entity.<p>In this recipe, we will be using the raw CQL driver in a more extensive project. It is highly likely that something like a DataMapper or another ORM-like solution will be of benefit. A higher level library will help hide type conversion between Cassandra and Java, manage relations, and help generalize data access.</p><p>The following is a brief list of CQL-friendly libraries to use as a starting point:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Astyanax</strong></span>: This<a id="id618" class="indexterm"/> is available at <a class="ulink" href="https://github.com/Netflix/astyanax">https://github.com/Netflix/astyanax</a></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spring data-cassandra</strong></span>: This is <a id="id619" class="indexterm"/>available at <a class="ulink" href="http://projects.spring.io/spring-data-cassandra/">http://projects.spring.io/spring-data-cassandra/</a></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hecate</strong></span>: This<a id="id620" class="indexterm"/> is available at <a class="ulink" href="https://github.com/savoirtech/hecate">https://github.com/savoirtech/hecate</a></li></ul></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is defining a service interface. The service interface will define the user API to our project. In our sample code, we implement the <code class="literal">RecipeBookService</code> class, which provides the methods required to interact with a collection of recipes. Consider the following code:<div class="informalexample"><pre class="programlisting">package com.packt.cassandra.demo.api;

import java.util.Collection;
import com.packt.jpa.demo.entity.Recipe;

public interface RecipeBookService {

   public Collection&lt;Recipe&gt; getRecipes();

   public void addRecipe(String title, String ingredients);

   public void deleteRecipe(String title);

}</pre></div><p>The interface's implementation follows standard Java conventions, requiring no special OSGi packages.</p></li><li class="listitem">The next step is implementing<a id="id621" class="indexterm"/> the<a id="id622" class="indexterm"/> service DAO. Now that we have defined our service interface, we'll provide an implementation as a DAO. By using Cassandra, we don't necessarily need to follow a DAO pattern, although this isn't a bad idea, as this will make refactoring existing JPA code fairly simple and approachable. We use the same interface and accommodate the implementation to work with the Cassandra CQL syntax instead. Consider the following code:<div class="informalexample"><pre class="programlisting">public class RecipeBookServiceDAOImpl implements RecipeBookService {

  private Cluster cluster;
  private Session session;

  public void connect(String node) {
    cluster = Cluster.builder().addContactPoint(node).build();
    Metadata metadata = cluster.getMetadata();
    System.out.printf("Connected to cluster: %s\n", metadata.getClusterName());
    for (Host host : metadata.getAllHosts()) {
      System.out.printf("Datatacenter: %s; Host: %s; Rack: %s\n", host.getDatacenter(), host.getAddress(), host.getRack());
    }
    session = cluster.connect("karaf_demo");
  }

  public void destroy() {
    cluster.close();
  }

  public void init() {
    connect("127.0.0.1");
  }

  @Override
  public List&lt;Recipe&gt; getRecipes() {
    List&lt;Recipe&gt; result = new ArrayList&lt;Recipe&gt;();
    ResultSet results = session.execute("SELECT * FROM karaf_demo.recipes;");

    for (Row row : results) {
      Recipe recipe = new Recipe();
      recipe.setTitle(row.getString("title"));
      recipe.setIngredients(row.getString("ingredients"));
      result.add(recipe);
    }
    return result;
  }

  @Override
  public void addRecipe(String title, String ingredients) {

    ResultSet resultSet = session.execute("INSERT INTO karaf_demo.recipes (title, ingredients) VALUES ('" + title + "', '" + ingredients + "');");
    System.out.println("Result = " + resultSet);
  }

  @Override
  public void deleteRecipe(String title) {
    ResultSet resultSet = session.execute("DELETE from karaf_demo.recipes where title='" + title + "';");
    System.out.println("Result = " + resultSet);
  }
}</pre></div><p>Our Cassandra class is now pretty much self-contained. On startup, we print out some information about where we are connected and which rack and datacenter our cluster node exists in.</p></li><li class="listitem">The <a id="id623" class="indexterm"/>next step is implementing entities. These entities in the Cassandra case are just plain old POJO classes. They no longer contain persistence information. We utilize them to conform to the existing API and ensure that we hide the underlying Cassandra implementation from the end user. Consider the following code:<div class="informalexample"><pre class="programlisting">public class Recipe {
    private String title;
    private String ingredients;
    public Recipe() {
    }
    public Recipe(String title, String ingredients) {
        super();
        this.title = title;
        this.ingredients = ingredients;
    }
    public String getTitle() {
        return title;
    }
    public void setTitle(String title) {
        this.title = title;
    }
    public String getIngredients() {
        return ingredients;
    }
    public void setIngredients(String ingredients) {
        this.ingredients = ingredients;
    }
    public String toString() {
        return "" + this.title + " " + this.ingredients;
    }
}</pre></div><p>This<a id="id624" class="indexterm"/> class is now just a plain old POJO class that we use to transport data in accordance with the already established API. For more complex mapping and ORM-like structure, there are several DataMappers that are built on top of the existing Cassandra data drivers. As they are not standardized as JPA, recommending one in particular isn't as easy. They all have their little quirks.</p></li></ol></div></li><li class="listitem">The next step is the optional creation of Karaf commands to directly test the persistence service. To simplify manual testing of our <code class="literal">recipeBookService</code> instance, we can create a set of custom Karaf commands, which will invoke our Cassandra storage and retrieval operations. The sample implementations of these optional commands are available in the book's code bundle. Of particular interest is how they obtain a reference to the <code class="literal">recipeBookService</code> instance and<a id="id625" class="indexterm"/> make <a id="id626" class="indexterm"/>calls to the service.<p>Now, we must wire the command implementation into Karaf via Blueprint as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&lt;!-- Apache Karaf Commands --&gt;
&lt;command-bundle &gt;
  &lt;command&gt;
    &lt;action class="com.packt.cassandra.demo.commands.AddRecipe"&gt;
      &lt;property name="recipeBookService" ref="recipeBookService"/&gt;
    &lt;/action&gt;
  &lt;/command&gt;
  &lt;command&gt;
    &lt;action class="com.packt.cassandra.demo.commands.RemoveRecipe"&gt;
      &lt;property name="recipeBookService" ref="recipeBookService"/&gt;
    &lt;/action&gt;
  &lt;/command&gt;
  &lt;command&gt;
    &lt;action class="com.packt.cassandra.demo.commands.ListRecipes"&gt;
      &lt;property name="recipeBookService" ref="recipeBookService"/&gt;
    &lt;/action&gt;
  &lt;/command&gt;
&lt;/command-bundle&gt;</pre></div><p>Each of our custom commands implementation classes are wired to our <code class="literal">recipeBookService</code> instance.</p></li><li class="listitem">The next step is deploying the project into Karaf. We install our project bundle by executing the <code class="literal">install</code> command on its Maven coordinates as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt;  install –s mvn:com.packt/chapter8-recipe1/1.0.0-SNAPSHOT</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note137"/>Note</h3><p>This demo will require a running instance of Cassandra!</p></div></div></li><li class="listitem">The final step is testing the project. As soon as the bundle is deployed, you'll see some startup information; it'll be something like the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt; Cassandra Demo Bundle stopping...</strong></span>
<span class="strong"><strong>Cassandra Demo Bundle starting...</strong></span>
<span class="strong"><strong>Connected to cluster: Cluster</strong></span>
<span class="strong"><strong>Datatacenter: datacenter1; Host: /127.0.0.1; Rack: rack1</strong></span>
</pre></div><p>Consider the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt; test:addrecipe "Name" "Ingredients"</strong></span>
<span class="strong"><strong>karaf@root()&gt; test:listrecipes</strong></span>
</pre></div><p>When you run the preceding commands, you'll have the stored recipe displayed on the console as output!</p></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec216"/>How it works…</h2></div></div></div><p>Our<a id="id627" class="indexterm"/> persistence layer works with the official<a id="id628" class="indexterm"/> Apache Cassandra driver in a Karaf container. Providing access to Cassandra is quite simple; we only need to look at one external project and make sure that we have two dependencies and that we can connect to an existing Cassandra cluster. The key to using Cassandra lies more in data modeling, structuring of clusters, amount of read/writes, and the size of clusters and participating nodes and their usage and storage patterns.</p><p>The data driver we connect with will provide us with cluster awareness, failover, and high availability as well as the features necessary to control replication factors and cluster behavior <a id="id629" class="indexterm"/>of our data. The official Cassandra documentation can be found at <a class="ulink" href="http://cassandra.apache.org/">http://cassandra.apache.org/</a>.</p><p>An extremely useful resource to get the historical context of the evolution of Cassandra can be found at <a class="ulink" href="http://www.datastax.com/documentation/articles/cassandra/cassandrathenandnow.html">http://www.datastax.com/documentation/articles/cassandra/cassandrathenandnow.html</a>. This will show and explain why things have changed, how CQL came about, and why certain designs were chosen.</p></div></div></body></html>