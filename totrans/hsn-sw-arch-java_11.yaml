- en: 'Chapter 9: Designing Cloud-Native Architectures'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：设计云原生架构
- en: Nowadays, the microservices architectural model is mainstream. At the time of
    writing, we are likely in the *Trough of Disillusionment*. This widespread terminology
    comes from the Gartner Hype Cycle model and is a way of identifying phases in
    the adoption of technology, starting from bleeding edge and immaturity and basically
    going through to commodity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，微服务架构模型已成为主流。在撰写本文时，我们可能正处于“幻灭的低谷”。这个广泛使用的术语来自Gartner的炒作周期模型，它是一种识别技术采用阶段的方法，从尖端和未成熟阶段，基本上过渡到商品阶段。
- en: This means, in my opinion, that even if we are starting to recognize some disadvantages,
    microservices are here to stay. However, in this chapter, I would like to broaden
    the point of view and look at the so-called cloud-native architectures. Don't
    get confused by the term *cloud*, as these kinds of architectures don't necessarily
    require a public cloud to run (even if one cloud, or better, many clouds, is the
    natural environment for this kind of application).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这意味着即使我们开始认识到一些不利因素，微服务仍然会持续存在。然而，在本章中，我想拓宽观点，看看所谓的云原生架构。不要被“云”这个词所迷惑，因为这类架构不一定需要公共云来运行（即使一个云，或者更好的是多个云，是这类应用的天然环境）。
- en: A cloud-native architecture is a way to build resistant, scalable infrastructure
    able to manage traffic peaks with little to no impact and to quickly evolve and
    add new features by following an Agile model such as **DevOps**. However, a cloud-native
    architecture is inherently complex, as it requires heavy decentralization, which
    is not a matter that can be treated lightly. In this chapter, we will see some
    concepts regarding the design and implementation of cloud-native architectures.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生架构是一种构建具有抵抗力和可扩展性基础设施的方法，能够以几乎无影响的方式管理流量峰值，并通过遵循敏捷模型如**DevOps**快速演进和添加新功能。然而，云原生架构本质上很复杂，因为它需要高度去中心化，这不是一个可以轻视的问题。在本章中，我们将看到一些关于云原生架构设计和实现的概念。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解以下主题：
- en: Why create cloud-native applications?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要创建云原生应用？
- en: Learning about types of cloud service models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解云服务模型类型
- en: Defining twelve-factor applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义十二要素应用
- en: Well-known issues in the cloud-native world
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云原生世界中的常见问题
- en: Adopting microservices and evolving existing applications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用微服务并演进现有应用
- en: Going beyond microservices
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越微服务
- en: Refactoring apps as microservices and serverless
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用重构为微服务和无服务器
- en: That's a lot of interesting stuff, and we will see that many of these concepts
    can help improve the quality of the applications and services you build, even
    if you are in a more traditional, less cloud-oriented setup. I am sure that everybody
    reading this chapter already has an idea, maybe a detailed idea, of what a cloud-native
    application is. However, after reading this chapter, this idea will become more
    and more structured and complete.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这有很多有趣的内容，我们将看到许多这些概念可以帮助提高你构建的应用和服务质量，即使你处于更传统、云服务导向性较弱的设置中。我相信，阅读这一章的每个人已经对云原生应用有一个概念，可能是一个详细的概念。然而，在阅读完这一章后，这个概念将变得更加结构化和完整。
- en: So, to start the chapter, we are going to better elaborate on what a cloud-native
    application is, what the benefits are, and some tools and principles that will
    help us in building one (and achieving such benefits).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了开始本章，我们将更详细地阐述什么是云原生应用，它的好处，以及一些有助于我们构建云原生应用（并实现这些好处）的工具和原则。
- en: Why create cloud-native applications?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要创建云原生应用？
- en: Undoubtedly, when defining cloud-native, there are a number of different nuances
    and perspectives that tackle the issue from different perspectives and with different
    levels of detail, ranging from technical implications to organizational and business
    impacts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，在定义云原生时，有众多不同的细微差别和视角，从不同的角度和不同层次的细节来处理这个问题，从技术影响延伸到组织和企业影响。
- en: 'However, in my opinion, a cloud-native application (or architecture, if you
    want to think in broader terms) must be designed to essentially achieve three
    main goals (somewhat interrelated):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我看来，云原生应用（或者如果你愿意从更广泛的角度思考，架构）必须设计成本质上实现三个主要目标（某种程度上相互关联）：
- en: '**Scalability** is, of course, immediately related to being able to absorb
    a higher load (usually because of more users coming to our service) with no disruption
    and that''s a fundamental aspect. However, scalability also means, in broader
    terms, that the application must be able to downscale (hence, reducing costs)
    when less traffic is expected, and this may imply the ability to run with minimal
    or no changes in code on top of different environments (such as on-premises and
    the public cloud, which may be provided by different vendors).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**当然与能够吸收更高的负载（通常是因为更多的用户来到我们的服务）而没有中断直接相关，这是一个基本方面。然而，从更广泛的角度来看，可扩展性还意味着应用程序必须能够在预期流量较少时进行降级（因此降低成本），这可能意味着能够在不同的环境中运行，而无需在代码上进行最小或无更改（例如在本地和公共云上，这些可能由不同的供应商提供）。'
- en: '**Modularity** is about the application being organized in self-contained,
    modular components, which must be able to interoperate with each other and be
    replaced by other components wherever needed. This has huge impacts on the other
    two points (scalability and resiliency). A modular application is likely to be
    scalable (as you can increase the number of instances of a certain module that
    is suffering from a load) and can be easily set up on different infrastructures.
    Also, it may be using the different backing services provided by each infrastructure
    (such as a specific database or filesystem), thereby increasing both scalability
    and resiliency.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**是指应用程序被组织成自包含的、模块化的组件，这些组件必须能够相互操作，并在需要时被其他组件替换。这对其他两个点（可扩展性和弹性）有巨大影响。模块化应用程序很可能是可扩展的（因为你可以增加某个模块的实例数量，该模块正在承受负载），并且可以轻松地部署在不同的基础设施上。此外，它可能正在使用每个基础设施提供的不同备份服务（例如特定的数据库或文件系统），从而提高可扩展性和弹性。'
- en: '**Resiliency**, as we just mentioned, can be defined as being able to cope
    with unpredicted events. Such events include application crashes, bugs in the
    code, external (or backing) services misbehaving, or infrastructure/hardware failures.
    A cloud-native application must be designed in a way that avoids or minimizes
    the impact of such issues on the user experience. There are a number of ways to
    address those scenarios and we will see some in this section. Resiliency includes
    being able to cope with unforeseen traffic spikes, hence it is related to scalability.
    Also, as has been said, resiliency can also be improved by structuring the overall
    application in modular subsystems, optionally running in multiple infrastructures
    (minimizing single-point-of-failure problems).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性**，正如我们刚才提到的，可以被定义为能够应对不可预见的事件。这些事件包括应用程序崩溃、代码中的错误、外部（或备份）服务行为不当，或基础设施/硬件故障。一个云原生应用程序必须以避免或最小化这些问题对用户体验的影响的方式进行设计。有几种方法可以解决这些场景，我们将在本节中看到一些。弹性包括能够应对不可预见的流量峰值，因此它与可扩展性相关。此外，正如所说，通过将整体应用程序结构化为模块化子系统，可以选择在多个基础设施上运行（最小化单点故障问题），也可以提高弹性。'
- en: 'As a cloud-native architect, it is very important to see the business benefits
    provided by such (preceding) characteristics. Here are the most obvious ones,
    for each point:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为云原生架构师，看到这种（前面提到的）特性带来的商业效益非常重要。以下是每个点的最明显效益：
- en: '**Scalability** implies that a system can behave better under stress (possibly
    with no impacts), but also has predictable costs, meaning a higher cost when more
    traffic comes and a lower one when it''s not needed. In other words, the cost
    model scales together with the requests coming into the system. Eventually, a
    scalable application will cost less than a non-scalable one.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**意味着系统在压力下（可能没有影响）表现更好，同时具有可预测的成本，这意味着当流量增加时成本更高，而当不需要时成本更低。换句话说，成本模型与进入系统的请求一起扩展。最终，可扩展的应用程序的成本将低于不可扩展的应用程序。'
- en: '**Modularity** will positively impact system maintainability, meaning reduced
    costs as regards changes and the evolution of the system. Moreover, a properly
    modularized system will facilitate the development process, reducing the time
    needed for releasing fixes and new features in production, and likely reducing
    the time to market. Last but not least, a modular system is easier to test.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化**将积极影响系统可维护性，这意味着在系统更改和演变方面的成本降低。此外，一个正确模块化的系统将促进开发过程，减少在生产中发布修复和新功能所需的时间，并可能减少上市时间。最后但同样重要的是，模块化系统更容易进行测试。'
- en: '**Resiliency** means a higher level of availability and performance of the
    service, which in turn means happier customers, a better product, and overall,
    a better quality of the user experience.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性**意味着服务具有更高的可用性和性能水平，这反过来意味着更满意的客户、更好的产品，以及整体上更好的用户体验质量。'
- en: While cloud-native is a broad term, implying a big number of technical characteristics,
    benefits, and technological impacts, I think that the points that we have just
    seen nicely summarize the core principles behind the cloud-native concept.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然云原生是一个广泛的术语，暗示着大量的技术特性、优势和科技影响，但我认为我们刚刚看到的要点很好地总结了云原生概念背后的核心原则。
- en: 'Now, it''s hard to give a perfect recipe to achieve each of those goals (and
    benefits). However, in this chapter, we are going to link each point with some
    suggestions on how to achieve it:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很难给出一个完美的食谱来实现每个目标（和好处）。然而，在本章中，我们将把每个要点与一些关于如何实现它的建议联系起来：
- en: '**PaaS** is an infrastructural paradigm providing support services, among other
    benefits, for building scalable infrastructures.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PaaS**是一种基础设施范式，提供支持服务，以及其他好处，以构建可扩展的基础设施。'
- en: The **twelve-factor apps** are a set of principles that assist in building modular
    applications.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**十二要素应用**是一套原则，有助于构建模块化应用程序。'
- en: '**Cloud-native patterns** are a well-known methodology (also implemented in
    the MicroProfile specification) for building resilient applications.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云原生模式**是构建弹性应用程序的一种知名方法（也在MicroProfile规范中得到实现）。'
- en: In the next section, we are going to define what PaaS is and how our application
    is going to benefit from it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将定义PaaS是什么以及我们的应用程序将如何从中受益。
- en: Learning about types of cloud service models
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解云服务模型类型
- en: Nowadays, it is common to refer to modern, cloud-native architectures by means
    of a number of different terms and acronyms. The *as a service* phrase is commonly
    used, meaning that every resource should be created and disposed of on-demand,
    automatically. *Everything as a service* is a wider term for this kind of approach.
    Indeed, with cloud computing and microservices, applications can use the resources
    of a swarm (or a cloud, if you want) of smaller components cooperating in a network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，用许多不同的术语和缩写来指代现代、云原生架构是很常见的。"作为服务"这个短语被广泛使用，意味着每个资源都应该按需自动创建和销毁。"一切作为服务"是这种方法的更广泛术语。实际上，随着云计算和微服务的发展，应用程序可以使用网络中较小组件的资源（或者如果你愿意，就是一个云）。
- en: However, such architectures are hard to design and maintain because, in the
    real world, the network is basically considered unreliable or at least has non-predictable
    performances. Even if the network behaves correctly, you will still end up with
    a lot of *moving parts* to develop and manage in order to provide core features,
    such as deploying and scaling. A common tool for addressing those issues is PaaS.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样的架构很难设计和维护，因为在现实世界中，网络基本上被认为是不可靠的，或者至少具有不可预测的性能。即使网络表现正确，你仍然需要开发和管理大量的*移动部件*来提供核心功能，例如部署和扩展。解决这些问题的常用工具是PaaS。
- en: 'PaaS is an inflated term, or, better yet, every *as a service* term is overused,
    and sometimes there is no exact agreement and definition of the meaning and the
    boundaries between each *as a service* set of tools. This is my personal view
    regarding a small set of *as a service* layers (that can be regarded as common
    sense, and indeed is widely adopted):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PaaS是一个夸张的术语，或者，更好的说法是，每个*作为服务*的术语都被过度使用，有时在每种*作为服务*工具集的意义和边界上没有确切的一致和定义。这是我关于一组*作为服务*层（可以被视为常识，并且确实被广泛采用）的个人观点：
- en: '**Infrastructure as a Service** (**IaaS**) refers to a layer providing *on-demand*
    computational resources needed to run workloads. This implies **Virtual Machines**
    (**VMs**) (or physical servers) can network to make them communicate and store
    persistent data. It does not include anything above the OS; once you get your
    machines, you will have to install everything needed by your applications (such
    as a **Java Virtual Machine** (**JVM**), application servers, and dependencies).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施即服务**（**IaaS**）指的是提供运行工作负载所需的*按需*计算资源的层。这暗示**虚拟机**（**VMs**）（或物理服务器）可以联网以实现通信和存储持久数据。它不包括操作系统之上的任何内容；一旦你得到你的机器，你将不得不安装应用程序所需的所有内容（例如**Java虚拟机**（**JVM**）、应用服务器和依赖项）。'
- en: '**Platform as a Service** (**PaaS**) is used for a layer that abstracts most
    of the infrastructure details and provides services that are useful for the application
    to be built and run. So, in PaaS, you can specify the runtimes (VM, dependencies,
    and servers) needed by your application and the platform will provide it for you
    to use.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台即服务**（**PaaS**）用于抽象大多数基础设施细节并提供对构建和运行应用程序有用的服务。因此，在PaaS中，您可以指定应用程序需要的运行时（虚拟机、依赖项和服务器），平台将为您提供服务。'
- en: PaaS could also abstract other concepts for you, such as storage (by providing
    object storage, or other storage services for you to use), security, serverless,
    and build facilities (such as CI/CD). Last but not least, PaaS provides tools
    for supporting the upscale and downscale of the hosted applications. Most PaaS
    platforms provide their own CLIs, web user interfaces, and REST web services to
    provision, configure, and access each subsystem. PaaS, in other words, is a platform
    aiming to simplify the usage of an infrastructural layer to devs and ops. One
    common way of implementing PaaS is based on containers as a way to provision and
    present a runtime service to developers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PaaS还可以为您抽象其他概念，例如存储（通过提供对象存储或其他存储服务供您使用）、安全、无服务器和构建设施（如CI/CD）。最后但同样重要的是，PaaS提供支持托管应用程序的升级和降级的工具。大多数PaaS平台提供自己的CLIs、Web用户界面和REST
    Web服务来配置、配置和访问每个子系统。换句话说，PaaS是一个旨在简化基础设施层使用方式的平台，针对开发和运维人员。实现PaaS的一种常见方式是基于容器，作为向开发者提供和展示运行时服务的方式。
- en: '**Software as a Service** (**SaaS**) is one layer up from PaaS. It is mostly
    targeted at final users more than developers and implies that the platform provides,
    on-demand, applications ready to use, completely abstracting the underlying infrastructure
    and implementation (usually behind an API). However, while the applications can
    be complex software suites, ready for users to access (such as office suites or
    webmail), they can also be specific services (such as security, image recognition,
    or reporting services) that can be used and embedded by developers into more complex
    applications (usually via API calls).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件即服务**（**SaaS**）位于PaaS之上的一层。它主要针对最终用户而不是开发者，意味着平台按需提供即用应用程序，完全抽象了底层基础设施和实现（通常在API后面）。然而，虽然应用程序可以是复杂的软件套件，准备好供用户访问（如办公套件或网络邮件），它们也可以是特定服务（如安全、图像识别或报告服务），开发者可以将这些服务用于更复杂的应用程序中（通常通过API调用）。'
- en: 'The following diagram shows you a comparison of **IaaS**, **PaaS**, and **SaaS**:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表展示了**IaaS**、**PaaS**和**SaaS**的比较：
- en: '![Figure 9.1 – IaaS versus PaaS versus SaaS'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1 – IaaS与PaaS与SaaS的比较](img/Figure_9.1_B16354.jpg)'
- en: '](img/Figure_9.1_B16354.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_9.1_B16354.jpg](img/Figure_9.1_B16354.jpg)'
- en: Figure 9.1 – IaaS versus PaaS versus SaaS
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – IaaS与PaaS与SaaS的比较
- en: Now, we have a definition of boundaries between some *as a service* layers.
    We should get back to our initial thoughts, *how is PaaS a good way to support
    a heavily distributed, cloud-native architecture such as "the network is the computer"?*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了一些“即服务”层之间的边界。我们应该回到我们的最初想法，*PaaS如何成为支持高度分布式、云原生架构（如“网络即计算机”）的好方法？*
- en: PaaS simplifies the access to the underlying computing resources by providing
    a uniform packaging and delivering model (usually, by using containers). It orchestrates
    such components by deploying them, scaling them (both up and down), and trying
    to maintain a service level wherever possible (such as restarting a faulty component).
    It gives a set of administration tools, regardless of the technology used inside
    each component. Those tools address features such as log collection, metrics and
    observabilities, and configuration management. Nowadays, the most widely used
    tool for orchestration is **Kubernetes**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PaaS通过提供统一的打包和交付模型（通常使用容器）简化了对底层计算资源的访问。它通过部署、扩展（包括向上和向下）以及尽可能维护服务级别来编排这些组件（例如重启故障组件）。它提供了一套管理工具，无论每个组件内部使用的技术如何。这些工具涵盖了日志收集、指标和可观察性以及配置管理等功能。如今，最广泛使用的编排工具是**Kubernetes**。
- en: Introducing containers and Kubernetes
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍容器和Kubernetes
- en: '**Container** technology has a longstanding history. It became popular around
    2013 with the Docker implementation, but initial concepts have their roots in
    the Linux distributions well before then (such as **Linux Containers** (**LXC**),
    launched around 2008). Even there, concepts were already looking very similar
    to the modern containers that can be found in older systems and implementations
    (**Solaris** zones are often mentioned in this regard).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器**技术有着悠久的历史。它大约在2013年随着Docker的实现而流行起来，但最初的概念在那时之前的Linux发行版中就已经有根（例如，大约在2008年推出的**Linux容器**（**LXC**））。即使在那里，概念也已经非常类似于现代容器，这些容器可以在旧系统实现中找到（**Solaris**区域经常被提及）。即使在那里，概念也已经非常类似于现代容器，这些容器可以在旧系统实现中找到（**Solaris**区域经常被提及）。'
- en: We could fill a whole book just talking about containers and Kubernetes, but
    for the sake of simplicity and space, we will just touch on the most important
    concepts useful for our overview on defining and implementing cloud-native architectures,
    which is the main goal of this book. First things first, let's start with what
    a container is, simplified and explained to people with a development background.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写一本书来讨论容器和Kubernetes，但为了简单和节省空间，我们只会触及对定义和实现云原生架构概述最有用的最重要的概念，这是本书的主要目标。首先，让我们从什么是容器开始，以简化和解释开发背景的人的角度来解释。
- en: Defining containers and why they are important
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义容器及其重要性
- en: In a single sentence, a container is a way to use a set of technologies in order
    to fool an application into thinking it has a full machine at its disposal.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，容器是一种使用一系列技术来欺骗应用程序，使其认为它拥有一个完整的机器可供使用的手段。
- en: To explain this a bit better, containers wrap a set of concepts and features,
    usually based on Linux technology (such as `runc` and `cgroups`), which are used
    to isolate and limit a process to make it play nicely with other processes sharing
    the same computational power (physical hardware or VMs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地解释这一点，容器封装了一组概念和特性，通常基于Linux技术（如`runc`和`cgroups`），这些技术用于隔离和限制一个进程，使其与其他共享相同计算能力（物理硬件或虚拟机）的进程友好地协同工作。
- en: To achieve those goals, the container technology has to deal with the assignment
    and management of computing resources, such as networking (ports, IP addresses,
    and more), CPU, filesystems, and storage. The supporting technology can create
    *fake* resources, mapping them to the real ones offered by the underlying resources.
    This means that a container may think to expose a service on port `80`, but in
    reality, such a service is bound to a different port on the host system, or it
    can think to access the root filesystem, but in reality, is confined to a well-defined
    folder.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，容器技术必须处理计算资源的分配和管理，例如网络（端口、IP地址等）、CPU、文件系统和存储。支持技术可以创建*虚拟*资源，将它们映射到底层资源提供的真实资源。这意味着容器可能会认为在端口`80`上公开一个服务，但实际上，这样的服务绑定在主机系统上的不同端口上，或者它可能会认为可以访问根文件系统，但实际上，它被限制在一个定义良好的文件夹中。
- en: 'In this way, it''s the container technology that administers and partitions
    the resources and avoids conflicts between different applications running and
    competing for the same objects (such as network ports). But that''s just one part
    of the story: to achieve this goal, our application must be packaged in a standard
    way, which is commonly a file specifying all the components and resources needed
    by our application to run.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，容器技术负责管理和分配资源，避免不同应用程序在运行和竞争相同对象（如网络端口）时发生冲突。但这只是故事的一部分：为了实现这一目标，我们的应用程序必须以标准化的方式进行打包，这通常是一个文件，指定了应用程序运行所需的所有组件和资源。
- en: 'Once we create our container (starting from such a descriptor), the result
    is an immutable container image (which is a binary runtime that can also be signed
    for integrity and security purposes). A container runtime can then take our container
    image and execute it. This allows container applications to do the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了我们的容器（从这样的描述符开始），结果就是一个不可变的容器镜像（这是一个二进制运行时，也可以为了完整性和安全目的进行签名）。然后，容器运行时可以取我们的容器镜像并执行它。这使得容器应用程序能够执行以下操作：
- en: '**Maximize portability**: Our app will run where a compatible runtime is executed,
    regardless of the underlying OS version, or irrespective of whether the resources
    are provided by a physical server, a VM, or a cloud instance.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大化可移植性**：我们的应用程序将在兼容的运行时执行的地方运行，无论底层操作系统版本如何，或者无论资源是由物理服务器、虚拟机还是云实例提供的。'
- en: '**Reduce moving parts**: Anything tested in a development environment will
    look very similar to what will be in production.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少可移动部件**：在开发环境中测试的任何内容都将非常类似于生产环境中的内容。'
- en: '**Isolate configurations from executable code**: The configurations will need
    to be external and injected into our immutable runtime image.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从可执行代码中隔离配置**：配置需要是外部的，并注入到我们的不可变运行时镜像中。'
- en: '**Describe all the components**: You should nicely describe all the components
    of our application, for both documentation purposes and inspection (so you can
    easily understand, for example, the patch level of all of your Java machines).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述所有组件**：您应该详细描述我们应用程序的所有组件，这不仅是为了文档目的，也是为了检查（这样您可以轻松地了解，例如，所有Java机器的补丁级别）。'
- en: '**Unify packaging, deployment, and management**: Once you have defined your
    container technology, you can package your applications and they will be managed
    (started, stopped, scaled, and more) all in the same way regardless of the internal
    language and technologies used.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一打包、部署和管理**：一旦您定义了您的容器技术，您就可以打包您的应用程序，并且它们将以相同的方式进行管理（启动、停止、扩展等），无论使用的内部语言和技术如何。'
- en: '**Reduce the footprint**: While you could achieve most of the advantages with
    a VM, a container is typically way lighter (because it will carry only what''s
    needed by the specific application, and not a full-fledged OS). For such a reason,
    you can run more applications using the same number of resources.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少占用空间**：虽然您可以使用虚拟机实现大多数优势，但容器通常要轻得多（因为它只会携带特定应用程序所需的资源，而不是一个完整的操作系统）。正因为如此，您可以使用相同数量的资源运行更多的应用程序。'
- en: 'Those are more or less the reasons why container technology became so popular.
    While some of those are specific to infrastructural aspects, the advantages for
    developers are evident: think how this will simplify, as an example, the creation
    of a complete test or dev environment in which every component is containerized
    and running on the correct version (maybe a production one because you are troubleshooting
    or testing a fix).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是容器技术变得如此流行的更多或更少的原因。虽然其中一些是特定于基础设施方面的，但开发者所获得的优点是显而易见的：想想这会如何简化，例如，创建一个完整的测试或开发环境，其中每个组件都被容器化，并运行在正确的版本上（可能是一个生产版本，因为您正在调试或测试一个修复）。
- en: 'So far so good: containers work well and are a nice tool for building modern
    applications. *What''s the warning here?* The point is, if you are in a local
    environment (or a small testing infrastructure), you can think of managing all
    the containers manually (or using some scripts), such as provisioning it on a
    few servers and assigning the configurations required. *But what will happen when
    you start working with containers at scale?* You will need to worry about running,
    scaling, securing, moving, connecting, and much more, for hundreds or thousands
    of containers. This is something that for sure is not possible to do manually.
    You will need an orchestrator, which does exactly that. The standard orchestrator
    for containers today is Kubernetes.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利：容器运行良好，是构建现代应用的不错工具。*这里有什么警告吗？* 重点是，如果您处于本地环境（或小型测试基础设施），您可以手动管理所有容器（或使用一些脚本），例如在几台服务器上配置它并分配所需的配置。*但是当您开始大规模使用容器时会发生什么？*
    您将需要担心运行、扩展、安全、移动、连接等等，对于数百或数千个容器。这肯定是不可能手动完成的。您需要一个编排器，它正好能做这件事。目前容器标准的编排器是Kubernetes。
- en: Orchestrating containers with Kubernetes
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Kubernetes编排容器
- en: '**Kubernetes** (occasionally shortened to **K8s**) is, at the time of writing,
    the core of many PaaS implementations. As will become clear at the end of this
    section, Kubernetes offers critical supporting services to container-based applications.
    It originated from work by Google (originally known as *Borg*) aimed at orchestrating
    containers providing most of the production services of the company. The Kubernetes
    operating model is sometimes referred to as declarative. This means that Kubernetes
    administrators define the target status of the system (such as *I want two instances
    of this specific application running*) and Kubernetes will take care of it (as
    an example, creating a new instance if one has failed).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes**（有时简称为 **K8s**）在撰写本文时是许多 PaaS 实现的核心。正如本节末尾将变得清楚的那样，Kubernetes
    为基于容器的应用程序提供了关键的支持服务。它起源于 Google（最初被称为 *Borg*）的工作，旨在编排提供公司大部分生产服务的容器。Kubernetes
    的运营模式有时被称为声明式。这意味着 Kubernetes 管理员定义系统的目标状态（例如，“我想运行这个特定应用程序的两个实例”）而 Kubernetes
    将负责实现它（例如，如果有一个实例失败，将创建一个新的实例）。'
- en: Following its initial inception at Google, Kubernetes was then released as an
    open source project and is currently being actively developed by a heterogeneous
    community of developers, both enterprise sponsored and independent, under the
    Cloud Native Computing Foundation umbrella.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google 初始推出后，Kubernetes 被作为一个开源项目发布，目前正由一个异构的开发者社区积极开发，包括企业赞助的独立开发者，在云原生计算基金会（Cloud
    Native Computing Foundation）的旗下。
- en: Kubernetes basic objects
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes 基本对象
- en: 'Kubernetes provides a set of objects, used to define and administer how the
    applications run on top of it. Here is a list of these objects:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了一套对象，用于定义和管理应用程序在其上运行的方式。以下是这些对象的列表：
- en: 'A `Pod` is the most basic unit in a Kubernetes cluster, including at least
    one container (more than one container is allowed for some specific use cases).
    Each **Pod** has an assigned set of resources (such as CPU and memory) and can
    be imagined as a **Container** plus some metadata (including network resources,
    application configurations, and storage definitions). Here is a diagram for illustration:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 是 Kubernetes 集群中最基本的单元，包括至少一个容器（对于某些特定用例，允许使用多个容器）。每个 **Pod** 都分配了一组资源（如
    CPU 和内存），可以想象为一个 **容器** 加上一些元数据（包括网络资源、应用程序配置和存储定义）。以下是用于说明的图示：
- en: '![Figure 9.2 – A Pod'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 一个 Pod'
- en: '](img/Figure_9.2_B16354.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_9.2_B16354.jpg)'
- en: Figure 9.2 – A Pod
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 一个 Pod
- en: '`Namespaces` are how Kubernetes organizes all the other resources and avoids
    overlaps. In that sense, they can be intended as *projects*. Of course, it''s
    possible to restrict access for users to specific namespaces. Commonly, namespaces
    are used to group containers belonging to the same application and to define different
    environments (such as dev, test, and prod) in the same Kubernetes cluster.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`命名空间` 是 Kubernetes 组织所有其他资源并避免重叠的方式。从这个意义上说，它们可以被看作是 *项目*。当然，可以限制用户对特定命名空间的访问。通常，命名空间用于将属于同一应用程序的容器分组，并在同一个
    Kubernetes 集群中定义不同的环境（如 dev、test 和 prod）。'
- en: '`Services` are network load balancers and DNS servers provided by the Kubernetes
    cluster. Behind a **Service**, there are a number of Pods answering incoming requests.
    In this way, each **Pod** can access functions exposed from other Pods, thereby
    circumventing accessing such Pods directly via their internal IP (which is considered
    a bad and unreliable way). Services are, by default, internal, but they could
    be exposed and accessed from outside the Kubernetes cluster (by using other Kubernetes
    objects and configurations that aren''t covered here). The following diagram illustrates
    the structure of a Service:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`服务` 是 Kubernetes 集群提供的网络负载均衡器和 DNS 服务器。在 **服务** 后面，有一系列 Pod 响应传入的请求。通过这种方式，每个
    **Pod** 可以访问来自其他 Pod 的公开功能，从而绕过直接通过它们的内部 IP（这被认为是一种不好且不可靠的方式）访问这些 Pod。默认情况下，服务是内部的，但它们可以被公开并从
    Kubernetes 集群外部访问（通过使用此处未涵盖的其他 Kubernetes 对象和配置）。以下图示说明了服务的结构：'
- en: '![Figure 9.3 – A Service'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – 一个 Service'
- en: '](img/Figure_9.3_B16354.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_9.3_B16354.jpg)'
- en: Figure 9.3 – A Service
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 一个 Service
- en: '`Volumes` are a means for Kubernetes to define access to persistent storage
    to be provided to the Pods. Containers do indeed use, by default, ephemeral storage.
    If you want a container to have a different kind of storage assigned, you have
    to deal with volumes. The storage (like many other aspects) is managed by Kubernetes
    in a pluggable way, meaning that behind a volume definition, many implementations
    can be attached (such as different storage resources provided by cloud services
    providers or hardware vendors).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Volumes` 是 Kubernetes 定义提供给 Pods 的持久存储访问的手段。容器默认确实使用临时存储。如果您希望容器具有不同类型的存储分配，您必须处理卷。存储（如许多其他方面）由
    Kubernetes 以可插拔的方式管理，这意味着在卷定义的背后可以附加许多实现（例如，云服务提供商或硬件供应商提供的不同存储资源）。'
- en: '`ConfigMaps` and `Secrets` are the standard way, in Kubernetes, of providing
    configurations to Pods. They are basically used to inject application properties
    (such as database URLs and user credentials) without needing an application rebuild.
    Secrets are essentially the same idea but are supposed to be used for confidential
    information (such as passwords).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConfigMaps` 和 `Secrets` 是 Kubernetes 中向 Pods 提供配置的标准方式。它们基本上用于注入应用程序属性（如数据库
    URL 和用户凭证），而无需重新构建应用程序。Secrets 实质上是相同的概念，但预期用于机密信息（如密码）。'
- en: By default, `Secrets` are strings encoded in Base64 (and so are not really secure),
    but they can be encrypted in various ways. `ConfigMaps` and `Secrets` can be consumed
    by the application as environment variables or property files.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Secrets` 是以 Base64 编码的字符串（因此并不真正安全），但可以通过各种方式加密。`ConfigMaps` 和 `Secrets`
    可以由应用程序作为环境变量或属性文件来消费。
- en: '`ReplicaSet`, `StatefulSet`, and `DaemonSet` are objects that define the way
    each Pod should be run. `ReplicaSet` defines the number of instances (replicas)of
    the Pods to be running at any given time. `StatefulSet` is a way to define the
    ordering in which a given set of Pods should be started or the fact that a Pod
    should have only one instance running at a time. For this reason, they are useful
    for running stateful applications (such as databases) that often have these kinds
    of requirements.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReplicaSet`、`StatefulSet` 和 `DaemonSet` 是定义每个 Pod 应如何运行的对象。`ReplicaSet` 定义了在任何给定时间应运行的
    Pods 的实例数（副本数）。`StatefulSet` 是定义一组给定 Pods 应该启动的顺序或一个 Pod 应该在任何时候只运行一个实例的方式。因此，它们对于运行具有这些类型要求的状态化应用程序（如数据库）非常有用。'
- en: '`DaemonSet`, instead, is used to ensure that a given Pod has an instance running
    in each server of the Kubernetes cluster (more on this in the next section). `DaemonSet`
    is useful for some particular use cases, such as monitoring agents or other infrastructural
    support services.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，`DaemonSet` 用于确保在 Kubernetes 集群中的每个服务器上运行给定 Pod 的实例（更多内容将在下一节中介绍）。`DaemonSet`
    对于某些特定用例非常有用，例如监控代理或其他基础设施支持服务。
- en: '`Th`e Deployment is a concept related to `ReplicaSet` and Pods. A Deployment
    is a way to deploy ReplicaSets and Pods by defining the intermediate steps and
    strategy to perform Deployments, such as rolling releases and rollbacks. Deployments
    are useful for automating the release process and reducing the risk of human error
    during such processes.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Th`e Deployment 是与 `ReplicaSet` 和 Pods 相关的概念。Deployment 通过定义执行部署的中间步骤和策略，如滚动发布和回滚，来部署
    ReplicaSets 和 Pods。Deployment 对于自动化发布过程和减少此类过程中的人为错误风险非常有用。'
- en: '`Labels` are the means that Kubernetes uses to identify and select basically
    every object. In Kubernetes, indeed, it is possible to tag everything with a label
    and use that as a way to query the cluster for objects identified by it. This
    is used by both administrators (such as to group and organize applications) and
    the system itself (as a way to link objects to other objects). As an example,
    Pods that are load-balanced to respond to a specific service are identified using
    labels.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Labels` 是 Kubernetes 用来识别和选择几乎所有对象的手段。在 Kubernetes 中，确实可以对一切进行标记，并使用标签作为查询集群中由其识别的对象的方式。这既被管理员（例如，用于分组和组织应用程序）也被系统本身（作为将对象链接到其他对象的方式）所使用。例如，使用标签识别那些被负载均衡以响应特定服务的
    Pods。'
- en: Now that we have had a glimpse into Kubernetes' basic concepts (and a glossary),
    let's have a look at the Kubernetes architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 Kubernetes 的基本概念（以及术语表）有了一定的了解，让我们来看看 Kubernetes 的架构。
- en: The Kubernetes architecture
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes 架构
- en: Kubernetes is practically a set of Linux machines, with different services installed,
    that play different roles in the cluster. It may include some Windows servers,
    specifically for the purpose of running Windows workloads (such as .NET applications).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 实际上是一组 Linux 机器，安装了不同的服务，在集群中扮演不同的角色。它可能包括一些 Windows 服务器，专门用于运行 Windows
    工作负载（如 .NET 应用程序）。
- en: 'There are two basic server roles for Kubernetes:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有两种基本的服务器角色：
- en: '**Masters** are the nodes that coordinate with the entire cluster and provide
    the administration features, such as managing applications and configurations.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大师**是协调整个集群并提供管理功能（如管理应用程序和配置）的节点。'
- en: '**Workers** are the nodes running the entire applications.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作者**是运行整个应用程序的节点。'
- en: Let's see a bit about what's in each server role. We'll start with the **master
    node components**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个服务器角色中包含的内容。我们将从 **主节点组件** 开始。
- en: 'Master nodes, as said, carry out the role of coordinating workloads across
    the whole cluster. To do so, these are the services that a master node commonly
    runs:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所说，主节点执行协调整个集群工作负载的角色。为此，这些是主节点通常运行的服务：
- en: '`etcd` `etcd` server. `etcd` is a stateful component, more specifically, a
    key-value database. Kubernetes uses it to store the configuration of the cluster
    (such as the definition of Pods, Services, and Deployments), basically representing
    the desired state of the entire cluster (remember that Kubernetes works in a declarative
    way). `etcd` is particularly suited to such needs because it has quite good performance
    and works well in a distributed setup (optionally switching to a read-only state
    if something bad happens to it, thereby allowing limited operativity even under
    extreme conditions, such as a server crash or a network split).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd` `etcd` 服务器。`etcd` 是一个有状态组件，更具体地说，是一个键值数据库。Kubernetes 使用它来存储集群的配置（如 Pods、Services
    和 Deployments 的定义），基本上代表了整个集群的期望状态（记住 Kubernetes 以声明式方式工作）。`etcd` 特别适合这种需求，因为它具有相当好的性能，并且在分布式设置中运行良好（如果发生某些问题，可以选择切换到只读状态，从而即使在极端条件下（如服务器崩溃或网络分裂）也能保持有限的操作性）。'
- en: '`etcd`, Kubernetes provides an API server. Most of the actions that happen
    in Kubernetes (such as administration, configuration checks, and the reporting
    of state) are done through calls to the API server. Such calls are basically JSON
    via HTTP.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`，Kubernetes 提供了一个 API 服务器。在 Kubernetes 中发生的许多操作（如管理、配置检查和状态报告）都是通过调用
    API 服务器来完成的。这些调用基本上是通过 HTTP 的 JSON 进行的。'
- en: '**Scheduler**: This is the component that handles selecting the right worker
    to execute a Pod. To do so, it can handle the basic requirements (such as the
    first worker with enough resources or lower loads) or advanced, custom-configured
    policies (such as anti-affinity and data locality).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**：这是处理选择合适的工节点来执行 Pod 的组件。为此，它可以处理基本要求（如资源充足或负载较低的第一个工节点）或高级、自定义配置的策略（如反亲和性和数据本地性）。'
- en: '`etcd`) and operates the requisite changes if needed. So, for example, if a
    **Pod** is running fewer instances than what is configured, the **Controller**
    manager creates the missing instances, as shown here:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`）并在需要时操作必要的更改。例如，如果一个 **Pod** 运行的实例少于配置的数量，**控制器**管理器会创建缺失的实例，如下所示：'
- en: '![Figure 9.4 – Master and worker nodes'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – 主节点和工节点'
- en: '](img/Figure_9.4_B16354.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.4_B16354.jpg)'
- en: Figure 9.4 – Master and worker nodes
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 主节点和工节点
- en: This set of components, running into master nodes, is commonly referred to as
    the `etcd` requirements for high availability).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这组组件，运行在主节点上，通常被称为 `etcd` 高可用性要求）。
- en: As is common in a master/slave setup, the master nodes are considered to be
    precious resources configured in a high-availability setup. Everything that is
    reasonably possible should be done to keep the master nodes running and healthy,
    as there can be unforeseen effects on the Kubernetes cluster in case of a failure
    (especially if all the master instances fail at the same time).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在主/从设置中，主节点被认为是配置在高可用性设置中的宝贵资源。应该尽一切可能保持主节点运行和健康，因为主节点故障（尤其是如果所有主实例同时失败）可能会对
    Kubernetes 集群产生不可预见的影响。
- en: 'The other component in a Kubernetes cluster is the worker nodes: **worker node
    components**.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群中的另一个组件是工作节点：**工作节点组件**。
- en: 'The worker nodes are the servers on a Kubernetes cluster that actually run
    the applications (in the form of Pods). Unlike masters, workers are a disposable
    resource. With some exceptions, it is considered safe to change the number of
    worker nodes (by adding or removing them) in a running Kubernetes cluster. Indeed,
    that''s a very common use case: it is one of the duties of the master nodes to
    ensure that all the proper steps (such as recreating Pods and rebalancing the
    workload) are implemented following such changes.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点是Kubernetes集群中的服务器，实际上运行应用程序（以Pod的形式）。与主节点不同，工作节点是一种可消耗的资源。在少数例外的情况下，被认为在运行中的Kubernetes集群中更改工作节点的数量（通过添加或删除它们）是安全的。事实上，这是一个非常常见的用例：确保所有适当的步骤（如重新创建Pod和重新平衡工作负载）在更改后得到实施，这是主节点的一项职责。
- en: 'Of course, if the changes to the cluster are planned, it is likely to have
    less impact on the application (because, as an example, Kubernetes can evacuate
    Pods from the impacted nodes before removing it from the cluster), while if something
    unplanned, such as a crash, happens, this may imply some service disruptions.
    Nevertheless, Kubernetes is more or less designed to handle this kind of situation.
    Master nodes run the following components:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果集群的更改是计划好的，那么它对应用程序的影响可能较小（因为，例如，Kubernetes可以在从集群中移除之前，从受影响的节点中疏散Pod），而如果发生意外情况，如崩溃，这可能会导致一些服务中断。尽管如此，Kubernetes或多或少是设计来处理这种情况的。主节点运行以下组件：
- en: '**Container runtime**: This is a core component of a worker node. It''s the
    software layer responsible for running the containers included in each Pod. Kubernetes
    supports, as container runtimes, any implementation of the **Container Runtime
    Interface** (**CRI**) standard. Widespread implementations, at the time of writing,
    include containerd, Docker, and CRI-O.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**: 这是工作节点的一个核心组件。它是负责运行每个Pod中包含的容器的软件层。Kubernetes支持任何符合**容器运行时接口**（**CRI**）标准的实现。在撰写本文时，广泛使用的实现包括containerd、Docker和CRI-O。'
- en: '**kubelet**: This is an agent running on each worker. Kubelet registers itself
    with the Kubernetes API server and communicates with it in order to check that
    the desired state of Pods scheduled to run in the node is up and running. Moreover,
    kubelet reports the health status of the node to the master (hence, it is used
    to identify a faulty node).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kubelet**: 这是运行在每个工作节点上的代理。kubelet将自己注册到Kubernetes API服务器，并与它通信，以检查计划在节点上运行的Pod的期望状态是否已启动并运行。此外，kubelet向主节点报告节点的健康状态（因此，它被用来识别有故障的节点）。'
- en: '**kube-proxy**: This is a network component running on the worker node. Its
    duty is to connect applications running on the worker node to the outside world,
    and vice versa.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**: 这是运行在工作节点上的网络组件。它的任务是连接运行在工作节点上的应用程序与外部世界，反之亦然。'
- en: Now that we have a clear understanding of the Kubernetes objects, server roles,
    and related components, it's time to understand why Kubernetes is an excellent
    engine for PaaS, and what is lacking to define it as PaaS *per se*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清楚地理解了Kubernetes对象、服务器角色和相关组件，是时候了解为什么Kubernetes是一个出色的PaaS引擎，以及将其定义为PaaS*本身*所缺乏的内容。
- en: Kubernetes as PaaS
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes作为PaaS
- en: If the majority of your experience is in the dev area, you may feel a bit lost
    after going through all those Kubernetes concepts. Even if everything is clear
    for you (or you already have a background in the infrastructural area), you'll
    probably agree that Kubernetes, while being an amazing tool, is not the easiest
    approach for a developer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的大部分经验都在开发领域，那么在经历了所有这些Kubernetes概念之后，你可能会有点迷茫。即使一切对你来说都很清楚（或者你已经有了基础设施领域的背景），你可能会同意Kubernetes虽然是一个惊人的工具，但并不是最容易让开发者上手的方法。
- en: Indeed, most of the interactions between a developer and a Kubernetes cluster
    may involve working with `.yaml` files (this format is used to describe the API
    objects that we have seen) and the command line (usually using kubectl, the official
    CLI tool for Kubernetes) and understanding advanced container-based mechanisms
    (as persistent volumes, networking, security policies, and more).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，开发人员与Kubernetes集群之间的大多数交互可能都涉及处理`.yaml`文件（这种格式用于描述我们所看到的API对象）和命令行（通常使用kubectl，Kubernetes的官方CLI工具）以及理解高级基于容器的机制（如持久卷、网络、安全策略等）。
- en: Those aren't necessarily the most natural skills for a developer. For such reasons
    (and similar reasons on the infrastructure side), Kubernetes is commonly not regarded
    as a PaaS *per se*; it is more like being a core part of one (an engine). Kubernetes
    is sometimes referred to as **Container as a Service** (**CaaS**), being essentially
    an infrastructure layer that orchestrates containers as the core feature.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技能对于开发者来说并不一定是自然而然就具备的。出于这样的原因（以及基础设施方面的类似原因），Kubernetes通常不被视为PaaS *本身*；它更像是一个核心部分（一个引擎）。Kubernetes有时被称为**容器即服务**（**CaaS**），本质上是一个基础设施层，它以容器编排为核心功能。
- en: One common metaphor used in this regard is with the Linux OS. Linux is made
    by a low-level, very complex, and very powerful layer, which is the kernel. The
    kernel is vital for everything in the Linux OS, including managing processes,
    resources, and peripherals. But no Linux users exclusively use the kernel; they
    will use the Linux distributions (such as Fedora, Ubuntu, or RHEL), which top
    up the kernel with all the high-level features (such as tools, utilities, and
    interfaces) that make it usable to final users.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，一个常用的隐喻是与Linux操作系统。Linux是由一个低级、非常复杂且非常强大的层构成的，即内核。内核对于Linux操作系统的所有内容都至关重要，包括管理进程、资源和外围设备。但没有任何Linux用户仅使用内核；他们将会使用Linux发行版（如Fedora、Ubuntu或RHEL），这些发行版在内核之上添加了所有高级功能（如工具、实用程序和接口），使得它对最终用户可用。
- en: 'To use it productively, Kubernetes (commonly referred to in this context as
    vanilla Kubernetes) is usually complemented with other tools, plugins, and software,
    covering and extending some areas. The most common are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地使用它，Kubernetes（在此上下文中通常称为vanilla Kubernetes）通常需要与其他工具、插件和软件相结合，覆盖并扩展一些领域。最常见的是以下内容：
- en: '**Runtime**: This is related to the execution of containers (and probably the
    closer, more instrumental extensions that Kubernetes needs to rely on to work
    properly and implement a PaaS model). Indeed, strictly speaking, Kubernetes doesn''t
    even offer a *container runtime*; but, as seen in the previous section, it provides
    a standard (the CRI) that can be implemented by different runtimes (we mentioned
    containerd, Docker, and CRI-O). In the area of runtimes, *network* and *storage*
    are also worth mentioning as stacks are used to provide connectivity and persistence
    to the containers. As in the container runtime, in both network and storage runtimes,
    there is a set of standards and a glossary (the aforementioned services, or volumes)
    that is then implemented by the technology of choice.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时**：这与容器的执行（以及可能更接近、更关键的Kubernetes需要依赖以正常工作并实现PaaS模型的功能扩展）有关。实际上，严格来说，Kubernetes甚至不提供*容器运行时*；但，如前所述章节所见，它提供了一个标准（即CRI），该标准可以被不同的运行时实现（我们提到了containerd、Docker和CRI-O）。在运行时领域，*网络*和*存储*也值得提及，因为堆栈被用来提供容器的连接性和持久性。正如在容器运行时一样，在网络和存储运行时中，也存在一套标准和术语表（上述服务或卷），然后由选择的技术实现。'
- en: '**Provisioning**: This includes aspects such as automation, infrastructure
    as code (commonly used tools here include Ansible and Terraform), and container
    registries in order to store and manage the container images (notable implementations
    include Harbor and Quay).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**：这包括自动化、基础设施即代码（这里常用的工具包括Ansible和Terraform），以及容器注册库，以便存储和管理容器镜像（值得注意的实现包括Harbor和Quay）。'
- en: '**Security**: This spans many different aspects of security, from *policy*
    definition and enforcement (one common tool in this area is Open Policy Agent),
    runtime security and threat detection (a technology used here is Falco), and image
    scanning (Clair is one of the implementations available) to vault and secret encryption
    and management (one product covering this aspect is HashiCorp Vault).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：这涵盖了安全性的许多不同方面，从*策略*定义和执行（该领域的一个常用工具是Open Policy Agent），运行时安全和威胁检测（这里使用的技术是Falco），以及镜像扫描（Clair是可用的实现之一）到保险库和秘密加密与管理（涵盖这一方面的产品之一是HashiCorp
    Vault）。'
- en: '**Observability**: This is another important area to make Kubernetes a PaaS
    solution that can be operated easily in production. One de facto standard here
    is **Prometheus**, which is a time series database widely used to collect metrics
    from the different components running on Kubernetes (including core components
    of the platform itself). Another key aspect is log collection, to centralize the
    logs produced.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：这是使 Kubernetes 成为易于在生产环境中操作的平台即服务（PaaS）解决方案的另一个重要领域。这里的一个事实上的标准是 **Prometheus**，它是一个时间序列数据库，广泛用于收集在
    Kubernetes 上运行的不同组件的指标（包括平台本身的核心组件）。另一个关键方面是日志收集，以集中收集产生的日志。'
- en: '**Fluentd** is a common choice in this area. Another key point (that we already
    introduced in [*Chapter 7*](B16354_07_Final_JM_ePUB.xhtml#_idTextAnchor164), *Exploring
    Middleware and Frameworks*, in the sections on micro profiling) is tracing, as
    in the capability of correlating calls to different systems and identifying the
    execution of a request when such a request is handled by many different subsystems.
    Common tools used for this include Jaeger and OpenTracing. Last but not least,
    most of the telemetry collected in each of those aspects is commonly represented
    as dashboards and graphics. A common choice for doing that is with **Grafana**.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**Fluentd** 是这个领域的常见选择。另一个关键点（我们已经在 [*第7章*](B16354_07_Final_JM_ePUB.xhtml#_idTextAnchor164)，*探索中间件和框架*，在微调部分中介绍过）是追踪，即在关联不同系统调用和识别当请求由许多不同的子系统处理时的请求执行。常用的工具包括
    Jaeger 和 OpenTracing。最后但同样重要的是，每个方面收集的大多数遥测数据通常表示为仪表板和图形。做这件事的一个常见选择是使用 **Grafana**。'
- en: '**Endpoint management**: This is a topic related to networking, but at a higher
    level. It involves the definition, inventory, discovery, and management of application
    endpoints (that is, an API or similar network endpoints). This area is commonly
    addressed with a service mesh. It offloads communication between the containers
    by using a network proxy (using the so-called **sidecar pattern**) so that such
    a proxy can be used for tracing, securing, and managing all the calls entering
    and exiting the container. Common implementations of a service mesh are Istio
    and Linkerd. Another core area of endpoint management is so-called API management,
    which is similar conceptually (and technically) to a service mesh but is more
    targeted at calls coming from outside the Kubernetes cluster (while the service
    mesh mostly addresses Pod-to-Pod communication). Commonly used API managers include
    3scale and Kong.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端点管理**：这是一个与网络相关的主题，但处于更高的层次。它包括定义、库存、发现和管理应用程序端点（即 API 或类似的网络端点）。这个领域通常使用服务网格来解决。它通过使用网络代理（使用所谓的
    **边车模式**）来卸载容器之间的通信，这样代理就可以用于追踪、保护和管理工作进入和离开容器的所有调用。服务网格的常见实现包括 Istio 和 Linkerd。端点管理的另一个核心领域是所谓的
    API 管理，它在概念上（和技术上）类似于服务网格，但更针对来自 Kubernetes 集群外部的调用（而服务网格主要解决 Pod 到 Pod 的通信）。常用的
    API 管理器包括 3scale 和 Kong。'
- en: '**Application management**: Last but not least, this is an area related to
    how applications are packaged and installed in Kubernetes (in the form of container
    images). Two core topics are application definition (where two common implementations
    are Helm and the Operator Framework) and CI/CD (which can be implemented, among
    others, using Tekton and/or Jenkins).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序管理**：最后但同样重要的是，这是一个与如何在 Kubernetes 中打包和安装应用程序（以容器镜像的形式）相关的领域。两个核心主题是应用程序定义（其中两种常见实现是
    Helm 和 Operator 框架）和 CI/CD（可以使用 Tekton 和/或 Jenkins 等工具实现）。'
- en: All the aforementioned technologies (and many more) are mentioned and cataloged
    (using a similar glossary) in the CNCF landscape (visit [https://landscape.cncf.io](https://landscape.cncf.io)).
    **CNCF** is the **Cloud Native Computing Foundation**, which is an organization
    related to the Linux Foundation, aiming to define a set of vendor-neutral standards
    for cloud-native development. The landscape is their assessment of technologies
    that can be used for such goals including and revolving around Kubernetes (which
    is one of the core software parts of it).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述技术（以及更多）都在 CNCF 生态系统中被提及和编目（使用类似的词汇表）。**CNCF** 是 **云原生计算基金会**，这是一个与 Linux
    基金会相关的组织，旨在为云原生开发定义一套供应商中立的标准化。这个生态系统是他们对这些技术进行的评估，包括围绕 Kubernetes（它是其核心软件部分之一）的技术。
- en: So, I think it is now clear that Kubernetes and containers are core components
    of PaaS, which is key for cloud-native development. Nevertheless, such components
    mostly address runtime and orchestration needs, but many more things are needed
    to implement a fully functional PaaS model to support our applications.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我认为现在很清楚，Kubernetes和容器是PaaS的核心组件，这对于云原生开发至关重要。尽管如此，这些组件主要解决运行时和编排需求，但还需要更多的事情来实现一个完全功能的PaaS模型来支持我们的应用程序。
- en: Looking at things the other way around, you can wonder what the best practices
    are that each application (or component or microservice) should implement in order
    to fit nicely in PaaS and behave in the best possible way in a cloud-native, distributed
    setup. While it's impossible to create a magical checklist that makes every application
    a cloud-native application, there is a well-known set of criteria that can be
    considered a good starting point. Applications that adhere to this list are called
    twelve-factor applications.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，你可以思考每个应用程序（或组件或微服务）应该实施的最佳实践，以便在PaaS中很好地适应，并在云原生、分布式设置中以最佳方式表现。虽然不可能创建一个神奇的清单，使每个应用程序都成为云原生应用程序，但有一个众所周知的标准集，可以作为一个良好的起点。遵循此列表的应用程序被称为十二要素应用。
- en: Defining twelve-factor applications
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义十二要素应用
- en: 'The **twelve-factor applications** are a collection of good practices suggested
    for cloud-native applications. Applications that adhere to such a list of practices
    will most likely benefit from being deployed on cloud infrastructures, face *web-scale*
    traffic peaks, and resiliently recover from failures. Basically, twelve-factor
    applications are the closest thing to a proper definition of microservices. PaaS
    is very well suited for hosting twelve-factor apps. In this section, we are going
    to have a look at this list of factors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**十二要素应用**是一组针对云原生应用提出的良好实践。遵循此类实践列表的应用程序很可能会从部署在云基础设施上、面对*Web规模*流量峰值和从故障中弹性恢复中受益。基本上，十二要素应用是微服务适当定义的最近似物。PaaS非常适合托管十二要素应用。在本节中，我们将查看这个因素列表：'
- en: '**Codebase**: This principle simply states that all the source code related
    to an app (including scripts, configurations, and every asset needed) should be
    versioned in a single repo (such as a Git source code repository). This implies
    that different apps should not share the same repo (which is a nice way to reduce
    coupling between different apps, at the cost of duplication).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码库**：这个原则简单地说，与一个应用程序相关的所有源代码（包括脚本、配置和所需的所有资产）都应该在一个单一的仓库中版本控制（例如Git源代码仓库）。这意味着不同的应用程序不应该共享同一个仓库（这是一种减少不同应用程序之间耦合的好方法，但代价是重复）。'
- en: Such a repo is then the source for creating Deployments, which run instances
    of the application, compiled (where relevant) by a CI/CD toolchain and launched
    in a number of different environments (such as production, test, and dev). A Deployment
    can be based on different versions of the same repo (as an example, a dev environment
    could run experimental versions, containing changes not yet tested and deployed
    in production, but still part of the same repo).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的仓库是创建部署的来源，部署运行应用程序的实例，由CI/CD工具链编译（如果相关），并在多个不同的环境中启动（例如生产、测试和开发环境）。部署可以基于同一仓库的不同版本（例如，开发环境可以运行实验性版本，包含尚未在生产中测试和部署的更改，但仍然是同一仓库的一部分）。
- en: '**Dependencies**: A twelve-factor app must explicitly declare all the dependencies
    that are needed at runtime and must isolate them. This means avoiding depending
    on implicit and system-wide dependencies. This used to be a problem with traditional
    applications, as with Java applications running on an application server, or in
    general with applications *expecting* some dependencies provided by the system.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖项**：一个十二要素应用必须明确声明在运行时需要的所有依赖项，并且必须将它们隔离。这意味着避免依赖于隐式和系统范围的依赖项。这在传统的应用程序中曾经是一个问题，例如在运行在应用程序服务器上的Java应用程序，或者一般而言，在*期望*由系统提供某些依赖项的应用程序中。'
- en: Conversely, twelve-factor apps specifically declare and isolate the applications
    needed. In this way, the application behavior is more repeatable, and a dev (or
    test) environment is easier to set up. Of course, this comes at the cost of consuming
    more resources (disk space and memory, mostly). This requirement is one of the
    reasons for containers being so popular for creating twelve-factor apps, as containers,
    by default, declare and carry all the necessary dependencies for each application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，十二要素应用专门声明并隔离所需的应用。这样，应用的行为就更具可重复性，并且开发（或测试）环境更容易设置。当然，这会消耗更多资源（主要是磁盘空间和内存）。这是容器在创建十二要素应用中如此受欢迎的原因之一，因为容器默认声明并携带每个应用所需的所有依赖项。
- en: '**Config**: Simply put, this factor is about strictly separating the configurations
    from the application code. Configurations are intended to be the values that naturally
    change in each environment (such as credentials for accessing the database or
    endpoints pointing to external services). In twelve-factor apps, the configurations
    are supposed to be stored in environment variables. It is common to relax this
    requirement and store a little configuration in other places (separated from code),
    such as config files.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**: 简而言之，这个要素是关于严格地将配置与应用代码分离。配置的目的是自然地存在于每个环境中的值（例如访问数据库的凭证或指向外部服务的端点）。在十二要素应用中，配置应该存储在环境变量中。通常，人们会放宽这一要求，在其他地方（与代码分离）存储少量配置，例如配置文件。'
- en: Another point is that the twelve-factor apps approach suggests avoiding grouping
    configurations (such as grouping a set of config values for prod, or one for test)
    because this approach does not scale well. The advice is to individually manage
    each configuration property, associating it with the related Deployment. While
    there are some good rationalizations beyond this concept, it's also not uncommon
    to relax at this point and have a grouping of configurations following the naming
    of the environment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点是，十二要素应用的方法建议避免将配置分组（例如，将一组配置值分组为生产环境，或一组为测试环境），因为这种方法扩展性不好。建议单独管理每个配置属性，将其与相关的部署关联起来。虽然这个概念之外还有一些合理的解释，但在这个点上放宽并按照环境命名分组配置也很常见。
- en: '**Backing services**: A twelve-factor app must consider each backing service
    that is needed (such as databases, APIs, queues, and other services that our app
    depends on) as attached resources. This means that each backing service should
    be identified by a set of configurations (something such as the URL, username,
    and password) and it should be possible to replace it without any change to the
    application code (maybe requiring a restart or refresh).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端服务**: 一个十二要素应用必须考虑每个所需的后端服务（例如数据库、API、队列以及其他我们应用所依赖的服务）作为附加资源。这意味着每个后端服务都应该通过一组配置（例如URL、用户名和密码）来识别，并且应该能够在不修改应用代码的情况下替换它（可能需要重启或刷新）。'
- en: By adhering to such factors, our app will be loosely coupled to an external
    service, hence scaling better and being more portable (such as from on-premises
    to the cloud, and vice versa). Moreover, the testing phase will benefit from this
    approach because we can easily swap each service with a mock, where needed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些要素，我们的应用将与外部服务松散耦合，因此扩展性更好，更易于迁移（例如从本地到云端，反之亦然）。此外，测试阶段将受益于这种方法，因为我们可以轻松地用模拟服务替换每个服务，如果需要的话。
- en: Last but not least, the resiliency of the app will increase because we could,
    as an example, swap a faulty service with an alternative one in production with
    little to no outage. In this context, it's also worth noticing that in the purest
    microservices theory, each microservice should have its own database, and no other
    microservice should access that database directly (but to obtain the data, it
    should be mediated by the microservice itself).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，应用的可恢复性将提高，因为我们可以在生产环境中用替代服务替换一个有故障的服务，而几乎不会出现中断。在这个背景下，也值得注意，在最纯粹的微服务理论中，每个微服务应该有自己的数据库，并且没有其他微服务应该直接访问该数据库（但为了获取数据，应该由微服务本身来中介）。
- en: '**Build, release, and run**: The twelve factor approach enforces strict separation
    for the build, release, and run phases. Build includes the conversion of source
    code into something executable (usually as the result of a compile process) and
    the release phase associates the executable item with the configuration needed
    (considering the target environment).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建、发布和运行**：十二要素方法强制执行构建、发布和运行阶段之间的严格分离。构建包括将源代码转换为可执行文件（通常是通过编译过程的结果）以及发布阶段将可执行项与所需的配置关联起来（考虑到目标环境）。'
- en: Finally, the run phase is about executing such a release in the chosen environment.
    An important point here is that the whole process is supposed to be stateless
    and repeatable (such as using a CI/CD pipeline), starting from the code repo and
    configurations. Another crucial point is that each release must be associated
    with a unique identifier, to map and track exactly where the code and config ended
    up in each runtime. The advantages of this approach are a reduction in moving
    parts, support for troubleshooting, and the simplification of rollbacks in case
    of unexpected behaviors.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行阶段是关于在所选环境中执行这样的发布。这里的一个重要观点是，整个过程应该是无状态的且可重复的（例如使用CI/CD管道），从代码仓库和配置开始。另一个关键点是，每个发布都必须与一个唯一的标识符相关联，以精确地映射和跟踪代码和配置在每个运行时最终的位置。这种方法的优点是减少了移动部件，支持故障排除，以及在意外行为发生时简化回滚。
- en: '**Processes**: An app compliant with the twelve factors is executed as one
    or more processes. Each process is considered to be stateless and shares nothing.
    The state must be stored in ad hoc backing services (such as databases). Each
    storage that is local to the process (being disk or memory) must be considered
    an unreliable cache.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程**：符合十二要素的应用程序作为一个或多个进程执行。每个进程都被认为是无状态的，并且不共享任何内容。状态必须存储在专用的后端服务中（如数据库）。每个属于进程本地的存储（无论是磁盘还是内存）都必须被视为不可靠的缓存。'
- en: It can be used, but the app must not depend on it (and must be capable of recovering
    in case something goes wrong). An important consequence of the stateless process
    model is that sticky sessions must be avoided. A sticky session is when consequent
    requests must be handled by the same instance in order to function properly. Sticky
    sessions violate the idea of being stateless and limit the horizontal scalability
    of applications, and hence should be avoided. Once again, the state must be offloaded
    to relevant backing services.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以使用，但应用程序不得依赖于它（并且必须能够在出现问题的情况下恢复）。无状态进程模型的一个重要后果是必须避免粘性会话。粘性会话是指后续请求必须由同一实例处理才能正常工作。粘性会话违反了无状态的概念，并限制了应用程序的水平可扩展性，因此应该避免。再次强调，状态必须卸载到相关的后端服务。
- en: '**Port binding**: Each twelve-factor app must directly bind to a network port
    and listen to requests on such a port. In this way, each app can become a backing
    service for other twelve-factor apps. A common consideration around this factor
    is that usually, applications rely on external servers (such as PHP or a Java
    application server) to expose their services, whereas twelve-factor apps embed
    the dependencies needed to directly expose such services.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口绑定**：每个十二要素应用程序必须直接绑定到网络端口并监听该端口的请求。这样，每个应用程序都可以成为其他十二要素应用程序的后端服务。关于这个因素的一个常见考虑是，通常应用程序依赖于外部服务器（如PHP或Java应用程序服务器）来公开其服务，而十二要素应用程序则嵌入直接公开此类服务的所需依赖项。'
- en: That's basically what we saw with JEE to cloud-native in the previous chapter;
    Quarkus, as an example, has a dependency to undertow to directly bind on a port
    and listen for HTTP requests. It is common to then have infrastructural components
    routing requests from the external world to the chosen port, wherever is needed.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是我们在前一章中从JEE看到云原生的情况；以Quarkus为例，它依赖于Undertow来直接绑定端口并监听HTTP请求。通常，基础设施组件会将来自外部世界的请求路由到所需的端口，无论何时需要。
- en: '**Concurrency**: The twelve-factor app model suggests implementing a horizontal
    scalability model. In such a model, concurrency is handled by spinning new instances
    of the affected components. The smallest scalability unit, suggested to be scaled
    following the traffic profiles, is the process. Twelve-factor apps should rely
    on underlying infrastructure to manage the process''s life cycle.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发**：十二要素应用程序模型建议实现水平可扩展模型。在这种模型中，并发通过启动受影响组件的新实例来处理。建议根据流量配置文件进行扩展的最小可扩展单元是进程。十二要素应用程序应依赖底层基础设施来管理进程的生命周期。'
- en: This infrastructure can be the OS process manager (such as `systemd` in a modern
    Linux system) or other similar systems in a distributed environment (such as PaaS).
    Processes are suggested to span different servers (such as VMs or physical machines)
    if those are available, in order to use resources correctly. Take into account
    the fact that such a concurrency model does not replace other internal concurrency
    models provided by the specific technology used (such as threads managed by each
    JVM application) but is considered a kind of extension of it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基础设施可以是操作系统进程管理器（例如，现代Linux系统中的`systemd`）或分布式环境中的其他类似系统（例如PaaS）。如果可用，建议将进程跨不同的服务器（例如虚拟机或物理机）扩展，以正确使用资源。考虑到这种并发模型并不取代特定技术提供的其他内部并发模型（例如每个JVM应用程序管理的线程），而是被视为其扩展。
- en: '**Disposability**: Applications adhering to the twelve-factor app should be
    disposable. This implies that apps should be fast to start up (ideally, a few
    seconds between the process being launched and the requests being correctly handled)
    and to shut down. Also, the shutdown should be handled gracefully, meaning that
    all the external resources (such as database connections or open files) must be
    safely deallocated, and every inflight request should be managed before the application
    is stopped.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可丢弃性**：遵循十二要素应用的应用应该是可丢弃的。这意味着应用应该快速启动（理想情况下，从进程启动到正确处理请求之间只有几秒钟）和关闭。此外，关闭应该优雅地处理，这意味着所有外部资源（例如数据库连接或打开的文件）必须安全地释放，并且在应用程序停止之前，必须管理所有正在进行的请求。'
- en: This *fast to start up*/*safe to shut down* mantra will allow for horizontal
    scalability with more instances being created to face traffic peaks and the ones
    being destroyed to save resources when no more are needed. Another suggestion
    is to create applications to be tolerant to hard shutdown (as in the case of a
    hardware failure or a forced shutdown, such as a process kill). To achieve this,
    the application should have special procedures in place to handle incoherent states
    (think about an external resource improperly closed or requests partially handled
    and potentially corrupted).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个**快速启动**/**安全关闭**的咒语将允许进行横向扩展，通过创建更多实例来应对流量峰值，并在不再需要时销毁实例以节省资源。另一个建议是创建能够容忍硬关闭（例如，在硬件故障或强制关闭的情况下，如进程终止）的应用程序。为了实现这一点，应用程序应具备特殊程序来处理不一致的状态（例如，考虑外部资源不当关闭或请求部分处理且可能损坏的情况）。
- en: '**Dev/prod parity**: A twelve-factor app must reduce the differences between
    production and non-production environments as much as possible. This includes
    differences in software versions (meaning that a development version must be released
    in production as soon as possible by following the *release early, release often*
    mantra).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发和生产环境一致性**：十二要素应用必须尽可能减少生产环境和非生产环境之间的差异。这包括软件版本差异（这意味着开发版本必须尽快通过遵循“尽早发布，经常发布”的咒语在生产环境中发布）。'
- en: But the mantra also referred to when different teams are working on it (devs
    and ops should cooperate in both production and non-production environments, avoiding
    the handover following the production release and implementing a DevOps model).
    Finally, there is the technology included in each environment (the backing services
    should be as close as possible, trying to avoid, as an example, different types
    of databases in dev versus production environments).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个咒语也适用于当不同的团队在处理它时（开发和运维团队应在生产和非生产环境中合作，避免在生产发布后的交接，并实施DevOps模式）。最后，还有每个环境包含的技术（后端服务应尽可能接近，例如，尝试避免在开发和生产环境中使用不同类型的数据库）。
- en: This approach will provide multiple benefits. First of all, in the case of a
    production issue, it will be easier to troubleshoot and test fixes in all the
    environments, due to those environments being as close as possible to the production
    one. Another positive effect is that it will be harder for a bug to find its way
    into production because the test environments will look like the production ones.
    Last but not least, having similar environments will reduce the hassle of having
    to manage multiple variants of stacks and versions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将提供多种好处。首先，在出现生产问题时，由于这些环境尽可能接近生产环境，因此将更容易在所有环境中进行故障排除和测试修复。另一个积极的影响是，由于测试环境看起来像生产环境，因此错误更难进入生产环境。最后但同样重要的是，拥有相似的环境将减少管理多个堆栈和版本变体的麻烦。
- en: '**Logs**: This factor points to the separation of log generation and log storage.
    A twelve-factor app simply considers logs as an event stream, continuously generated
    (usually in a textual format) and sent to a stream (commonly the standard output).
    The app shouldn''t care about persisting logs with all the associated considerations
    (such as log rotation or forwarding to different locations).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志**：这个要素指向日志生成和日志存储的分离。一个十二要素应用程序仅仅将日志视为一个事件流，持续生成（通常以文本格式）并发送到一个流（通常是标准输出）。应用程序不应该关心使用所有相关考虑（如日志轮换或转发到不同位置）来持久化日志。'
- en: Instead, the hosting platform should provide services capable of retrieving
    such events and handling them, usually on a multitier persistence (such as writing
    recent logs to files and sending the older entries to indexed systems to support
    aggregation and searches). In this way, the logs can be used for various purposes,
    including monitoring and business intelligence on platform performance.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，托管平台应提供能够检索此类事件并处理它们的服务，通常是在多层持久性（例如将最近的日志写入文件并将较旧的条目发送到索引系统以支持聚合和搜索）上。这样，日志可以用于各种目的，包括监控和平台性能的商业智能。
- en: '**Admin processes**: Many applications provide supporting tools to implement
    administration processes, such as performing backups, fixing malformed entries,
    or other maintenance activities. The twelve-factor apps are no exception to this.
    However, it is recommended to implement such admin processes in an environment
    as close as possible to the rest of the application.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理进程**：许多应用程序提供支持工具以实现管理进程，例如执行备份、修复格式不正确的条目或其他维护活动。十二要素应用程序也不例外。然而，建议尽可能在接近应用程序其余部分的环境中实现此类管理进程。'
- en: Wherever possible, the code (or scripts) providing those features should be
    executed by the same runtime (such as a JVM) and using the same dependencies (the
    database driver). Moreover, such code must be checked out in the same code repo
    and must follow the same versioning schema as the application's main code. One
    approach to achieving this outcome is to provide an interactive shell (properly
    secured) as part of the application itself and run the administrative code against
    such a shell, which is then approved to use the same facilities (connection to
    the database and access to sessions) as the rest of the application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，提供那些功能的代码（或脚本）应该由相同的运行时（例如JVM）执行，并使用相同的依赖项（数据库驱动程序）。此外，这样的代码必须从相同的代码仓库检出，并且必须遵循与应用程序主代码相同的版本控制方案。实现这一结果的一种方法是在应用程序本身中提供交互式外壳（适当的安全保护），并在该外壳上运行管理代码，然后批准使用与应用程序其余部分相同的设施（连接到数据库和访问会话）。
- en: Probably, while reading this list, many of you were thinking about how those
    twelve factors can be implemented, especially with the tools that we have seen
    so far (such as Kubernetes). Let's try to explore those relationships.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读此列表时，你们中的许多人可能都在思考如何实现这十二要素，特别是使用我们迄今为止看到的工具（例如Kubernetes）。让我们尝试探索这些关系。
- en: Twelve-factor apps and the supporting technology
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 十二要素应用程序和支持技术
- en: 'Let''s review the list of the twelve factors and which technologies covered
    so far can help us to implement applications adhering to such factors:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下十二要素的列表以及到目前为止哪些技术可以帮助我们实现遵循这些要素的应用程序：
- en: '**Codebase**: This is less related to the runtime technology and more to the
    tooling. As mentioned before, nowadays, the versioning tool widely used is Git,
    basically being the standard (and for good reason). The containers and Kubernetes,
    however, are well suited for supporting this approach, providing constructs such
    as containers, Deployments, and namespaces, which are very useful for implementing
    multiple deploys from the same codebase.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码库**：这与运行时技术关系不大，更多与工具相关。如前所述，如今广泛使用的版本控制工具是Git，基本上已成为标准（并且有很好的理由）。然而，容器和Kubernetes非常适合支持这种方法，提供容器、部署和命名空间等结构，这对于从同一代码库实施多个部署非常有用。'
- en: '**Dependencies**: This factor is, of course, dependent on the programming language
    and framework of choice. However, modern container architectures solve the dependency
    issue in different stages. There is usually one dependency management solution
    and declaration at a language level (such as Maven for Java projects and npm for
    JavaScript), useful for building and running the application prior to containerization
    (as in the dev environment, on a local developer machine).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖关系**：这个因素当然取决于所选择的编程语言和框架。然而，现代容器架构在不同的阶段解决了依赖问题。通常有一个依赖管理解决方案和声明在语言级别（例如
    Java 项目的 Maven 和 JavaScript 的 npm），这对于在容器化之前构建和运行应用程序很有用（如在开发环境，在本地开发机器上）。'
- en: Then, when containers come into play, their filesystem layering technology can
    further separate and declare the dependencies from the application (which constitutes
    the very top layer of a container). Moreover, the application technology is basically
    able to formalize every dependency of the application, including the runtime (such
    as JVM) and the OS version and utilities (which is an inherent capability of the
    container technology).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当容器发挥作用时，它们的文件系统分层技术可以进一步将依赖关系与应用程序（构成容器最顶层）分离和声明。此外，应用程序技术基本上能够形式化应用程序的每个依赖关系，包括运行时（如
    JVM）和操作系统版本以及实用工具（这是容器技术固有的能力）。
- en: '`ConfigMap` objects and making them available to the application as environment
    variables or configuration files. That makes it pretty easy to integrate into
    almost every programming language and framework, and makes the configuration easy
    to be versioned and organized per environment. This is also a nice way to standardize
    configuration management regardless of the technology used.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `ConfigMap` 对象并将它们作为环境变量或配置文件提供给应用程序。这使得它几乎可以集成到任何编程语言和框架中，并使得配置易于按环境进行版本控制和组织。这也是一种很好的方式，无论使用什么技术，都可以标准化配置管理。
- en: '**Backing services**: This factor can be mapped one to one to the Kubernetes
    Services object. A cloud-native application can easily query the Kubernetes API
    to retrieve the service it needs, by name or by using a selector. However, it''s
    worth noticing that Kubernetes does not allow a Pod to explicitly declare the
    dependencies to other Services, likely because it delegates the handling of corner
    cases (such as a service missing or crashing, or the need for reconnection) to
    each application. There are, however, multiple ways (such as using Helm charts
    or operators) in which to set up multiple Pods together, including the Services
    to talk to each other.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端服务**：这个因素可以一一映射到 Kubernetes 的 Services 对象。一个云原生应用可以轻松查询 Kubernetes API，通过名称或使用选择器来检索所需的服务的详细信息。然而，值得注意的是，Kubernetes
    不允许 Pod 明确声明对其他服务的依赖，这可能是由于它将处理边缘情况（例如服务缺失或崩溃，或需要重新连接）的任务委托给每个应用程序。然而，有多种方式（例如使用
    Helm 图表或操作员）可以一起设置多个 Pod，包括相互通信的服务。'
- en: '`config`) needed to import the containerized application into Kubernetes. Last
    but not least, Kubernetes handles (using the container runtime of choice) the
    running of the application.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要导入容器化应用程序到 Kubernetes 的 `config`)。最后但同样重要的是，Kubernetes 处理（使用选择的容器运行时）应用程序的运行。
- en: '**Processes**: This factor is also quite well represented in Kubernetes. Indeed,
    each container is, by default, confined in its own runtime, while sharing nothing
    between each other. We know that containers are stateless too. A common strategy
    for handling the state is by using external resources, such as connections to
    databases, services, or persistent volumes. It''s worth observing that Kubernetes,
    by using DaemonSets and similar constructs, allows exceptions to this behavior.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程**：这个因素在 Kubernetes 中也得到了很好的体现。实际上，每个容器默认情况下都是在其自己的运行时中隔离的，彼此之间不共享任何资源。我们也知道容器是无状态的。处理状态的一个常见策略是使用外部资源，例如数据库连接、服务或持久卷。值得注意的是，Kubernetes
    通过使用 DaemonSets 和类似的结构，允许对此行为进行例外处理。'
- en: '**Port binding**: Even in this case, Kubernetes and containers allow all the
    requisite infrastructure to implement apps adhering to the port binding factor.
    Indeed, with Kubernetes, you can declare the port that your node will listen to
    (and Kubernetes will manage conflicts that could potentially arise between Pods
    asking for the same port). With Services, you can add additional capabilities
    to it, such as port forwarding and load balancing.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口绑定**：即使在这种情况，Kubernetes 和容器也允许实现符合端口绑定因素的所有必要基础设施。实际上，使用 Kubernetes，您可以声明您的节点将监听的端口（Kubernetes
    将管理可能出现的 Pod 请求相同端口的冲突）。通过服务，您可以添加额外的功能，例如端口转发和负载均衡。'
- en: '**Concurrency**: This is inherent to the Kubernetes containers model. You can
    easily define the number of instances each Pod should run at any point in time.
    The infrastructure guarantees that all the required resources are allocated for
    each Pod.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发性**：这是 Kubernetes 容器模型固有的。您可以轻松定义每个 Pod 在任何时间点应运行的实例数量。基础设施保证为每个 Pod 分配所有必需的资源。'
- en: '**Disposability**: In Kubernetes, the Pod life cycle is managed to allow each
    application to shut down gracefully. Indeed, Kubernetes can shut down each Pod
    and prevent new network traffic from being routed to that specific Pod, hence
    providing the basics for zero downtime and zero data loss. Then, Kubernetes can
    be configured to run a pre-stop hook, to allow a custom action to be done before
    the shutdown.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可丢弃性**：在 Kubernetes 中，Pod 生命周期被管理以允许每个应用程序优雅地关闭。实际上，Kubernetes 可以关闭每个 Pod
    并阻止新的网络流量被路由到该特定 Pod，从而为零停机时间和零数据损失提供基础。然后，Kubernetes 可以配置为运行预停止钩子，以便在关闭之前执行自定义操作。'
- en: 'Following that, Kubernetes sends a **SIGTERM** signal (which is a standard
    Linux termination signal) to communicate with the application with the intention
    of stopping it. The application is considered to be responsible for trapping and
    managing such a signal (disposing of resources and shutting down) if it''s still
    up and running. Finally, after a timeout, if the application has not yet stopped,
    Kubernetes forcefully terminates it by using a **SIGKILL** signal (which is a
    more drastic signal than SIGTERM, meaning that it cannot be ignored by the application
    that will be terminated). Something similar can be said for the startup: Kubernetes
    can be configured to do some actions in case the start of a Pod goes wrong (as
    an example, taking too long).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，Kubernetes 发送一个 **SIGTERM** 信号（这是一个标准的 Linux 终止信号），目的是与应用程序通信并意图停止它。如果应用程序仍然在运行，则认为应用程序负责捕获和管理此类信号（释放资源并关闭）。最后，在超时后，如果应用程序尚未停止，Kubernetes
    将通过使用 **SIGKILL** 信号（这是一个比 SIGTERM 更强烈的信号，意味着即将终止的应用程序无法忽略它）强制终止它。对于启动过程，也可以说类似的情况：Kubernetes
    可以配置为在 Pod 启动失败时执行某些操作（例如，启动时间过长）。
- en: To do this, each application can be instrumented with probes, to detect exactly
    when an application is running and is ready to take new traffic. So, even in this
    case, the infrastructure provides all the necessary pieces to create an application
    compliant with this specific factor.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，每个应用程序都可以通过探针进行配置，以精确检测应用程序何时正在运行并准备好接受新的流量。因此，即使在这种情况，基础设施也提供了创建符合这一特定因素的应用程序所需的所有必要组件。
- en: '**Dev/prod parity**: Similar to the other factors in this list, this is more
    about the processes and disciplines practiced in your particular development life
    cycle, meaning that no tool can ensure adherence to this factor if there is no
    willingness to do so. However, with Kubernetes natively being a declarative environment,
    it''s pretty easy to define different environments (normally mapping to namespaces)
    that implement each development stage needed (such as dev and prod) and make such
    environments are as similar as possible (with frequent deploys, implementing checks
    if the versions and configuration differ too much, and using the same kind of
    backing services).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发/生产一致性**：与列表中的其他因素类似，这更多是关于在您的特定开发生命周期中实践的过程和纪律，这意味着如果没有意愿这样做，则没有任何工具可以确保遵守这一因素。然而，由于
    Kubernetes 本身就是一个声明性环境，因此很容易定义不同的环境（通常映射到命名空间），以实现每个开发阶段所需的环境（如开发和生产），并使这些环境尽可能相似（通过频繁部署，实施检查版本和配置是否差异过大，并使用相同类型的后端服务）。'
- en: '**Logs**: These play a big part in Kubernetes architecture as there are many
    alternatives to manage them. The most important lesson, however, is that a big
    and complex infrastructure based on Kubernetes is mandated to use some log collection
    strategy (usually based on dealing with logs as an event stream). Common implementations
    of such an approach include using Fluentd as a log collector or streaming log
    lines into a compatible event broker (such as Kafka).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志**：在Kubernetes架构中，它们扮演着重要角色，因为有许多方法可以管理它们。然而，最重要的教训是，基于Kubernetes的庞大而复杂的基础设施必须使用某种日志收集策略（通常基于将日志作为事件流处理）。此类方法的常见实现包括使用Fluentd作为日志收集器或将日志行流式传输到兼容的事件代理（如Kafka）。'
- en: '**Admin processes**: This is perhaps a bit less directly mapped to Kubernetes
    and container concepts and more related to the specific language, framework, and
    development approaches that are used. However, Kubernetes allows containers to
    be accessed using a shell, so this way it can be used if the Pod provides the
    necessary administrative shell tools. Another approach can be to run specific
    Pods that can use the same technologies as our applications, just for the time
    needed to perform administrative processes.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理过程**：这可能不太直接与Kubernetes和容器概念相关联，而更多地与所使用的特定语言、框架和开发方法相关。然而，Kubernetes允许通过shell访问容器，因此如果Pod提供了必要的行政shell工具，则可以使用这种方式。另一种方法是可以运行特定的Pod，这些Pod可以使用与我们的应用程序相同的技术，但仅限于执行管理过程所需的时间。'
- en: As I've said many other times, there is no magic recipe for an application to
    be cloud-native (or microservices-compliant, or simply performant and well written).
    However, the twelve factors provide an interesting point of view and give some
    food for thought. Some of the factors are achievable by using features provided
    by the hosting platform or other dependencies (think about config or logs), while
    others are more related to the application architecture (backing services and
    disposability) or development model (codebase and dev/prod parity).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我多次说过的，没有一个神奇的方法可以使应用程序成为云原生（或微服务兼容，或仅仅性能良好且编写得很好）。然而，十二要素提供了一个有趣的观点，并引发了一些思考。其中一些要素可以通过使用托管平台或其他依赖项提供的功能来实现（想想配置或日志），而其他要素则更多地与应用程序架构（后端服务和可丢弃性）或开发模式（代码库和开发/生产一致性）相关。
- en: Following (and extending, where needed) this set of practices will surely be
    beneficial for the application's performance, resiliency, and cloud readiness.
    To go further in our analysis, let's look at some of the reasoning behind well-known
    patterns for cloud-native development and what supporting technologies we can
    use to implement them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循（并在需要时扩展）这一系列实践肯定会对应用程序的性能、弹性和云就绪性产生益处。为了进一步分析，让我们看看云原生开发的一些常见模式背后的推理，以及我们可以使用哪些支持技术来实现它们。
- en: Well-known issues in the cloud-native world
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云原生世界的常见问题
- en: Monolithic applications, while having many downsides (especially in the area
    of Deployment frequency and scalability), often simplify and avoid certain issues.
    Conversely, developing an application as cloud-native (hence, a distributed set
    of smaller applications) implies some intrinsic questions to face. In this section,
    we are going to see some of those issues. Let's start with fault tolerance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 单体应用程序，尽管有许多缺点（特别是在部署频率和可扩展性方面），通常可以简化并避免某些问题。相反，将应用程序作为云原生（因此，一组较小的分布式应用程序）开发意味着需要面对一些固有的问题。在本节中，我们将探讨一些这些问题。让我们从容错性开始。
- en: Fault tolerance
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容错性
- en: '**Fault tolerance** is an umbrella term for a number of aspects related to
    resiliency. The concept basically boils down to protecting the service from the
    unavailability (or minor failures) of its components. In other words, if you have
    chained services (which is very common, maybe between microservices composing
    your application or when calling external services), you may want to protect the
    overall application (and user experience), making it resilient to the malfunction
    of some such services.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**容错性**是一个涵盖与弹性相关的一系列方面的术语。这个概念基本上可以归结为保护服务免受其组件不可用（或轻微故障）的影响。换句话说，如果您有链式服务（这在微服务组成的应用程序之间或调用外部服务时非常常见），您可能希望保护整个应用程序（和用户体验），使其对某些此类服务的故障具有弹性。'
- en: By architecting your application in this way, you can avoid overstressing downstream
    components that are already misbehaving (such as giving exceptions or taking too
    long) and/or implementing a graceful degradation of the application's behavior.
    Fault tolerance can be obtained in various ways.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: To keep this section practical and interesting, I am going to provide an introduction
    to each pattern and discuss how this can be implemented in Java. As a reference
    architecture, we are keeping the MicroProfile (as per what we saw in [*Chapter
    7*](B16354_07_Final_JM_ePUB.xhtml#_idTextAnchor164), *Exploring Middleware and
    Frameworks*), so we can have a vendor-independent implementation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most famous fault-tolerance technique is the **circuit breaker**. It became
    famous thanks to a very widespread implementation, which is **Netflix Hystrix**
    (now no longer actively developed). Resilience4j is widely accepted and commonly
    maintained as an alternative.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: The circuit breaker pattern implies that you have a configurable threshold when
    calling another service. If such a call fails, according to the threshold, the
    circuit breaker will open, blocking further calls for a configurable amount of
    time. This is similar to a circuit breaker in an electrical plant, which will
    open in case of issues, preventing further damages.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'This will allow the next calls to fail fast and avoid further calling to the
    downstream service (which may likely be already overloaded). The downstream system
    then has some time to recover (perhaps by manual intervention, with an automatic
    restart, or autoscaling). Here is an example where a circuit breaker is not implemented:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Without a circuit breaker'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.5_B16354.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Without a circuit breaker
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the preceding diagram, without a circuit breaker, calls to
    a failed service (due to a crash or similar outages) keep going in timeout. This,
    in a chain of calls, can cause the whole application to fail. In the following
    example, we''ll implement a circuit breaker:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – With a circuit breaker'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.6_B16354.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – With a circuit breaker
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, in an implementation using a circuit breaker, in the case of a service
    failing, the circuit breaker will immediately identify the outage and provide
    the responses to the service calling (**Service A**, in our case). The response
    sent can simply be an error code (such as HTTP 500) or something more complex,
    such as a default static response or a redirection to an alternative service.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'In a MicroProfile, you can configure a circuit breaker as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The annotation is configured in a way that, if you have a failure ratio of 50%
    over four requests (so two failures in four calls), the circuit breaker will stay
    open for 10 seconds, failing immediately on calls in such a time window (without
    directly calling the downstream instance). However, after 10 seconds, the next
    call will be attempted to the target system. If the call succeeds, `CircuitBreaker`
    will be back to closed, hence working as before. It's worth noticing that the
    circuit breaker pattern (as well as other patterns in this section) can be implemented
    at a service mesh level (especially, in a Kubernetes context). As we saw a couple
    of sections ago, the service mesh works at a network level in Pod-to-Pod communication
    and can then be configured to behave as a circuit breaker (and more).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Fallback
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **fallback** technique is a good way to implement a *plan B* against external
    services not working. This will allow for a graceful fallback if the service fails,
    such as a default value or calling an alternative service.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this in a MicroProfile, you can simply use the following annotation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this way, if you get an exception in your `getMyPojo` method, `myfallbackmethod`
    will be called. Such methods must, of course, have a compatible return value.
    The fallback method, as said, may be an alternative implementation for such default
    values or different external services.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Retries
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another powerful way to deal with non-working services is to **retry**. This
    may work well if the downstream service has intermittent failures, but it will
    answer correctly or fail in a reasonable amount of time.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, you can decide that it''s good enough to retry the call in
    the event of a failure. In a MicroProfile, you can do that using the following
    annotation:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, the maximum number of retries and the delay between each retry
    are configurable with the annotation. Of course, this kind of approach may lead
    to a high response time if the downstream system does not fail fast.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Timeout
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Last but not least, the **timeout** technique will precisely address the problem
    that we have just seen. Needless to say, a timeout is about timeboxing a call,
    imposing a maximum amount of time for it to be completed before an exception is
    raised.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'In a MicroProfile, you can simply annotate a method and be sure that the service
    call will succeed or fail within a configured amount of time:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In such a configuration, the desired service will have to complete the execution
    within `300` ms or will fail with a timeout exception. In this way, you can have
    a predictable amount of time in your chain of services, even if your external
    service takes too much time to answer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: All the techniques discussed in this section aim to enhance the resiliency of
    cloud-native applications and address one very well-known problem of microservices
    (and, more generally, distributed applications), which is failure cascading. Another
    common issue in the cloud-native world concerns transactionality.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中讨论的所有技术旨在增强云原生应用的弹性，并解决微服务（以及更广泛的分布式应用）的一个非常著名的问题，即故障级联。在云原生世界中，另一个常见问题与事务性相关。
- en: Transactionality
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务性
- en: When working on a classical monolithic Java application, **transactions** are
    kind of a resolved issue. You can appropriately mark your code and the container
    you are running in (be it an application server or other frameworks) to take care
    of it. This means that all the things that you can expect from a transactional
    **Atomicity**, **Consistency**, **Isolation**, **Durability** (**ACID**)-compliant
    system are provided, such as rollbacks in the case of failures and recovery.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当在经典单体Java应用程序上工作时，**事务**似乎是一个已经解决的问题。你可以适当地标记你的代码和你运行的容器（无论是应用程序服务器还是其他框架），以处理它。这意味着所有你可以从符合**原子性**、**一致性**、**隔离性**、**持久性**（**ACID**）的系统期望的东西都得到了提供，例如在失败情况下的回滚和恢复。
- en: In a distributed system, this works differently. Since the components participating
    in a transaction are living in different processes (that may be different containers,
    optionally running on different servers), traditional transactionality is not
    a viable option. One intuitive explanation for this is related to the network
    connection between each participant in the transaction.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，这有所不同。由于参与事务的组件生活在不同的进程中（这些进程可能是不同的容器，可选地运行在不同的服务器上），传统的可串行性不是一个可行的选项。一个直观的解释与事务中每个参与者之间的网络连接有关。
- en: If one system asks another to complete an action (such as persisting a record),
    no answer might be returned. *What should the client do then?* It should assume
    that the action has been successful and the answer is not coming for external
    reasons (such as a network split), or it should assume that the action has failed
    and optionally retry. Of course, there is no easy way to face these kinds of events.
    The following are a couple of ideas for dealing with data integrity that can be
    adopted.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个系统请求另一个系统完成一个动作（例如持久化一个记录），可能不会返回任何答案。*那么客户端应该怎么做呢？* 它应该假设该操作已成功，并且答案没有返回是由于外部原因（例如网络分裂），或者它应该假设该操作已失败，并可选择重试。当然，面对这类事件没有简单的办法。以下是一些处理数据完整性的想法，可以采用。
- en: Idempotent actions
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 幂等操作
- en: One way to solve some issues due to distributed transactionality is **idempotency**.
    A service is considered idempotent if it can be safely called more than once with
    the same data as input, and the result will still be a correct execution. This
    is something naturally obtained in certain kinds of operations (such as read operations
    or changes to specific information, such as the address or other data of a user
    profile), while it must be implemented in some other situations (such as money
    balance transactions, where a double charge for the same payment transaction is,
    of course, not allowed).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 解决分布式事务性引起的一些问题的方法之一是**幂等性**。如果一个服务可以在安全地多次使用相同的数据作为输入调用，并且结果仍然是正确的执行，那么这个服务被认为是幂等的。这在某些类型的操作中是自然获得的（例如读取操作或对特定信息的更改，如用户的地址或其他数据），而在某些其他情况下（如货币余额交易，对于同一支付交易的重复收费当然是不允许的）则必须实现。
- en: The most common way to correctly handle idempotency relies on a specific key
    associated with each call, which is usually obtained from the payload itself (as
    an example, calculating a hash over all the data passed). Such a key is then stored
    in a repository (this can be a database, filesystem, or in-memory cache). A following
    call to the same service with the same payload will create a clash on such a key
    (meaning that the key already exists in the repository) and will then be handled
    as a no-operation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正确处理幂等性最常见的方法依赖于与每个调用相关联的特定键，这通常从有效负载本身获得（例如，对所有传递的数据计算哈希）。然后，将此键存储在存储库中（这可以是数据库、文件系统或内存缓存）。具有相同有效负载的后续调用到同一服务将在此键上产生冲突（意味着该键已存在于存储库中），然后将其作为无操作处理。
- en: So, in a system implemented in this way, it's safe to call a service more than
    once (as an example, in case we got no response from the first attempt). In the
    event that the first call was successful and we received no answer for external
    reasons (such as a network drop), the second call will be harmless. It's common
    practice to define an expiration for such entries, both for performance reasons
    (avoiding growing the repository too much, since it will be accessed at basically
    every call) and for correctly supporting the use case of your specific domain
    (for instance, it may be that a second identical call is allowed and legitimate
    after a specific timeout is reached).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种方式实现的系统中，多次调用服务是安全的（例如，如果我们第一次尝试没有收到响应）。如果第一次调用成功，但由于外部原因（例如网络中断）我们没有收到任何回答，第二次调用将无害。定义此类条目的过期时间是一种常见做法，这既是为了性能原因（避免仓库增长过多，因为基本上每次调用都会访问它）也是为了正确支持您特定领域的用例（例如，在达到特定超时后，可能允许并合法地进行第二次相同的调用）。
- en: The Saga pattern
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Saga 模式
- en: The **Saga pattern** is a way to deal with the problem of transactions in a
    distributed environment more fully. To implement the Saga pattern, each of the
    systems involved should expose the opposite of each business operation that includes
    updating the data. In the payments example, a charge operation (implying a write
    on a data source, such as a database) should have a twin *undo* operation that
    implements a top-up of the same amount of money (and likely provides some more
    descriptions, such as the reason for such a cancellation).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '** Saga 模式** 是一种更全面地处理分布式环境中事务问题的方法。要实现 Saga 模式，涉及到的每个系统都应该暴露每个包含更新数据的业务操作的相反操作。在支付示例中，一个扣款操作（意味着对数据源，如数据库的写入）应该有一个双倍
    *撤销* 操作，该操作实现相同金额的充值（并且可能提供一些更多描述，例如取消此类操作的原因）。'
- en: 'Such a complementary operation is called compensation, and the goal is to undo
    the operation in the event of a failure somewhere else. Once you have a list of
    services that must be called to implement a complex operation and the compensation
    for each of them, the idea is to call each service sequentially. If one of the
    services fails, all the previous services are notified, and the undo operation
    is called on them to put the whole system in a state of consistency. An alternative
    way to implement this is to call the first service, which will be in charge of
    sending a message to the second service, and so on. If one of the services fails,
    the messages are sent back to trigger the requisite compensation operations. There
    are two warnings regarding this approach:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这种互补操作被称为补偿，其目标是如果在其他地方发生故障，则撤销操作。一旦你有一个必须调用的服务列表来实施复杂操作以及每个服务的补偿，那么想法就是依次调用每个服务。如果其中一个服务失败，所有前面的服务都会被通知，并且会在它们上调用撤销操作，以使整个系统处于一致性状态。实现这种方法的另一种方式是调用第一个服务，该服务将负责向第二个服务发送消息，依此类推。如果其中一个服务失败，消息将被发送回触发必要的补偿操作。关于这种方法有两个警告：
- en: The signaling of the operation outcome after each write operation (which will
    trigger the compensations in case of a failure) must be reliable. So, the case
    in which a failure happens somewhere and the compensations are not called must
    be avoided as far as possible.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次写入操作后对操作结果的信号（这将触发补偿，以防发生故障）必须是可靠的。因此，必须尽可能避免在某个地方发生故障而补偿没有被调用的这种情况。
- en: The whole system must be considered as eventually consistent. This means that
    there are some specific timeframes (likely to be very short) in which your system
    is not in a consistent state (because the downstream systems are yet to be called,
    or a failure just happened and the compensations are not yet executed).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个系统必须被视为最终一致性的。这意味着在某些特定的时间框架内（可能非常短），您的系统不在一致性状态（因为下游系统尚未被调用，或者刚刚发生故障且补偿尚未执行）。
- en: An elegant way to implement this pattern is based on the concept of change data
    capture. **Change data capture** is a pattern used for listening to changes on
    a data source (such as a database). There are many different technologies to do
    that, including the polling of the data source or listening for some specific
    events in the database transaction logs. By using change data capture, you can
    be notified when a write happens in the data source, which data is involved, and
    whether the write has been successful. From such events, you can trigger a message
    or a call for the other systems involved, continuing your distributed transaction
    or *rolling back* by executing the compensation methods.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此模式的一种优雅方式是基于变更数据捕获的概念。**变更数据捕获**是一种用于监听数据源（如数据库）上变化的模式。有许多不同的技术可以做到这一点，包括轮询数据源或监听数据库事务日志中的某些特定事件。通过使用变更数据捕获，您可以在数据源发生写入时收到通知，涉及哪些数据，以及写入是否成功。从这些事件中，您可以触发消息或调用其他系统，继续您的分布式事务或通过执行补偿方法来*回滚*。
- en: The Saga pattern, in a way, makes us think about the importance of the flow
    of calls in a microservices application. As seen in this section (and also in
    the *Why create cloud-native applications?* section regarding resiliency), the
    order (and the way) in which we call the services needed to compose our functionality
    can change the transactionality, resiliency, and efficiency (think about parallel
    versus serial, as discussed in [*Chapter 6*](B16354_06_Final_JM_ePUB.xhtml#_idTextAnchor141),
    *Exploring Essential Java Architectural Patterns*, under the *Implementing for
    large-scale adoption* section). In the next section, we are going to elaborate
    a bit more on this point.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，Saga模式让我们思考微服务应用中调用流的重要性。正如本节（以及关于弹性的*为什么创建云原生应用？*部分）中所示，我们调用所需服务以组合我们的功能的顺序（以及方式）可以改变事务性、弹性和效率（想想并行与串行，如在第[*第6章*](B16354_06_Final_JM_ePUB.xhtml#_idTextAnchor141)，*探索关键Java架构模式*下的*为大规模采用实现*部分所讨论的）。在下一节中，我们将更详细地阐述这一点。
- en: Orchestration
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编排
- en: The Saga pattern highlights the sequence (and operations) in which every component
    must be called in order to implement eventual consistency. This is a topic that
    we have somewhat taken for granted.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Saga模式突出了每个组件必须调用的顺序（和操作），以便实现最终一致性。这是一个我们多少有些视为理所当然的话题。
- en: 'We have talked about the microservice characteristics and ways of modeling
    our architectures in order to be flexible and define small and meaningful sets
    of operations. *But what''s the best way to compose and order the calls to those
    operations, so as to create the higher-level operations implementing our use case?*
    As usual, there is no easy answer to this question. The first point to make concerns
    the distinction between **orchestration** and another technique often considered
    an alternative to it, which is **choreography**. There is a lot of debate ongoing
    about the differences between orchestration and choreography. I don''t have the
    confidence to speak definitively on this subject, but here is my take on it:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了微服务的特性和建模我们架构的方法，以便使其灵活并定义小而意义明确的操作集。*但是，如何最佳地组合和排序对这些操作的调用，以创建实现我们用例的高级操作呢？*
    通常，这个问题没有简单的答案。首先需要指出的是**编排**与另一种常被视为其替代技术的区别，即**编排**。关于编排和编排之间的区别，存在很多争论。我没有信心对这个主题做出明确的陈述，但以下是我对这个问题的看法：
- en: '**Orchestration**, as in an orchestra, implies the presence of a conductor.
    Each microservice, like a musician, can use many services (many sounds, if we
    stay within the metaphor), but it looks for hints from the conductor to make something
    that, in cooperation with the other microservices, simply works.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编排**，就像一个管弦乐队一样，意味着存在一个指挥。每个微服务，就像一个音乐家，可以使用许多服务（如果我们保持在这个比喻中，就是许多声音），但它会寻找指挥的提示，以使与其他微服务合作时，简单有效地工作。'
- en: '**Choreography**, as in a dance, is studied beforehand and requires each service
    (dancer) to reply to an external event (other dancers doing something, music playing,
    and so on). In this case, we see some similarities with what we saw in the *Event-driven
    and reactive* section of [*Chapter 6*](B16354_06_Final_JM_ePUB.xhtml#_idTextAnchor141),
    *Exploring Essential Java Architectural Patterns*).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we are going to focus on orchestration.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating in the backend or the frontend
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A first, simple approach to orchestration implies a **frontend aggregation**
    level (this may be on the client side or server side). This essentially means
    having user experience (as in the flow of different pages, views, or whatever
    the client technology provides) dictate how the microservice functions are called.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this approach is that it's easy and doesn't need extra layers,
    or other technology in your architecture, to be implemented.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The downsides, in my opinion, are more than one. First of all, you are tightly
    coupling the behavior of the application with the technical implementation of
    the frontend. If you need to change the flow (or any specific implementation to
    a service), you are most likely required to make changes in the frontend.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if you need to have more than one frontend implementation (which is
    very common, as we could have a web frontend and a couple of mobile applications),
    the logic will become sprawled in all of those frontends and a change must be
    propagated everywhere, thereby increasing the possibility of making mistakes.
    Last but not least, directly exposing your services to the frontend may imply
    having a mismatch of granularity between the amount of data microservices offer
    with the amount of data the frontend will need. So, choices you may need to make
    in the frontend (as pagination) will need to slip into the backend microservices
    implementation. This is not the best solution, as every component will have unclear
    responsibilities.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The obvious alternative is moving the orchestration to the backend. This means
    having a component between the microservices implementing the backend and the
    technologies implementing the frontend, which has the role of aggregating the
    services and providing the right granularity and sequence of calls required by
    the frontend.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the fun begins: *How should this component be implemented?*'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: One common alternative to aggregating at the frontend level is to pick an API
    gateway to do the trick. **API gateways** are pieces of software, loosely belonging
    to the category of integration middlewares, that sit as a man in the middle between
    the backend and frontend, and proxy the API calls between the two. An API gateway
    is an infrastructural component that is commonly equipped with additional features,
    such as authentication, authorization, and monetization.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that API gateways are usually not really tools designed to handle
    aggregation logic and sequences of calls. So sometimes, they are not capable of
    handling complex orchestration capabilities, but simply aggregate more calls into
    one and perform basic format translation (such as SOAP to REST).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'A third option is to use a **custom aggregator**. This means delegating one
    (or more than one) microservices to act as an orchestrator. This solution provides
    the maximum level of flexibility with the downside of centralizing a lot of functionalities
    into a single architectural component. So, you have to be careful to avoid scalability
    issues (so it must be appropriately scalable) or the solution becoming a single
    point of failure (so it must be appropriately highly available and resilient).
    A custom aggregator implies a certain amount of custom code in order to define
    the sequence of calls and the integration logic (such as formal translation).
    There are a couple of components and techniques that we have discussed so far
    that can be embedded and used in this kind of component:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: A **business workflow** (as seen in [*Chapter 8*](B16354_08_Final_JM_ePUB.xhtml#_idTextAnchor200),
    *Designing Application Integration and Business Automation*) can be an idea for
    describing the sequence of steps orchestrating the calls. The immediate advantage
    is having a graphical, standard, and business-understandable representation of
    what a higher-level service is made of. However, this is not a very common practice,
    because the current technology of business workflow engines is designed for a
    different goal (being a stateful point to persist process instances).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, it may have a performance impact and be cumbersome to implement (as BPMN
    is a business notation, while this component is inherently technological). So,
    if this is your choice, it is worthwhile considering a lightweight, non-persistent
    workflow engine.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration patterns** are to be considered with a view of implementing complex
    aggregation (such as protocol translation) and composition logic (such as sequencing
    or parallelizing calls). Even in this case, to keep the component scalable and
    less impactful from a performance standpoint, it is worthwhile considering lightweight
    integration platforms and runtimes.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **fault-tolerance** techniques that we saw a couple of sections ago are
    a good fit in this component. They will allow our composite call to be resilient
    in case of the unavailability of one of the composing services and to fail fast
    if one of them is misbehaving or simply answering slowly. Whatever your choice
    for the aggregation component, you should consider implementing fault tolerance
    using the patterns seen.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, a consideration to be made about orchestration is whether
    and how to implement the **backend for frontend** pattern (as briefly seen in
    [*Chapter 4*](B16354_04_Final_JM_ePUB.xhtml#_idTextAnchor089), *Best Practices
    for Design and Development*). To put it simply, different frontends (or better,
    different clients) may need different formats and granularity for the higher-level
    API. A web UI requires a different amount of data (and of different formats) than
    a mobile application, and so on. One way to implement this is to create a different
    instance of the aggregation component for each of the frontends. In this way,
    you can slightly change the frontend calls (and the user experience) without impacting
    the microservices implementation in the backend.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: However, as with the frontend aggregation strategy, a downside is that the business
    logic becomes sprawled across all the implementations (even if, in this case,
    you at least have a weaker coupling between the frontend and the orchestration
    component). In some use cases, this may lead to inconsistency in the user experience,
    especially if you want to implement omnichannel behavior, as in, you can start
    an operation in one of the frontends (or channels) and continue with it in another
    one. So, if you plan to have multiple aggregation components (by means of the
    backend for frontend pattern), you will likely need to have a consistency point
    somewhere else (such as a database persisting the state or a workflow engine keeping
    track of the current and previous instances of each call).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: This section concludes our overview of microservices patterns. In the next section,
    we are going to consider when and how it may make sense to adopt microservices
    and cloud-native patterns or evolve existing applications toward such paradigms.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Adopting microservices and evolving existing applications
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, we had an overview of the benefits of microservices applications and some
    of their particular characteristics. I think it is now relevant to better consider
    why you should (or should not) adopt this architectural style. This kind of consideration
    can be useful both for the creation of new applications from scratch (in what
    is called *green-field development*) and modernization (termed *brown-field applications*).
    Regarding the latter aspect, we will discuss some of the suggested approaches
    for modernizing existing applications in the upcoming sections.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'But back to our main topic for this section: *why should you adopt the microservices-based
    approach?*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The first and most important reason for creating microservices is the **release
    frequency**. Indeed, the most famous and successful production experiences of
    microservices applications are related to services heavily benefitting from being
    released often.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: This is because a lot of features are constantly released and experimented with
    in production. Remembering what we discussed in relation to the Agile methodologies,
    doing so allows us to test what works (what the customers like), provide new functionalities
    often (to stay relevant to the market), and quickly fix issues (which will inevitably
    slip into production because of the more frequent releases).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the first question to ask is: *Will your application benefit
    from frequent releases?* We are talking about once a week or more. Some well-known
    internet applications (such as e-commerce and streaming services) even push many
    releases in production every day.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: So, if the service you are building will not benefit from releasing this often
    – or worse, if it's mandated to release according to specific timeframes – you
    may not need to fully adhere to the microservices philosophy. Instead, it could
    turn out to be just a waste of time and money, as of course, the application will
    be much more complicated than a simple monolithic or *n*-tier application.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is **scalability**. As stated before, many successful
    production implementations of microservices architectures are related to streaming
    services or e-commerce applications. Well, that's not incidental. Other than requiring
    constant experimentation and the release of new features (hence, release frequency),
    such services need to scale very well. This means being able to handle many concurrent
    users and absorbing peaks in demand (think about Black Friday in an e-commerce
    context, or the streaming of live sporting events). That's supposed to be done
    in a cost-effective way, meaning that the resource usage must be minimized and
    allocated only when it is really needed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'So, I think you get the idea: microservices architectures are supposed to be
    applied to projects that need to handle thousands of concurrent requests and that
    need to absorb peaks of 10 times the average load. If you only need to manage
    much less traffic, once again microservices could be overkill.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: A less obvious point to consider is **data integrity**. As we mentioned a few
    sections ago, when talking about the Saga pattern, a microservices application
    is a heavily distributed system. This implies that transactions are hard or maybe
    impossible to implement. As we have seen, there are workarounds to mitigate the
    problem, but in general, everybody (especially business and product managers)
    should be aware of this difficulty.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: It should be thoroughly explained that there will be features that may not be
    updated in real time, providing stale or inaccurate data (and maybe some missing
    data too). The system as a whole may have some (supposedly short) timeframes in
    which it's not consistent. Note that it is a good idea to contextualize and describe
    which features and scenarios may present these kinds of behaviors to avoid bad
    surprises when testing.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, on the technical design side, we should ensure we integrate
    all possible mechanisms to keep these kinds of misalignments to a minimum, including
    putting in place all the safety nets required and implementing any reconciliation
    procedure that may be needed, in order to provide a satisfactory experience to
    our users.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Once again, if this is not a compromise that everybody in the project team is
    willing to make, maybe microservices should be avoided (or used for just a subset
    of the use cases).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have already seen in [*Chapter 5*](B16354_05_Final_JM_ePUB.xhtml#_idTextAnchor109),
    *Exploring the Most Common Development Models*, a prerequisite for microservices
    and cloud-native architectures is to be able to operate as a DevOps team. That''s
    not a minor change, especially in big organizations. But the implications are
    obvious: since each microservice has to be treated as a product with its own release
    schedule, and should be as independent as possible, then each team working on
    each microservice should be self-sufficient, breaking down silos and maximizing
    collaboration between different roles. Hence, DevOps is basically the only organizational
    model known to work well in supporting a microservices-oriented project. Once
    again, this is a factor to consider: if it''s hard, expensive, or impossible to
    adopt this kind of model, then microservices may not be worth it.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'An almost direct consequence of this model is that each team should have a
    supporting technology infrastructure that is able to provide the right features
    for microservices. This implies having an automated release process, following
    the CI/CD best practices (we will see more about this in [*Chapter 13*](B16354_13_Final_JM_ePUB.xhtml#_idTextAnchor307),
    *Exploring the Software Life Cycle*). And that''s not all: the environments for
    each project should also be easy to provision on-demand, and possibly in a self-service
    fashion. Kubernetes, which we looked at a couple of sections ago, is a perfect
    fit for this.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: It is not the only option, however, and in general, cloud providers offer great
    support to accelerate the delivery of environments (both containers and VMs) by
    freeing the operations teams from many responsibilities, including hardware provisioning
    and maintaining the uptime of some underlying systems.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it will be very hard (or even impossible) to implement microservices
    if we rely on complex manual release processes, or if the infrastructure we are
    working on is slow and painful to extend and doesn't provide features for the
    self-service of new environments.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of the microservices architecture is the **extensibility**
    and **replaceability** of each component. This means that each microservice is
    related to the rest of the architecture via a well-defined API and can be implemented
    with the technology best suited for it (in terms of the language, frameworks,
    and other technical choices). Better yet, each component may be evolved, enhanced,
    or replaced by something else (a different component, an external service, or
    a SaaS application, among others). So, of course, as you can imagine, this has
    an impact in terms of integration testing (more on that in [*Chapter 13*](B16354_13_Final_JM_ePUB.xhtml#_idTextAnchor307),
    *Exploring the Software Life Cycle*), so you should really consider the balance
    between the advantages provided and the impact created and resources needed.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: So, as a summary for this section, microservices provide a lot of interesting
    benefits and are a really cool architectural model, worth exploring for sure.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, before you decide to implement a new application following
    this architectural model, or restructuring an existing one to adhere to it, you
    should consider whether the advantages will really outweigh the costs and disadvantages
    by looking at your specific use case and whether you will actually use these advantages.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: If the answer is no, or partially no, you can still take some of the techniques
    and best practices for microservices and adopt them in your architecture.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'I think that''s definitely a very good practice: maybe part of your application
    requires strong consistency and transactionality, while other parts have less
    strict requirements and can benefit from a more flexible model.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Or maybe your project has well-defined release windows (for external constraints),
    but will still benefit from fully automated releases, decreasing the risk and
    effort involved, even if they are not scheduled to happen many times a day.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'So, your best bet is to not be too dogmatic and use a mix-and-match approach:
    in this way, the architecture you are designing will be better suited to your
    needs. Just don''t adopt microservices out of FOMO. It will just be hard and painful,
    and the possibility of success will be very low.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: With that said, the discussion around new developmental and architectural methodologies
    never stops, and there are, of course, some ideas on what's coming next after
    microservices.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Going beyond microservices
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like everything in the technology world, microservices got to an inflection
    point (the *Trough of Disillusionment*, as we called it at the beginning of this
    chapter). The reasoning behind this point is whether the effort needed to implement
    a microservices architecture is worth it. The benefit of well-designed microservices
    architectures, beyond being highly scalable and resilient, is to be very quick
    in deploying new releases in production (and so experiment with a lot of new features
    in the real world, as suggested by the adoption of Agile methodology). But this
    comes at the cost of having to develop (and maintain) infrastructures that are
    way more complex (and expensive) than monolithic ones. So, if releasing often
    is not a primary need of your particular business, you may think that a full microservices
    architecture constitutes overkill.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Miniservices
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this reason, many organizations started adopting a compromise approach,
    sometimes referred to as **miniservices**. A miniservice is something in the middle
    between a microservice and a monolith (in this semantic space, it is regarded
    as a macroservice). There is not a lot of literature relating to miniservices,
    mostly because, it being a compromise solution, each development team may decide
    to make trade-offs based on what it needs. However, there are a number of common
    features:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Miniservices may break the dogma of one microservice and one database and so
    two miniservices can share the same database if needed. However, bear in mind
    that this will mean tighter coupling between the miniservices, so it needs to
    be evaluated carefully on a case-by-case basis.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miniservices may offer APIs of a higher level, thereby requiring less aggregation
    and orchestration. Microservices are supposed to provide specific APIs related
    to the particular domain model (and database) that a particular microservice belongs
    to. Conversely, a miniservice can directly provide higher-level APIs operating
    on different domain models (as if a miniservice is basically a composition of
    more than one microservice).
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miniservices may share the deployment infrastructure, meaning that the deployment
    of a miniservice may imply the deployment of other miniservices, or at least have
    an impact on it, while with microservices, each one is supposed to be independent
    of the others and resilient to the lack of them.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, at the end of the day, miniservices are a *customized* architectural solution,
    relaxing on some microservices requirements in order to focus on business value,
    thereby minimizing the technological impact of a full microservices implementation.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Serverless and Function as a Service
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the last point, we cannot conclude this chapter without talking about serverless.
    At some point in time, many architects started seeing serverless as the natural
    evolution of the microservices pattern. **Serverless** is a term implying a focus
    on the application code with very little to no concern regarding the underlying
    infrastructure. That''s what the *less* part in serverless is about: not that
    there are no servers (of course), but that you don''t have to worry about them.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Looking from this point of view, serverless is truly an evolution of the microservices
    pattern (and PaaS too). While serverless is a pattern, a common implementation
    of it takes the container as the smallest computing unit, meaning that if you
    create a containerized application and deploy it to a serverless platform, the
    platform itself will take care of scaling, routing, security, and so on, thereby
    absolving the developer of responsibility for it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: A further evolution of the serverless platform is referred to as **Function
    as a Service** (**FaaS**). In serverless, in theory, the platform can manage (almost)
    every technology stack, provided that it can be packaged as a container, while
    with FaaS, the developer must comply with a well-defined set of languages and
    technologies (usually Java, Python, JavaScript, and a few others). The advantage
    that balances such a lack of freedom is that the dev does not need to care about
    the layers underlying the application code, which is really just writing the code,
    and the platform does everything else.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: One last core characteristic, common to both serverless and FaaS, is the scale
    to zero. To fully optimize platform usage, the technology implementing serverless
    and FaaS can shut the application down completely if there are no incoming requests
    and quickly spin up an instance when a request comes. For this reason, those two
    approaches are particularly suitable for being deployed on a cloud provider, where
    you will end up paying just for what you need. Conversely, for implementing the
    scale to zero, the kind of applications (both the framework and the use case)
    must be appropriate. Hence, applications requiring a warmup or requiring too long
    to start are not a good choice.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Also, the management of state in a serverless application is not really an easy
    problem to solve (usually, as in microservices, the state is simply offloaded
    to external services). Moreover, while the platforms providing serverless and
    FaaS capabilities are evolving day after day, it is usually harder to troubleshoot
    problems and debug faulty behaviors.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, there are no real standards (yet) in this particular niche,
    hence the risk of lock-in is high (meaning that implementations made to run on
    a specific cloud stack will be lost if we want to change the underlying technology).
    Considering all the pros and cons of serverless and FaaS, these approaches are
    rarely used for implementing a full and complex application. They are, instead,
    a good fit for some specific use cases, including batch computations (such as
    file format conversions and more) or for providing *glue* code connecting different,
    more complex functions (such as the ones implemented in other microservices).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to discuss a very hot topic on a strategy
    for evolving existing applications toward cloud-native microservices and other
    newer approaches such as serverless.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring apps as microservices and serverless
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed a couple of sections earlier, software projects are commonly
    categorized as either green- or brown-field.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Green-field projects are those that start from scratch and have very few constraints
    on the architectural model that could be implemented.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: This scenario is common in start-up environments, for example, where a brand-new
    product is built and there is no legacy to deal with.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The situation is, of course, ideal for an architect, but is not so common in
    all honesty (or at least, it hasn't been so common in my experience so far).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: The alternative scenario, brown-field projects, is where the project we are
    implementing involves dealing with a lot of legacy and constraints. Here, the
    target architecture cannot be designed from scratch, and a lot of choices need
    to be made, such as what we want to keep, what we want to ditch, and what we want
    to adapt. That's what we are going to discuss in this section.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The five Rs of application modernization
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown-field projects are basically application modernization projects. The existing
    landscape is analyzed, and then some decisions are made to either develop a new
    application, implement a few new features, or simply enhance what's currently
    implemented, making it more performant, cheaper, and easier to operate.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The analysis of what's existing is often an almost manual process. There are
    some tools available for scanning the existing source code or artifacts, or even
    to dynamically understand how applications behave in production. But often, most
    of the analysis is done by architects, starting from the data collected with the
    aforementioned tools, using existing architectural diagrams, interviewing teams,
    and so on.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Then, once we have a clear idea about what is running and how it is implemented,
    choices have to be made component by component.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: There is a commonly used methodology for this that defines five possible outcomes
    (the *five Rs*). It was originally defined by Gartner, but most consultancy practices
    and cloud providers (such as Microsoft and AWS) provide similar techniques, with
    very minor differences.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'The five Rs define what to do with each architectural component. Once you have
    a clear idea about how a brown-field component is implemented and what it does,
    you can apply one of the following strategies:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '**Rehost**: This means simply redeploying the application on more modern infrastructure,
    which could be different hardware, a virtualized environment, or using a cloud
    provider (in an IaaS configuration). In this scenario, no changes are made to
    the application architecture or code. Minor changes to packaging and configuration
    may be necessary but are kept to a minimum. This scenario is also described as
    lift-and-shift migration and is a way to get advantages quickly (such as cheaper
    infrastructure) while reducing risks and transformation costs. However, of course,
    the advantages provided are minimal, as the code will still be old and not very
    adherent to modern architectural practices.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Refactor**: This is very similar to the previous approach. There are no changes
    to architecture or software code. The target infrastructure, however, is supposed
    to be a PaaS environment, possibly provided by a cloud provider. In this way,
    advantages such as autoscaling or self-healing can be provided by the platform
    itself while requiring only limited effort for adoption. CI/CD and release automation
    are commonly adopted here. However, once again, the code will still be unchanged
    from the original, so it may be hard to maintain and evolve.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Revise**: This is a slightly different approach. The application will be
    ported to a more modern infrastructure (PaaS or cloud), as with the Rehost and
    Refactor strategies. However, small changes to the code will be implemented. While
    the majority of the code will stay the same, crucial features, such as session
    handling, persistence, and interfaces, will be changed or extended to derive some
    benefits from the more modern underlying infrastructure available. The final product
    will not benefit from everything the new infrastructure has to offer but will
    have some benefits. Plus, the development and testing efforts will be limited.
    The target architecture, however, will not be microservices or cloud-native, rather
    just a slightly enhanced monolith (or *n* tier).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rebuild**: Here, the development and testing effort is way higher. Basically,
    the application is not ported but instead is rewritten from scratch in order to
    use new frameworks and a new architecture (likely microservices or microservices
    plus something additional). The rebuilt architecture is targeted for hosting on
    a cloud or PaaS. Very limited parts of the application may be reused, such as
    pieces of code (business logic) or data (existing databases), but more generally,
    it can be seen as a complete green-field refactoring, in which the same requirements
    are rebuilt from scratch. Of course, the effort, cost, and risk tend to be high,
    but the benefits (if the project succeeds) are considered worthwhile.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replace**: In this, the existing application is completely discarded. It
    may be simply retired because it''s not needed anymore (note that in some methodologies,
    *Retire* is a *sixth R*, with its own dedicated strategy). Or it may be replaced
    with a different solution, such as SaaS or an existing off-the-shelf product.
    Here, the implementation cost (and the general impact) may be high, but the running
    cost is considered to be lower (or zero, if the application is completely retired),
    as less maintenance will be required. The new software is intended to perform
    better and offer enhanced features.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following table, we can see a summary of the characteristics of each
    of the approaches:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 9.1 – The characteristics of the five Rs'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_9.1_B16354.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 – The characteristics of the five Rs
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding table, getting the most benefits means a trade-off
    of taking on the most effort and risk.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, some considerations of the benefits of each of these
    approaches can be seen:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 9.2 – The benefits of the five Rs'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_9.2_B16354.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.2 – The benefits of the five Rs
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Once again, most of the benefits come with the last two or three approaches.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it''s worth noticing that the last two (**Rebuild** and **Replace**)
    fit into a much bigger discussion, often considered in the world of software development:
    that of *build versus buy*. Indeed, **Rebuild** is related to the *build* approach:
    you design the architecture and develop the software tailored to your own needs.
    It may be harder to manage this, but it guarantees maximum control. Most of this
    book, after all, is related to this approach.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '*Buy* (which is related to **Replace**), on the other hand, follows another
    logic: after a software selection phase, you find an off-the-shelf product (be
    it on-premises or SaaS) and use it instead of your old application. In general,
    it''s easier, as it requires limited to no customization. Very often, maintenance
    will also be very limited, as you will have a partner or software provider taking
    care of it. Conversely, the new software will give you less control and some of
    your requirements and use cases may need to be adapted to it.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: As said, an alternative to *buy* in the **Replace** strategy is simply to ditch
    the software altogether. This may be because of changing requirements, or simply
    because the features are provided elsewhere.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: The five Rs approach is to be considered in a wider picture of application modernization
    and is often targeted at big chunks of an enterprise architecture, targeting tens
    or hundreds of applications.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: I would like to relate this approach to something more targeted to a single
    application, which is the **strangler pattern**. The two approaches (five Rs and
    strangler) are orthogonal and can also be used together, by using the strangler
    pattern as part of revising (**Revise**) or rebuilding (**Rebuild**) an application.
    Let's look into this in more detail.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The strangler pattern
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As outlined in the previous section, the five Rs model is a programmatic approach
    to identify what to do with each application in an enterprise portfolio, with
    changes ranging from slightly adapting the existing application to a complete
    refactoring or replacement.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The strangler pattern tackles the same issue but from another perspective. Once
    an application to be modernized has been identified, it gives specific strategies
    to do so, targeting a path ranging from small improvements to a progressive coexistence
    between old and new, up to the complete replacement of the old technologies.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: This approach was originally mentioned by Martin Fowler in a famous paper and
    relates to the *strangler fig*, which is a type of tree that progressively encloses
    (and sometimes completely replaces) an existing tree.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'The metaphor here is easy to understand: new application architectures (such
    as microservices) start growing alongside existing ones, progressively strangling,
    and ultimately replacing, them. In order to this, it''s essential to have control
    of the ingress points of our users into our application (old and new) and use
    them as a *routing layer*. Better yet, there should be an ingress point capable
    of controlling each specific feature. This is easy if every feature is accessed
    via an API call (SOAP or REST), as the routing layer can then simply be a network
    appliance with routing capabilities (a load balancer) that decides where to direct
    each call and each user. If you are lucky enough, the existing API calls are already
    mediated by an API manager, which can be used for the same purposes.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: In most real applications, however, this can be hard to find, and most likely
    some of the calls are internal to the platform (so it is not easy to position
    a network load balancer in the middle). It can also happen that such calls are
    done directly in the code (via method calls) or using protocols that are not easily
    redirected over the network (such as Java RMI).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, a small intervention will be needed by writing a piece of code
    that adapts such calls from the existing infrastructure to standard over-the-network
    APIs (such as REST or SOAP), on both the client and server sides.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: An alternative technique is to implement the routing functionality in the client
    layers. A common way to do so is to use feature flags, which have hidden settings
    and are changeable on the fly by the application administrators who set the feature
    that must be called by every piece of the UI or the client application.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: However, while this approach can be more fine-grained than redirecting at the
    network level, it may end up being more complex and invasive as it also changes
    the frontend or client side of the application.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a mechanism to split and redirect each call, the strangler pattern
    can finally start to happen. The first step is to identify the first feature –
    one as isolated and self-contained as possible – to be reimplemented with a new
    stack and architecture.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: The best option is to start with simple but not trivial functionality, in order
    to keep the difficulty low but still allow you to test the new tools and framework
    on something meaningful. In order to exactly identify the boundary of each feature,
    we can refer to the concept of bounded context in DDD, as we saw in [*Chapter
    4*](B16354_04_Final_JM_ePUB.xhtml#_idTextAnchor089), *Best Practices for Design
    and Development*.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: In order to finance the project, it's a common practice to piggyback the modernization
    activities together with the implementation of new features, so it is possible
    that the new piece we are developing is not completely *isofunctional* with the
    old one, but contains some additional new functionalities.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Once such a piece of software is ready, we start testing it by sending some
    traffic toward it. To do so, we can use whatever routing layer is available, be
    it a network load balancer or a piece of custom code, as we have seen before.
    For such a goal, advanced routing techniques, such as canary or A/B testing, can
    be used (more on this in [*Chapter 13*](B16354_13_Final_JM_ePUB.xhtml#_idTextAnchor307),
    *Exploring the Software Life Cycle*).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: If something goes wrong, a rollback will always be possible, as the old functionalities
    will still be present in the existing implementation. If the rollout is successful
    and the new part of the application works properly, it's time to extend and iterate
    the application.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: More features and pieces of the application are implemented in the new way,
    and deprecated from the old implementation in an approach that can be parallelized
    but needs to be strictly governed, in order to easily understand and document
    which functionality is implemented where and potentially switch back in case of
    any issue.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, all the features of the platform will now be implemented in the
    new stack, which will most likely be based on microservices or something similar.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: After a grace period, the old implementation can be discarded and our modernization
    project will finally be completed, delivering the maximum benefit it has to offer
    (even more so as we no longer need to keep running and maintaining the old part).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – The strangler pattern approach'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.7_B16354.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – The strangler pattern approach
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram that you see is simplified for the sake of space. There
    will be more than one phase between the start (where all the features are running
    in the legacy implementation) and the end (where all the features have been reimplemented
    as microservices, or in any other modern way).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In each intermediate phase (not fully represented in the diagram, but indicated
    by the *dotted* lines), the legacy implementation starts receiving less traffic
    (as less of its features are used), while the new implementation grows in the
    number of functionalities implemented. Moreover, the new implementation is represented
    as a whole block, but it will most likely be made up of many smaller implementations
    (microservices), growing around and progressively strangling and replacing the
    older application.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Note that the strangler pattern as explained here is a simplification and doesn't
    take into account the many complexities of modernizing an application. Let's see
    some of these complexities in the next section.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Important points for application modernization
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whether the modernization is done with the strangler pattern (as seen just
    now) or a more end-to-end approach covering the whole portfolio (as with the five
    Rs approach, seen earlier), the approach to modernize an existing application
    must take care of many, often underestimated, complexities. The following gives
    some suggestions for dealing with each of them:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing suit**: This is maybe the most important of them all. While we will
    see more about testing in [*Chapter 13*](B16354_13_Final_JM_ePUB.xhtml#_idTextAnchor307),
    *Exploring the Software Life Cycle*, it''s easy to understand how a complete testing
    suite offers the proof needed to ensure that our modernization project is going
    well and ultimately is complete. In order to ensure that the new implementation
    is working at least as well as the old one, it''s crucial that everything is covered
    by test suites, possibly automated. If you lack test coverage on the existing
    implementation, you may have a general feeling that everything is working, but
    you will likely have some bad surprises on production release. So, if the test
    coverage on the whole platform is low, it''s better to invest in this first before
    any refactoring project.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: While it wasn''t underlined in the techniques we have
    seen, refactoring often impacts the existing data layer by adding new data technologies
    (such as NoSQL stores) and/or changing the data structure of existing data setups
    (such as altering database schemas). Hence, it is very important to have a strategy
    around data too. It is likely that, if we migrated one bounded context at a time,
    the new implementation would have a dedicated and self-consistent data store.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, to do so, we will need to have existing data migrated (by using a data
    integration technique, as we saw in [*Chapter 8*](B16354_08_Final_JM_ePUB.xhtml#_idTextAnchor200),
    *Designing Application Integration and Business Automation*). When the new release
    is ready, it will likely have to exchange data with the older applications. To
    do so, you can provide an API, completely moving the integration from the data
    layer to the application layer (this is the best approach), or move the data itself
    using, as an example, the *change data capture pattern*. As discussed earlier,
    however, you must be careful of any unwanted data inconsistency in the platform
    as a whole.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '**Session handling**: This is another very important point, as for a certain
    amount of time, the implementation will remain on both the old and new applications
    and users will share their sessions between both. This includes all the required
    session data and security information as well (such as if the user is logged in).
    To handle such sessions, the best approach is to externalize the session handling
    (such as into an external in-memory database) and make both the old and new applications
    refer to it when it comes to storing and retrieving session information. An alternative
    is to keep two separate session-handling systems up to date (manually), but as
    you can imagine, it''s more cumbersome to implement and maintain them.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Troubleshooting**: This has a big impact. For a certain amount of time, the
    features are implemented using many different systems, across old and new technologies.
    So, in case of any issue, it will be definitively harder to understand what has
    gone wrong where. There is not much we could do to mitigate the impact of an issue
    here. My suggestion is to maintain up-to-date documentation and governance of
    the project, in order to make clear to everybody where each feature is implemented
    at any point in time. A further step is to provide a unique identifier to each
    call, to understand the path of each call, and correlate the execution on every
    subsystem that has been affected. Last but not least, you should invest in technical
    training for all staff members to help them master the newly implemented system,
    which brings us to the next point.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: Other than for the technical staff, to help them support and
    develop the new technologies of choice, training may be useful for everybody involved
    in the project, sometimes including the end users. Indeed, while the goal is to
    keep the system *isofunctional* and improve the existing one, it is still likely
    that there will be some impact on the end users. It may be that the UI is changed
    and modernized, the APIs will evolve somehow, or we move from a fat client for
    desktop to a web and mobile application. Don''t forget that most of these suggestions
    are also applicable to the five Rs methodology, so you may end up completely replacing
    one piece of the application with something different (such as an off-the-shelf
    product), which leads us to the final point.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling endpoints**: As in the previous point, it would be great if we could
    keep the API as similar as possible to minimize the impact on the final customers
    and the external systems. However, this is rarely possible. In most real-world
    projects, the API signature will slightly (or heavily) change, along with the
    UIs. Hence, it''s important to have a communication plan to inform everybody involved
    about the rollout schedule of the new project, taking into account that this may
    mean changing something such as remote clients; hence, the end users and external
    systems must be ready to implement such changes, which may be impactful and expensive.
    To mitigate the impact, you could also consider keeping the older version available
    for a short period.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you have seen, modernizing an application with a microservice or cloud-native
    architecture is definitely not easy, and many options are available.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: However, in my personal experience, it may be really worth it due to the return
    on investment and the reduction of legacy code and technical debt, ultimately
    creating a target architecture that is easier and cheaper to operate and provides
    a better service to our end users. This section completes our chapter on cloud-native
    architectures.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen the main concepts pertaining to cloud-native architectures.
    Starting with the goals and benefits, we have seen the concept of PaaS and Kubernetes,
    which is currently a widely used engine for PaaS solutions. An interesting excursus
    involved the twelve-factor applications, and we also discussed how some of those
    concepts more or less map to Kubernetes concepts.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to the well-known issues in cloud-native applications, including
    fault tolerance, transactionality, and orchestration. Lastly, we touched on the
    further evolution of microservices architectures, that is, miniservices and serverless.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: With these concepts in mind, you should be able to understand the advantages
    of a cloud-native application and apply the basic concepts in order to design
    and implement cloud-native architectures.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Then, we moved on to look at a couple of methodologies for application modernization,
    and when and why these kinds of projects are worth undertaking.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start discussing user interactions. This means
    exploring the standard technologies for web frontends in Java (such as Jakarta
    Server Pages and Jakarta Server Faces) and newer approaches, such as client-side
    web applications (using the React framework in our case).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The JBoss community, *Undertow* ([https://undertow.io/](https://undertow.io/))
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martin Fowler, *Patterns of Enterprise Application Architecture* ([https://martinfowler.com](https://martinfowler.com))
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netflix OSS, *Hystrix* ([https://github.com/Netflix/Hystrix](https://github.com/Netflix/Hystrix))
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eclipse MicroProfile, *Microprofile Fault Tolerance* ([https://github.com/eclipse/microprofile-fault-tolerance](https://github.com/eclipse/microprofile-fault-tolerance))
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chris Richardson, *Microservices Architecture* ([https://microservices.io](https://microservices.io))
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Richard Watson, *Five Ways to Migrate Applications to the Cloud* ([https://www.pressebox.com/pressrelease/gartner-uk-ltd/Gartner-Identifies-Five-Ways-to-Migrate-Applications-to-the-Cloud/boxid/424552](https://www.pressebox.com/pressrelease/gartner-uk-ltd/Gartner-Identifies-Five-Ways-to-Migrate-Applications-to-the-Cloud/boxid/424552))
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martin Fowler, *StranglerFigApplication* ([https://martinfowler.com/bliki/StranglerFigApplication.html](https://martinfowler.com/bliki/StranglerFigApplication.html))
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paul Hammant, *Strangler Applications* ([https://paulhammant.com/2013/07/14/legacy-application-strangulation-case-studies](https://paulhammant.com/2013/07/14/legacy-application-strangulation-case-studies))
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
