- en: Chapter 4. Exploring the Collection API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we return to MVT in order to take on challenges that span multiple
    MVT teams. The market data team requires improved critical path order book performance
    to handle increased cancel request volume. The data science team wants better
    ad hoc data analysis tools to research trading strategies. Everyone has a problem
    that had to be solved yesterday. That's the start-up lifestyle!
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the functional paradigm, our existing knowledge, and the Scala collections
    API to our advantage to solve these challenges. The power of the Scala language
    and its collections API allow you to approach problems in ways that you may not
    have thought possible before. As we work through these challenges and encounter
    new Scala collection usage, we detail collection implementation and tradeoffs
    to consider. We will consider the following collections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: List
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TreeMap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-throughput systems – improving the order book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1.  The Road to Performance"), *The Road to
    Performance*, you met MVT's head trader, Dave, under tense circumstances. The
    financial markets underwent a period of extreme volatility that exposed a weakness
    in the order book design. After speaking to Dave, you learned that in volatile
    markets, order volume is dominated by cancels because traders are reacting to
    quickly changing market conditions. Through order book benchmarking and profiling,
    you confirmed the suspicion that under high volume, cancel performance causes
    high order book response latency.
  prefs: []
  type: TYPE_NORMAL
- en: Although the market volatility that caused trading losses has passed, Dave recognizes
    the risk that future volatility poses for MVT's returns. Dave wants to invest
    engineering effort into making the order book more performant when cancelations
    frequently occur. By working with the data science team, Dave analyzed historical
    order book activity over a three month period and discovered interesting market
    characteristics. He shares with you that in the three months analyzed, on a per
    trading day basis, cancels comprised, on average, 70% of order book commands.
    The analysis also revealed that on the most volatile market days, cancel activity
    represents about 85% of order book activity. Known for his puns, Dave concludes
    with, "Now, you know everything I know. Like the order book, we are counting on
    you to execute!"
  prefs: []
  type: TYPE_NORMAL
- en: Understanding historical trade-offs – list implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Excited to improve order book performance, your first step is to familiarize
    yourself with the order book implementation. As you open up the order book repository,
    you ping Gary, a fellow engineer who has prior order book development experience.
    As Gary knows the history of order book development, he tells you to check out
    `ListOrderBook`. "This was our first attempt at modeling the order book. I think
    you can learn from our design by seeing its first incarnation," he adds, "Once
    you understand the implementation, check out `QueueOrderBook`. That's the next
    version of the order book. You profiled an older iteration of this implementation
    when we had the volatility wave. Let me know if you have any questions!" After
    thanking him, you dig into the repository to find `ListOrderBook`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ListOrderBook` class defines the following state to manage buys (bids)
    and sells (offers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To refresh our memory, here are definitions of `Price`, `BuyLimitOrder`, and `SellLimitOrder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `LimitOrder` is an **algebraic data type** (**ADT**) that represents the
    two possible order sides. The `Price` class is a strongly-typed wrapper for `BigDecimal`.
    Recalling the performance boost that value classes provide, you modify the definition
    of `Price`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `ListOrderBook` class uses two Scala collection types to maintain its state: `List`
    and `TreeMap`. Let's have a deeper look at these data structures to understand
    the tradeoffs that they present.
  prefs: []
  type: TYPE_NORMAL
- en: List
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scala implements `List` as an immutable singly-linked list. A `List` is an
    ordered collection of elements of the same type. A `List` is a sealed abstract
    class with two implementations: `Nil`, which represents the empty list, and `::`
    (often called cons), which is used to represent an element and a tail. To make
    things more concrete, let''s look at some pseudocode, which is close to the actual
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A `List` of three integers can be constructed using the following notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note the plus sign in the definition of the `List` trait. The plus (`+`) sign
    indicates that `List` is covariant on its type parameter, `A`. Covariance allows
    you to express polymorphic constraints with generic types. To make this more concrete,
    consider the following definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sealed trait Base`'
  prefs: []
  type: TYPE_NORMAL
- en: '`case class Impl(value: Int) extends Base`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, a relationship is expressed between `Base` and `Impl`. The `Impl` class
    is a subtype of `Base`. When used with `List`, covariance allows us to express
    that `List[Impl]` is a subtype of `List[Base]`. Expressed with an example, covariance
    is what allows the following snippet to compile:'
  prefs: []
  type: TYPE_NORMAL
- en: '`val bases: List[Base] = List[Impl](Impl(1))`'
  prefs: []
  type: TYPE_NORMAL
- en: Covariance belongs to the broader topic of variances. If you wish to learn more
    about variances in Scala, refer to this excellent blog post by Andreas Schroeder
    at [https://blog.codecentric.de/en/2015/03/scala-type-system-parameterized-types-variances-part-1/](https://blog.codecentric.de/en/2015/03/scala-type-system-parameterized-types-variances-part-1/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike most other Scala collections, `List` supports pattern matching on its
    content. This is a powerful way to write expressive code that handles multiple
    scenarios while retaining compile-time safety that all possible cases are handled.
    Consider the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this simple pattern match, we are able to express several concerns. Here,
    `1` is `1`, `x` is `2`, and `rest` is `List(3,4)`. When compiled, this snippet
    elicits a compiler warning because the Scala compiler infers that there are possible `List`
    patterns that were unmatched (for example, empty `List`). Compiler-provided warnings
    minimize the chance of your forgetting to handle a valid input.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `List` is optimized for prepend operations. Adding 0 to the previous list
    is as easy as doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a constant-time operation, and it has almost no memory cost, as `List`
    implements data sharing. In other words, the new list, `listWithZero`, is not
    a deep copy of `list`. Instead, it re-uses all its allocated elements and allocates
    only one new element, the cell containing `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![List](img/image_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In contrast to prepend operations, append operations (that is, adding an element
    to the end of the list) are computationally expensive because the entire `List`
    must be copied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![List](img/image_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given the poor append performance of List, you may wonder whether it is safe
    to use a `map` transform. A `map` transform occurs by applying a function to successive
    elements in the `List`, which can be logically represented by appending transformed
    values to a new `List`. To avoid this performance pitfall, `List.map` overrides
    the default implementation provided by the trait `TraversableOnce` to apply the
    transform using prepend operations. This provides improved `List.map` performance
    while retaining the same API. Overriding default behavior to provide a specialized
    implementation is a common Scala collections pattern. Constant time head operations
    make `List` ideal for algorithms involving last-in, first-out (LIFO) operations.
    For random access and first-in, first-out (FIFO) behaviors, you should employ `List`
    selectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we investigate `TreeMap`. The `TreeMap` class is the implementation
    of the `SortedMap` trait that is used to maintain bids and offers.
  prefs: []
  type: TYPE_NORMAL
- en: TreeMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `TreeMap` class is a map that orders keys according to a provided ordering
    strategy. The following snippet of its class definition makes the ordering requirement
    clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `Ordering` class is a type class that defines a contract for the natural
    ordering of elements of the `A` type.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If type classes are a concept that is new to you, we encourage you to read Daniel
    Westheide's well-written blog post on the topic at [http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html](http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In `ListOrderBook`, we see that `Price` is the key. Looking at the companion
    object of `Price`, we see that the ordering is defined by delegating to the underlying
    `BigDecimal` type''s ordering definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `TreeMap `class referenced by `ListOrderBook`, like `List`, is immutable.
    Immutability provides strong reasoning guarantees. We can be certain that there
    are no side effects because the effect of adding or removing a value from the
    map is always reflected as a new map.
  prefs: []
  type: TYPE_NORMAL
- en: The `TreeMap` class implementation is a special type of binary search tree,
    the red-black tree. This tree implementation provides logarithmic operation time
    for lookups, additions, and removals. You might be surprised to see `TreeMap`
    in place of `HashMap`. As documented in the Scala collections performance overview
    ([http://docs.scala-lang.org/overviews/collections/performance-characteristics.html](http://docs.scala-lang.org/overviews/collections/performance-characteristics.html)), `HashMap`
    provides constant time lookups, additions, and removals, which is faster than `TreeMap`.
    However, `TreeMap` offers superior performance when performing ordered traversals.
    For example, finding the largest key in the map can be done in logarithmic time
    with `TreeMap`, while this is done in linear time for `HashMap`. This difference
    is an indicator that the order book implementation requires efficient ordered `Price`
    traversals.
  prefs: []
  type: TYPE_NORMAL
- en: Adding limit orders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coming back to the `ListOrderBook` implementation, we see the following partial
    method definition reflects the heart of the order book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It might seem curious that a function is supplied as an argument to retrieve
    the current time. A potentially simpler way to achieve the same effect is to invoke
    `System.currentTimeMillis()`. The shortcoming of this approach is that accessing
    the system clock is a side-effect, which means that the function is no longer
    referentially transparent. By providing a function to retrieve the current time,
    we are able to control how this side-effect happens and produce repeatable test
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a `Command`, an order book instance, and a way to obtain the current
    time for event timestamps, an `Event` and a new state are produced. To refresh
    our memory, here are the commands the order book can process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the possible events created by processing commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s focus on supporting the `AddLimitOrder` command to better understand
    the algorithmic properties of historical design choices. When adding a limit order,
    one of two outcomes is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: The incoming order price crosses the book resulting in `OrderExecuted`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The oncoming order rests on the book resulting in `LimitOrderAdded`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deducing whether or not the order crosses the book requires looking at the
    best price on the opposing side. Returning to the definition of `LimitOrderBook`
    with complete implementation of `bestBid` and `bestOffer`, we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The implementation shows that we are taking advantage of the logarithmic ordered
    search property of `TreeMap`. The best bid is the key with the highest price,
    which is the last value in the tree because the ordering is ascending. The best
    offer is the key with the lowest price, which is the first value in the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Focusing specifically on the addition of a buy limit order and given the best
    offer, the following comparison occurs to determine whether the incoming buy order
    crosses the book or rests on the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first assume that the incoming buy order''s price is lower than the
    best offer, which means the order is added to the book (that is, rests on the
    book). The question we are trying to answer is, "where in the book should the
    order be added?" The order book performs a logarithmic search to find the price
    level associated with the order price. From the definition of `ListOrderBook`,
    you know that each value in the map (the price level) is represented as a `List`
    of orders. Recalling a discussion with the head trader, Dave, you remember that
    orders within a price level are executed based on time priority. The first order
    added to a price level is the first order to be executed. Conceptually, a price
    level is a first-in, first-out (FIFO) queue. The implication is that adding an
    order to a price level is a linear time operation because the order is appended
    to the end. The following snippet confirms your hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The snippet shows that adding a resting limit order to the book involves a linear
    time append operation to `List` of `BuyLimitOrder`. In your mind, you are beginning
    to wonder how MVT was able to trade profitably at all with this order book. Before
    leaping to this harsh judgment, you consider how crossing the book is handled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that the incoming buy order''s price is greater than or equal to the
    best offer price, then the buy order crosses the book, causing an execution. Time
    priority dictates that the first sell order received is executed against the incoming
    buy order, which translates to taking the first sell order in the price level.
    When generating an execution, you realize that modeling a price level with a `List`
    provides constant time performance. The following snippet shows how a price level
    is modified on a buy execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ListOrderBook` takes advantage of the `List` pattern matching to handle
    the two possible cross scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The executed sell order is the only order available in the price level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional sell orders remain at the price level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the former scenario, the price level is removed from the book by removing
    the key from the offers `TreeMap`. In the latter scenario, the remaining orders
    form the new price level. Clearly, the order book is optimized for executions
    over adding resting orders. You wonder why this bias exists in the order book
    implementation. You wonder to yourself, "perhaps, executions are more much more
    prevalent than resting orders?" You are unsure and make a mental note to chat
    with Dave.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pause for a moment to consider biases in systems that you have designed. Did
    you optimize operations proportional to usage or latency constraints? Looking
    back, did your design choices lead you towards the best possible performance for
    the most important operations? Of course, hindsight makes it easy to call out
    suboptimal design choices. By reflecting on how you made these choices, you might
    be better able to avoid similar deficiencies in future systems.
  prefs: []
  type: TYPE_NORMAL
- en: Canceling orders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ListOrderBook` also supports the `CancelOrder` command to remove an existing
    order by ID. Cancel requests pose an algorithmic challenge to `ListOrderBook`.
    As only the order ID is provided, `ListOrderBook` cannot efficiently determine
    which side the order rests on (that is, buy or sell). To determine the side, the
    buy and sell price levels are swept to find the order ID. This is an operation
    that is proportional to the number of price levels per side and the length of
    each price level. The worst case scenario is submitting an order ID that does
    not exist in the order book. The entire book must be swept to identify the absence
    of the provided order ID. A malicious trader could slow down MVT order book operations
    by submitting a constant stream of nonexistent order IDs. You make a note to talk
    with Dave about malicious trading activities and what MVT can do to defend against
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that the order referenced by the cancel request exists in the book
    and its price level is discovered, the act of removing the cancelled order from
    the book is also expensive. Canceling is a linear time operation that requires
    traversing the linked list of orders and removing the node with the matching order
    ID. The following snippet implements canceling a sell order in `ListOrderBook`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Studying this snippet, it is unsurprising to you that cancelation performance
    is the least performant order book operation. There are two linear time passes
    performed per price level to cancel the order. First, `exists` traverses the list
    of price level orders to determine whether the ID to be canceled exists in the
    price level. Once the price level containing the ID is found, there is a second
    traversal via `filter` to update the state of the order book.
  prefs: []
  type: TYPE_NORMAL
- en: The cancelation implementation in `ListOrderBook` is an illustration of the
    double-edged sword of Scala's expressive collection API. By virtue of being expressive,
    the cancelation logic is simple to understand and to maintain. However, its expressiveness
    also makes it easy to hide that the runtime performance of removing an order from
    a price level is *2 * N*, where *N* is the number of orders in a price level.
    This simple example makes it clear that in a performance-sensitive environment,
    it is important to take a step back from the code to consider the runtime overhead
    of the data structure that is being used.
  prefs: []
  type: TYPE_NORMAL
- en: The current order book – queue implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You refrain from judging `ListOrderBook` too harshly because you know from
    your prior software development experiences that there were likely extenuating
    circumstances that led to this implementation. You turn your attention to the
    current order book implementation, which is in `QueueOrderBook`. Looking over
    the source code, you are surprised to discover the implementation appears to match `ListOrderBook`
    except for the price level data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between the two implementations is the use of `scala.collection.immutable.Queue`
    in place of `List` to represent a price level. From a modeling perspective, using
    a FIFO queue makes sense. As time priority dictates execution order, a FIFO queue
    is a natural fit to store resting orders. You begin wondering whether switching
    out `List` for `Queue` was done purely for modeling purposes. The question on
    your mind is, "how does replacing `List` with `Queue` improve order book performance?"
    Understanding this change requires digging deeper into Scala's `Queue` implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This snippet of a `Queue` definition reveals an interesting insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Without reading deeply into the `Queue` implementation, we see that it uses
    two `Lists` to manage state. Given the usage of `List` to model a FIFO queue in `ListOrderBook`,
    it should not be surprising to see the usage of `List` to build an immutable FIFO
    queue data structure. Let''s look at the enqueue and dequeue operations to understand
    how in and out impact `Queue` performance. The following snippet shows the implementation
    of enqueue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As the element is prepended to `in`, enqueueing is a constant time operation.
    Recall that the analogous `ListOrderBook` operation is adding a resting order,
    which has linear runtime performance. This is a clear performance win for `QueueOrderBook`.
    Next, we consider dequeue implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the implementation shows, dequeue throws an exception when invoked with an
    empty `Queue`. The exception is an unexpected outcome to invoking `dequeue` and
    feels out of place in the functional programming paradigm. For this reason, `Queue`
    also provides `dequeueOption` that returns an `Option`. This makes the handling
    of an empty `Queue` explicit and easier to reason about. We recommend using `dequeueOption`
    in any situation where you cannot guarantee that `dequeue` will always be called
    on a nonempty `Queue`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dequeue` operation is more involved than `enqueue` due to the interaction
    between `in` and `out`. To understand how the `Queue` state is managed with the `dequeue`
    operations, review the following table. This table walks through a series of the `enqueue`
    and `dequeue` operations, listing the state of `in` and `out` at each step. As
    you review the table, consider which  `dequeue` patterns match statements that
    are invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation** | **In** | **Out** |'
  prefs: []
  type: TYPE_TB
- en: '| enqueue(1) | List(1) | Nil |'
  prefs: []
  type: TYPE_TB
- en: '| enqueue(2) | List(1, 2) | Nil |'
  prefs: []
  type: TYPE_TB
- en: '| enqueue(3) | List(1, 2, 3) | Nil |'
  prefs: []
  type: TYPE_TB
- en: '| dequeue | Nil | List(2, 3) |'
  prefs: []
  type: TYPE_TB
- en: '| dequeue | Nil | List(3) |'
  prefs: []
  type: TYPE_TB
- en: '| enqueue(4) | List(4) | List(3) |'
  prefs: []
  type: TYPE_TB
- en: '| dequeue | List(4) | Nil |'
  prefs: []
  type: TYPE_TB
- en: '| dequeue | Nil | Nil |'
  prefs: []
  type: TYPE_TB
- en: As the `enqueue` and `dequeue` invocations are intermingled, both `in` and `out`
    retain state. In the final sequence displayed, the queue returns to its initial
    state (that is, both `in` and `out` empty). The key insight from this implementation
    is that `Queue` amortizes the cost of `dequeue` to be constant time by deferring
    transfers from `in` and `out`. Each element transfer from `in` and `out` is a
    linear time `reverse` operation to maintain first-in, first-out ordering. Deferring
    the cost of this expensive operation until `out` is empty is a form of lazy evaluation.
    This is an illustrative example of how lazy evaluation can be used to improve
    runtime performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have an understanding of how `Queue` is implemented, you can reason
    about the performance improvements delivered by `QueueOrderBook`. The following
    table itemizes the runtime performance of each scenario to modify a price level:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scenario** | **ListOrderBook** | **QueueOrderBook** |'
  prefs: []
  type: TYPE_TB
- en: '| Add resting limit order | Linear | Constant |'
  prefs: []
  type: TYPE_TB
- en: '| Generate execution | Constant | Amortized constant |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel order | Linear | Linear |'
  prefs: []
  type: TYPE_TB
- en: This table illustrates how understanding the runtime characteristics of the
    Scala collection API can result in tangible performance wins with small changes
    to your implementation. Recall that when `QueueOrderBook` was introduced, it was
    noted that its implementation is identical to `ListOrderBook`, the module changes
    to replace `List` operations with analogous `Queue` operations. This is a comparatively
    simple change for the performance boost shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: You are excited to see the performance win to handle limit orders with `QueueOrderBook`,
    but you are left wondering about what can be done about cancelation performance.
    It remains unsettling to you that `QueueOrderBook` retains the same cancelation
    performance. In particular, because of the recent market volatility that exposed
    order book cancelation performance's weakness that caused MVT to trade unprofitably.
    Lazy evaluation was a big performance win to handle limit orders. Can this principle
    also be applied to cancel requests?
  prefs: []
  type: TYPE_NORMAL
- en: Improved cancellation performance through lazy evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Queue provides high-performance `enqueue` and `dequeue` operations using the
    additional state, the second `List`, to defer and to batch expensive operations.
    This principle can be applied to the order book. When canceling an order, there
    are two expensive operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the price level containing the order-to-be-canceled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traversing a `Queue` or `List` to remove the canceled order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Focusing on the second operation, the motivating question is, "how can the
    order book defer the cost of linear traversal to modify internal state?" To answer
    this question, it is often helpful to consider the strengths of your implementation.
    With either order book implementation, we know there is excellent execution performance.
    One strategy that takes advantage of this insight is to defer cancellation until
    order execution occurs. The approach is to use additional state to maintain the
    intent to cancel without removing the order from order book state until it is
    performant to do so. This approach could look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `LazyCancelOrderBook` class adds additional state in the form of a `scala.collection.immutable.Set`
    to manage the IDs of canceled requests that have not been reflected into the the
    state of `bids` and `offers`. Before diving into how `pendingCancelIds` is used,
    let's investigate the Scala implementation of `Set`.
  prefs: []
  type: TYPE_NORMAL
- en: Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scala''s implementation of `Set` is neither an ADT, such as `List`, nor a concrete
    implementation, such as `TreeMap`. Instead, it is a trait, as shown in this snippet
    of its definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason the standard library defines it is as a trait is to support specific
    implementations depending upon the element count. The `Set` companion object defines
    five implementations for sizes zero to four. Each implementation contains a fixed
    number of elements, as shown in `Set3`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When the number of elements is small, the runtime performance is faster with
    hand-rolled `Set` implementations. With this technique, additions and removals
    point to the next or previous hand-rolled implementation. For example, consider `+`
    and `-` from `Set3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After `Set4`, the standard library uses an implementation named `HashSet`.
    This is visible when adding an element to `Set4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `HashSet` is analogous to `TreeMap` because it is backed by an efficient
    data structure to manage internal state. For `HashSet`, the backing data structure
    is a hash trie. The hash trie provides amortized constant time performance for
    additions, removals, and contains operations as per the Scala collections performance
    overview ([http://docs.scala-lang.org/overviews/collections/performance-characteristics.html](http://docs.scala-lang.org/overviews/collections/performance-characteristics.html)).
    If you want to dig deeper into how a hash trie works, the Scala hash trie overview
    ([http://docs.scala-lang.org/overviews/collections/concrete-immutable-collection-classes.html#hash-tries](http://docs.scala-lang.org/overviews/collections/concrete-immutable-collection-classes.html#hash-tries))
    is a good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the `LazyCancelOrderBook`, we now know that common set operations
    with `pendingCancelIds` are completed in amortized constant time. Provided that
    we focus on additions and removals, and contains operations, this suggests there
    will be minimal overhead as the size of the set increases. We can use `pendingCancelIds`
    to represent the intent to remove an order from the order book without paying
    the cost of performing the removal. This simplifies the handling of a cancel order
    to be a constant time addition to `pendingCancelIds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of `handleCancelOrder` becomes trivial because the work
    to remove the order from the book is deferred. While this is a performance win,
    this implementation suffers from a serious deficiency. This implementation is
    no longer able to identify order IDs that are absent from the order book, which
    result in `OrderCancelRejected`. One way to account for this requirement is to
    maintain an additional `Set` containing order IDs actively resting on the book.
    Now, the `LazyCancelOrderBook` state looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'With this definition, we can rewrite `handleCancelOrder` to account for nonexistent
    order IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This implementation involves three amortized, constant time operations when
    the order ID exists in the book. First, there is an operation to identify whether
    or not the order ID exists in the order book. Then, the provided order ID is removed
    from the active ID set and added to the pending cancel set. Previously, this scenario
    required two linear runtime operations. The degenerate scenario of handling a
    nonexistent order ID now shrinks to a single amortized constant time operation.
  prefs: []
  type: TYPE_NORMAL
- en: Before celebrating performance wins, bear in mind that we still need to remove
    canceled orders from the book. To reduce the cost of cancelations, two potentially
    large sets were added to the order book, which increases the size of the memory
    footprint and garbage collection pressure. Additionally, benchmarking is needed
    to prove that theoretical performance improvements translate to real-world performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete `LazyCancelOrderBook` implementation, we need to account for `activeIds`
    when handling a limit order and `pendingCancelIds` when generating an execution.
    As you may recall, handling a limit order involved two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a resting limit order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crossing the book to generate an execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a partially implemented snippet that prepares us to handle these two
    scenarios for a `BuyLimitOrder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To support resting buy orders, the provided buy order must be enqueued and
    additionally, the buy order ID must be added to the `activeOrderIds` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic to add a resting limit order is shown in the preceding code and extracted
    into a method named `restLimitOrder`. This logic resembles the analogous scenario
    for `ListOrderBook` with the added amortized constant time active order ID addition
    operation. This change is straightforward and adds little processing time overhead.
    Finally, we consider the more complicated order crossing scenario. This scenario
    is analogous to `Queue.dequeue` in that this implementation pays the cost of the
    deferred action. The first dilemma to solve is identifying which order can be
    executed and which orders must be removed because they are canceled. `findActiveOrder`
    supplies this functionality and is shown with the assumption that `orderBook`
    is lexically in scope, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`findActiveOrder` recursively inspects a sell price level until an executable
    order is found or the price level is empty. In addition to optionally resolving
    a sell order that can be executed, the method returns the remaining price level.
    These order IDs have been canceled and must be removed from `pendingCancelIds`.
    Here, we see the bulk of the canceled work deferred when the cancel request was
    handled. Execution is now amortized to be a constant time operation when executions
    occur repeatedly without a cancelation in-between. The worst case scenario is
    a linear runtime that is proportional to the number of canceled orders in the
    price level. Let''s look at how `findActiveOrder` is used to update the state
    of the order book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Order crossing implementation is now arguably more complicated than in `ListOrderBook`
    or `QueueOrderBook` due to the work to remove canceled orders and to remove the
    removed order IDs from `pendingCancelIds`. In all three pattern match statements,
    the set of returned order IDs returned as the final tuple member is removed from
    `pendingCancelIds` to indicate that the order is now removed from the book. The
    first two pattern match statements handle the distinction between finding an active
    order with one or more remaining orders in the price level and finding an active
    order with zero remaining orders in the price level. In the latter scenario, the
    price level is removed from the book. The third pattern match statement accounts
    for the scenario where an active order is not found. If an active order is not
    found because all orders were pending cancelation, then, by definition, the entire
    price level was searched, and it is, therefore, now empty.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking LazyCancelOrderBook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a rigorous performance engineer, you realize that although your code compiles
    and your tests pass, your work is not yet complete. You begin pondering how to
    benchmark `LazyCancelOrderBook` to determine whether or not your changes have
    improved real-world performance. Your first idea is to test cancelation in isolation
    to confirm that this operation has indeed been optimized. To do this, you rework `CancelBenchmarks`,
    which was introduced in [Chapter 2](ch02.html "Chapter 2.  Measuring Performance
    on the JVM"), *Measuring Performance on the JVM*, to work with `QueueOrderBook`
    and `LazyCancelOrderBook`. This benchmark sweeps different price level sizes canceling
    the first order, the last order, and a nonexistent order. We omit the source code
    because it is identical to the previous implementation and instead consider the
    results. These results were produced by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The benchmark provides us with the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Enqueued order count** | **Throughput (ops per second)**
    | **Error as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerCancelFirstOrderInLine` | 1 | 6,912,696.09 | ± 0.44 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyCancelFirstOrderInLine` | 1 | 25,676,031.5 | ± 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerCancelFirstOrderInLine` | 10 | 2,332,046.09 | ± 0.96 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyCancelFirstOrderInLine` | 10 | 12,656,750.43 | ± 0.31 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerCancelFirstOrderInLine` | 1 | 5,641,784.63 | ± 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyCancelFirstOrderInLine` | 1 | 25,619,665.34 | ± 0.48 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerCancelFirstOrderInLine` | 10 | 1,788,885.62 | ± 0.39 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyCancelFirstOrderInLine` | 10 | 13,269,215.32 | ± 0.30 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerCancelFirstOrderInLine` | 1 | 9,351,630.96 | ± 0.19 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyCancelFirstOrderInLine` | 1 | 31,742,147.67 | ± 0.65 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerCancelFirstOrderInLine` | 10 | 6,897,164.11 | ± 0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyCancelFirstOrderInLine` | 10 | 24,102,925.78 | ± 0.24 |'
  prefs: []
  type: TYPE_TB
- en: This test demonstrates that `LazyCancelOrderBook` consistently outperforms `QueueOrderBook`
    when canceling the first order, the last order, and a nonexistent order across
    order queue sizes of one and ten. This is exactly as expected because `LazyCancelOrderBook`
    defers the most expensive work until an order is executed. We see constant performance
    independent of the position of the order-to-be-canceled, which is further proof
    that the removal work is deferred. Also as expected, we see that canceling a nonexistent
    order results in improved performance because a linear traversal is no longer
    required to ascertain the absence of an order. However, we notice the performance
    hit as the enqueued order count increases from one to ten for `LazyCancelOrderBook`.
    We can hypothesize that the nearly 50% throughput reduction is due to the overhead
    of managing the state of active and pending cancel order IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This result is a promising sign that your changes are indeed improving the
    real-world performance. As the new implementation passed the initial litmus test,
    you think about how to representatively simulate a combination of executions and
    cancelations. You decide to focus on creating a microbenchmark that combines executions
    and cancelations to exercise `LazyCancelOrderBook` in scenarios that more closely
    resemble production. You think back to a recent lunch conversation you had with
    Dave about market trading flows and recall that he said it is common to see about
    two cancelations per execution. Running with this idea, you create a benchmark
    that interleaves trades and cancelations. For both order book implementations,
    you want to test performance when during the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Two trades per cancelation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One trade per cancelation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two cancelations per trade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three scenarios will help reveal shortcomings in `LazyCancelOrderBook`
    by focusing on production-like order book activities. The benchmark requires initializing
    each order book with a set of resting orders to be canceled or executed against.
    The following snippet demonstrates how to initialize the order books in a JMH
    test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Before each trial, both order books will be filled with `maxOrderCount` (defined
    to be 30) resting bids. As there are three scenarios to test and two order books,
    there are six benchmarks defined for this test. Each set of three scenarios is
    the same per order book implementation. To avoid duplication, the following snippet
    shows the three benchmarks implemented for `LazyCancelOrderBook`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'These benchmarks follow the convention of denoting the cancelation frequency
    ("C") first and the trade frequency ("T") second. For example, the final benchmark
    implements the scenario that represents one cancelation for every two trades.
    The commands are defined as values out-of-scope to avoid generating garbage during
    benchmark invocation. The benchmark invocation looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This invocation produces the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Throughput (ops per second)** | **Error as percentage of
    throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerOneToTwoCT` | 797,339.08 | ± 2.63 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyOneToTwoCT` | 1,123,157.94 | ± 1.26 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerOneToOneCT` | 854,635.26 | ± 2.48 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyOneToOneCT` | 1,469,338.46 | ± 1.85 |'
  prefs: []
  type: TYPE_TB
- en: '| `eagerTwoToOneCT` | 497,368.11 | ± 0.72 |'
  prefs: []
  type: TYPE_TB
- en: '| `lazyTwoToOneCT` | 1,208,671.60 | ± 1.69 |'
  prefs: []
  type: TYPE_TB
- en: 'Across the board, `LazyCancelOrderBook` outperforms `QueueOrderBook`. The relative
    difference between lazy and eager performance shows an interesting relationship.
    The following table captures the relative performance difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **LazyCancelOrderBook percentage performance improvement**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `OneToTwoCT` | 141.00% |'
  prefs: []
  type: TYPE_TB
- en: '| `OneToOneCT` | 172.00% |'
  prefs: []
  type: TYPE_TB
- en: '| `TwoToOneCT` | 243.00% |'
  prefs: []
  type: TYPE_TB
- en: Studying the preceding table, we observe that `LazyCancelOrderBook` shows the
    greatest performance win when there are two cancelations per trade. This result
    demonstrates the benefit of deferring the cost of processing a cancelation request.
    The next trend that we see is that as the frequency of trades increases and the
    frequency of cancelations decreases, `QueueOrderBook` performance improves relative
    to `LazyCancelOrderBook`. This result makes sense because `LazyCancelOrderBook`
    incurs extra costs when performing a trade. In addition to searching for canceled
    orders, `LazyCancelOrderBook` must update `activeIds`. The `QueueOrderBook` avoids
    these costs, but we see the overwhelming cost of cancelation processing continues
    to overshadow `QueueOrderBook` performance. Summarizing these results, we have
    more confidence that `LazyCancelOrderBook` is a stand-in replacement for `QueueOrderBook`.
    In scenarios involving heavy volumes of cancelations, it appears to be a clear
    winner, and in other scenarios, it appears to maintain parity with `QueueOrderBook`.
  prefs: []
  type: TYPE_NORMAL
- en: Lessons learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we leveraged Scala collections, in conjunction with the judicious
    use of lazy evaluation, to improve the performance of a critical component in
    MVT's infrastructure. By working through several order book implementations, you
    learned first-hand how a well-suited data structure can improve performance while
    a less optimal choice can derail performance. This exercise also exposed you to
    how Scala implements several of its collections, which you can now use to your
    advantage when working on a performance problem.
  prefs: []
  type: TYPE_NORMAL
- en: '`LazyCancelOrderBook` illustrates how valuable deferred evaluation can be in
    a performance-sensitive environment. When faced with a performance challenge,
    ask yourself the following questions to see whether it is possible to defer work
    (CPU work, not your actual work!). The following table lists each question and
    how it was answered with the order book:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Question** | **Application to order book example** |'
  prefs: []
  type: TYPE_TB
- en: '| How can I decompose into smaller discrete chunks? | The act of canceling
    was decomposed into identifying the event that was sent to the requester and removing
    the canceled order from the book state. |'
  prefs: []
  type: TYPE_TB
- en: '| Why am I performing all of these steps now? | Originally, order removal happened
    eagerly because it was the most logical way to model the process. |'
  prefs: []
  type: TYPE_TB
- en: '| Can I change any constraints to allow me to model the problem differently?
    | Ideally, we would have liked to remove the constraint requiring rejection of
    nonexistent orders. Unfortunately, this was out of our control. |'
  prefs: []
  type: TYPE_TB
- en: '| What operations in my system are most performant? | Executing an order and
    resting an order on the book are the most performant operations. We leveraged
    fast execution time to perform removals of canceled orders from the book. |'
  prefs: []
  type: TYPE_TB
- en: Like any approach, deferred evaluation is not a panacea. Diligent benchmarking
    and profiling are necessary to validate the benefit delivered by the change. Arguably
    the implementation of `LazyCancelOrderBook` is more complicated than `QueueOrderBook`,
    which will increase the cost to maintain the system. In addition to making implementation
    more complicated, it is now more difficult to reason about runtime performance
    due to the variable cost of order execution. For the scenarios that we tested, `LazyCancelOrderBook`
    remained at parity with or better than `QueueOrderBook`. However, we only exercised
    a few of the many possible scenarios, and we did so with only a single price level
    in the order book. In a real-world environment, additional benchmarking and profiling
    are needed to build enough confidence that this new implementation delivers better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Historical data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have done great work with the order book, and we hope, have learned valuable
    skills along the way! It is now time to explore a new facet of MVT's activities.
    A group of expert traders and data scientists are constantly studying historical
    market data to design performant trading strategies. Until now, the company has
    not had the luxury of allocating technical resources to this team. As a result,
    this group has been using clunky, unreliable, and under-performing tools to analyze
    market data and build elaborate trading strategies. With a performant order book,
    the top priority is to focus on improving the strategies implemented by the company.
    Your new best friend, Dave, has explicitly asked for you to join the team and
    help them modernize their infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Lagged time series returns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main tool used by the team is a simple program designed to compute lagged
    time series returns from historical trade execution data. So far, this tool has
    been a big disappointment. Not only does it return mostly invalid results, it
    is also slow and fragile. Before diving into the code, Dave gives you a short
    presentation of the business rules involved. Return time series are derived from
    midpoint time series. A midpoint is calculated on each minute, and it is based
    on the bid and ask prices of each trade execution. Consider the following table
    as a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Execution time** | **Bid price** | **Ask price** | **Midpoint** |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:45 | 2.3 | 2.5 | 2.55 |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:46 | 2.1 | 2.4 | 2.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:47 | 2.9 | 3.4 | 3.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:48 | 3.2 | 3.4 | 3.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:49 | 3.1 | 3.3 | 3.2 |'
  prefs: []
  type: TYPE_TB
- en: The formula to calculate a midpoint is *(bid_price + ask_price) / 2*. For example,
    the midpoint at 01/29/16 07:47 is *(2.9 + 3.4) / 2*, that is, 3.15.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the real world, a midpoint would be weighed by the volume of the transaction,
    and the time series would use a more fine-grained time unit, such as seconds or
    even milliseconds. To keep the example simple, we disregard the volume dimension
    by assuming a volume of 1 for all executions. We also focus on calculating one
    data point per minute instead of a more granular time series that would use seconds
    or even milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'A series of midpoints is used to compute a series of returns. A series of returns
    is defined for a certain rollup value in minutes. To calculate the three minute
    return at time t[3], the formula is: (midpoint_at_t[3] - midpoint_at_t[0]) / midpoint_at_t[0].
    We also multiply the result by 100 to use percentages. If we use the previous
    midpoint series to calculate a three minute return series, we obtain the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Midpoint** | **3 minute return** |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:45 | 2.55 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:46 | 2.25 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:47 | 3.15 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:48 | 3.3 | 22.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 01/29/16 07:49 | 3.2 | 29.69 |'
  prefs: []
  type: TYPE_TB
- en: Note that the first three midpoints do not have a corresponding three minute
    return as there is no midpoint that is old enough to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are now familiar with the domain and can have a look at the existing code.
    Starting with this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Everything looks straightforward. Note that prices, midpoints, and returns are
    represented as `Int` and `Double`. We assume that our system is able to normalize
    the prices as integers instead of decimals. This simplifies our code, and also
    improves the performance of the program since we use primitive `Double` instead
    of, for example, `BigDecimal` instances. `TimestampMinutes` is similar to the
    more commonly used Epoch timestamp, but only down to the minute (see [https://en.wikipedia.org/wiki/Unix_time](https://en.wikipedia.org/wiki/Unix_time)).
  prefs: []
  type: TYPE_NORMAL
- en: 'After studying the model, we look at the existing implementation of the `computeReturnsWithList`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This method assumes that the list of midpoint received as input is already
    sorted by execution time. This randomly accesses various indices of the list to
    read the midpoints that are required to compute each return. To compute the second
    return value (index 1 in the returned list) with a rollup value of three minutes,
    we access elements at index 4 and 1 in the input list. The following diagram provides
    a visual reference for how returns are computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lagged time series returns](img/image_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You have been warned that this method is slow, but it is also incorrect. Dave
    has verified many times that it returns incorrect results. Before tackling the
    performance issue, you have to handle the correctness problem. Optimizing an incorrect
    approach would not be a good use of your time and, therefore, of the company''s
    money! Rapidly, you realize that this method puts too much trust in the data that
    it is fed. For this algorithm to work, the input list of midpoints has to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: This has to be properly sorted by execution time, from the oldest to the newest
    execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This has to have no more than one midpoint per minute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This has to not contain any minutes without a midpoint, that is, it has no missing
    data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You bring this up to Dave to better understand how the midpoint series is generated.
    He explains that it is loaded from sequential logs that are recorded by the order
    book. It is certain that the list is sorted by execution time. Also, he assures
    you that considering the large volume of trades handled by the order book, it
    is impossible to have a minute without a single execution. However, he acknowledges
    that it is more than likely that more than one midpoint is computed for the same
    execution time. It looks like you have found the problem causing invalid returns.
    Fixing it should not be too complicated, and you think that it is now time to
    reflect on the performance issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'We spent time studying the structure of a singly-linked list in the previous
    section. You know that it is optimized for operations involving the head and the
    tail of the list. On the contrary, randomly accessing an element by its index
    is an expensive operation requiring linear time. To improve midpoint execution
    performance, we turn to a data structure with improved random access performance:
    `Vector`.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To improve the performance of our system, we should reconsider the data structure
    that stores `Midpoint` values. A good option is to replace `List` with `Vector`,
    another Scala collection provided by the standard library. The `Vector` is an
    efficient collection that provides effectively constant time random access. The
    cost of random access operations depends on various assumptions, such as, the
    maximum length of the `Vector`. The `Vector` is implemented as an ordered tree
    data structure called a trie. In a trie, the keys are the indices of the values
    stored in the `Vector` (to learn more about tries and their use cases, see [https://en.wikipedia.org/wiki/Trie](https://en.wikipedia.org/wiki/Trie)).
    As `Vector` implements the `Seq` trait, just like `List`, modifying the existing
    method is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Changing the type of the collection is enough to switch to a more performant
    implementation. To make sure that we actually improved the performance, we devise
    a simple benchmark that is designed to use a few hours of historical trade executions
    and measure the throughput of each implementation. The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Return rollup in minutes** | **Throughput (ops per second)**
    | **Error as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `computeReturnsWithList` | 10 | 534.12 | ± 1.69 |'
  prefs: []
  type: TYPE_TB
- en: '| `computeReturnsWithVector` | 10 | 49,016.77 | ± 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| `computeReturnsWithList` | 60 | 621.28 | ± 0.64 |'
  prefs: []
  type: TYPE_TB
- en: '| `computeReturnsWithVector` | 60 | 51,666.50 | ± 1.64 |'
  prefs: []
  type: TYPE_TB
- en: '| `computeReturnsWithList` | 120 | 657.44 | ± 1.07 |'
  prefs: []
  type: TYPE_TB
- en: '| `computeReturnsWithVector` | 120 | 43,297.88 | ± 0.99 |'
  prefs: []
  type: TYPE_TB
- en: Not only does `Vector` yield significantly better performance, it delivers the
    same throughput regardless of the size of the rollup. As a general rule, it is
    better to use `Vector` as a default implementation for immutable indexed sequences. Vector
    effectively provides constant time complexity not only for element random access
    but also for head and tail operations, as well as to append and prepend elements
    to an existing `Vector`.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of `Vector` is a tree structure of parity 32\. Each node
    is implemented as an array of size 32, and it can store either up to 32 references
    to child nodes or up to 32 values. This 32-ary tree structure explains why the
    complexity of `Vector` is "effectively constant" instead of "constant". The real
    complexity of the implementation is log(32, N), where N is the size of the vector.
    This is considered close enough to actual constant time. This collection is a
    good choice to store very large sequences because the memory is allocated in chunks
    of 32 elements. These chunks are not preallocated for all levels of the tree,
    but only allocated as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Until Scala 2.10, one downside of `Vector` as compared to `List` was the lack
    of pattern matching support. This is now fixed and you can pattern-match an instance
    of `Vector` in the same way you pattern match a `List`. Consider this short example
    of a method pattern matching a `Vector` to access and return its third element
    or return `None` if it contains fewer than three elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking this method in the REPL demonstrates that pattern matching can be
    applied, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Data clean up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The return algorithm is now blazingly fast. That is, blazingly fast to return
    incorrect results! Remember that we still have to handle some edge cases and clean
    up the input data. Our algorithm only works if there is exactly one midpoint per
    minute, and Dave informed us that we are likely to see more than one midpoint
    computed for the same minute.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle this problem, we create a dedicated `MidpointSeries` module and make
    sure that an instance of `MidpointSeries`, wrapping a series of `Midpoint` instances,
    is properly created without duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Our `removeDuplicates` method uses a tail recursive method (Refer to [Chapter
    3](ch03.html "Chapter 3. Unleashing Scala Performance"), *Unleashing Scala Performance*).
    This groups all the midpoints with the same execution time, calculates the average
    value of these data points, and builds a new series with these average values.
    Our module provides a `fromExecution` factory method to build an instance of `MidpointSeries`
    from a `Vector` of `Execution`. This factory method calls `removeDuplicates` to
    clean up the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve our module, we add our previous `computeReturns` method to the `MidpointSeries`
    class. That way, once constructed, an instance of `MidpointSeries` can be used
    to compute any return series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This is the same code that we previously wrote, but this time, we are confident
    that `points` does not contain duplicates. Note that the constructor is marked `private`,
    so the only way to instantiate an instance of `MidpointSeries` is via our factory
    method. This guarantees that it is impossible to create an instance of `MidpointSeries`
    with a "dirty" `Vector`. You release this new version of the program, wish good
    luck to Dave and his team, and leave for a well deserved lunch break.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you return, you are surprised to find Vanessa, one of the data scientists,
    waiting at your desk. "The return series code still doesn''t work", she says.
    The team was so excited to finally be given a working algorithm that they decided
    to skip lunch to play with it. Unfortunately, they discovered some inconsistencies
    with the results. You try to collect as much data as possible, and spend an hour
    looking at the invalid results that Vanessa is talking about. You noticed that
    they all involved trade executions for two specific symbols: FOO and BAR. A surprisingly
    small amount of trades is recorded for these symbols, and it is not unusual for
    several minutes to elapse between trade executions. You questioned Dave about
    these symbols. He explains that these are thinly traded tickers, and it is expected
    to see a lower trading volume for them. The problem is now clear to you. The midpoint
    series recorded for these symbols do not fulfill one of the prerequisite of your
    algorithm: at least one execution per minute. You refrain from reminding Dave
    that he assured you this situation was impossible and start working on a fix.
    The trader is always right!'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are not confident that you can rework the algorithm to make it more robust
    while preserving the current throughput. A better option would be to find a way
    to clean up the data to generate the missing data points. You seek advice from
    Vanessa. She explains that it would not disturb the trading algorithm to perform
    a linear extrapolation of the missing data points, based on the surrounding existing
    points. You write a short method to extrapolate a midpoint at a certain time using
    the previous and following points (respectively, `a` and `b` in the following
    snippet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'With this method, we can write a clean up method that follows the model of
    the previously mentioned `removeDuplicates` function to preprocess the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Our internal tail-recursive method handles the case where two points are already
    consecutive, and the case where a point is missing. In the latter case, we create
    a new point with our `extrapolate` method and insert it in the result `Vector`.
    Note that we use this new point to extrapolate consecutive missing points. We
    update our factory method to perform this additional clean up after removing possible
    duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We now have the assurance that our input data is clean and ready to be used
    by our return series algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Handling multiple return series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The team is impressed by the improvements that you implemented, and by how quickly
    you were able to fix the existing code. They mention a project they have had in
    mind for a while without knowing how to approach it. A couple of weeks ago, Vanessa
    designed a machine learning algorithm to evaluate trading strategies over several
    tickers, based on their return series. This algorithm requires that all the return
    series involved contain the same amount of data points. Your previous changes
    already took care of this requirement. However, another condition is that the
    return values must be normalized or scaled. A feature is a machine learning term
    for an individual measurable property. In our example, each return data point
    is a feature. Feature scaling is used to standardize the range of possible values
    to ensure that broad ranges of values do not distort a learning algorithm. Vanessa
    explains that scaling features will help her algorithm to deliver better results.
    Our program will handle a set of return series, compute a scaling vector, and
    calculate a new set of normalized return series.
  prefs: []
  type: TYPE_NORMAL
- en: Array
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this system, we consider switching from `Vector` to `Array`. `Array` is
    a mutable, indexed collection of values. It provides real constant complexity
    for random access, as opposed to `Vector`, which implements this operation in
    effectively constant time. However, contrary to `Vector`,  `Array` is allocated
    once as a single and contiguous chunk of memory. Furthermore, it does not permit
    append and prepend operations. A Scala `Array` is implemented with a Java `Array`,
    which is memory optimized. A Scala `Array` is more user-friendly than the native
    Java `Array`. Most methods that are available on other Scala collections are made
    available when using `Array`. Implicit conversions are used to augment `Array`
    with `ArrayOps` and `WrappedArray`. `ArrayOps` is a simple wrapper for `Array`
    to temporarily enrich `Array` with all the operations found in indexed sequences.
    Methods called on `ArrayOps` will yield an `Array`. On the contrary, a conversion
    from `Array` to `WrappedArray` is permanent. Transformer methods called on `WrappedArray`
    yield another `WrappedArray`. We see this in the standard library documentation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Having decided to use `Array` for our new module, we start working on the code
    to scale the features of each return series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'A scaling vector is computed for a set of series. The first value of the vector
    is used to scale the first series, the second value for the second series, and
    so on. The scaling value is simply the greatest value in the series. We can now
    write the code to use the scaling vector and compute the normalized version of
    the frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We zip each series with its scaling value, and create a new scaled return series.
    We can compare the presented version of the code using `Array` with another, almost
    identical, implementation using `Vector` (this code is omitted here for brevity,
    but it can be found in the source code attached to the book):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Series Size** | **Throughput in operations per second**
    | **Error as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithVector` | 60 | 101,116.50 | ± 0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithArray` | 60 | 176,260.52 | ± 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithVector` | 1,440 | 4,077.74 | ± 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithArray` | 1,440 | 7,865.85 | ± 1.39 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithVector` | 28,800 | 282.90 | ± 1.06 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithArray` | 28,800 | 270.36 | ± 1.85 |'
  prefs: []
  type: TYPE_TB
- en: These results show that `Array` performs better than `Vector` for shorter series.
    As the size of the series increases, their respective performances are on-par.
    We can even see that the throughput is identical for a series containing 20 days
    of data (28,800 minutes). For larger sequences, the locality of `Vector` and its
    memory allocation model alleviate the difference with `Array`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our implementation is idiomatic: it uses higher-order functions and immutable
    structures. However, using transform functions, such as `zip` and `map`, creates
    new instances of `Array`. An alternative is to leverage the mutable nature of `Array`
    to limit the amount of garbage generated by our program.'
  prefs: []
  type: TYPE_NORMAL
- en: Looping with the Spire cfor macro
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scala supports two loop constructs: the `for` loop and the `while` loop. The
    latter, in spite of its good performance characteristics, is usually avoided in
    functional programming. It requires the usage of mutable state and `var` to keep
    track of the looping condition. In this section, we will show you a technique
    to take advantage of `while` loop performance that prevents mutable references
    from leaking into application code.'
  prefs: []
  type: TYPE_NORMAL
- en: Spire is a numeric library written for Scala that allows developers to write
    efficient numeric code. Spire leverages patterns, such as, type classes, macros,
    and specialization (remember specialization from [Chapter 3](ch03.html "Chapter 3. Unleashing
    Scala Performance"), *Unleashing Scala Performance*). You can learn more about
    Spire at [https://github.com/non/spire](https://github.com/non/spire).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the macros made available by Spire is `cfor`. Its syntax is inspired
    from the more traditional for loop that is encountered in Java. In the following
    implementation of feature scaling, we use the `cfor` macro to iterate over our
    series and normalize the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This example highlights that `cfor` macros can be nested. The macro is essentially
    syntactic sugar that compiles to a Scala `while` loop. We can examine the following
    generated bytecode to prove this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice the two `goto` statements, instructions `96` and `84`, which are
    used to loop back respectively to the beginning of the outer loop and the inner
    loop (which respectively begin with instructions `11` and `39`). We can run a
    benchmark of this new implementation to confirm the performance gain:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Benchmark** | **Series size** | **Throughput (ops per second)** | **Error
    as percentage of throughput** |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithArray` | 60 | 176,260.52 | ± 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithCfor` | 60 | 256,303.49 | ± 1.33 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithArray` | 1,440 | 7,865.85 | ± 1.39 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithCfor` | 1,440 | 11,446.47 | ± 0.89 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithArray` | 28,800 | 270.36 | ± 1.85 |'
  prefs: []
  type: TYPE_TB
- en: '| `normalizeWithCfor` | 28,800 | 463.56 | ± 1.51 |'
  prefs: []
  type: TYPE_TB
- en: The macro, which is compiled to a while loop, is able to deliver better performance.
    Using the `cfor` construct, we are able to retain performance while avoiding the
    introduction of multiple vars. Although this approach sacrifices immutability,
    the scope of mutability is limited and less error-prone than an equivalent implementation
    using an imperative `while` or `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored and experimented with various collection implementations.
    We discussed the underlying representation, complexity, and use cases of each
    data structure. We also introduced a third-party library, Spire, to improve the
    performance of our programs. Some of the implementations presented drifted away
    from typical functional programming practices, but we were able to restrict the
    use of mutable state to internal modules, while still exposing functional public
    APIs. We expect that you are eager to learn more, but in the next chapter, we
    will become lazy! In contrast to this chapter, which focused on eager collections,
    we turn our attention to lazy collections in the next chapter.
  prefs: []
  type: TYPE_NORMAL
