- en: '*Chapter 12*: Beyond Functional Requirements'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes I feel like I am being forgotten.
  prefs: []
  type: TYPE_NORMAL
- en: — Anonymous
  prefs: []
  type: TYPE_NORMAL
- en: While the functional requirements of the core of the system may be met adequately,
    it is just as important to place focus on the operational characteristics of the
    system. In this chapter, we will look at common pitfalls and how to get past them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance and scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trunk-based development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refactoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invocation style
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have learned about various aspects of the
    software life cycle to create a robust solution from a cross-functional perspective.
    We will also discuss additional features that we will need to add to make our
    solution performant, scalable, resilient to failure, and gain the ability to make
    changes reliably, repeatably, and rapidly. Furthermore, we will also examine the
    implications of making these changes and the potential impacts this may have on
    our bounded contexts and their boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we saw how it is possible to break down an existing application
    along bounded context boundaries. We also saw how it is possible to split bounded
    contexts to be extremely fine-grained, often as physically disparate components.
    Failure in any of these components can cause disruptions in others that are dependent
    on them. Obviously, early detection and more importantly attribution to specific
    components through a combination of proactive and reactive monitoring can ideally
    prevent or, at the very least, minimize business disruption.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to monitoring, most teams seem to think of **technology runtime
    metrics** that we associate with components (such as CPU utilization, memory consumed,
    queue depths, exception count, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Lending Objectivity to Metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it more formal, we use the terms **Service-Level Objectives** (**SLOs**)
    and **Service-Level Indicators** (**SLIs**) specified within a **Service-Level
    Agreement** (**SLA**) to mean the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SLO**: An agreement between the provider and customer about a specific measurable
    metric. For example, 99.99% uptime, 100 ms response time for 1,000 concurrent
    users for requests in the 99th percentile, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLA**: A collection of SLOs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLI**: The actual numbers against an SLO. For example, your system might
    have an uptime SLI of 99.95%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technology metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to monitoring, most teams seem to think of technology runtime
    metrics that we associate with components (such as CPU utilization, memory consumed,
    queue depths, exception count, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: However, it is just as much if not more important to be able to associate a
    set of business-relevant metrics (such as the number of LC applications submitted
    in the last hour, the number of LC applications rejected, and so on) and DevOps
    metrics (such as lead time, mean time to restore, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Business metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An inability to associate and monitor business SLIs with a component may be
    an indicator of the component being too fine-grained. On the flip side, if there
    are too many business SLIs associated with a single component that is of interest
    to a multitude of business stakeholder groups, it may be an indicator that a more
    fine-grained decomposition may be justified. At the end of the day, the monitoring
    apparatus we have in place should be able to tell us if we are violating/meeting/exceeding
    SLOs.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **DevOps Research and Assessment** (**DORA**) research foundation has published
    an online quickcheck ([https://www.devops-research.com/quickcheck.html](https://www.devops-research.com/quickcheck.html))
    tool and report ([https://www.devops-research.com/research.html](https://www.devops-research.com/research.html))
    to quickly provide information on how organizations compare with industry peers
    and how to make progress toward elite status. While discussing the full nuance
    of what it takes to establish a long-term culture of continuous improvement is
    out of scope for this book, we reference the four key metrics highlighted in the
    research paper as indicators of software delivery performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lead time**: How long does it take to go from code committed to code successfully
    running in production?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment frequency**: How often does your organization deploy code to production
    or release it to end users?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time to restore**: How long does it generally take to restore service when
    a service incident or a defect that impacts users occurs?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change failure percentage**: What percentage of changes to production or
    releases to users results in degraded service?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When it comes to observability, there is the risk of focusing on specific metrics
    in isolation and missing the forest for the trees. To avoid metrics being misused
    and, more importantly, running the risk of drawing incorrect conclusions, we recommend
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Take a holistic view**: Focusing more or less equally on all aspects of the
    delivery life cycle as opposed to focusing on just a particular area can go a
    long way. If you are able to include information from planning, requirements intake,
    development, build, test, deploy, and feedback from running production systems,
    then you may be able to conclude reasonably that you have a high-performing team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Employ ratcheting**: Having recognized an improvement area, how do you go
    about setting yourself up for improvement? Setting clear, objective goals that
    are measurable and trackable (no pun intended) for improvement is paramount to
    be able to subsequently meet them. In order to ensure that there is continuous
    incremental improvement, ratcheting is a technique that can be employed. A ratchet
    is a device that resembles a wrench but is unique in that it only turns in one
    direction. In this context, ratcheting involves doing the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the current level as the minimum starting point.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a small incremental improvement in a relatively small amount of time.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-adjust the baseline to the new level attained as part of step 2.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If levels descend below the baseline, take stop-the-line action until baselines
    are restored.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 1.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Ratcheting allows teams to set incremental milestones as intermediate goals
    while moving closer to a much better place all the time.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting an attitude of constant learning and incremental improvement through
    ratcheting as opposed to one that looks to police and penalize can go a long way
    toward instituting a system that can be effective.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we have spent a lot of energy splitting our system
    into multiple, fine-grained independent components. For example, the LC application
    is submitted against the command-side component, whereas the status of the LC
    application is serviced by the query side. Because these are distinct components,
    there will be a time lag during which the two systems are not consistent with
    each other. So, querying the status of an LC application immediately after submitting
    may produce a stale response until the time that the query side processes the
    submit event and updates its internal state. In other words, the command side
    and the query side are considered to be *eventually consistent*. This is one of
    the trade-offs that we need to embrace when working with distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Eric Brewer (professor emeritus of computer science at the University of California,
    Berkeley) formalized the trade-offs involved in building distributed systems in
    what is called the *CAP theorem*. The theorem postulates that distributed systems
    can either be highly available or consistent in the event of a network partition,
    not both at the same time. Given the three characteristics, **c**onsistency, **a**vailability,
    and **p**artition tolerance, the theorem postulates that distributed systems can
    either be highly available or consistent in the event of a network partition,
    not both at the same time. This means that distributed applications that are expected
    to be highly available will have to forsake strong consistency.
  prefs: []
  type: TYPE_NORMAL
- en: This may make it appear that this is a deal-breaker, but in reality, most real-world
    business problems are tolerant to being eventually consistent. For example, there
    may be a requirement that an order cannot be canceled after it has shipped. In
    an eventually consistent system, there may exist a time window (albeit small)
    where we may allow a shipped order to be canceled. To deal with such scenarios,
    we may need to enhance the business process to account for these inconsistencies.
    For example, before issuing a refund for a canceled order, we may need to validate
    that the order has not physically shipped or has been returned. Even in the extreme
    case where we may have erroneously issued a refund for a shipped order, we can
    request the customer to return it before an expiry period to avoid getting charged.
    If the customer fails to return the order, we may charge the customer or write
    off the amount as lost business. Obviously, all this adds a level of complexity
    to the solution because we may need to account for edge conditions through a series
    of compensating actions. If none of this complexity is acceptable and strong consistency
    is non-negotiable, then shipping and order cancellation functionality will have
    to be part of the same bounded context.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we saw how it is possible and sometimes even necessary
    to break functionality down into fine-grained components that are physically separated
    from each other – requiring a network to collaborate. Let’s assume that this collaboration
    is achieved in a loosely coupled manner – justifying the need for disparate bounded
    contexts from a logical perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance is a very important SLO that is typically associated with most
    applications. When it comes to performance, it is essential to understand the
    basic terms. This is best illustrated using an example as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – The elements of network performance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – The elements of network performance
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown here, the following terms are relevant in the context of performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: The delay introduced by the network (A + B)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response Time**: the total time taken by the system to respond to the user
    (A + B + C)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bandwidth**: The maximum capacity of the network (D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput**: The amount of data processed in a given amount of time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The introduction of a network between two components introduces constraints
    in the form of network latency and bandwidth. Even if processing time on the server
    is theoretically reduced to zero, latency and bandwidth constraints cannot be
    avoided. This problem can only get worse as the number of network hops increases.
    This means that it is impossible for networked applications to provide the same
    level of performance as their non-networked counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: The need to scale to support a larger number of requests can further complicate
    things. Given that Moore’s law has slowed down considerably in the last decade
    or so, it is less feasible to continue scaling up by using more and more powerful
    machines. This means that beyond a point, scaling out by using multiple instances,
    and thereby (re-)introducing a reliance on the network, is inevitable.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it evident that performance and scale requirements can have a significant
    impact on how we choose to distribute our components. Having a clear understanding
    of performance and scale SLOs is a necessary prerequisite before attempting to
    distribute distinct components. On the flip side, if you are in a situation where
    you already have distributed components that are not meeting performance and scale
    SLOs, one option is to aggregate them back together. If that is not feasible,
    it may be worth embracing alternative customer experiences along with a non-blocking,
    event-driven style of architecture to create a perception of better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Trunk-based development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eric Evans, the inventor of DDD, talks about how **continuous integration**
    (**CI**) helps preserve the sanctity of the domain model within a bounded context.
    When more than one person works in the same bounded context, it tends to fragment.
    Obviously, the bigger the team, the higher the likelihood of this problem occurring.
    Even a team as small as three or four people can encounter serious issues. We
    have also seen that beyond a point, there may be diminishing returns if we try
    to break the system into extremely fine-grained bounded contexts.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it very important to institute a process of merging/integrating all
    code and other implementation artifacts frequently, aided by **automated tests**
    to flag such fragmentation. In addition, this allows the team to apply the ubiquitous
    language relentlessly, each time refining the domain model to represent the problem
    more accurately. In other words, it is critical to practice continuous integration.
    Many teams make use of a CI server to run tests but tend to postpone integration
    until very late making use of an excessive number of long-living branches (popularized
    by Gitflow; [https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow)
    and merge requests – practicing an anti-pattern known as CI theatre ([https://www.gocd.org/2017/05/16/its-not-CI-its-CI-theatre.html](https://www.gocd.org/2017/05/16/its-not-CI-its-CI-theatre.html)).
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to branch-based development is *trunk-based development* where
    each developer works in incremental batches and merges that work into the main
    (also called trunk) branch at least once (and potentially several times) a day.
    The DORA team has published research ([https://services.google.com/fh/files/misc/state-of-devops-2021.pdf#page=27](https://services.google.com/fh/files/misc/state-of-devops-2021.pdf#page=27))
    that shows that elite performers practice trunk-based development to maximize
    the effectiveness of their CI practice and by extension their ability to continuously
    enhance their domain models and keep up with changing business needs.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, every commit to the trunk would constitute finished, production-ready
    work. But it is also fairly normal for certain pieces of work to take longer to
    complete. This may make it appear that there is a need to forsake trunk-based
    development and resort to branch-based development. However, there is no need
    to compromise the continuous integration flow to accommodate for such eventualities.
    Paul Hammant ([https://paulhammant.com/](https://paulhammant.com/)) talks about
    this technique called *branch by abstraction* where the effects of unfinished
    pieces of work are hidden behind an abstraction layer. This abstraction layer
    is typically implemented by either making the new piece of functionality hidden
    from the end user or in more sophisticated cases, using feature flags ([https://martinfowler.com/articles/feature-toggles.html](https://martinfowler.com/articles/feature-toggles.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Continuous testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an ideal world, continuous integration will enable us to adopt continuous
    testing, which provides us with constant and early feedback. This is essential
    because our bounded contexts and the resulting domain models are in a constant
    state of evolution. Without the bedrock of a stable suite of tests, it can become
    very hard to sustain a reliable process. Approaches such as the test pyramid,
    testing trophy, honeycomb, and so on are acknowledged as reasonable ways to implement
    a sound continuous testing strategy. All of these approaches are based on the
    premise that a large number of cheap (computationally and cognitively) unit tests
    form the foundation of the strategy, with the number of tests in other categories
    (service, UI, manual, and so on) reducing as we move through the chain.
  prefs: []
  type: TYPE_NORMAL
- en: However, we are in this new world of fine-grained components that work by communicating
    with each other. Hence, there is a bigger need to verify interactions at the periphery
    in a robust manner. Unit tests alone that rely mostly on mocks and stubs may not
    suffice because the behavior of collaborators may change inadvertently. This may
    lead to a situation where unit tests may run successfully, but the overall functionality
    may be broken. This may cause teams to lose faith in the practice of unit testing
    as a whole and resort to using more end-to-end functional tests. However, these
    styles of tests can be extremely expensive ([https://www.youtube.com/watch?v=VDfX44fZoMc](https://www.youtube.com/watch?v=VDfX44fZoMc))
    to set up and maintain, especially when we are looking to automate them. Consequently,
    most teams ignore the results of a majority of automated testing methods and rely
    almost exclusively on manual testing to verify anything but the most trivial functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Any manual testing requires most if not all functionality to be ready before
    any meaningful testing can commence. Furthermore, it is time-consuming, error-prone,
    and usually not repeatable. Consequently, almost all testing can be carried out
    only when it is very close to the end, rendering the idea of continuous testing
    a pipe dream. Despite all its limitations, teams continue to rely on manual testing
    because it seems to provide the most psychological safety in comparison to its
    automated counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, what we need is the speed of unit tests and the confidence
    provided by manual testing. We will look at a few specific forms of testing that
    can help restore the balance.
  prefs: []
  type: TYPE_NORMAL
- en: Contract testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The limitation of unit testing is that the assumptions made in mocks/stubs
    can be invalid or become stale as producers make changes to the contract. On the
    other hand, manual tests suffer from being slow and wasteful. Contract tests can
    provide a means to bridge the gap by providing a happy medium where the producer
    and consumer share an executable contract that both producer and consumer can
    rely on as functionality changes/evolves. At a high level, this works in the manner
    depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Contract testing: high-level flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.2 – Contract testing: high-level flow'
  prefs: []
  type: TYPE_NORMAL
- en: This allows the consumers and the producers to work collaboratively, and get
    feedback a lot earlier in the cycle. For the consumer, they get to participate
    in sharing their expectations with the producer and make use of versioned, producer-approved
    stubs for their own testing without having to depend on the producer’s real system.
    Likewise, producers gain a deeper understanding of how their services are consumed,
    setting them free to make bolder changes as long as they remain compatible.
  prefs: []
  type: TYPE_NORMAL
- en: Test-First Design
  prefs: []
  type: TYPE_NORMAL
- en: The essence of domain-driven design is all about gaining as thorough an understanding
    of the problem in order to solve the problem right. Test-first design enables
    gaining a better understanding of the problem because it mitigates the risk of
    becoming biased by the solution we have built. In addition, it also promotes the
    automated verification of these requirements, which allows them to be used as
    an effective aid to regression testing. We are strong proponents of this practice
    for this reason and encourage you to consider adopting TDD as a core practice
    to accentuate your effectiveness with DDD.
  prefs: []
  type: TYPE_NORMAL
- en: Mutation testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of teams author a variety of tests to ensure that they are building a
    high-quality solution. Test coverage is typically used as a quantitative measure
    to assess the quality of testing. However, test coverage is a necessary but not
    sufficient condition to establish test quality. Low test coverage almost definitely
    means there is a test quality problem, whereas high coverage does not imply better
    tests. In an ideal world, even a single line change in production code (caused
    by a change in business requirements), without changing test code, will result
    in a test failure. If this can be guaranteed for every single change across the
    code base, you may be able to safely rely on such a test suite.
  prefs: []
  type: TYPE_NORMAL
- en: Mutation testing is a practice that automatically inserts small bugs in production
    code (called *mutants*) and reruns an existing suite of tests to ascertain the
    quality of tests. If your tests failed, the mutant is killed. Whereas if your
    tests passed, the mutant survives. The higher the number of mutants killed, the
    more effective your tests are.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it may apply mutations such as inverting conditionals, replacing
    relational operators, returning nulls from methods, and so on, and then you can
    check the effect this has on your existing tests. If no tests fail despite these
    mutations, these tests may not be as helpful as you hoped them to be. This allows
    us to draw more objective conclusions about the quality of our tests. Given how
    it works (by mutating code), it is computationally intensive and hence may take
    a long time to run. If you employ a test-first design and have a fast suite of
    unit tests, mutation testing can be a great complement that can help discover
    missed requirements and/or test cases earlier in the development cycle. From that
    perspective, we see it as an invaluable tool to augment the adoption of DDD within
    teams. Tools such as PITest ([https://pitest.org/](https://pitest.org/)) are a
    great tool to perform mutation testing in your Java applications.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen earlier, mutation testing can help point out chinks in the functional
    aspects of your application. Chaos testing plays a similar role to help identify
    shortcomings in meeting non-functional requirements caused by reliance on network
    and infrastructure. It started becoming popular through the use of large-scale
    distributed, cloud-based architectures pioneered by companies such as Amazon,
    Netflix, and so on. Netflix initially released a tool called Chaos Monkey ([https://netflix.github.io/chaosmonkey/](https://netflix.github.io/chaosmonkey/))
    that randomly terminated instances in production(!) to ensure that engineers implement
    services that are resilient to failure. They followed this by releasing a set
    of related tools, collectively called the Simian Army (which is now defunct) to
    test a variety of non-functional aspects such as latency, security compliance,
    unused resources, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: While Netflix performs this style of testing in production, the rest of us will
    benefit immensely if we adopt these practices even in lower environments at the
    outset. From a strategic perspective, chaos testing can provide feedback on the
    amount of coupling between components and whether the boundaries of these components
    are appropriate. For example, if a component that you are dependent on goes down
    or experiences problems, does this take you down as well? If so, are there ways
    to mitigate this? It can also provide feedback about your monitoring and alerting
    apparatus. From a tactical perspective, it can provide insights into the shortcomings
    of the invocation style being used to communicate among components.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have chosen to highlight contract testing, mutation testing,
    and chaos testing because we see them as game-changers in the application of DDD.
    Teams will benefit by looking at these methods as augmentations to other testing
    methods when coming up with a well-rounded testing strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The intent of applying domain-driven design is to create an ecosystem of loosely
    coupled components – so that each of these components can evolve independently
    of each other. This includes how these components are deployed to production.
    At a high level, we have at least three styles of deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-process monolith**: Where large portions of the application are deployed
    as a single unit, with all components that are included in the deployment running
    in a single process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed monolith**: Where the application is split into multiple components
    with each running in its own process and/or host, but deployed as a single unit
    and/or requiring non-trivial amounts of coordination and tight coupling among
    components and their owners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent components**: Where the application is split into multiple components
    with each running in its own process and/or host, deployed independently of each
    other and requiring minimal to no coordination among component owners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also have a number of deployment strategies that we can employ. We list
    some of the more popular ones in order of increasing complexity and richness:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic**: Likely the oldest style of deployment where the newer version of
    the application replaces the old, usually with some amount of downtime. Rollback
    typically means redeploying the previously live version, again taking some amount
    of downtime. This is a fairly common deployment strategy for those applications
    where a certain amount of downtime is acceptable. This may include non-business
    critical applications and/or third-party packages where we do not have a say in
    how those applications manage their deployments. In the case of certain monoliths,
    this may be the only feasible option due to the overall complexity of the system
    as a whole. This style of deployment typically starts out being fairly simple
    and well understood and may suffice for non-critical applications. On the flip
    side, it requires the deployment and release to happen in one single tightly coupled
    step and may involve some amount of downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blue-green**: A deployment strategy that makes use of two identical environments,
    a “blue” and a “green” environment, with one representing the current production
    and another representing the newer version. The current version continues to service
    traffic, while testing and acceptance are carried out on the new version without
    exposing it to end users. User traffic is switched to the newer version once testing
    activities are deemed to be successfully completed. It is pertinent to note that
    live user traffic is directed only to one environment at any given time. This
    style of deployment enables deployment with (near) zero downtime and also allows
    decoupling of the process of deployment and release. Rollbacks are easier because
    it simply means redirecting traffic to the older version. On the other hand, it
    requires double the amount of capacity at least during the time of deployment.
    This may make it cost-prohibitive for monolithic applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rolling**: A deployment strategy where a small subset of current version
    instances is incrementally replaced by newer version instances. Both old and new
    versions of the software continue to run in parallel until all instances of the
    old are replaced with new ones. In simple cases, rollback typically means replacing
    the newer version instances with older ones. This style of deployment also enables
    zero-downtime deployment, while also allowing side-by-side testing of old and
    new versions with real users. Rolling deployments can make rollbacks relatively
    easy by aborting the introduction of instances of the new version and re-introducing
    the old version and hence can reduce the *blast radius* of a bad release. Unlike
    the case with blue-green deployments, here deployment and release cannot be decoupled.
    Deployment means that the system is released (at least for a subset of users).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Canary**: A variation of the rolling deployment where traffic is routed to
    newer instances in a controlled and phased manner, typically an increasing proportion
    of request volume (for example, 2% → 25% → 75% → 100% of users). This deployment
    style enables more fine-grained control of the extent of the release as compared
    to rolling deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A/B deployment**: A variation of canary deployment where multiple versions
    (with one or more variations) of new functionality may run simultaneously as “experiments”
    along with the current version. Further, these variations may be targeted to specific
    sets of users. It allows for testing more than two combinations at the same time
    with real users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When working with monolithic applications, teams are usually forced to restrict
    themselves to either basic or at the most blue-green deployments because the cost
    and complexity involved in adopting more sophisticated deployment strategies are
    a lot higher. On the other hand, distributed monoliths make this even more complicated
    because it now requires coordination among physically disparate components and
    teams. As long as we are able to maintain a balance between component granularity
    and coupling, we should be able to support a variety of advanced deployment strategies.
  prefs: []
  type: TYPE_NORMAL
- en: In today’s modern ecosystem where there is a tremendous amount of competition
    to deliver new features and innovate faster, there is a need to support more complex
    forms of deployment with the least amount of risk and disruption to the business.
    If supporting flexible deployment strategies proves to be too hard, there is very
    likely a need to re-examine your context boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over a period of time, there will be a need to realign context boundaries, domain
    events, APIs, and so on. There tends to be a stigma associated with things not
    working perfectly the first time and justifying the need for refactoring at the
    inter-component scale. However, this may be required for multiple reasons outside
    our control, ranging from competitor ecosystem changes, evolving/misunderstood
    requirements, inability to meet non-functional requirements, organizational and
    team responsibility changes, and so on. Hence, refactoring is a core discipline
    that software teams will need to embrace as a first-class practice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We are covering only the strategic (inter-component) aspects of refactoring
    in this chapter. There are several great works on the tactical (intra-component)
    aspects of refactoring, such as Martin Fowler’s *Refactoring* ([https://refactoring.com/](https://refactoring.com/))
    book and Michael Feathers’ *Working Effectively with Legacy Code*, among others.
  prefs: []
  type: TYPE_NORMAL
- en: From a strategic perspective, this may mean having to break an existing monolith
    into finer-grained bounded contexts or merge fine-grained bounded contexts into
    more coarse-grained ones. Let’s look at each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking an existing monolith
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In previous chapters (10 and 11), we have looked at how it is possible to break
    an existing monolith into finer-grained components. However, it is arguable that
    the monolith was relatively well-structured to start with. Lots of teams may not
    be as fortunate. In such a case, here are some prerequisites that may need to
    be fulfilled:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Perform tactical refactorings**: This will allow you to gain a better understanding
    of the existing system. To do this, start with a set of fitness functions ([https://en.wikipedia.org/wiki/Fitness_function](https://en.wikipedia.org/wiki/Fitness_function))
    and a set of black-box functional tests, perform a refactor, and then replace
    the functional tests with faster-running unit tests. Finally, use the fitness
    functions to evaluate the success of the effort. Repeat this process until there
    is a level of comfort to attempt more complex refactorings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Continuous improvement loop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.3_NEW.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Continuous improvement loop
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduce domain events**: Identify software seams ([http://wiki.c2.com/?SoftwareSeam](http://wiki.c2.com/?SoftwareSeam))
    and publish domain events along those seams. Use the domain events to start decoupling
    the producers and the consumers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pick low-hanging components**: If possible, pick areas with low afferent
    coupling and low to medium complexity at the outset. This will allow you to get
    a firmer grasp of applying these techniques before attempting more complex ones.
    Please refer to [*Chapter 10*](B16716_10_Final_NM_ePub.xhtml#_idTextAnchor150),
    *Beginning the Decomposition Journey* and [*Chapter 11*](B16716_11_Final_NM_ePub.xhtml#_idTextAnchor164),
    *Decomposing into Finer-Grained Components* for details on how to proceed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merging into coarse-grained bounded contexts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Merging two distinct bounded contexts can be relatively less complex than breaking
    down an existing one. However, there are a few nuances that are worth paying attention
    to, in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unification of the ubiquitous language**: In [*Chapter 9*](B16716_09_Final_NM_ePub.xhtml#_idTextAnchor138),
    *Integrating with External Systems*, we examined a variety of ways in which bounded
    contexts can integrate with each other. If the relationship between these bounded
    contexts is symmetric, there may be less work to do. This is because, in a symmetric
    relationship, there likely exists a lot of synergies in the first place. However,
    if the relationship is asymmetric, for example, through an open-host service on
    the producer side and an anti-corruption layer on the consuming side, it means
    that there are possibly two varying ubiquitous languages and likely distinct domain
    models at play. Careful thought will need to be applied to arrive at a ubiquitous
    language that is applicable across the newly merged bounded context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust internal domain models**: Adoption of a common ubiquitous language
    primarily means making use of a common domain model across the newly merged bounded
    context. This means that the aggregates, entities, and value objects will need
    to be unified, which may then require changes at the persistence layer as well.
    If there are domain events that are published and consumed exclusively between
    these components, those domain events may be candidates to be retired. At this
    stage, it may not be prudent to make any changes to any public interfaces – specifically
    those exposed using an open-host service (for example, public HTTP APIs and other
    domain events).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust the public API design**: As a final step, it will be prudent to refactor
    redundant and/or inefficient public interfaces to conclude the exercise and derive
    the intended benefits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is pertinent to note that this style of continuous improvement can be extremely
    challenging to adopt without the solid bedrock of a sound set of engineering practices,
    specifically the testing and deployment automation practices that we discussed
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Invocation style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When integrating two bounded contexts that are running in distinct processes,
    there are two ways to consummate interactions: synchronous and asynchronous.'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous invocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The client blocks until the server provides a response. Optionally, implementations
    can choose to wait for an amount of time for the invoked operation to complete
    before timing out. An example of such an interaction is a blocking HTTP call made
    to start a new LC application like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Synchronous invocation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Synchronous invocation
  prefs: []
  type: TYPE_NORMAL
- en: When the call returns successfully, the client is sure that their request to
    create a new LC application has worked. If the server is slow to respond, it can
    result in a performance bottleneck, especially in high-scale scenarios. To cope
    with this, the client and the server may agree on a response time SLO for that
    interaction. The client can choose to wait for a response from the server for
    the agreed amount of time after which the client times out the request and considers
    it a failure. Given that the client blocks on a server response, it is not able
    to do anything else while it waits, even though it may have the resources to do
    other things. To deal with this, the client can employ an asynchronous invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous invocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In an asynchronous style of invocation, the client interacts with the server
    in a manner that frees it to perform other activities. There are a few ways to
    do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fire and forget**: The client initiates a request with the server, but does
    not wait for a response from the server and also does not care about the outcome.
    Such a style of interaction may suffice for *low-priority* activities such as
    logging to a remote server, push notifications, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Fire and forget'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Fire and forget
  prefs: []
  type: TYPE_NORMAL
- en: '**Deferred response**: In some (many?) cases, the client may need to know the
    outcome of the request they had previously made. If the server supports it, the
    client can submit a request, just wait for a confirmation that the request was
    received along with an identifier of the resource to be tracked, and then poll
    the server to track the status of its original request as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Deferred response using poll'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Deferred response using poll
  prefs: []
  type: TYPE_NORMAL
- en: '**Request with callback**: When the client polls for the response, the server
    may not be finished with processing the original request. This means that the
    client may need to poll the server more than once to understand the status of
    the request, which can be wasteful. An alternative is for the server to push a
    response back to the client when it has finished processing by invoking a callback
    that the client provided when making the request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Deferred response using callback'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Deferred response using callback
  prefs: []
  type: TYPE_NORMAL
- en: Given that these interactions happen over a network that can be unreliable,
    clients and servers need to employ a variety of techniques to achieve some semblance
    of reliability. For example, clients may need to implement support for timeouts,
    retries, compensating transactions, client-side load balancing, and so on. Similarly,
    the server may need to protect itself from errant clients by making use of techniques
    such as rate limiters, circuit breakers, bulkheads, fallbacks, health endpoints,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Elaborating on the specific techniques mentioned here is out of scope for this
    book. Books such as *Release It* and *Mastering Non-Functional Requirements* cover
    these patterns in a lot more depth.
  prefs: []
  type: TYPE_NORMAL
- en: In a lot of cases, there is usually a need to employ a combination of several
    of the preceding techniques to provide a resilient solution. Just as we discussed
    in the logging section, mixing these concerns with core business logic can obscure
    the original intent of the problem. In order to avoid this, it is advisable to
    apply these patterns in a manner that is peripheral to core business logic. It
    may also be prudent to consider the use of libraries such as Resilience4j ([https://resilience4j.readme.io/](https://resilience4j.readme.io/))
    or Sentinel ([https://github.com/alibaba/Sentinel](https://github.com/alibaba/Sentinel)).
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Application logging is one of the most fundamental aids when it comes to diagnosing
    issues in running code. In a lot of code bases, logging tends to be an after-thought
    where developers add log statements only after they encounter problems. This results
    in log statements being strewn almost randomly throughout the code base. Here
    is a simple example of code within a command handler to log its execution time
    among other things:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ch12-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is no doubt that this logging code can be invaluable when troubleshooting
    issues. However, when we look at the preceding code, the logging code seems to
    dominate the entire method obscuring the domain logic. This might feel innocuous,
    but when this is done in multiple places, it can get quite repetitive, cumbersome,
    and error-prone – compromising readability. In fact, we have seen cases where
    seemingly innocent log statements have introduced performance issues (for example,
    within a loop with an expensive argument evaluation) or even bugs (for example,
    the dreaded `NullPointerException` when trying to evaluate arguments). In our
    opinion, it is very important to treat logging as a first-class citizen and afford
    it the same rigor as core domain logic. This means that it needs to obey all the
    good practices that we associate with well-factored production code.
  prefs: []
  type: TYPE_NORMAL
- en: Segregating logging code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ideally, we will be able to maintain a balance between readability and debuggability.
    This can be achieved if we can segregate these two concerns. One way to segregate
    this cross-cutting logic is to use aspect-oriented programming (read more about
    AOP at [https://www.eclipse.org/aspectj/](https://www.eclipse.org/aspectj/) and
    [https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop))
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ch12-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A pointcut defines an `around` aspect for all methods annotated with the `@CommandHandler`
    annotation. In this example, we are using compile-time weaving as opposed to runtime
    weaving available through the Spring Framework, to inject execution time logic
    using AspectJ. You can find more details on the pros and cons of using specific
    weaving techniques in this article ([https://www.baeldung.com/spring-aop-vs-aspectj](https://www.baeldung.com/spring-aop-vs-aspectj)).
  prefs: []
  type: TYPE_NORMAL
- en: In the style shown here, we have separated logging code from application code
    through the use of aspect-oriented programming. In the example, the logging code
    applies to all methods annotated with the `@CommandHandler` annotation. This has
    the advantage that all such methods will now produce consistent entry/exit logging
    statements. On the flip side, if there is a need for additional logging for a
    specific command handler, it will still have to be done within the body of that
    method. If you see yourself requiring lots of ad hoc logging statements in addition
    to simple entry/exit logs, it might be a smell and a sign that your methods may
    need to be refactored.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with sensitive data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, when adding logging code, it helps to include as much context as
    possible. This can be challenging in certain domains such as healthcare or finance
    where there may be legal/regulatory requirements to restrict access to sensitive
    information. For example, during the LC application process, we may need to perform
    a credit check for the applicant using their government-issued identifier such
    as the `toString` method of an `SSN` value type can ensure that the sanctity of
    the business need is met uniformly within the bounded context.
  prefs: []
  type: TYPE_NORMAL
- en: While masking may suffice in a majority of use cases, it suffers from the limitation
    of not being able to access the original information even by authorized users.
    If this is a requirement, it may be necessary to make use of **tokenization**
    (the process of replacing a sensitive piece of information with a non-sensitive
    placeholder value called a **token**) solution. This can allow logging tokenized
    values in an unrestricted manner within the bounded context and in general, can
    be a lot more secure. But this can mean having to deal with the additional complexity
    of another bounded context to provide tokenized values and authorization controls
    when the real value needs to be accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Log format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Thus far, we have focused on just the log message. However, logging is more
    than just that. It is typical to include additional information such as the time
    of occurrence, log level, and so on to aid in rapid troubleshooting. For example,
    Spring Boot uses the following log format by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '`2022-06-05 10:57:51.253 INFO 45469 --- [ost-startStop-1] c.p.lc.app.domain.LCApplication
    : Root WebApplicationContext: Ending StartNewLCApplication in 1200495 ns.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is an excellent default, it still is primarily unstructured text
    with certain information being lost in order to improve readability (for example,
    the logger name is abbreviated). While logs are primarily meant to be consumed
    by humans, a large volume of logs can get in the way of being able to locate the
    relevant logs. So it is important to produce logs that are also machine-friendly
    so that they can be easily indexed, searched, filtered, and so on. In other words,
    using a *structured logging* format like the one shown next can go a long way
    toward meeting the goals of both machine and human readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ch12-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Making use of a structured log format elevates their use from being just a debugging
    tool to becoming yet another rich and cheap source to derive actionable business
    insights.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While it may be tempting to pick a custom log format, we strongly recommend
    picking formats that are compatible with popular ones such as Apache’s **Common
    Log Format** (**CLF**) ([https://httpd.apache.org/docs/current/logs.html#common](https://httpd.apache.org/docs/current/logs.html#common))
    or Logstash’s default format ([https://github.com/logfellow/logstash-logback-encoder#standard-fields](https://github.com/logfellow/logstash-logback-encoder#standard-fields)).
  prefs: []
  type: TYPE_NORMAL
- en: Log aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fact that our applications are decomposed into multiple components with
    each usually running multiple instances means that this can produce a lot of logs
    that are disconnected from each other. To be able to work with these logs meaningfully,
    we need to aggregate and sequence them chronologically. It may be worth considering
    the use of a formal log aggregation solution for this purpose. Using a structured
    logging solution as previously discussed can go a long way when working with logs
    from multiple systems.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on logging best practices, please refer to this logging
    cheatsheet from OWASP ([https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Logging_Cheat_Sheet.md](https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Logging_Cheat_Sheet.md))
    and also this article on the art of logging ([https://www.codeproject.com/Articles/42354/The-Art-of-Logging](https://www.codeproject.com/Articles/42354/The-Art-of-Logging)).
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating logs in one place allows us to view diagnostic information from
    multiple applications. However, we still need to correlate this information when
    in the midst of a flow. Distributed tracing solutions can help with this. Let’s
    look at this next.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine a situation where an applicant submitted an LC application through
    the UI. When all goes well, within a few milliseconds, the applicant should get
    a notification of successful submission as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Submit LC application flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16716_Figure_12.8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Submit LC application flow
  prefs: []
  type: TYPE_NORMAL
- en: Even in this simple example, there are several components involved, each of
    which produces logs of its own. When an engineer is looking to diagnose an issue,
    there is a need to correlate log entries from multiple components. In order to
    accomplish this, there is a need to introduce a correlation identifier as close
    to the start of the interaction and propagate it across component boundaries.
    Furthermore, log entries in each component need to carry this correlation identifier
    as they produce logs. Doing this will allow us to view log entries spanning process
    boundaries using the correlation identifier as a unifying thread. In technical
    terms, the entire flow is called a *trace*, and each segment within the flow is
    called a *span*. This process of instrumenting log entries with such information
    is termed *distributed tracing*.
  prefs: []
  type: TYPE_NORMAL
- en: As is evident here, user flows may – and usually do – span more than one bounded
    context. For this to work effectively, bounded contexts need to agree on propagating
    trace and span identifiers uniformly. Tools such as Spring Cloud Sleuth and OpenTracing
    can help simplify implementation for teams using disparate technology stacks.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, distributed tracing visualizations can aid in diagnosing performance
    bottlenecks and chattiness between components. But what may not be obvious is
    the insights it can provide in gaining a richer understanding of how components
    interact in an end-to-end user journey. In a lot of ways, this can be thought
    of as a near real-time context map visualization of your system, and how components
    are coupled with each other. From a DDD perspective, this can provide greater
    insights into re-evaluating bounded context boundaries if necessary. For this
    reason, we strongly recommend making it easy to set up and configure distributed
    tracing apparatus painlessly right from the outset.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are working with a monolithic application, we have large portions bundled
    as a single cohesive unit. This means that other than third-party dependencies,
    we don’t have to worry about explicitly versioning our own components. However,
    when we start breaking components into their individual deployable units, there
    is a need to pay careful attention to how the components, APIs, and data elements
    of our solution are versioned. Let’s look at each in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we create components, there are two broad categories – those that are **deployed**
    on their own and those that are **embedded** within another component. In the
    case of deployable components, there is a need to use an explicit version to identify
    specific instances of the component, even if only for deployment purposes. In
    the case of the embedded component, again there is a need to use an explicit version
    because other components need to understand what instance they depend upon. In
    other words, *all components* need to have a version to uniquely identify themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'It follows that we then need to choose a sound versioning strategy for our
    components. We recommend the use of semantic versioning ([https://semver.org/](https://semver.org/)),
    which uses a version identifier that uses three numeric components that match
    the **MAJOR.MINOR.PATCH** scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MAJOR**: Increment when you make backward-incompatible changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MINOR**: Increment when you add functionality in a backward-compatible manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PATCH**: Increment when you make backward-compatible bug fixes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we can make use of optional extensions to indicate pre-release
    and build metadata. For example, the version identifier for our component might
    read 3.4.1-RC1 to reflect that this is a release candidate for version 3.4.1 of
    our component. Using a standard versioning scheme enables the use of build tools
    such as Maven and Gradle to declare fine-grained upgrade rules and constraints
    for direct and transitive dependencies. A good practice here is to declare dependencies
    without versions and make use of dependency management ([https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Management](https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Management))
    or dependency constraints (https://docs.gradle.org/current/userguide/dependency_constraints.html#sec:adding-constraints-transitive-deps)
    to centralize version management of dependent components.
  prefs: []
  type: TYPE_NORMAL
- en: APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As producers, we expose APIs in a number of ways. In this case, we are specifically
    referring to APIs made available over remote interfaces such as HTTP, events,
    and so on. When it comes to APIs, first and foremost, it is important to keep
    consuming applications functionally. One effective way of making this possible
    is by thinking from the consumer’s standpoint and embracing consumer-driven contracts
    ([https://martinfowler.com/articles/consumerDrivenContracts.html](https://martinfowler.com/articles/consumerDrivenContracts.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'From a consumer’s perspective, the robustness principle (Postel’s law) applies:
    *be conservative in what you send, be liberal in what you accept*. In other words,
    when sending requests to providers, strictly obey the constraints laid down by
    the producer. For example, don’t send unexpected data in the request. Whereas,
    when receiving responses, be tolerant towards what you get from the producer.
    For example, ignore unknown attributes in the response as long as all the attributes
    you expect are present. This will allow producers to evolve without breaking existing
    consumers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our recommendation is to keep APIs versionless for as long as possible by continuing
    to maintain backward compatibility. Despite all our efforts, there may come a
    need to make breaking changes to our APIs. Breaking changes include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing/renaming one or more attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the type of one or more existing attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the format of the request/response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In such cases, make use of a version identifier to indicate major version changes
    (for example, v2 to v3). Common options include specifying the version in the
    URI, in a header, or in the payload. But as we have mentioned earlier, API versioning
    needs to be used sparingly. If you find yourself in a situation where you are
    required to introduce backward-incompatible changes frequently, it might be an
    indicator of requirements being misunderstood and whether DDD principles are truly
    being applied.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a world of well-defined bounded contexts, we should no longer be in a situation
    where we need to expose data directly to our consumers. However, there may be
    situations where we may need to integrate by directly exposing data to our consumers.
    For example, we may have to expose a reporting database for analytical purposes.
    All the good practices that we outlined for APIs apply to data as well.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, from a producer’s perspective, there will be a need to evolve the
    data schema to accommodate changing business requirements. When working with relational
    databases, using good schema migration tools such as Liquibase ([https://liquibase.org/](https://liquibase.org/))
    or Flyway ([https://flywaydb.org/](https://flywaydb.org/)) can go a long way.
    NoSQL databases also have similar tools such as MongoBee ([https://github.com/mongobee/mongobee](https://github.com/mongobee/mongobee))
    and Cassandra-Migration ([https://cassandra.tools/cassandra-migration](https://cassandra.tools/cassandra-migration)).
  prefs: []
  type: TYPE_NORMAL
- en: In this context, it is pertinent to think about data as a product and apply
    product thinking to domain-aligned data. For more information, please refer to
    this article on how to move from a monolithic data lake to a distributed data
    mesh ([https://martinfowler.com/articles/data-monolith-to-mesh.html#DomainDataAsAProduct](https://martinfowler.com/articles/data-monolith-to-mesh.html#DomainDataAsAProduct)).
  prefs: []
  type: TYPE_NORMAL
- en: It is not uncommon to find ourselves in situations where there may be a need
    to support more than one active version of a given component, API, or data. This
    can add significant levels of complexity to the solution. To keep complexity in
    check, it is important to make provisions for deprecating and eventually ending
    support for older versions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at aspects purely beyond functional requirements
    – each of which can have a profound impact on our ability to apply domain-driven
    design effectively. Specifically, we looked at how each of these is interrelated
    and they have to be looked at holistically to achieve and sustain high levels
    of success.
  prefs: []
  type: TYPE_NORMAL
- en: Closing thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Domain-driven design, although conceived in the early 2000s, was way ahead of
    its time. We are in the age of solving the most complex problems yet. Given the
    advancements in technology, there is an expectation to build these solutions a
    lot faster. While the overall cognitive complexity of the solution is directly
    proportional to the complexity of the problem, there is a need to effectively
    manage this complexity. DDD and its principles enable us to achieve this by breaking
    down complex problems into smaller, manageable parts. In this book, we have made
    an attempt to distill our experiences and provide a set of concrete techniques
    to apply DDD in your respective contexts.
  prefs: []
  type: TYPE_NORMAL
