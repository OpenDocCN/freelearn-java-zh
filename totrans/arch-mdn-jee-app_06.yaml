- en: Application Development Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: kIn the previous chapter, we saw the necessity for software companies to move
    fast. This has an impact on the infrastructure and runtime environments and on
    the way teams of engineers are working together. The motivations behind modern
    environments are scalability, flexibility and minimizing time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: Development workflows are even more important than the infrastructure alone.
    The whole process of writing source code until the running application is in production
    should be specified in a reasonable and productive way. Again, moving fast in
    a fast-moving world implies that these processes run automated and reliably with
    as little human intervention as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The motivations and necessity of Continuous Delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The contents of a productive pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to automate all steps involved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to sustainably ensure and improve software quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required team culture and habits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation and goals of productive development workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Moving fast in terms of development workflows aims to enable fast feedback by
    fast turnarounds. In order to increase productivity, developers who work on the
    application's behavior need to verify the implemented features and bug fixes in
    a timely manner. This includes the time spent on builds, software tests, and deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The key to productive workflows is automation. Software engineers should spend
    as much time as possible on designing, implementing, and discussing business logic
    and as little as possible on cross-cutting concerns and repetitive tasks. Computers
    are designed for quickly and reliably performing deterministic, straightforward
    tasks. Humans, however, are better at designing, thinking, and brainstorming creative
    and complex tasks. Simple, straightforward processes that don't require a lot
    of decision-making should therefore be performed by software.
  prefs: []
  type: TYPE_NORMAL
- en: Build systems are a good start. They automate compiling, resolving dependencies,
    and packaging of software projects. Continuous Integration servers take this approach
    further. They orchestrate the whole development workflow from building artifacts
    to automated testing and deployments. Continuous Integration servers are the *golden
    source of truth* of software delivery. They continuously integrate the work of
    all developers in a central place, making sure the project is in a shippable state.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery continues the approach of Continuous Integration by automatically
    shipping the built software to certain environments on each build. Since software
    changes have to be verified properly before they go to production, applications
    are first deployed to test and staging environments. All deployment actions have
    to make sure the environment is prepared and configured and rolled out properly.
    Automated and manual end-to-end tests make sure the software works as expected.
    Deployment to production is then done in a *half-automated* way by triggering
    the automated deployment manually.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Continuous Delivery and Continuous Deployment is that
    the latter automatically deploys each committed software version to production,
    if the quality requirements are met, of course.
  prefs: []
  type: TYPE_NORMAL
- en: All these approaches minimize the developer intervention required, minimize
    turnaround times, and improve productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the Continuous Delivery approach supports not only rollouts but also
    reliable rollbacks. Software versions, although verified before, sometimes need
    to be rolled back for some reason. In such a situation, there is either the way
    of rolling forward, for example, by committing a new version that will undo the
    recent changes, or by rolling back to the working state.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, software should be built in a reliable way. All versions
    of used technology, such as build dependencies or application servers, are specified
    explicitly. Rebuilt applications and containers produce the same result. In the
    same way, pipeline steps of development workflows should result in the same outcome.
    It is crucial that the same application artifact that has been verified in test
    environments is deployed to production later on. Later in this chapter, we cover
    how to achieve reproducible, repeatable, and independent builds.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of reliability, automated processes are an important aspect as well.
    Especially, deployments that are executed by software rather than human intervention
    are far less prone to error. All necessary pipeline steps are well defined and
    implicitly verified each and every time they are executed. This builds confidence
    into the automated processes, ultimately more than executing processes manually.
  prefs: []
  type: TYPE_NORMAL
- en: Verification and testing are important prerequisites of Continuous Delivery.
    Experience shows that the vast majority of software tests can be executed in an
    automated way. The next chapter will cover this topic in depth. Besides testing,
    quality assurance also covers the software quality of the project in regard to
    architecture and code quality.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery workflows include all steps necessary in order to build,
    test, ship, and deploy software in a productive and automated way. Let's see how
    to build effective workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Realizing development workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous Delivery pipelines consist of several pipeline build steps that are
    executed in sequence or in parallel, respectively. All the steps are executed
    as part of a single build. Builds are usually triggered by committing or rather
    pushing code changes into version control.
  prefs: []
  type: TYPE_NORMAL
- en: The following examines the aspects of a Continuous Delivery pipeline. These
    general steps are indifferent to the used technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a high-level overview of a simplified Continuous
    Delivery pipeline. The steps are executed in a Continuous Integration server and
    use external repositories such as version control, artifact, and container repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b958f901-28cc-4ee0-bbd3-aa877f33d055.png)'
  prefs: []
  type: TYPE_IMG
- en: Version control everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developers agree that source code should be kept under version control. Distributed
    version controls such as Git have been widely accepted as state-of-the-art tools.
    However, as mentioned earlier, besides application source code, there are more
    assets to track.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind infrastructure as code is to keep all artifacts needed
    to ship the application in one central place. All changes made to the application,
    configuration, or environment are represented as code and checked in to the repository.
    Infrastructure as code leverages reproduciblity and automation. Taking this approach
    further also includes the definition of Continuous Delivery pipelines as code.
    The *Pipeline as code* section will cover this approach with the widely used Jenkins
    server as an example.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in the previous chapter, the first principle of 12-factor applications
    is in fact to keep all files and artifacts needed to build and run the application
    in one repository.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of the Continuous Delivery pipeline is to check out a specific
    commit from the version control repository. Teams that use distributed version
    control systems need to synchronize the desired state to a centralized repository.
    The Continuous Integration server takes the state of a specific commit in history
    to start the build process.
  prefs: []
  type: TYPE_NORMAL
- en: The reason behind taking a specific commit version rather than just the latest
    state is to enable reproducibility. Rebuilding the same build version can only
    reliably result in the same outcome if the build is based on a specific commit.
    This is only possible if the build originated from checking-in to version control
    with a particular commit. Check-in actions usually trigger builds from the corresponding
    commit version.
  prefs: []
  type: TYPE_NORMAL
- en: Checking out the state of the repository provides all sources and files necessary.
    The next step is to build the software artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Building binaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the first chapter, the term binaries includes all executable
    artifacts that run the enterprise application. The project repository only contains
    source code and files and artifacts required by the infrastructure. The binaries
    are built by the Continuous Integration server.
  prefs: []
  type: TYPE_NORMAL
- en: A step in the pipeline is responsible for building these binaries and making
    them accessible in a reliable way.
  prefs: []
  type: TYPE_NORMAL
- en: Java artifacts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Java EE, binaries first of all include the packaged enterprise application
    in form of an archive. Following the approach of zero-dependency applications
    results in building and packaging the project into a thin WAR file, containing
    only the application's business logic. This build action includes to resolve required
    dependencies, compile Java sources, and package the binary classes and other files
    into the archive. The WAR files are the first produced artifact within the build
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The application artifacts are built using build systems such as Maven or Gradle,
    which are installed and executed on the CI server. Usually, the project build
    already executes basic code level tests. Tests that are executed on code level
    without requiring a container runtime can verify the behavior of classes and components
    early in the pipeline. The Continuous Delivery approach of failing fast and breaking
    the build as early as possible minimizes turnaround times.
  prefs: []
  type: TYPE_NORMAL
- en: If required, build systems can publish the artifacts onto an artifact repository.
    Artifact repositories, such as **Sonatype Nexus** or **JFrog Artifactory**, save
    the built artifact versions for later retrieval. However, if the application is
    shipped in Linux containers, the artifact doesn't necessarily have to be deployed
    onto a repository.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Chapter 2, *Designing and Structuring Java Enterprise Applications*,
    a Java project is built using Maven via the command `mvn package`. The package
    phase compiles all Java production sources, compiles and executes the test sources,
    and packages the application in our case, to a WAR file. The CI server executes
    a build system command similar to this to build the artifact in its local workspace
    directory. The artifact can be deployed to an artifact repository, for example,
    using the `mvn deploy` command, to be used in subsequent steps; or it will be
    taken directly from the workspace directory.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, the build systems need to produce artifacts in a reliable
    way. This requires that Java artifacts are built and archived with a distinct
    version, which is identifiable later on. Software tests verify specific versions
    of enterprise applications. Later deployments need to refer the identical versions
    in later build steps as well. Being able to identify and refer to distinct artifact
    versions is necessary. This is true for all binaries.
  prefs: []
  type: TYPE_NORMAL
- en: One of the 12-factor principles is to explicitly declare dependencies, not only
    for dependencies being used but also in regard to their versions. As mentioned
    earlier, the same holds true for container builds. Specified Docker base images
    as well as installed software should be explicitly, uniquely identified by their
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite common, however, to specify Java builds as *snapshot* versions,
    for example, `0.1-SNAPSHOT`. A snapshot, as opposed to a release version, represents
    a software state which is currently being developed. Dependency resolution always
    attempts to include the latest snapshot when several snapshot versions are existent,
    comparable to the Docker `latest` tag. The workflow behind snapshots is to release
    the snapshot version to a uniquely numbered version, once the level of development
    is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: However, snapshot versioning contradicts the idea of Continuous Delivery. In
    CD pipelines every commit is a potential candidate for production deployment.
    Snapshot versions are naturally not meant to be deployed on production. This implies
    that the workflow would need to change the snapshot to a release version, once
    the software version has been sufficiently verified. However, once built, Java
    artifacts are not meant to be changed. The same artifact that has been verified
    should be used for deployment. Therefore, snapshot versions do not fit Continuous
    Delivery pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Following the widely adopted approach of **semantic versioning**, application
    developers need to take care of their versions in regard to backward-compatibility.
    A semantic versioning describes software versions such as `1.1.0`, `1.0.0-beta`,
    or `1.0.1+b102`. In order to represent versions that are both eligible for Continuous
    Delivery and provide semantic versioning metadata, properly numbered versions
    with unique build metadata are a good solution. An example is `1.0.1+b102`, for
    *major* version `1`, *minor* version `0`, *patch* version `1`, and build number
    `102`. The part after the plus sign represents the optional build metadata. Even
    if the semantic version was not changed in between a number of builds, the produced
    artifacts are still identifiable. The artifacts can be published to an artifact
    repository and retrieved via these version numbers later on.
  prefs: []
  type: TYPE_NORMAL
- en: This versioning approach targets enterprise application projects rather than
    products. Products which have multiple shipped and supported versions at a time,
    require to have more complex versioning workflows.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, there isn't a de facto standard for versioning containers
    yet. Some companies follow a semantic versioning approach whereas others exclusively
    use CI server build numbers or commit hashes. All of these approaches are valid,
    as long as container images aren't rebuilt or distributed using the same tag twice.
    A single build must result in a distinct container image version.
  prefs: []
  type: TYPE_NORMAL
- en: Building containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Container images also represent binaries, since they contain the running application,
    including runtime and operating system binaries. In order to build container images,
    base images and all artifacts that are added at build time need to be present.
    If they don't already exist on the build environment, base images are retrieved
    implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: For each build step defined in the Dockerfile, an image layer is added on top
    of the previous layer. Last but not least, the application that was built just
    before is added to the container image build. As shown previously, Java EE application
    containers consist of an installed and configured application server that auto-deploys
    the web archive at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: This image build is orchestrated by the CI server as part of the pipeline. One
    solution is to have the Docker runtime installed, in the same way as the Maven
    build system. The pipeline step then simply invokes an image build similar to
    `docker build -t docker.example.com/hello-cloud:1 .` in the job workspace directory.
    The Docker image build, for example, takes the WAR file under Maven's `target`
    directory and adds it into the container.
  prefs: []
  type: TYPE_NORMAL
- en: The built image is tagged with an image name and unique tag, depending on the
    build number or some other unique information. Docker image names imply the registry
    they will be pushed to. An image identifier such as `docker.example.com/hello-cloud:1`
    will implicitly be transmitted from and to the host `docker.example.com`. The
    pipeline pushes the image to the Docker registry in most cases, a company-specific
    registry.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the company's workflow, Docker images can be re-tagged as part
    of the pipeline as well. For example, special tags such as the `latest` tag can
    refer to the actual *latest* built versions and so on. This is accomplished by
    explicitly re-tagging the image, so that two identifiers point to the same image.
    Unlike Java archives, Docker images can be re-tagged without changing their contents.
    The second tag needs to be pushed to the repository, as well. However, the rest
    of this chapter will show you that it's not required to refer to images using
    *latest* versions, such as the Docker `latest` tag. In fact, similar to snapshot
    versioning it's advisable to avoid *latest* versions. Being explicit in all artifact
    versions is less prone to error.
  prefs: []
  type: TYPE_NORMAL
- en: Some engineers argue that running Docker builds inside the CI server may not
    be the best idea if the CI server itself runs as a Docker container. Docker image
    builds start temporarily running containers. It is certainly possible to either
    run containers in a container or connect the runtime to another Docker host, without
    opening the whole platform to potential security concerns. However, some companies
    choose to build the images outside of the CI server instead. For example, OpenShift,
    a PaaS built on top of Kubernetes, provides build functionality that comprises
    a CI server as well as image builds. It is therefore possible to orchestrate image
    builds from the CI server which are then built in the OpenShift platform. This
    provides an alternative to building container images directly on the CI server.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Java artifact build already performs some basic quality assurance. It executes
    included code level tests, such as unit tests. A reasonable pipeline consists
    of several test scopes and scenarios, all with slightly different strengths and
    weaknesses. The included unit tests operate at code level and can be executed
    without any further running environment. They aim to verify the behavior of the
    individual classes and components and provide fast feedback in case of test failures.
    We will see in the next chapter that unit tests need to run self-sufficiently
    and fast.
  prefs: []
  type: TYPE_NORMAL
- en: Test results are usually recorded from the CI server for visibility and monitoring
    reasons. Making the outcome of the pipeline steps visible is an important aspect
    of Continuous Delivery. The CI server can track the number of passed unit tests
    and show trends over time.
  prefs: []
  type: TYPE_NORMAL
- en: There are build system plugins available that track the code coverage of the
    executed tests. The coverage shows which parts of the code base have been executed
    during the test runs. Generally speaking, a greater code coverage is desirable.
    However, a high percentage of coverage alone tells nothing about the quality of
    tests and coverage of test assertions. The test results, together with their coverage,
    are just one of a few quality characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Source code can already provide a lot of information about the software's quality.
    So-called **static code analysis** performs certain quality checks on the static
    source code files of the project without executing them. This analysis gathers
    information about code statements, class and method sizes, dependencies between
    classes and packages, and complexity of methods. Static code analysis can already
    find potential errors in the source code, such as resources that are not properly
    closed.
  prefs: []
  type: TYPE_NORMAL
- en: '**SonarQube** is one of the most well-known code quality tools. It provides
    information about the quality of software projects by correlating the results
    of different analysis methods, such as static code analysis or test coverage.
    The merged information is used to provide helpful quality metrics for software
    engineers and architects. For example, which methods are complex but at the same
    time sufficiently tested? Which components and classes are the biggest in size
    and complexity and therefore candidates to be refactored? Which packages have
    cyclic dependencies and likely contain components that should be merged together?
    How does the test coverage evolve over time? How many code analysis warnings and
    errors are there and how does this number evolve over time?'
  prefs: []
  type: TYPE_NORMAL
- en: It's advisable to follow some basic guidelines regarding static code analysis.
    Some metrics just give insights in terms of rough ideas about the software quality.
    Test coverage is such an example. A project with high coverage does not necessarily
    imply well-tested software; the assertion statements could be impractical or insufficient.
    However, the trend of test coverage does give an idea about the quality, for example,
    whether software tests are added for new and existing functionality and bug fixes.
  prefs: []
  type: TYPE_NORMAL
- en: There are also metrics that should be strictly followed. Code analysis warnings
    and errors are one of these. Warnings and errors tell engineers about code style
    and quality violations. They are indicators about issues that need to be fixed.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, there should be no such things as compilation or analysis warnings.
    Either the build passes the quality checks sufficiently, a *green traffic light*;
    or the quality is not sufficient for deployment, a *red traffic light*. There
    is nothing reasonable in between. Software teams need to clarify which issues
    are plausible and to be resolved and which aren't. Warnings that indicate minor
    issues in the project therefore are treated as errors; if there is a good reason
    to resolve them, then the engineers have to, otherwise the build should fail.
    If the detected error or warning represents a *false positive*, it won't be resolved;
    instead, it has to be ignored by the process. In that case, the build is successful.
  prefs: []
  type: TYPE_NORMAL
- en: Following this approach enables a **zero-warning policy**. Project builds and
    analyses that contain a lot of errors and warnings all the time, even if they
    are not critical, introduce certain issues. The existing warnings and errors obfuscate
    the quality view of the project. Engineers won't be able to tell on the first
    look whether the hundreds of issues are actually issues or not. Besides that,
    having a lot of issues already demotivates engineers to fix newly introduced warnings
    at all. For example, imagine a house that is in a terrible condition, with damaged
    walls and broken windows. Nobody would care if another window gets broken or not.
    But a recently broken window of an otherwise pristine house that has been taken
    good care of urges the person in charge to take action. The same is true for software
    quality checks. If there are hundreds of warnings already, nobody cares about
    that last commit's newly introduced violation. Therefore, the number of project
    quality violation should be zero. Errors in builds or code analyses should break
    the pipeline build. Either the project code needs to be fixed or the quality rules
    need to be adjusted for the issue to be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Code quality tools such as SonarQube are integrated in a build pipeline step.
    Since the quality analysis operates on static input only, the step can easily
    be parallelized to the next pipeline steps. If the quality gate does not accept
    the result, the build will fail and the engineers need resolve the issue before
    continuing development. This is an important aspect to integrate quality into
    the pipeline. The analysis should not only give insights but also actively prevent
    the execution to force action.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the binaries have been built and after, or during, the software quality
    is being verified, the enterprise application will be deployed. There are usually
    several environments for testing purposes, depending on the project circumstances,
    such as test or staging and, of course, production. As mentioned earlier, these
    environments should be as similar as possible. This vastly simplifies the deployment
    process orchestrated by the CI server.
  prefs: []
  type: TYPE_NORMAL
- en: The process of deploying the application generally takes the binaries in the
    version that has just been built and deploys them onto the environment. Depending
    on what the infrastructure looks like, this can take place using plain scripts
    or more sophisticated technology. The principle should be the same, the binaries
    as well as the configuration are made available to the environment in an automated
    and reliable way. Preparation steps that are potentially required by the application
    or the environment will be executed in this step as well.
  prefs: []
  type: TYPE_NORMAL
- en: Modern environments such as container orchestration frameworks support infrastructure
    as code. Infrastructure configuration is captured in files in the project's repository
    and applied to all environments at deployment time. Potential differences, such
    as Kubernetes config maps contents, are represented as different manifestations
    in the repository as well.
  prefs: []
  type: TYPE_NORMAL
- en: Using IaC as well as containers provides even more reliability than home-grown
    shell scripts. The application should always be rolled out in an idempotent way,
    independent of which state the environment was in. Since container images contain
    the whole stack, the outcome is the same as if the software was installed from
    scratch. Required environment configuration is applied from IaC files as well.
  prefs: []
  type: TYPE_NORMAL
- en: New container image versions can be deployed by orchestration frameworks in
    many ways. There are certain commands that explicitly set Docker images used in
    Kubernetes deployments. However, in order to fulfill the requirement of reliability
    and reproducibility, it makes sense to only edit the infrastructure as code files
    and apply them on the cluster. This ensures that the configuration files stay
    the single source of truth. The CI server can edit the image definitions in the
    IaC files and commit the changes to the VCS repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the previous chapter, Docker images are specified in Kubernetes
    deployment definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These image definitions are updated within the CI server process and applied
    to the Kubernetes cluster. The CI server executes Kubernetes commands via the
    `kubectl` CLI. This is the standard way to communicate with Kubernetes clusters.
    `kubectl apply -f <file>` applies the infrastructure as code contents of a file
    or directory containing YAML or JSON definitions. The pipeline step executes a
    command similar to this, providing the updated Kubernetes files which were updated
    in the project repository.
  prefs: []
  type: TYPE_NORMAL
- en: Following this approach enables that infrastructure as code files both contain
    the current state of the environments as well as changes made by engineers. All
    updates are rolled out by applying the Kubernetes files in the corresponding version
    to the cluster. The cluster aims to satisfy the new desired state, containing
    the new image version, and will therefore perform a rolling update. After triggering
    this the update, the CI server validates whether the deployment has been executed
    successfully. Kubernetes rollout actions can be followed by commands similar to
    `kubectl rollout status <deployment>`, which waits until the deployment is either
    rolled out successfully, or failed.
  prefs: []
  type: TYPE_NORMAL
- en: This procedure is executed on all environments. If single deployment definitions
    are used for several environments, the image tag definition only has to be updated
    once, of course.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give a more concrete example, the following shows a potential configuration
    file structure of a Maven project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa21a40d-2b34-45f4-a5c2-4bfa4b5e9c86.png)'
  prefs: []
  type: TYPE_IMG
- en: The `hello-cloud.yaml` file contains multiple Kubernetes resource definitions.
    This is possible by separating each YAML object definitions with a three-dashed
    line (`---`). It's equally doable to provide separate files for each resource
    type, such as `deployment.yaml`, `service.yaml`, and so on. Kubernetes can handle
    both approaches. The `kind` type definitions in the YAML objects indicate the
    type of the resource.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter showed how container orchestration frameworks enable zero-downtime
    deployments out of the box. Applying new image versions to the environments orchestrated
    by the CI server also accomplishes this goal. The environments will therefore
    be able to serve traffic with at least one active application at a time. This
    approach is especially important for production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, infrastructure as code covers all aspects required to define the whole
    environment, including runtimes, networking, and configuration. Using container
    technologies and container orchestration greatly supports and simplifies this
    approach. As mentioned earlier, confidential content such as credentials should
    not be put under version control. This should be configured manually on the environment
    by an administrator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration that differs in several environments can be represented using
    multiple files in the project repository. For example, it makes sense to include
    subfolders for each environment. The following image shows an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90519667-dfc0-4342-9896-3edeb17e8f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The contents of the `configmap.yaml` file include the specific config map contents
    as well as potentially different namespace definitions. As mentioned in the previous
    chapter, Kubernetes namespaces are a way to differentiate environments. The following
    code shows an example of a specific production config map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Credentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to security reasons, secret content such as credentials is typically not
    included in the project repository. An administrator usually configures them manually
    on the specific environments. Similar to other Kubernetes resources, secrets are
    bound to a specific namespace.
  prefs: []
  type: TYPE_NORMAL
- en: If a project requires multiple secrets, for example, specific credentials for
    various external systems, manually configuring them can become cumbersome and
    difficult to keep track of. Configured secrets have to be documented and tracked
    in a secure form, external to the project repository.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to store encrypted credentials that can be decrypted using
    a single master key in the repository. The repository can therefore safely contain
    the configured credentials, in encrypted form, and still be safe from disclosing
    the secrets. The running application will use the dynamically provided master
    key to decrypt the configured credentials. This approach provides security as
    well as manageability.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a potential solution. Encrypted configuration values can safely
    be stored in Kubernetes config maps, since the decrypted values will only be visible
    to the container process. The project can define the encrypted credentials together
    with other configuration values in the config maps definitions as code. An administrator
    adds a secret to each environment, containing the master key which was used to
    symmetrically encrypt the credentials. This master key is provided to the running
    container, for example, using environment variables as seen earlier. The running
    application uses this single environment variable to decrypt all encrypted credential
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the used technology and algorithm, one solution is to use the Java
    EE application to decrypt the credentials directly when loading properties files.
    To provide a secure solution using recent encryption algorithms, the **Java Cryptographic
    Extensions** (**JCE**) should be installed in the runtime. Another approach is
    to decrypt the values before the application is being deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Data migration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications that use a database to store their state are bound to a specific
    database schema. Changes in the schema usually require the application model to
    change and vice versa. With an application being actively developed and a domain
    model being continuously refined and refactored, the model will eventually require
    the database schema to change. New model classes or properties thereof which are
    added need to be persisted in the database as well. Classes and properties that
    are refactored or removed should be migrated in the database also, so that the
    schema doesn't diverge.
  prefs: []
  type: TYPE_NORMAL
- en: However, data migrations are more difficult than code changes. Stateless applications
    can simply be replaced by new versions thereof, containing the new functionality.
    A database that contains the application's state, however, needs to carefully
    migrate the state when the schema changes.
  prefs: []
  type: TYPE_NORMAL
- en: This happens in migration scripts. Relational databases support altering their
    tables while keeping the data intact. These scripts are executed before the new
    version of the software is deployed, making sure the database schema matches the
    application.
  prefs: []
  type: TYPE_NORMAL
- en: There is an important aspect to keep in mind when deploying applications using
    a zero-downtime approach. Rolling updates will leave at least one active instance
    running in the environment at a time. This results in having both the old and
    the new software version active for a short period of time. The orchestration
    should take care that the applications are gracefully started and shut down, respectively,
    letting in-flight requests finish their work. Applications that connect to a central
    database instance will result in several versions of the application simultaneously
    accessing the database. This requires the application to support so-called **N-1
    compatibility**. The current application version needs to function with the same
    database schema version plus and minus one version, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To support N-1 compatibility, the rolling update approach needs to both deploy
    a new application version and to updates the database schema, making sure the
    versions do not differ more than one version. This implies that, the corresponding
    database migrations are executed just before the application deployment takes
    place. The database schema, as well as the application, therefore evolves in small
    migration steps, not in jumps.
  prefs: []
  type: TYPE_NORMAL
- en: This approach, however, is not trivial and involves certain planning and caution.
    Especially, application version rollbacks require particular attention.
  prefs: []
  type: TYPE_NORMAL
- en: Adding database structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding tables or table columns to a database schema is comparatively straightforward.
    The new table or column does not collide with older application versions, since
    they are unknown to them.
  prefs: []
  type: TYPE_NORMAL
- en: New tables that resulted from new domain entities can simply be added to the
    schema, resulting in version *N+1*.
  prefs: []
  type: TYPE_NORMAL
- en: New table columns that define certain constraints, such as *not null* or *unique*,
    need to take care of the current state of the table. The old application version
    can still write to the table; it will ignore the new column. Therefore, constraints
    can not necessarily be satisfied. New columns first need to be *nullable* and
    without further constraints. The new application version has to deal with empty
    values in that column, presumably `null` values, which originate from the old
    application version.
  prefs: []
  type: TYPE_NORMAL
- en: Only the next version (*N+2*) will then, after the current deployment has been
    completed, contain the correct constraints. This means that adding a column that
    defines constraints needs at least two separate deployments. The first deployment
    adds the column and enhances the application's model in a `null`-safe way. The
    second deployment makes sure all contained values fulfill the column constraints,
    adds the constraints, and removes the `null`-safe behavior. These steps are, of
    course, only required, if the column target state defines constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Rollbacks to the old versions work in a similar way. Rolling back to the intermediate
    deployment (*N+2* to *N+1*) requires the constraints to be removed again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling back to the original state (*N+0*) would remove the whole column. However,
    data migrations should not remove data that is not transferred somewhere else.
    Rolling back to the state without the column could also simply leave the column
    untouched so as not to lose data. The question the business experts have to answer
    is: What happens with the data that was added in the meantime? Intentionally not
    deleting this data could be a reasonable approach. However, when the column is
    added again, the rollout script needs to take already existing columns into consideration.'
  prefs: []
  type: TYPE_NORMAL
- en: Changing database structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Changing existing database tables or columns is more complex. Whether columns
    are renamed or changed in type or constraint, the transitions have to be executed
    in several steps. Directly renaming or changing columns would lead to incompatibilities
    with the deployed application instances; changes require intermediate columns.
  prefs: []
  type: TYPE_NORMAL
- en: Let's examine this approach using an example. Assume the car entity has a property
    *color*, which must be set, represented in the database column `color`. Assuming
    it will be refactored to the name *chassis color* or `chassis_color` in the database
    column.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous approach, the change is executed in several deployments.
    The first deployment adds a nullable column `chassis_color`. The application code
    is enhanced to use the new model property. Since the older application version
    doesn't know about the property yet, it is not reliably written from all places
    during the first deployment. Therefore, the first code version still reads the
    color from the old, `color` column, but writes values to both the old and new
    column.
  prefs: []
  type: TYPE_NORMAL
- en: The migration script on the next deployment updates the missing column values
    by overwriting the `chassis_color` column with the `color` column contents. By
    doing this, it is ensured that the new column is populated consistently. The not
    null constraint is added to the new column as well. The application code version
    will then only read from the new, but still write to both, because of the short
    period when the older version is still active.
  prefs: []
  type: TYPE_NORMAL
- en: The next deployment step removes the not null constraint from the `color` column.
    The application code of this version doesn't use the old column anymore, and both
    reads and writes to `chassis_color`.
  prefs: []
  type: TYPE_NORMAL
- en: The next and final deployment then drops the `color` column. Now all data has
    been gradually transferred to the new `chassis_color` column. The application
    code doesn't include the old model property anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Changing column types or foreign key constraints require similar steps. The
    only way to gradually migrate databases with zero-downtime is to migrate in small
    steps, using intermediate columns and properties. It is advisable to perform several
    commits that only contain these changes to both the migration scripts and application
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous approach, rollback migrations have to be executed in
    reverse, for both the database scripts and code changes.
  prefs: []
  type: TYPE_NORMAL
- en: Removing database structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Removing tables or columns is more straightforward than changing them. Once
    certain properties of the domain model are not required anymore, their usages
    can be removed from the application.
  prefs: []
  type: TYPE_NORMAL
- en: The first deployment changes the application code to stop reading from the database
    column but still to write to it. This is required to ensure that the old version
    can still read values other than `null`.
  prefs: []
  type: TYPE_NORMAL
- en: The next deployment will remove a potential not null constraint from the database
    column. The application code stops writing to column. In this step, occurrences
    of the model property can already be removed from the code base.
  prefs: []
  type: TYPE_NORMAL
- en: The final deployment step will drop the column. As mentioned before, it highly
    depends on the business use case whether column data should actually be dropped.
    Rollback scripts would need to recreate removed columns, which implies that the
    previous data is gone.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing migration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, data migrations have to be executed in several steps. Rollout
    as well as rollback scripts are executed right before the deployment. This implies
    that the application supports N-1 compatibility as well as that only one deployment
    is being executed at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The migration process requires to perform several software releases, each of
    them consistent in application code and schema migration scripts. Engineers need
    to plan their commits accordingly. It's advisable to perform the full schema migration
    in a timely manner, to keep the database schema clean and to ensure that ongoing
    migrations aren't simply forgotten about.
  prefs: []
  type: TYPE_NORMAL
- en: It's in the nature of the corresponding model refactoring, whether existing
    data needs to be kept or can be discarded. Generally speaking, it is advisable
    to not throw away data. This means not to drop structures containing data that
    doesn't exist somewhere else.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in the examples, the migrations will be applied in graceful
    steps; especially in regard to database constraints, such as *not null* or referential
    integrity constraints. Migration scripts should be resilient. For example, the
    migration should not fail when trying to create already existing columns. They
    could already exist from previous rollbacks. In general, it makes sense to think
    through and test different rollout and rollback scenarios upfront.
  prefs: []
  type: TYPE_NORMAL
- en: Engineers need to keep the update time in mind when updating table contents.
    Updating huge tables at once will take a non-negligible amount of time in which
    the data is potentially locked. This needs to be considered upfront; ideally,
    by testing the scripts in a separate database. For huge amount of data involved,
    the update steps can be executed in shards, for example, by partitioning the data
    by their IDs.
  prefs: []
  type: TYPE_NORMAL
- en: All rollout and rollback migration scripts should reside in the project repository.
    The database schema comprises a schema version that corresponds to the numbered
    migration scripts. This version is stored in the database as metadata together
    with the current schema state. Before every deployment, the database schema is
    migrated to its desired version. Right after that, the application with a corresponding
    version is deployed, making sure that the versions don't differ by more than one.
  prefs: []
  type: TYPE_NORMAL
- en: In a container orchestration framework this means that the database migration
    needs to be performed right before the new application version is deployed via
    rolling updates. Since there can be many replicas of pods, this process has to
    be idempotent. Executing the migration of a database schema to the same version
    twice, has to result in the same outcome. Kubernetes pods can define so-called
    **init containers** which execute *one-shot* processes before the actual containers
    start. Init containers run mutually exclusive. They have to exit successfully
    before the actual pod container process can be started.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows an example of `initContainer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example implies that the init container image contains the correct
    tooling to connect to the database instance as well as all recent migration scripts.
    In order to make this possible, this image is built as part of the pipeline, as
    well, including all migration scripts from the repository.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, many solutions to migrate database schemas. The important
    aspect here is that the idempotent migration needs to be executed upfront, while
    no second deployment action is being rolled out. The migration scripts of the
    corresponding versions would be executed in ascending or descending order, depending
    on whether the database schema version is upgraded or rolled back, until the version
    matches. After the scripts have been executed, the metadata version is updated
    in the database, as well.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation between code and database versions can be tracked in the project
    repository. For example, the most recent rollout script contained in a commit
    version corresponds to the required database schema. The *Build metadata* section
    covers the topic of required metadata and where to store it in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the chosen migration solution highly depends on the project''s technology,
    there is no *silver bullet* approach that can be shown here. The following example
    gives one possible solution on migration file structure and execution in *pseudo
    code*. It shows migration files for the example of changing the `color` column
    to `chassis_color` discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63d62980-027e-4ce4-a646-eec8916ec559.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding example shows the rollout and rollback scripts that migrate the
    database schema version to the desired state. Rollout script `004_remove_color.sql`
    transposes the schema version to version `4` by removing the `color` column of
    the example shown earlier. The corresponding rollback script `003_add_color.sql`
    rolls back the schema to version `3`, where the `color` column still existed;
    in other words, version `3` contains the `color` column whereas version `4` doesn't,
    with these two migration files being able to roll back and forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the pseudo code of the script that performs the migrations.
    The desired version to migrate to is provided as an argument when invoking the
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This migration script is executed in the init container before the actual deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Verifying the output of the pipeline steps is one of the most important aspects
    in Continuous Delivery. It increases the software quality by detecting potential
    errors before going live. Proper verification creates reliability in the processes.
    By writing software tests in general and regression tests in particular, developers
    become confident in changing and refactoring functionality. Ultimately, software
    tests enable us to automate development processes.
  prefs: []
  type: TYPE_NORMAL
- en: Building binaries already executes code level tests. Other tests contained in
    the project may be executed in separate pipeline steps, depending whether they
    operate at code level or a running container. End-to-end tests, especially, require
    a running environment.
  prefs: []
  type: TYPE_NORMAL
- en: After the application has been deployed on test environments, end-to-end tests
    can be executed there. Usually, a project contains several layers of tests, with
    separate responsibilities, running in separate steps. There can be a great variety
    of tests, depending on the project and used technology. The approach is always
    to execute pipeline steps and sufficiently verify the outcome. By doing so, the
    risk of breaking new or existing functionality and introducing potential errors
    is minimized. Especially, container orchestration frameworks with their *production-ready*
    nature support companies in the goal to ship scalable, highly available enterprise
    applications with high quality. Chapter 7, *Testing*, covers all different manifestations
    of testing, including its execution in Continuous Delivery pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Failing tests will immediately cause the pipeline to stop and will prevent the
    corresponding binary from being used further. This is an important aspect to enable
    fast feedback and also to enforce software quality in the process. Engineers should
    absolutely avoid to bypass steps of the normal process and other *quick fixes*.
    They contradict the idea of continuous improvement and building quality into the
    Continuous Delivery process and ultimately lead to errors. If a test or quality
    gate fails, the build has to break and either the application's code or the verification
    has to change.
  prefs: []
  type: TYPE_NORMAL
- en: Failing tests should not only break the build but also provide insights into
    why the step failed and record the result. This is part of the build's metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Build metadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Build metadata records all information that is gathered during the execution
    of the build. Especially, the specific versions of all assets should be tracked
    for further reference.
  prefs: []
  type: TYPE_NORMAL
- en: Builds that run from the beginning to the end don't necessarily need further
    information. The steps are executed in one run until either the build breaks or
    finishes successfully. If, however, specific steps or artifacts are required to
    be referenced or re-executed, further information is required.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact versions are the prime example for this necessity. A WAR file and its
    contents corresponds to a specific version in the VCS commit history. In order
    to track the originating commit from a deployed application, this information
    needs to be tracked somewhere. The same is true for container image versions.
    In order to identify the origin and contents of a container, the versions need
    to be traceable. Database schema versions are another example. A database schema
    version matches a specific application version, including the previous and the
    next version, by following N-1 compatibility. A deployment that migrates the database
    schema needs to know the schema version to migrate to for the desired application
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build metadata is especially required when the process enables rolling out
    specific application versions. In general, Continuous Delivery deployments roll
    forward to the current repository version. However, especially with database schemas
    and migrations involved, the possibility of rolling the environments to an arbitrary
    state is a huge benefit. The process in theory works like this: *take this specific
    application version and perform everything required in order to run it on this
    specific environment*, no matter whether the rollout is moving forward or backward.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to improve traceability and reproducibility, it is advisable to track
    quality information about the build as well. This includes, for example, results
    of automated tests, manual tests, or code quality analyses. The deployment steps
    then are able to verify the existence of specific metadata before deploying.
  prefs: []
  type: TYPE_NORMAL
- en: There are many solutions possible for representing metadata. Some artifact repositories
    such as JFrog Artifactory provide the possibility of linking built artifacts with
    custom metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to use the CI server to track this information. This sounds
    like a good fit to store metadata for a build; however, depending on how the CI
    server is operated and set up, it is not necessarily advisable to use it to store
    persistent data. Old builds can be discarded and lose information.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the number of *points of truth*, for example, to store artifacts
    and information, should be kept low and explicitly defined. Using artifact repositories
    for metadata therefore certainly makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Another, more custom solution, is to use company VCS repositories to track certain
    information. The big benefit of using, for example, Git to store metadata is that
    it provides full flexibility of the data and structure being persisted. CI servers
    already contain functionality to access VCS repositories, therefore no vendor-specific
    tooling is required. Repositories can store all kind of information that are persisted
    as files, such as recorded test result.
  prefs: []
  type: TYPE_NORMAL
- en: The metadata repository, however implemented, is accessed at various points
    in the pipeline, for example, when performing deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Going to production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last step in the Continuous Delivery pipeline is deploying to production.
    This deployment is either triggered manually or, when sufficient verification
    and automated tests are implemented, automatically. The vast majority of companies
    use a manually triggered deployment. But even if the pipeline does not go *all
    the way* from the beginning, Continuous Delivery provides great benefits by automating
    all steps necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline then only has two kick-off spots: the initial commit to the repository
    that triggers the execution, and the final deployment to production after all
    steps have been verified, manually and automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: In a container orchestration environment, deploying to production, that is,
    either deploying to a separate namespace or a separate cluster, happens in the
    same way as deploying to test environments. Since the infrastructure as code definitions
    are similar or ideally identical to the ones executed before, this technology
    lowers the risk of environment mismatches to production.
  prefs: []
  type: TYPE_NORMAL
- en: Branching models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software development processes can make use of different branching models. Software
    branches emerge from the same origin and differ in the state of development to
    make it possible to develop on multiple development stages in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Especially feature branches are a popular approach. Feature branching creates
    a separate branch which is used to develop a certain software feature. The branch
    is merged into the *master* branch or *trunk* after the feature is finished. The
    master branch and other branches remain untouched while the feature is being developed.
  prefs: []
  type: TYPE_NORMAL
- en: Another branching model is to use release branches. Release branches contain
    single software releases of a specific version. The idea is to have a dedicated
    point for a released version where bug fixes and features can be added. All changes
    made to the master branch that apply for the specific release as well are also
    made in the release branch.
  prefs: []
  type: TYPE_NORMAL
- en: However, branching models like these contradict the idea of Continuous Delivery.
    Feature branches, for example, postpone the integration of features into the master
    branch. The longer the integration of new functionality is delayed, the bigger
    the possibility for potential merge conflicts. Feature branches are therefore
    only advisable in Continuous Delivery pipelines if they are short-lived and integrated
    into master in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: Release versions and working upon these releases in parallel contradicts the
    idea of continuously shipping versions as well. Features that are implemented
    are ideally shipped to production as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: This is at least the case for enterprise projects. The continuous life cycle
    implies that every commit is a potential candidate for production deployment.
    It makes sense to integrate and apply the work on the master branch, making it
    possible to integrate and deploy features as early as possible, verified by automated
    tests. The branching model of Continuous Delivery and Continuous Deployment, respectively,
    therefore is quite straightforward. Changes are directly applied to the master
    branch, built, verified, and deployed by the build pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: It's usually not required to manually tag releases. Every commit in the Continuous
    Delivery pipeline implicitly qualifies for being released and deployed to production,
    unless the automated verification identifies errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the concept of a Continuous Deployment branching
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb2ffb01-a248-4930-8d3f-226ca071588e.png)'
  prefs: []
  type: TYPE_IMG
- en: Individual features branches are kept short-lived and are merged back to *master*
    in a timely manner. The releases are implicitly created on successful builds.
    Broken builds won't result in a deployment to production.
  prefs: []
  type: TYPE_NORMAL
- en: Products, as well as libraries, however, may advisably have different branching
    models. With multiple supported *major* and *minor* versions, and their potential
    bug fixes, it makes sense to implement branches for separate release versions.
    The release version branches, such as `v1.0.2` can then be used to continue support
    for bug fixes, for example, into `v1.0.3`, while the major development continues
    on a newer version, such as `v2.2.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing Continuous Delivery pipelines, the question remains, which technology
    to use. This includes not only the CI server itself, but all the tools used within
    the development workflow such as version control, artifact repositories, build
    systems, and runtime technologies.
  prefs: []
  type: TYPE_NORMAL
- en: What technology is being used depends on the actual requirements and not least
    of what the team is familiar with. The following examples will make use of Jenkins,
    Git, Maven, Docker, and Kubernetes. As of writing this book these are widely-used
    technologies. However, for engineers it's more important to comprehend the underlying
    principles and motivations. The technology is quite interchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: No matter what tools are selected, it's advisable to use the tools for what
    they are meant for. Experience shows that tools are quite often being misused
    for tasks that would better be executed using different technology. A prime example
    for this is the build system, for example Maven. Projects often define build processes
    that have more responsibilities than just building the artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense not to mix responsibilities of building containers or deploying
    software into the artifact build. These concerns are preferably realized directly
    by the Continuous Integration server. Bringing these steps into the build process
    unnecessarily couples the build technology to the environment.
  prefs: []
  type: TYPE_NORMAL
- en: It's therefore advisable to use the tools for what they were intended to do,
    in a straightforward way. For example, Docker containers are advisably built via
    the corresponding Docker binaries rather than build system plugins. Required abstraction
    layers are rather added in pipeline as code definitions, as demonstrated in the
    following examples.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline-as-code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We previously saw the benefits of representing configuration as code, primarily
    infrastructure as code files. The same motivations led to pipeline as code definitions,
    configuration that specifies the CI server pipeline steps.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, many CI servers such as Jenkins required to be configured manually.
    CI server jobs had to be laboriously *clicked together* to build up pipelines.
    Especially, rebuilding pipelines for new applications or feature branches thereof
    required cumbersome manual work.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline as code definitions specify the Continuous Delivery pipeline as part
    of the software project. The CI server builds up and executes the pipeline appropriately,
    following the script. This vastly simplifies defining and reusing project build
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of CI servers that support pipeline definitions as code. The
    most important aspect is that engineers understand the motivations and benefits
    behind this technology. The following shows examples for Jenkins, a widely used
    CI server in the Java ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Users of Jenkins can craft pipelines in a `Jenkinsfile`, which is defined using
    a Groovy DSL. Groovy is an optionally typed, dynamic JVM language, that suits
    well for DSL and scripts. Gradle build scripts use a Groovy DSL, as well.
  prefs: []
  type: TYPE_NORMAL
- en: The following examples show the steps of a very simple pipeline of a Java enterprise
    project. The examples are meant to give a rough understanding of the executed
    process. For full information on Jenkinsfiles, their syntax and semantics, refer
    to the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The following shows an example `Jenkinsfile`, containing a basic pipeline definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `stage` definitions refer to steps in the Jenkins pipeline. Since the Groovy
    script offers a full-fledged programming language, it is possible and advisable
    to apply clean code practices that produce readable code. Therefore, the contents
    of the specific steps are refactored to separate methods, all in the same layer
    of abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `prepare()` step, for example, encapsulates several executions to fulfill
    build prerequisites, such as checking out the build repository. The following
    code shows its method definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The build stage also encapsulates several sub-steps, from executing the Maven
    build, recording metadata and test results, to building the Docker images. The
    following code shows its method definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These examples provide insight into how to define and encapsulate specific behavior
    into steps. Providing detailed Jenkinsfile examples is beyond the scope of this
    book. I will show you the rough steps necessary to give an idea of what logical
    executions are required, and how to define them in these pipeline scripts in a
    readable, productive way. The actual implementations, however, heavily depend
    on the project.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins pipeline definitions provide the possibility to include so-called pipeline
    libraries. These are predefined libraries that contain often-used functionality
    to simplify usage and reduce duplication beyond several projects. It is advisable
    to outsource certain functionality, especially in regard to environment specifics,
    into company-specific library definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the deployment of the *car manufacture* application
    to a Kubernetes environment. The `deploy()` method would be called from within
    the build pipeline when deploying a specific image and database schema version
    to a Kubernetes namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This example updates and commits the Kubernetes YAML definitions in the VCS
    repository. The execution applies the infrastructure as code to the Kubernetes
    namespace and waits for the deployment to finish.
  prefs: []
  type: TYPE_NORMAL
- en: These examples aim to give the reader an idea of how to integrate Continuous
    Delivery pipelines as pipeline as code definitions with a container orchestration
    framework such as Kubernetes. As mentioned earlier, it is also possible to make
    use of pipeline libraries to encapsulate often-used `kubectl` shell commands.
    Dynamic languages such as Groovy allow engineers to develop pipeline scripts in
    a readable way, treating them with the same effort as other code.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows with Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The demonstrated examples cover general Java build piplines which are, of course,
    applicable to Java EE as well. In fact, using Java Enterprise highly supports
    productive development pipelines. Fast builds and therefore fast developer feedback
    is crucial to effective Continuous Delivery workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-dependency applications, especially when packaged in containers, leverage
    these principles as we have seen in [Chapter 4](f0a49441-e411-49c4-a4b6-c6193ba36094.xhtml),
    *Lightweight Java EE*. The enterprise application in the packaged artifact or
    the container layer, respectively, only contains the business logic that was developed
    against the API. The application container provides the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The Continuous Delivery pipeline benefits from zero-dependency applications,
    since the involved build and distribution steps only require short execution and
    transmission times, respectively. Artifact builds as well as container builds
    run as fast as they can get, with only copying what's absolutely necessary. In
    the same way, publishing and deploying artifacts, as well as container layers,
    only contain the required business concerns, to minimize transmission time. This
    leads to fast turnaround and fast feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Having effective pipelines is crucial to implementing a Continuous Delivery
    culture in the development team. Engineers are motivated to check in early and
    often, since the pipeline runs fast, provides fast feedback, and increases the
    confidence that the software quality is met.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, build times should not take more than a few seconds. Build
    pipeline executions, including end-to-end tests, should not take more than a few
    minutes, ideally even faster.
  prefs: []
  type: TYPE_NORMAL
- en: Putting effort into making builds and pipelines run faster should be a goal
    of the engineering team. During a workday, developers often build and check in
    the project. Every check-in results in a Continuous Delivery build that is a potential
    candidate for production deployment. If this overall process takes just, for example,
    1 minute longer, all developers in the team wait 1 minute longer, every time they
    build the software. One can imagine that this delay adds up to a big number over
    time. Developers are tempted to check in less often if they have to wait for their
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the stability and performance of the pipeline, therefore, is a long-term
    investment in the team's productivity. Tests and steps that provide quick, helpful
    feedback by breaking the build faster in case of errors should run as early as
    possible. If some end-to-end tests run inevitably longer in time, due to the nature
    of the project and the tests, they can be defined in separate downstream pipelines
    steps, to not delay feedback of earlier verification. Steps that can run in parallel,
    such as static code analyses, should do so, to speed up the overall execution.
    Using the modern approaches of Java EE development greatly supports crafting productive
    build pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Still, technology is only one aspect of effective Continuous Delivery. Introducing
    Continuous Delivery has an even bigger impact on the development team's culture.
    Let's have a closer look into this.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery culture and team habits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective Continuous Delivery depends on a healthy team culture. If the team
    does not live by the principles and recommendations Continuous Delivery makes,
    the best technology doesn't help much. Pipelines that implement automated deployments
    have little value if there aren't sufficient software tests verifying the deployed
    software. The most eager CI server can't help much if developers seldom check
    in their changes, making integration hard and cumbersome. Full test coverage and
    code quality checks have no value if the team doesn't react to failing tests or,
    in the worst case, set the test execution to ignore.
  prefs: []
  type: TYPE_NORMAL
- en: Responsibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous Delivery starts with being responsible for the software. As mentioned
    earlier, for the DevOps movement, it is not sufficient for developers to just
    build their software and let other teams deal with potential errors. The development
    team that creates and owns the application knows about its responsibilities, used
    technologies, and troubleshooting in case of potential errors.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a small startup that has only a single developer who responsible for
    the application. This person obviously has to deal with all technical issues,
    such as development, builds, deployment, and troubleshooting the application.
    He or she will have the best knowledge about the application's internals and will
    be able to fix potential issues effectively. Obviously, this single point of responsibility
    approach is the opposite of scalability and only works for tiny teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'In bigger companies, there are more applications, more developers, and more
    teams with different responsibilities. The challenge with splitting and shifting
    responsibilities is to transfer knowledge. The knowledge is ideally spread within
    a team of engineers who closely work on the same software. Like in small startups,
    the mantra for developing applications should be: *you build it, you run it*.
    For a single team, this is only possible with the support of central, well-defined
    and automated processes. Implementing Continuous Delivery pipelines implement
    these processes to reliably ship software.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing and refining these processes becomes the responsibility of the whole
    team of engineers and is no longer an *ops problem*. All developers are equally
    responsible for building and shipping working software that provides value to
    the business. This certainly involves some duties, or team habits.
  prefs: []
  type: TYPE_NORMAL
- en: Check in early and often
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous Delivery has to be lived by the whole team. Developers who work on
    features or bug fixes should check in into the master branch early and often.
    This is crucial to enable Continuous Integration. The more time passes before
    changes are merged into the master branch, the harder the merging and integration
    of features becomes. Adding complex functionality in a big bang contradicts the
    idea of *continuous* evolution of software. Functionality that should not be visible
    to users yet can be excluded by feature toggles.
  prefs: []
  type: TYPE_NORMAL
- en: Checking in often encourages developers to write sufficient, automated software
    tests from the beginning. This is certainly an effort to make during development
    but will always pay off in the long run. While developing a feature, engineers
    are aware of its functionality and boundaries. It's far less effort to include
    not only unit tests but sophisticated end-to-end tests from the beginning then
    it is after the feature has been written.
  prefs: []
  type: TYPE_NORMAL
- en: Especially for less-experienced developers it's important to mention that committing
    early, premature versions of features is nothing to be embarrassed about, but
    part of the development process. Code which hasn't been refactored yet and doesn't
    look perfect, but fulfills the requirements and provides business value, can be
    cleaned up in a second run. It's far more helpful to commit code early in the
    process than refraining from committing until the very last minute.
  prefs: []
  type: TYPE_NORMAL
- en: Immediately fixing issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Immediately solving build errors is another important team habit to develop.
    Tests that fail should not be ignored or postponed but fixed as soon as possible.
    Builds that fail often and are not taken good care of decrease the productivity
    of all team members. A failing test that makes the project unable to be built,
    for example, prevents other developers from integrating and verifying their features.
    Still, failing builds due to test failures or quality violations is a sign that
    the validation works and is, obviously, much better than false negatives, that
    is, mistakenly green builds. It is, however, important to fix project builds as
    soon as they fail. Developers should execute basic and fast verifications, such
    as building the Java project and executing code level tests, on their local machines
    before pushing to the central repository. They should take care not to misuse
    the pipeline to find careless mistakes which unnecessarily disturb other team
    members.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, compiler or code analysis warnings should be treated as
    errors that break the build. This introduces a zero-warning policy that urges
    engineers to either fix the issue or adjust the validation. Build, compilation,
    or code style warnings are therefore also errors that break the build and need
    to be fixed as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The team member whose commit caused the build to break should ideally be the
    first to look into the root cause. It is, however, a responsibility of the whole
    team to keep the pipeline in a healthy state. This goes back to the whole team
    being responsible for the whole project. There should not be exclusive *code ownership*,
    that is, parts of the projects which are exclusively known to a single team member.
    It will always be the case that developers who wrote specific functionality have
    better knowledge about it. Still, in all cases, the team should be able to work
    on all areas of the project and fix potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: Visibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The visibility that Continuous Delivery enables is another important aspect.
    The whole development process, including commits, builds, verifications, and deployments,
    can be tracked and comprehended in a single place. What visibility aspects are
    important in a Continuous Delivery pipeline?
  prefs: []
  type: TYPE_NORMAL
- en: First of all, it needs to be represented whether the software is in a shippable
    state. This includes the build's health in terms of compilation, tests, and code
    analyses. A dashboard or so-called **extreme feedback** device, such as physical
    green and red LEDs, provide a quick overview about it.
  prefs: []
  type: TYPE_NORMAL
- en: A reasonable build visibility ideally does not overload with information if
    the build is green but provides clear and direct insight in case of failing builds.
    This again follows the principle that there are no such things as warnings in
    the build; it either passes successfully and there is nothing else to do or it
    breaks and requires action. Dashboards or other devices that provide this *green
    or red* information already provide helpful insights. These visibility instruments
    should be accessible to all team members to embrace collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: However, in order not to disrupt the day-to-day development too much, it makes
    sense to notify persons in charge, whose commits caused the build to break, first.
    They likely have further knowledge how to fix the build again without disturbing
    the work of their teammates if not necessary. CI servers provide functionality
    to send emails, use chat communication, or other forms of notification. This both
    increases the quality of the software as well as the developer's productivity.
  prefs: []
  type: TYPE_NORMAL
- en: The information that is gathered during builds can be used to measure the quality
    of the software project. This first of all includes build and test results and
    code quality metrics, such as test coverage. This information can be displayed
    over time to provide insights and trends about the software quality.
  prefs: []
  type: TYPE_NORMAL
- en: Other very interesting metadata concerns the build pipeline itself. How long
    does a build usually take? How many builds are there in a day? How often does
    the build fail? What is the most common failure cause? How long does it take a
    failing build to be fixed again (*time to recover*)? The answers to these questions
    provide helpful insights about the quality of the Continuous Delivery process.
  prefs: []
  type: TYPE_NORMAL
- en: The gathered information serves as good starting points to improve the process
    further. Visibility of Continuous Delivery not only illuminates the current project
    status but can also draw the engineers' attention to certain hotspots. The overall
    goal is to continuously improve the software.
  prefs: []
  type: TYPE_NORMAL
- en: Improve continuously
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The whole mindset of Continuous Delivery aims to delivery software with consistent
    quality. Automated processes encourage the usage of quality verifications.
  prefs: []
  type: TYPE_NORMAL
- en: Good software quality, of course, does not come for free. Sufficient test cases
    as well as code quality analyses require a certain time and effort. Automation
    and continuously improving the quality, however, will, after an initial threshold,
    pay off in the long run, and eventually lead to better software.
  prefs: []
  type: TYPE_NORMAL
- en: New features as well as found bugs need to be verified sufficiently during development
    in order to ensure that functionality works as expected. By automating the tests
    and keeping them as regression, developers can be sure that no new bugs can disrupt
    the functionality in the future. The same is true for code quality analyses. Once
    the analysis is set up with appropriate rules and the found errors are eradicated,
    it ensures that no new violations can find their way into the software. If new
    false positive violations emerge, the rules are adjusted and will prevent new
    false positives in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing new test scenarios, such as end-to-end tests, also highly supports
    this approach. Regression tests decrease the risk of newly introduced bugs more
    and more. Again, automation is the key. As we will see in Chapter 7, *Testing*,
    human intervention is helpful for defining reasonable test scenarios. However,
    it is crucial to the software quality that these test are then automated made
    part of the pipeline. By doing so, the quality is improved more and more over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, requires the engineers to put a certain priority into quality
    improvements. Improving software quality, as well as refactoring, doesn't provide
    any immediate benefits for the business. These efforts will, instead, pay off
    in the long run-by still being able to produce new features with a constant
    velocity or changing existing behavior with certainty that nothing else breaks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Productive development workflows require fast turnaround times as well as fast
    feedback. Automating repetitive tasks minimizes the times spent on build, tests
    and deployments. Zero-dependency Java EE applications supports fast feedback by
    minimizing build, publish, and deployment times.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to define which category of errors will break the build. Developers
    should be aware that a build is either broken, due to legitimate errors, or passed,
    without anything to complain about. Warnings that have no effect on the build
    outcome have little value.
  prefs: []
  type: TYPE_NORMAL
- en: Data migration is another important topic to consider. Deploying stateless applications
    is comparably easy; what needs to be taken into account are the database schemas
    that need to match the application code. Rolling updates together with migration
    scripts, that rollout modifications in small changes, enable applications to be
    deployed with zero-downtime. Applications therefore need to support N-1 compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery depends on a healthy team culture. It's not sufficient to
    implement just the technical necessities; all software engineers need to embrace
    the principles. Potential build issues, test results, software quality, and deployment
    statuses should be visible to the whole software team.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery processes support to continuously improve the software.
    Verification steps that are added, such as automated software tests, run every
    time the application is built, enabling regression tests and avoiding specific
    bugs to happen twice. This of course requires developers to put effort into the
    quality improvement. The effort put into Continuous Delivery will pay off in the
    long run.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapter stays in the field of software quality and will cover
    testing enterprise applications.
  prefs: []
  type: TYPE_NORMAL
