<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predict the Class of a Flower from the Iris Dataset</h1>
                </header>
            
            <article>
                
<p>This chapter kicks off a <strong>machine learning</strong> (<strong>ML</strong>) initiative in Scala and Spark. Speaking of Spark, its <strong>Machine Learning Library</strong> (<strong>MLlib</strong>) <span>l</span><span>iving under</span><span> the </span><kbd>spark.ml</kbd><span> </span><span>package and accessible via its MLlib <kbd>DataFrame</kbd>-based API will help us </span><span>develop scalable data analysis applications. The MLlib <kbd>DataFrame</kbd>-based API, also known as Spark ML, provides powerful learning al</span>gorithms and pipeline building tools for data analysis. Needless to say,<span> we will, starting this chapter, leverage </span>MLlib's <span>classification algorithms.</span></p>
<p>The Spark ecosystem, also boasting of APIs to R, Python, and Java in addition to Scala, empowers our <span>readers, be they beginner, or seasoned data professionals, to make sense of and extract analytics from various datasets. </span></p>
<p><span>Speaking of datasets, the Iris dataset is the simplest, yet the most famous data analysis task in the ML space. This chapter builds a solution to the data analysis classification task that the Iris dataset represents. </span></p>
<p>Here is the dataset we will refer to:</p>
<ul>
<li><span>UCI Machine Learning Repository: Iris Data Set</span></li>
<li><span>Accessed July 13, 2018</span></li>
<li><span>Website URL: <a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a></span></li>
</ul>
<p><span>The overarching learning objective of this chapter is to implement a Scala solution to the so-called <strong>multivariate</strong> classification task represented by the Iris dataset.</span></p>
<p>The following list is a section-wise breakdown of individual learning outcomes:</p>
<ul>
<li>A multivariate classification problem</li>
<li>Project overview—problem formulation</li>
<li>Getting started with Spark</li>
<li>Implementing a multiclass classification pipeline</li>
</ul>
<p><span>The following section offers the reader an in-depth perspective on the Iris dataset classification problem.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A multivariate classification problem</h1>
                </header>
            
            <article>
                
<p class="mce-root">The most famous dataset in data science history is Sir Ronald Aylmer Fisher's classical Iris flower dataset, also known as Anderson's dataset. It was introduced in 1936, as a study in understanding multivariate (or multiclass) classification. What then is multivariate?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding multivariate</h1>
                </header>
            
            <article>
                
<p class="mce-root">The term multivariate can bear two meanings:</p>
<ul>
<li class="mce-root">In terms of an adjective, multivariate means having or involving one or more variables.</li>
<li class="mce-root">In terms of a noun, multivariate may represent a mathematical vector whose individual elements are variate. Each individual element in this vector is a measurable quantity or variable.</li>
</ul>
<p><span>Both meanings mentioned have a common denominator variable. Conducting a multivariate analysis of an experimental unit involves at least one measurable quantity or variable. A classic example of such an analysis is the Iris dataset, having one or more (outcome) variables per observation.</span></p>
<p><span>In this subsection, we understood multivariate in terms of variables. In the next subsection, we briefly touch upon different kinds of variables, one of them being categorical variables.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Different kinds of variables</h1>
                </header>
            
            <article>
                
<p><span>In general, variables are of two types:</span></p>
<ul>
<li><strong>Quantitative variable</strong>: It is a variable representing a measurement that is quantified by a numeric value. Some examples of quantitative variables are:</li>
<li style="padding-left: 30px">A variable representing the age of a girl called <kbd>Huan</kbd> (<kbd>Age_Huan</kbd>). In September of 2017, the variable representing her age contained the value <kbd>24</kbd>. Next year, one year later, that variable would be the number 1 (arithmetically) added to her current age.</li>
<li style="padding-left: 30px">The variable representing the number of planets in the solar system (<kbd>Planet_Number</kbd>).<span> Currently, pending the discovery of any new planets in the future, this variable contains the number <kbd>12</kbd>. I</span><span>f scientists found a new celestial body tomorrow that they think qualifies to be a planet, the <kbd>Planet_Number</kbd> variable's new value would be bumped up from its current value of <kbd>12</kbd> to <kbd>13</kbd>. </span></li>
<li><strong>Categorical variable</strong>: A variable that cannot be assigned a numerical measure in the natural order of things. For example, the status of an individual in the United States. It could be one of the following values: a citizen, permanent resident, or a non-resident.</li>
</ul>
<p>In the next subsection, we will describe categorical variables in some detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical variables </h1>
                </header>
            
            <article>
                
<p><span>We will draw upon the definition of a categorical variable from the previous subsection. Categorical variables distinguish themselves from quantitative variables in a fundamental way. As opposed to a quantitative variable that represents a measure of a something in numerical terms, a categorical variable represents a grouping name or a category name, which can take one of the finite numbers of possible categories. For example, the species of an Iris flower is a categorical variable and the value it takes could be one value from a finite set of categorical values: Iris-setosa, Iris-virginica, and Iris-versicolor.</span></p>
<p>It may be useful to draw on other examples of categorical variables; these are listed here as follows:</p>
<ul>
<li>The<span> </span>blood group of an individual as in A+, A-, B+, B-, AB+, AB-, O+, or O-</li>
<li>The county that an individual is a resident of given a finite list of counties in the state of Missouri</li>
<li>The<span> </span>political affiliation of a United States citizen could take up categorical values in the form of Democrat, Republican, or Green Party</li>
<li>In global warming studies, the type of a forest is a categorical variable that could take one of three values in the form of tropical, temperate, or<span> taiga</span></li>
</ul>
<p><span>The first item in the preceding list, the blood group of a person, is a categorical variable whose corresponding data (values) are categorized (classified) into eight groups (A, B, AB, or O with their positives or negatives). In a similar vein, the s</span><span>pecies of an Iris flower is a categorical variable whose data (values) are categorized (classified) into three species groups—Iris-setosa, Iris-versicolor, and Iris-virginica. </span></p>
<p><span>That said, a common data analysis task in ML is to index, or encode, current string representations of categorical values into a numeric form; doubles for example. Such indexing is a prelude to a prediction on the target or label, which we shall talk more about shortly.</span></p>
<p><span>In respect to the Iris flower dataset, its species variable data is subject to a classification (or categorization) task with the express purpose of being able to make a prediction on the species of an Iris flower. At this point, we want to examine the Iris dataset, its rows, row characteristics, and much more, which is the focus of the upcoming topic.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fischer's Iris dataset</h1>
                </header>
            
            <article>
                
<p class="mce-root">The Iris flower dataset comprises of a total of 150 rows, where each row represents one flower. Each row is also known as an <strong>observation</strong>. This 150 observation Iris dataset is made up of three kinds of observations related to three different Iris flower species. The following table is an illustration:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fefaf096-d868-407a-9302-a18314b30487.png" style="width:38.50em;height:13.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Iris dataset observation breakup table</div>
<p>Referring to the preceding table, it is clear that three flower species are represented in the Iris dataset. Each flower species in this dataset contributes equally to 50 observations apiece. Each observation holds four measurements. One measurement corresponds to one flower feature, where each flower feature corresponds to one of the following:</p>
<ul>
<li class="mce-root"><strong>Sepal Length</strong></li>
<li class="mce-root"><strong>Sepal Width</strong></li>
<li class="mce-root"><strong>Petal Length</strong></li>
<li class="mce-root"><strong>Petal Width</strong> </li>
</ul>
<p>The features listed earlier are illustrated in the following table for clarity:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f45af5d4-2ac7-4f79-8aef-5bad685bc9d2.png" style="width:21.00em;height:11.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Iris features</div>
<p><span>Okay, so three flower species are represented in the Iris dataset. </span>Speaking of species, we will henceforth replace the term <em>species</em> with the term <em>classes</em> whenever there is the need to stick to an ML terminology context. That means <strong>#1</strong>-<strong>Iris-setosa</strong> from earlier refers to <strong>Class # 1</strong>, <strong>#2</strong>-<strong>Iris-virginica</strong> to <strong>Class # 2</strong>, and <strong>#3</strong>-<strong>Iris-versicolor</strong> to <strong>Class # 3</strong>.</p>
<p>We just listed three different Iris flower species that are represented in the Iris dataset. What do they look like? What do their features look like? These questions are answered in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3d0b8100-a058-4788-98fd-34b3ed8bdc60.jpg" style="width:36.42em;height:45.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Representations of three species of Iris flower</div>
<p><span> </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>That said, let's look at the <strong>Sepal</strong> and <strong>Petal</strong> portions of each class of Iris flower. The <strong>Sepal</strong> (the larger lower part) and <strong>Petal</strong> (the lower smaller part) dimensions are how e</span>ach class of Iris flower bears a relationship to the other two classes of Iris flowers. In the next section, we will summarize our discussion and expand the scope of the discussion of the Iris dataset to a <span>multiclass, multidimensional classification task.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Iris dataset represents a multiclass, multidimensional classification task</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will restate the facts about the Iris dataset and describe it in the context of an ML classification task:</span></p>
<ul>
<li>The Iris dataset classification task is multiclass because a prediction of the class of a new incoming Iris flower from the wild can belong to any of three classes.</li>
<li>Indeed, this chapter is all about attempting a <span>species classification (inferring the target class of a new Iris flower) using sepal and petal dimensions as feature parameters.</span></li>
<li>The Iris dataset classification is multidimensional because there are four features. </li>
<li><span>There are 150 observations, where each observation is comprised of measurements on four features. These measurements are also known by the following terms:</span></li>
<li style="padding-left: 30px"><span>Input attributes or instances</span></li>
<li style="padding-left: 30px"><span>Predictor variables (<kbd>X</kbd>)</span></li>
<li style="padding-left: 30px">Input variables (<kbd>X</kbd>)</li>
</ul>
<ul>
<li><span>Classification of an Iris flower picked in the wild is carried out by a model (the computed mapping function) that is given four </span>flower feature measurements.</li>
<li><span>The outcome of the Iris flower classification task is the identification of a (computed) predicted value for the response from the predictors by a process of learning (or fitting) a discrete number of targets or category labels (<kbd>Y</kbd>). The outcome or predicted value may mean the same as the following:</span></li>
<li style="padding-left: 30px"><span>Categorical response variable: In a later section, we shall see that an indexer algorithm will transform all categorical values to numbers</span></li>
<li style="padding-left: 30px">Response or outcome variable (<kbd>Y</kbd>)</li>
</ul>
<p class="mce-root"/>
<p>So far, we have claimed that the outcome (<kbd>Y</kbd>) of our multiclass classification task is dependent on inputs (<kbd>X</kbd>). Where will these inputs come from? This is answered in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The training dataset</h1>
                </header>
            
            <article>
                
<p><span>An integral aspect of our data analysis or classification task we did not hitherto mention is the training dataset. A training dataset is our classification task's source of input data (<kbd>X</kbd>). We take advantage of this dataset to obtain a prediction on each target class, simply by deriving optimal perimeters or boundary conditions. We just redefined our classification process by adding in the extra detail of the training dataset. For a classification task, then we have <kbd>X</kbd> on one side and <kbd>Y</kbd> on the other, with an inferred mapping function in the middle. That brings us to the mapping or predictor function, which is the focus of the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The mapping function</h1>
                </header>
            
            <article>
                
<p><span>We have so far talked about an input variable (<kbd>X</kbd>) and an output variable (<kbd>Y</kbd>). The goal of any classification task, therefore, is to discover patterns and find a mapping (predictor) function that will take feature measurements (<kbd>X</kbd>) and map input over to the output (<kbd>Y</kbd>). That function is mathematically formulated as:</span></p>
<pre>Y = f(x) </pre>
<p><span>This mapping is how supervised learning works. A supervised learning algorithm is said to learn or discover this function. This will be the goal of the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An algorithm and its mapping function </h1>
                </header>
            
            <article>
                
<p>This section starts with a schematic depicting the components of the mapping function and an algorithm that learns the mapping function. The algorithm is learning the mapping function, as shown in the following diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/313714d2-55c9-4136-91d6-336e3d311575.png" style="width:27.17em;height:23.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An input to output mapping function and an algorithm learning the mapping function</div>
<p>The goal of our classification process is to let the algorithm derive the best possible approximation of a mapping function by a learning (or fitting) process. When we find an Iris flower out in the wild and want to classify it, we use its input measurements as new input data that our algorithm's mapping function will accept in order to give us a predictor value (<strong>Y</strong>). In other words, given feature measurements of an Iris flower (the new data), the mapping function produced by a supervised learning algorithm (this will be a random forest) will classify the flower.</p>
<p>Two kinds of ML problems exist that s<span>upervised learning classification algorithms can solve. These are as follows:</span></p>
<ul>
<li><span>Classification tasks</span></li>
<li><span>Regression tasks</span></li>
</ul>
<p class="mce-root"/>
<p>In the following paragraph, we will talk about a mapping function with an example.  We explain the role played by a "supervised learning classification task" in deducing the mapping function. The concept of a model is introduced.</p>
<p><span><span>Let's say we already knew that the (mapping) function</span></span> <kbd>f(x)</kbd> for the Iris dataset classification task is exactly of the form <kbd>x + 1</kbd>,   then there is there no need for us to find a new mapping function.  If we recall, a mapping function is one that maps the relationship between flower features, such as sepal length and sepal width, on the species the flower belongs to? No.</p>
<p>Therefore, there is no preexisting function <kbd>x + 1</kbd> that clearly maps the relationship between <span>flower features and the flower's species. W</span>hat we need is a model that will model the aforementioned relationship as closely as possible. Data and its classification seldom tend to be straightforward. A supervised learning classification task starts life with no knowledge of what function <kbd>f(x)</kbd> is. A supervised learning classification process applies ML techniques and strategies in <span>an iterative process of deduction to ultimately</span> learn what <kbd>f(x)</kbd> is.</p>
<p>In our case, such an ML endeavor is a classification task, a task where the function or mapping function is referred to in statistical or ML terminology as a <strong>model</strong>.</p>
<p>In the next section, we will describe what supervised learning is and how it relates to the Iris dataset classification.  <span>Indeed, this apparently simplest of ML techniques finds wide applications in data analysis, especially in the business domain.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning – how it relates to the Iris classification task</h1>
                </header>
            
            <article>
                
<p><span>At the outset, the following is a list of salient aspects of supervised learning:</span></p>
<ul>
<li><span>The term </span><strong>supervised</strong><span> in supervised learning stems from the fact that the algorithm is learning or inferring what the mapping function is.</span></li>
<li>A data analysis task, either classification or regression.</li>
<li>It contains a process of learning or inferring a mapping function from a labeled training dataset.</li>
<li>Our Iris training dataset has training examples or samples, where each example may be represented by an input feature vector consisting of four measurements.</li>
</ul>
<p class="mce-root"/>
<ul>
<li>A supervised learning algorithm learns or infers or derives the best possible approximation of a mapping function by carrying out a data analysis on the training data. The mapping function is also known as a model in statistical or ML terminology.</li>
<li>The algorithm provides our model with parameters that it learns from the training example set or training dataset in an iterative process, as follows:</li>
<li style="padding-left: 30px">Each iteration produces predicted class labels for new input instances from the wild</li>
<li style="padding-left: 30px">Each iteration of the learning process produces progressively better generalizations of what the output class label should be, and as in anything that has an end, the learning process for the algorithm also ends with a high degree of reasonable correctness on the prediction</li>
</ul>
<ul>
<li>An ML classification process employing supervised learning has algorithm samples with correctly predetermined labels.</li>
<li>The Iris dataset is a typical example of a supervised learning classification process. The term supervised arises from the fact that the algorithm at each step of an iterative learning process applies an appropriate correction on its previously generated model building process to generate its next best model.</li>
</ul>
<p>In the next section, we will define a training dataset. <span>In the next section, and in the remaining sections, we will use the R</span><span>andom Forest classification algorithm to run data analysis transformation tasks. One such task worth noting here is a process of transformation of string labels to an indexed label column represented by doubles.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random Forest classification algorithm</h1>
                </header>
            
            <article>
                
<p><span>In a preceding section, we noted the crucial role played by the input or training dataset. In this section, we reiterate the importance of this dataset. That said, the training dataset from an ML algorithm standpoint is one that the Random Forest algorithm takes advantage of to train or fit the model by generating the parameters it needs. These are parameters the model needs to come up with the next best-predicted value. In this chapter, we will put the Random Forest algorithm to work on training (and testing) Iris datasets. Indeed, the next paragraph starts with a discussion on Random Forest algorithms or simply Random Forests.</span></p>
<p class="mce-root">A Random Forest algorithm encompasses decision tree-based supervised learning methods. It can be viewed as a composite whole comprising a large number of decision trees. In ML terminology, a Random Forest is an ensemble resulting from a profusion of decision trees.</p>
<p class="mce-root"/>
<p>A decision tree, as the name implies, is a progressive decision-making process, made up of a root node followed by successive subtrees. The decision tree algorithm snakes its way up the tree, stopping at every node, starting with the root node, to pose a <span>do-you-belong-to-a-certain-category question. Depending on whether the answer is a yes or a no, a decision is made to travel up a certain branch until the next node is encountered, where the algorithm repeats its interrogation. Of course, at each node, the</span> answer received by the algorithm determines the next branch to be on. The final outcome is a predicted outcome on a leaf that terminates.</p>
<p>Speaking of trees, branches, and nodes, the dataset can be viewed as a tree made up of multiple subtrees. Each decision at a node of the dataset and the decision tree algorithm's choice of a certain branch is the result of an optimal composite of feature variables. Using a Random Forest algorithm, multiple decision trees are created. Each decision tree in this ensemble is the outcome of a randomized ordering of variables. That brings us to what random forests are—an ensemble of a multitude of decision trees.</p>
<p>It is to be noted that one decision tree by itself cannot work well for a smaller sample like the Iris dataset. This is where the Random Forest algorithm steps in. It <span>brings together or aggregates all of the predictions from its forest of decision trees. All of the </span>aggregated results from individual decision trees in this forest would form one ensemble, better known as a Random Forest.</p>
<p>We chose the Random Forest method to make our predictions for a good reason. The net prediction formed out of an ensemble of predictions is significantly more accurate.</p>
<p>In the next section, we will formulate our classification problem, and in the <em>Getting started with Spark</em> section that follows, implementation details for the project are given.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project overview – problem formulation</h1>
                </header>
            
            <article>
                
<p><span>The intent of this project is to develop an ML workflow or more accurately a pipeline. The goal is to s</span><span>olve the classification problem on the most famous dataset in data science history. </span></p>
<p><span>If we saw a flower out in the wild that we know belongs to one of three Iris species, we have a classification problem on our hands. If we made measurements (<kbd>X</kbd>) on the unknown flower, the task is to learn to recognize the species to which the flower (and its plant) belongs.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>Categorical variables represent types of data which may be divided into groups. Examples of categorical variables are race, sex, age group, and educational level. While the latter two variables may also be considered in a numerical manner by using exact values for age and highest grade completed, it is often more informative to categorize such variables into a relatively small number of groups.</span></p>
<p>Analysis of categorical data generally involves the use of data tables. A<span> </span>two-way table<span> </span>presents categorical data by counting the number of observations that fall into each group for two variables, one divided into rows and the other divided into columns.<span> </span></p>
<p>In a nutshell, the high-level formulation of the classification problem is given as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5ba34af0-8b84-4465-b1e7-d4b7478c0f0e.png" style="width:45.33em;height:11.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">High-level formulation of the Iris supervised learning classification problem</div>
<div class="packt_infobox">In the Iris dataset, each row contains categorical data (values) in the fifth column. Each such value is associated with a label (<strong>Y</strong>). </div>
<p>The formulation consists of the following:</p>
<ul>
<li>Observed features </li>
<li>Category labels </li>
</ul>
<p>Observed features are also known as <strong>predictor variables</strong>. Such variables have predetermined measured values. These are the inputs X. On the other hand, category labels denote possible output values that predicted variables can take. </p>
<p>The predictor variables are as follows:</p>
<ul>
<li><kbd>sepal_length</kbd>: It represents sepal length, in centimeters, used as input</li>
<li><kbd>sepal_width</kbd>: It represents sepal width, in centimeters, used as input</li>
<li><kbd>petal_length</kbd>: It represents petal length, in centimeters, used as input</li>
</ul>
<p class="mce-root"/>
<ul>
<li><kbd>petal_width</kbd>: It represents petal width, in centimeters, used as input</li>
<li><kbd>setosa</kbd>: It represents Iris-setosa, true or false, used as target</li>
<li><kbd>versicolour</kbd>: It represents Iris-versicolour, true or false, used as target</li>
<li><kbd>virginica</kbd>: It represents Iris-virginica, true or false, used as target</li>
</ul>
<p><span>Four outcome variables were measured from each sample; the length and the width of the sepals and petals.</span></p>
<p>The total build time of the project should be no more than a day in order to get everything working. For those new to the data science area, understanding the background theory, setting up the software, and getting to build the pipeline could take an extra day or two.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with Spark</h1>
                </header>
            
            <article>
                
<p><span>The instructions are for Windows users. Note that to run Spark Version 2 and above, Java Version 8 and above, Scala Version 2.11, <strong>Simple Build Tool</strong> (<strong>SBT</strong>) version that is at least 0.13.8 is a prerequisite. The code for the Iris project depends on Spark 2.3.1, the latest distribution at the time of writing this chapter. This version was released on December 1, 2017. Implementations in subsequent chapters would likely be based on Spark 2.3.0, released February 28, 2017. Spark 2.3.0 is a major update version that comes with fixes to over 1,400 tickets. </span></p>
<p>The Spark 2.0 brought with it a raft of improvements. The introduction of the dataframe as the fundamental abstraction of data is one such improvement. Readers will find that the dataframe abstraction and its supporting APIs enhance their data science and analysis tasks, not to mention this powerful feature's improved performance over <strong>Resilient Distributed Datasets</strong> (<strong>RDDs</strong>). Support for RDDs is very much available in the latest Spark release as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up prerequisite software</h1>
                </header>
            
            <article>
                
<p><span>A note on hardware before jumping to prerequisites. The hardware infrastructure I use throughout in this chapter comprises of a 64-bit Windows Dell 8700 machine running Windows 10 with Intel(R) Core(TM) i7-4770 CPU @ 3.40 GHz and an installed memory of 32 GB.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"><span>In this subsection, we document three software prerequisites that must be in place before installing Spark.</span></p>
<div class="packt_infobox"><span>At the time of this writing, my prerequisite software setup consisted of JDK 8, Scala 2.11.12, and SBT 0.13.8, respectively. The following list is a minimal, recommended setup (note that you are free to try a higher JDK 8 version and Scala 2.12.x).</span></div>
<p>Here is the required prerequisite list for this chapter:</p>
<ul>
<li><span>Java SE Development Kit 8 </span></li>
<li>Scala 2.11.12</li>
<li>SBT 0.13.8 or above</li>
</ul>
<p><span>If you are like me, dedicating an entire box with the sole ambition of evolving your own Spark big data ecosystem is not a bad idea. With that in mind, start with an appropriate machine (with ample space and at least 8 GB of memory), running your preferred OS, and install the preceding mentioned prerequisites listed in order. What about lower versions of the JDK, you may ask? </span><span>Indeed, lower versions of the JDK are not compatible with Spark 2.3.1.</span></p>
<div class="packt_infobox">While I will not go into the JDK installation process here, here are a couple of notes. Download Java 8 (<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a>) and once the installer is done installing the <kbd>Java</kbd> folder, do not forget to set up two new system environment variables—the <kbd>JAVA_HOME</kbd> environment variable pointing to the root folder of your Java installation, and the <kbd>JAVA_HOME/bin</kbd> in your system path environment variable.</div>
<p>After setting the system <kbd>JAVA_HOME</kbd> environment, here is how to do a quick sanity check by listing the value of <kbd>JAVA_HOME</kbd> on the command line:</p>
<pre><strong>C:\Users\Ilango\Documents\Packt-Book-Writing-Project\DevProjects\Chapter1&gt;echo %JAVA_HOME%</strong><br/><strong>C:\Program Files\Java\jdk1.8.0_102</strong></pre>
<p>Now what remains is to do another quick check to be certain you installed the JDK flawlessly. Issue the following commands on your command line or Terminal.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Note that this screen only represents the Windows command line:</p>
<pre><strong>C:\Users\Ilango\Documents\Packt\DevProjects\Chapter1&gt;java -version</strong><br/><strong>java version "1.8.0_131"</strong><br/><strong>Java(TM) SE Runtime Environment (build 1.8.0_131-b11)</strong><br/><strong>Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)</strong><br/><br/><strong>C:\Users\Ilango\Documents\Packt\DevProjects\Chapter1&gt;javac -version</strong><br/><strong>javac 1.8.0_102</strong></pre>
<p>At this point, if your sanity checks passed, the next step is to install Scala. The following brief steps outline that process. <span>The Scala download page at <a href="https://archive.ics.uci.edu/ml/datasets/iris">https://archive.ics.uci.edu/ml/datasets/iris</a> documents many ways to install Scala (for different OS environments). However, we only list three methods to install Scala.</span></p>
<div class="packt_infobox">Before diving into the Scala installation, a quick note here. <span>While the latest stable version of Scala is 2.12.4, I prefer a slightly older version, version 2.11.12, which is the version I will use in this chapter. You may download it at</span> <a href="http://scala-lang.org/download/2.11.12.html">http://scala-lang.org/download/2.11.12.html</a>. Whether you prefer version 2.12 or 2.11, the choice is yours to make, as long as the version is not anything below 2.11.x. The following installation methods listed will get you started down that path.</div>
<p>Scala can be installed through the following methods:</p>
<ul>
<li><span><strong>Install Scala</strong>: Locate the section titled <span class="packt_screen">Other ways to install Scala</span> at <a href="http://scala-lang.org/download/">http://scala-lang.org/download/</a> and download the Scala binaries from there. Then you can install Scala by following the instructions at <a href="http://scala-lang.org/download/install.html">http://scala-lang.org/download/install.html.</a> I</span>nstall SBT from <a href="https://www.scala-sbt.org/download.html">https://www.scala-sbt.org/download.html</a> <span>and follow the setup instructions at </span><a href="https://www.scala-sbt.org/1.0/docs/Setup.html">https://www.scala-sbt.org/1.0/docs/Setup.html.</a></li>
<li><strong>Scala in the IntelliJ IDE</strong>: Instructions are given at <a href="https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html">https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html.</a></li>
<li><strong>Scala in the IntelliJ IDE with SBT</strong>: This is another handy way to play with Scala. Instructions are given at <a href="https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html">https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html.</a></li>
</ul>
<div class="packt_infobox"><span>The acronym <strong>SBT</strong> that just appeared in the preceding list is short for <strong>Simple Build Tool</strong>. Indeed, you will run into references to SBT fairly often throughout this book. </span></div>
<p class="mce-root"/>
<p>Take up the item from the first method of the preceding list and work through the (mostly self-explanatory) instructions. Finally, if you forgot to set environment variables, do set up a brand new <kbd>SCALA_HOME</kbd> system environment variable (l<span>ike <kbd>JAVA_HOME</kbd>)</span>, or simply update an existing <kbd>SCALA_HOME</kbd>. Naturally, the <span><kbd>SCALA_HOME/bin</kbd> entry is added to the </span>path environment variable.</p>
<div class="packt_tip">You do not necessarily need Scala installed system-wide. The SBT environment gives us access to its own Scala environment anyway. However, having a system-wide Scala installation allows you to quickly implement Scala code rather than spinning up an entire SBT project. </div>
<p>Let us review what we have accomplished so far. We installed Scala by working through the first method Scala installation. </p>
<p>To confirm that we did install Scala, let's run a basic test:</p>
<pre><strong>C:\Users\Ilango\Documents\Packt\DevProjects\Chapter1&gt;scala -version</strong><br/><strong>Scala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL</strong></pre>
<p>The preceding code listing confirms that our most basic Scala installation went off without a hitch. This paves the way for a system-wide SBT installation. Once again, it comes down to setting up the <kbd>SBT_HOME</kbd> system environment variable and setting <kbd>$SBT_HOME/bin</kbd> in the path. This is the most fundamental bridge to cross. Next, let's run a sanity check to verify that SBT is all set up. Open up a command-line window or Terminal. We installed SBT 0.13.17, as shown in the following code:</p>
<pre><strong>C:\Users\Ilango\Documents\Packt\DevProjects\Chapter1&gt;sbt sbtVersion</strong><br/><strong>Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0</strong><br/><strong>[info] Loading project definition from C:\Users\Ilango\Documents\Packt\DevProjects\Chapter1\project</strong><br/><strong>[info] Set current project to Chapter1 (in build file:/C:/Users/Ilango/Documents/Packt/DevProjects/Chapter1/)</strong><br/><strong>[info] 0.13.17</strong></pre>
<p>We are left with method two and method three. These are left as an exercise for the reader. Method three will let us take advantage of all the nice features that an IDE like IntelliJ has.</p>
<p class="mce-root"/>
<p>Shortly, the approach we will take in developing our pipeline involves taking an existing SBT project and importing it into IntelliJ, or we just create the SBT project in IntelliJ. </p>
<p>What's next? The Spark installation of course. Read all about it in the upcoming section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Spark in standalone deploy mode</h1>
                </header>
            
            <article>
                
<p><span>In this section, we set up a Spark development environment in standalone deploy mode. To get started with Spark and start developing quickly, Spark's shell is the way to go. </span></p>
<div class="packt_infobox"><span>Spark supports Scala, Python, R, and Java with appropriate APIs. </span></div>
<p><span>The Spark binary download offers developers two components:</span></p>
<ul>
<li>The Spark's shell</li>
<li>A standalone cluster</li>
</ul>
<p><span>Once the binary is downloaded and extracted (instructions will follow), the Spark shell and standalone Scala application will let you spin up a standalone cluster in standalone cluster mode.</span></p>
<p><span>This cluster is self-contained and private because it is local to one machine. The Spark shell allows you to easily configure this standalone cluster. Not only does it give you quick access to an interactive Scala shell, but also lets you develop a Spark application that you can deploy into the cluster (lending it the name standalone deploy mode), right in the Scala shell. </span></p>
<p><span>In this mode, the cluster's driver node and worker nodes reside on the same machine, </span><span>not to mention the fact that our Spark application will take up all the cores available on that machine by default. The important feature of this mode that makes all this possible is the interactive (Spark) Scala shell.</span></p>
<div class="packt_tip"><span>Spark 2.3 is the latest version. It comes with over 1,400 fixes. A Spark 2.3 installation on Java 8 might be the first thing to do before we get started on our next project in <a href="a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml">Chapter 2</a>, <em>Build a Breast Cancer Prognosis Pipeline with the Power of Spark and Scala.</em></span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Without further ado, let's get started setting up Spark in standalone deploy mode. The following sequence of instructions are helpful:</p>
<ol>
<li>System checks: First make sure you have at least 8 GB of memory, leaving at least 75% of this memory for Spark. Mine has 32 GB. Once the system checks pass, download the Spark 2.3.1 binary from here: <a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html.</a></li>
<li><span>You will need a decompression utility capable of extracting the </span><kbd>.tar.gz</kbd><span> and </span><kbd>.gz</kbd><span> archives because Windows does not have native support for these archives. 7-Zip is a suitable program for this. You can obtain it from </span><a href="http://7-zip.org/download.html">http://7-zip.org/download.html</a><span>.</span></li>
<li>Choose the package type prebuilt for Apache Hadoop 2.7 and later and download <kbd>spark--2.2.1-bin-hadoop2.7.tgz</kbd>.</li>
<li>Extract the package to someplace convenient, which will become your Spark root folder. For example, my Spark root folder is: <kbd>C:\spark-2.2.1-bin-hadoop2.7</kbd>.</li>
<li>Now, set up the <span>environment variable, <kbd>SPARK_HOME</kbd> pointing to the Spark root folder. We would also need a path entry in the <kbd>PATH</kbd> variable to point to</span> <kbd>SPARK_HOME/bin</kbd>.</li>
<li>Next, set up the environment variable, <kbd>HADOOP_HOME</kbd>, to, say, <kbd>C:\Hadoop</kbd>, and create a new path entry for Spark by pointing it to the <kbd>bin</kbd> folder of the Spark home directory. Now, launch <kbd>spark-shell</kbd> <span>like this:</span></li>
</ol>
<pre style="padding-left: 90px"><strong>spark-shell --master local[2]</strong></pre>
<p>What happens next might frustrate Windows users. If you are one of those users, you will run into the following error. The following screenshot is a representation of this problem:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3efbec97-5ec2-46dc-b401-c0436cd0fbec.png" style="width:38.83em;height:14.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Error message on Windows</div>
<p class="mce-root"/>
<p>To get around this issue, you may proceed with the following steps:</p>
<ol>
<li>Create a new folder as <kbd>C\tmp\hive</kbd>.</li>
<li>Then get the missing <kbd>WINUTILS.exe</kbd> binary from here: <a href="https://github.com/steveloughran/winutils">https://github.com/steveloughran/winutils</a>. D<span>rop this into <kbd>C\Hadoop\bin</kbd>. </span></li>
</ol>
<p>The preceding step 2 is necessary because the Spark download does not contain the <kbd>WINUTILS.exe</kbd> that is required to run Hadoop. That, then, is the source of the <kbd>java.io.IOException</kbd>. </p>
<p>With that knowledge, open up the Command Prompt window in a<span>dministrator mode </span>and execute the newly downloaded <kbd>WINUTILS.EXE</kbd> like this:</p>
<pre><strong>winutils.exe chmod -R 777 C:\tmp\hive</strong></pre>
<p>Next, <span>issue the <kbd>spark-shell</kbd> command. This time around, Spark's interactive development environment launches normally, spinning up its own <kbd>SparkContext</kbd> instance <kbd>sc</kbd> and a <kbd>SparkSession</kbd> <kbd>spark</kbd> session, respectively. While the <kbd>sc</kbd> feature is a powerful entry point to the underlying local standalone cluster, <kbd>spark</kbd> is the main entry point to Spark's </span><span>data processing APIs.</span></p>
<p><span>The following is the output from the <kbd>spark-shell</kbd> command.</span> <span><kbd>SparkContext</kbd> is made available to you as <kbd>sc</kbd> and the Spark session is available to you as <kbd>spark</kbd></span>:</p>
<pre><strong>C:\Users\Ilango\Documents\Packt\DevProjects\Chapter1&gt;spark-shell --master local[2]</strong><br/><strong>Spark context Web UI available at http://192.168.56.1:4040</strong><br/><strong>Spark context available as 'sc' (master = local[2], app id = local-1520484594646).</strong><br/><strong>Spark session available as 'spark'.</strong><br/><strong>Welcome to</strong><br/><strong> ____ __</strong><br/><strong> / __/__ ___ _____/ /__</strong><br/><strong> _\ \/ _ \/ _ `/ __/ '_/</strong><br/><strong> /___/ .__/\_,_/_/ /_/\_\ version 2.2.1</strong><br/><strong>/_/</strong><br/><strong><span>Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_102)</span></strong><br/><strong><span>Type in expressions to have them evaluated.</span></strong><br/><strong><span>Type :help for more information.<br/>scala&gt;</span></strong></pre>
<p>The <kbd>local[2]</kbd> option in the <kbd>spark-shell</kbd> launch shown earlier lets us run Spark locally with <kbd>2</kbd> threads. </p>
<p class="mce-root"/>
<p>Before diving into the next topic in this section, it is a good idea to understand the following Spark shell development environment features that make development and data analysis possible:</p>
<ul>
<li><kbd>SparkSession</kbd></li>
<li><kbd>SparkBuilder</kbd></li>
<li><kbd>SparkContext</kbd></li>
<li><kbd>SparkConf</kbd></li>
</ul>
<p>The <kbd>SparkSession</kbd> API<span> </span>(<a href="https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession">https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession</a>) describes <kbd>SparkSession</kbd> as a programmatic access entry point to<span> Spark's dataset and dataframe APIs, respectively.</span></p>
<p>What is <kbd>SparkBuilder</kbd>? The <kbd>SparkBuilder</kbd> companion object contains a <kbd>builder</kbd> method, which, when invoked, allows us to retrieve an existing <kbd>SparkSession</kbd> or even create one. We will now obtain our <kbd>SparkSession</kbd> instance in a two-step process, as follows: </p>
<ol>
<li><span>Import the</span> <kbd>SparkSession</kbd> <span>class.</span></li>
<li>Invoke the <kbd>builder</kbd> method with <kbd>getOrCreate</kbd> on the resulting <kbd>builder</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>scala&gt; import org.apache.spark.sql.SparkSession</strong><br/><strong>import org.apache.spark.sql.SparkSession</strong><br/><br/><strong><span>scala&gt; lazy val session: SparkSession = SparkSession.builder().getOrCreate()</span></strong><br/><strong><span>res7: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f68756d</span></strong></pre>
<p>The <kbd>SparkContext</kbd> API (<a href="https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext">https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext</a>) describes <kbd>SparkContext</kbd> as a first-line entry point for setting or configuring Spark cluster <span>properties (RDDs, accumulators, broadcast variables, and much more) affecting the cluster's </span>functionality. One way this configuration happens is by passing in a<span> <kbd>SparkConf</kbd> instance as a <kbd>SparkContext</kbd> constructor parameter. One <kbd>SparkContext</kbd> exists per JVM instance. </span></p>
<p>In a sense, <kbd>SparkContext</kbd> is also how a Spark driver application connects to a cluster through, for example, Hadoop's Yarn <strong>ResourceManager</strong> (<strong>RM</strong>). </p>
<p class="mce-root"/>
<p>Let's inspect our Spark environment now. We will start by launching the Spark shell. That said, a <span>typical S</span><span>park shell interactive environment screen has its own SparkSession available as </span> <span><kbd>spark</kbd>,   whose value we try to read off in the code block as follows:</span></p>
<pre><strong>scala&gt; spark</strong><br/><strong>res21: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f68756d</strong></pre>
<p><span> The Spark shell also boasts of its own <kbd>SparkContext</kbd> instance <kbd>sc</kbd>, which is associated with <kbd>SparkSession</kbd> <kbd>spark</kbd>. In the following code, <kbd>sc</kbd> returns <kbd>SparkContext</kbd></span>:</p>
<div>
<pre><strong>scala&gt; sc</strong><br/><strong>res5: org.apache.spark.SparkContext = org.apache.spark.SparkContext@553ce348</strong></pre></div>
<div>
<p><kbd>sc</kbd> can do more. In the following code, invoking the <kbd>version</kbd> method on <kbd>sc</kbd> gives us the <span>version of Spark running in our cluster: </span></p>
<pre><strong>scala&gt; sc.version</strong><br/><strong>res2: String = 2.2.1</strong><br/><strong><span>scala&gt; spark</span></strong><br/><strong><span>res3: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6f68756d</span></strong></pre>
<p>Since <kbd>sc</kbd> represents a connection to the Spark cluster, it holds a special object called <kbd>SparkConf</kbd>, holding cluster configuration properties in an <kbd>Array</kbd>. Invoking the <kbd>getConf</kbd> method on the <kbd>SparkContext</kbd> yields <kbd>SparkConf</kbd>, whose <kbd>getAll</kbd> method (shown as follows) yields an <kbd>Array</kbd> of cluster (or connection) properties, as shown in the following code:</p>
</div>
<div>
<pre><strong>scala&gt; sc.getConf.getAll</strong><br/><strong>res17: Array[(String, String)] = Array((spark.driver.port,51576), (spark.debug.maxToStringFields,25), (spark.jars,""), (spark.repl.class.outputDir,C:\Users\Ilango\AppData\Local\Temp\spark-47fee33b-4c60-49d0-93aa-3e3242bee7a3\repl-e5a1acbd-6eb9-4183-8c10-656ac22f71c2), (spark.executor.id,driver), (spark.submit.deployMode,client), (spark.driver.host,192.168.56.1), (spark.app.id,local-1520484594646), (spark.master,local[2]), (spark.home,C:\spark-2.2.1-bin-hadoop2.7\bin\..))</strong></pre></div>
<div class="packt_infobox"><span>There may be references to <kbd>sqlContext</kbd> and <kbd>sqlContext.implicits._</kbd> in the Spark shell. What is <kbd>sqlContext</kbd>? As of Spark 2 and the preceding versions, <kbd>sqlContext</kbd> is deprecated and <kbd>SparkSession.builder</kbd> is used instead to return a <kbd>SparkSession</kbd> instance, which we reiterate is the entry point to programming Spark with the dataset and dataframe API. Hence, we are going to ignore those <kbd>sqlContext</kbd> instances and focus on</span> <kbd>SparkSession</kbd> instead. </div>
<p>Note that <kbd>spark.app.name</kbd> bears the default name <kbd>spark-shell</kbd>. Let's assign a different name to the <kbd>app-name</kbd> property as <kbd>Iris-Pipeline</kbd>. We do this by invoking the <kbd>setAppName</kbd> method and passing to it the new app name, as follows:</p>
<pre><strong>scala&gt; sc.getConf.setAppName("Iris-Pipeline")</strong><br/><strong>res22: org.apache.spark.SparkConf = org.apache.spark.SparkConf@e8ce5b1</strong></pre>
<p>To check if the configuration change took effect, let's invoke the <kbd>getAll</kbd> method again. The following output should reflect that change. It simply illustrates how <kbd>SparkContext</kbd> can be used to modify our cluster environment:</p>
<pre><strong>scala&gt; sc.conf.getAll</strong><br/><strong>res20: Array[(String, String)] = Array((spark.driver.port,51576), (spark.app.name,Spark shell), (spark.sql.catalogImplementation,hive), (spark.repl.class.uri,spark://192.168.56.1:51576/classes), (spark.debug.maxToStringFields,150), (spark.jars,""), (spark.repl.class.outputDir,C:\Users\Ilango\AppData\Local\Temp\spark-47fee33b-4c60-49d0-93aa-3e3242bee7a3\repl-e5a1acbd-6eb9-4183-8c10-656ac22f71c2), (spark.executor.id,driver), (spark.submit.deployMode,client), (spark.driver.host,192.168.56.1), (spark.app.id,local-1520484594646), (spark.master,local[2]), (spark.home,C:\spark-2.2.1-bin-hadoop2.7\bin\..))</strong></pre>
<p>The <kbd>spark.app.name</kbd> property just had its value updated to the new name. Our goal in the next section is to use <kbd>spark-shell</kbd> to analyze data in an interactive fashion. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a simple interactive data analysis utility</h1>
                </header>
            
            <article>
                
<p>We will develop a simple Scala program in the Spark shell's interactive Scala shell. We will restate our goal, which is that we want to be able to analyze data interactively. That dataset—<span>an external <strong>comma-separated values</strong> (<strong>CSV</strong>) file called <kbd>iris.csv</kbd>—</span>resides in the same folder where <kbd>spark-shell</kbd> is launched from.</p>
<p class="mce-root"/>
<p>This program, which could just as well be written in a <span>regular Scala <strong>Read Eval Print Loop </strong>(<strong>REPL</strong>) shell,</span> reads a file, and prints out its contents, getting a data analysis task done. However, what is important here is that the Spark shell is flexible in that it also allows you to write Scala code that will allow you to easily connect your data with various Spark APIs and derive abstractions, such as dataframes or RDDs, in some useful way. More about <kbd>DataFrame</kbd> and <kbd>Dataset</kbd> to follow:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b8fa8a32-975c-4def-9bca-edae287ad5f0.jpg" style="width:43.42em;height:26.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Reading iris.csv with source</div>
<p>In the preceding program, nothing fancy is happening. We are trying to read a file called <kbd>iris.csv</kbd> using the <kbd>Source</kbd> class. We import the <kbd>Source.scala</kbd> file from the <kbd>scala.io</kbd> package and from there on, we create an object called <kbd>DataReader</kbd> and a <kbd>main</kbd> method inside it. Inside the <kbd>main</kbd> method, we invoke the <kbd>fromFile</kbd> method of the companion object <kbd>Source</kbd>. The <kbd>fromFile</kbd> method takes in a string representation of the dataset file path as an argument and returns a <kbd>BufferedSource</kbd> instance, which we assign to a <kbd>val</kbd> that we name <kbd>datasrc</kbd>. By the way, the API for <kbd>Source</kbd> can be found at <a href="https://www.scala-lang.org/api/current/scala/io/Source.html">https://www.scala-lang.org/api/current/scala/io/Source.html.</a></p>
<p class="mce-root"/>
<p>On the <kbd>BufferedSource</kbd> handle, <span>we then invoke the <kbd>getLines</kbd> method that </span>returns an iterator, which in turn invokes <kbd>foreach</kbd> that will print out all the lines in <kbd>iris.csv</kbd> minus the newline characters. We wrap all of this code in a <kbd>try</kbd> and a <kbd>catch</kbd> and a <kbd>finally</kbd>. The <kbd>finally</kbd> construct exists for a reason and that has to do with the fact that we need to close the <kbd>BufferedSource</kbd> instance <kbd>datasrc</kbd> after it is done working on the file.</p>
<p>Initially, we ran into a <kbd>FileNotFoundException</kbd> because the dataset file <kbd>iris.csv</kbd> was not found. The CSV file is then dropped in, the program is run, and the output is what we expect.</p>
<p>That wasn't so hard. In the next subsection, the goal is to read our <kbd>iris.csv</kbd> file and derive <kbd>Dataset</kbd> or <kbd>DataFrame</kbd> out of it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading a data file and deriving DataFrame out of it</h1>
                </header>
            
            <article>
                
<p>The Spark API for <a href="https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset">https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset</a> has it that a <kbd>DataFrame</kbd> is <kbd>Dataset[Row]</kbd> and that <kbd>Dataset</kbd> contains a view called <kbd>DataFrame</kbd>. Falling back to the <span>description of <kbd>Dataset</kbd> in the </span>Spark documentation, we can redefine <kbd>Dataset</kbd> as a Spark abstraction of distributed collections holding data items. That said, <kbd>Dataset[Row]</kbd> contains rows. <kbd>Row</kbd> could be an abstraction representing a row from the raw file dataset.</p>
<p>We need to read the <kbd>iris.csv</kbd> file and transform it into <kbd>DataFrame</kbd>. That is the stated goal of this subsection and that is exactly what we shall accomplish very soon.</p>
<p>With all this in mind, lets get down to building <kbd>DataFrame</kbd>. We start by invoking the <kbd>read</kbd> method on <kbd>spark</kbd>, our <kbd>SparkSession</kbd>: </p>
<pre><strong>scala&gt; val dfReader1 = spark.read</strong><br/><strong>dfReader1: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c</strong></pre>
<p class="mce-root"><span>The <kbd>read()</kbd> invoke produced <kbd>DataFrameReader</kbd> <kbd>dfReader1</kbd>, which according to <a href="https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader">https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader</a> </span><span>is an interface to load a dataset from external storage systems.</span></p>
<p>Next, we will inform Spark that our data is in CSV format. This is done by invoking the <kbd>format</kbd> method with a <kbd>com.databricks.spark.csv</kbd> argument that Spark recognizes:</p>
<pre><strong>scala&gt; val dfReader2 = dfReader1.format("com.databricks.spark.csv")</strong><br/><strong>dfReader2: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>format</kbd> method simply returned <kbd>DataFrameReader</kbd> again. The <kbd>iris.csv</kbd> file contains <kbd>header</kbd>. We could specify this as an input <kbd>option</kbd>:</p>
<pre><strong>scala&gt; val dfReader3 = dfReader2.option("header", true)</strong><br/><strong>dfReader3: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c</strong></pre>
<p>That returned our same old <kbd>DataFrameReader</kbd>.</p>
<p>What we need next is a way to identify the schema for us. Invoking the <kbd>option</kbd> method again with a key <kbd>inferSchema</kbd> and a value of <kbd>true</kbd> lets Spark infer the schema automatically for us:</p>
<pre><strong>scala&gt; val dfReader4 = dfReader3.option("inferSchema",true)</strong><br/><strong>dfReader4: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@66df362c</strong></pre>
<p>Let's <kbd>load</kbd> our input now:</p>
<pre><strong>scala&gt; val dFrame = dfReader4.load("iris.csv")</strong><br/><strong>dFrame: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields]</strong></pre>
<p><kbd>DataFrameReader</kbd> transformed our input CSV into <kbd>DataFrame</kbd>! This was exactly what we set out to do. </p>
<div class="packt_infobox"><kbd>DataFrame</kbd> is simply an untyped view of <kbd>Dataset</kbd> as <kbd>type DataFrame = Dataset[Row]</kbd>.</div>
<p>With our <kbd>DataFrame</kbd> being a view on <kbd>Dataset[Row]</kbd>, all the methods on <kbd>Dataset</kbd> are available.</p>
<p>For now, we want to see what this dataset has in it. The raw file had 150 columns in it. Therefore, we want Spark to:</p>
<ul>
<li>Return the row count in our dataset</li>
<li>Display the top 20 rows of our dataset</li>
</ul>
<p>Next, we will invoke the <kbd>count</kbd> method. We want to reaffirm the <span>number of rows contained in the dataset:</span></p>
<pre><strong>scala&gt; dFrame.count</strong><br/><strong>res1: Long = 150</strong></pre>
<p class="mce-root"/>
<p>We just invoked the <kbd>count</kbd> method on our <kbd>DataFrame</kbd>. That returned the number <kbd>150</kbd>, which is right.</p>
<p>Next, we will bring together all of the code developed in this section into one line of code: </p>
<pre><strong>scala&gt; val irisDataFrame = spark.read.format("com.databricks.spark.csv").option("header",true).option("inferSchema", true).load("iris.csv").show</strong><br/><br/></pre>
<div>
<p>We just created <kbd>DataFrame</kbd> <kbd>irisDataFrame</kbd> . If you want to view the DataFrame,  just invoke the <kbd>show</kbd> method on it. This will return the first 20 rows of the irisDataFrame  <kbd>DataFrame</kbd>:</p>
</div>
<div class="mce-root CDPAlignCenter CDPAlign"> <img src="assets/2667d94e-9b3a-41bd-802c-c9719b96c6df.jpg" style="width:35.08em;height:20.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">First 20 rows of the Iris dataset</div>
<p>At this point, type <kbd>:quit</kbd> or <em>Ctrl</em> + <em>D</em> to exit the Spark shell. This wraps up this section, but opens a segue to the next, where we take things to the next level. Instead of relying on <kbd>spark-shell</kbd> to develop a larger program, we will create our Iris prediction pipeline program in an SBT project. This is the focus of the next section.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the Iris pipeline </h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will set forth what our pipeline implementation objectives are. We will document tangible results as we step through individual implementation steps.</p>
<p class="mce-root">Before we implement the Iris pipeline, we want to understand what a pipeline is <span>from a conceptual and practical perspective. Therefore, we define </span>a pipeline as a <kbd>DataFrame</kbd> processing workflow with multiple pipeline stages operating in a certain sequence.</p>
<p>A DataFrame is a Spark abstraction that provides an API. This API lets us work with collections of objects.  At a high-level it represents a distributed collection holding rows of data, much like a relational database table. Each member of a row (for example, a Sepal-Width measurement) in this DataFrame falls under a named column called Sepal-Width.</p>
<p class="mce-root">Each stage in a pipeline is an algorithm that is either a  <kbd>Transformer</kbd> or an  <kbd>Estimator</kbd>.   As a <kbd>DataFrame</kbd> or DataFrame(s) flow through the pipeline,  two types of  stages (algorithms) exist:</p>
<ul>
<li><span><kbd>Transformer</kbd> stage:  This involves a transformation action that transforms one <kbd>DataFrame</kbd> into another <kbd>DataFrame</kbd></span></li>
<li><kbd>Estimator</kbd><span> stage: This involves a </span>training action on a <kbd>DataFrame</kbd> that produces another <kbd>DataFrame</kbd>. </li>
</ul>
<p>In summary, a pipeline is a single unit, requiring stages, but inclusive of parameters and DataFrame(s). The entire pipeline structure is listed as follows:</p>
<ul>
<li><kbd>Transformer</kbd></li>
<li><kbd>Estimator</kbd></li>
<li><kbd>Parameters</kbd> (hyper or otherwise)</li>
<li><kbd>DataFrame</kbd></li>
</ul>
<p>This is where Spark comes in. Its MLlib library provides a set of pipeline APIs allowing developers to access multiple algorithms and facilitates their combining into a single pipeline of ordered stages, much like a sequence of choreographed motions in a ballet. In this chapter, we will use the random forest classifier.</p>
<p>We covered essential pipeline concepts. These are practicalities that will help us move into the section, where we will list implementation objectives.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iris pipeline implementation objectives</h1>
                </header>
            
            <article>
                
<p><span>Before listing the implementation objectives, we will lay out an architecture for our pipeline. Shown here under are two diagrams representing an ML workflow, a pipeline.</span></p>
<p><span>The following diagrams together help in understanding the different components of this project. </span><span>That said, this p</span>ipeline involves training (fitting), transformation, and validation operations. More than one model is trained and the best model (or mapping function) is selected to give us an accurate approximation predicting the species of an Iris flower (based on measurements of those flowers):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ea4c627-29bd-46c6-b668-41beeb8136f5.png" style="width:31.75em;height:30.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Project block diagram</span></div>
<p class="mce-root"/>
<p>A breakdown of the project block diagram is as follows:</p>
<ul>
<li> <strong>Spark</strong>, which represents the Spark cluster and its ecosystem</li>
<li class="mce-root"><strong>Training dataset</strong></li>
<li><strong>Model</strong></li>
<li><strong>Dataset attributes</strong> or feature measurements</li>
<li>An <strong>inference</strong> process, that produces a prediction column</li>
</ul>
<p>The following diagram represents a more detailed description of the different phases in terms of the functions performed in each phase. Later we will come to visualize pipeline in terms of its constituent stages.</p>
<p>For now, the diagram depicts four stages, starting with a <strong>data pre-processing</strong> phase, which is considered separate from the numbered phases deliberately. Think of the pipeline as a two-step process:</p>
<ol>
<li> A <strong>data cleansing</strong> phase, or <strong>pre-processing</strong> phase. An important phase that could include a subphase of <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>) (not explicitly depicted in the latter diagram).</li>
<li>A data analysis phase that begins with <strong>Feature Extraction</strong>, followed by <strong>Model Fitting</strong>, and <strong>Model validation</strong>, all the way to deployment of an Uber pipeline JAR into Spark:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/521c0e06-1391-4a31-9d5b-e5d9de1e7f9a.png" style="width:25.33em;height:32.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Pipeline diagram</div>
<p><span>Referring to the preceding diagram, the first implementation objective is to set up Spark inside an SBT project. An SBT project is a self-contained application, which we can run on the command line to predict Iris labels. In the </span>SBT project,  dependencies are specified in a <kbd>build.sbt</kbd> file and our application code will create its  own  <kbd>SparkSession</kbd> and <kbd>SparkContext</kbd>.</p>
<p>So that brings us to a listing of implementation objectives and these are as follows:</p>
<ol>
<li>Get the Iris dataset from the UCI Machine Learning Repository</li>
<li>Conduct preliminary EDA in the Spark shell</li>
<li>Create a new Scala project in IntelliJ, and carry out all implementation steps, until the evaluation of the Random Forest classifier</li>
<li><span>Deploy the application to your local Spark cluster</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – getting the Iris dataset from the UCI Machine Learning Repository</h1>
                </header>
            
            <article>
                
<p>Head over to the UCI Machine Learning Repository website at <a href="https://archive.ics.uci.edu/ml/datasets/iris">https://archive.ics.uci.edu/ml/datasets/iris</a> and click on <span class="packt_screen">Download:</span> <span class="packt_screen">Data Folder</span>. Extract this folder someplace convenient and copy over <kbd>iris.csv</kbd> into the root of your project folder.</p>
<p>You may refer back to the project overview for an in-depth description of the Iris dataset. We depict the contents of the <kbd>iris.csv</kbd> file here, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/71fbb790-c523-461f-b89a-951043a47d3b.jpg" style="width:23.58em;height:40.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A snapshot of the Iris dataset with 150 sets</div>
<p>You may recall that the <kbd>iris.csv</kbd> file is a 150-row file, with comma-separated values. </p>
<p>Now that we have the dataset, the first step will be performing EDA on it. The Iris dataset is multivariate, meaning there is more than one (independent) variable, so we will carry out a basic multivariate EDA on it. But we need <kbd>DataFrame</kbd> to let us do that. How we create a dataframe as a prelude to EDA is the goal of the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – preliminary EDA</h1>
                </header>
            
            <article>
                
<p>Before we get down to building the <span>SBT pipeline project, we will conduct a preliminary EDA in <kbd>spark-shell</kbd>. The plan is to derive a dataframe out of the dataset and then calculate basic statistics on it.</span></p>
<p>We have three tasks at hand for <kbd>spark-shell</kbd>:</p>
<ol>
<li>Fire up <kbd>spark-shell</kbd></li>
<li>Load the <kbd>iris.csv</kbd> file and build <kbd>DataFrame</kbd></li>
<li>Calculate the statistics</li>
</ol>
<p><span>We will then port that code over to a Scala file inside our SBT project. </span></p>
<p>That said, let's get down to loading the <span><kbd>iris.csv</kbd> file (i</span><span>nputting the data source) before eventually </span>building <kbd>DataFrame</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Firing up Spark shell</h1>
                </header>
            
            <article>
                
<p>Fire up the Spark Shell by issuing the following command on the command line.</p>
<pre>spark-shell --master local[2]</pre>
<p>In the next step, we start with the available Spark session 'spark'.  'spark' will be our entry point to programming with Spark. It also holds properties required to connect to our Spark (local) cluster. With this information, our next goal is to load the iris.csv file and produce a DataFrame</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the iris.csv file and building a DataFrame</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first step to loading the iris csv file is to invoke the <kbd>read</kbd> method on <kbd>spark</kbd>. The <kbd>read</kbd> method returns <kbd>DataFrameReader</kbd>, which can be used to read our dataset:</p>
<pre>val dfReader1 = spark.read<br/>dfReader1: org.apache.spark.sql.DataFrameReader=org.apache.spark.sql.DataFrameReader@6980d3b3</pre>
<p><kbd>dfReader1</kbd> is of type <kbd>org.apache.spark.sql.DataFrameReader</kbd>. Calling the <kbd>format</kbd> method on <kbd>dfReader1</kbd> with Spark's <kbd>com.databricks.spark.csv</kbd> CSV format-specifier string returns <kbd>DataFrameReader</kbd> again:</p>
<pre>val dfReader2 = dfReader1.format("com.databricks.spark.csv")<br/>dfReader2: org.apache.spark.sql.DataFrameReader=org.apache.spark.sql.DataFrameReader@6980d3b3</pre>
<p>After all, <kbd>iris.csv</kbd> is a CSV file. </p>
<p>Needless to say, <kbd>dfReader1</kbd> and <kbd>dfReader2</kbd> are the same <kbd>DataFrameReader</kbd> instance.</p>
<p>At this point, <kbd>DataFrameReader</kbd> needs an input data source <kbd>option</kbd> in the form of a key-value pair. Invoke the <kbd>option</kbd> method with two arguments, a key <kbd>"header"</kbd> of type string and its value <kbd>true</kbd> of type Boolean:</p>
<pre>val dfReader3 = dfReader2.option("header", true)</pre>
<p>In the next step, we invoke the <kbd>option</kbd> method again with an argument <kbd>inferSchema</kbd> and a <kbd>true</kbd> value:</p>
<pre style="padding-left: 30px">val dfReader4 = dfReader3.option("inferSchema", true)</pre>
<p>What is <kbd>inferSchema</kbd> doing here? We are simply telling Spark to guess the schema of our input data source for us.</p>
<p>Up until now, we have been preparing <kbd>DataFrameReader</kbd> to load <kbd>iris.csv</kbd>. External data sources require a path for Spark to load the data for <kbd>DataFrameReader</kbd> to process and spit out <kbd>DataFrame</kbd>. </p>
<p>The time is now right to invoke the <kbd>load</kbd> method on <kbd>DataFrameReader</kbd> <kbd>dfReader4</kbd>. Pass into the <kbd>load</kbd> method the path to the Iris dataset file. In this case, the file is right under the root of the project folder:</p>
<pre>val dFrame1 = dfReader4.load("iris.csv")<br/>dFrame1: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields]</pre>
<p>That's it. We now have <kbd>DataFrame</kbd>!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating statistics</h1>
                </header>
            
            <article>
                
<p>Invoking the <kbd>describe</kbd> method on this <kbd>DataFrame</kbd> should cause Spark to perform a basic statistical analysis on each column of <kbd>DataFrame</kbd>: </p>
<pre>dFrame1.describe("Id","SepalLengthCm","SepalWidthCm","PetalLengthCm","PetalWidthCm","Species")<br/><span>WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.</span><br/><span>res16: org.apache.spark.sql.DataFrame = [summary: string, Id: string ... 5 more fields]</span></pre>
<p><span><span>Lets fix the <kbd>WARN.Utils</kbd> issue described in the preceding code block. </span></span>The fix is to locate the file <kbd>spark-defaults-template.sh</kbd> under <kbd>SPARK_HOME/conf</kbd> and save it as <kbd>spark-defaults.sh</kbd>.</p>
<p>At the bottom of this file, add an entry for <kbd>spark.debug.maxToStringFields</kbd>. The following screenshot illustrates this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/46d332b7-6858-429d-a493-bbb6b451cb28.jpg" style="width:33.67em;height:23.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Fixing the WARN Utils problem in <span>spark-defaults.sh</span></div>
<p>Save the file and restart <kbd>spark-shell</kbd>.</p>
<p class="mce-root"/>
<p>Now, inspect the updated Spark configuration again. We updated the value of <kbd>spark.debug.maxToStringFields</kbd> in the <kbd>spark-defaults.sh</kbd> file. This change is supposed to fix the truncation problem reported by Spark. We will confirm imminently that the change we made caused Spark to update its configuration also. That is easily done by inspecting <kbd>SparkConf</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inspecting your SparkConf again</h1>
                </header>
            
            <article>
                
<p>As before, invoking the <kbd>getConf</kbd> returns the <kbd>SparkContext</kbd> instance that stores configuration values. Invoking <kbd>getAll</kbd> on that instance returns an <kbd>Array</kbd> of configuration values. One of those values is an updated value of <kbd>spark.debug.maxToStringFields</kbd>:</p>
<pre>sc.getConf.getAll<br/>res4: Array[(String, String)] = Array((spark.repl.class.outputDir,C:\Users\Ilango\AppData\Local\Temp\spark-10e24781-9aa8-495c-a8cc-afe121f8252a\repl-c8ccc3f3-62ee-46c7-a1f8-d458019fa05f), (spark.app.name,Spark shell), (spark.sql.catalogImplementation,hive), (spark.driver.port,58009), <strong>(spark.debug.maxToStringFields,150),</strong></pre>
<p>That updated value for <kbd>spark.debug.maxToStringFields</kbd> is now <kbd>150</kbd>.</p>
<div class="packt_infobox"><span><kbd>spark.debug.maxToStringFields</kbd> had a </span>default value of <kbd>25</kbd> inside a private object called <kbd>Utils</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating statistics again</h1>
                </header>
            
            <article>
                
<p>Run the invoke on the dataframe <kbd>describe</kbd> method and pass to it column names:</p>
<pre>val dFrame2 =  dFrame1.describe("Id","SepalLengthCm","SepalWidthCm","PetalLengthCm","PetalWidthCm","Species"<br/>)<br/>dFrame2: org.apache.spark.sql.DataFrame = [summary: string, Id: string ... 5 more fields]</pre>
<p><span>The invoke on the <kbd>describe</kbd> method of <kbd>DataFrame</kbd> <kbd>dfReader</kbd> results in a transformed <kbd>DataFrame</kbd> that we call dFrame2.  On dFrame2, we invoke the <kbd>show</kbd> method to return a table of statistical results. This completes the first phase of a basic yet important EDA: </span></p>
<pre>val dFrame2Display= = dfReader2.show</pre>
<p class="mce-root"/>
<p>The results of the statistical analysis are shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/44bb7dee-e4c6-4dde-8476-ccc52bbd88ca.jpg" style="width:57.33em;height:6.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Results of statistical analysis</div>
<p>We did all that extra work simply to demonstrate the individual data reading, loading, and transformation stages. Next, we will wrap all of our previous work in one line of code:</p>
<pre>val dfReader = spark.read.format("com.databricks.spark.csv").option("header",true).option("inferSchema",true).load("iris.csv")<br/>dfReader: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields] </pre>
<p>That completes the EDA on <kbd>spark-shell</kbd>. In the next section, we undertake steps to implement, build (using SBT), deploy (using <kbd>spark-submit</kbd>), and execute our Spark pipeline application. We start by creating a skeletal SBT project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – creating an SBT project</h1>
                </header>
            
            <article>
                
<p>Lay out your SBT project in a folder of your choice and name it <kbd>IrisPipeline</kbd> or any name that makes sense to you. This will hold all of our files needed to implement and run the pipeline on the Iris dataset. </p>
<p>The structure of our SBT project looks like the following: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ff1f985f-e989-4cd6-944f-9b8fd4a473be.jpg" style="width:33.25em;height:10.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Project structure</div>
<p class="mce-root"/>
<p><span>We will list dependencies in the <kbd>build.sbt</kbd> file. This is going to be an SBT project. Hence, we will bring in the following key libraries:</span></p>
<ul>
<li>Spark Core</li>
<li>Spark <span>MLlib</span></li>
<li>Spark SQL</li>
</ul>
<p>The following screenshot illustrates the <kbd>build.sbt</kbd> file:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/92bebe43-485c-4ddb-98b2-5cff6f2efa44.jpg" style="width:39.75em;height:22.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The build.sbt file with Spark dependencies</div>
<p>The <kbd>build.sbt</kbd> file referenced in the preceding snapshot is readily available for you in the book's download bundle. Drill down to the folder <kbd>Chapter01</kbd> code under <kbd>ModernScalaProjects_Code</kbd> and copy the folder over to a convenient location on your computer.</p>
<p>Drop the <kbd>iris.csv</kbd> file that you downloaded in <em>Step 1 – getting the Iris dataset from the UCI Machine Learning Repository</em> into the root folder of our new SBT project. Refer to the earlier screenshot that depicts the updated project structure with the <kbd>iris.csv</kbd> file inside of it.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – creating Scala files in SBT project</h1>
                </header>
            
            <article>
                
<p>Step 4 is broken down into the following steps:</p>
<ol>
<li>Create the Scala file <kbd>iris.scala</kbd> in the <kbd>com.packt.modern.chapter1</kbd> package.</li>
<li>Up until now, we relied on <kbd>SparkSession</kbd> and <kbd>SparkContext</kbd>, which <kbd>spark-shell</kbd> gave us. This time around, we need to create <kbd>SparkSession</kbd>, which will, in turn, give us <kbd>SparkContext</kbd>.</li>
</ol>
<p>What follows is how the code is laid out in the <kbd>iris.scala</kbd> file.</p>
<p>In <kbd>iris.scala</kbd>, after the package statement, place the following <kbd>import</kbd> statements:</p>
<pre><span>import </span>org.apache.spark.sql.SparkSession</pre>
<p> Create <kbd>SparkSession</kbd> inside a trait, which we shall call <kbd>IrisWrapper</kbd>:</p>
<pre>lazy val session: SparkSession = SparkSession.builder().getOrCreate()</pre>
<p>Just one <kbd>SparkSession</kbd> is made available to all classes extending from <kbd>IrisWrapper</kbd>. Create <kbd>val</kbd> to hold the <kbd>iris.csv</kbd> file path:</p>
<pre>val dataSetPath = "&lt;&lt;path to folder containing your iris.csv file&gt;&gt;\\iris.csv"</pre>
<p>Create a method to build <kbd>DataFrame</kbd>. <span>This method takes in the complete path to the Iris dataset path as <kbd>String</kbd> and returns <kbd>DataFrame</kbd></span>:</p>
<pre><span>def </span>buildDataFrame(dataSet: <span>String</span>): <span>DataFrame </span>= {<br/>/*<br/> The following is an example of a dataSet parameter string: "C:\\Your\\Path\\To\\iris.csv"<br/>*/</pre>
<p>Import the <kbd>DataFrame</kbd> class by updating the previous <kbd>import</kbd> statement for <kbd>SparkSession</kbd>:</p>
<pre>import org.apache.spark.sql.{DataFrame, SparkSession}</pre>
<p>Create a nested function inside <kbd>buildDataFrame</kbd> to process the raw dataset. Name this function <kbd>getRows</kbd>. <kbd>getRows</kbd> which takes no parameters but returns <kbd>Array[(Vector, String)]</kbd>. The <kbd>textFile</kbd> method on the <kbd>SparkContext</kbd> variable processes the <span><kbd>iris.csv</kbd> into <kbd>RDD[String]</kbd></span>:</p>
<pre>val result1: Array[String] = session.sparkContext.textFile(&lt;&lt;path to iris.csv represented by the dataSetPath variable&gt;&gt;)</pre>
<p>The resulting RDD contains two partitions. Each partition, in turn, contains rows of strings separated by a newline character, <kbd>'\n'</kbd>. Each row in the RDD represents its original counterpart in the raw data.</p>
<p>In the next step, we will attempt several data transformation steps. <span>We start by applying a <kbd>flatMap</kbd> operation over the RDD, </span>culminating in the <kbd>DataFrame</kbd> creation. <span><kbd>DataFrame</kbd> is a view over <kbd>Dataset</kbd>, which happens to the fundamental data abstraction unit in the Spark 2.0 line.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – preprocessing, data transformation, and DataFrame creation</h1>
                </header>
            
            <article>
                
<p>We will get started by invoking <kbd>flatMap</kbd>, by passing a function block to it, and successive transformations listed as follows, eventually resulting in <kbd>Array[(org.apache.spark.ml.linalg.Vector, String)]</kbd>. A vector represents a row of feature measurements. </p>
<p>The Scala code to give us <kbd>Array[(org.apache.spark.ml.linalg.Vector, String)]</kbd> is as follows:</p>
<pre>//Each line in the RDD is a row in the Dataset represented by a String, which we can 'split' along the new //line character<br/>val result2: RDD[String] = result1.flatMap { partition =&gt; partition.split("\n").toList }<br/><br/>//the second transformation operation involves a split inside of each line in the dataset where there is a //comma separating each element of that line<br/>val result3: <span>RDD[Array[String]]</span> = result2.map(_.split(","))</pre>
<p>Next, drop the <kbd>header</kbd> column, but not before doing a collection that returns an <kbd>Array[Array[String]]</kbd>:</p>
<pre>val result4: <span>Array[Array[String]] </span>= result3.collect.drop(1)</pre>
<p>The header column is gone; now import the <kbd>Vectors</kbd> class:</p>
<pre>import org.apache.spark.ml.linalg.Vectors</pre>
<p>Now, transform <span><kbd>Array[Array[String]]</kbd> into</span> <kbd>Array[(Vector, String)]</kbd>:</p>
<pre class="mce-root">val result5 = result4.map(row =&gt; (Vectors.dense(row(1).toDouble, row(2).toDouble, row(3).toDouble, row(4).toDouble),row(5)))</pre>
<p>The last step remaining is to create a final DataFrame</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DataFrame Creation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, we invoke the <kbd>createDataFrame</kbd> method with a parameter, <kbd>getRows</kbd>. This returns <kbd>DataFrame</kbd> with <kbd>featureVector</kbd> and <kbd>speciesLabel</kbd> (for example, Iris-setosa):</p>
<pre>val dataFrame = spark.createDataFrame(result5).toDF(featureVector, speciesLabel)</pre>
<p>Display the top 20 rows in the new dataframe:</p>
<pre><strong>dataFrame.show</strong><br/><strong>+--------------------+-------------------------+</strong><br/><strong>|iris-features-column|iris-species-label-column|</strong><br/><strong>+--------------------+-------------------------+</strong><br/><strong>| [5.1,3.5,1.4,0.2]| Iris-setosa|</strong><br/><strong>| [4.9,3.0,1.4,0.2]| Iris-setosa|</strong><br/><strong>| [4.7,3.2,1.3,0.2]| Iris-setosa|</strong><br/><strong>.....................</strong><br/><strong>.....................</strong><br/><strong>+--------------------+-------------------------+</strong><br/><strong>only showing top 20 rows</strong></pre>
<p>We need to index the species label column by converting the strings Iris-setosa, Iris-virginica, and Iris-versicolor into doubles. We will use a <kbd>StringIndexer</kbd> to do that.</p>
<p>Now create a file called <kbd>IrisPipeline.scala</kbd>.</p>
<p>Create an object <kbd>IrisPipeline</kbd> that extends our <kbd>IrisWrapper</kbd> trait:</p>
<pre><span>object IrisPipeline extends IrisWrapper {</span> </pre>
<p>Import the <kbd>StringIndexer</kbd> algorithm class:</p>
<pre><span>import </span>org.apache.spark.ml.feature.StringIndexer</pre>
<p>Now create a <kbd>StringIndexer</kbd> algorithm instance. The <kbd>StringIndexer</kbd> will map our species label column to an indexed learned column:</p>
<pre><span>val indexer = new StringIndexer().setInputCol<br/>(irisFeatures_CategoryOrSpecies_IndexedLabel._2).setOutputCol(irisFeatures_CategoryOrSpecies_IndexedLabel._3)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 – creating, training, and testing data</h1>
                </header>
            
            <article>
                
<p>Now, let's split our dataset in two by providing a random seed:</p>
<pre><span>val splitDataSet: Array[org.apache.spark.sql.Dataset<br/>[org.apache.spark.sql.Row]] = dataSet.randomSplit(Array(0.85, 0.15), 98765L)</span></pre>
<p>Now our new <kbd>splitDataset</kbd> contains two datasets:</p>
<ul>
<li><strong>Train dataset:</strong> A dataset containing <kbd>Array[(Vector, iris-species-label-column: String)]</kbd></li>
<li><strong>Test dataset:</strong> A<span> dataset containing <kbd>Array[(Vector, iris-species-label-column: String)]</kbd></span></li>
</ul>
<p>Confirm that the new dataset is of size <kbd>2</kbd>:</p>
<pre>splitDataset.size<br/>res48: Int = 2</pre>
<p>Assign the training dataset to a variable, <kbd>trainSet</kbd>:</p>
<pre><span>val trainDataSet = splitDataSet(0)</span><br/>trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iris-features-column: vector, iris-species-label-column: string]</pre>
<p>Assign the testing dataset to a variable, <kbd>testSet</kbd>:</p>
<pre><span>val testDataSet = splitDataSet(1)</span><br/>testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [iris-features-column: vector, iris-species-label-column: string]</pre>
<p>Count the number of rows in the training dataset:</p>
<pre>trainSet.count<br/>res12: Long = 14</pre>
<p>Count the number of rows in the testing dataset:</p>
<pre>testSet.count<br/>res9: Long = 136</pre>
<p>There are 150 rows in all.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 7 – creating a Random Forest classifier</h1>
                </header>
            
            <article>
                
<p>In reference to Step 5 - DataFrame Creation. This DataFrame 'dataFrame' contains <span><span>column names that corresponds to the columns present in the DataFrame produced in that step</span></span></p>
<p><span>The first step to create a classifier is to  </span>pass<span> into it (hyper) parameters. A fairly comprehensive list of parameters look like this:</span></p>
<ul>
<li>From 'dataFrame' we need the Features column name - <strong>iris-features-column</strong></li>
<li>From 'dataFrame' we also need the Indexed label column name - <strong>iris-species-label-column</strong></li>
<li><span>The </span><kbd>sqrt</kbd><span> setting for </span><kbd>featureSubsetStrategy</kbd><span> </span></li>
<li>Number of features to be considered per split (we have 150 observations and four features that will make our<span> </span><kbd>max_features</kbd> value<span> </span><kbd>2</kbd>)</li>
<li><span>Impurity settings—values can be gini and entropy</span></li>
<li><span>Number of trees to train (since the number of trees is greater than one, we set a t</span><span>ree maximum depth), which is a number equal to the number of nodes</span></li>
<li><span>The required minimum number of feature measurements (sampled observations), also known as the minimum instances per node</span></li>
</ul>
<p>Look at the <kbd>IrisPipeline.scala</kbd> file for values of each of these parameters.</p>
<p>But this time, we will employ an exhaustive grid search-based model selection process based on combinations of parameters, where parameter value ranges are specified.</p>
<p>Create a <kbd>randomForestClassifier</kbd> instance. Set the features and <kbd>featureSubsetStrategy</kbd>:</p>
<pre><span>val </span><span>randomForestClassifier </span>= <span>new </span>RandomForestClassifier()<br/>  .setFeaturesCol(<span>irisFeatures_CategoryOrSpecies_IndexedLabel</span>._1)<br/>  .setFeatureSubsetStrategy(<span>"sqrt"</span>)</pre>
<p>Start building <kbd>Pipeline</kbd>, which has two stages, <kbd>Indexer</kbd> and <kbd>Classifier</kbd>:</p>
<pre><span>val </span><span>irisPipeline </span>= <span>new </span>Pipeline().setStages(<span>Array</span>[PipelineStage](<span>indexer</span>) ++  <span>Array</span>[PipelineStage](<span>randomForestClassifier</span>))</pre>
<p>Next, set the hyperparameter <kbd>num_trees</kbd> (number of trees) on the classifier to <kbd>15</kbd>, a <kbd>Max_Depth</kbd> parameter, and an impurity with two possible values of gini and entropy.</p>
<p>Build a parameter grid with all three hyperparameters:</p>
<pre>val finalParamGrid: Array[ParamMap] = gridBuilder3.build()</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 8 – training the Random Forest classifier</h1>
                </header>
            
            <article>
                
<p>Next, we want to split our training set into a validation set and a training set:</p>
<pre><span>val </span><span>validatedTestResults</span>: <span>DataFrame </span>= <span>new </span>TrainValidationSplit()</pre>
<p>On this variable, set <kbd>Seed</kbd>, set <kbd>EstimatorParamMaps</kbd>, set <kbd>Estimator</kbd> with <kbd>irisPipeline</kbd>, and set a training ratio to <kbd>0.8</kbd>:</p>
<pre><span>val </span><span>validatedTestResults</span>: <span>DataFrame </span>= <span>new </span>TrainValidationSplit().setSeed(1234567L).setEstimator(irisPipeline)</pre>
<p>Finally, do a fit and a transform with our training dataset and testing dataset. Great! Now the classifier is trained. In the next step, we will apply this classifier to testing the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 9 – applying the Random Forest classifier to test data</h1>
                </header>
            
            <article>
                
<p>The purpose of our validation set is to be able to make a choice between models. We want an evaluation metric and hyperparameter tuning. We will now create an instance of a validation estimator called <kbd>TrainValidationSplit</kbd>, which will split the training set into a validation set and a training set:</p>
<pre><span>val validatedTestResults.setEvaluator(new MulticlassClassificationEvaluator())</span></pre>
<p>Next, we fit this estimator over the training dataset to produce a model and a transformer that we will use to transform our testing dataset. <span>Finally, we perform a validation for hyperparameter tuning by </span>applying an evaluator for a metric.</p>
<p>The new <kbd>ValidatedTestResults</kbd> <kbd>DataFrame</kbd> should look something like this:</p>
<pre><strong>--------+</strong><br/><strong> |iris-features-column|iris-species-column|label| rawPrediction| probability|prediction|</strong><br/><strong> +--------------------+-------------------+-----+--------------------+</strong><br/><strong> | [4.4,3.2,1.3,0.2]| Iris-setosa| 0.0| [40.0,0.0,0.0]| [1.0,0.0,0.0]| 0.0|</strong><br/><strong> | [5.4,3.9,1.3,0.4]| Iris-setosa| 0.0| [40.0,0.0,0.0]| [1.0,0.0,0.0]| 0.0|</strong><br/><strong> | [5.4,3.9,1.7,0.4]| Iris-setosa| 0.0| [40.0,0.0,0.0]| [1.0,0.0,0.0]| 0.0|</strong></pre>
<p class="mce-root"/>
<p>Let's return a new dataset by passing in column expressions for <kbd>prediction</kbd> and <kbd>label</kbd>:</p>
<pre><span>val </span><span>validatedTestResultsDataset</span>:<span>DataFrame </span>= <span>validatedTestResults</span>.select(<span>"prediction"</span>, <span>"label"</span>)</pre>
<p>In the line of code, we produced a new <kbd>DataFrame</kbd> with two columns:</p>
<ul>
<li>An input label</li>
<li>A predicted label, which is compared with its corresponding value in the input label column</li>
</ul>
<p>That brings us to the next step, an evaluation step. We want to know how well our model performed. That is the goal of the next step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 10 – evaluate Random Forest classifier </h1>
                </header>
            
            <article>
                
<p>In this section, we will test the accuracy of the model. We want to know how well our model performed. Any ML process is incomplete without an evaluation of the classifier.</p>
<p>That said, we perform an evaluation as a two-step process:</p>
<ol>
<li>Evaluate the model output</li>
<li>Pass in three hyperparameters:</li>
</ol>
<pre>val modelOutputAccuracy: Double = new MulticlassClassificationEvaluator()</pre>
<p>Set the label column, a metric name, the prediction column <kbd>label</kbd>, and invoke evaluation with the <kbd>validatedTestResults</kbd> dataset. </p>
<p>Note the accuracy of the model output results on the testing dataset from the <kbd>modelOutputAccuracy</kbd> variable.</p>
<p>The other metrics to evaluate are how close the predicted label value in the <kbd>'predicted'</kbd> column is to the actual label value in the (indexed) label column.</p>
<p>Next, we want to extract the metrics:</p>
<pre>val multiClassMetrics = new MulticlassMetrics(validatedRDD2)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Our pipeline produced predictions. As with any prediction, we need to have a healthy degree of skepticism. Naturally, we want a sense of how our engineered prediction process performed. The algorithm did all the heavy lifting for us in this regard. That said, everything we did in this step was done for the purpose of evaluation. Who is being evaluated here or what evaluation is worth reiterating? That said, we wanted to know how close the predicted values were compared to the actual label value. To obtain that knowledge, we decided to use the <kbd>MulticlassMetrics</kbd> class to evaluate metrics that will give us a measure of the performance of the model via two methods:</p>
<ul>
<li>Accuracy</li>
<li>Weighted precision</li>
</ul>
<p>The following lines of code will give us value of Accuracy and Weighted Precision. First we will create an accuracyMetrics tuple, which should contain the values of both accuracy and weighted precision</p>
<pre><span>val </span>accuracyMetrics = (multiClassMetrics.<span>accuracy</span>, multiClassMetrics.<span>weightedPrecision</span>)</pre>
<p>Obtain the value of accuracy.</p>
<pre><span>val </span>accuracy = accuracyMetrics._1</pre>
<p>Next, obtain the value of weighted precision.</p>
<pre><br/><span>val </span>weightedPrecsion = accuracyMetrics._2</pre>
<p>These metrics represent evaluation results for our classifier or classification model. In the next step, we will run the application as a packaged SBT application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 11 – running the pipeline as an SBT application</h1>
                </header>
            
            <article>
                
<p>At the root of your project folder, issue the <kbd>sbt console</kbd> command, and in the Scala shell, import the <kbd>IrisPipeline</kbd> object and then invoke the <kbd>main</kbd> method of <kbd>IrisPipeline</kbd> with the argument <kbd>iris</kbd>:</p>
<pre><strong>sbt console</strong><br/><strong>scala&gt;</strong><br/><strong>import com.packt.modern.chapter1.IrisPipeline</strong><br/><strong>IrisPipeline.main(Array("iris")</strong><br/><strong>Accuracy (precision) is 0.9285714285714286 Weighted Precision is: 0.9428571428571428</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the next section, we will show you how to package the application so that it is ready to be deployed into Spark as an Uber JAR.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 12 – packaging the application</h1>
                </header>
            
            <article>
                
<p>In the root folder of your SBT application, run:</p>
<pre><strong>sbt package</strong></pre>
<p>When SBT is done packaging, the Uber JAR can be deployed into our cluster, using <kbd>spark-submit</kbd>, but since we are in standalone deploy mode, it will be deployed into <kbd>[local]</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bfb1ec64-d3dd-4ec2-aa50-49d358ac5a6f.jpg" style="width:45.17em;height:9.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The application JAR file</div>
<p>The package command created a JAR file that is available under the target folder. In the next section, we will deploy the application into Spark.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 13 – submitting the pipeline application to Spark local</h1>
                </header>
            
            <article>
                
<p>At the root of the application folder, issue the <kbd>spark-submit</kbd> command with the class and JAR file path arguments, respectively.</p>
<p>If everything went well, the application does the following:</p>
<ol>
<li>Loads up the data.</li>
<li>Performs EDA.</li>
<li>Creates training, testing, and validation datasets.</li>
<li>Creates a Random Forest classifier model.</li>
<li>Trains the model.</li>
<li>Tests the accuracy of the model. This is the most important part—the ML classification task.</li>
</ol>
<ol start="7">
<li>To accomplish this, we apply our trained Random Forest classifier model to the test dataset. This dataset consists of Iris flower data of so far not seen by the model. Unseen data is nothing but Iris flowers picked in the wild.</li>
<li>Applying the model to the test dataset results in a prediction about the species of an unseen (new) flower.</li>
<li>The last part is where the pipeline runs an evaluation process, which essentially is about checking if the model reports the correct species. </li>
<li>Lastly, pipeline reports back on how important a certain feature of the Iris flower turned out to be. As a matter of fact, the petal width turns out to be more important than the sepal width in carrying out the classification task.</li>
</ol>
<p>That brings us to the last section of this chapter. We will summarize what we have learned. Not only that, we will give readers a glimpse into what they will learn in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we implemented an ML workflow or an ML pipeline. The pipeline combined several stages of data analysis into one workflow. We started by loading the data and from there on, we created training and test data, <span>preprocessed the dataset,</span><span> trained the <kbd>RandomForestClassifier</kbd> model, applied the Random Forest classifier to test data, evaluated the classifier, and computed a process that demonstrated the importance of each feature in the classification. We fulfilled the goal that we laid out early on in the <em>Project overview – problem formulation</em> section.<br/></span></p>
<p>In the next chapter, we will analyze the <strong>Wisconsin Breast Cancer Data Set</strong>. This dataset has only categorical data. We will build another pipeline, but this time, we will set up the Hortonworks Development Platform Sandbox to develop and deploy a breast cancer prediction pipeline. Given a set of categorical feature variables, this pipeline will predict whether a given sample is benign or malignant. In the next and the last section of the current chapter, we will list a set of questions that will test your knowledge of what you have learned so far. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>Here are a list of questions for your reference:</p>
<ol>
<li>What do you understand by EDA? Why is it important?</li>
<li>Why do we create training and test data?</li>
<li>Why did we index the data that we pulled from the UCI Machine Learning Repository?</li>
<li>Why is the Iris dataset so famous?</li>
<li>Name one powerful feature of the random forest classifier.</li>
<li>What is supervisory as opposed to unsupervised learning?</li>
<li>Explain briefly the process of creating our model with training data.</li>
<li>What are feature variables in relation to the Iris dataset?</li>
<li>What is the entry point to programming with Spark?</li>
</ol>
<p>Task: The Iris dataset problem was a statistical classification problem. Create a confusion or error matrix with the rows being predicted setosa, predicted versicolor, and predicted virginica, and the columns being actual species, such as setosa, versicolor, and virginica. Having done that, interpret this matrix.</p>


            </article>

            
        </section>
    </body></html>