- en: Scale Up – Threading and Implications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalability has always been a concern for Java and, thus, a thread-related API
    was introduced in Java version 1.0\. The idea is to be able to benefit from the
    most modern hardware updates in order to parallelize the processing of applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Being able to handle multiple requests in parallel is crucial for a Java EE
    server to scale, but in our modern Java world, you also need to be able to control
    your own threads. Also, Java EE introduced the required API to do it in good conditions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Java EE threading model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data consistency across threads
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java EE hidden thread usages
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to integrate reactive programming with the Java EE programming model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java EE threading model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Java EE philosophy has, for a long time, been able to give its users a well-defined
    and safe programming model. This is why most of the Java EE defaults are about
    being thread-safe, and that several specifications such as **Enterprise Java Beans**
    (**EJB**) defaults were preventing custom thread usage. It does not mean Java
    EE was ignoring threads at all, but explicitly using thread pools from an application
    was not very natural. Also, most of the time, the adopted coding style was either
    against Java EE's (strict) rules or were very verbose.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Before detailing the new API added by Java EE to help you develop concurrent
    applications, let's see the basic Java EE model and how it can already help you
    to scale.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take back the specifications included in Java EE 8 (full profile), we''ll
    get a long list. Now, if we check which specifications use threads, the list will
    be shorter and we can find some common points among them. Here is a table trying
    to represent whether the specifications manage dedicated threads or not and whether
    they explicitly interact with threads (handling cross-thread calls by using the
    provided threads) or simply use the caller (contextual) thread:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '| **Specification** | **Manage dedicated threads** | **Interacts with threads**
    | **Comment** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
- en: '| EJB 3.2 | Yes | Yes | `@Asynchronous` allows you to execute tasks in a dedicated
    thread pool. `@Lock` used with `@Singleton` allows you to control the thread safety
    of the bean. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '| Servlet 4.0 | Yes | Yes | Every request is executed in a single thread provided
    by the container by default. When using `AsyncContext`, you can execute the task
    in a custom thread and resume the request from another thread later. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '| JSP 2.3/JSP Debugging 1.0 | No | No | Inherits from the servlet model. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| EL 3.0 | No | No | Uses the caller context. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: '| JMS 2.0 | Yes | No | By itself, JMS can be seen as a specific sort of connector
    (as in *Connector 1.7*) but put a few specific words on this case. JMS has two
    sorts of usages: on the server side and on the client side. The server side is
    generally a network server expecting connection. This is where dedicated threads
    will be used (such as any socket server). Then, the processing will be fully delegated
    to the connector. On the client side, it generally inherits from the caller context
    but also uses custom threads, as it is asynchronous by design. So, it needs its
    own thread pools to handle this part of the JMS specification. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| JMS 2.0 | 是 | 否 | 本身，JMS 可以被视为一种特定的连接器（如 *Connector 1.7*），但在这个案例中要特别说明。JMS
    有两种使用方式：在服务器端和客户端。服务器端通常是一个网络服务器，等待连接。这就是将使用专用线程的地方（如任何套接字服务器）。然后，处理将完全委托给连接器。在客户端，它通常继承自调用上下文，但也使用自定义线程，因为它按设计是异步的。因此，它需要自己的线程池来处理
    JMS 规范的这一部分。|'
- en: '| JTA 1.2 | No | Yes | JTA doesn''t manage threads but *binds* its context
    to threads. Concretely, when a transaction starts, it is only valid for the initial
    thread. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| JTA 1.2 | 否 | 是 | JTA 不管理线程，而是 *绑定* 其上下文到线程。具体来说，当事务开始时，它仅对初始线程有效。|'
- en: '| JavaMail 1.6 | Yes | No | JavaMail being the link between your Java code
    and the way mails are sent/received, the implementation is, here again, linked
    to a socket, and thus, it often relies on dedicated threads. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| JavaMail 1.6 | 是 | 否 | JavaMail 作为你的 Java 代码与发送/接收邮件方式之间的桥梁，其实现再次与套接字相关联，因此，它通常依赖于专用线程。|'
- en: '| Connector 1.7 | Yes | Yes | Connector specification is the standard way to
    interact with external systems (bi-directional ways even if connector implementations
    often handle only one way). However, it generally uses dedicated threads in two
    layers, the first one being related to the network interactions and the second
    one being related to the container interaction that generally goes through `WorkManager`,
    which is the ancestor of the *Concurrency Utilities for Java EE* specification.
    Like JTA, it also uses context-related information that is often bound to the
    thread. Finally, since it interacts with the JTA, a part of the interactions is,
    by design, bound to threads. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Connector 1.7 | 是 | 是 | Connector 规范是与外部系统交互的标准方式（即使连接器实现通常只处理单向，双向方式）。然而，它通常在两层中使用专用线程，第一层与网络交互相关，第二层与容器交互相关，通常通过
    `WorkManager` 进行，它是 *Java EE 并发实用工具规范* 的前身。像 JTA 一样，它也使用与线程相关联的上下文信息。最后，由于它与 JTA
    交互，交互的一部分按设计绑定到线程上。|'
- en: '| Web Services 1.4 / JAX-RPC 1.1 | No | No | Web services generally just inherit
    from the servlet contextual threading model. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Web Services 1.4 / JAX-RPC 1.1 | 否 | 否 | Web 服务通常只是继承自 servlet 上下文线程模型。|'
- en: '| JAX-RS 2.1 | Yes | Yes | JAX-RS inherits from the servlet contextual model,
    but since JAX-RS 2.0, the server can asynchronously handle the requests, thanks
    to Servlet 3.0 `AsyncContext`. In this case, the developer must notify the container
    when a request is completed and interacts with threads, as it is generally done
    from a different thread from the servlet one.On the client side, JAX-RS 2.1 now
    has a reactive API, able to use custom threads to do the execution. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| JAX-RS 2.1 | 是 | 是 | JAX-RS 继承自 servlet 上下文模型，但自从 JAX-RS 2.0 以来，服务器可以异步处理请求，这得益于
    Servlet 3.0 的 `AsyncContext`。在这种情况下，当请求完成时，开发者必须通知容器，并且与线程交互，因为这通常是在与 servlet
    线程不同的线程中完成的。在客户端，JAX-RS 2.1 现在有一个反应式 API，能够使用自定义线程进行执行。|'
- en: '| WebSocket 1.1 | Yes/No | Yes/No | Normally, the WebSocket specification was
    designed to be implemented on top of the servlet specification, which is really
    the Java EE central transport. However, for several cases, it may be needed to
    use some customization of the threading for WebSocket needs (long connections).
    This part highly depends on the container. The last thing is that some custom
    WebSocket threads may be needed to handle connection evictions and things like
    that, but it has less impact on the end user and performance. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| WebSocket 1.1 | 是/否 | 是/否 | 通常，WebSocket 规范被设计为在 servlet 规范之上实现，这实际上是 Java
    EE 的核心传输。然而，在几个情况下，可能需要使用一些针对 WebSocket 需求（长连接）的线程定制。这部分高度依赖于容器。最后，可能需要一些自定义的
    WebSocket 线程来处理连接驱逐等问题，但这对接端用户和性能的影响较小。|'
- en: '| JSON-P 1.1 / JSON-B 1.0 | No | No | This specification (JSON low-level API
    and JSON binding) does not have any thread-related operations and simply executes
    in the caller context. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| Concurrency Utilities for Java EE 1.0 | Yes | Yes | Concurrency utilities
    mainly have the ability to define *Java EE thread pools* and, indeed, they manage
    custom threads. It also transparently facilitates (through `ContextService`) the
    propagation of some contexts, such as security, JNDI context, transaction, and
    so on. Note that, however, the propagation is not standard and you may need to
    check out your server documentation to know what it does precisely. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| Batch 1.0 | Yes | No | JBatch is asynchronous by design. When you launch
    a batch, the invocation returns before the batch is done, as it can be very long.
    To handle such behavior JBatch has its own thread pools. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| JAXR 1.0 | No | No | This specification is rarely used and has become old
    (before Java introduced nio). Being a client, it doesn''t use custom threads.
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| Java EE Management 1.1 (or 1.2) | Yes/No | No | This specifications allows
    you to interact with the server and its definitions (resources and applications).
    It uses another transport technology, so it generally needs dedicated threads.
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| JACC 1.5 | No | No | This is a specification linking an authorization policy
    with the Java EE container. It is contextually executed. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| JASPIC 1.1 | No | No | This is a security specification, also contextually
    executed. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| Java EE Security API 1.0 | No | No | This is the last security API of Java
    EE, making it pretty usable, but it stays contextual to the caller. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| JSTL 1.2 | No | No | Inherits from the servlet model. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| Web Service Metadata 2.1 | No | No | This mainly involves annotations for
    web services, so there''s  no particular threading model. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| JSF 2.3 | No | No | Inherits from the servlet threading model (this is a
    simplification but good enough for this book''s context). |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| Common annotations 1.3 | No | No | Just a set of APIs reused by other specifications,
    no particular behavior directly bound here. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Java Persistence 2.2 (JPA) | No | No | Inherits from the caller context.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| Bean Validation 2.0 | No | No | Inherits from the caller context. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| Interceptors 1.2 | No | No | Inherits from the caller context. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| Contexts and Dependency Injection for Java EE 2.0 (CDI) | Yes | Yes | CDI
    2.0 supports asynchronous events, which rely on a dedicated thread pool. CDI being
    about *contexts*, also binds contextual data to threads such as the `@RequestScoped`
    context. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Dependency Injection for Java 1.0 (@Inject) | No | No | This is mainly a
    set of annotations of CDI, so there''s no real thread-related behavior here. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: 'If we review all the thread usages, we can distinguish between some categories:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous usages: the specification using threads not to block the caller
    execution (such as JAX-RS client, Batch API, CDI asynchronous events, and so on)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network-related implementations that need threads for the selector (partly accepting
    the connections) and request handling
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要线程进行选择器（部分接受连接）和请求处理的网络相关实现
- en: 'In terms of code context, this is generally related to the outbound layers
    of the code. Indeed, the network is an outbound of the application, but asynchronous
    usages are also in the sense that they split the execution into two branches:
    the caller context that continues and a new branch that is no more linked to the
    caller.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码上下文中，这通常与代码的外出层有关。确实，网络是应用程序的外出，但异步使用也在此意义上，因为它们将执行分成两个分支：继续执行的调用上下文和一个不再与调用者相关的新分支。
- en: What does it mean for you? When you take the lead on an application, at least
    from a performance or configuration point of view, you need to be clear about
    the thread execution path of the application (when the application uses a different
    thread from the one it got affected by, when the request entered into the system).
    This is also true for inter-system architectures, such as microservices, where
    you need to track the execution context breakdowns.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这对你意味着什么？当你负责一个应用程序时，至少从性能或配置的角度来看，你需要清楚了解应用程序的线程执行路径（当应用程序使用一个不同于受影响的线程时，当请求进入系统时）。这也适用于像微服务这样的跨系统架构，你需要跟踪执行上下文的分解。
- en: Thread data and consistency
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程数据和一致性
- en: Before getting Java EE-specific, it is important to step back a moment and understand
    the implications of concurrency on the programming model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体到Java EE之前，重要的是退一步理解并发对编程模型的影响。
- en: Concurrent data access
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发数据访问
- en: When a single thread accesses data, then the access is always thread-safe, and
    it is not possible for a thread to mutate data while another one is reading it.
    When you increase the number of threads and multiple threads can access the same
    data structure instances, it is possible for a thread to read the data that's
    currently being modified, or for two concurrent modifications to happen, leading
    to an inconsistent result.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当单个线程访问数据时，访问总是线程安全的，一个线程在另一个线程读取数据时不可能修改数据。当你增加线程数量，并且多个线程可以访问相同的数据结构实例时，一个线程可能会读取正在被修改的数据，或者可能发生两个并发修改，导致不一致的结果。
- en: 'Here is a schema representing this issue with two threads. Keep in mind that
    a Java EE server often handles  around 100 to 1000 threads, so the effects are
    more impacting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个用两个线程表示这个问题的模式。请记住，Java EE服务器通常处理大约100到1000个线程，因此影响更为显著：
- en: '![](img/68e59b4f-1bb0-4fab-8e82-d359d7e6bb26.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68e59b4f-1bb0-4fab-8e82-d359d7e6bb26.png)'
- en: In this simple example, we have a thread (**Thread 1**) setting data that is
    supposed to be a complex structure. In parallel, we have another thread (**Thread
    2**) accessing the data. In the  preceding diagram, the very thin black line represents
    the thread life, whereas the bold black line represents the method execution in
    the thread's context. The blue box represents the data setter/getter execution
    time and the red zone represents the overlap of both threads on the data usage.
    In other words, you can consider that the vertical unit is time.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们有一个线程（**Thread 1**）正在设置数据，这些数据应该是一个复杂结构。同时，我们还有另一个线程（**Thread 2**）正在访问这些数据。在先前的图中，非常细的黑线代表线程的生命周期，而粗黑线则代表线程上下文中的方法执行。蓝色方框代表数据设置/获取的执行时间，红色区域代表两个线程在数据使用上的重叠。换句话说，你可以认为垂直单位是时间。
- en: 'To understand what can be the impact of such a code without any protection,
    let''s materialize the data structure with this simple code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解没有保护的代码可能产生的影响，让我们用以下简单代码具体化数据结构：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This simple structure aims to store a `String` value and handle a state (`initialized`),
    which allows the structure to prevent access to the data if uninitialized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的结构旨在存储一个`String`值并处理一个状态（`initialized`），这允许结构在未初始化时防止对数据的访问。
- en: 'If we apply this structure on our previous picture timeline, it is possible
    that **Thread 2** calls `getData` and fails with `IllegalStateException`, whereas
    the `setData` method is called and sets the `initialized` variable. In other words,
    the structure (`getData`) was accessed while it was changing and, thus, the behavior
    was not consistent with the complete state of the data. In this case, the error
    is not dramatic, but if you take another example summing some values, you will
    just get the wrong data:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If the execution of `sum` is done and `value` is accessed at the same time,
    then the account value will be wrong, the reporting will potentially be inconsistent,
    and the validation (which likely considers *credit+debit=0*) will fail, making
    this account erroneous.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look one step further and integrate some EE features, you will quickly
    understand that the situation is even worse. Let''s take the case of a JPA entity,
    such as `Quote`, and assume that two threads are differently modifying the entity:
    one thread modifies the price and the other one modifies the name. What will happen
    when each thread updates the entity? The database can''t handle both the updates
    at the same time, so if there is no failure, then the last update will win and
    only one of the two updates will be taken into account.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: JPA provides optimistic and pessimistic locking to properly solve the aforementioned
    problem. As a general rule, try to use optimistic locking until you really need
    pessimistic locking. In fact, it will give you a better performance even if it
    will require you to potentially handle a retry logic if relevant.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Java and thread safety
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section doesn't intend to explain all the solutions that Java provides
    to ensure the thread safety of your code but just to give you some highlights
    on Java Standalone API, you can reuse in Java EE application if needed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Synchronized
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Surely the oldest solution, but still available in Java, the `synchronized`
    keyword allows you to ensure that methods are not concurrently executed. You just
    need to add it in your method definition as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is the exact same structure as the one we just saw. But now, thanks to
    the `synchronized` keyword added to each method, the call to a method would enforce
    concurrent calls to other synchronized methods to wait for the first one to end
    before being executed. Concretely, it will chain method execution like in a single
    thread.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The `synchronized` keyword is linked to an instance, so two different instances
    that are synchronized will not lock each other. It is also possible to use `synchronized`
    as a block and pass the instance to synchronize on. If you do so and pass a static
    instance, then you will lock all the instances that can prevent the application
    from scaling if on a common code path.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Locks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java 1.5 introduces the `Lock` interface with its `java.util.concurrent` package,
    which contains a lot of concurrency-related classes. It achieves the same goal
    as that of `synchronized` but allows you to control manually the scope of the
    synchronized code.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The performance of `Lock` and `synchronized` differs between Java versions,
    but recent versions have progressed a lot. And generally, if you don't optimize
    a computing algorithm, choosing one or the other will lead to something close.
    However, it is generally better to use `Lock` if the number of concurrent threads
    accessing the instance is high.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'As `Lock` is an interface, it needs an implementation. The JRE comes with a
    default implementation called `ReentrantLock`. Replacing a `synchronized` block
    is done in the following way:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The lock instantiation is directly done through the `new` keyword. We will note
    here that `ReentrantLock` can take a Boolean parameter to request it to respect
    a fair order for the lock invocations (the default is `false` and generally good
    enough in terms of performance). Once you have a locked instance, you can call
    the `lock()` method to ensure that you are the only one executing the code. Once
    you are done with your protected code, you can call `unlock()` to release the
    current thread and let another one execute its code. Also, note that all this
    locking logic assumes that the lock is shared across the thread. Thus, the instantiation
    is generally done once per instance (in the constructor or in a `@PostConstruct`
    method).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: It is vital to call the `unlock()` method; otherwise, other locked threads will
    never be released.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'A more common usage of the Lock API is to split the lock and unlock calls into
    two. For instance, to take Java EE usage, you can lock an instance when a request
    starts and unlock it when the request ends in order to ensure that a single request
    is accessing it. This is feasible with Servlet 3 through a listener, even for
    asynchronous requests. But you will not have a block that you can surround; instead,
    you will have multiple callbacks, which you need to integrate with the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With this listener added to `AsyncContext`, the lock will follow the request''s
    life cycle. The usage will probably look as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once `AsyncContext` is obtained, we add the lock listener onto it and execute
    the request. The lock will be released when the request ends because of a timeout,
    an exception, or, simply, a normal termination.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: This sort of implementation with a synchronized block is quite hard and often
    requires some workarounds. This is an example where the Lock API is more powerful.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not detail it here, but the `ReadWriteLock` API gives you a holder
    for two locks: one is used to protect read accesses, and the other one for write
    accesses. The goal is to let read accesses be done in parallel and ensure that
    write accesses are done only when a single thread accesses the data.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: java.util.concurrent
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although a detailed coverage of the Java Standalone is beyond the context of
    this book, it is important to know that Java 7 and Java 8 got a lot of enhancements
    in this area. So don''t hesitate to go through its packages. Among the interesting
    classes, we can note the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '`CountDownLatch`: This is a simple and efficient way to ensure that N threads
    are waiting a condition owned by another thread (a bit like a starter in a race).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Semaphore`: This allows you to represent and implement permission *buckets*.
    The most interesting part is that you can increase and decrease the associated
    counter. It can be a simple way to implement a bulkhead solution.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CyclicBarrier`: This is a way to synchronize multiple threads in some points
    of the code. Its API is interesting because it allows you to add shared logic
    that can be executed on all the threads. It can be seen as the opposite of `CountDownLatch`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Phaser`: This is a more flexible barrier implementation than `CyclicBarrier`
    and `CountDownlatch`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Atomic*`: This is a way to update a data instance atomically.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volatile data
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, to conclude this part of the Java Standalone concurrent programming,
    it is necessary to keep in mind why the `volatile` keyword is important. This
    keyword allows you to request the JVM to refresh the value it reads every time
    it accesses the data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very simple to use; just add the `volatile` keyword on the field declaration:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To understand why this keyword changes everything, you need to keep in mind
    that the JVM can have some thread-related *caching* of the field values (this
    is very low-level caching and has nothing to do with what we'll see in the next
    section). Adding this keyword as in the previous snippet forces us to bypass this
    cache. It is supposed to be a bit slower. However, the usage is often fast by
    itself, so it is generally worth it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Java EE and threads
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw at the beginning of the chapter, Java EE can silently use threads.
    Any thread usage is important to identify even if it is good to rely on the Java
    EE implementation, because it is code that you don't have to maintain. The issue
    with not identifying the thread is that you can come across cases where your context
    (`ThreadLocal`) will not be available or will be available with the wrong values.
    The other pitfall of not identifying the thread is that you may end up abusing
    the thread and consuming way more resources on the machine than you need. Let's
    review a few representative cases of such usages.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: CDI asynchronous events
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CDI 2.0 introduces the notion of asynchronous events. It is a manner of asynchronously
    firing an event for the caller. Here is a sample usage:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet sends an asynchronous event, thanks to `fireAsync`. The interesting
    part of this API is that it returns `CompletionStage`, which enables you to chain
    some logic after the event has notified all the asynchronous observers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous events notify only asynchronous observers. It uses a new observer
    marker: `@ObserverAsync`. Here is a signature sample:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '`public void onEvent(@ObservesAsync MyEvent event);`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The way the CDI handles the submission of this kind of events is by using `CompletionFuture.all`
    or by chaining the asynchronous observers in a single asynchronous thread (this
    is configurable in Weld; OpenWebBeans only supports the first solution). In any
    case, it submits individual futures to a CDI thread pool. The pool configuration
    is not yet completely standard, though it is important to know that it is feasible
    in all the containers and is an important tuning configuration for your application
    if you rely on it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Several containers will default to the common fork join pool of the JVM, which
    doesn't scale a lot. So, you will probably want to provide a custom thread pool
    dedicated to your application usage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the user code, it is important to prefer the signature taking a `NotificationOptions` instance
    to `fireAsync` (which generally falls back on the default container pool). Doing
    so will allow you to give a custom pool:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This signature enables you to pass a custom pool and to properly tune it (using
    EE concurrency utilities for Java EE, for instance). It will also enable you to
    specify the pool you are using by event and to avoid putting them all in the same
    bucket. In fact, it can potentially lead to locking your application if there
    are some dependencies between the usages!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Last tip on this API is to make sure that you synchronize the event if you mutate
    it, to get a new state after through `CompletionStage`. You can use any of the
    Java Standalone techniques we talked about previously.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: EJB @Asynchronous
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EJB was one of the earliest Java EE specifications. At that time, it was the
    specification getting the most attention and features. Now, it is slowly being
    replaced by CDI and integrations, with other specifications such as JTA, JPA,
    and so on. However, it still contains a set of useful features that you don't
    find elsewhere in Java EE.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'EJB also has an asynchronous solution. It is more direct compared to CDI, since
    you can mark a method as asynchronous:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`@Asynchronous` requests the server to execute the task in the EJB thread pool.
    The method can return void, but if it needs to return a value, it must return
    `Future`. With Java 8, it is easy to return `CompletionFuture`. However, since
    this API was designed before that, the easiest way was to return `AsyncResult`,
    which was provided by the specification, and pass it the actual value you want
    to return. Note that the container will wrap the returned `Future` value to add
    a particular specification handling, so you will not be able to cast it to `CompletionFuture`,
    even if that is the implementation you choose in your code.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Here again, the pool configuration is highly dependent on the server, but it
    is generally workable and important, depending on the usage of this API in the
    application. If your application uses it a lot but the container provides only
    two threads and a small pool queue, then you will not scale very far.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: EE concurrency utilities for Java EE
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java EE 7 introduces a new specification called EE concurrency utilities for
    Java EE. The main target is not only to provide a way to work with threads from
    your EE application, but also to handle EE context propagation, including security,
    transaction, and so on.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: When using this API, remember that the propagated context highly depends on
    the container. This API is, however, a very good choice compared to a custom thread
    pool management because the configuration is outside the application and *standard* for
    the container, and also because it gives you the ability to benefit from the API
    that we will see in this section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: What is very clever to do at the specification level is to reuse the standard
    APIs, such as `ExecutorService`, `ScheduledExecutorService`, and so on. This gives
    the developers the ability to use them as a replacement for the SE API. In particular,
    it enables you to integrate transparently with third-party libraries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can integrate with RxJava ([https://github.com/ReactiveX/RxJava](https://github.com/ReactiveX/RxJava)), as
    you would do with any thread pool:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This code integrates the Java EE WebSocket API with the RxJava Flowable API.
    The global idea is to let the consumers handle the WebSocket messages, without
    knowing it comes from WebSocket. This makes it easier to test (replacing the WebSocket
    layer by a mock) and it decouples the code from the WebSocket layer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: In our service, we inject `ManagedExecutorService`, which is mainly `ExecutorService` managed
    by the container, and we wrap it in the thread pool API of RxJava through the `Scheduler` API.
    Then we are done; we can use any asynchronous operation of RxJava relying on the
    Java EE threads and, therefore, the context. In the previous code snippet, it
    allowed us to debounce the messages (limit the number of messages emitted per
    unit of time) in one simple line.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, we implement `Iterator<>` to integrate with RxJava, but we could
    use `Future` or any other type supported by the Flowable API. The iterator is
    the part integrating with RxJava. However, to integrate with the WebSocket API,
    we can also implement `MessageHandler`, which allows us to see the incoming message
    and register it at our endpoint in the previous snippet. Here is a potential handler
    implementation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Our publisher stacks the received messages and then serves them through the `Iterator<>`
    API. It requires some synchronization, as we saw in the previous section, to make
    sure we are able to correctly answer the iterator contract. Concretely, we cannot
    return anything in `hasNext()` if the connection was not closed or if we did not
    receive any message. Otherwise, it will stop the iterations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: ManagedExecutorService
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a quick reminder, `ExecutorService` is the standard Java Standalone abstraction
    for a thread pool. `ManagedExecutorService` is the Java EE flavor. If you compare
    both the APIs, you will notice that it inherits from all the features of its standalone
    siblings, but it gets enriched with an auditing: a submitted task (`Runnable`)
    can implement the ManagedTask API, which will associate a listener to the task,
    which will be notified of the task''s phase (`Submitted`, `Starting`, `Aborted`,
    and `Done`).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'ManagedTask globally gives the following to the container:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The listener that ManagedTask uses
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A set of properties to customize the behavior of the execution. Three main
    standard properties are defined and portably usable on all the containers:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`javax.enterprise.concurrent.LONGRUNNING_HINT`: This allows the container to
    change the thread setup for a long time, taking the tasks to complete (using other
    thread priorities or potentially using dedicated threads)'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`javax.enterprise.concurrent.TRANSACTION`: This can take the `SUSPEND` value that
    will suspend the current transaction (if any) and resume it after the task is
    completed'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`USE_TRANSACTION_OF_EXECUTION_THREAD`: This propagates the transaction of the
    calling thread'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you cannot make your task implementing ManagedTask, then you also have *bridge*
    adapters to link a normal task to a listener through the  `ManagedExecutors` factory:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This simple invocation will create `Runnable` and you can therefore submit to
    `ManagedExecutorService`, which also implements `ManagedTask` with the `myListener` listener.
    Indeed, there are wrapper factory methods for `Callable`, with the properties
    we mentioned earlier, to ensure it covers all the `ManagedTask` API's features.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the overall platform consistency, it is important that this API
    tends to make EJB `@Asynchronous` legacy.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: ManagedScheduledExecutorService
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ManagedScheduledExecutorService` is `ScheduledExecutorService` Java EE API.
    Like `ExecutorService`, it integrates with the ManagedTask API.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: However, this scheduling-related API goes a bit further, providing two new methods
    to schedule a task—`Runnable` or `Callable` —based on a dedicated API (`Trigger`).
    This API enables you to handle the scheduling programmatically and it avoids relying
    on a constant time interval or delay.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Even if, theoretically, this scheduling API can be distributed and was designed
    to support it, it is generally implemented with local support only. However, it
    is a good alternative to EJB `@Schedule` or `TimerService` when clustering is
    not mandatory, which is actually often the case in practice.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Java EE threads
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The EE concurrency utilities also provide a Java EE ThreadFactory, which creates `ManageableThread`
    instead of a plain thread. The main difference is that they provide an `isShutdown()`
    method, allowing you to know whether the current thread is shutting down and,
    thereby, exit the process if it is indeed shutting down. `ManagedExecutors.isCurrentThreadShutdown()`
    allows you to directly test this flag, handling the casting of the thread automatically.
    This means that a long running task can be implemented as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You may think that testing only the thread state would be enough, but you still
    need an application state to ensure that you integrate with the application life
    cycle. Don't forget that the thread can be bound to the container and not the
    application deployment time. Also, depending on the strategy you define for the
    threads, you can evict them at runtime, potentially through the administration
    API of the container.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: ContextService
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last interesting API of the EE concurrency utilities is `ContextService`.
    It allows you to create proxies, based on interfaces, inheriting from the context
    propagation of the `Managed*` API. You can see it as a way of using managed thread
    pool features in the standalone thread pools that you don''t control:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we wrap our task in a contextual proxy and we submit the wrapped task
    through a framework that we don't control. However, the execution will still be
    done in the EE context of the caller (same JNDI context, for instance), and using
    another framework is not much affecting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: This `ContextService` is limited to proxy interfaces and doesn't support subclass
    proxying like CDI does for instance. Java EE understands that modern development
    is composed of motley frameworks and stacks and that it can't control everything
    and anything. This trend is traduced by the introduction of a new sort of API
    to easily integrate with others and enable any use case, rather than introducing
    a lot of new APIs, which will not evolve very well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: It is important in terms of performance and monitoring—it not only allows you
    to easily trace invocations and application behavior but also to optimize the
    application with caching, as we will see in the next chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: EJB @Lock
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous part showed that EJB `@Asynchronous` and `@Schedule` can be replaced
    with the new EE concurrency utilities in some measure. However, there are still
    some EJB APIs that are not easy to replace without coding them yourself. The `@Lock`
    API is one of them.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The global idea is to ensure that the data owned by the EJB (`@Singleton`) is
    accessed in a thread-safe context.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, this API is limited to the singleton EJB, since without a single instance
    usable in a concurrent environment, it doesn't make sense to lock.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage is straightforward, as you just decorate a method or the bean with `@Lock`,
    passing `READ` or `WRITE` in the parameter, depending on the kind of access:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you remember the part about Java Standalone, it is very close to the `synchronized`
    usage, as we define a lock on a block. However, the semantic is closer to the `ReadWriteLock`
    API.  This was a will of the API design as this is the way it is often implemented.
    Now, why mix both the styles (block and read/write API)? It enables you to scale
    on the read, keeping the API very simple (bound to blocks). However, it already
    fits a lot of cases!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the performance, it is important to know that you can mix it with
    a timeout through `@AccessTimeout`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we request the container to fail with the `ConcurrentAccessTimeout` exception
    if the lock (read or write) is not acquired after 500 milliseconds.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: For a reactive application, correctly configuring the timeouts is crucial and
    important to ensure that the application doesn't start with a huge response time
    because of all the threads waiting for a response. This means that you have to
    define a fallback behavior in the case of a timeout. To say it differently, you
    need to define a timeout to ensure you match your SLA, but you also need to define
    what to do when you get a timeout in order to avoid 100% of errors in case the
    server is overloaded.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The microprofile initiative has been created by most Java EE vendors and is
    mainly based on the CDI. So, even if it is not part of the Java EE, it integrates
    very well with it. Its primary targets are microservices, and, therefore, they
    define a bulkhead API and other concurrent solutions. However, it is an interesting
    solution if the `@Lock` is too simple for your needs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: HTTP threads
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HTTP layers (server and client) are about network and connections. Therefore,
    they require some threading to handle the client connections on the server side
    and, potentially, the reactive processing on the client side. Let's go through
    these particular settings, which directly impact the scalability of your application.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Server side
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The entry point of any web request is the HTTP container. Here, the server configuration
    is always server-dependent, but most of the vendors will share the same concepts.
    It is important to tune that part to make sure that the outbound of your application
    is not unintentionally throttling it too much; otherwise, you will limit the scalability
    of your application for no reason.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, for GlassFish, you can configure the HTTP connector in the UI
    administration or the corresponding configuration file. Here is what it looks
    like:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d05f44a1-4cbe-429c-bfd0-36dfd19509f1.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'This page is really about the tuning of the HTTP connector (not the binding
    address, port, or the supported SSL cipher algorithms). The corresponding configurations
    are summarized in the following table:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '| **Configuration name** | **Description** |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| Max connections | This is the maximum number of requests per client in the
    keep-alive mode. This is not the maximum number of connections the server supports,
    compared with the other Java EE servers. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| Timeout | This is the timeout after which the connection can be dropped in
    the keep-alive mode if still idle. Here again, it is a client-based configuration
    and not a request timeout like in most of the other servers. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Request timeout | This is the duration after which the request will timeout
    and fail from the client point of view. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Buffer size/length | Buffers are used to read the incoming data in the input
    streams. Adjusting this size to avoid memory overflows will significantly increase
    the performance, since the server will no longer have to create a new volatile
    buffer to read the data. This tuning can be hard to do if the application does
    lots of things. The trade-off is to not use too much memory and to avoid unexpected
    allocations. Thus, the closer you are to the most common requests in terms of
    size (a bit more than this value actually), the better you will behave.  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| Compression | Compression is mainly for browser-based clients (supporting
    GZIP). It will automatically compress the content of the configure mime types
    if the size of the resource is more than the minimum configuration size. Concretely,
    it can, for instance, affect a JavaScript of 2MB (which is no longer rare today).
    This will use some CPU resources to do the compression, but the space gain on
    text-based resources (HTML, JavaScript, CSS, and so on) is generally worth it,
    as the network duration will be reduced a lot. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: 'These parameters are mainly about the network optimization but are crucial
    to ensure that the application stays responsive. They are also influencing the
    way the HTTP threads are used, because bad tuning can imply more work for the
    server. Now, you also have an HTTP thread pool in GlassFish (as in most servers)
    that you can configure. Here is the corresponding screen:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b6b61da-be49-4375-9305-9ea9e4cc52d5.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: The configuration of GlassFish is very common for a thread pool—its sizes (maximum/minimum),
    the queue size (the number of tasks that can be added even if the pool is full),
    and the timeout (when a thread is removed from the pool if not used).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: When you are benchmarking your application, ensure that you monitor the CPU
    usage of your application and the thread stacks (or profiling, depending on the
    way you monitor your server/application) to identify bad configuration. For instance,
    if you see a CPU usage of 50% and a few active threads, then you may need to increase
    the pool size. The overall goal is to make the CPU usage very high (85-95%) and
    the response time of the server almost constant. Note that it is not recommended
    to go up to 100% for the CPU usage because, then, you'll reach the limitations
    of the machine; the performance won't be relevant anymore and you will just see
    the response time increasing boundlessly.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: This is a general rule for any thread pool that can become very important when
    going reactive. So, always try to name the threads of the application with a prefix
    that corresponds to the role that they have in order to ensure that you can identify
    them in the thread dumps.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Client side
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since JAX-RS 2.1, the client has been made to be reactive. As a quick reminder,
    here is what it can look like:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is normal JAX-RS client API usage, except for the call to `rx()`, which
    wraps the response into `CompletionStage`. The only interest is to become asynchronous;
    otherwise, it will just be another layer with poor gain in terms of user experience.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The way the implementation handles asynchronous invocations is up to the implementation,
    but with Jersey (the reference implementation) and in a Java EE container, you
    will default to the managed executor service. Note that outside an EE container,
    Jersey will create a very big thread pool.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This kind of configuration is the key to your application, since each client
    is supposed to have a different pool to ensure that it doesn't affect the other
    parts of the application, and also because thread usages can be different and
    may need different constraints. However, it is not yet standardized and, thus,
    you will need to check which implementation your server uses and how the configuration
    can be used. In general, the client-side configuration is accessible through the
    client's properties, so it is not that hard. However, sometimes, you may be limited
    by container integration. In such a case, you can wrap the invocation into your
    own pool and not use the `rx()` API to fully control it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, we can expect in some time (Java EE 8 and this new
    JAX-RS 2 API) that this `rx()` method will be implemented directly using the NIO
    API, and therefore, it becomes really reactive at the network level and not just
    through another thread pool.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: We just saw that Java EE brings solutions to handle your application threading
    properly, but modern developments often require new paradigms. These modifications
    require a small change in the way the application is developed. Let's go through
    one of these new patterns.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Reactive programming and Java EE
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reactive programming lets your code be called instead of calling your code.
    You can visualize it as being event-based instead of procedural. Here is an example
    to compare both the styles:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is a very simple and common implementation of a business method where we
    call two services: `validator` and `service`. The first one will validate the
    data by checking whether it exists in the database, the values are in the expected
    ranges, and so on, while the second one will actually process the updates (a database,
    for instance).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The issue with this style is that the data validation and persistence are bound
    in a single `processData` method, which defines the entire execution environment
    (threading, context, and so on).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'In the reactive style, it can be rewritten to replace the synchronous calls
    by a *chain*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this example, we used the Java 8 stream API, but using a reactive library
    such as RxJava generally makes more sense; you will understand why in the next
    paragraph. This code does the same thing as the previous one, but it orchestrates
    the calls through the definition of a chain, instead of making the calls directly.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: What is interesting with this pattern is that you split your logic (`validator`, `service`)
    from the way it is used (the stream in the previous example). It implies that
    you can enrich the way the calls are orchestrated, and if you think about the
    example of RxJava that we saw earlier, you can immediately think about executing
    each method in different threads.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: One common use case of such a pattern is when the response time is more important
    than the resources used. In other words, if you don't care about consuming more
    CPU cycles if it helps reduce the time you need to respond to your client, then
    you can put this pattern in place. If you are working with multiple concurrent
    data providers, or if you need to contact multiple remote services to process
    the data, then you will do the three invocations concurrently upfront. And once
    you have all the responses, you will execute the actual processing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, you can assume that the data has a contract identifier,
    a customer identifier, and an account identifier associated with the corresponding
    entities through three different remote services. The synchronous implementation
    of such a case will be something like the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will work. However, assuming that a remote call is about 10 ms, your method
    will then take more than 30 ms to process the data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'You can optimize it a bit by doing the three requests concurrently:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this case, you will reduce the invocation duration to 10 ms, more or less.
    However, you will block the thread for 10 ms (the three parallel invocations).
    The `CompletableFuture.allOf(...).get()` line waits for all the three asynchronous
    operations (`CompletableFutures`) to complete, keeping the thread unusable for
    other requests/processing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The direct implication is that you will not scale and will not be able to process
    many concurrent requests even if your CPU is probably doing nothing (that is,
    you are waiting on I/O if you obtain a thread dump at that time).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to enhance this is to ensure that the main thread is not blocked and
    that the processing is triggered only when all the data is received:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this case, we still execute our three remote invocations in parallel—potentially,
    in a managed executor service if you need to access EE features—and, then, we
    wait for all three results to be retrieved in order to do our processing. However,
    we just register our processing to be done once the entire data is readable, and
    we don't block the thread waiting for this *ready* state; thus, we will be able
    to serve more requests simultaneously.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: What is important in going reactive is to try to avoid synchronizations as much
    as possible in order to ensure any thread time is active processing. Of course,
    it has limitations, like some JDBC drivers, which are still synchronous, it will
    block a thread waiting for I/O operations. Yet, with microservices becoming common,
    it is easy to add a lot of latency to your code and reduce the application scalability
    if you don't take care of it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: A way to represent this kind of programming mentally is to visualize the CPU
    usage as a big queue and each element of the queue as some active computing time
    consumer (that is, a task). Then, your program is just a big event loop polling
    this task queue and executing the tasks. What is the result?—almost no passive
    time, only active time!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, being asynchronous implies that all the work will become asynchronous
    (thread handling, context switching, queue management, and synchronization). Even
    if most of these tasks are hidden and done for you by the stack, it may make the
    CPU busy and slow down the application, compared with the same code executed in
    a single thread. This is true and means that you can't use this pattern for every
    single invocation. You need to ensure that you use it when relevant and when there
    is potentially a passive usage of the CPU (blocking time, sleep, and so on). Though,
    if you respect this pattern, you should be able to work with concurrency better
    than staying synchronous everywhere. Of course, this is a compromise because if
    you have a background task (a scheduled task executed once a day, for instance),
    you will not care about the waiting time since it concerns a single thread. This
    type of programming will only pay when used in accurate places, but if you respect
    this usage, you will really get a saner final behavior. However, don't forget
    that it brings more complexity because tracking is no more natural in Java (stack
    traces are almost no more useful since you don't have the full stack if you don't
    use a thread-tracing solution).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Message passing and Java EE
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Message passing pattern refers to several theories, but in this part, we'll
    mainly care about the asynchronous flavor. One illustration of this pattern is
    the actor flavor. An actor is *something* that can receive messages, send messages
    to other actors, create other actors, and designate the behavior for the next
    received message.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand the basis of the underlying concepts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Global communication relies on an asynchronous bus
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C*urrent* message processing of an actor is based on a state (a bit like an
    internal state machine)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An actor can create other actors to process a message
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With such a pattern, it is highly recommended to have immutable messages to
    avoid any concurrency issues and hard-to-debug behavior (or non-deterministic
    behavior) going across the actor flow.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Java EE doesn''t allow you to handle everything of this pattern out of the
    box, but most of it is already here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: CDI provides a bus
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CDI (asynchronous) observers are beans, so you can have a state machine
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The delegation chain (new actors) can be handled through a CDI context bound
    to the messages
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, this stays a poor man's implementation, compared with real actor
    systems, but it already gives you a solid pattern to avoid passive usage of threads,
    and, by the way, you should think about it when creating an internal architecture
    for your application.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how Java EE ensures that you can parallelize the computing
    of your applications, and make your applications scale better and process multiple
    concurrent requests.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Using Java Standalone synchronization mechanisms, Java EE threading management
    solutions and API will let you get the best out of your hardware and integrate
    with third-party libraries very easily.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have seen what is related to the CPU, we need to go through the
    machine''s other main resource that you can exploit to make your application''s
    behavior better: the memory. When processing can''t be optimized and is too impacting
    on the application, the solution is often just to skip it as much as possible.
    The most common—and probably, efficient—way of doing so is to make sure that the
    data is computed once and reused while valid. This is where the caching enters
    into the game and this is what our next chapter will be about.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
