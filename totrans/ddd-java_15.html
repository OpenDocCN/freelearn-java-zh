<html><head></head><body>
		<div id="_idContainer246">
			<h1 id="_idParaDest-177"><em class="italic"><a id="_idTextAnchor176"/>Chapter 12</em>: Beyond Functional Requirements</h1>
			<p class="author-quote">Sometimes I feel like I am being forgotten.</p>
			<p class="author-quote">— Anonymous</p>
			<p>While the functional requirements of the core of the system may be met adequately, it is just as important to place focus on the operational characteristics of the system. In this chapter, we will look at common pitfalls and how to get past them.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Observability</li>
				<li>Consistency</li>
				<li>Performance and scale</li>
				<li>Trunk-based development</li>
				<li>Continuous testing</li>
				<li>Deployment automation</li>
				<li>Refactoring</li>
				<li>Invocation style</li>
				<li>Logging</li>
				<li>Versioning</li>
			</ul>
			<p>By the end of this chapter, we will have learned about various aspects of the software life cycle to create a robust solution from a cross-functional perspective. We will also discuss additional features that we will need to add to make our solution performant, scalable, resilient to failure, and gain the ability to make changes reliably, repeatably, and rapidly. Furthermore, we will also examine the implications of making these changes and the potential impacts this may have on our bounded contexts and their boundaries.</p>
			<p>Let’s begin!</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Observability</h1>
			<p>In previous chapters, we saw how it is possible to break down an existing application along bounded context <a id="_idIndexMarker712"/>boundaries. We also saw how it is possible to split bounded contexts to be extremely fine-grained, often as physically disparate components. Failure in any of these components can cause disruptions in others that are dependent on them. Obviously, early detection and more importantly attribution to specific components through a combination of proactive and reactive monitoring can ideally prevent or, at the very least, minimize business disruption.</p>
			<p>When it comes to monitoring, most teams seem to think of <strong class="bold">technology runtime metrics</strong> that we associate <a id="_idIndexMarker713"/>with components (such as CPU utilization, memory consumed, queue depths, exception count, and so on).</p>
			<p class="callout-heading">Lending Objectivity to Metrics</p>
			<p class="callout">To make it more formal, we use the terms <strong class="bold">Service-Level Objectives</strong> (<strong class="bold">SLOs</strong>) and <strong class="bold">Service-Level Indicators </strong>(<strong class="bold">SLIs</strong>) specified within a <strong class="bold">Service-Level Agreement</strong> (<strong class="bold">SLA</strong>) to mean the following:</p>
			<ul>
				<li class="callout"><strong class="bold">SLO</strong>: An agreement between <a id="_idIndexMarker714"/>the provider and customer about a specific measurable metric. For example, 99.99% uptime, 100 ms response time for 1,000 concurrent users for requests in the 99th percentile, and so on.</li>
				<li class="callout"><strong class="bold">SLA</strong>: A collection <a id="_idIndexMarker715"/>of SLOs.</li>
				<li class="callout"><strong class="bold">SLI</strong>: The actual <a id="_idIndexMarker716"/>numbers against an SLO. For example, your system might have an uptime SLI of 99.95%.</li>
			</ul>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/>Technology metrics</h2>
			<p>When it comes to monitoring, most teams seem to think of technology runtime metrics that we associate <a id="_idIndexMarker717"/>with components (such as CPU utilization, memory consumed, queue depths, exception count, and so on).</p>
			<p>However, it is just as much if not more important to be able to associate a set of business-relevant metrics (such as the number of LC applications submitted in the last hour, the number of LC applications rejected, and so on) and DevOps metrics (such as lead time, mean time to restore, and so on).</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor179"/>Business metrics</h2>
			<p>An inability to associate and monitor business SLIs with a component may be an indicator of the <a id="_idIndexMarker718"/>component being too fine-grained. On the flip side, if there are too many business SLIs associated with a single component that is of interest to a multitude of business stakeholder groups, it may be an indicator that a more fine-grained decomposition may be justified. At the end of the day, the monitoring apparatus we have in place should be able to tell us if we are violating/meeting/exceeding SLOs.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>DevOps metrics</h2>
			<p>The <strong class="bold">DevOps Research and Assessment</strong> (<strong class="bold">DORA</strong>) research <a id="_idIndexMarker719"/>foundation has published an online quickcheck (<a href="https://www.devops-research.com/quickcheck.html">https://www.devops-research.com/quickcheck.html</a>) tool and report (<a href="https://www.devops-research.com/research.html">https://www.devops-research.com/research.html</a>) to quickly <a id="_idIndexMarker720"/>provide information on how organizations compare with industry peers and how to make progress toward elite status. While discussing <a id="_idIndexMarker721"/>the full nuance of what it <a id="_idIndexMarker722"/>takes to establish a long-term culture of continuous improvement is out of scope for this book, we reference the four key metrics highlighted in the research paper as indicators of software delivery performance:</p>
			<ul>
				<li><strong class="bold">Lead time</strong>: How long does it take to go from code committed to code successfully running in production?</li>
				<li><strong class="bold">Deployment frequency</strong>: How often does your organization deploy code to production or release it to end users?</li>
				<li><strong class="bold">Time to restore</strong>: How long does it generally take to restore service when a service incident or a defect that impacts users occurs?</li>
				<li><strong class="bold">Change failure percentage</strong>: What percentage of changes to production or releases to users results in degraded service?</li>
			</ul>
			<p>When it comes to observability, there is the risk of focusing on specific metrics in isolation and <a id="_idIndexMarker723"/>missing the forest for the trees. To avoid metrics being misused and, more <a id="_idIndexMarker724"/>importantly, running the risk of drawing incorrect conclusions, we recommend the following:</p>
			<ul>
				<li><strong class="bold">Take a holistic view</strong>: Focusing more or less equally on all aspects of the delivery life cycle as opposed to focusing on just a particular area can go a long way. If you are able to include information from planning, requirements intake, development, build, test, deploy, and feedback from running production systems, then you may be able to conclude reasonably that you have a high-performing team.</li>
				<li><strong class="bold">Employ ratcheting</strong>: Having recognized an improvement area, how do you go about setting yourself up for improvement? Setting clear, objective goals that are measurable and trackable (no pun intended) for improvement is paramount to be able to subsequently meet them. In order to ensure that there is continuous incremental improvement, ratcheting is a technique that can be employed. A ratchet is a device that resembles a wrench but is unique in that it only turns in one direction. In this context, ratcheting involves doing the following:<ol><li value="1">Set the current level as the minimum starting point.</li><li>Make a small incremental improvement in a relatively small amount of time.</li><li>Re-adjust the baseline to the new level attained as part of step 2.</li><li>If levels descend below the baseline, take stop-the-line action until baselines are restored.</li><li>Repeat from step 1.</li></ol></li>
			</ul>
			<p>Ratcheting allows teams to set incremental milestones as intermediate goals while moving closer to a much better place all the time.</p>
			<p>Adopting an attitude of constant learning and incremental improvement through ratcheting as <a id="_idIndexMarker725"/>opposed to one that looks to police and penalize can go a long way <a id="_idIndexMarker726"/>toward instituting a system that can be effective.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor181"/>Consistency</h1>
			<p>In the previous chapters, we have spent a lot of energy splitting our system into multiple, fine-grained independent components. For example, the LC application is submitted against <a id="_idIndexMarker727"/>the command-side component, whereas the status of the LC application is serviced by the query side. Because these are distinct components, there will be a time lag during which the two systems are not consistent with each other. So, querying the status of an LC application immediately after submitting may produce a stale response until the time that the query side processes the submit event and updates its internal state. In other words, the command side and the query side are considered to be <em class="italic">eventually consistent</em>. This is one of the trade-offs that we need to embrace when working with distributed systems.</p>
			<p>Eric Brewer (professor emeritus of computer science at the University of California, Berkeley) formalized the <a id="_idIndexMarker728"/>trade-offs involved in building distributed systems in what is called the <em class="italic">CAP theorem</em>. The theorem postulates that distributed systems can either be highly available or consistent in the event of a network partition, not both at the same time. Given the three characteristics, <strong class="bold">c</strong>onsistency, <strong class="bold">a</strong>vailability, and <strong class="bold">p</strong>artition tolerance, the theorem postulates that distributed systems can either be highly available or consistent in the event of a network partition, not both at the same time. This means that distributed applications that are expected to be highly available will have to forsake strong consistency.</p>
			<p>This may make it appear that this is a deal-breaker, but in reality, most real-world business problems are tolerant to being eventually consistent. For example, there may be a requirement that an order cannot be canceled after it has shipped. In an eventually consistent system, there may exist a time window (albeit small) where we may allow a shipped order to be canceled. To deal with such scenarios, we may need to enhance the business process to account for these inconsistencies. For example, before issuing a refund for a canceled order, we may need to validate that the order has not physically shipped or has been returned. Even in the extreme case where we may have erroneously issued a refund for a shipped order, we can request the customer to return it before an expiry period to avoid getting charged. If the customer fails to return the order, we may <a id="_idIndexMarker729"/>charge the customer or write off the amount as lost business. Obviously, all this adds a level of complexity to the solution because we may need to account for edge conditions through a series of compensating actions. If none of this complexity is acceptable and strong consistency is non-negotiable, then shipping and order cancellation functionality will have to be part of the same bounded context.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Performance and scale</h1>
			<p>In previous chapters, we saw how it is possible and sometimes even necessary to break functionality down <a id="_idIndexMarker730"/>into fine-grained components that are physically separated <a id="_idIndexMarker731"/>from each other – requiring a network to collaborate. Let’s assume that this collaboration is achieved in a loosely coupled manner – justifying the need for disparate bounded contexts from a logical perspective.</p>
			<p>Performance is a very important SLO that is typically associated with most applications. When it comes to performance, it is essential to understand the basic terms. This is best illustrated using an example as shown here:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/B16716_Figure_12.1.jpg" alt="Figure 12.1 – The elements of network performance&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – The elements of network performance</p>
			<p>As shown here, the <a id="_idIndexMarker732"/>following terms are relevant in the context of performance: </p>
			<ul>
				<li><strong class="bold">Latency</strong>: The delay introduced by the network (A + B)</li>
				<li><strong class="bold">Response Time</strong>: the total time taken by the system to respond to the user (A + B + C)</li>
				<li><strong class="bold">Bandwidth</strong>: The maximum capacity of the network (D)</li>
				<li><strong class="bold">Throughput</strong>: The amount of data processed in a given amount of time</li>
			</ul>
			<p>The introduction of a network between two components introduces constraints in the form of network latency and bandwidth. Even if processing time on the server is theoretically reduced to zero, latency and bandwidth constraints cannot be avoided. This problem can <a id="_idIndexMarker733"/>only get worse as the number of network hops increases. This means that it <a id="_idIndexMarker734"/>is impossible for networked applications to provide the same level of performance as their non-networked counterparts.</p>
			<p>The need to scale to support a larger number of requests can further complicate things. Given that Moore’s law has slowed down considerably in the last decade or so, it is less feasible to continue scaling up by using more and more powerful machines. This means that beyond a point, scaling out by using multiple instances, and thereby (re-)introducing a reliance on the network, is inevitable.</p>
			<p>This makes it evident that performance and scale requirements can have a significant impact on how we choose to distribute our components. Having a clear understanding of performance and scale SLOs is a necessary prerequisite before attempting to distribute distinct components. On the flip side, if you are in a situation where you already have distributed components that are not meeting performance and scale SLOs, one option is to aggregate them back together. If that is not feasible, it may be worth embracing alternative customer experiences along with a non-blocking, event-driven style of architecture to create a perception of better performance.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Trunk-based development</h1>
			<p>Eric Evans, the inventor of DDD, talks about how <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) helps preserve the sanctity of <a id="_idIndexMarker735"/>the domain model within a bounded <a id="_idIndexMarker736"/>context. When more than one person works in the same bounded context, it tends to fragment. Obviously, the bigger the team, the higher the likelihood of this problem occurring. Even a team as small as three or four people can encounter serious issues. We have also seen that beyond a point, there may be diminishing returns if we try to break the system into extremely fine-grained bounded contexts.</p>
			<p>This makes it very important to institute a process of merging/integrating all code and other implementation artifacts frequently, aided by <strong class="bold">automated tests</strong> to flag such fragmentation. In <a id="_idIndexMarker737"/>addition, this allows the team to apply the ubiquitous language relentlessly, each time refining the domain model to represent the problem more accurately. In other words, it is critical to practice continuous integration. Many teams make use of a CI server to run tests but tend to postpone integration <a id="_idIndexMarker738"/>until very late making use of an excessive number of long-living branches (popularized by Gitflow; <a href="https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow">https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow</a> and merge <a id="_idIndexMarker739"/>requests – practicing an anti-pattern known as CI theatre (<a href="https://www.gocd.org/2017/05/16/its-not-CI-its-CI-theatre.html">https://www.gocd.org/2017/05/16/its-not-CI-its-CI-theatre.html</a>).</p>
			<p>An alternative to branch-based development is <em class="italic">trunk-based development</em> where each developer works in incremental batches and merges that work into the main (also called trunk) branch at <a id="_idIndexMarker740"/>least once (and potentially several times) a day. The DORA team has published research (<a href="https://services.google.com/fh/files/misc/state-of-devops-2021.pdf#page=27">https://services.google.com/fh/files/misc/state-of-devops-2021.pdf#page=27</a>) that shows that elite performers practice trunk-based development to maximize the effectiveness of their CI practice and by extension their ability to continuously enhance their domain models and keep up with changing business needs.</p>
			<p>In an ideal world, every commit to the trunk would constitute finished, production-ready work. But it is also fairly normal for certain pieces of work to take longer to complete. This may make it appear that there is a need to forsake trunk-based development and resort to branch-based development. However, there is no need to compromise the continuous integration flow to accommodate for such eventualities. Paul Hammant (<a href="https://paulhammant.com/">https://paulhammant.com/</a>) talks about this technique called <em class="italic">branch by abstraction</em> where the effects of unfinished pieces of work are hidden behind an <a id="_idIndexMarker741"/>abstraction layer. This abstraction layer is typically implemented by either making the new piece of functionality hidden from the end user or in <a id="_idIndexMarker742"/>more sophisticated cases, using feature flags (<a href="https://martinfowler.com/articles/feature-toggles.html">https://martinfowler.com/articles/feature-toggles.html</a>).</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor184"/>Continuous testing</h1>
			<p>In an ideal world, continuous integration will enable us to adopt continuous testing, which provides us with constant and early feedback. This is essential because our bounded contexts <a id="_idIndexMarker743"/>and the resulting domain models are in a constant state of evolution. Without the bedrock of a stable suite of tests, it can become very hard to sustain a reliable process. Approaches such as the test pyramid, testing trophy, honeycomb, and so on are acknowledged as reasonable ways to implement a sound continuous testing strategy. All of these approaches are based on the premise that a large number of cheap (computationally and cognitively) unit tests form the foundation of the strategy, with the number of tests in other categories (service, UI, manual, and so on) reducing as we move through the chain.</p>
			<p>However, we are in this new world of fine-grained components that work by communicating with each other. Hence, there is a bigger need to verify interactions at the periphery in a robust manner. Unit tests alone that rely mostly on mocks and stubs may not suffice because the behavior of collaborators may change inadvertently. This may lead to a situation where unit tests may run successfully, but the overall functionality may be broken. This may cause teams to lose faith in the practice of unit testing as a whole and resort to using more end-to-end functional tests. However, these styles of tests can be extremely expensive (<a href="https://www.youtube.com/watch?v=VDfX44fZoMc">https://www.youtube.com/watch?v=VDfX44fZoMc</a>) to set up and maintain, especially when we are looking to automate them. Consequently, most teams ignore the results of a majority of automated testing methods and rely almost exclusively on manual testing to verify anything but the most trivial functionality.</p>
			<p>Any manual testing requires most if not all functionality to be ready before any meaningful testing can commence. Furthermore, it is time-consuming, error-prone, and usually not repeatable. Consequently, almost all testing can be carried out only when it is very close to the end, rendering the idea of continuous testing a pipe dream. Despite all its limitations, teams continue to rely on manual testing because it seems to provide the most psychological safety in comparison to its automated counterparts.</p>
			<p>In an ideal world, what we need is the speed of unit tests and the confidence provided by manual testing. We will look at a few specific forms of testing that can help restore the balance.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor185"/>Contract testing</h2>
			<p>The limitation of unit testing is that the assumptions made in mocks/stubs can be invalid or become stale <a id="_idIndexMarker744"/>as producers make changes to the contract. On the other <a id="_idIndexMarker745"/>hand, manual tests suffer from being slow and wasteful. Contract tests can provide a means to bridge the gap by providing a happy medium where the producer and consumer share an executable contract that both producer and consumer can rely on as functionality changes/evolves. At a high level, this works in the manner depicted here:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/B16716_Figure_12.2.jpg" alt="Figure 12.2 – Contract testing: high-level flow&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Contract testing: high-level flow</p>
			<p>This allows the consumers and the producers to work collaboratively, and get feedback a lot earlier in the cycle. For the consumer, they get to participate in sharing their expectations with the producer and make use of versioned, producer-approved stubs for their own testing without having to depend on the producer’s real system. Likewise, producers gain a deeper understanding of how their services are consumed, setting them free to make bolder changes as long as they remain compatible.</p>
			<p class="callout-heading">Test-First Design</p>
			<p class="callout">The essence of domain-driven design is all about gaining as thorough an understanding of the problem <a id="_idIndexMarker746"/>in order to solve the problem right. Test-first design enables gaining a better understanding of the problem because it mitigates the risk of becoming biased by the solution we have built. In addition, it also promotes the automated verification of these requirements, which allows them to be used as an effective aid to regression testing. We are strong proponents of this practice for this reason and encourage you to consider adopting TDD as a core practice to accentuate your effectiveness with DDD.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/>Mutation testing</h2>
			<p>A lot of teams author a variety of tests to ensure that they are building a high-quality solution. Test coverage <a id="_idIndexMarker747"/>is typically used as a quantitative measure to assess <a id="_idIndexMarker748"/>the quality of testing. However, test coverage is a necessary but not sufficient condition to establish test quality. Low test coverage almost definitely means there is a test quality problem, whereas high coverage does not imply better tests. In an ideal world, even a single line change in production code (caused by a change in business requirements), without changing test code, will result in a test failure. If this can be guaranteed for every single change across the code base, you may be able to safely rely on such a test suite.</p>
			<p>Mutation testing is a practice <a id="_idIndexMarker749"/>that automatically inserts small bugs in production code (called <em class="italic">mutants</em>) and reruns an existing suite of tests to ascertain the quality of tests. If your tests failed, the mutant is killed. Whereas if your tests passed, the mutant survives. The higher the number of mutants killed, the more effective your tests are.</p>
			<p>For example, it may apply mutations such as inverting conditionals, replacing relational operators, returning nulls from methods, and so on, and then you can check the effect this has on your existing tests. If no tests fail despite these mutations, these tests may not be as helpful as you hoped them to be. This allows us to draw more objective conclusions about the quality of our tests. Given how it works (by mutating code), it is computationally intensive and hence may take a long time to run. If you employ a test-first design and have a fast suite of unit tests, mutation testing can be a great complement that can help discover missed requirements and/or test cases earlier in the development cycle. From that perspective, we see it as an invaluable tool to augment the adoption of <a id="_idIndexMarker750"/>DDD within teams. Tools such as PITest (<a href="https://pitest.org/">https://pitest.org/</a>) are a great tool to perform mutation testing in your Java applications.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/>Chaos testing</h2>
			<p>As we have seen earlier, mutation testing can help point out chinks in the functional aspects of your <a id="_idIndexMarker751"/>application. Chaos testing plays a similar role to help <a id="_idIndexMarker752"/>identify shortcomings in meeting non-functional requirements caused by reliance on network and infrastructure. It started becoming popular through the use of large-scale distributed, cloud-based architectures pioneered by <a id="_idIndexMarker753"/>companies such as Amazon, Netflix, and so on. Netflix initially released a tool called Chaos Monkey (<a href="https://netflix.github.io/chaosmonkey/">https://netflix.github.io/chaosmonkey/</a>) that randomly terminated instances in production(!) to ensure that engineers implement services that are resilient to failure. They followed this by releasing a set of related tools, collectively called the Simian Army (which is now defunct) to test a variety of non-functional aspects such as latency, security compliance, unused resources, and so on.</p>
			<p>While Netflix performs this style of testing in production, the rest of us will benefit immensely if we adopt these practices even in lower environments at the outset. From a strategic perspective, chaos testing can provide feedback on the amount of coupling between components and whether the boundaries of these components are appropriate. For example, if a component that you are dependent on goes down or experiences problems, does this take you down as well? If so, are there ways to mitigate this? It can also provide feedback about your monitoring and alerting apparatus. From a tactical perspective, it can provide insights into the shortcomings of the invocation style being used to communicate among components.</p>
			<p>In this section, we have chosen to highlight contract testing, mutation testing, and chaos testing because we see them as game-changers in the application of DDD. Teams will benefit by looking at these methods as augmentations to other testing methods when coming up with a well-rounded testing strategy.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor188"/>Deployment automation</h1>
			<p>The intent of applying <a id="_idIndexMarker754"/>domain-driven design is to create an ecosystem of loosely coupled components – so that each of these components can evolve independently of each other. This includes how these components are deployed to production. At a high level, we have at least three styles of deployment:</p>
			<ul>
				<li><strong class="bold">Single-process monolith</strong>: Where large portions of the application are deployed as a single unit, with all components that are included in the deployment running in a single process</li>
				<li><strong class="bold">Distributed monolith</strong>: Where the application is split into multiple components with each running in its own process and/or host, but deployed as a single unit and/or requiring non-trivial amounts of coordination and tight coupling among components and their owners</li>
				<li><strong class="bold">Independent components</strong>: Where the application is split into multiple components with each running in its own process and/or host, deployed independently of each other and requiring minimal to no coordination among component owners</li>
			</ul>
			<p>We also have a number of deployment strategies that we can employ. We list some of the more popular ones <a id="_idIndexMarker755"/>in order of increasing complexity and richness:</p>
			<ul>
				<li><strong class="bold">Basic</strong>: Likely the oldest style of deployment where the newer version of the application replaces the old, usually with some amount of downtime. Rollback typically means redeploying the previously live version, again taking some amount of downtime. This is a fairly common deployment strategy for those applications where a certain amount of downtime is acceptable. This may include non-business critical applications and/or third-party packages where we do not have a say in how those applications manage their deployments. In the case of certain monoliths, this may be the only feasible option due to the overall complexity of the system as a whole. This style of deployment typically starts out being fairly <a id="_idIndexMarker756"/>simple and well understood and may suffice for non-critical applications. On the flip side, it requires the deployment and release to happen in one single tightly coupled step and may involve some amount of downtime.</li>
				<li><strong class="bold">Blue-green</strong>: A deployment strategy that makes use of two identical environments, a “blue” and a “green” environment, with one representing the current production and another representing the newer version. The current version continues to service traffic, while testing and acceptance are carried out on the new version without exposing it to end users. User traffic is switched to the newer version once testing activities are deemed to be successfully completed. It is pertinent to note that live user traffic is directed only to one environment at any given time. This style of deployment enables deployment with (near) zero downtime and also allows decoupling of the process of deployment and release. Rollbacks are easier because it simply means redirecting traffic to <a id="_idIndexMarker757"/>the older version. On the other hand, it requires double the amount of capacity at least during the time of deployment. This may make it cost-prohibitive for monolithic applications.</li>
				<li><strong class="bold">Rolling</strong>: A deployment strategy where a small subset of current version instances is incrementally replaced by newer version instances. Both old and new versions of the software continue to run in parallel until all instances of the old are replaced with new ones. In simple cases, rollback typically means replacing the newer version instances with older ones. This style of deployment also enables zero-downtime deployment, while also allowing side-by-side testing of old and new versions with real users. Rolling deployments can make rollbacks relatively easy by aborting the introduction of instances of the new version and re-introducing the old version and hence can reduce the <em class="italic">blast radius</em> of a bad release. Unlike the case with blue-green deployments, here deployment and release cannot be decoupled. Deployment means that the system is released (at least for a subset of users).</li>
				<li><strong class="bold">Canary</strong>: A variation of the rolling deployment where traffic is routed to newer instances in a controlled and phased manner, typically an increasing proportion of request volume (for example, 2% → 25% → 75% → 100% of users). This deployment style <a id="_idIndexMarker758"/>enables more fine-grained control of the extent of the release as compared to rolling deployment.</li>
				<li><strong class="bold">A/B deployment</strong>: A variation of canary deployment where multiple versions (with one or more variations) of new functionality may run simultaneously as “experiments” along with the current version. Further, these variations may be targeted to specific sets of users. It allows for testing more than two combinations at the same time with real users.</li>
			</ul>
			<p>When working with monolithic applications, teams are usually forced to restrict themselves to either basic or at the most blue-green deployments because the cost and complexity involved in adopting more sophisticated deployment strategies are a lot higher. On the other hand, distributed monoliths make this even more complicated because it now requires coordination among physically disparate components and teams. As long as we are able to maintain a balance between component granularity and coupling, we should be able to support a variety of advanced deployment strategies.</p>
			<p>In today’s modern ecosystem where there is a tremendous amount of competition to deliver new features and <a id="_idIndexMarker759"/>innovate faster, there is a need to support more complex forms of deployment with the least amount of risk and disruption to the business. If supporting flexible deployment strategies proves to be too hard, there is very likely a need to re-examine your context boundaries.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/>Refactoring</h1>
			<p>Over a period of time, there will be a need to realign context boundaries, domain events, APIs, and so on. There tends to be a stigma associated with things not working perfectly the first time <a id="_idIndexMarker760"/>and justifying the need for refactoring at the inter-component scale. However, this may be required for multiple reasons outside our control, ranging from competitor ecosystem changes, evolving/misunderstood requirements, inability to meet non-functional requirements, organizational and team responsibility changes, and so on. Hence, refactoring is a core discipline that software teams will need to embrace as a first-class practice.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We are covering only the strategic (inter-component) aspects of refactoring in this chapter. There are several great works on the tactical (intra-component) aspects of refactoring, such <a id="_idIndexMarker761"/>as Martin Fowler’s <em class="italic">Refactoring</em> (<a href="https://refactoring.com/">https://refactoring.com/</a>) book and Michael Feathers’ <em class="italic">Working Effectively with Legacy Code</em>, among others.</p>
			<p>From a strategic perspective, this may mean having to break an existing monolith into finer-grained bounded <a id="_idIndexMarker762"/>contexts or merge fine-grained bounded contexts into more coarse-grained ones. Let’s look at each of these in turn.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor190"/>Breaking an existing monolith</h2>
			<p>In previous chapters (10 and 11), we have looked at how it is possible to break an existing monolith into <a id="_idIndexMarker763"/>finer-grained components. However, it is arguable that the monolith was relatively well-structured to start with. Lots of teams may not be as fortunate. In such a case, here are some prerequisites that may need to be fulfilled:</p>
			<ul>
				<li><strong class="bold">Perform tactical refactorings</strong>: This will allow you to gain a better understanding of <a id="_idIndexMarker764"/>the existing system. To do this, start with a set of fitness functions (<a href="https://en.wikipedia.org/wiki/Fitness_function">https://en.wikipedia.org/wiki/Fitness_function</a>) and a set of black-box functional tests, perform a refactor, and then replace the functional tests with faster-running unit tests. Finally, use the fitness functions to evaluate the success of the effort. Repeat this process until there is a level of comfort to attempt more complex refactorings.</li>
			</ul>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/B16716_Figure_12.3_NEW.jpg" alt="Figure 12.3 – Continuous improvement loop&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Continuous improvement loop</p>
			<ul>
				<li><strong class="bold">Introduce domain events</strong>: Identify software seams (<a href="http://wiki.c2.com/?SoftwareSeam">http://wiki.c2.com/?SoftwareSeam</a>) and publish domain events along those seams. Use the domain <a id="_idIndexMarker765"/>events to start decoupling the producers and the consumers.</li>
				<li><strong class="bold">Pick low-hanging components</strong>: If possible, pick areas with low afferent coupling and low to medium complexity at the outset. This will allow you to get a firmer grasp of <a id="_idIndexMarker766"/>applying these techniques before attempting more complex ones. Please refer to <a href="B16716_10_Final_NM_ePub.xhtml#_idTextAnchor150"><em class="italic">Chapter 10</em></a>, <em class="italic">Beginning the Decomposition Journey</em> and <a href="B16716_11_Final_NM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 11</em></a>, <em class="italic">Decomposing into Finer-Grained Components</em> for details on how to proceed.</li>
			</ul>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor191"/>Merging into coarse-grained bounded contexts</h2>
			<p>Merging two <a id="_idIndexMarker767"/>distinct bounded contexts can be relatively less complex than breaking down an existing one. However, there are a few nuances that are worth paying attention to, in the following order:</p>
			<ul>
				<li><strong class="bold">Unification of the ubiquitous language</strong>: In <a href="B16716_09_Final_NM_ePub.xhtml#_idTextAnchor138"><em class="italic">Chapter 9</em></a>, <em class="italic">Integrating with External Systems</em>, we examined a variety of ways in which bounded contexts can integrate with each other. If the relationship between these bounded contexts is symmetric, there may be less work to do. This is because, in a symmetric relationship, there likely exists a lot of synergies in the first place. However, if the relationship is asymmetric, for example, through an open-host service on the producer side and an anti-corruption layer on the consuming side, it means that there are possibly two varying ubiquitous languages and likely distinct domain models at play. Careful thought will need to be applied to arrive at a ubiquitous language that is applicable across the newly merged bounded context.</li>
				<li><strong class="bold">Adjust internal domain models</strong>: Adoption of a common ubiquitous language primarily means making use of a common domain model across the newly merged bounded context. This means that the aggregates, entities, and value objects will need to be unified, which may then require changes at the persistence layer as well. If there are domain events that are published and consumed exclusively between these components, those domain events may be candidates to be retired. At this stage, it may not be prudent to make any changes to any public interfaces – specifically those exposed using an open-host service (for example, public HTTP APIs and other domain events).</li>
				<li><strong class="bold">Adjust the public API design</strong>: As a final step, it will be prudent to refactor redundant and/or inefficient public interfaces to conclude the exercise and derive the intended benefits.</li>
			</ul>
			<p>It is pertinent to note <a id="_idIndexMarker768"/>that this style of continuous improvement can be extremely challenging to adopt without the solid bedrock of a sound set of engineering practices, specifically the testing and deployment automation practices that we discussed in this section.</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor192"/>Invocation style</h1>
			<p>When integrating two <a id="_idIndexMarker769"/>bounded contexts that are running in distinct processes, there are two ways to consummate interactions: synchronous and asynchronous.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor193"/>Synchronous invocation</h2>
			<p>The client blocks until the server provides a response. Optionally, implementations can choose to wait for <a id="_idIndexMarker770"/>an amount of time for the invoked operation to complete <a id="_idIndexMarker771"/>before timing out. An example of such an interaction is a blocking HTTP call made to start a new LC application like so:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B16716_Figure_12.4.jpg" alt="Figure 12.4 – Synchronous invocation&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Synchronous invocation</p>
			<p>When the call returns successfully, the client is sure that their request to create a new LC application has worked. If the server is slow to respond, it can result in a performance <a id="_idIndexMarker772"/>bottleneck, especially in high-scale scenarios. To cope <a id="_idIndexMarker773"/>with this, the client and the server may agree on a response time SLO for that interaction. The client can choose to wait for a response from the server for the agreed amount of time after which the client times out the request and considers it a failure. Given that the client blocks on a server <a id="_idIndexMarker774"/>response, it is not able to do anything else <a id="_idIndexMarker775"/>while it waits, even though it may have the resources to do other things. To deal with this, the client can employ an asynchronous invocation.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor194"/>Asynchronous invocation</h2>
			<p>In an asynchronous <a id="_idIndexMarker776"/>style of invocation, the client interacts <a id="_idIndexMarker777"/>with the server in a manner that frees it to perform other activities. There are a few ways to do this:</p>
			<ul>
				<li><strong class="bold">Fire and forget</strong>: The client initiates a request with the server, but does not wait for a response from the server and also does not care about the outcome. Such a style of interaction may suffice for <em class="italic">low-priority</em> activities such as logging to a remote server, push notifications, and so on.</li>
			</ul>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B16716_Figure_12.5.jpg" alt="Figure 12.5 – Fire and forget&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Fire and forget</p>
			<ul>
				<li><strong class="bold">Deferred response</strong>: In some (many?) cases, the client may need to know the outcome of the request they had previously made. If the server supports it, the client can submit a request, just wait for a confirmation that the request was received along with an identifier of the resource to be tracked, and then poll the server to track the status of its original request as shown here:</li>
			</ul>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/B16716_Figure_12.6.jpg" alt="Figure 12.6 – Deferred response using poll&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Deferred response using poll</p>
			<ul>
				<li><strong class="bold">Request with callback</strong>: When the client polls for the response, the server may not be finished with processing the original request. This means that the client may <a id="_idIndexMarker778"/>need to poll the server more than once to <a id="_idIndexMarker779"/>understand the status of the request, which can be wasteful. An alternative is for the server to push a response back to the client when it has finished processing by invoking a callback that the client provided when making the request.</li>
			</ul>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B16716_Figure_12.7.jpg" alt="Figure 12.7 – Deferred response using callback&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Deferred response using callback</p>
			<p>Given that these interactions happen over a network that can be unreliable, clients and servers need to <a id="_idIndexMarker780"/>employ a variety of techniques to achieve some <a id="_idIndexMarker781"/>semblance of reliability. For example, clients may need to implement support for timeouts, retries, compensating transactions, client-side load balancing, and so on. Similarly, the server may need to protect itself from errant clients by making use of techniques such as rate limiters, circuit breakers, bulkheads, fallbacks, health endpoints, and so on.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Elaborating on the specific techniques mentioned here is out of scope for this book. Books such as <em class="italic">Release It</em> and <em class="italic">Mastering Non-Functional Requirements</em> cover these patterns in a lot more depth.</p>
			<p>In a lot of cases, there is usually a need to employ a combination of several of the preceding techniques to provide a resilient solution. Just as we discussed in the logging section, mixing these concerns with core business logic can obscure the original intent of the problem. In order to avoid this, it is advisable to apply these patterns in a manner that is peripheral to core <a id="_idIndexMarker782"/>business logic. It may also be prudent to consider the use of libraries such <a id="_idIndexMarker783"/>as Resilience4j (<a href="https://resilience4j.readme.io/">https://resilience4j.readme.io/</a>) or Sentinel (<a href="https://github.com/alibaba/Sentinel">https://github.com/alibaba/Sentinel</a>).</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/>Logging</h1>
			<p>Application logging is one of the most fundamental aids when it comes to diagnosing issues in running <a id="_idIndexMarker784"/>code. In a lot of code bases, logging tends to be an after-thought where developers add log statements only after they encounter problems. This results in log statements being strewn almost randomly throughout the code base. Here is a simple example of code within a command handler to log its execution time among other things:</p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/ch12-1.jpg" alt=""/>
				</div>
			</div>
			<p>There is no doubt that this logging code can be invaluable when troubleshooting issues. However, when <a id="_idIndexMarker785"/>we look at the preceding code, the logging code seems to dominate the entire method obscuring the domain logic. This might feel innocuous, but when this is done in multiple places, it can get quite repetitive, cumbersome, and error-prone – compromising readability. In fact, we have seen cases where seemingly innocent log statements have introduced performance issues (for example, within a loop with an expensive argument evaluation) or even bugs (for example, the dreaded <strong class="source-inline">NullPointerException</strong> when trying to evaluate arguments). In our opinion, it is very important to treat logging as a first-class citizen and afford it the same rigor as core domain logic. This means that it needs to obey all the good practices that we associate with well-factored production code.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor196"/>Segregating logging code</h2>
			<p>Ideally, we will be <a id="_idIndexMarker786"/>able to maintain a balance between readability and debuggability. This can be achieved if we can segregate these two concerns. One way to segregate this cross-cutting logic is to use aspect-oriented <a id="_idIndexMarker787"/>programming (read more about AOP  at <a href="https://www.eclipse.org/aspectj/">https://www.eclipse.org/aspectj/</a> and <a href="https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop">https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop</a>) as shown here:</p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/ch12-2.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Note</p>
			<p class="callout">A pointcut defines an <strong class="source-inline">around</strong> aspect for all methods annotated with the <strong class="source-inline">@CommandHandler</strong> annotation. In this example, we are using compile-time weaving as opposed to runtime weaving available through the Spring Framework, to inject execution time logic using AspectJ. You can find more details on the pros and cons of using specific weaving <a id="_idIndexMarker788"/>techniques in this article (<a href="https://www.baeldung.com/spring-aop-vs-aspectj">https://www.baeldung.com/spring-aop-vs-aspectj</a>).</p>
			<p>In the style shown here, we have separated logging code from application code through the use of aspect-oriented programming. In the example, the logging code applies to all methods annotated with the <strong class="source-inline">@CommandHandler</strong> annotation. This has the advantage that all such methods will now produce consistent entry/exit logging statements. On the flip side, if there is a <a id="_idIndexMarker789"/>need for additional logging for a specific command handler, it will still have to be done within the body of that method. If you see yourself requiring lots of ad hoc logging statements in addition to simple entry/exit logs, it might be a smell and a sign that your methods may need to be refactored.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor197"/>Dealing with sensitive data</h2>
			<p>In general, when adding logging code, it helps to include as much context as possible. This can be challenging <a id="_idIndexMarker790"/>in certain domains such as healthcare or finance where there may be legal/regulatory requirements to restrict access to sensitive information. For example, during the LC application process, we may need to perform a credit check for the applicant using their government-issued identifier such as the <strong class="bold">social security number</strong> (<strong class="bold">SSN</strong>) in the USA. In such cases, it is common to mask a significant portion of this information in the logs to <a id="_idIndexMarker791"/>maintain a balance between privacy and debuggability. Implementing such masking logic is a domain concern. This accentuates the need to use value types (as opposed to primitives) to control the appropriate behavior. For example, overriding the <strong class="source-inline">toString</strong> method of an <strong class="source-inline">SSN</strong> value type can ensure that the sanctity of the business need is met uniformly within the bounded context.</p>
			<p>While masking may suffice in a majority of use cases, it suffers from the limitation of not being able to access the original information even by authorized users. If this is a requirement, it may be <a id="_idIndexMarker792"/>necessary to make use of <strong class="bold">tokenization</strong> (the process of replacing a sensitive piece of information with a non-sensitive placeholder value called a <strong class="bold">token</strong>) solution. This can allow logging tokenized values in an unrestricted manner within the bounded context and in general, can be a lot more secure. But this can mean having to deal with the additional complexity of another bounded context to provide tokenized values and authorization controls when the real <a id="_idIndexMarker793"/>value needs to be accessed.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor198"/>Log format</h2>
			<p>Thus far, we have focused on just the log message. However, logging is more than just that. It is typical to <a id="_idIndexMarker794"/>include additional information such as the time of occurrence, log level, and so on to aid in rapid troubleshooting. For example, Spring Boot uses the following log format by default:</p>
			<p><strong class="source-inline">2022-06-05 10:57:51.253  INFO 45469 --- [ost-startStop-1] c.p.lc.app.domain.LCApplication            : Root WebApplicationContext: Ending StartNewLCApplication in 1200495 ns.</strong></p>
			<p>While this is an excellent default, it still is primarily unstructured text with certain information being lost in order to improve readability (for example, the logger name is abbreviated). While logs are primarily meant to be consumed by humans, a large volume of logs can get in the way of being able to locate the relevant logs. So it is important to produce logs that are also machine-friendly so that they can be easily indexed, searched, filtered, and so on. In other words, using a <em class="italic">structured logging</em> format like the one shown next can go a long way toward meeting the goals of both machine and human readability:</p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/ch12-3.jpg" alt=""/>
				</div>
			</div>
			<p>Making use of a structured log format elevates their use from being just a debugging tool to becoming <a id="_idIndexMarker795"/>yet another rich and cheap source to derive actionable business insights.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">While it may be tempting to pick a custom log format, we strongly recommend picking formats that are <a id="_idIndexMarker796"/>compatible with popular <a id="_idIndexMarker797"/>ones such as Apache’s <strong class="bold">Common Log Format</strong> (<strong class="bold">CLF</strong>) (<a href="https://httpd.apache.org/docs/current/logs.html#common">https://httpd.apache.org/docs/current/logs.html#common</a>) or Logstash’s default format (<a href="https://github.com/logfellow/logstash-logback-encoder#standard-fields">https://github.com/logfellow/logstash-logback-encoder#standard-fields</a>).</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor199"/>Log aggregation</h2>
			<p>The fact that our applications are decomposed into multiple components with each usually running <a id="_idIndexMarker798"/>multiple instances means that this can produce a lot of logs that are disconnected from each other. To be able to work with these logs meaningfully, we need to aggregate and sequence them chronologically. It may be worth considering the use of a formal log aggregation solution for this purpose. Using a structured logging solution as previously discussed can go a long way when working with logs from multiple systems.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information <a id="_idIndexMarker799"/>on logging best practices, please refer to this logging cheatsheet from OWASP (<a href="https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Logging_Cheat_Sheet.md">https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Logging_Cheat_Sheet.md</a>) and also <a id="_idIndexMarker800"/>this article on the art of logging (<a href="https://www.codeproject.com/Articles/42354/The-Art-of-Logging">https://www.codeproject.com/Articles/42354/The-Art-of-Logging</a>).</p>
			<p>Aggregating logs in one place allows us to view diagnostic information from multiple applications. However, we still need to correlate this information when in the midst of a flow. Distributed tracing solutions can help with this. Let’s look at this next.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/>Tracing</h2>
			<p>Imagine a situation where <a id="_idIndexMarker801"/>an applicant submitted an LC application through the UI. When all goes well, within a few milliseconds, the applicant should get a notification of successful submission as shown here:</p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/B16716_Figure_12.8.jpg" alt="Figure 12.8 – Submit LC application flow&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Submit LC application flow</p>
			<p>Even in this simple example, there are several components involved, each of which produces logs of its own. When an engineer is looking to diagnose an issue, there is a need to correlate log entries from multiple components. In order to accomplish this, there is a need to introduce a correlation identifier as close to the start of the interaction and propagate it across component boundaries. Furthermore, log entries in each component need to carry this correlation identifier as they produce logs. Doing this will allow us to view log entries spanning process boundaries using the correlation identifier as a unifying thread. In technical terms, the entire flow is called a <em class="italic">trace</em>, and each segment within <a id="_idIndexMarker802"/>the flow is <a id="_idIndexMarker803"/>called a <em class="italic">span</em>. This process of instrumenting log <a id="_idIndexMarker804"/>entries with such information is termed <em class="italic">distributed tracing</em>.</p>
			<p>As is evident here, user flows may – and usually do – span more than one bounded context. For this to work effectively, bounded contexts need to agree on propagating trace and span identifiers <a id="_idIndexMarker805"/>uniformly. Tools such as Spring Cloud Sleuth and OpenTracing can help simplify implementation for teams using disparate technology stacks.</p>
			<p>Fundamentally, distributed tracing visualizations can aid in diagnosing performance bottlenecks and chattiness between components. But what may not be obvious is the insights it can provide in gaining a richer understanding of how components interact in an end-to-end user journey. In a lot of ways, this can be thought of as a near real-time context map visualization of your system, and how components are coupled with each other. From a DDD perspective, this can provide greater insights into re-evaluating bounded context boundaries if necessary. For this reason, we strongly recommend making it easy to set up and configure distributed tracing apparatus painlessly right from the outset.</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor201"/>Versioning</h1>
			<p>When we are working with a monolithic application, we have large portions bundled as a single cohesive unit. This means that other than third-party dependencies, we don’t have to worry about explicitly versioning our own components. However, when we start breaking components into their individual deployable units, there is a need to pay careful attention to how the components, APIs, and data elements of our solution are versioned. Let’s look at each in turn.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor202"/>Components</h2>
			<p>When we create <a id="_idIndexMarker806"/>components, there are two broad categories – those that are <strong class="bold">deployed</strong> on their own and those that are <strong class="bold">embedded</strong> within another component. In the case of deployable components, there is a need to use an explicit version to identify specific instances of the component, even if only for deployment purposes. In the case of the embedded component, again there is a need to use an explicit version because other components need to understand what instance they depend upon. In other words, <em class="italic">all components</em> need to have a version to uniquely identify themselves.</p>
			<p>It follows that we then need to choose a sound versioning strategy for our components. We recommend the <a id="_idIndexMarker807"/>use of semantic versioning (<a href="https://semver.org/">https://semver.org/</a>), which uses a <a id="_idIndexMarker808"/>version identifier that uses <a id="_idIndexMarker809"/>three numeric components that match the <strong class="bold">MAJOR.MINOR.PATCH</strong> scheme:</p>
			<ul>
				<li><strong class="bold">MAJOR</strong>: Increment when you make backward-incompatible changes.</li>
				<li><strong class="bold">MINOR</strong>: Increment when you add functionality in a backward-compatible manner.</li>
				<li><strong class="bold">PATCH</strong>: Increment when you make backward-compatible bug fixes.</li>
			</ul>
			<p>In addition, we can make use of optional extensions to indicate pre-release and build metadata. For example, the version identifier for our component might read 3.4.1-RC1 to reflect that this is a release candidate for version 3.4.1 of our component. Using a standard versioning scheme enables the use of build tools such as Maven and Gradle to declare fine-grained upgrade rules and constraints for direct and transitive dependencies. A good practice here is to declare dependencies without versions and make <a id="_idIndexMarker810"/>use of dependency management (<a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Management">https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Management</a>) or dependency <a id="_idIndexMarker811"/>constraints (https://docs.gradle.org/current/userguide/dependency_constraints.html#sec:adding-constraints-transitive-deps) to centralize version management of dependent components.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor203"/>APIs</h2>
			<p>As producers, we <a id="_idIndexMarker812"/>expose APIs in a number of ways. In this case, we are specifically referring to APIs made available over remote interfaces such as HTTP, events, and so on. When it comes to APIs, first and foremost, it is important to keep consuming applications functionally. One effective way of making this possible is by thinking <a id="_idIndexMarker813"/>from the consumer’s standpoint and embracing consumer-driven contracts (<a href="https://martinfowler.com/articles/consumerDrivenContracts.html">https://martinfowler.com/articles/consumerDrivenContracts.html</a>).</p>
			<p>From a consumer’s perspective, the robustness principle (Postel’s law) applies: <em class="italic">be conservative in what you send, be liberal in what you accept</em>. In other words, when sending requests to providers, strictly obey the constraints laid down by the producer. For example, don’t send unexpected data in the request. Whereas, when receiving responses, be tolerant towards what you get from the producer. For example, ignore unknown attributes in the response as long as all the attributes you expect are present. This will allow producers to evolve without breaking existing consumers.</p>
			<p>Our recommendation is to keep APIs versionless for as long as possible by continuing to maintain backward <a id="_idIndexMarker814"/>compatibility. Despite all our efforts, there may come a need to make breaking changes to our APIs. Breaking changes include the following:</p>
			<ul>
				<li>Removing/renaming one or more attributes</li>
				<li>Changing the type of one or more existing attributes</li>
				<li>Changing the format of the request/response</li>
			</ul>
			<p>In such cases, make use of a version identifier to indicate major version changes (for example, v2 to v3). Common options include specifying the version in the URI, in a header, or in the payload. But as we have mentioned earlier, API versioning needs to be used sparingly. If you find yourself in a situation where you are required to introduce backward-incompatible changes frequently, it might be an indicator of requirements being misunderstood and whether DDD principles are truly being applied.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor204"/>Data</h2>
			<p>In a world of well-defined bounded contexts, we should no longer be in a situation where we need to <a id="_idIndexMarker815"/>expose data directly to our consumers. However, there may be situations where we may need to integrate by directly exposing data to our consumers. For example, we may have to expose a reporting database for analytical purposes. All the good practices that we outlined for APIs apply to data as well.</p>
			<p>In addition, from a producer’s perspective, there will be a need to evolve the data schema to accommodate changing business requirements. When working with relational databases, using <a id="_idIndexMarker816"/>good schema migration tools such as Liquibase (<a href="https://liquibase.org/">https://liquibase.org/</a>) or Flyway (<a href="https://flywaydb.org/">https://flywaydb.org/</a>) can go a long way. NoSQL databases <a id="_idIndexMarker817"/>also have <a id="_idIndexMarker818"/>similar tools <a id="_idIndexMarker819"/>such as MongoBee (<a href="https://github.com/mongobee/mongobee">https://github.com/mongobee/mongobee</a>) and Cassandra-Migration (<a href="https://cassandra.tools/cassandra-migration">https://cassandra.tools/cassandra-migration</a>).</p>
			<p>In this context, it is pertinent to think about data as a product and apply product thinking to domain-aligned data. For more information, please refer to this article on how to move from a monolithic data lake to a distributed data mesh (<a href="https://martinfowler.com/articles/data-monolith-to-mesh.html#DomainDataAsAProduct">https://martinfowler.com/articles/data-monolith-to-mesh.html#DomainDataAsAProduct</a>).</p>
			<p>It is not uncommon to find ourselves in situations where there may be a need to support more than <a id="_idIndexMarker820"/>one active version of a given component, API, or data. This can add significant levels of complexity to the solution. To keep complexity in check, it is important to make provisions for deprecating and eventually ending support for older versions.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor205"/>Summary</h1>
			<p>In this chapter, we looked at aspects purely beyond functional requirements – each of which can have a profound impact on our ability to apply domain-driven design effectively. Specifically, we looked at how each of these is interrelated and they have to be looked at holistically to achieve and sustain high levels of success.</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor206"/>Closing thoughts</h1>
			<p>Domain-driven design, although conceived in the early 2000s, was way ahead of its time. We are in the age of solving the most complex problems yet. Given the advancements in technology, there is an expectation to build these solutions a lot faster. While the overall cognitive complexity of the solution is directly proportional to the complexity of the problem, there is a need to effectively manage this complexity. DDD and its principles enable us to achieve this by breaking down complex problems into smaller, manageable parts. In this book, we have made an attempt to distill our experiences and provide a set of concrete techniques to apply DDD in your respective contexts.</p>
		</div>
		<div>
			<div id="_idContainer247">
			</div>
		</div>
	</body></html>