- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the previous chapter, Continuous Delivery pipelines allow
    developers to ship software with constant velocity and quality. In order to meet
    this quality, automated software tests are required. Engineers that work on features
    want to be sure that everything works as expected. This is even more the case
    when the software project advances, changes, and potentially breaks existing behavior.
    Developers need to be sure that no unwanted side-effects are introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the software tests contained in a build pipeline are sufficient, without
    further manual verification, to deploy to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The requirements of software tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different test levels and scopes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit, component, integration, system, and performance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to run test scenarios locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to craft maintainable tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required test technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The necessity of tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tests are necessary to be able to rely on that a certain functionality behaves
    in a certain way later in production. In all kinds of manufacturing businesses,
    tests are a natural part of the process. A car has countless parts that need to
    be tested independently as well as interdependently. Nobody wants to drive a car
    which has its first test run on a real street with the actual customer.
  prefs: []
  type: TYPE_NORMAL
- en: Tests simulate production behavior and verify components in a safe environment.
    Manufactured parts that break during test runs are something positive;â€‰they have
    just pointed out potential errors and nothing more than time and materials is
    lost. Parts that break in production can cause more harm.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true for software tests. Test failures are something positive, at
    worst they used up some time and effort, at best they prevent potential bugs from
    going to production.
  prefs: []
  type: TYPE_NORMAL
- en: As seen previously, tests need to run with the least required human interaction
    possible. Humans are good at thinking about reasonable test cases and crafting
    creative test scenarios. Computers, however, are better at executing them. Verifying
    complex tests is something computers also do well, after they have been given
    clear verification instructions. With software growing more complex over time,
    the effort of manually verifying behavior gets bigger and bigger and more prone
    to error over time. Computers perform better and more reliably at repetitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Reliable automated software tests are a prerequisite of moving fast. Automated
    tests can be executed many times, verifying the whole application. Builds run
    many times a day, execute all tests every time - even if only minor changes were
    introduced - and enable verified versions to go to production. This would not
    be feasible with tests executed by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Automated tests increase the reliability of and confidence in the Continuous
    Delivery process. For Continuous Deployment, that is, going directly to production,
    sufficient, automated test scenarios are absolutely required. When all commits
    are potential candidates for production deployment, all software behavior must
    be adequately verified upfront. Without this automated verification, Continuous
    Deployment wouldn't be possible.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements of well-crafted tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today's software world has agreed that tests are crucial to working software.
    But what makes a good software test? What software components do we have to test?
    And, more importantly, how can we develop well-crafted tests?
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, tests should fulfill the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following describes these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Predictability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, software tests have to be stable, predictable, and reproducible.
    The same project circumstances must predictably produce the same test case outcomes,
    that is, passing or failing. Tests that sometimes pass and sometimes fail are
    not helpful at all. They either distract developers by providing false positive
    results or suppress actual bugs with false negative outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Circumstances that need to be taken into account are, among others, the current
    time, time zones, locales, randomly generated data, and concurrent execution of
    other tests that could interfere. The test scenarios should be predictably and
    explicitly set up, so that these circumstances have no influence on the outcome.
    If the tested functionality is in fact influenced by these factors, this is a
    sign that there is a need for additional test scenarios, considering different
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The requirement of predictability also relates to isolation. Test cases have
    to run self-sufficiently, without affecting other tests. Changing and maintaining
    test cases should also have no impact on other test scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Besides leveraging predictability and maintainability, isolating tests also
    has an impact on the reproducibility of errors. Complex test scenarios may contain
    a lot of concerns and responsibilities that can make it hard to find the root
    causes of failing tests. Isolated tests with smaller scope, however, limit the
    possibilities of causes and enable developers to find bugs faster.
  prefs: []
  type: TYPE_NORMAL
- en: The several test scopes an enterprise project usually has, which we will see
    later in this chapter, also come with several test isolation layers. Tests with
    a small scope, such as unit tests, run more isolated than, for example, end-to-end
    tests. It certainly makes sense to write test cases in different scopes, which
    implies different test isolation layers.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, software tests of a project reliably test all functionality. The mantra
    should be that software that passes the tests is ready for production usage. This
    is of course a goal to strive for, for example by continuous improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Using Continuous Delivery and especially Continuous Deployment requires a reliable
    and sufficient test harness. The software tests are the ultimate quality barrier
    before production deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Reliable tests that pass should not require any further interaction. Therefore,
    they should not output verbose logs if the overall execution was successful. While
    a detailed explanation of what happened during execution is very helpful in failing
    tests, it becomes distracting in passing runs.
  prefs: []
  type: TYPE_NORMAL
- en: Fast execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As said before, tests are required to execute quickly. Fast running tests are
    a necessity for development pipelines providing fast feedback. Especially with
    the number of tests growing over time by applying continuous improvement, the
    only way to keep the pipeline effective is to keep test execution time low.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, test execution spends the most time in starting up the test technology.
    Integration tests in particular, which use an embedded container, consume a lot
    of startup time. The time spent performing the actual test is in most cases not
    such a big issue.
  prefs: []
  type: TYPE_NORMAL
- en: Tests that consume a lot of time contradict the idea of continuous improvement
    of quality. The more test cases and scenarios that are added to the project, the
    longer the overall test execution and the slower is the feedback. Especially with
    the challenges of a fast-moving world, software tests need to perform as fast
    as possible. The rest of this chapter will show you how we can achieve this goal,
    particularly in regard to end-to-end test scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automation is a prerequisite for fast feedback. Continuous Delivery pipeline
    steps should run with the least human intervention as possible. The same is true
    for test scenarios. Executing software tests and validating their results should
    run completely and reliably without human interaction.
  prefs: []
  type: TYPE_NORMAL
- en: The test cases define the functionality's expected behavior and validate the
    outcome against it. The test will then reliably pass without additional notice
    or fail with a detailed explanation. Passing tests should not require any further
    human interaction.
  prefs: []
  type: TYPE_NORMAL
- en: A scenarios with huge or complex test data in particular represent a certain
    challenge in automating test cases. In order to deal with this issue, engineers
    should craft test cases in a maintainable way.
  prefs: []
  type: TYPE_NORMAL
- en: Maintainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing test cases is one thing. Keeping efficient test cases with good coverage
    when functionality changes is another thing. The challenge with having poorly-crafted
    test scenarios is that as soon as production functionality changes, the tests
    need to change as well, requiring a lot of time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting test cases requires the same attention and effort as production code.
    Experience shows that without this effort put in, tests contain a lot of duplication
    and multiple responsibilities. In the same way as for production code, test code
    requires refactoring.
  prefs: []
  type: TYPE_NORMAL
- en: It should be possible to change or extend test scenarios without much effort
    required. In particular the test data that changes needs to be represented effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Maintainable tests are a prerequisite for enterprise projects that have proper
    test coverage and yet are flexible for changes in their business logic. Being
    able to adapt in a fast-moving world requires adjustable test scenarios as well.
  prefs: []
  type: TYPE_NORMAL
- en: What to test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we go into the topic of how to craft effective, fast, reliable, automated,
    and maintainable test cases, let's have a look at what assets to test. There are
    tests on a code layer as well as end-to-end tests. Code layer tests are based
    on the project's source code and are usually executed during development and build
    time, whereas end-to-end tests, of all kinds, operate on a running application.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the test scopes, which we will get to know in the next section,
    there are different layers of tests, whether tests operate on classes, multiple
    components, enterprise applications, or whole environments. In all cases the test
    object needs to be isolated from external concerns. The nature of tests is that
    they verify certain behavior under specific conditions. The environment surrounding
    the test object, such as the test cases as well as used components, have to interact
    with the test object accordingly. The test case will therefore control the test
    object. This not only includes tests at code level, but also end-to-end tests
    with external systems being simulated and mocked away.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, software tests should verify business behavior. The specified
    use cases all have to perform certain logic that has to be tested before production
    deployment. Software tests should therefore verify that the application fulfills
    the business requirements. Special and corner cases need to be covered as well
    as negative tests.
  prefs: []
  type: TYPE_NORMAL
- en: For example, testing authentication functionality not only needs to verify that
    a user can log in with the correct credentials, but also that they can not log
    in using wrong the credentials. A corner case of this example would be to verify
    that the authentication component notifies a user whose password is about to expire
    as soon as he logs in successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Besides business behavior, technical aspects and cross-cutting components also
    need to be tested. Accessed databases and external systems and the form of the
    communication is required to be verified on both ends in order to guarantee a
    working team. These concerns are best tested in end-to-end tests.
  prefs: []
  type: TYPE_NORMAL
- en: In all cases the test object should not be modified during the test, but work
    in the way as it will in production. This is crucial for crafting reliable tests
    that will not alter their behavior later on. For code level tests, this only requires
    that the contents of all involved components are the same. For end-to-end tests,
    this includes the whole enterprise application as well as the installation and
    configuration of the application's runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of test scopes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several test scopes and responsibilities to consider. The following
    will introduce the different scopes the rest of this chapter will cover.
  prefs: []
  type: TYPE_NORMAL
- en: Certain namings, such as *integation tests*, are used ambiguously in various
    enterprise projects. This sub-chapter defines consistent test scope names that
    are used for the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit tests verify the behavior of individual units of an application. A unit
    test usually represents a single class, in some cases a few interdependent classes.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests operate on code level. They are usually executed in the IDE during
    development as well as part of the build process before the application is packaged.
    Unit tests have the shortest execution time of all test scopes. They only execute
    limited functionality that can be instantiated easily on code level. Potential
    dependencies of the units are simulated using mocks or dummy classes.
  prefs: []
  type: TYPE_NORMAL
- en: Component tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Component tests verify the behavior of a coherent component. They span more
    than just an individual unit, but still operate on code level. Component tests
    aim to integrate several components together, which verify the interdependent
    behavior without setting up container environments.
  prefs: []
  type: TYPE_NORMAL
- en: The scope of component tests is to provide more integration than unit tests
    without running the application in potentially slow, simulated environments. Similar
    to unit tests, they use mocking functionality to delimit and simulate test boundaries.
    An embedded or remote enterprise container is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a lot of disagreement as to what integration tests represent and how
    they are designed. The aimed integration can happen on various levels.
  prefs: []
  type: TYPE_NORMAL
- en: I will use the term as it is quite widely used in the Java ecosystem and as
    it is represented in the Maven conventions. Integration tests run on code level,
    providing integration of several units and components, and usually run some more-or-less
    complex testing framework. This is the main distinction from component tests here.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests have a similar scope as component tests also integrate several
    units; however, the focus is on the integration. This integration is more technology
    than business related. For example, managed beans can make use of CDI injection
    to acquire certain dependencies using qualifiers or CDI producers. Developers
    need to verify whether the CDI *plumbing* has been done properly, that is, the
    correct annotations have been used, without necessarily deploying the application
    to a server.
  prefs: []
  type: TYPE_NORMAL
- en: Testing frameworks start up an embedded runtime that will build up several components
    and run code level tests against them.
  prefs: []
  type: TYPE_NORMAL
- en: Component tests, however, solely focus on the business logic and are limited
    to simple dependencies that are easily resolvable without sophisticated containers.
    In general, component tests are preferable for testing business use cases since
    they contain less moving parts and will run faster.
  prefs: []
  type: TYPE_NORMAL
- en: System tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term system tests is sometimes also used ambiguously. In this context, the
    term covers all test cases that run the application or the system as a whole,
    verifying use cases in an end-to-end manner. Sometimes the terms acceptance or
    integration tests are used respectively. However, this book consistently uses
    the term system tests to refer to end-to-end tests.
  prefs: []
  type: TYPE_NORMAL
- en: System tests are quite important to verify that a deployed application works
    as expected, including both business logic and technical concerns. Whereas the
    majority of business logic should already be covered by unit and component tests,
    system tests verify that the overall behavior, including all external systems,
    is met. This includes how functionality is integrated and interacts within the
    system landscape.
  prefs: []
  type: TYPE_NORMAL
- en: For an application to provide value, it is not sufficient to only include business
    logic, but also how that logic is accessed. This needs to be verified in an end-to-end
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: Since this book is targeted for backend applications, UI level tests are not
    considered here; this includes UI end-to-end tests as well as UI reactiveness
    tests. Developers typically develop UI tests using test technology such as **Arquillian
    Graphene**. The system test approaches described in this chapter are applicable
    to UI level tests as well.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tests verify the non-functional aspect of how a system performs
    in terms of responsiveness and correct behavior under certain workloads.
  prefs: []
  type: TYPE_NORMAL
- en: It needs to be ensured that an application can provide business value, not only
    under laboratory conditions but also in production. In production the load on
    the system can vary heavily, depending on the nature of the application and its
    use cases. Applications that are publicly available also run the risk of becoming
    the target of denial of service attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests are a helpful tool to detect potential performance issues
    that are caused by the application. This includes, for example, resource leaks,
    misconfiguration, deadlock situations, or missing timeouts. Putting the application
    under simulated workload will bring these issues to light.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we will see in Chapter 9, *Monitoring, Performance, and Logging*,
    performance tests aren't necessarily helpful to predict production responsiveness
    or tune an application's performance. They should be used as a barrier against
    obvious mistakes, providing fast feedback.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this book, I will use the term performance tests to describe
    performance as well as load or stress tests that put the application under performance
    load.
  prefs: []
  type: TYPE_NORMAL
- en: Stress tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to performance tests, stress tests aim to put the system under a certain
    stress to verify correct behavior in abnormal situations. Whereas performance
    tests mainly target the application's performance in terms of responsibility and
    stability, stress tests can cover all aspects and attempts that try to bring the
    system down.
  prefs: []
  type: TYPE_NORMAL
- en: This includes invalid calls, neglecting communication contracts, or random,
    unexpected events from the environment. This list of tests is non-exhaustive here
    and beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: However, to give a few examples, stress test may verify against misuse of HTTP
    connections, such as SYN flooding, DDoS attacks in general, unexpected shutdowns
    of infrastructure, or further, so-called fuzz or monkey testing.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a sophisticated test harness containing a lot of stress tests would
    practically be beyond the scope of most projects. However, for enterprise projects
    it makes sense to include a few reasonable stress tests that match the used environment.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the motivations, requirements, and different scopes, let's have a closer
    look at how to craft test cases in Java Enterprise projects.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit tests verify the behavior of individual units of an application. In a Java
    EE application, this usually regards single entity, boundary, and control classes.
  prefs: []
  type: TYPE_NORMAL
- en: In order to unit test a single class, no exhaustive test case should be required.
    Ideally, instantiating the test object and setting up minimum dependencies should
    be sufficient to be able to invoke and verify its business functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Java EE supports this approach. Java EE components, such as EJBs as well
    as CDI managed beans are testable in a straightforward way by simply instantiating
    the classes. As we saw previously, modern enterprise components represent plain
    Java objects, including annotations, without extending or implementing technically
    motivated superclasses or interfaces, so-called no-interface views.
  prefs: []
  type: TYPE_NORMAL
- en: This allows tests to instantiate EJB or CDI classes and to wire them up as required.
    Used delegates such as injected controls that are irrelevant to the test case
    are mocked away. By doing so, we define the boundaries of the test case, what
    should be tested, and what is not relevant. Mocked delegates enable to verify
    the test object interaction.
  prefs: []
  type: TYPE_NORMAL
- en: A mock object simulates behavior of an actual instance of its type. Calling
    methods on mocks usually only returns dummy or mock values. Test objects are not
    aware that they communicate with a mock object. The behavior of mocks, as well
    as the verification of invoked methods, is controlled within the test scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with a unit test of a Java EE core component. The `CarManufacturer`
    boundary executes certain business logic and invokes a `CarFactory` delegate control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the EJB boundary is a plain Java class, it can be instantiated and set
    up in a unit test. The most commonly used Java unit test technology is **JUnit**
    together with **Mockito** for mocking. The following code snippet shows the car
    manufacturer test case, instantiating the boundary test object and using Mockito
    to mock away used delegates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The JUnit framework instantiates the `CarManufacturerTest` test class once during
    the test execution.
  prefs: []
  type: TYPE_NORMAL
- en: The `@Before` method, `setUp()` here, is executed every time before a `@Test`
    method runs. Similarly, methods annotated with `@After` run after every test run.
    The `@BeforeClass` and `@AfterClass` methods, however, are only executed once
    per test class, before and after the execution, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Mockito methods, such as `mock()`, `when()`, or `verify()` are used to create,
    set up, and verify mocking behavior, respectively. Mock objects are instructed
    to behave in a certain way. After the test execution, they can verify whether
    certain functionality has been called on them.
  prefs: []
  type: TYPE_NORMAL
- en: This is an admittedly easy example, but it contains the essence of unit testing
    core components. No further custom test runner, neither an embedded container
    is required to verify the boundary's behavior. As opposed to custom runners, the
    JUnit framework can run unit tests at a very high rate. Hundreds of examples like
    these will be executed on modern hardware in no time. The startup time is short
    and the rest is just Java code execution, with a tiny overhead from the testing
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: Some readers may have noticed the package-private visibility on the `CarManufacturer`
    class. This is due to providing better testability in order to be able to set
    the delegate on instantiated classes. Test classes that reside in the same package
    as the boundary are able to modify its dependencies. However, engineers might
    argue that this violates the encapsulation of the boundary. Theoretically they're
    right, but no caller will be able to modify the references once the components
    run in an enterprise container. The referenced object is not the actual delegate,
    but a proxy thereof, hence the CDI implementation can prevent misuse. It certainly
    is possible to inject the mock object using reflection or by using constructor-based
    injection. However, field-based injection together with directly setting the dependencies
    in the test cases provides better readability with the same production behavior.
    As of today, many enterprise projects have agreed upon using field dependency
    injection with package-private visibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another discussion is whether to use custom JUnit runners such as `MockitoJUnitRunner`
    together with custom mocking annotations or a plain setup approach, as shown previously.
    The following code snippet shows a more dense example using a custom runner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using the custom Mockito runner allows developers to configure tests with less
    code as well as to define injections with private visibility in the service class.
    Using a plain approach, as shown previously, provides more flexibility for complex
    mock scenarios. However, which method to use in order to run and define Mockito
    mocks is indeed a question of taste.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameterized tests is an additional JUnit functionality to define test cases
    that are similar in the scenario, but differ in input and output data. The `manufactureCar()`
    method could be tested with a variety of input data, resulting in a slightly different
    outcome. Parameterized test cases enable to develop these scenarios more productively.
    The following code snippet shows an example of such test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameterized test classes are instantiated and executed following the data
    in the `@Parameters` test data method. Each element in the returned collection
    results in a separate test execution. The test class populates its parameter properties
    and continues the text execution as usual. The test data contains test input parameters
    as well as expected values.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this parameterized approach is that it enables developers to
    add new test cases by simply adding another line within the `testData()` method.
    The preceding example shows the combination of a parameterized unit test using
    mocks. That combination is only possible using a plain Mockito approach, as described
    previously, instead of using `MockitoJUnitRunner`.
  prefs: []
  type: TYPE_NORMAL
- en: Technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These examples use JUnit 4 which, at the time of writing, is the most used unit
    testing framework version. Mockito is used to mock objects and it provides sufficient
    flexibility for the majority of use cases. In order to assert conditions, these
    examples use **AssertJ** as the test matching library. It provides functionality
    to verify the state of objects using productive method-chaining invocations.
  prefs: []
  type: TYPE_NORMAL
- en: These technologies serve as examples for the required test aspects. The point
    here, however, is not to dictate certain functionalities rather it is to showcase
    specific, reasonable choices for these test requirements. Other technology that
    provides similar functionality and benefits is equally advisable.
  prefs: []
  type: TYPE_NORMAL
- en: A typical example of another widely-used technology is Hamcrest matchers as
    a test matching library or the less frequently used **TestNG** unit test framework.
  prefs: []
  type: TYPE_NORMAL
- en: By the time you're reading this, JUnit version 5 will have emerged, which provides
    some additional functionalities, especially regarding to dynamic tests. Dynamic
    tests have similar motivations as parameterized tests, by programmatically and
    dynamically defining test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Component tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Component tests verify the behavior of a coherent component. They provide more
    integration than unit tests without running the application in simulated environments.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The behavior of coherent functionality represented by several interdependent
    classes needs to be verified in order to test the integration thereof. Component
    tests should run as fast as unit tests by still isolating functionality, that
    is, testing coherent units. Therefore they aim to provide fast feedback by integrating
    yet more logic than just unit tests. Component tests verify business use cases,
    from the boundary down to involved controls.
  prefs: []
  type: TYPE_NORMAL
- en: Code level component tests are possible since the vast majority of managed beans
    use quite straightforward delegates. The injected types are in most cases directly
    resolvable without interfaces or qualifiers and could practically be instantiated
    and injected, even without embedded containers. This enables component tests to
    be implemented using the same testing frameworks as unit tests. Required delegates
    and mocks are set up as part of the test case. The test scenario we want to show
    starts from the beginning of a business use case down to injected controls.
  prefs: []
  type: TYPE_NORMAL
- en: The following examples will examine how to implement component tests with some
    basic code quality practices, that help writing maintainable tests.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine the whole manufacture car use case shown in the previous example in
    the *Unit tests* section, needs to be tested. A car is created, using a delegate
    `CarFactory`, and then is persisted into the database. Testing the persistence
    layer is out of this test scope, therefore the entity manager will be mocked away.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the component test to the manufacture car
    use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example is quite similar to the previous ones, with the exception
    that `CarFactory` is instantiated, using the actual business logic. The mocks,
    which represent the boundaries of the test case, verify correct behavior.
  prefs: []
  type: TYPE_NORMAL
- en: However, while this approach works for straightforward use cases, it is somewhat
    naive in regard to more sophisticated real-world scenarios. The boundaries of
    the test case are as seen in the test class, for the `CarFactory` delegate to
    be self-sufficient and not inject further controls. Of course, all interdependent
    units that are part of a component test can define delegates. Depending on the
    nature of the test and the use case, these nested delegates also need to be instantiated
    or mocked away.
  prefs: []
  type: TYPE_NORMAL
- en: This will eventually lead to a lot of effort required in setting up the test
    case. We could make use of test framework functionality such as Mockito annotations
    here. Doing so, the test case injects all classes that are involved in the test
    case. Developers specify which of them will be instantiated or mocked away, respectively.
    Mockito provides functionality to resolve references, which is sufficient for
    the majority of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows a component test of a similar scenario, this
    time using a `CarFactory` delegate that has an `AssemblyLine` and `Automation`
    as nested dependencies. These are mocked away in the test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `@InjectMocks` functionality of Mockito attempts to resolve object references
    with mock objects injected as `@Mock` in the test case. The references are set
    using reflection. If boundaries or controls define new delegates, they need to
    be defined at least as a `@Mock` object in the test cases to prevent `NullPointerException`.
    However, this approach only partially improves the situation since it leads to
    a lot of dependencies being defined in the test class.
  prefs: []
  type: TYPE_NORMAL
- en: An enterprise project with a growing number of component tests introduces a
    lot of verbosity and duplication if it follows only this approach.
  prefs: []
  type: TYPE_NORMAL
- en: To make the test code less verbose and restrict this duplication, we could introduce
    a test superclass for a specific use case scenario. That superclass would contain
    all `@Mock` and `@InjectMock` definitions, setting up required dependencies, delegates,
    and mocks. However, such test superclasses also contain a lot of implicit logic,
    which delegates are defined and being used somewhere in the extended test cases.
    This approach leads to test cases that are tightly coupled to commonly used superclasses,
    eventually leading to implicitly coupling the test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Delegating test components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is more advisable to use delegation rather than extension.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking and verification logic that depends on the used components is delegated
    to separate test objects. The delegates thus encapsulate and manage this logic
    individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the test case using components that define
    the car manufacture and car factory dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `Component` test dependencies specify the declared dependencies and mocks
    including the setup and verification behavior for our test cases. The idea is
    to define components that are reusable within multiple component tests, wiring
    up similar logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the definition of `CarManufacturerComponent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The class resides in the same package as the `CarManufacturer` class, but under
    the test sources. It can subclass the boundary to add mocking and verification
    logic. In this example, it is dependent on the `CarFactory` component, that also
    provides additional test logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These components serve as reusable test objects that wire up certain dependencies
    and configure mocking behavior, accordingly. They can be reused within multiple
    component tests and enhanced without affecting usages.
  prefs: []
  type: TYPE_NORMAL
- en: These examples aim to give an idea of what is possible in order to write maintainable
    tests. For components being reused, more refactoring approaches should be considered,
    for example, using a builder-pattern like configuration to satisfy different situations.
    The *Maintaining test data and scenarios* section in this chapter contains more
    about how to write maintainable test code.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of component tests is that they run as fast as unit tests and yet
    verify more complex integration logic. The complex logic is tackled by delegation
    and encapsulation, increasing maintainability. The code and overhead required
    to setup is limited.
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to verify coherent business logic using component tests. Use
    case invocations are tested on a business level with technical low-level aspects
    being mocked away.
  prefs: []
  type: TYPE_NORMAL
- en: Technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These examples again demonstrate plain JUnit and Mockito test approaches. With
    some code quality practices, it's certainly possible to write maintainable, dense
    test cases with limited configuration overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Component tests implemented as shown previously are a practical approach to
    wire up components that use straightforward dependency injection. If the production
    code makes use of CDI producers and qualifiers, the injection logic of the test
    components will change accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Component tests aim to verify the business use case behavior of coherent units.
    They usually don't verify the technical wiring. It's advisable to use integration
    tests in order to verify whether CDI injection was used correctly, for example,
    in terms of custom qualifiers, producers, or scopes.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are test technologies that provide dependency injection into
    test cases. Examples for these are **CDI-Unit** or the more sophisticated **Aquillian
    Testing Framework**. Test cases using these frameworks run in containers, either
    embedded or even remotely, and are able to further verify the integration of components.
  prefs: []
  type: TYPE_NORMAL
- en: Sophisticated test frameworks certainly provide test cases that are closer to
    the enterprise application, but also come with the challenge of slow application
    startups. The containers are usually executed and configured in every test case,
    typically taking a few hundred milliseconds or more. This does not sound that
    much but quickly adds up as more tests arrive.
  prefs: []
  type: TYPE_NORMAL
- en: For component tests that aim to solely verify business behavior, faster, and
    lightweight approaches like the one presented, are therefore preferable. With
    their fast nature, component tests as well as unit tests are per default executed
    during the project build. They should be the default way how to verify application
    business logic.
  prefs: []
  type: TYPE_NORMAL
- en: The following shows code level integration tests that make use of simulated
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Component tests verify coherent business logic in isolated and fast tests. Sophisticated
    Java EE integration behavior, such as injection, custom qualifiers, CDI events,
    or scopes are not covered by these tests.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests aim to verify the technical collaboration of components within
    the enterprise system. This covers concerns such as configuration of Java EE core
    components, communication with external systems, or persistence. Are the Java
    EE components annotated correctly? Does the JSON-B mapping produce the desired
    JSON format? Is the JPA ORM mapping defined properly?
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind code level integration tests is to provide faster feedback by
    verifying correct integration without the need to build and deploy the application
    to a test environment.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since unit test technology is not aware of Java EE specifics, integration tests
    need more sophisticated test functionality in the form of containers. There are
    several technologies available that start up an embedded container and make parts
    of the application available.
  prefs: []
  type: TYPE_NORMAL
- en: An example for this is CDI-Unit. It provides functionality to run test cases
    in a CDI container, further enabling developers to enhance and modify its configuration.
    CDI-Unit scans the dependencies of tested objects and configures them accordingly.
    Required mocks and test specific behavior are defined in a declarative approach.
    A managed bean such as the car manufacturer boundary is set up within the test
    case, with all required dependencies and mocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach detects configuration errors, such as missing CDI annotations.
    The following code snippet shows a car manufacture test, similar to the component
    test before, that instantiates the boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The custom JUnit runner detects beans injected into the test case and resolves
    them accordingly. Since CDI-Unit only supports the CDI standard and not the full
    Java EE API, the test explicitly mocks and sets the entity manager. All other
    used controls, such as the car factory, automation, and assembly line, are instantiated
    and injected, accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: CDI-Unit tests can be enhanced to serve more sophisticated scenarios. It's possible
    to produce beans that are being used within the test scope.
  prefs: []
  type: TYPE_NORMAL
- en: However, this technology certainly has its limits. CDI-Unit is helpful to quickly
    verify configuration and collaboration of managed beans.
  prefs: []
  type: TYPE_NORMAL
- en: Another, more sophisticated technology for testing applications is Arquillian.
    Arquillian bundles integration test cases into deployable archives, manages enterprise
    containers, either embedded or remotely, and deploys, executes, and verifies the
    test archives. It makes it possible to enhance test cases with custom test behavior
    depending on the scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of Arquillian is that it supports containers with full Java EE
    support. This enables integration tests to operate in more production-near scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows a simple example of deploying the car manufacturer
    boundary to an embedded enterprise container managed by Arquillian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This test case will create a dynamic web archive that ships the boundary and
    required delegates and deploys it into an embedded container. The test itself
    can inject and call methods on the specific components.
  prefs: []
  type: TYPE_NORMAL
- en: The container does not necessarily have to run in an embedded way, it can also
    be a managed or remote container. Containers that run for longer than just the
    test execution avoid the container startup time and execute tests much more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Executing these integration tests will take a comparatively long time, but operate
    closer to production environments. Misconfigured managed beans will be detected
    during development before the application is shipped. The flexibility and customization
    of Arquillian, by including custom bean definitions that reside in the test scope,
    enables pragmatic test scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: However, this example only slightly touches the functionality of embedded container
    tests. Test frameworks such as Arquillian can be used for validating the integration
    of container configuration, communication, persistence, and UI. In the rest of
    this chapter, we will see some shortcomings of integration tests that operate
    on simulated or embedded environments.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mapping persistence of domain entities is usually defined using JPA annotations.
    Validating this mapping before an actual server deployment prevents careless mistakes
    and saves time.
  prefs: []
  type: TYPE_NORMAL
- en: In order to verify a correct database mapping, a database is required. Besides
    using deployed environment database instances, embedded databases provide similar
    verification with fast feedback. Embedded container tests running on frameworks
    such as Arquillian can be used to access this functionality. However, for basic
    verification it's not necessary for the application to run inside a container.
  prefs: []
  type: TYPE_NORMAL
- en: JPA ships with the possibility to run standalone, in any Java SE environment.
    We can make use of this and write test cases that wire up the JPA configuration
    and connect against an embedded or local database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a car part that is manufactured and assembled in the car manufacture.
    The car part domain entity is mapped with JPA as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to verify correct persistence, a test entity bean should at least
    be persisted and reloaded from the database. The following code snippet shows
    an integration test that sets up a standalone JPA persistence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the persistence runs standalone, there is no container taking care of
    handling transactions. The test case does this programmatically, as well as setting
    up the entity manager, using the persistence unit `it`. The persistence unit is
    configured in test scope `persistence.xml`. For this test purpose it''s sufficient
    to configure a resource local transactional unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The involved entity classes such as `CarPart` have to be specified explicitly,
    since there is no container that takes care of annotation scanning. The JDBC configuration
    points to an embedded database, in this case **Apache Derby**.
  prefs: []
  type: TYPE_NORMAL
- en: The enterprise project does not include the Java EE implementation, only the
    API. Therefore, an JPA implementation, such as **EclipseLink**, is added as a
    test dependency, together with the Derby database.
  prefs: []
  type: TYPE_NORMAL
- en: This integration test provides faster feedback for configuration mismatches
    and careless mistakes by validating the persistence mapping locally. For example,
    the shown test case would fail because the `order` property of the `CarPart` type
    isn't able to be mapped, since `order` is a reserved SQL keyword. The solution
    to this is to change the column mapping, for example, by renaming the column with
    `@Column(name = "part_order")`.
  prefs: []
  type: TYPE_NORMAL
- en: This is a typical example of mistakes developers make while configuring the
    persistence. Preventing these errors, that otherwise won't be detected before
    deployment time, provides faster feedback and saves time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this approach will not find all database related integration mismatches.
    There is no container being used and persistence errors, for example, related
    to concurrent transactions, won't be found before fully-fledged system tests are
    executed. Still, it provides a helpful first verification in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Running integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attentive readers may have noticed the naming convention of integration tests
    ending with `IT` for integration test. This naming emerged from a Maven naming
    convention, excluding test classes, that don't match the `Test` naming pattern,
    in the *test* phase. Classes ending with `IT` will be run by a different life
    cycle plugin.
  prefs: []
  type: TYPE_NORMAL
- en: This approach supports developers in crafting effective development pipelines,
    since code level integration tests shouldn't necessarily run in the first build
    step depending on the time they take. With the example of Maven, the **Failsafe
    Plugin** runs integration tests, using the command `mvn failsafe:integration-test
    failsafe:verify`, after the project has been built.
  prefs: []
  type: TYPE_NORMAL
- en: The IDE, of course, supports both running `Test` named tests as well as other
    naming conventions.
  prefs: []
  type: TYPE_NORMAL
- en: Gradle doesn't take this naming structure into account. In order to achieve
    the same goal, Gradle projects would use multiple sets of test sources that are
    executed separately.
  prefs: []
  type: TYPE_NORMAL
- en: Code level integration tests versus system tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code level tests, such as unit, component, or integration tests, provide fast
    feedback during development. They enable developers to verify whether the business
    logic works as expected for isolated components and the overall configuration
    is sane.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, in order to verify the application's production behavior, these tests
    are not sufficient. There will be differences in technology, configuration, or
    runtime that eventually lead to gaps in the test cases. Examples are enterprise
    containers with different versions, mismatches in the bean configuration once
    the whole application is deployed, different database implementations, or differences
    in JSON serialization.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the application runs in production. It makes a lot of sense to verify
    the behavior in environments that are equivalent to production.
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, it is advisable to craft several test scopes to both have tests with
    isolated scope and faster feedback as well as integrational tests. The shortcoming
    of code level integration tests is that they often take a great amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: In my projects in the past, integration tests running containers such as Arquillian,
    usually were responsible for the vast majority of the build time, resulting in
    build with 10 minutes and more. This greatly slows down the Continuous Delivery
    pipeline, resulting in slow feedback and fewer builds. An attempt to solve this
    shortcoming is to use remote or managed containers in Arquillian tests. They will
    run with a longer life cycle than the test run and eliminate the startup times.
  prefs: []
  type: TYPE_NORMAL
- en: Code level integration tests are a helpful way to quickly verify application
    configuration, what cannot be tested using unit or component tests. They are not
    ideal for testing business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests that deploy the whole application on simulated environments,
    such as embedded containers, provide certain value, but are not sufficient to
    verify production behavior since they are not equivalent to production. No matter
    whether on code level or simulated environments, integration tests tend to slow
    down the overall pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of system tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System tests which test an application that is deployed to a production-like
    environment in an end-to-end fashion provide the most representative verification.
    Since they run in a later step in the Continuous Delivery pipeline, they provide
    slower feedback. Test cases, such as verifying the JSON mappings of an HTTP endpoint,
    will take longer before they provide feedback to engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling and maintaining complex test scenarios is an aspect that takes quite
    some time and effort. Enterprise applications require definition and maintenance
    of test data and configuration, especially when many external systems are involved.
    For example, an end-to-end test that verifies creating a car in the car manufacture
    application requires external concerns such as the assembly line to be set up
    as well as test data. Managing these scenarios involves a certain effort.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests attempt to use external systems and databases similarly to
    production. This introduces the challenge to handle unavailable or erroneous environments.
    External systems or databases that are unavailable cause the tests to fail; however,
    the application is not responsible for this test failure. This scenario violates
    the requirement of predictability, that tests should not depend on external factors
    that provide false positives. Therefore, it's advisable that system tests mock
    away external systems that are not part of the application under test. Doing this
    enables to construct predictable end-to-end tests. The Sub-chapter *System tests*
    covers how to implement this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code level unit and component tests verify isolated, specific business logic.
    They provide immediate feedback and prevent careless mistakes. Component tests,
    in particular, cover the integration of business related software units.
  prefs: []
  type: TYPE_NORMAL
- en: The delimitation of component tests is that they run without a simulated container,
    setting up the test cases in a programmatic fashion. Integration tests rely on
    inversion of control, similar to application containers that wire up components
    with less developer effort involved. However, crafting maintainable test cases
    using a programmatic approach with unit test technology ultimately leads to more
    effective tests. We will see in the *Maintaining test data and scenarios* section
    in this chapter, what methods support us in crafting productive test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests verify the technical integration as well as configuration
    of application components. Their feedback is certainly faster than deploying the
    application as part of the pipeline. However, integration tests do not provide
    sufficient verification compared to production.
  prefs: []
  type: TYPE_NORMAL
- en: They are a good fit to provide a first basic barrier against common errors and
    careless mistakes. Since starting up integration tests usually takes quite some
    time, it makes a lot of sense to run a limited number of them. Ideally test frameworks
    such as Arquillian deploy to managed or remote containers that keep running beyond
    a single test case.
  prefs: []
  type: TYPE_NORMAL
- en: System tests verify the application's behavior in the most production-like fashion.
    They provide the ultimate feedback, whether the whole enterprise application works
    as expected, including business as well as technical aspects. In order to construct
    predictable test scenarios, it's important to consider external concerns, such
    as databases and external systems.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting test cases, especially complex test scenarios, takes a lot of time
    and effort. The question is where does it make the most sense to spend this effort
    on? In order to test business logic, and especially coherent components, it's
    advisable to use component tests. Integration tests don't provide ultimate verification,
    but still take certain time and effort. It makes sense to use a few of them for
    fast integration feedback, but not to test business logic. Developers may also
    find ways to reuse created scenarios in several test scopes, for example both
    integration tests and system tests.
  prefs: []
  type: TYPE_NORMAL
- en: The overall goal should be to minimize the time and effort spent to craft and
    maintain test cases, to minimize the overall pipeline execution time and to maximize
    the application verification coverage.
  prefs: []
  type: TYPE_NORMAL
- en: System tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System tests run against a deployed enterprise application. The application
    contains the same code, configuration, and runtime as in production. The test
    cases use external communication, such as HTTP, to initiate use cases. They verify
    that the overall outcome, such as HTTP responses, database state, or communication
    with external systems, matches the expectations of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'System tests answer the question what to test with: the application that runs
    in the same way as in production, excluding external concerns, accessed using
    its regular interfaces. External concerns will be simulated, ensuring predictable
    tests and enabling verification of the communication. It depends on the scenario
    whether a used database is seen as part of the application and used similarly,
    or mocked away as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing test scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System test scenarios can easily get quite complex, involving several concerns
    and obfuscating the actual use case that is to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: In order to manage the complexity of scenarios it makes sense to first craft
    the procedure of the test case without writing actual code. Defining the required
    steps in comments, or even on paper first, provides a good overview of what the
    test scenario is about. Implementing the actual test case afterwards with regard
    to reasonable abstraction layers will result in more maintainable test cases,
    with potentially reusable functionality. We will cover this approach in an example
    later in this sub-chapter.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to take test data into consideration. The more responsibilities
    a scenario holds, the more complex it will be to define and maintain the test
    data. It makes sense to put some effort into test data functionality that is being
    used commonly in the test cases. Depending on the nature of the application and
    its domain, it may even make sense to define a specific engineer role for this.
    Providing reusable functionality that is usable effectively can provide some relief;
    however, it may still be necessary to at least define and document common test
    data and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately it doesn't help to ignore the complexity of test data. If the application
    domain does include sophisticated scenarios, ignoring this situation by leaving
    out certain test cases or postponing test scenarios until production doesn't improve
    the application's quality.
  prefs: []
  type: TYPE_NORMAL
- en: In order to craft predictable isolated test cases, the scenario should run as
    stateless and self-sufficient as possible. Test cases should have a starting point
    similar to production and not rely on a certain state of the system. They should
    consider other potential tests and usages running simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: For example, creating a new car should not make assumptions about the number
    of existing cars. The test case should not verify that the list of all cars is
    empty before the creation or that it only contains the created car afterwards.
    It rather verifies that the created car is part of the list at all.
  prefs: []
  type: TYPE_NORMAL
- en: For the same reason it should be avoided that system tests have an impact on
    the environment life cycle. In situations that involve external systems, it's
    necessary to control the behavior of the mocked systems. If possible, these cases
    should be limited in order to enable to execute other scenarios concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating external concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System test scenarios use external systems in the same way as in production.
    However, similar to mock objects in unit and component tests, system tests simulate
    and mock away external systems. In this way, potential issues that the application
    isn't responsible for are eliminated. System tests run in dedicated environments,
    for example, provided by container orchestration frameworks. The test object is
    the sole application, deployed, executed and configured in the same way as in
    production.
  prefs: []
  type: TYPE_NORMAL
- en: Simulated external systems are configured to provide the expected behavior once
    accessed by the application. Similar to a mock object, they verify correct communication
    depending on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: For the majority of use cases, used databases would not be mocked away. The
    test scenario can manage and populate database contents as part of the test life
    cycle, if required. If the database system contains a lot of concerns external
    to the application, for example containing a lot of database code or representing
    a search engine, it may make sense to mock and simulate this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration strongly supports these efforts by abstracting systems
    as services. Pod images can be replaced by other implementations without affecting
    the tested application. The mocked services can be accessed and configured from
    within the running system test, defining behavior and external test data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abb98818-2655-46ba-9880-823c439a14f8.png)'
  prefs: []
  type: TYPE_IMG
- en: The dotted line illustrates the control and verification of the mocked system
    as part of the test scenario. The running application will use the external service
    as usual, with the difference that this service is in fact, backed by a mock.
  prefs: []
  type: TYPE_NORMAL
- en: Designing system tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: System tests run as a step within the Continuous Delivery pipeline. They connect
    against a running application on a test environment, invoke business use cases,
    and verify the overall outcome.
  prefs: []
  type: TYPE_NORMAL
- en: System test cases usually don't impact the application's life cycle. The application
    is deployed upfront as part of the CD pipeline. If required, the system tests
    control the state and behavior of external mocks and contents of databases.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, it makes sense to develop system tests as separate build
    projects without any code dependency to the project. Since system tests access
    the application from the outside there should be no implications on how the system
    is being used. System tests are developed against the application's endpoint contracts.
    Similarly, the system tests should not use classes or functionality that is part
    of the application, such as using the application's JSON mapping classes. Defining
    technology and system access from the outside as separate build projects prevents
    unwanted side effects caused by existing functionality. The system test project
    can reside besides the application project in the same repository.
  prefs: []
  type: TYPE_NORMAL
- en: The following example will construct a system test from a top-down approach,
    defining test scenarios and appropriate abstraction layers.
  prefs: []
  type: TYPE_NORMAL
- en: The business use cases of the car manufacture application are accessed via HTTP.
    They involve external systems and database accesses. In order to verify the creation
    of a car, the system test will connect against the running application, as a real-world
    use case would.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to manage the test scenario, the case is crafted using logical steps
    with comments as placeholders first, and then implemented in several abstraction
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These comments represent the logical steps that are executed and verified when
    testing creation of a car. They are related to the business rather than the technical
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We realize these comments in private methods, or better, own delegates. The
    delegates encapsulate technical concerns, as well as potential life cycle behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: We define `CarManufacturer` and `AssemblyLine` delegates that abstract the access
    and behavior of the applications and delegates. They are defined as part of the
    system test and have no relation to or knowledge of the managed beans with the
    same name in the application code. The system test project code is defined independently.
    It could also be implemented using a different technology, only depending on the
    communication interface of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the integration of the delegates. The car
    creation system test only contains business logic relevant to implementation,
    with the delegates realizing the actual invocations. This leverages readable as
    well as maintainable test cases. Similar system tests will reuse the delegate
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This serves as a basic example for an application system test. The delegates
    such as `CarManufacturer` handle the lower-level communication and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The test delegate is configured against the car manufacture test environment.
    This configuration could be made configurable, for example, by a Java system property
    or environment variable in order to make the test reusable against several environments.
  prefs: []
  type: TYPE_NORMAL
- en: If the delegate needs to hook up into the test case life cycle, it can be defined
    as a JUnit 4 rule or JUnit 5 extension model.
  prefs: []
  type: TYPE_NORMAL
- en: This example connects against a running car manufacture application via HTTP.
    It can create and read cars, mapping and verifying the responses. The reader may
    have noted how the delegate encapsulates communication internals, such as HTTP
    URLs, status codes, or JSON mapping. Its public interface only comprises classes
    that are relevant to the business domain of the test scenario, such as `Car` or
    `EngineType`. The domain entity types used in system tests don't have to match
    the ones defined in the application. For reasons of simplicity, system tests can
    use different, simpler types that are sufficient for the given scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and controlling external mocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just saw how to connect a system test against a running enterprise application.
    But how can we control and manipulate the external system that is used inside
    the application's use case?
  prefs: []
  type: TYPE_NORMAL
- en: An external system can be mocked away using mock server technology such as **WireMock**.
    WireMock runs as a standalone web server, which is configured to answer specific
    requests accordingly. It acts like a code level test mock object, that stubs and
    verifies behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using container orchestration frameworks for system tests is
    that services can be easily replaced by mock servers. The external system's infrastructure
    as code configuration for the system test environment can contain a WireMock Docker
    image, which is executed instead of the actual system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows an example Kubernetes configuration for the
    assembly line system, using a WireMock Docker image in the running pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The system test connects against this service, using an administration URL to
    set up and modify the mock server's behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows an implementation of the `AssemblyLine` test
    delegate, using the WireMock API to control the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The initial behavior instructs the WireMock instance to answer HTTP requests
    appropriately. The behavior can also be modified during the test case, if more
    complex processes and conversations have to be represented.
  prefs: []
  type: TYPE_NORMAL
- en: If a more sophisticated test scenario involves asynchronous communication such
    as long-running processes, the test cases can use polling to wait for verifications.
  prefs: []
  type: TYPE_NORMAL
- en: The defined car manufacturer and assembly line delegates can be reused within
    multiple test scenarios. Some cases might require to run system tests mutually
    exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Maintaining test data and scenarios* section, we will see what further
    methods and approaches support developers in writing maintainable test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tests verify the non-functional requirement of how a system performs
    in terms of responsiveness. They don't verify business logic, they verify the
    application's technology, implementation, and configuration.
  prefs: []
  type: TYPE_NORMAL
- en: In production systems the load on the systems can vary heavily. This is especially
    true for applications that are publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to tests that verify business behavior, it can be helpful to test upfront
    whether an application, or component thereof, is likely to meet their performance
    expectations in production. The motivation is to prevent major performance drops,
    potentially caused by introduced errors.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to consider the application logic when constructing performance
    test scenarios. Some invocations perform more expensive processes than others.
    Generally it makes sense to construct performance tests after realistic production
    scenarios, in regard to the frequency and nature of requests.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the ratio of guests browsing an online shop to customers actually
    performing purchase transactions should somehow reflect the real world.
  prefs: []
  type: TYPE_NORMAL
- en: However, it also makes sense to construct tests that excessively perform expensive
    invocations, to detect potential issues that emerge when the system is under stress.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 9, *Monitoring, Performance, and Logging*, we will see why performance
    tests on environments other than production are a poor tool to explore the application's
    limits and potential bottlenecks. Instead of putting great effort into crafting
    sophisticated performance test scenarios, it makes more sense to invest into technical
    insights into production systems.
  prefs: []
  type: TYPE_NORMAL
- en: Still, we will see a few techniques of how to craft simple load tests that put
    a simulated application under pressure to find evident issues.
  prefs: []
  type: TYPE_NORMAL
- en: A reasonable attempt is to simulate usual load, ramp up the number of concurrent
    users, and explore at which point the application becomes unresponsive. If the
    responsiveness breaks sooner than from an earlier test run, this could indicate
    an issue.
  prefs: []
  type: TYPE_NORMAL
- en: Key performance indicators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Key performance indicators give information about the application's responsiveness
    during normal behavior as well as under simulated workload. There are several
    indicators that can be gathered that directly affect the user, such as the response
    time or the error rate. These gauges represent the system's state and will provide
    insights about its behavior under performance tests. The indicated values will
    change depending on the number of concurrent users as well as the test scenario.
  prefs: []
  type: TYPE_NORMAL
- en: A first interesting insight is the application's response time - the time it
    takes to respond to a client's request including all transmissions. It directly
    affects the quality of the offered service. If the response time falls below a
    certain threshold, timeouts may occur that cancel and fail the request. The latency
    time is the time it takes until the server receives the first byte of the request.
    It mainly depends on the network setup.
  prefs: []
  type: TYPE_NORMAL
- en: During performance tests, it's especially interesting to see how the response
    time and latency time change compared to their average. When increasing the load
    on an application, at some point the application will become unresponsive. This
    unresponsiveness can originate from all kinds of reasons. For example, available
    connections or threads may be consumed, timeouts can occur, or database optimistic
    locking may fail. The request error rate represents the ratio of failed requests.
  prefs: []
  type: TYPE_NORMAL
- en: The number of concurrent users or load size in a specific interval of time affects
    the performance metrics of the application and needs to be considered in the test
    results. A higher number of users will put the system under more stress, depending
    on the nature of the request. This number is related to the number of concurrent
    transactions, technical transactions in this case, that indicate how many transactions
    the application can handle at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU and memory utilization provide insights about the application's resources.
    Whereas the values don't necessarily say much about the application's health,
    they represent the trend of resource consumption during load simulation.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the overall throughput indicates the total amount of data that the
    server transmits to the connected users at any point in time.
  prefs: []
  type: TYPE_NORMAL
- en: The key performance indicators provide insights about the application's responsiveness.
    They help gather experience and especially trends during development. This experience
    can be used to verify future application versions. Especially after making changes
    in technology, implementation, or configuration, performance tests can indicate
    a potential performance impact.
  prefs: []
  type: TYPE_NORMAL
- en: Developing performance tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It makes sense to design performance test scenarios that are close to the real
    world. Performance test technology should support scenarios that not only ramp
    up a big amount of users, but simulate user behavior. Typical behavior could be,
    for example, a user visits the home page, logs in, follows a link to an article,
    adds the article to their shopping cart, and performs the purchase.
  prefs: []
  type: TYPE_NORMAL
- en: There are several performance test technologies available. At the time of writing,
    the arguably most used ones are **Gatling** and **Apache JMeter**.
  prefs: []
  type: TYPE_NORMAL
- en: Apache JMeter executes test scenarios that put applications under load and generates
    reports from the test execution. It uses XML-based configuration, supports multiple
    or custom communication protocols and can be used to replay recorded load test
    scenarios. Apache JMeter defines test plans that contain compositions of so-called
    samplers and logic controllers. They are used to define test scenarios that simulate
    user behavior. JMeter is distributed, using a master/slave architecture and can
    be used to generate load from several directions. It ships a graphical UI which
    is is used to edit the test plan configuration. Command-line tools execute the
    tests locally or on a Continuous Integration server.
  prefs: []
  type: TYPE_NORMAL
- en: Gatling provides a similar performance test solution, but it defines test scenarios
    programmatically written in Scala. It therefore provides a lot of flexibility
    in defining test scenarios, behavior of virtual users, and how the test progresses.
    Gatling can also record and reuse user behavior. Since the tests are defined programmatically,
    there are a lot of flexible solutions possible, such as dynamically feeding cases
    from external sources. The so-called checks and assertions are used to verify
    whether a single test request or the whole test case was successful.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike JMeter, Gatling runs on a single host, not distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the definition of a simple Gatling simulation
    in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `create_car` scenario involves three client requests, which retrieve all
    cars, create a car, and follow the created resource. The scenarios configure multiple
    virtual users. The number of users starts at `10` and is ramped up to `20` users
    within `10` seconds runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simulation is triggered via the command line and executed against a running
    environment. Gatling provides test results in HTML files. The following code snippet
    shows the Gatling HTML output of the test example run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87e5c982-56ad-4d5a-9fcf-65516195e3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: This example gives an idea of what is possible with Gatling tests.
  prefs: []
  type: TYPE_NORMAL
- en: Since performance tests should reflect somewhat realistic user scenarios, it
    makes sense to reuse existing system test scenarios for performance tests. Besides
    programmatically defining user behavior, pre-recorded test runs can be used to
    feed in data from external sources such as web server log files.
  prefs: []
  type: TYPE_NORMAL
- en: Insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The point in executing performance tests is less a *green* or *red* outcome
    than the insights gathered from the runs. The test reports and the application's
    behavior are collected during the test runs. These collections enable us to gain
    experience and discover trends in performance.
  prefs: []
  type: TYPE_NORMAL
- en: While the performance tests can be executed standalone, they ideally run continuously
    as part of a Continuous Delivery pipeline. It's already helpful to gain these
    insights without needing to affect the outcome of the pipeline step. After some
    metrics have been gathered, engineers can consider defining a performance run
    as failed if the measured performance indicated a major drop from the usual expectations.
  prefs: []
  type: TYPE_NORMAL
- en: This matches the idea of continuous improvement or in this case avoiding responsiveness
    deterioration.
  prefs: []
  type: TYPE_NORMAL
- en: Running tests locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter covered development workflows and Continuous Delivery pipelines.
    It's crucial for modern enterprise applications to define an effective pipeline.
    However, while the CI server takes care of all build, test, and deploy steps,
    software engineers are still required to build and test on their local environments.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery pipelines with proper tests sufficiently verify that enterprise
    applications work as expected. However, the shortcoming with only relying on the
    pipeline is that engineers receive feedback later and only after they have pushed
    their changes to the central repository. Whereas this is the idea behind Continuous
    Integration, developers still want certainty in their changes before committing
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Committing changes that contain careless mistakes disturbs other team members
    by unnecessarily breaking the build. Errors that are easy to detect can be prevented
    by verifying the commit locally. This is certainly doable in code level tests,
    such as unit, component, and integration tests, which run on local environments
    as well. Performing code level tests before committing prevents the majority of
    mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: When developing technical or cross-cutting concerns, such as interceptors or
    JAX-RS JSON mapping, engineers also want feedback before committing the changes
    to the pipeline. As mentioned before, system tests against actually running applications
    provide the most realistic verification.
  prefs: []
  type: TYPE_NORMAL
- en: For local environments, developers could write sophisticated integration tests,
    running on embedded containers, to receive faster feedback. However, as we saw
    previously, this requires quite some time and effort and still does not reliably
    cover all situations.
  prefs: []
  type: TYPE_NORMAL
- en: Using container technologies enables engineers to run the same software images
    on multiple environments, including locally. There are Docker installations available
    for the major operating systems. Local machines can run Docker containers in the
    same way as in production, setting up custom configuration or wiring up their
    own networks, if required.
  prefs: []
  type: TYPE_NORMAL
- en: This enables us to run fully-fledged system tests on local environments as well.
    Whereas this step doesn't necessarily have to be performed during development,
    it's helpful for developers that want to verify integrational behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can perform build and test steps locally, similar to the Continuous
    Delivery pipeline. Running steps via the command line greatly facilitates this
    approach. Docker `run` commands enable us to dynamically configure volumes, networks,
    or environment variables based on the local host.
  prefs: []
  type: TYPE_NORMAL
- en: In order to automate the process, the separate build, deploy, and test commands
    are combined into shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows one example of a Bash script that performs
    several steps. Bash scripts can be run on Windows as well, via Unix-console emulators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The *hello-cloud* application is contained in the `hello-cloud/` subfolder and
    built with Maven and Docker. The Docker `run` command configures a custom properties
    file. This is similar to the orchestration configuration example shown in [Chapter
    5](a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml), *Container and Cloud Environments
    with Java EE*.
  prefs: []
  type: TYPE_NORMAL
- en: The `hello-cloud-st/` directory contains system tests that connect against a
    running application. In order to direct the system test to the local environment,
    the *hosts* configuration of the local machine can be adapted. The Maven test
    run executes the system tests.
  prefs: []
  type: TYPE_NORMAL
- en: This approach enables developers to verify behavior in fully-fledged system
    tests that are executed in the Continuous Delivery pipelines as well as locally,
    if required.
  prefs: []
  type: TYPE_NORMAL
- en: If the system test scenario requires several external systems, they are equally
    run as Docker containers, similar to the test environment. Applications that run
    in container orchestration environments use logical service names to resolve external
    systems. The same is possible for natively running Docker containers, that are
    part of custom Docker networks. Docker resolves container names in containers
    running in the same network.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is used to run all kinds of services locally and is especially
    useful to run mock servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows an example of the idea of running a local test
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the system test example, the WireMock server will be configured as
    part of the test case. The local environment needs to ensure that hostnames point
    to the corresponding localhost containers.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex setups, it makes sense to run the services in a container orchestration
    cluster as well. There are local installation options for Kubernetes or OpenShift
    available. The container orchestration abstracts cluster nodes. It therefore makes
    no difference for infrastructure as code definitions, whether a cluster runs locally,
    as a single node, in a server environment on-premises, or in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: This enables engineers to use the very same definitions that are used in the
    test environments. Running a local Kubernetes installation simplifies the shell
    scripts to a few `kubectl` commands.
  prefs: []
  type: TYPE_NORMAL
- en: If installing Kubernetes or OpenShift locally is too oversized, orchestration
    alternatives such as Docker Compose can be used as lightweight alternatives. Docker
    Compose also defines multi-container environments and their configuration in infrastructure
    as code files - executable by a single command. It provides similar benefits as
    Kubernetes. Arquillian Cube is another sophisticated way of orchestrating and
    running Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Automating steps locally via scripts, highly increases the developer's productivity.
    Running system tests on local machines benefits engineers by providing faster
    feedback with less disruption.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining test data and scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test cases verify that the application will behave as expected when deployed
    to production. The tests also ensure that the expectations are still met when
    new features are developed.
  prefs: []
  type: TYPE_NORMAL
- en: However, it's not sufficient to define test scenarios and test data only once.
    Business logic will evolve and change over time and test cases need to adapt.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of maintainable tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both for writing and managing test cases, it's crucial to create maintainable
    test code. Over time the number of test cases will increase. In order to stay
    productive during development, some time and effort needs to be spent on the test
    code quality.
  prefs: []
  type: TYPE_NORMAL
- en: For production code, every engineer agrees that code quality is an important
    requirement. Since tests are not part of the application that is running in production
    they are often treated differently. Experience shows that developers rarely invest
    time and effort in test code quality. However, the quality of test cases has a
    huge impact on the developer's productivity.
  prefs: []
  type: TYPE_NORMAL
- en: There are some signs that indicate poorly written tests.
  prefs: []
  type: TYPE_NORMAL
- en: Signs of lack of test quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally speaking, development time that is overly spent in test code rather
    than in production code can be a sign of poorly designed or crafted tests. A feature
    that is being implemented or changed will cause some tests to fail. How fast can
    the test code adapt? How many occurrences of test data or functionality are there
    that need to be changed? How easy is it to add test cases to the existing code
    base?
  prefs: []
  type: TYPE_NORMAL
- en: Failing tests that are being `@Ignored` for more than a very short period of
    time are also an indicator of a potential flaw in test quality. If the test case
    is logically still relevant, it needs to be stabilized and fixed. If it becomes
    obsolete, it should be deleted. However, tests should never be deleted in order
    to save time and effort that would be necessary to fix them when the test scenarios
    would logically still be relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Copy and pasting test code should also be an alarming signal. This practice
    is sadly quite common in enterprise projects, especially when test scenarios slightly
    differ in their behavior. Copy and pasting violates the **don't repeat yourself**
    (**DRY**) principle and introduces a lot of duplication which makes future changes
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Test code quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While production code quality is important for keeping a constant development
    velocity, test code quality is so as well. Tests, however, are mostly not treated
    in the same way. Experience shows that enterprise projects rarely invest time
    and effort into refactoring tests.
  prefs: []
  type: TYPE_NORMAL
- en: In general the same practices for high code quality apply for test code as they
    do for live code. Certain principles are especially important for tests.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, the DRY principle certainly has its importance. On code level
    this means to avoid repeating definitions, test procedures, and code duplication
    that contains just minor differences.
  prefs: []
  type: TYPE_NORMAL
- en: For test data, the same principle applies. Experience shows that multiple test
    case scenarios that use similar test data tempt developers to use copy and pasting.
    However, doing so will lead to an unmaintainable code base, once changes in the
    test data have to be made.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true for assertions and mock verifications. Assert statements and
    verifications that are applied one by one directly in the test method, similarly
    lead to duplication and challenges with maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Typically the biggest issue in test code quality is missing abstraction layers.
    Test cases too often contain different aspects and responsibilities. They mix
    business with technical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me give an example of a poorly written system test in pseudo code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Readers might have noticed that it requires quite some effort to comprehend
    the test case. The inline comments provide some help, but comments like these
    are in general rather a sign of poorly constructed code.
  prefs: []
  type: TYPE_NORMAL
- en: The example, however, is similar to the system test example crafted previously.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with test cases like these is not only that they're harder to
    comprehend. Mixing multiple concerns, both technically and business motivated
    into a single class, or even a single method, introduces duplication and rules
    out maintainability. What if the payload of the car manufacture service changes?
    What if the logical flow of the test case changes? What if new test cases with
    similar flow but different data need to be written? Do developers copy and paste
    all code and modify the few aspects then? Or what if the overall communication
    changes from HTTP to another protocol?
  prefs: []
  type: TYPE_NORMAL
- en: For test cases, the most important code quality principles are to apply proper
    abstraction layers together with delegation.
  prefs: []
  type: TYPE_NORMAL
- en: Developers need to ask themselves which concerns this test scenario has. There
    is the test logical flow, verifying the creation of a car with required steps.
    There is the communication part, involving HTTP invocations and JSON mapping.
    There might be an external system involved, maybe represented as a mock server
    that needs to be controlled. And there are assertions and verifications to be
    performed on these different aspects.
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason why we crafted the previous system test example with several
    components, all of them concerning different responsibilities. There should be
    one component for accessing the application under test, including all communication
    implementation details required. In the previous example, this was the responsibility
    of the car manufacturer delegate.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the assembly line delegate, it makes sense to add one component for
    every mock system involved. These components encapsulate configuration, control,
    and verification behavior of the mock servers.
  prefs: []
  type: TYPE_NORMAL
- en: Verifications that are made on test business level are advisably outsourced
    as well, either into private methods or delegates depending on the situation.
    The test delegates can then again encapsulate logic into more abstraction layers,
    if required by the technology or the test case.
  prefs: []
  type: TYPE_NORMAL
- en: All of these delegate classes and methods become single points of responsibility.
    They are reused within all similar test cases. Potential changes only affect the
    points of responsibility, leaving other parts of the test cases unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: This requires the definition of clear interfaces between the components that
    don't leak the implementation details. For this reason it makes sense, especially
    for the system test scope, to have a dedicated, simple model representation. This
    model can be implemented simply and straightforward with potentially less type
    safety than the production code.
  prefs: []
  type: TYPE_NORMAL
- en: A reasonable green field approach, similar to the previous system test example,
    is to start with writing comments and continuously replacing them with delegates
    while going down the abstraction layers. This starts with what the test logically
    executes first, implementation details second. Following that approach naturally
    avoids mixing business and technical test concerns. It also enables simpler integration
    of test technology that supports writing tests productively, such as **Cucumber-JVM**
    or **FitNesse**.
  prefs: []
  type: TYPE_NORMAL
- en: Test technology support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some test technology also support crafting maintainable tests. AssertJ, for
    example, provides the possibility to create custom assertions. In our test case
    the car needs to verify the correct engine and color encapsulated into car specifications.
    Custom assertions can decrease overall duplication in the test scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows a custom `AssertJ` assertion for verifying a car:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The assertion is then usable within the test scope. The correct static import
    of the `CarAssert` class has to be chosen for the `assertThat()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The examples in this chapter showed tests written mainly with Java, JUnit, and
    Mockito, with the exception of embedded application containers and Gatling. There
    are dozens of other test technologies that uses different frameworks as well as
    dynamic JVM languages.
  prefs: []
  type: TYPE_NORMAL
- en: A famous example of this the **Spock Testing Framework** which uses Groovy.
    The motivation behind this technology was to write leaner, more maintainable tests.
    Since dynamic JVM languages such as Groovy or Scala are less verbose than plain
    Java, this idea sounds reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Test frameworks, such as Spock, indeed result in test cases that require minimal
    code. They make use of dynamic JVM language features such as less-constraint method
    names such as `def "car X123A234 should be created"()`. Spock testing also provides
    clear readability with low effort.
  prefs: []
  type: TYPE_NORMAL
- en: However, readable tests are achievable with all test technologies if test code
    quality is minded. Maintainability, in particular, is rather a question of well-crafted
    test cases and proper abstraction layers than of the technology being used. Once
    test cases become quite complex, the impact of the technology on maintainability
    becomes less relevant.
  prefs: []
  type: TYPE_NORMAL
- en: When choosing test technology, the team's familiarity with the technology should
    also be considered. At the time of writing, enterprise Java developers are usually
    less familiar with dynamic JVM languages.
  prefs: []
  type: TYPE_NORMAL
- en: However, the test code quality should be more important than the used technology.
    Applying good practices of software engineering to tests should be considered
    as mandatory, using other test frameworks as optional. Refactoring test cases
    frequently increases the maintainability and reusability of test components and
    ultimately the quality of the software project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tests are required to verify software functionality in simulated environments.
    Software tests should run predictably, isolated, reliably, fast, and in an automated
    way. In order to enable productive project life cycles, it's important to keep
    tests maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests verify the behavior of individual units of an application, mostly
    single entity, boundary, or control classes. Component tests verify the behavior
    of coherent components. Integration tests fulfill the need to verify the interaction
    of the Java EE components. Database integration tests use embedded databases together
    with standalone JPA to verify the persistence mapping. System tests verify deployed
    applications that run on actual environments. Container orchestration heavily
    supports running system test environments with potential mock applications.
  prefs: []
  type: TYPE_NORMAL
- en: In order to verify functionality before it is pushed to the central repository,
    engineers need the ability to run tests on their local environment. Changes that
    contain careless mistakes disturb other teammates by unnecessarily breaking the
    build. Docker, Docker Compose, and Kubernetes can run on local environments as
    well, enabling developers to verify behavior upfront. It's advisable to craft
    simple automation scripts that include the required steps.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve a constant development velocity, it's required to develop
    maintainable test cases. In general, test code should have a similar quality to
    production code. This includes refactoring, proper abstraction layers, and software
    quality in general.
  prefs: []
  type: TYPE_NORMAL
- en: These approaches are, in fact, more helpful than introducing sophisticated test
    frameworks using dynamic JVM languages. Whereas frameworks such as Spock certainly
    enable readable, minimal test cases, the benefits of using proper practices of
    software craftsmanship have a more positive impact on the overall test code quality,
    especially once test scenarios become complex. No matter what testing technology
    is being used, software engineers are advised to mind the test code quality in
    order to keep test cases maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapter will cover the topic of distributed systems and microservices
    architectures.
  prefs: []
  type: TYPE_NORMAL
