- en: Concurrency and Parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The need for concurrency has grown rapidly in the past 10 years and has become
    a necessity for every professional Java programmer. Concurrency (also called **multithreading**)
    is essentially multitasking, where you have several processes executing at the
    same time. If you want to fully utilize your hardware's computing power (whether
    it is a phone, server, laptop, or desktop computer), you need to learn how to
    multithread and leverage concurrency. Thankfully, RxJava makes concurrency much
    easier and safer to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of concurrency and its necessity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subscribeOn()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`observeOn()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unsubscribeOn()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why concurrency is necessary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In simpler times, computers had only one CPU and this marginalized the need
    for concurrency. Hardware manufacturers successfully found ways to make CPUs faster,
    and this made single-threaded programs faster. But eventually, this had a diminishing
    return, and manufacturers found they could increase computational power by putting
    multiple CPUs in a device. From desktops and laptops to servers and smartphones,
    most hardware nowadays sports multiple CPUs, or cores.
  prefs: []
  type: TYPE_NORMAL
- en: For developers, this is a major disruption in building software and how coding
    is done. Single-threaded software is easier to code and works fine on a single-core
    device. But a single-threaded program on a multi-core device will only use one
    core, leaving the others not utilized. If you want your program to scale, it needs
    to be coded in a way that utilizes all cores available in a processor.
  prefs: []
  type: TYPE_NORMAL
- en: However, concurrency is traditionally not easy to implement. If you have several
    independent processes that do not interact with each other, it is easier to accomplish.
    But when resources, especially mutable objects, are shared across different threads
    and processes, chaos can ensue if locking and synchronization are not carefully
    implemented. Not only can threads race each other chaotically to read and change
    an object's properties, but a thread may simply not see a value changed by another
    thread! This is why you should strive to make your objects immutable and make
    as many properties and variables `final` as possible. This ensures that properties
    and variables are thread-safe and anything that is mutable should be synchronized
    or at least utilize the `volatile` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, RxJava makes concurrency and multithreading much easier and safer.
    There are ways you can undermine the safety it provides, but generally, RxJava
    handles concurrency safely for you mainly using two operators: `subscribeOn()`
    and `observeOn()`.  As we will find out in this chapter, other operators such
    as `flatMap()` can be combined with these two operators to create powerful concurrency
    dataflows.'
  prefs: []
  type: TYPE_NORMAL
- en: While RxJava can help you make safe and powerful concurrent applications with
    little effort, it can be helpful to be aware of the traps and pitfalls in multithreading.
    Joshua Bloch's famous book *Effective Java* is an excellent resource that every
    Java developer should have, and it succinctly covers best practices for concurrent
    applications. If you want deep knowledge in Java concurrency, ensure that you
    read Brian Goetz' *Java Concurrency in Practice* as well.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency, also called **multithreading**, can be applied in a variety of
    ways. Usually, the motivation behind concurrency is to run more than one task
    simultaneously in order to get work done faster. As we discussed in the beginning
    of this book, concurrency can also help our code resemble the real world more,
    where multiple activities occur at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's cover some fundamental concepts behind concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'One common application of concurrency is to run different tasks simultaneously.
    Imagine that you have three yard chores: mow the lawn, trim the trees, and pull
    the weeds. If you do these three chores by yourself, you can only do one chore
    at a time. You cannot mow the lawn and trim the trees simultaneously. You have
    to sequentially mow the lawn first, then trim the trees, then pull the weeds.
    But if you have a friend to help you, one of you can mow the lawn while the other
    trims the trees. The first one of you to get done can then move on to the third
    task: pulling the weeds. This way, these three tasks get done much more quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Metaphorically, you and your friend are **threads**. You do work together. Collectively,
    you both are a **thread pool** ready to execute tasks. The chores are tasks that
    are queued for the thread pool, which you can execute two at a time. If you have
    more threads, your thread pool will have more bandwidth to take on more tasks
    concurrently. However, depending on how many cores your computer has (as well
    as the nature of the tasks), you can only have so many threads. Threads are expensive
    to create, maintain, and destroy, and there is a diminishing return in performance
    as you create them excessively. That is why it is better to have a thread pool
    to *reuse* threads and have them work a queue of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelization (also called parallelism) is a broad term that could encompass
    the preceding scenario. In effect, you and your friend are executing two tasks
    at the same time and are thus processing in parallel. But let's apply parallelization
    to processing multiple identical tasks at the same time. Take, for example, a
    grocery store that has 10 customers waiting in a line for checkout. These 10 customers
    represent 10 tasks that are identical. They each need to check out their groceries.
    If a cashier represents a thread, we can have multiple cashiers to process these
    customers more quickly. But like threads, cashiers are expensive. We do not want
    to create a cashier for each customer, but rather pool a fixed number of cashiers
    and reuse them. If we have five cashiers, we can process five customers at a time
    while the rest wait in the queue. The moment a cashier finishes a customer, they
    can process the next one.
  prefs: []
  type: TYPE_NORMAL
- en: This is essentially what parallelization achieves. If you have 1000 objects
    and you need to perform an expensive calculation on each one, you can use five
    threads to process five objects at a time and potentially finish this process
    five times more quickly. It is critical to pool these threads and reuse them because
    creating 1000 threads to process these 1000 objects could overwhelm your memory
    and crash your program.
  prefs: []
  type: TYPE_NORMAL
- en: With a conceptual understanding of concurrency, we will move on to discussing
    how it is achieved in RxJava.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing RxJava concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency in RxJava is simple to execute, but somewhat abstract to understand.
    By default, Observables execute work on the immediate thread, which is the thread
    that declared the `Observer` and subscribed it. In many of our earlier examples,
    this was the main thread that kicked off our `main()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'But as hinted in a few other examples, not all Observables will fire on the
    immediate thread. Remember those times we used `Observable.interval()`, as shown
    in the following code? Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This `Observable` will actually fire on a thread other than the main one. Effectively,
    the main thread will kick-off `Observable.interval(),` but not wait for it to
    complete because it is operating on its own separate thread now. This, in fact,
    makes it a concurrent application because it is leveraging two threads now. If
    we do not call a `sleep()` method to pause the main thread, it will charge to
    the end of the `main()` method and quit the application before the intervals have
    a chance to fire.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, concurrency is useful only when you have long-running or calculation-intensive
    processes. To help us learn concurrency without creating noisy examples, we will
    create a helper method called `intenseCalculation()` to emulate a long-running
    process. It will simply accept any value and then sleep for 0-3 seconds and then
    return the same value.  Sleeping a thread, or pausing it, is a great way to simulate
    a busy thread doing work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create two Observables with two Observers subscribing to them. In each
    operation. map each emission to the `intenseCalculation()` method in order to
    slow them down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note how both Observables fire emissions slowly as each one is slowed by 0-3
    seconds in the `map()` operation. More importantly, note how the first `Observable`
    firing `Alpha`, `Beta`, `Gamma` must finish first and call `onComplete()` before
    firing the second `Observable` emitting the numbers `1` through `6`.  If we fire
    both Observables at the same time rather than waiting for one to complete before
    starting the other, we could get this operation done much more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can achieve this using the `subscribeOn()` operator, which suggests to the
    source to fire emissions on a specified `Scheduler`. In this case, let us use
    `Schedulers.computation()`, which pools a fixed number of threads appropriate
    for computation operations. It will provide a thread to push emissions for each
    `Observer`. When `onComplete()` is called, the thread will be given back to `Scheduler`
    so it can be reused elsewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (yours may be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Your output will likely be different from mine due to the random sleeping times.
    But note how both operations are firing simultaneously now, allowing the program
    to finish much more quickly. Rather than the main thread becoming occupied, executing
    emissions for the first `Observable` before moving onto the second, it will fire-off
    both Observables immediately and move on. It will not wait for either `Observable`
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple processes occurring at the same time is what makes an application
    concurrent. It can result in much greater efficiency as it will utilize more cores
    and finish work more quickly. Concurrency also makes code models more powerful
    and more representative of how our world works, where multiple activities occur
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Something else that is exciting about RxJava is its operators (at least the
    official ones and the custom ones built properly). They can work with Observables
    on different threads safely. Even operators and factories that combine multiple
    Observables, such as `merge()` and `zip()`, will safely combine emissions pushed
    by different threads. For instance, we can use  `zip()` on our two Observables
    in the preceding example even if they are emitting on two separate computation
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Being able to split and combine Observables happening on different threads is
    powerful and eliminates the pain points of callbacks. Observables are agnostic
    to whatever thread they work on, making concurrency easy to implement, configure,
    and evolve at any time.
  prefs: []
  type: TYPE_NORMAL
- en: When you start making reactive applications concurrent, a subtle complication
    can creep in.  By default, a non-concurrent application will have one thread doing
    all the work from the source to the final `Observer`. But having multiple threads
    can cause emissions to be produced faster than an `Observer` can consume them
    (for instance, the `zip()` operator may have one source producing emissions faster
    than the other). This can overwhelm the program and memory can run out as backlogged
    emissions are cached by certain operators. When you are working with a high volume
    of emissions (more than 10,000) and leveraging concurrency, you will likely want
    to use Flowables instead of Observables, which we will cover in [Chapter 8](14efb9e9-14a6-41ba-86cb-20b5674dce8e.xhtml), *Flowables
    and Backpressure*.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping an application alive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this point, we have used a `sleep()` method to keep concurrent reactive
    applications from quitting prematurely, just long enough for the Observables to
    fire. If you are using Android, JavaFX, or other frameworks that manage their
    own non-daemon threads, this is not a concern as the application will be kept
    alive for you. But if you are simply firing off a program with a `main()` method
    and you want to kick off long-running or infinite Observables, you may have to
    keep the main thread alive for a period longer than 5-20 seconds. Sometimes, you
    may want to keep it alive indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to keep an application alive indefinitely is to simply pass `Long.MAX_VALUE`
    to the `Thread.sleep()` method, as shown in the following code, where we have `Observable.interval()`
    firing emissions forever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Okay, sleeping your main thread for 9,223,372,036,854,775,807 milliseconds is
    not forever, but that is the equivalent to  292,471,208.7 years. For the purposes
    of sleeping a thread, that might as well be forever!
  prefs: []
  type: TYPE_NORMAL
- en: There are ways to keep an application alive only long enough for a subscription
    to finish. With classical concurrency tools discussed in Brian Goetz' book *Java
    Concurrency in Practice*, you can keep an application alive using `CountDownLatch`
    to wait for two subscriptions to finish. But an easier way is to use blocking
    operators in RxJava.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use blocking operators to stop the declaring thread and wait for emissions.
    Usually, blocking operators are used for unit testing (as we will discuss in [Chapter
    10](ec80132f-c411-4cc1-87b2-7a8ebba089b8.xhtml), *Testing and Debugging*), and
    they can attract antipatterns if used improperly in production. However, keeping
    an application alive based on the life cycle of a finite `Observable` subscription
    is a valid case to use a blocking operator. As shown here, `blockingSubscribe()`
    can be used in place of `subscribe()` to stop and wait for `onComplete()` to be
    called before the main thread is allowed to proceed and exit the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We will discuss blocking operators in further detail in [Chapter 10](ec80132f-c411-4cc1-87b2-7a8ebba089b8.xhtml),
    *Testing and Debugging*. For the remainder of this chapter, we will explore concurrency
    in detail using the `subscribeOn()` and `observeOn()` operators. But first, we
    will cover the different `Scheduler` types available in RxJava.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, thread pools are a collection of threads. Depending on
    the policy of that thread pool, threads may be persisted and maintained so they
    can be reused. A queue of tasks is then executed by that thread pool.
  prefs: []
  type: TYPE_NORMAL
- en: Some thread pools hold a fixed number of threads (such as the  `computation()`
    one we used earlier), while others dynamically create and destroy threads as needed.
    Typically in Java, you use an `ExecutorService` as your thread pool. However,
    RxJava implements its own concurrency abstraction called `Scheduler`.  It define
    methods and rules that an actual concurrency provider such as an `ExecutorService`
    or actor system must obey. The construct flexibly makes RxJava non-opinionated
    on the source of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the default `Scheduler` implementations can be found in the `Schedulers`
    static factory class. For a given `Observer`, a `Scheduler` will provide a thread
    from its pool that will push the emissions. When `onComplete()` is called, the
    operation will be disposed of and the thread will be given back to the pool, where
    it may be persisted and reused by another `Observer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep this book practical, we will only look at Schedulers in their natural
    environment: being used with `subscribeOn()` and `observeOn()`. If you want to
    learn more about Schedulers and how they work in isolation, refer to Appendix
    X to learn more.'
  prefs: []
  type: TYPE_NORMAL
- en: Here are a few Scheduler types in RxJava. There are also some common third-party
    ones available in other libraries such as RxAndroid (covered in [Chapter 11](4d8d0f1a-6015-4c42-82db-cb7f966e9f7c.xhtml), *RxJava
    for Android*) and RxJavaFX (covered later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already saw the computation `Scheduler`, which you can get the global instance
    of by calling `Schedulers.computation()`. This will maintain a fixed number of
    threads based on the processor count available to your Java session, making it
    appropriate for computational tasks. Computational tasks (such as math, algorithms,
    and complex logic) may utilize cores to their fullest extent. Therefore, there
    is no benefit in having more worker threads than available cores to perform such
    work, and the computational `Scheduler` will ensure that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When you are unsure how many tasks will be executed concurrently or are simply
    unsure which `Scheduler` is the right one to use, prefer the computation one by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: A number of operators and factories will use the computation `Scheduler` by
    default unless you specify a different one as an argument. These include one or
    more overloads for `interval()`, `delay()`, `timer()`, `timeout()`, `buffer()`,
    `take()`, `skip()`, `takeWhile()`, `skipWhile()`, `window()`, and a few others.
  prefs: []
  type: TYPE_NORMAL
- en: IO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'IO tasks such as reading and writing databases, web requests, and disk storage
    are less expensive on the CPU and often have idle time waiting for the data to
    be sent or come back. This means you can create threads more liberally, and `Schedulers.io()`
    is appropriate for this. It will maintain as many threads as there are tasks and
    will dynamically grow, cache, and reduce the number of threads as needed. For
    instance, you may use `Schedulers.io()` to perform SQL operations using RxJava-JDBC
    ([https://github.com/davidmoten/rxjava-jdbc](https://github.com/davidmoten/rxjava-jdbc)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: But you have to be careful! As a rule of thumb, assume that each subscription
    will result in a new thread.
  prefs: []
  type: TYPE_NORMAL
- en: New thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Schedulers.newThread()` factory will return a `Scheduler` that does not
    pool threads at all. It will create a new thread for each `Observer` and then
    destroy the thread when it is done. This is different than `Schedulers.io()` because
    it does not attempt to persist and cache threads for reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This may be helpful in cases where you want to create, use, and then destroy
    a thread immediately so it does not take up memory. But for complex applications
    generally, you will want to use `Schedulers.io()` so there is some attempt to
    reuse threads if possible.  You also have to be careful as  `Schedulers.newThread()`
    can run amok in complex applications (as can `Schedulers.io()`) and create a high
    volume of threads, which could crash your application.
  prefs: []
  type: TYPE_NORMAL
- en: Single
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you want to run tasks sequentially on a single thread, you can invoke
    `Schedulers.single()`. This is backed by a single-threaded implementation appropriate
    for event looping. It can also be helpful to isolate fragile, non-threadsafe operations
    to a single thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Trampoline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Schedulers.trampoline()` is an interesting `Scheduler`. In practicality, you
    will not invoke it often as it is used primarily in RxJava''s internal implementation.
    Its pattern is also borrowed for UI Schedulers such as RxJavaFX and RxAndroid.
    It is just like default scheduling on the immediate thread, but it prevents cases
    of recursive scheduling where a task schedules a task while on the same thread.
    Instead of causing a stack overflow error, it will allow the current task to finish
    and *then* execute that new scheduled task afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: ExecutorService
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can build a Scheduler off a standard Java `ExecutorService`. You may choose
    to do this in order to have more custom and fine-tuned control over your thread
    management policies. For example, say, we want to create a Scheduler that uses
    20 threads. We can create a new fixed `ExecutorService` specified with this number
    of threads. Then, you can wrap it inside a `Scheduler` implementation by calling
    `Schedulers.from()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`ExecutorService` will likely keep your program alive indefinitely, so you
    have to manage its disposal if its life is supposed to be finite. If I only wanted
    to support the life cycle of one `Observable` subscription, I need to call its
    `shutdown()` method. That is why I called its `shutdown()` method after the operation
    terminates or disposes via the `doFinally()` operator.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting and shutting down Schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each default `Scheduler` is lazily instantiated when you first invoke its usage.
    You can dispose the `computation()`, `io()`, `newThread()`, `single()`, and `trampoline()`
    Schedulers at any time by calling their `shutdown()` method or all of them by
    calling `Schedulers.shutdown()`. This will stop all their threads and forbid new
    tasks from coming in and will throw an error if you try otherwise. You can also
    call their `start()` method, or `Schedulersers.start()`, to reinitialize the Schedulers
    so they can accept tasks again.
  prefs: []
  type: TYPE_NORMAL
- en: In desktop and mobile app environments, you should not run into many cases where
    you have to start and stop the Schedulers. On the server side, however, J2EE-based
    applications (for example, Servlets) may get unloaded and reloaded and use a different
    classloader, causing the old Schedulers instances to leak. To prevent this from
    occurring, the Servlet should shut down the `Schedulers` manually in its `destroy()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Only manage the life cycle of your Schedulers if you absolutely have to. It
    is better to let the Schedulers dynamically manage their usage of resources and
    keep them initialized and available so tasks can quickly be executed at a moment's
    notice. Note carefully that it is better to ensure that all outstanding tasks
    are completed or disposed of before you shut down the Schedulers, or else you
    may leave the sequences in an inconsistent state.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding subscribeOn()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We kind of touched on using `subscribeOn()` already, but in this section, we
    will explore it in more detail and look at how it works.
  prefs: []
  type: TYPE_NORMAL
- en: The `subscribeOn()` operator will suggest to the source `Observable` upstream
    which `Scheduler` to use and how to execute operations on one of its threads.
    If that source is not already tied to a particular `Scheduler`, it will use the `Scheduler`
    you specify. It will then push emissions *all the way* to the final `Observer`
    using that thread (unless you add `observeOn()` calls, which we will cover later).
    You can put `subscribeOn()` anywhere in the `Observable` chain, and it will suggest
    to the upstream all the way to the origin `Observable` which thread to execute
    emissions with.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, it makes no difference whether you put this `subscribeOn()`
    right after `Observable.just()` or after one of the operators. The `subscribeOn()`
    will communicate upstream to the `Observable.just()` which `Scheduler` to use
    no matter where you put it. For clarity, though, you should place it as close
    to the source as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Having multiple Observers to the same `Observable` with `subscribeOn()` will
    result in each one getting its own thread (or have them waiting for an available
    thread if none are available). In the `Observer`, you can print the executing
    thread''s name by calling `Thread.currentThread().getName()`. We will print that
    with each emission to see that two threads, in fact, are being used for both Observers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how one `Observer` is using a thread named `RxComputationThreadPool-2`,
    while the other is using `RxComputationThreadPool-1`. These names indicate which
    `Scheduler` they came from (which is the `Computation` one) and what their index
    is. As shown here, if we want only one thread to serve both Observers, we can
    multicast this operation. Just make sure `subscribeOn()` is before the multicast
    operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Most `Observable` factories, such as `Observable.fromIterable()` and `Observable.just()`,
    will emit items on the `Scheduler` specified by `subscribeOn()`. For factories
    such as `Observable.fromCallable()` and `Observable.defer()`, the initialization
    of these sources will also run on the specified `Scheduler` when using `subscribeOn()`.
    For instance, if you use `Observable.fromCallable()` to wait on a URL response,
    you can actually do that work on the IO Scheduler so the main thread is not blocking
    and waiting for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Nuances of subscribeOn()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to note that  `subscribeOn()` will have no practical effect
    with certain sources (and will keep a worker thread unnecessarily on standby until
    that operation terminates). This might be because these Observables already use
    a specific `Scheduler`, and if you want to change it, you can provide a `Scheduler`
    as an argument. For example, `Observable.interval()` will use `Schedulers.computation()` and
    will ignore any `subscribeOn()`you specify otherwise. But you can provide a third
    argument to specify a different `Scheduler` to use. Here, I specify `Observable.interval()`
    to use `Schedulers.newThread()`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This also brings up another point: if you have multiple `subscribeOn()` calls
    on a given `Observable` chain, the top-most one, or the one closest to the source,
    will win and cause any subsequent ones to have no practical effect (other than
    unnecessary resource usage). If I call `subscribeOn()` with `Schedulers.computation()` and
    then call `subscribeOn()` for  `Schedulers.io()`, `Schedulers.computation()` is
    the one that will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This can happen when an API returns an `Observable` already preapplied with
    a `Scheduler` via `subscribeOn()`, although the consumer of the API wants a different
    `Scheduler`. API designers are, therefore, encouraged to provide methods or overloads
    that allow parameterizing which `Scheduler` to use, just like RxJava's Scheduler-dependent
    operators (for example,  `Observable.interval()`).
  prefs: []
  type: TYPE_NORMAL
- en: In summary, `subscribeOn()` specifies which `Scheduler` the source `Observable`
    should use, and it will use a worker from this `Scheduler` to push emissions all
    the way to the final `Observer`. Next, we will learn about `observeOn()`, which
    switches to a different `Scheduler` at that point in the `Observable` chain.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding observeOn()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `subscribeOn()` operator instructs the source `Observable` which `Scheduler`
    to emit emissions on. If `subscribeOn()` is the only concurrent operation in an
    `Observable` chain,  the thread from that `Scheduler` will work the entire `Observable`
    chain, pushing emissions from the source all the way to the final `Observer`.
    The `observeOn()` operator, however, will intercept emissions at that point in
    the `Observable` chain and switch them to a different `Scheduler` going forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike `subscribeOn()`, the placement of `observeOn()` matters. It will leave
    all operations upstream on the default or `subscribeOn()`-defined `Scheduler`, but
    will switch to a different `Scheduler` downstream. Here, I can have an `Observable`
    emit a series of strings that are `/`-separated values and break them up on an
    IO `Scheduler`. But after that, I can switch to a computation `Scheduler` to filter
    only numbers and calculate their sum, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this example is not computationally intensive, and in real life,
    it should be done on a single thread. The overhead of concurrency that we introduced
    is not warranted, but let's pretend it is a long-running process.
  prefs: []
  type: TYPE_NORMAL
- en: Again, use `observeOn()` to intercept each emission and push them forward on
    a different `Scheduler`.  In the preceding example, operators before `observeOn()`
    are executed on  `Scheduler.io()`, but the ones after it are executed by `Schedulers.computation()`.
    Upstream operators before `observeOn()` are not impacted, but downstream ones
    are.
  prefs: []
  type: TYPE_NORMAL
- en: You might use  `observeOn()` for a situation like the one emulated earlier.
    If you want to read one or more data sources and wait for the response to come
    back, you will want to do that part on `Schedulers.io()` and will likely leverage
    `subscribeOn()` since that is the initial operation. But once you have that data,
    you may want to do intensive computations with it, and `Scheduler.io()` may no
    longer be appropriate. You will want to constrain these operations to a few threads
    that will fully utilize the CPU. Therefore, you use `observeOn()` to redirect
    data to  `Schedulers.computation()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can actually use multiple `observeOn()` operators to switch `Schedulers`
    more than once. Continuing with our earlier example, let''s say we want to write
    our computed sum to a disk and write it in a file. Let''s pretend this was a lot
    of data rather than a single number and we want to get this disk-writing operation
    off the computation `Scheduler` and put it back in the IO `Scheduler`. We can
    achieve this by introducing a second `observeOn()`. Let''s also add some `doOnNext() `
    and `doOnSuccess()` (due to the `Maybe`) operators to peek at which thread each
    operation is using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If you look closely at the output, you will see that the `String` emissions
    were initially pushed and split on the IO `Scheduler` via the thread `RxCachedThreadScheduler-1`.
    After that, each emission was switched to the computation `Scheduler` and pushed
    into a sum calculation, which was all done on the thread `RxComputationThreadPool-1`.
    That sum was then switched to the IO `scheduler` to be written to a text file
    (which I specified to output on my Linux Mint desktop), and that work was done
    on `RxCachedThreadScheduler-1` (which happened to be the thread that pushed the
    initial emissions and was reused!).
  prefs: []
  type: TYPE_NORMAL
- en: Using observeOn() for UI event threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to building mobile apps, desktop applications, and other user
    experiences, users have little patience for interfaces that hang up or freeze
    while work is being done. The visual updating of user interfaces is often done
    by a single dedicated UI thread, and changes to the user interface must be done
    on that thread. User input events are typically fired on the UI thread as well.
    If a user input triggers work, and that work is not moved to another thread, that
    UI thread will become busy. This is what makes the user interface unresponsive,
    and today's users expect better than this. They want to still interact with the
    application while work is happening in the background, so concurrency is a must-have.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, RxJava can come to the rescue! You can use `observeOn()` to move
    UI events to a computation or IO `Scheduler` to do the work, and when the result
    is ready, move it back to the UI thread with another `observeOn()`. This second
    usage of `observeOn()` will put emissions on a UI thread using a custom `Scheduler`
    that wraps around the UI thread. RxJava extension libraries such as RxAndroid
    ([https://github.com/ReactiveX/RxAndroid](https://github.com/ReactiveX/RxAndroid)),
    RxJavaFX ([https://github.com/ReactiveX/RxJavaFX](https://github.com/ReactiveX/RxJavaFX)),
    and RxSwing ([https://github.com/ReactiveX/RxSwing](https://github.com/ReactiveX/RxSwing))
    come with these custom `Scheduler` implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, say we have a simple JavaFX application that displays a `ListView<String>`
    of the 50 U.S. states every time a button is clicked on. We can create `Observable<ActionEvent>`
    off the button and then switch to an IO `Scheduler` with `observeOn()` ( `subscribeOn()`
    will have no effect against UI event sources). We can load the 50 states from
    a text web response while on the IO `Scheduler`. Once the states are returned,
    we can use another `observeOn()` to put them back on `JavaFxScheduler`, and safely
    populate them into `ListView<String>` on the JavaFX UI thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The code should run the JavaFX application shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb3da1d9-e11e-47e3-a35a-86be88253a82.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot demonstrates that hitting the REFRESH button will emit
    an event but switch it to an IO `Scheduler` where the work is done to retrieve
    the U.S. states. When the response is ready, it will emit a `List<String>` and
    put it back on the JavaFX `Scheduler` to be displayed in a `ListView`.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts apply to Android development as well, and you put all operations
    affecting the app user interface on `AndroidSchedulers.mainThread()` rather than `JavaFxScheduler.platform()`.
    We will cover Android development in [Chapter 11](4d8d0f1a-6015-4c42-82db-cb7f966e9f7c.xhtml),
    *RxJava for Android*.
  prefs: []
  type: TYPE_NORMAL
- en: Nuances of observeOn()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`observeOn()`comes with nuances to be aware of, especially when it comes to
    performance implications due to lack of backpressure, which we will cover in [Chapter
    8](14efb9e9-14a6-41ba-86cb-20b5674dce8e.xhtml), *Flowables and Backpressure*.'
  prefs: []
  type: TYPE_NORMAL
- en: Say, you have an `Observable` chain with two sets of operations, Operation A
    and Operation B. Let's not worry what operators each one is using. If you do not
    have any `observeOn()`between them, the operation will pass emissions strictly
    one at a time from the source to Operation A, then Operation B, and finally to
    the `Observer`. Even with a `subscribeOn()`, the source will not pass the next
    emission down the chain until the current one is passed all the way to the `Observer`.
  prefs: []
  type: TYPE_NORMAL
- en: This changes when you introduce an `observeOn()` and say we put it between Operation
    A and Operation B. What happens is after Operation A hands an emission to the
    `observeOn(),` it will immediately start the next emission and not wait for the
    downstream to finish the current one, including Operation B and the `Observer`.
    This means that the source and Operation A can *produce* emissions faster than
    Operation B and the `Observer` can *consume* them. This is a classic producer/consumer
    scenario where the producer is producing emissions faster than the consumer can
    consume them. If this is the case, unprocessed emissions will be queued in `observeOn()`
    until the downstream is able to process them. But if you have a lot of emissions,
    you can potentially run into memory issues.
  prefs: []
  type: TYPE_NORMAL
- en: This is why when you have a flow of 10,000 emissions or more, you will definitely
    want to use a `Flowable` (which supports backpressure) instead of an `Observable`.
    Backpressure communicates upstream all the way to the source to slow down and
    only produce so many emissions at a time. It restores *pull-based* requesting
    of emissions even when complex concurrency operations are introduced. We will
    cover this in [Chapter 8](14efb9e9-14a6-41ba-86cb-20b5674dce8e.xhtml), *Flowables
    and Backpressure*.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelization, also called **parallelism** or **parallel computing**, is a
    broad term that can be used for any concurrent activity (including what we covered).
    But for the purposes of RxJava, let's define it as processing multiple emissions
    at a time for a given `Observable`. If we have 1000 emissions to process in a
    given `Observable` chain, we might be able to get work done faster if we process
    eight emissions at a time instead of one. If you recall, the `Observable` contract
    dictates that emissions must be pushed *serially* down an `Observable` chain and
    never race each other due to concurrency. As a matter of fact, pushing eight emissions
    down an `Observable` chain at a time would be downright catastrophic and wreak
    havoc.
  prefs: []
  type: TYPE_NORMAL
- en: This seems to put us at odds with what we want to accomplish, but thankfully,
    RxJava gives you enough operators and tools to be clever. While you cannot push
    items concurrently on the same `Observable`, you are allowed to have multiple
    Observables running at once, each having its own single thread pushing items through.
    As we have done throughout this chapter, we created several Observables running
    on different threads/schedulers and *even combined them*. You actually have the
    tools already, and the secret to achieving parallelization is in the `flatMap()`
    operator, which is, in fact, a powerful concurrency operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have an `Observable` emitting 10 integers, and we are performing `intenseCalculation()` on
    each one. This process can take a while due to the artificial processing we emulated
    with `sleep()`. Let''s print each one with the time in the `Observer` so we can
    measure the performance, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows(yours will be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The randomness causes some variability, of course, but in this instance, it
    took roughly 17 seconds to complete (although your time will likely vary). We
    will probably get better performance if we process emissions in parallel, so how
    do we do that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, serialization (emitting items one at a time) only needs to happen
    on the same `Observable`. The `flatMap()` operator will merge multiple Observables
    derived off each emission even if they are *concurrent*. If a light bulb has not
    gone off yet, read on. In `flatMap()`, let''s wrap each emission into `Observable.just()`,
    use `subscribeOn()` to emit it on  `Schedulers.computation()`, and then `map` it
    to the `intenseCalculation()`. For good measure, let''s print the current thread
    in the `Observer` as well, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (yours will be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This took three seconds to complete, and you will find that this processes items
    much faster.  Of course, my computer has eight cores and that is why my output
    likely indicates that there are eight threads in use. If you have a computer with
    less cores, this process will take longer and use fewer threads. But it will likely
    still be faster than the single-threaded implementation we ran earlier.
  prefs: []
  type: TYPE_NORMAL
- en: What we did is we created a `Observable` off each emission, used `subscribeOn()`
    to emit it on the computation `Scheduler`, and then performed the `intenseCalculation()`,
    which will occur on one of the computation threads. Each instance will request
    its own thread from the computation `Scheduler`, and `flatMap()` will merge all
    of them safely back into a serialized stream.
  prefs: []
  type: TYPE_NORMAL
- en: The `flatMap()`will only let one thread out of it at a time to push emissions
    downstream, which maintains that the `Observable` contract demanding emissions
    stays serialized. A neat little behavior with `flatMap()` is that it will not
    use excessive synchronization or blocking to accomplish this. If a thread is already
    pushing an emission out of `flatMap()` downstream toward `Observer`, any threads
    also waiting to push emissions will simply leave their emissions for that occupying
    thread to take ownership of.
  prefs: []
  type: TYPE_NORMAL
- en: The example here is not necessarily optimal, however. Creating an `Observable`
    for each emission might create some unwanted overhead. There is a leaner way to
    achieve parallelization, although it has a few more moving parts. If we want to
    avoid creating excessive `Observable` instances, maybe we should split the source
    `Observable` into a fixed number of Observables where emissions are evenly divided
    and distributed through each one. Then, we can parallelize and merge them with
    `flatMap()`. Even better, since I have eight cores on my computer, maybe it would
    be ideal that I have eight Observables for eight streams of calculations.
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve this using a `groupBy()` trick. If I have eight cores, I want
    to key each emission to a number in the range 0 through 7\. This will yield me
    eight `GroupedObservables` that cleanly divide the emissions into eight streams.
    More specifically,  I want to cycle through these eight numbers and assign them
    as a key to each emission. `GroupedObservables` are not necessarily impacted by
    `subscribeOn()` (it will emit on the source's thread with the exception of the
    cached emissions), so I will need to use `observeOn()` to parallelize them instead.
    I can also use an `io()` or `newThread()` scheduler since I have already constrained
    the number of workers to the number of cores, simply by constraining the number
    of `GroupedObservables`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how I do this, but instead of hardcoding for eight cores, I dynamically
    query the number of cores available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output (yours will be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: For each emission, I will need to increment the number it groups on, and after
    it reaches `7`, it will start over at `0`. This ensures that the emissions are
    distributed as evenly as possible. We achieve this using `AtomicInteger` with
    a modulus operation. If we keep incrementing `AtomicInteger` for each emission,
    we can divide that result by the numbers of cores, but return the remainder, which
    will always be a number between `0` and `7`.
  prefs: []
  type: TYPE_NORMAL
- en: '`AtomicInteger` is just an integer protected inside a `threadsafe` container
    and has convenient `threadsafe` methods, such as `incrementAndGet()`. Typically,
    when you have an object or state existing outside an `Observable` chain but is
    modified by the `Observable` chain''s operations (this is known as creating side
    effects), that object should be made `threadsafe`, especially when concurrency
    is involved. You can learn more about `AtomicInteger` and other utilities in Brian
    Goetz''s *Java Concurrency in Practice*.'
  prefs: []
  type: TYPE_NORMAL
- en: You do not have to use the processor count to control how many `GroupedObservables`
    are created. You can specify any number if you, for some reason, deem that more
    workers would result in better performance. If your concurrent operations are
    a mix between IO and computation, and you find that there is more IO, you might
    benefit from increasing the number of threads/`GroupedObservables` allowed.
  prefs: []
  type: TYPE_NORMAL
- en: unsubscribeOn()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last concurrency operator that we need to cover is  `unsubscribeOn()`. When
    you dispose an `Observable`, sometimes, that can be an expensive operation depending
    on the nature of the source. For instance, if your `Observable` is emitting the
    results of a database query using RxJava-JDBC, ([https://github.com/davidmoten/rxjava-jdbc](https://github.com/davidmoten/rxjava-jdbc))
    it can be expensive to stop and dispose that `Observable` because it needs to
    shut down the JDBC resources it is using.
  prefs: []
  type: TYPE_NORMAL
- en: This can cause the thread that calls `dispose()` to become busy, as it will
    be doing all the work stopping an `Observable` subscription and disposing it.
    If this is a UI thread in JavaFX or Android (for instance, because a CANCEL PROCESSING button
    was clicked), this can cause undesirable UI freezing because the UI thread is
    working to stop and dispose the `Observable` operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple `Observable` that is emitting every one second. We stop the
    main thread for three seconds, and then it will call `dispose()` to shut the operation
    down. Let''s use `doOnDispose()` (which will be executed by the disposing thread)
    to see that the main thread is indeed disposing of the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add `unsubscribeOn()` and specify to unsubscribe on `Schedulers.io()`.
    You should put `unsubscribeOn()` wherever you want all operations upstream to
    be affected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now you will see that disposal is being done by the IO `Scheduler`, whose thread
    is identified by the name `RxCachedThreadScheduler-1`.  This allows the main thread
    to kick off disposal and continue without waiting for it to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Like any concurrency operators, you really should not need to use `unsubscribeOn()`
    for lightweight operations such as this example, as it adds unnecessary overhead.
    But if you have `Observable` operations that are heavy with resources which are
    slow to dispose of, `unsubscribeOn()` can be a crucial tool if threads calling
    `dispose()` are sensitive to high workloads.
  prefs: []
  type: TYPE_NORMAL
- en: You can use multiple `unsubscribeOn()` calls if you want to target specific
    parts of the `Observable` chain to be disposed of with different Schedulers. Everything
    upstream to an `unsubscribeOn()` will be disposed of with that `Scheduler` until
    another `unsubscribeOn()` is encountered, which will own the next upstream segment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was probably our most intense chapter yet, but it provides a turning point
    in your proficiency as an RxJava developer as well as a master of concurrency!
    We covered the different Schedulers available in RxJava as well as ones available
    in other libraries such as RxJavaFX and RxAndroid. The `subscribeOn()` operator
    is used to suggest to the upstream in an `Observable` chain which `Scheduler`
    to push emissions on.  The `observeOn()`will switch emissions to a different `Scheduler`
    *at that point* in the `Observable` chain and use that `Scheduler` downstream.
    You can use these two operators in conjunction with `flatMap()` to create powerful
    parallelization patterns so you can fully utilize your multi-CPU power. We finally
    covered `unsubscribeOn()`, which helps us specify a different `Scheduler` to dispose
    operations on, preventing subtle hang-ups on threads we want to keep free and
    available even if they call the `dispose()` method.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that when you start playing with concurrency, you need
    to become wary of how much data you are juggling between threads now. A lot of
    data can queue up in your `Observable` chain, and some threads will produce work
    faster than other threads can consume them. When you are dealing with 10,000+
    elements, you will want to use Flowables to prevent memory issues, and we will
    cover this in [Chapter 8](14efb9e9-14a6-41ba-86cb-20b5674dce8e.xhtml), *Flowables
    and Backpressure*.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will look into this topic of dealing with Observables that
    produce emissions too quickly, and there are some operators that can help with
    this without backpressure. We will hit that next.
  prefs: []
  type: TYPE_NORMAL
