- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Architecting a Real-Time Processing Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to architect a big data solution for
    a high-volume batch-based data engineering problem. Then, we learned how big data
    can be profiled using Glue DataBrew. Finally, we learned how to logically choose
    between various technologies to build a Spark-based complete big data solution
    in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to analyze, design, and implement a real-time
    data analytics solution to solve a business problem. We will learn how the reliability
    and speed of processing can be achieved with the help of distributed messaging
    systems such as Apache Kafka to stream and process the data. Here, we will discuss
    how to write a Kafka Streams application to process and analyze streamed data
    and store the results of a real-time processing engine in a NoSQL database such
    as MongoDB, DynamoDB, or DocumentDB using Kafka connectors.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to build a real-time streaming
    solution to predict the risk category of a loan application using Java and Kafka-related
    technologies. You will also know how a real-time data analytics problem is designed
    and architected. Throughout this journey, you will learn how to publish events
    to Kafka, analyze that data using Kafka Streams, and store the result of the analytics
    in MongoDB in real time. By doing so, you will know how to approach a real-time
    data engineering problem and build an effective streaming solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and analyzing the streaming problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing and verifying the design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this chapter, you’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prior knowledge of Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java 1.8 or above, Maven, Apache Kafka, and PostgreSQL installed on your local
    system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A MongoDB Atlas subscription in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ IDEA Community or Ultimate edition installed on your local system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and analyzing the streaming problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at data engineering problems that involve ingesting,
    storing, or analyzing the stored data. However, in today’s competitive business
    world, online web apps and mobile applications have made consumers more demanding
    and less patient. As a result, businesses must adapt and make decisions in real
    time. We will be trying to solve such a real-time decision-making problem in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A financial firm, XYZ, that offers credit cards, has a credit card application
    that works in real time and uses various user interfaces such as mobile and online
    web applications. Since customers have multiple options and are less patient,
    XYZ wants to make sure that the credit loan officer can decide on credit card
    approval in a split second or in real time. To do that, the application needs
    to be analyzed and a credit risk score needs to be generated for each application.
    This risk score, along with the necessary application parameters, will help the
    credit loan officer decide quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s analyze the given problem. First, let’s analyze the requirements in terms
    of the four dimensions of data.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will try to answer the question, *what is the velocity of the data?*
    This is the most important factor of this problem. As evident from the problem
    statement, unlike our previous problems, source data is being received in real
    time and the data analysis also needs to happen in real time. This kind of problem
    is well suited for a real-time streaming solution.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the next dimension that we need to discuss is *volume*. However, since
    our problem involves streaming data, it doesn’t make sense to discuss the total
    volume of the data. Rather, we should be answering questions such as, *how many
    applications are submitted every minute or every hour on average, as well as at
    peak times? Will this volume increase in the future? If it does, how many times
    and how frequently it is likely to increase?* We should go back to the client
    with these questions. Often, in a business, these answers are not readily available
    if the client is creating a real-time pipeline for the first time. In such a scenario,
    we should ask for the most granular average data velocity information (in this
    case, the number of applications filed) available with the client – for each day,
    week, or month and then calculate the average expected volume in a minute. Also,
    to understand the increase in volume, we can ask about the target projections
    as far as sales are concerned over a year and try to predict the volume increase.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s suppose that the client is getting one million applications per day and
    that their target is to increase sales by 50% over the next 2 years. Considering
    that the usual approval rate is 50%, we can expect a two times increase in the
    application submission rate. This would mean that we could expect a volume of
    2 million applications per day in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our solution needs to be real-time, must process more than a million
    records, and the volume is likely to increase in the future, the following characteristics
    are essential for our streaming solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Should be robust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should support asynchronous communication between various systems within the
    solution and external source/sink
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should ensure zero data loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should be fault tolerant as we are processing data in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should be scalable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should give great performance, even if the volume increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping all these factors in mind, we should choose a pub-sub messaging system
    as this can ensure scalability, fault tolerance, higher parallelism, and message
    delivery guarantees. Distributed messaging/streaming platforms such as Apache
    Kafka, AWS Kinesis, and Apache Pulsar are best suited to solve our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will be focusing on the *variety* of the data. In a typical streaming
    platform, we receive data as events. Each event generally contains one record,
    though sometimes, it may contain multiple records. Usually, these events are transmitted
    in platform-independent data formats such as JSON and Avro. In our use case, we
    will receive the data in JSON format. In an actual production scenario, there’s
    a chance that the data may be in Avro format.
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges that real-time streaming solutions face is the *veracity*
    of the data. Often, veracity is determined based on the various possibilities
    of noise that can come from the data. However, accurate analysis of the veracity
    happens as a real-time project gets implemented and tests are run with real data.
    As with many software engineering solutions, real-time data engineering solutions
    mature over time to handle noise and exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, we will assume that our data that is getting published
    in the input topic is already clean, so we won’t discuss veracity in our current
    use case. However, in the real world, the data that is received over the input
    topic contains anomalies and noise, which needs to be taken care of. In such cases,
    we can write a Kafka Streams application to clean and format the data and put
    it in a processing topic. Also, erroneous records are moved to the error topic
    from the input topic; they are not sent to the processing topic. Then, the streaming
    app for data analytics consumes the data from the processing topic (which contains
    clean data only).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have analyzed the dimensions of data for this problem and have concluded
    that we need to build a real-time streaming pipeline, our next question will be,
    *which platform? Cloud or on-premise?*
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, let’s look at any constraints that we have. To analyze
    the streaming data, we must pull and read each customer’s credit history record.
    However, since the credit history of a customer is sensitive information, we would
    prefer to use that information from an on-premise application. However, the company’s
    mobile or web backend systems are deployed on the cloud. So, it makes sense to
    store the analyzed data on the cloud since it will take less time for the mobile
    or other web applications to fetch the data from the cloud than from on-premise.
    So, in this case, we will go with a hybrid approach, in which credit history data
    will be stored on-premise and the data will be analyzed and processed on-premise,
    but the resultant data will be stored in the cloud so that it can easily be retrieved
    from mobile and web backend systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we analyzed the data engineering problem and realized that
    this is a real-time stream processing problem, where the processing will happen
    on-premise. In the next section, we will use the result of this analysis and connect
    the dots to design the data pipeline and choose the correct technology stack.
  prefs: []
  type: TYPE_NORMAL
- en: Architecting the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To architect the solution, let’s summarize the analysis we discussed in the
    previous section. Here are the conclusions we can make:'
  prefs: []
  type: TYPE_NORMAL
- en: This is a real-time data engineering problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This problem can be solved using a streaming platform such as Kafka or Kinesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 million events will be published daily, with a chance of the volume of events
    increasing over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution should be hosted on a hybrid platform, where data processing and
    analysis are done on-premise and the results are stored in the cloud for easy
    retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since our streaming platform is on-premise and can be maintained on on-premise
    servers, Apache Kafka is a great choice. It supports a distributed, fault-tolerant,
    robust, and reliable architecture. It can be easily scaled by increasing the number
    of partitions and provides an at-least-once delivery guarantee (which ensures
    that at least one copy of all events will be delivered without event drops).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we will determine how the results and other information will
    be stored. In this use case, the credit history of an individual has a structured
    format and should be stored on-premise. RDBMS is a great option for such data
    storage. Here, we will be using PostgreSQL for this because PostgreSQL is open
    source, enterprise-ready, robust, reliable, and high performing (and also because
    we used it as an RDBMS option in [*Chapter 4*](B17084_04.xhtml#_idTextAnchor062),
    *ETL Data Load – A Batch-Based Solution to Ingesting Data in a Data Warehouse*).
    Unlike credit history, the applications need to be accessed by mobile and web
    backends running on AWS, so the data storage should be on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Also, let’s consider that this data will primarily be consumed by mobile and
    web backend applications. So, would it be worth storing the data in a document
    format that can be readily pulled and used by the web and mobile backends? MongoDB
    Atlas on AWS cloud is a great option for storing documents in a scalable way and
    has a pay-as-you-go model. We will use MongoDB Atlas on AWS as the sink of the
    resultant data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss how we will process the data in real time. The data will
    be sent as events to a Kafka topic. We will write a streaming application to process
    and write the result event on an output topic. The resulting record will contain
    the risk score as well. To dump the data from Kafka to any other data store or
    database, we can either write a consumer application or use Kafka Sink connectors.
    Writing a Kafka consumer app requires development and maintenance effort. However,
    if we choose to use Kafka Connect, we have to just configure it to get the benefits
    of a Kafka consumer. Kafka Connect is faster to deliver, easier to maintain, and
    more robust as all exception handling and edge cases are already taken care of
    and well-documented. So, we will use a Kafka Sink connector to save the result
    events from the output topic to the MongoDB Atlas database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram describes the solution architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Solution architecture for our real-time credit risk analyzer
    ](img/B17084_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Solution architecture for our real-time credit risk analyzer
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, our solution architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A new application event gets published in the input Kafka topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka Streams application – the Risk Calculator app – reads the application
    event and fetches the corresponding credit history of the applicant from the credit
    history database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Risk Calculator app creates and sends an HTTP request to the Risk Score
    Generator app with all the required parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Risk Score Generator app uses the already trained ML models to calculate
    the risk score of the application and returns the result to the Risk Calculator
    app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Risk Calculator app generates the enriched application event and writes
    the resultant event in the output topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kafka Sink connector, which is configured on the output topic, is responsible
    for consuming and writing the data to the MongoDB Atlas cloud database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a processing error during Kafka streaming, an error message, along
    with the input event, will be written in the error database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have learned how to architect a solution for our real-time data
    analysis needs, let’s learn how to implement the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and verifying the design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in a real-time implementation like this is to set up the streaming
    platform. To implement our architecture, we need to install Apache Kafka and create
    the necessary topics on our local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Apache Kafka on your local machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to set up an Apache Kafka cluster, run
    it, and create and list topics. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Apache Kafka version 2.8.1 from [https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz](https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the `kafka_2.12-2.8.1.tgz` archive file. The following command will
    help you do the same on Linux or macOS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate to the Kafka installation root directory and start zookeeper using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, run the Kafka server using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create the topics using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For the sake of simplicity, we have defined one partition and set the replication
    factor as 1\. But in a real production environment, the replication factor should
    be three or more. The number of partitions is based on the volume and velocity
    of data that needs to be processed and the optimum speed at which they should
    be processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can list the topics that we created in the cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have installed Apache Kafka and created the topics that we need
    for the solution, you can focus on creating the credit records table and error
    table in a PostgreSQL instance installed on your local machine. The DDL and DML
    statements for these tables are available at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/SQL](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/SQL).
  prefs: []
  type: TYPE_NORMAL
- en: Reference notes
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are new to Kafka, I recommend learning the basics by reading the official
    Kafka documentation: [https://kafka.apache.org/documentation/#gettingStarted](https://kafka.apache.org/documentation/#gettingStarted).
    Alternatively, you can refer to the book *Kafka, The Definitive Guide*, by *Neha
    Narkhede*, *Gwen Sharipa*, and *Todd Palino*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we set up the Kafka streaming platform and the credit record
    database. In the next section, we will learn how to implement the Kafka streaming
    application to process the application event that reaches *landingTopic1* in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the Kafka streaming application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we implement the solution, let’s explore and understand a few basic concepts
    about Kafka Streams. Kafka Streams provides a client library for processing and
    analyzing data on the fly and sending the processed result into a sink (preferably
    an output topic).
  prefs: []
  type: TYPE_NORMAL
- en: 'A stream is an abstraction that represents unbound, continuously updating data
    in Kafka Streams. A stream processing application is a program written using the
    Kafka Streams library to process data that is present in the stream. It defines
    processing logic using a topology. A Kafka Streams topology is a graph that consists
    of stream processors as nodes and streams as edges. The following diagram shows
    an example topology for Kafka Streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Sample Kafka Streams topology ](img/B17084_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Sample Kafka Streams topology
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, a topology consists of **Stream Processors** – these are nodes
    and edges that represent streams. There can be two kinds of special stream processor
    nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source Processor**: This is a special stream processing node that produces
    an input stream of data from consuming messages from one or multiple Kafka topics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sink Processor**: As the name suggests, a sink processor consumes data from
    upstream and writes it to a sink or target topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A topology in a Kafka streaming application can be built using a low-level Processor
    API or using high-level **Domain-Specific Language** (**DSL**) APIs. When an event
    is published to a source Kafka topic, the topology gets triggered, which processes
    the event using the topology definition and publishes the processed event to the
    Sink topic. Once a topology is successfully invoked and completed on a source
    event, the event offset is committed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our use case, the Kafka Streams application will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For the application event received, find the credit history from the credit
    record database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the ML request body using the data received from Kafka and the data pulled
    out from the credit record database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a REST call to the Risk Score Generator application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Form the final output record
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send the final output record to a sink topic using a Sink processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First and foremost, we need to create a Spring Boot Maven project and add the
    required Maven dependencies. The following Spring Maven dependencies should be
    added to the `pom.xml` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from this, as we are planning to develop a Kafka streaming application,
    we also need to add Kafka-related Maven dependencies, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s write the `main` class, where we will initialize the Kafka Spring
    Boot application. However, in our application, we must exclude `KafkaAutoConfiguration`
    (as we intend to use our own property names for Kafka-related fields and not Spring
    Boot’s default Kafka property names), as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the `main` class, we will create the main `KafkaStreamConfiguration`
    class, where all streaming beans will be defined and instantiated. This is where
    we will use Kafka Streams DSL to build the topology. This class must be annotated
    with `@EnableKafka` and `@EnableKafkaStreams`, as shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create the `KafkaStreamsConfiguration` bean. The following code
    snippet shows the implementation of the `KafkaStreamsConfiguration` bean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: While creating the `KafkaStreamsConfiguration` bean, we must pass all Kafka
    streaming-related properties. Here, it is mandatory to set `StreamsConfig.APPLICATION_ID`
    and `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG`. In this case, `StreamsConfig.APPLICATION_ID`
    corresponds to the consumer group ID of the Kafka Streams application, while `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG`
    corresponds to the Kafka broker address. Without these values, no Kafka streaming
    or consumer application can run or connect to the Kafka cluster. Kafka Streams
    applications can distribute the traffic coming from a topic within a consumer
    group among multiple consumers that share the same consumer group ID. By increasing
    the running instance of the streaming application while using the same ID, we
    can have more parallelism and better throughput. However, increasing the number
    of instances beyond the number of partitions in the Kafka topic will not have
    any effect on the throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have created the `KafkaStreamsConfiguration` bean, let’s create
    `KStream`. While creating this `KStream` bean, we must define the topology. The
    following code creates the `KStream` bean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Each message in a Kafka topic consists of a key and a value. The value contains
    the actual message, while the key helps determine the partition while the message
    is published. However, when we consume the message using streams, we must mention
    the type of key and value that we are expecting. In our case, we are expecting
    both the key and value to be `String`. So, the `KStream` bean is created as an
    instance of `KStream<String,String>`. First, we must create a stream using the
    `StreamsBuilder` class, which is part of the Kafka Streams API. In our use case,
    the topology is built as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, using the `StreamsBuilder` API, input streams are created from `inputTopic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transform processor is applied to the resultant input stream using the `transform()`
    DSL function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A custom Transformer called `RiskCalculatorTransformer` is used to transform/process
    the data coming from the input stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The processed output event is written to `outputTopic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s learn how to write a custom Transformer for a Kafka Streams application.
    In our scenario, we have created `RiskCalculatorTransformer`. The following discussion
    explains how to develop a custom Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must create a class that implements the `org.apache.kafka.streams.kstream.Transformer`
    interface. It has three methods – `init`, `transform`, and `close` – that need
    to be implemented. The following code shows the definition of the `Transformer`
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `Transformer` interface expects three generic types – `K`,
    `V`, and `R`. `K` specifies the data type of the key of the message, `V` specifies
    the data type of the value of the message, and `R` specifies the data type of
    the result of the message. While `init` and `close` are only used when some pre
    or post-processing is needed before the message is processed, `transform` is a
    mandatory method that defines the actual transformation or processing logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our use case, we receive the value of the message as a JSON string, process
    it, add the risk score, and send out the resultant value as a JSON string. The
    data type of the key remains unchanged. Hence, we send out a `KeyValue` pair object
    as a result. Our final `Transformer` outline looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, our Transformer is expecting the key and value
    of the message to be of the `String` type, and it returns a `KeyValue` pair where
    both the key and value are of the `String` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our Transformer, we don’t need any pre or post-processing. So, let’s move
    on and discuss how to implement the `transform` method of our `Transformer`. The
    code of the `transform` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the step-by-step guide for implementing our `transform` method:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we deserialize the incoming value, which is a JSON string, into a POJO
    called `ApplicationEvent` using the Jackson `ObjectMapper` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initiate a JDBC call to the credit record database using Spring’s `JdbcTemplate`.
    While forming the SQL, we use the application ID that was deserialized in the
    previous step. We get a list of the `CreditRecord` objects because of the JDBC
    call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we form the request body for the HTTP REST call that we are going to make
    to get the risk score. Here, we populate an `MLRequest` object using the `ApplicationEvent`
    object (deserialized earlier) and the list of `CreditRecord` objects we obtained
    in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we wrap the `MLRequest` object in an `HTTPEntity` object and make the
    REST call using the Spring `RestTemplate` API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We deserialize the REST response to the `RiskScoreResponse` object. The model
    of the `RiskScoreResponse` object looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the REST response is `OK`, then we form the `EnrichedApplication` object
    using the `ApplicationEvent` and `RiskScoreResponse` objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we create and return a new `KeyValue` pair object, where the key is
    unchanged, but the value is the serialized string of the `EnrichedApplication`
    object we created in *step 6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For exception handling, we log any errors as well as send the error events to
    an error database for future analysis, reporting, and reconciliation. The reporting
    and reconciliation processes won’t be covered here and are usually done by some
    kind of batch programming.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we learned how to develop a Kafka Streams application from
    scratch. However, we should be able to successfully unit test a streaming application
    to make sure that our intended functionalities are working fine. In the next section,
    we will learn how to unit test a Kafka Streams application.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing a Kafka Streams application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To unit test a Kafka Streams application, we must add the Kafka Streams test
    utility dependencies to the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, before we do a JUnit test, we need to refactor our code a little bit.
    We have to break the definition of the `KStream` bean into two methods, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we took out the `KStream` formation code, put
    it in a utility method in a singleton class called `StreamBuilder`, and used the
    `Bean` method as a wrapper on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to write the JUnit test case. First, our transformation
    requires a JDBC call and a REST call. To do so, we need to mock the JDBC call.
    To do that, we will use Mockito libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can mock our `JdbcTemplate` call like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a mock `JdbcTemplate` object using the `@Mock` annotation.
    Then, we use Mockito’s `when().thenReturn()` API to define a mock output for a
    call made using the mock `JdbcTemplate` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar technique can be used to mock `RestTemplate`. The code for mocking
    `RestTemplate` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, first, we mock `RestTemplate` using the `@Mock` annotation.
    Then, using Mockito APIs, we mock any `POST` call that returns a `RiskScoreResponse`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s form the topology. You can use the following code to create the
    topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created an instance of the `org.apache.kafka.streams.StreamsBuilder`
    class. Using our `StreamBuilder` utility class, we defined the topology by calling
    the `getkStream` method. Finally, we built the topology by calling the `build()`
    method of the `org.apache.kafka.streams.StreamsBuilder` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka Stream’s test utils come with a `Utility` class called `TopologyTestDriver`.
    `TopologyTestDriver` is created by passing the topology and config details. Once
    `TopologyTestDriver` has been created, it helps to create `TestInputTopic` and
    `TestOutputTopic`. The following code describes how to instantiate a `TopologyTestDriver`
    and create `TestInputTopic` and `TestOutputTopic`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a `TestInputTopic`, we need to specify the name of the topic, as
    well as the key and value serializers. Similarly, `TestOutputTopic` requires key
    and value deserializers, along with the output topic name. We can push a test
    event to `TestInputTopic` using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can assert our expected result with the actual result using the
    `org.junit.Assert.assertEquals` static method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run this JUnit test by right-clicking and running the `Test` class,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Running a Kafka Streams JUnit test case ](img/B17084_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Running a Kafka Streams JUnit test case
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have run the JUnit test case, you will see the test result in the
    run window of IntelliJ IDE, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Verifying the JUnit test’s results ](img/B17084_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Verifying the JUnit test’s results
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to write a JUnit test case for a Kafka streaming
    application and unit test our Streams application. In the next section, we will
    learn how to configure the streaming application and run the application on our
    local system.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and running the application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run this application, we must configure the `application.yaml` file, which
    contains the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: Application port number (as we will launch two Spring Boot applications on our
    local machine)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data source details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka details such as bootstrap servers and topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The REST HTTP URL for the Risk Score Generator app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our sample `application.yaml` file will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can run the application by running the main class, `CreditRiskCalculatorApp`,
    of the CreditRiskCalculator application. But before we start the CreditRiskCalculator
    app, we should run the RiskScoreGenerator app by running its main class – that
    is, `RiskScoreGenerator`. Both these applications are Spring Boot applications;
    please refer to the *Implementing and unit testing the solution* section of [*Chapter
    4*](B17084_04.xhtml#_idTextAnchor062), *ETL Data Load – A Batch-Based Solution
    to Ingesting Data in a Data Warehouse*, to learn how to run a Spring Boot application.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting tips
  prefs: []
  type: TYPE_NORMAL
- en: If, while starting the CreditRiskCalculator application, you notice a warning
    message such as **Connection to node -1 (localhost/127.0.0.1:9092) could not be
    established. Broker may not be available.** in the logs, please ensure your Kafka
    server is reachable and running.
  prefs: []
  type: TYPE_NORMAL
- en: If you notice an exception such as `max.poll.interval.ms` or decrease the value
    of `max.poll.records`. This usually happens when the number of records polled
    takes more time to process than the maximum poll interval time configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you encounter an error such as **java.lang.IllegalArgumentException: Assigned
    partition**'
  prefs: []
  type: TYPE_NORMAL
- en: '`application.id`. Change your `application.id` to solve this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to create and unit test a Kafka Streams application.
    The source code for this application is available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/CreditRiskCalculator](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/CreditRiskCalculator).
  prefs: []
  type: TYPE_NORMAL
- en: Since implementing an ML-based Risk Score Generator app is outside the scope
    of this book, we have created a Spring Boot REST application that generates a
    dummy risk score between 1 to 100\. The code base of this dummy application is
    available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/RiskScoreGenerator](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/RiskScoreGenerator).
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world scenario, ML-based applications are more likely to be written
    in Python than Java since Python has better support for AI/ML libraries. However,
    a Kafka Streams application will be able to make a REST call and get the generated
    risk score from that application, as shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been receiving the event from an input Kafka topic, processing
    it on the fly, generating a risk score, and writing the enriched event to an output
    Kafka topic. In the next section, we will learn how to integrate Kafka with MongoDB
    and stream the events to MongoDB as soon as they are published to the output Kafka
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a MongoDB Atlas cloud instance and database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will create a cloud-based instance using MongoDB Atlas.
    Follow these steps to set up the MongoDB cloud instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign up for a MongoDB Atlas account if you haven’t done so already ([https://www.mongodb.com/atlas/database](https://www.mongodb.com/atlas/database)).
    While signing up, you will be asked for the type of subscription that you need.
    For this exercise, you can choose the shared subscription, which is free, and
    choose an AWS cluster as your preferred choice of cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following screen. Here, click the **Build a Database** button
    to create a new database instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – MongoDB Atlas welcome screen ](img/B17084_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – MongoDB Atlas welcome screen
  prefs: []
  type: TYPE_NORMAL
- en: 'To provision a new database, we will be asked to set a username and a password,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Provisioning a new database instance ](img/B17084_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Provisioning a new database instance
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will be asked to enter all the IP addresses that we want to grant
    access to the MongoDB instance. Here, since we will run our application from our
    local system, we will add our local IP address to the IP Access List. Then, click
    **Finish and Close**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Setting up an IP Access List during database provisioning ](img/B17084_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Setting up an IP Access List during database provisioning
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the cluster has been created, we will see the cluster on the dashboard,
    as shown in the following screenshot. Now, click the **Browse Collections** button
    to see the collections and data in this database instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Cluster dashboard in MongoDB Atlas ](img/B17084_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Cluster dashboard in MongoDB Atlas
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, currently, there are no collections or
    data. However, you can create collections or data manually while using this interface
    by clicking the **Add My Own Data** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Exploring collections and data in the MongoDB database instance
    ](img/B17084_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Exploring collections and data in the MongoDB database instance
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to create a cloud-based MongoDB instance using
    the online interface of MongoDB Atlas. In the next section, we will learn how
    to configure and deploy our MongoDB Kafka connectors to send the data from Kafka
    to MongoDB in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Kafka Connect to store the results in MongoDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka Connect is an open source, pluggable data integration framework for Kafka.
    It enables data sources and data sinks to easily connect with Kafka. Instead of
    writing cumbersome code to publish a message from the data source or consume a
    message from Kafka to write into a data sink, Kafka Connect provides declarative
    configuration to connect to a data source or sink.
  prefs: []
  type: TYPE_NORMAL
- en: A Kafka Connect cluster already ships with a few types of connectors, such as
    `FileSourceConnector`. However, we can install any available connectors by placing
    them in the `plugins` folder. For our use case, we will deploy the MongoDB connector
    plugin (discussed later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: A Kafka Connect instance can be deployed and run in either cluster or standalone
    mode. However, in production, it usually runs in cluster mode. When we run in
    cluster mode, we can register the Kafka connector configuration using Kafka Connects’
    REST API. In standalone mode, we can register a connector configuration while
    starting the Kafka Connect instance.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are running our Kafka cluster on our local machine, we will deploy
    our Kafka Connect instance in standalone mode for this implementation. But remember,
    if you are implementing for production purposes, you should run Kafka, as well
    as Kafka Connect, in a clustered environment (this can be a physical cluster or
    a virtual cluster, such as a virtual machine, AWS ECS, or Docker container).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s set up the Kafka Connect cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a new folder called `plugins` under the Kafka root installation
    folder, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Creating the plugins folder ](img/B17084_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Creating the plugins folder
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, navigate to `connect-standalone.properties`, which is present in the
    `<Kafka-root>/config` folder. Add the following property to the `connect-standalone.properties`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, download the MongoDB Kafka connector plugin from [https://www.confluent.io/hub/mongodb/kafka-connect-mongodb](https://www.confluent.io/hub/mongodb/kafka-connect-mongodb).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A ZIP file will be downloaded. Copy and extract the ZIP file in the `plugin`
    folder created under the Kafka root installation folder. At this point, the folder
    structure should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Kafka folder structure after deploying the mongo-kafka-connect
    plugin ](img/B17084_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Kafka folder structure after deploying the mongo-kafka-connect
    plugin
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to create and deploy a Kafka Connect worker configuration
    to create a pipeline between a Kafka topic and MongoDB sink.
  prefs: []
  type: TYPE_NORMAL
- en: 'To write a Kafka Connect worker, we must understand the various types of declarative
    properties that Kafka Connect supports. The following diagram depicts various
    kinds of components that a Kafka Connect worker consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Kafka Connect components ](img/B17084_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Kafka Connect components
  prefs: []
  type: TYPE_NORMAL
- en: 'A Kafka connector consists of three types of components. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Connector**: This interfaces Kafka with external data sources. It takes care
    of implementing whatever external protocol those data sources and sinks need to
    communicate with Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Converter**: Converters are used to serialize and deserialize events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: This is an optional property. It is a stateless function that’s
    used to slightly transform the data so that it is in the right format for the
    destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our use case, we don’t need a transformer, but we do need to set all the
    properties related to the connector and converter. The following code is for the
    Kafka Sink Connect worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding configuration code, the connector properties such
    as `connector.class` and other properties specific to MongoDB are configured,
    and the converter properties such as `key.converter` and `value.converter` are
    set. Next, in the sink connector configuration, we define all the MongoDB connection
    properties, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will set the `document.id` and `writemodel.strategy` properties in
    the sink connector configuration, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Save this configurations in a property file called `connect-riskcalc-mongodb-sink.properties`
    and place it in Kafka Connect’s `config` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run the Kafka Connect instance in standalone mode and start the
    `mongodb-sink` connector using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s learn how to troubleshoot possible issues that we may encounter.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting the Kafka Sink connector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When a source or a sink connector runs on a Kafka Connect cluster, you may
    encounter multiple issues. The following list specifies a few common issues and
    how to resolve them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you encounter an error similar to the following, then please check whether
    the JSON message should contain a schema or not. If the JSON message should not
    contain schema, make sure that you set the `key.converter.schemas.enable` and
    `value.converter.schemas.enable` properties to `false`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you encounter an error such as `org.apache.kafka.common.errors.SerializationException:
    Error deserializing Avro message for id -1`, then check if the payload is Avro
    or JSON. If the message is a JSON payload instead of Avro, please change the value
    of the `value.converter` property in the connector to `org.apache.kafka.connect.json.JsonConverter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You may encounter `BulkWriteExceptions` while writing to MongoDB. `BulkWriteExceptions`
    can be a `WriteError`, a `WriteConcernError`, or a `WriteSkippedError` (due to
    an earlier record failing in the ordered bulk write). Although we cannot prevent
    such errors, we can set the following parameters to move the rejected message
    to an error topic called `dead-letter-queue`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we successfully created, deployed, and ran the MongoDB Kafka
    Sink connector. In the next section, we will discuss how to test the end-to-end
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test the end-to-end pipeline, we must make sure that all the services, such
    as Kafka, PostgreSQL, Kafka Connect, and the MongoDB instance, are up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from that, the Kafka Streams application and the Risk Score Generator
    REST application should be up and running. We can start these applications by
    running the main Spring Boot application class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the application, open a new Terminal and navigate to the Kafka root
    installation folder. To start an instance of the Kafka console producer, use the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can publish input messages by using the console producer, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Publishing messages using the Kafka console producer ](img/B17084_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Publishing messages using the Kafka console producer
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as we publish the message in the input topic, it gets processed, and
    the result is written to the MongoDB instance. You can verify the results in MongoDB
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Verifying the results ](img/B17084_06_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Verifying the results
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to test the end-to-end solution for a real-time
    data processing problem and verify the result. Now, let’s summarize what we learned
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to analyze a real-time data engineering problem,
    identify the streaming platform, and considered the basic characteristics that
    our solution must have to become an effective real-time solution. First, we learned
    how to choose a hybrid platform to suit legal needs as well as performance and
    cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned how to use our conclusions from our problem analysis to build
    a robust, reliable, and effective real-time data engineering solution. After that,
    we learned how to install and run Apache Kafka on our local machine and create
    topics in that Kafka cluster. We also learned how to develop a Kafka Streams application
    to do stream processing and write the result to an output topic. Then, we learned
    how to unit test a Kafka Streams application to make the code more robust and
    defect-free. After that, we learned how to set up a MongoDB Atlas instance on
    the AWS cloud. Finally, we learned about Kafka Connect and how to configure and
    use a Kafka MongoDB Sink connector to send the processed event from the output
    topic to the MongoDB cluster. While doing so, we learned how to test and verify
    the real-time data engineering solution that we developed.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have learned how to develop optimized and cost-effective solutions
    for both batch-based and real-time data engineering problems. In the next chapter,
    we will learn about the various architectural patterns that are commonly used
    in data ingestion or analytics problems.
  prefs: []
  type: TYPE_NORMAL
