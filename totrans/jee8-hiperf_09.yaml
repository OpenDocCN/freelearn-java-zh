- en: Benchmarking Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how to develop a Java EE application to ensure
    it could scale later using multiple threads, asynchronous handling, pooled resources,
    and so on. We also saw how to get metrics on the performance and resource (CPU,
    memory) usage of your application and optimize the performance thanks to JVM or
    container tuning, as well as more aggressive techniques such as adding caching
    to your application.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should be able to work on the performance. However it does
    not mean you are safe to get surprises when going into production. The main reason
    is that the work we talked about previously is rarely done in an environment close
    enough to the production or final environment the application will be deployed
    to.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid these surprises, benchmarks can (or should) be organized, but it is
    not as easy as taking all we previously learned and putting it all together. It,
    most of the time, requires more preparation that you should be aware of, if you
    don't want to lose precious time when you go to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will prepare a benchmark, going through the following
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: What a benchmark is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a benchmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating during a benchmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to do after a benchmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking – validating your application against your SLA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmarks often enter into play when you have a **Service Level Agreement**
    (**SLA**) to respect. The SLA can be more or less explicit. Typically, you may
    have a very blurry definition of your SLA, such as *the application must provide
    good user experience*, or you may have them in a very precise manner in a contract,
    such as *the application must support Black Friday weekend and 10 million users
    a day, and each user action must be executed in less than one second*. There are
    even some standards to describe the SLA, such as the **Web Service Level Agreement**
    (**WSLA**) to define how to measure and expose your SLA.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, if an SLA is identified, and even more so if you have some compensation
    in your contract if it is not met, it is very important to go through a benchmark
    phase in your project to make sure you increase your performance when going to
    production.
  prefs: []
  type: TYPE_NORMAL
- en: The next and last chapter of the book will deal with the continuous evaluation
    of your performance and will help you to do it continuously and avoid this *phase* effect.
    Although, it is still common to have a dedicated phase because of the infrastructure
    constraints required by a benchmark, so we will consider it the case in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you know that you need to validate the performance of your application
    and your project manager, or you, has planned a benchmark. But what is this task
    about?
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark, benchmark, benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working on performance is not uniform work. We saw that in the previous section;
    there are lots of tools doing that and each of them gives more or less information,
    but also has more or less impact on the actual performance. For instance, instrumenting
    all the code of the application to get metrics on all layers will make the application
    very slow, but the report very rich. On the contrary, instrumenting only some
    parts—such as the outbounds—will not impact the application that much, yet the
    report will give you only a very small set of data. This means that depending
    on the layer you work on, you will not use the same tools to ensure you have the
    right level of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can distinguish multiple potential benchmark types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *algorithm benchmark*: You develop some code sections and want to validate
    the performance is correct or there is no bottleneck.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *layer benchmark*: You develop a layer—the persistence layer, front layer,
    and so on—and want to ensure the performance is correct before adding another
    layer or integrating it with another part of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *sizing* benchmark: You get the figures of the application performance
    to identify the number of machines to use. This is directly related to horizontal
    scaling—this doesn''t work as smoothly as for a vertical one since the performance
    can''t be linear. Note that this is exactly the same kind of logic big data frameworks
    are based on to distribute their work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *deliverable benchmark*: This is the benchmark validating that the application
    (delivery) and the performance of the final application matches expectations (SLA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, we can split the sort of benchmarks you can do into more precise
    categories, but these are the three you will encounter in most projects. Each
    kind of benchmark will use different tools and will have different preparation
    steps and output. However, each of them will validate criteria (one or more) against
    expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous benchmarks, we can clearly split the criteria into two very high-level
    categories, but this split will have a huge impact on the way you drive your benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: The *development benchmark*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *deliverable benchmark*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if a benchmark is done at delivery, by definition, this split means that
    the two first categories of benchmarks we identified are about validating the
    code is correctly done, and therefore it belongs to developer work in general,
    and it is rarely split from the development itself. The *layer benchmark* is generally
    done during multiple development iterations; it stays a development benchmark
    as it is still about validating an application internally, and not something exposed
    to the end user normally. The *deliverable* benchmark is about ensuring final
    performance is acceptable for the end user (or contract). It is therefore different
    from the previous categories of benchmarks because you need to have a deliverable
    complete enough to be tested.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of implications, the fact you will work on a deliverable benchmark
    mainly means you will not be able to do it on *your machine*. What you want is
    to validate your performance against a contract, so you will need to validate
    the application on the machine it is installed on.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is important not to get confused between a benchmark to validate
    the SLA and a benchmark to size the infrastructure needed for the application.
    Both will almost look the same and are organized the same way. But in the case
    of a *sizing benchmark*, you will define an infrastructure (machine power, memory,
    and so on) and measure performance to then deduce how many machines you need if
    you horizontally scale. However, the *SLA benchmark* already assumes the infrastructure
    is fixed and then you just validate the performance to encounter the SLA. In practice,
    both are often done at the same time which leads to this confusion between both
    types of benchmarks. This mainly comes from the fact that developers or project
    managers have an idea of the infrastructure needed for an application, so the
    starting infrastructure for sizing is close to the target one, and then the game
    is only to validate the performance to match the expectations. Nonetheless, if
    you start a sizing benchmark then you will need another benchmark *phase* to validate
    the SLA, which can be seen as a second benchmark. Never forget the phase you are
    in; otherwise, you may change too many parameters at the same time and lose track
    of the current state of the application (it is crucial to be able to compare benchmark
    iterations, as we will see later).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing your benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing a benchmark is probably the most important task you will have to do.
    In fact, if you miss it, it is guaranteed the benchmark will be a failure and
    useless. Even if tasks are not very complicated in general, they will not be done
    by themselves. So take your time to ensure they are done before the benchmark
    starts.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the benchmark criteria
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A benchmark is always done to make sure we encounter a metric. Therefore, the
    first step of benchmark preparation is to *clearly* define this metric.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a metric means clearly defining what is measured and how to measure
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining what is measured means to define the bounds of the measurement. In
    other words, when the metric starts and when the metric ends. This can sound simple
    to do, but don't forget we work in a multi-layer environment and that you can
    miss some layers if your monitoring is not well defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples, based on our quote-manager application, where not defining
    the bounds of the metric well enough can lead to incorrectly validating the application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring an endpoint execution duration with a CDI interceptor: You miss the
    JAX-RS layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Measuring an endpoint execution duration with a JAX-RS filter: You miss the
    servlet layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Measuring an endpoint execution duration with a servlet filter if the metric
    is the processing time of the request: You miss the container processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These examples are all server-side mistakes but illustrate the fact that being
    explicit about the metrics is not as trivial as it may seem, since the three mentioned
    solutions are easy and also very tempting ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a case that is worse: the client side. When the metric is a client-side
    metric—often the case for an SLA, since in that case we generally don''t care
    about what the server does if it is fast for the clients—then the measurement
    definition is very important. The client case implies some infrastructure you
    don''t always control. Thus, ensuring the definition is well done will avoid ambiguities
    and potential disagreements with customers or reviewers of the benchmark. Here
    are some examples of different interpretations of the same metric:'
  prefs: []
  type: TYPE_NORMAL
- en: The client execution time is measured from a client connected to the application
    server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client execution time is measured from a client connected to the load balancer
    in front of the application servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client execution time is measured from a client connected to an API gateway
    that redirects the call to a load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client execution time is measured from a client connected to a proxy in
    another **Wide Area Network** (**WAN**) that routes the request to an API gateway
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these lines adds an infrastructure layer on top of the previous one,
    and thus, adds some latency for the client. They all measure the *client execution
    time*. This is why precisely defining the infrastructure, and moreover how the
    metric is designed, is very important, before starting to benchmark the application.
  prefs: []
  type: TYPE_NORMAL
- en: Defining acceptance criteria
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have a metric clearly defined, you need to define the criteria based
    on that metric that will make the benchmark validated or rejected—is your application
    fast enough to rephrase it at a high level?
  prefs: []
  type: TYPE_NORMAL
- en: Generally, it is a number that can be expressed as a time unit or percentage,
    depending on the metric. If the measure is lower (or higher) than this number
    then the benchmark is rejected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, the metric is not self-sufficient and needs some additional
    parameters to be able to define the acceptance criteria in a measurable way. Here
    are some common examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The *client execution duration* must be under 1 second for *64 concurrent users*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *client latency* must be under 250 milliseconds when *128 messages per second* are
    received
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *insertion rate of the data into the database* must be higher than 1,500
    records per second for *two connections*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these examples, the bold expression is the metric we build our criteria on,
    and the italic one is another potential metric fixed in the context of the defined
    criteria (the underlined number).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it is possible to use more than two metrics in the same criteria
    and even to make them all vary at the same time. However, this leads to complicated
    acceptance criteria, and it is generally always possible to rephrase them based
    on acceptance criteria that are using constants. Don't hesitate to rebuild a criteria
    database from the ones you get in inputs, to ensure they are easy to validate
    and measure. A very simple example of this sort of rephrasing can be represented
    by changing *the client execution duration must be under 1 second for a number
    of concurrent users between 1 and 64* into *the client execution duration must
    be under 1 second for 64 concurrent users*. This change is not strictly equivalent
    and you will need to validate the first statement, but the second phrase is easier
    to work with, in particular, if you need some tuning. It is worth using this simpler
    one to start work and to get a rough estimate of your metrics and then, once it
    passes, just validate the original one.
  prefs: []
  type: TYPE_NORMAL
- en: 'One criteria, which was not mentioned before, is the *time*. Generally, all
    criteria are defined for an *infinite* duration. This means you will need to make
    sure that once they are reached they are respected for *long enough* to assume
    it will not be degraded after some time. This is something to take into account
    when you prepare your tooling, as lots of factors can degrade the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: A database that slows down after a certain number of records are inserted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cache that is wrongly sized and starts being too big for its configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A badly tuned memory, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these factors will not always prevent you from reaching your performance
    in a *short* period of time, but they will likely degrade the performance after
    some duration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea here will be to ensure you can associate the acceptance criteria with
    some environment metrics, such as the memory behavior. For instance, you can associate
    the acceptance of your criteria with memory usage and/or a garbage collector profile,
    such as the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3d7ce8f-72ae-4c05-b887-e43cf053d961.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Assuming the *X* axis is the time and the *Y* axis the memory used, then this
    profile shows that the garbage collection is regular (almost each vertical line)
    and that the memory usage is bounded and regular since it doesn't go over the
    red line representing the maximum, even after a few cycles.
  prefs: []
  type: TYPE_NORMAL
- en: This sort of definition is rarely self-sufficient as it implicitly defines that
    this validation happens when the application has already reached the criteria
    we measure, and that *some time* has passed. Although, it is better than just
    measuring the criteria and not validating that the result is true for a long-running
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the best case is to be able to test the application for days, but it
    is generally costly and not doable. If you can't do it, using this kind of strategy
    and high-level validation is generally a good fallback.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining the scenario is linked to the criteria but removes the constant constraint.
    This allows you to define more complex cases where all metrics can vary at the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common example is to make the user (client) number a variable moving with
    the time: *the response time will be constant from 5 users to 1,000 users with
    an increment of 5 users every 10 seconds*.'
  prefs: []
  type: TYPE_NORMAL
- en: A scenario is generally very close to the actual application usage but also
    harder to work on if you don't encounter it immediately because you are no longer
    running the application under a constant load. This is why they are seen more
    as validation checkpoints than work criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you know what you want to test, you will need to set up your application
    *somewhere* to be able to validate it and potentially optimize it.
  prefs: []
  type: TYPE_NORMAL
- en: It may sound obvious but, here, you have to be very strict on that point and
    benchmark your application in an environment comparable to your final environment.
    What does it mean? The same machine, same network setup, same load balancing strategy,
    same backends/databases, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Any part of the setup must match where you will run. Otherwise, when deploying
    in production, you may be surprised by some unexpected factors you should have
    seen coming and evaluated far before this final deployment. The best case is that
    the application is not functional, which is generally identified by some smoke
    tests done after the deployment. The worse case is that the application is functional,
    but its scalability or performance are affected by an unexpected factor. We rarely
    run performance tests in a production environment. Thus, limiting the potential
    error factors due to the environment is very important.
  prefs: []
  type: TYPE_NORMAL
- en: The machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before installing any application, you need a machine. From what we just said,
    the machine must be close to the final one, but what does it mean in the machine's
    context?
  prefs: []
  type: TYPE_NORMAL
- en: 'The machine is often seen as its resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CPU: The computing power the application can use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The memory: The space the application can use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disk space: The local storage the application can rely upon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your first choice will be to pick the same CPU/memory/disk as the production
    machine. Yet make sure, before going this way, that the machine is not shared
    with other services (like another application), which can completely deserve the
    1-1 choice in terms of resources (CPU, memory, disk, ...) because the resources
    would be consumed by the other application. That is to say, if the application
    is sharing its resources with other software, you will need to find a way to either
    estimate the available resources for your application and limit them to this amount,
    or isolate both applications to guarantee each of them will have a well-defined
    set of resources.
  prefs: []
  type: TYPE_NORMAL
- en: If you rely on Docker/Kubernetes for deployments, these recommendations apply
    as well, except they are no longer at *machine* level but *pod* level. Also, make
    sure your JVM is configured to support the pod (or container) settings that require
    some JVM tuning to use cgroup configuration instead of the whole machine setup—the
    Java default.
  prefs: []
  type: TYPE_NORMAL
- en: The network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The network is, nowadays, a very important factor in a deployment. If your application
    is self-sufficient, it is not very crucial, but this is almost never the case.
    All applications have either an HTTP layer (through a UI or web services), a (remote)
    database, or are remotely connected to other applications. This is becoming even
    more crucial in a microservices architecture where some libraries are even designed
    to handle that part more specifically (with fallbacks, bulhead, and concurrency,
    as we saw in previous chapters).
  prefs: []
  type: TYPE_NORMAL
- en: In this context, it is very important to be able to rely on a good network.
    In the same spirit as for the machine selection, you must choose a network comparable
    to the production network. Assuming the material is almost the same, this means
    that you will select networks with the same throughput, but this is not enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with a network, there are two other criteria to take into account
    very quickly to avoid surprises:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *distance* between the machines/hosts: If remote services are *far* then
    the latency will be increased and the code relying on these services will be *slower*.
    Ensuring you benchmark in conditions close to the production ones—the same latency
    and response times—is very important to be able to rely on the figures you obtain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The network usage: If the network is used a lot by other applications, the
    bandwidth available for your *new* application will be reduced, and the performance
    will be very quickly impacted. A common error in a benchmark is to have a network
    dedicated to the benchmark, whereas in production it is shared with some other
    applications. Ensuring you get a consistent setup here will avoid big surprises
    during your deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases and remote services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your application uses a remote service, which can be a classical **relational
    database management system** database (**RDBMS**), a NoSQL database, or another
    application, it is important to make sure you are benchmarking under realistic
    conditions. Concretely, if we take back our quote-manager application, which uses
    an RDBMS database, we should not test with local MySQL if our production database
    will be an Oracle instance. The idea is to get as close to the reality as possible—the
    latency our production environment will get.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases you (or your company) will own the other services/databases and
    can tune them to make them scale more. But in some other cases, you use external
    services you can't optimize, such as CBOE and Yahoo! Finance, in the quote-manager
    application. In any case, it will always be saner to come to the other node (service/database)
    manager to ask to make it faster. Realizing you are slow in production because
    you don't have the same setup as during the benchmark will slow you down and impact
    you more.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn't mean that *mocking* an external service is stupid. It can be very
    handy during the optimization phase of your own application, since it will make
    the external service interaction as fast as is potentially feasible. However,
    you must ensure you remove the mock when doing your *validation* benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: If you enable the application to be configured to use mocks or fast alternative
    systems, don't forget to write a log message (in the INFO or WARNING levels) during
    startup to ensure you can find this information later. It can save you a lot of
    time and avoid you re-running a benchmark because you are not sure of the *actual*
    running setup.
  prefs: []
  type: TYPE_NORMAL
- en: During the benchmark, in particular the tuning phase, you will likely configure
    your pools (connection pools). Thus, it is important to ensure you can rely on
    the database (or service) scalability. The goal is to avoid successfully passing
    a benchmark with a pool of 1,024 connections and realizing you can only use 20
    connections in production (20 being the maximum number of connections your database
    accepts).
  prefs: []
  type: TYPE_NORMAL
- en: More than the database/service type, more than the version, more than the environment
    (OS, machine), you need to make sure the configuration of the database is copied
    from the production instance (or, if you are in the tuning phase, that the final
    configuration you used can be copied to the production instance).
  prefs: []
  type: TYPE_NORMAL
- en: The server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are talking about a Java EE application—but it could be generalized to any
    application if we talk about *packaging—*so we deploy the application into a server.
    Even embedded applications are packaging (*bundling*) a server in their deliverable.
    As with all the previous points, it is important to align it with the target system:
    the production environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, it means that you shouldn't test against a WildFly server if your
    production environment is using Apache TomEE or GlassFish.
  prefs: []
  type: TYPE_NORMAL
- en: A server is not far from your application if we talk about the way it is developed
    and packaged. This means that it embeds several libraries selected by the server
    vendor. The direct implication is that a server version embeds several library
    versions. Since Java EE is between ~15 and ~30 specifications, it is at least
    as important as libraries packed together. Because it is software and you can't
    avoid some changes between versions—particularly during the early stages of a
    new specification—you should try to ensure you are using not only the same server
    as in production but also the same version.
  prefs: []
  type: TYPE_NORMAL
- en: This statement should be extended to all the code that is *outside* of your
    application. It can include your JDBC drivers, directly deployed in the container,
    or even some infrastructure/operation team services.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have picked your server, you need to ensure you use a setup (configuration)
    close enough to the production one. This includes the logging configuration you
    will need (typically, if you use a log aggregator, you may need a specific log
    format) and the resources deployed to it. If you auto-deploy resources from an
    infrastructure service, ensure you deploy them all to have the same thread usage,
    network usage (if it implies remote resources, such as JMS), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Finally (and linked to machine selection), ensure the setup is consistent with
    the production one. If you log on a **Solid State Drive** (**SSD**) disk in production,
    ensure you log on an SSD during your benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Automatize your scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have your scenarios, you can just describe them and manually execute
    them for simple ones you can script or code directly without much effort. But
    most of the time, you will need to automate them. This is typically the case for
    load testing scenarios. The advantage of automating them is that you can run them
    on demand (*in one click*), and thus, it is easy to test and retest them without
    a huge investment.
  prefs: []
  type: TYPE_NORMAL
- en: There are several tools to automate the scenarios, and they mainly depend on
    the scenario you need to test. We will go through some mainstream ones you can
    use if you don't know where to start.
  prefs: []
  type: TYPE_NORMAL
- en: JMeter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache JMeter ([http://jmeter.apache.org/](http://jmeter.apache.org/)) is a
    historical solution to load test an application. It supports several modes and
    is fully written in Java, which makes it easy to integrate and use for most Java
    developers. It supports main *connections* used by applications:'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP/HTTPS, SOAP/REST for JavaEE, NodeJs, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JDBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JMS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is immediately interesting for you is that you will be able to test your
    Java EE application but also your other backends, and thus can compare the performance
    (of the database and application, for instance) to potentially be able to report
    that the database is the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'It provides a nice UI, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d124e64e-8f7a-44c2-bb2f-da11c842251d.png)'
  prefs: []
  type: TYPE_IMG
- en: This interface is designed for building your test plans (scenarios); it allows
    you to create it without any configuration or deep knowledge of the tool. If you
    start the software from a command line, you will even have a warning message saying
    not to use it for actual load testing and to use the **command line interface**
    (**CLI**) for real runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, once you have started JMeter, you will build a *Test Plan* composed of
    steps. It will allow you to configure the threads and the way the number of *clients* is
    defined, add some assertions (validations of the output) to the scenario, and
    reuse variables between steps (for instance, a first step can get an OAuth2 token
    used to authenticate the next request or even handle the warm-up of your testing).
    In the elements you can add to the plan, there are some reports allowing you to
    get the figures you expect as output from a benchmark, such as the percentage
    of error, the min/max duration, the throughput, KB/sec, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/016d88b5-0a47-4a0a-b33c-383b7aef4c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: This screenshot represents the *aggregated report* of JMeter which contains
    statistics about the plan execution—or a subpart of it. What is interesting here
    is the error rate (100% in the previous example) which allows you also to validate
    that the execution was *good enough,* that is, there were not too many errors
    saying we didn't test anything.
  prefs: []
  type: TYPE_NORMAL
- en: Once your plan is defined, you can save it in a `.jmx` file (JMeter default
    extension), which will allow you to replay it. At that point, you should be able
    to *locally* test your scenario (changing the URL of the plan a bit to adjust
    it to your local instance), but you can't yet test a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, for real load testing, you will need to use the *remote testing* solution
    of JMeter. It will allow you to orchestrate multiple client nodes (often called
    *injectors* since they will *inject* requests into the system). The big advantages
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: You don't rely on your local machine anymore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You control which networks are used by the client (it can be the same as the
    server or not)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can horizontally scale, using *N* client machines instead of one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last point is very important because of the network usage. When doing HTTP
    requests, you will use the machine network, and one of the most limiting criteria
    will be the number of clients per nodes. The more clients you have, the slower
    they will globally be as they add noise for other clients. That is to say, before
    launching a full run, make sure to size your injector correctly to establish how
    many clients you can use per injector node, without being limited by the infrastructure.
    You will rarely have tons of clients for a single machine in a real deployment.
    Thus, it is acceptable to have only one or two clients per machine, in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to download JMeter, you can go to its download page ([http://jmeter.apache.org/download_jmeter.cgi](http://jmeter.apache.org/download_jmeter.cgi))
    on the Apache website.
  prefs: []
  type: TYPE_NORMAL
- en: Gatling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gatling ([https://gatling.io/](https://gatling.io/)) is an alternative to JMeter.
    You will find the same features as in JMeter (of course, there are some differences,
    but we will not list them here). The main difference is that you script your scenarios instead
    of configuring them, either in an XML file, or visually in a nice UI.
  prefs: []
  type: TYPE_NORMAL
- en: The scripting is based on a **Domain Specific Language** (**DSL**) and relies
    on the Scala language. This can sound like a **blocker** for a Java developer,
    since Scala is not very friendly if you have never done any Scala development.
    However, it is the strength of Gatling compared to JMeter; it is an Akka-Netty-based
    load testing solution. This means it is coded with technologies trying to be lock-free
    in their own backbone and enabling the injector code to scale. JMeter was known
    to be self-limiting in the way it was designed if you were requesting it to scale
    to too many users. In reality, this is not a huge limitation since, as we saw
    in the previous section, you will often also scale in terms of infrastructure
    to test your application reliably. Yet, it is interesting in development and in
    some highly scaling applications as you will not need so many machines to reach
    the same level of scalability of the injector.
  prefs: []
  type: TYPE_NORMAL
- en: This is often a point we forget during a benchmark, and this is why it is important
    to prepare it before; to ensure the injector does not throttle the benchmark.
    Otherwise, you are testing the client/injector instead of the server/application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to give you an idea, here is a simple Gatling script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This simple script defines a scenario named `QuotesScenario`*.* It will request
    our `findAll` quote endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you put this script in `$GATLING_HOME/user-files/simulations/packt/QuotesSimulation.scala`,
    be careful, as Scala uses the concept of packages as in Java, so you need the
    right nested folder compared to the `simulations` folder. Then you can run `$GATLING_HOME/bin/gatling.sh`,
    which will scan and compile the files inside the previous folder to find the simulations
    and ask you to select the one you want to launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `computerdatabase` simulations are the default ones; our simulation is the
    last one. Once selected, Gatling requests some metadata about the simulation,
    such as its `id`and `description`.
  prefs: []
  type: TYPE_NORMAL
- en: The first time you launch Gatling, the startup can be lengthy as it will compile
    the simulation—there are some samples with the default distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the simulation runs, you will get some progress in the console (whereas
    with JMeter, you were able to get it in the UI for your tests and see the reports
    in real time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: These small reports show the progress of the test. We can identify that we are
    at 84% of the simulation we configured, representing the 54/64 requests (users)
    we requested and that 50 seconds has elapsed already.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the test is finished, a small report is generated in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This report contains the statistics about the execution and the response time
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Gatling generates an HTML report (by default). Its location is logged
    at the very end of the program, just before it exits. You can open it with any
    browser and here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35dc2f5e-70b5-48d1-a37f-e52a97f32304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Back on the report, we find the statistics (on the left, in orange) and some
    detailed indicators in the center of the page. Between them, you have the number
    of requests, the response time distribution (allowing you to see if the response
    time is in an acceptable range or if the response time is constant enough for
    your target users), and so on. You can see, in the previous screenshot, that there
    are two tabs: GLOBAL and DETAILS. The DETAILS tab has this small menu on the left
    (with quotes on our screenshot), allowing you to drill down the details, per step,
    of the simulation/scenario. The quotes references the name we gave to the `http`
    request we defined in our simulation.'
  prefs: []
  type: TYPE_NORMAL
- en: Gatling has a lot more features and ways to compose scenarios, and since it
    is code, it is also quite flexible. This is not the topic of the book, but don't
    hesitate to have a deeper look.
  prefs: []
  type: TYPE_NORMAL
- en: Again, don't forget the injector machines (the machines where you put the processes
    simulating the clients, that is, the Gatling process here) may not be powerful
    enough or may have not enough bandwidth to scale very highly. For that reason,
    you will need to distribute your injectors across several machines to reach the
    right amount of users in general.
  prefs: []
  type: TYPE_NORMAL
- en: Gatling, like JMeter, supports this mode even if it requires more work. The
    procedure is explained on their website ([https://gatling.io/docs/2.3/cookbook/scaling_out/](https://gatling.io/docs/2.3/cookbook/scaling_out/)),
    but at a high level you will run the simulation on several nodes then grab all
    their outputs and aggregate them post-execution.
  prefs: []
  type: TYPE_NORMAL
- en: Other tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many other tools you can use to define your scenarios; even some DIY
    solutions can be used. In all cases, you should ensure it scales well enough to
    not limit your benchmark since you want to test your application and not your
    benchmark tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring you have support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you start benchmarking, you will probably encounter some issues, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: A network setup not correctly done
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bug in a framework/a library/your application/the injector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A remote service or database not absorbing enough load, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all potential cases you may encounter—given any *brick* of your software
    can have an issue—you should be able to have somebody you can call to help you
    fix the issue, or at least evaluate it quickly. This is particularly crucial if
    part of the benchmark costs *some* money (if you are renting some machines, consulting,
    and so on). The idea here is to be able to get rid of any blocker as fast as possible
    to not waste time on details.
  prefs: []
  type: TYPE_NORMAL
- en: Time boxing your optimization benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An evaluation benchmark is time boxed by design; you run the benchmark and report
    the figures. Although, an optimization benchmark is more blurry. Concretely, you
    can spend a whole year optimizing a simple web service just because of the layers
    it uses and all the small tuning options you can test, from the network configuration,
    through the JVM memory, to the caching solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it is crucial before starting to benchmark an application and optimizing
    it to define how long you will spend benchmarking your application. It can also
    be linked to a renting period and may require an estimation phase to work with
    the operation and development teams. But if you don't do it, the risk is that
    you will spend a lot of time on the details and not make the best of the benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Even if it is a high-level approximation, the Pareto principle can be used here
    to try to optimize the benchmark time. Concretely, try to do 20% of the optimization,
    which will give you 80% of the boost for your application. Then, if you have time,
    you can work on the remaining 20%.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and resetting procedures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It may sound obvious, but before starting the benchmark, you must know how to
    properly install your application and inter connect it with other systems (databases,
    other applications, and so on). This part should be written down in a document
    to make sure it is easy to find when needed, and that it has been tested at least
    once before the benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: The part we forget more often is the reset part, and it would be ideal if this
    part is automatized as well in the scenarios. This is mainly about ensuring each
    execution is repeatable, and executions are comparable.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark iterations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we prepared all we needed to start a benchmark in an
    efficient way. Now we need to see how we will work during the benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: The first important thing is to establish that we only deal with optimization
    iterations here and not evaluation ones, which are straightforward—you run the
    scenarios and gather the reports.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is probably natural, but you will iterate with your optimizations. This means
    that you will run the same test again and again to measure the result of a change—for
    instance, increasing the pool size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The direct implication of such a work structure is that you need to prepare
    yourself to store lots of reports in an organized way. There are many solutions
    for that and it mainly depends on the tools you are used to relying on. But at
    a very high level, you need to, at least, store:'
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark report.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benchmark date (to be able to sort them, it is often useful to replay the
    iterations done afterwards).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benchmark configuration (you can store the full configuration or just write
    it in a file, named `CHANGES.txt`, for instance, where you list what you changed
    from the previous run). Note that it is important here to include the changes
    of external systems—such as databases—since they can directly impact your performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of storage, you can just use a `benchmark`folder on your hard drive
    and create a folder per iteration containing the previous information. The folder
    name can contain the date. A common pattern is `<iteration number>_<date>_<short
    description>`, for instance `001_2017-11-14_increasing-pool-size`. Using a number
    (padded with *0)* will allow you to use your operating system sorting to sort
    the folder. The date gives you another entry point—when somebody tells you *yesterday,
    it was working better*, for instance. Finally, the small description allows you
    to more easily identify the reports to compare them.
  prefs: []
  type: TYPE_NORMAL
- en: It is not mandatory, but if you have a small tool (like a script or a small
    Java program) parsing the reports and configuration to store them in an index,
    you can more easily find the data and you will get a more powerful search. In
    the same way, if you already did the work to parse the data, you can easily implement
    a small `diff` tool to compare two reports, which will allow you to show the configuration
    changes and the impact on the performance—the reports. Generally, the reports
    are visual. Thus, being able to merge two reports allows you to compare them more
    efficiently (visually) than using two windows.
  prefs: []
  type: TYPE_NORMAL
- en: Changing a single setting at a time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While you are tuning the application, it is important to identify if a setting
    is the factor enhancing the performance or not, and thus identify it as important
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: If you change multiple settings for a single run, you will not be able to say
    which setting triggered a change, or you can even neutralize a good setting by
    using another bad one and missing an optimization factor.
  prefs: []
  type: TYPE_NORMAL
- en: Resist the temptation to change everything at the same time, and try to keep
    a *scientific* approach, changing a single setting at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Resetting any state between runs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw, in the previous section, that you must prepare as much data as the production
    database will work with, but also don't forget to reset the database state between
    each run.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't do it, the risk is you will slow down the execution between each
    run and make the optimizations you do completely invisible. This is because databases
    have a sort of size limit (quite huge), but while you benchmark, you will generally
    insert a lot of data very quickly so it wouldn't be surprising to reach that limit.
    Once you reach this size limit, the database is less efficient and performance
    degrades. To ensure you can compare the runs and validate some tuning options,
    you must run in the same conditions. So, you should ensure the database has the
    same data between each run.
  prefs: []
  type: TYPE_NORMAL
- en: This explanation used the database as the main illustration because it is the
    most common pitfall, but it is true for any part of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Warming up your system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another crucial step to doing a benchmark is to not measure the data on a *cold* system.
    The reason is that a Java EE application generally intends to be long-running
    software; for that reason, it is common to have a *hot* system which already got
    optimizations after having ran during weeks or hours. These optimizations can
    be in action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **JVM** **Just-In-Time** (**JIT**) compilation: This will optimize some
    common code paths. You can also investigate the `-XX:-TieredCompilation` option
    of the JVM to *pre-compile* the code, but you can encounter some issues with it
    on some servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use some caching and therefore the application will be faster once the
    cache has all the data you test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use some external system you may need to do some expensive connections
    you will reuse later on (SSL connections are slow, secured connections are slow,
    and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having some warm-up iterations before the actual measures start is very important
    to hide all these initializations and just measure the *final* performance of
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: After your benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have done your benchmark, you should have a *database* of your runs
    (the folder we talked about earlier with the reports, configuration, and so on).
    Now, to ensure you did the benchmark for a reason, there are few actions you should
    take.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we assume that you will do these steps after the benchmark,
    but you can do them during the benchmark itself. It is presented this way because
    it is something you can do *offline*, and if you have some costs associated with
    the benchmark, these are tasks you can postpone easily.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, at this point, you have collected all the data corresponding to the
    hard work you did during the benchmark. It is quite important to aggregate this
    in a report. The report will mainly explain the investigation (why you changed
    some settings and so on) and expose the results of the runs.
  prefs: []
  type: TYPE_NORMAL
- en: You can, of course, ignore the useless runs (no significant change in the performance),
    but it is always interesting to integrate the ones corresponding to a performance
    boost or degradation.
  prefs: []
  type: TYPE_NORMAL
- en: The last part of the report should explain how to properly configure the server
    for production. It can be done inline in the report or point to another document
    such as a reference guide if it is about a product or a white book for an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a high-level structure for a report:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Application description: What the application does.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metrics: If you have some not so obvious or specific metrics, explain them
    here (before the next part).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scenarios: What your test did.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Infrastructure/environment: How you deployed machines and external systems,
    how you set up the monitoring, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Injection: How you stimulated the application (you can explain that you had
    *N* JMeter nodes, for instance).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runs: All the relevant iterations you did and their results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conclusions: What do you keep from the benchmark? Which configuration should
    be used? You can also mention some tests you didn''t get time to run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating your default configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even if, as shown in the previous section, the *final* configuration is part
    of the report, it will not prevent you from propagating all the good practices
    you deduced from the benchmark in the code base. The goal is to reduce the mandatory
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you identified that you need a timeout of 1 second instead
    of the default 30 seconds to have a good performance, updating your defaults to
    1 second directly in the code base will avoid having a bad performance if the
    configuration is forgotten. This part is a trade-off between default usability
    and performance, but generally you can still enhance the default user/operation
    team experience by doing it.
  prefs: []
  type: TYPE_NORMAL
- en: If you have some provisioning recipes or a Dockerfile, don't forget to update
    them as well, if relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on your conclusions, you may need to cross-check, with developers
    or other members, that the outcome of the benchmark is valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you deduced on our quote-manager that you needed to cache
    the quotes, then you may desire to validate:'
  prefs: []
  type: TYPE_NORMAL
- en: If it is OK to cache them business-wise (you can check it with your product
    owner or manager)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long you can cache them for, as you will probably want some updates on the
    prices at some point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another common example is to validate that you can bypass or change the way
    you secured some part of the application because the security layer was too slow
    (switching from OAuth2 to HTTP signature, or some authentication mechanism to
    network security, for instance).
  prefs: []
  type: TYPE_NORMAL
- en: Once the conclusions are validated, you can also extract the part of the report
    related to the original SLA and make them validated by your customers, or the
    people you report to.
  prefs: []
  type: TYPE_NORMAL
- en: Enriching your project backlog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, you may have identified some issues in the code. They may or
    may not impact the performance, but in any case you need to create corresponding
    tasks to fix them upstream.
  prefs: []
  type: TYPE_NORMAL
- en: If you used some hotfix or patch during the benchmark, don't forget to mention
    it and reference it inside the report to let people track whether it is actually
    fixed or not. Note that it can also be related to external libraries or containers
    and not only your application.
  prefs: []
  type: TYPE_NORMAL
- en: The more you work across teams, the more this phase is important. Otherwise,
    you get a report where the SLA is reached, and a product is never able to respect
    that because the enhancements are never integrated into the mainstream source
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that a benchmark is something you need to prepare before
    ensuring you can benefit the most from the benchmark time, and that it requires
    some organization. We also saw that to be useful you need to extract, from the
    work done, the conclusions it implies. This is really a scientific procedure—but
    an easy one—and you need to respect it if you want to optimize your time.
  prefs: []
  type: TYPE_NORMAL
- en: The next and last chapter will go one step further and look at how to reduce
    the distance between the development and the benchmark to reach a continuous performance
    evaluation, making your benchmark no longer harmful, since everything is already
    prepared and under control.
  prefs: []
  type: TYPE_NORMAL
