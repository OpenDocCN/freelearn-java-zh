<html><head></head><body>
		<div id="_idContainer069">
			<h1 class="chapter-number" id="_idParaDest-107"><a id="_idTextAnchor175"/>10</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor176"/>Architecting Distributed Systems – Challenges and Anti-Patterns</h1>
			<p>In today’s digital landscape, the demand for scalable and reliable systems has led to the widespread adoption of distributed systems. These complex networks of interconnected components are designed to handle large-scale data processing, storage, and communication across multiple machines or nodes. However, architecting distributed systems comes with a unique set of challenges <span class="No-Break">and pitfalls.</span></p>
			<p>Building distributed systems aims to achieve high availability, fault tolerance, and better performance and scalability while distributing the workload across multiple nodes. However, the complexity of these systems often gives rise to various challenges that architects and developers must overcome. From ensuring data consistency and synchronization to managing network latency and optimizing performance, numerous factors should be considered when designing a <span class="No-Break">distributed system.</span></p>
			<p>One of the critical challenges in architecting distributed systems is achieving proper data consistency. Maintaining the integrity and coherence of data across different nodes is crucial, but it becomes increasingly challenging as the system scales. Ensuring that all replicas of a given piece of data are updated correctly and simultaneously poses a significant challenge and often requires implementing complex <span class="No-Break">synchronization mechanisms.</span></p>
			<p>Another challenge lies in managing network latency and communication overhead. In a distributed system, nodes communicate with each other over a network, and the time taken for messages to traverse the network can introduce delays and bottlenecks. Architects must carefully design communication protocols and choose appropriate network technologies to minimize latency and maximize <span class="No-Break">system performance.</span></p>
			<p>Scalability is a critical consideration when architecting distributed systems. As the demand for resources and processing power grows, the system should scale horizontally by adding more nodes seamlessly. Achieving this scalability while maintaining performance and avoiding bottlenecks is a complex task that requires careful planning and <span class="No-Break">architectural decisions.</span></p>
			<p>Despite these challenges, architects must also be aware of common anti-patterns that can undermine the effectiveness and reliability of distributed systems. Anti-patterns are recurring design or implementation practices that are considered suboptimal or counterproductive. These can include network congestion, single points of failure, improper load balancing, or overreliance on a central coordinator. Recognizing and avoiding these anti-patterns is crucial to ensuring the successful operation of <span class="No-Break">distributed systems.</span></p>
			<p>In this chapter, we will explore the pitfalls of modern architecture when we talk about <span class="No-Break">distributed systems:</span></p>
			<ul>
				<li>Data integration scales and <span class="No-Break">distributed transactions</span></li>
				<li>The <span class="No-Break">dual-write anti-pattern</span></li>
				<li>Microservices and <span class="No-Break">shared databases</span></li>
				<li>Eventual <span class="No-Break">consistency problems</span></li>
			</ul>
			<p>We will delve into the challenges architects face when designing distributed systems and explore common anti-patterns that can arise during the process. By understanding these challenges and avoiding the pitfalls, architects and developers can create robust and efficient distributed systems that meet the demands of modern applications. Through best practices and practical insights, we aim to equip you with the knowledge and tools to architect distributed systems and mitigate potential <span class="No-Break">risks effectively.</span></p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor177"/>Data integration scales and distributed transactions</h1>
			<p>Data integration is critical to building distributed systems, where disparate data sources must be harmonized and accessible to various system components. As the scale of data and the number of distributed nodes increase, the challenges associated with data integration becomes more challenging. One key consideration in this context is the coordination of <span class="No-Break">distributed transactions.</span></p>
			<p>Maintaining data integrity is crucial for distributed transactions and ensuring that the system behaves as if it’s executing a single transaction on a centralized database. Distributed transactions refer to related database operations that must be executed atomically across multiple nodes. In a distributed system, where data is spread across different nodes, ensuring consistency and isolation across these operations <span class="No-Break">becomes complex.</span></p>
			<p>The following figure shows data being integrated into two services, each with a database. At this point, orchestration is required to guarantee data consistency <span class="No-Break">and security:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="Figure 10.1: Transaction workflow in a distributed system" src="image/Figure_10.01_B19375.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Transaction workflow in a distributed system</p>
			<p>However, achieving distributed transactional consistency at scale poses significant challenges. Traditional <strong class="bold">atomicity, consistency, isolation, and durability</strong> (<strong class="bold">ACID</strong>) properties, typically guaranteed in a centralized database, become harder to enforce across distributed nodes due to network latency, node failures, and <span class="No-Break">concurrency issues.</span></p>
			<p>One approach to addressing these challenges is to use distributed transaction protocols such as <strong class="bold">two-phase commit</strong> (<strong class="bold">2PC</strong>) or <strong class="bold">three-phase commit</strong> (<strong class="bold">3PC</strong>). These protocols coordinate the commit or rollback decisions across multiple nodes in a distributed transaction. However, these protocols have limitations, including increased latency and failure vulnerability if a coordinator node becomes unavailable. The following diagram shows a sequence <span class="No-Break">of 2PCs:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="Figure 10.2: 2PC illustration" src="image/Figure_10.02_B19375.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2: 2PC illustration</p>
			<p>Another approach is to adopt a more relaxed consistency model, such as eventual consistency or optimistic concurrency control. These models trade off strict consistency guarantees for increased scalability and availability. These models can perform better when real-time consistency is not strictly required by allowing temporary inconsistencies and resolving <span class="No-Break">conflicts asynchronously.</span></p>
			<p>Furthermore, distributed data integration often involves dealing with heterogeneous data sources with varying schemas and formats. Data transformation and mapping become crucial to ensure that data from different sources can be effectively combined and processed and often come with a performance cost. To create a consistent view of a distributed system, you can use methods such as <strong class="bold">extract</strong>, <strong class="bold">transform</strong>, and <strong class="bold">load</strong> (<strong class="bold">ETL</strong>) or data virtualization to combine data from <span class="No-Break">various sources.</span></p>
			<p>Distributed transactional systems require careful design decisions to balance the trade-offs between consistency, scalability, and performance. When designing and architecting data integration at scale, it is essential to consider data consistency requirements, latency, fault tolerance, and performance factors. Understanding the characteristics and limitations of different transactional models and adopting appropriate data integration techniques can help architects and developers tackle the complexities associated with distributed data integration and ensure the reliability and efficiency of <span class="No-Break">their systems.</span></p>
			<p>In summary, data integration at scale in distributed systems requires addressing the challenges of distributed transactions and maintaining consistency across multiple nodes. Architects and developers must consider the trade-offs between consistency guarantees, scalability, and performance when designing distributed transactional systems. Organizations can effectively manage and integrate large-scale data into their distributed systems by employing appropriate transactional protocols, consistency models, and data <span class="No-Break">integration techniques.</span></p>
			<p>Distributed databases are challenging, so we should leverage the best architecture to minimize pitfalls. Next, we’ll discuss an error that’s recorded when managing a distrusted system that’s specifically related to the dual-write process and why it should <span class="No-Break">be avoided.</span></p>
			<h1 id="_idParaDest-110">Th<a id="_idTextAnchor178"/>e dual-write anti-pattern</h1>
			<p>Dual-write is a pattern or approach in software development where data is simultaneously written to two or more separate systems or databases in real time. Dual-write aims to ensure data consistency and synchronization across multiple systems that serve different purposes or require additional data. The following diagram shows this operation, where a single web app writes multiple times to a database, a cache, and a <span class="No-Break">second application.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer066">
					<img alt="Figure 10.3: Dua﻿l-write operation" src="image/Figure_10.03_B19375.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3: Dual-write operation</p>
			<p>While dual-write may seem convenient for data integration and synchronization, it is generally considered an anti-pattern. But what happens if one update succeeds and the other fails? Here are a few reasons why dual-write can <span class="No-Break">be problematic:</span></p>
			<ul>
				<li><strong class="bold">Complexity and coupling</strong>: Implementing dual-write introduces complexity and tight coupling between different systems. It increases the maintenance overhead and makes the system more fragile and prone to errors. Any change or update in one system may require corresponding changes in all the other systems involved in the <span class="No-Break">dual-write process.</span></li>
				<li><strong class="bold">Performance overhead</strong>: Dual-write can have a significant performance impact on the system. Writing data to multiple systems synchronously in real time can introduce latency and decrease the overall system performance. As the number of systems involved increases, the impact on performance becomes more pronounced, potentially leading to a degraded <span class="No-Break">user experience.</span></li>
				<li><strong class="bold">Inconsistencies and failures</strong>: Dual-write does not guarantee perfect consistency across all systems. Failures during writing, such as network issues or system failures, can lead to inconsistent data states across different systems. Handling these failures and resolving inconsistencies can be challenging <span class="No-Break">and time-consuming.</span></li>
				<li><strong class="bold">Data integrity challenges</strong>: Maintaining data integrity becomes more complex with dual-write. Ensuring that all the systems involved are updated correctly and simultaneously, without any data loss or corruption, requires implementing sophisticated mechanisms such as distributed transactions. These mechanisms add complexity and can further <span class="No-Break">impact performance.</span></li>
				<li><strong class="bold">Scalability limitations</strong>: Dual-write becomes increasingly challenging to scale as the system grows. As the number of designs and the volume of data increase, the overhead of synchronizing writes across all systems becomes more challenging to manage effectively. Scaling dual-write to handle high-throughput scenarios may require additional infrastructure and <span class="No-Break">optimization efforts.</span></li>
			</ul>
			<p>Instead of depending solely on dual-write, let’s explore other options for integrating and synchronizing data. Some recommended alternatives include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">ETL</strong>: Using ETL processes, data can be extracted from the source system, transformed into the appropriate format, and then loaded into the target system. This approach allows for more flexibility and decoupling between systems, enabling data transformations and mappings <span class="No-Break">as necessary.</span></li>
				<li><strong class="bold">Event-driven architecture</strong>: Employing an event-driven architecture can help propagate data changes or events across systems asynchronously. It decouples systems and allows for more flexible and scalable data integration. Events are published when data changes occur, and interested systems that are subscribed, can react to these <span class="No-Break">events accordingly.</span></li>
				<li><strong class="bold">Message queues</strong>: Leveraging message queues can provide reliable and scalable data integration and synchronization mechanisms. Systems can publish messages to the queue, and subscribing systems can consume them at their own pace, ensuring asynchronous and <span class="No-Break">decoupled communication.</span></li>
			</ul>
			<p>Organizations can achieve data integration and synchronization by adopting these alternative approaches while avoiding dual-write pitfalls. These approaches provide more flexibility, scalability, and maintainability, enabling better-distributed data <span class="No-Break">system management.</span></p>
			<p>Unfortunately, dual-write is the most popular anti-pattern we face as distributed architects and is a mistake. Now, let’s move to the second topic: microservices and <span class="No-Break">shared databases.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor179"/>Microservices and shared databases</h1>
			<p>The use of microservices architecture has become increasingly popular because it allows you to create scalable and flexible systems. This approach involves breaking down applications into smaller, independent services that can be developed, deployed, and scaled individually. However, despite its many advantages, sharing databases across multiple services can pose challenges <span class="No-Break">and disadvantages.</span></p>
			<p>The following figure illustrates a sample where three applications share the same database. In the short term, we can imagine that this will save us some power resources, but in the long term, we start wondering about the price. If we create an inconsistent data event, how do we know which application contains the bug? We may also have security issues, such as unauthorized <span class="No-Break">data access:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer067">
					<img alt="Figure 10.4: Shared database in a microservice" src="image/Figure_10.04_B19375.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4: Shared database in a microservice</p>
			<p>Multiple microservices sharing a database can introduce several challenges and drawbacks. These include data coupling and dependencies, performance bottlenecks, lack of autonomy and ownership, data integrity and consistency issues, and scalability and deployment flexibility limitations. The tight coupling between services due to shared data can slow development and hinder individual service flexibility. Contentions for database resources can lead to degraded performance, especially when multiple services concurrently access the same database. Shared databases also blur the lines of ownership and make it harder to identify responsible services for data-related issues. Ensuring data integrity and consistency becomes complex with multiple services writing to the same database, and conflicts and inconsistencies may arise. Scaling the database to accommodate the load from numerous services becomes challenging, and deploying new services or making changes can be complicated due to necessary schema changes and migrations affecting <span class="No-Break">other services.</span></p>
			<ul>
				<li><strong class="bold">Data coupling and dependencies</strong>: Sharing a database between multiple microservices introduces tight coupling between services. Database schema or data model changes can impact multiple services, requiring coordination and synchronization efforts. It can slow development and hinder individual services’ flexibility <span class="No-Break">and autonomy.</span></li>
				<li><strong class="bold">Performance bottlenecks</strong>: When multiple services access the same shared database, contention for database resources can become a bottleneck. Increased traffic and simultaneous requests from various services can lead to degraded performance since the database becomes a single point of contention. Scaling the database becomes more challenging as the load from multiple services must <span class="No-Break">be accommodated.</span></li>
				<li><strong class="bold">Lack of autonomy and ownership</strong>: Microservices architecture emphasizes the autonomy and ownership of individual services. Sharing a database blurs the lines of ownership as multiple services have access to and can modify the same data. It can create confusion and make identifying the responsible service for data-related issues or <span class="No-Break">errors easier.</span></li>
				<li><strong class="bold">Data integrity and consistency</strong>: Maintaining data integrity becomes more complex when multiple services are written to the same database. Coordinating transactions and managing concurrency becomes more complex when multiple services are involved. Ensuring consistency and enforcing business rules across services can be challenging as conflicts and data inconsistencies <span class="No-Break">may arise.</span></li>
				<li><strong class="bold">Scalability and deployment flexibility</strong>: Shared databases can limit microservices’ scalability and deployment flexibility. As the system grows, scaling the database becomes more challenging due to the increased load from multiple services. Additionally, deploying new services or changing existing services becomes more complicated as they may require database schema changes or data migrations that affect <span class="No-Break">other services.</span></li>
			</ul>
			<p>The following diagram shows the isolation between several services, where each service has a dedicated database and is responsible for it. All communication between applications will happen through an API; no application communicates directly with another <span class="No-Break">application’s database:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer068">
					<img alt="Figure 10.5: A microservice with each database" src="image/Figure_10.05_B19375.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5: A microservice with each microservice has its own database</p>
			<p>To tackle these obstacles, utilizing<a id="_idIndexMarker410"/> one database for each <a id="_idIndexMarker411"/>microservice is advisable. This approach offers numerous advantages, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Service autonomy and isolation</strong>: Each microservice has a dedicated database, providing independence and isolation. Each service can choose the database technology or schema that best suits its needs. Services can evolve independently without them impacting others, allowing faster development, deployment, <span class="No-Break">and scalability.</span></li>
				<li><strong class="bold">Simplified data management</strong>: Data management becomes more straightforward with a single database per microservice. It reduces coordination efforts and allows services to choose the most suitable data storage technology or approach. Services fully control their data, including schema changes, migrations, <span class="No-Break">and optimizations.</span></li>
				<li><strong class="bold">Improved performance and scalability</strong>: Dedicated databases enable services to scale horizontally and independently. Services can choose databases optimized for their specific workload, ensuring efficient data access and<a id="_idIndexMarker412"/> processing. Each service can handle its database load, improving performance <span class="No-Break">and scalability.</span></li>
				<li><strong class="bold">Clear ownership and responsibility</strong>: Having single databases per microservice ensures a clear sense of ownership and responsibility. Each service is responsible for<a id="_idIndexMarker413"/> its data, making troubleshooting<a id="_idIndexMarker414"/> and resolving issues easier. Additionally, it enhances the system’s maintainability <span class="No-Break">and supportability.</span></li>
				<li><strong class="bold">Simplified data consistency and integrity</strong>: Maintaining data consistency and integrity becomes more manageable with dedicated databases. Services can enforce their own business rules and transactional boundaries within their databases. It reduces the complexity of managing distributed transactions and mitigates data <span class="No-Break">consistency issues.</span></li>
			</ul>
			<p>Integration between services in a microservices architecture should ideally go through events, and it is generally considered a security best practice to avoid directly accessing or modifying another service’s database. By relying on events for communication and maintaining strict boundaries around each service’s database, you can enhance security and protect sensitive data within <span class="No-Break">the system.</span></p>
			<p>Here’s why events and avoiding direct database access <span class="No-Break">promote security:</span></p>
			<ul>
				<li><strong class="bold">Limited attack surface area</strong>: Accessing another service’s database increases the attack surface area. Exposing the database context of a service to other services introduces potential vulnerabilities, such as injection attacks or unauthorized access to sensitive data. Using events as a communication mechanism, you can limit the exposure of a service’s data and reduce the risk of <span class="No-Break">unauthorized access.</span></li>
				<li><strong class="bold">Data isolation</strong>: Each service in a microservices architecture has its specific context and boundaries. By avoiding direct access to another service’s database, you maintain data isolation and prevent unauthorized read or write operations on the database. This isolation ensures that only the service responsible for a specific data context can manipulate or access that data, enhancing security and <span class="No-Break">data privacy.</span></li>
				<li><strong class="bold">Separation of concerns</strong>: Microservices architecture emphasizes separation of concerns, where each service focuses on its specific domain. Allowing services to access each other’s databases can blur these boundaries and introduce potential data<a id="_idIndexMarker415"/> inconsistencies or unauthorized modifications. By relying on events, services can communicate and exchange relevant data without breaking the encapsulation and ownership of their <span class="No-Break">respective databases.</span></li>
				<li><strong class="bold">Auditing and compliance</strong>: Maintaining separate database contexts for each service simplifies auditing and compliance requirements. With dedicated databases, tracking and monitoring data access and modifications within a specific service’s context becomes easier. It supports compliance with regulatory standards and simplifies identifying and investigating security-related issues <span class="No-Break">or breaches.</span></li>
			</ul>
			<p>The <strong class="bold">Saga design pattern</strong> is used for<a id="_idIndexMarker416"/> long-running and distributed transactions. It allows a sequence of local transactions, each within the context of a specific service, to participate in a coordinated and<a id="_idIndexMarker417"/> consistent operation across multiple services. The Saga pattern enables communication and maintains data consistency across services without direct <span class="No-Break">database access.</span></p>
			<p>With the Saga pattern, each service involved in a transaction executes its part and emits an event to indicate the completion or progress of its task. Other services interested in the transaction listen to these events and continue their tasks accordingly. The Saga pattern ensures data consistency without directly exposing or modifying another service’s database by relying on events and a coordinated sequence of <span class="No-Break">local transactions.</span></p>
			<p>By adopting the event-driven architecture and leveraging the Saga pattern, microservices can securely communicate and maintain data consistency while upholding the principles of isolation, limited surface area, and separation of concerns. This approach enhances security and minimizes the risks associated with direct access to other service databases, enabling a more robust and secure <span class="No-Break">microservices ecosystem.</span></p>
			<p>Using several good practices in distributed architecture can reduce the number of pitfalls and challenges<a id="_idIndexMarker418"/> but not eliminate them. It is a perennial challenge to get consistency across persistent systems. However, there is one point that<a id="_idIndexMarker419"/> we need to understand and live with: <strong class="bold">eventual consistency</strong>. In the next section, we’ll discuss this in <span class="No-Break">mor<a id="_idTextAnchor180"/>e detail.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor181"/>Eventual consistency problems</h1>
			<p>In distributed <a id="_idIndexMarker420"/>systems, eventual consistency is a model where data updates are not instantly synchronized across all nodes. Instead, temporary inconsistencies are allowed, and the updates are gradually propagated until the system converges to a <span class="No-Break">consistent state.</span></p>
			<p>In eventual consistency, different nodes in the system may have different views of the data at any given point in time. This is primarily due to network latency, communication delays, and concurrent updates. However, eventual consistency ensures the system reaches a consistent state where all nodes converge on the <span class="No-Break">same data.</span></p>
			<p>To address the challenges and potential problems associated with eventual consistency, several techniques and <a id="_idIndexMarker421"/>mechanisms can <span class="No-Break">be employed:</span></p>
			<ul>
				<li><strong class="bold">Conflicts</strong> can occur when multiple updates are made to the same data simultaneously. To ensure consistency, conflict resolution mechanisms are used to determine how these conflicts should be resolved. Different techniques, including last-write-wins and application-defined conflict resolution strategies, can reconcile <span class="No-Break">conflicting updates.</span></li>
				<li><strong class="bold">Read repair</strong>: Read repair is a technique that’s used to repair inconsistencies by updating or synchronizing data during read operations. When a read operation encounters inconsistent or outdated data, it triggers a repair process that retrieves the latest version of the data from other nodes and updates the local copy, ensuring <span class="No-Break">eventual consistency.</span></li>
				<li><strong class="bold">Anti-entropy mechanisms</strong>: Anti-entropy mechanisms actively detect and reconcile inconsistencies in distributed systems. These mechanisms periodically compare data across nodes and initiate synchronization processes to ensure consistency. Examples of anti-entropy tools include Merkle trees, gossip protocols, and <span class="No-Break">vector clocks.</span></li>
				<li><strong class="bold">Quorum systems</strong>: Quorum systems determine the level of agreement required to achieve consistency in a distributed system. By defining quorums and quorum sizes, systems can ensure that a certain number of nodes must agree on an update or operation before it is considered consistent. This helps prevent inconsistencies due to partial updates <span class="No-Break">or failures.</span></li>
				<li><strong class="bold">Compensating actions</strong>: In cases where<a id="_idIndexMarker422"/> conflicts or inconsistent updates cannot be resolved automatically, compensating actions can be employed. Compensating actions are operations or processes that reverse or pay for incorrect or conflicting updates. These actions help restore consistency in <span class="No-Break">the system.</span></li>
				<li><strong class="bold">Idempotency</strong>: Designing <a id="_idIndexMarker423"/>operations to be idempotent can help mitigate inconsistencies. Idempotence, in programming and mathematics, is a property of some operations such that no matter how many times you execute them, you achieve the same result. It ensures that even if an operation is used numerous times due to communication delays or retries, the outcome remains the same, <span class="No-Break">preventing inconsistencies.</span></li>
			</ul>
			<p>If you’re familiar with NoSQL <a id="_idIndexMarker424"/>databases, you’ll remember <strong class="bold">BASE</strong> means <strong class="bold">basically available</strong>, where data values may change over time but will achieve eventual consistency. This eventual consistency is the data modeling concept we must consider to meet several horizontal scalabilities, and we can take advantage of the knowledge we learn from the NoSQL database. We could see several previously mentioned techniques being used on this database engine, such as Cassandra <span class="No-Break">as read-repair.</span></p>
			<p>It’s important to note that eventual consistency is unsuitable for all scenarios. Systems that require strict real-time consistency or those dealing with critical data may require more vital consistency models. However, for many distributed systems, eventual consistency strikes a balance between availability, performance, and <span class="No-Break">data integrity.</span></p>
			<p>Implementing and managing eventual consistency requires carefully considering the system’s requirements, using<a id="_idIndexMarker425"/> appropriate conflict resolution strategies, and choosing anti-entropy mechanisms. By employing these techniques, distributed systems can effectively handle temporary inconsistencies and converge toward a consistent s<a id="_idTextAnchor182"/>tate <span class="No-Break">over time.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor183"/>Summary</h1>
			<p>In conclusion, architecting distributed systems presents unique challenges that must be carefully addressed to ensure the success and effectiveness of the system. Throughout this chapter, we explored some challenges, such as dual-write and microservices with shared databases, and discussed why they could <span class="No-Break">be problematic.</span></p>
			<p>Although initially appealing for data consistency, dual-write can introduce complexity, performance overhead, and data integrity challenges. Similarly, sharing databases between microservices can lead to data coupling, performance bottlenecks, and compromised autonomy. These pitfalls emphasize the importance of carefully considering alternatives, such as event-driven architectures and single databases per microservice, to promote scalability, independence, <span class="No-Break">and maintainability.</span></p>
			<p>We also highlighted the significance of eventual consistency as a model for distributed systems. While it allows temporary data inconsistencies, eventual consistency balances availability, performance, and data integrity. Techniques such as conflict resolution, read repair, anti-entropy mechanisms, quorum systems, compensating actions, and idempotency help address any challenges and ensure <span class="No-Break">eventual consistency.</span></p>
			<p>Furthermore, documentation emerges as a critical aspect of distributed architecture. Good documentation provides a comprehensive overview of the system, its components, and their interactions. It enables better understanding, collaboration, and decision-making throughout development, maintenance, <span class="No-Break">and modernization.</span></p>
			<p>The next chapter will delve into modernization strategies and data integration. We will explore approaches to modernizing existing systems, leverage data integration techniques, and delve into the various patterns and technologies that facilitate smooth transitions and effective utilization of <span class="No-Break">distributed architectures.</span></p>
		</div>
	</body></html>