- en: '*Chapter 7*: MicroProfile Ecosystem with Open Liberty, Docker, and Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, in the previous chapters of this book, we focused on using the MicroProfile
    APIs to write a cloud-native application. In this chapter, we will look at how
    to run a cloud-native application. One of the features of MicroProfile that sets
    it apart from some other cloud-native application frameworks is that MicroProfile
    offers multiple implementations of the APIs. This reduces the chance of ending
    up locked into a particular implementation or finding that the open source community
    behind the APIs you were utilizing wasn''t as vibrant as you thought and the maintainers
    disappear. In addition, different implementations tend to take different design
    decisions, which may better suit your needs. At the time of writing, there were
    four implementations of the most recent release of the MicroProfile APIs: **Open
    Liberty**, **Payara**, **WebSphere Liberty**, and **WildFly**. In addition, **Helidon**,
    **JBoss EAP**, **KumuluzEE**, and **Quarkus** implement previous versions.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have chosen an implementation, you need to deploy the application into
    production. Increasingly, this is achieved using technologies such as **Docker**,
    **Kubernetes**, and a **Service Mesh**. This will be the focus of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying cloud-native applications to Open Liberty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerizing cloud-native applications using Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying cloud-native applications to Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MicroProfile and Service Mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to configure a MicroProfile application
    to run on Open Liberty, packaging it as a container and deploying it into a Kubernetes
    runtime like Red Hat OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build and run the samples mentioned in this chapter, you will need a Mac
    or PC (Windows or Linux) with the following software:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java Development Kit** (**JDK**) – Java 8 or later: [http://ibm.biz/GetSemerut](http://ibm.biz/GetSemerut)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Maven: [https://maven.apache.org](https://maven.apache.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Git client: [https://git-scm.com](https://git-scm.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Docker client: [https://www.docker.com/products](https://www.docker.com/products
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The OpenShift client: [https://docs.openshift.com/container-platform/4.6/cli_reference/openshift_cli/getting-started-cli.html](https://docs.openshift.com/container-platform/4.6/cli_reference/openshift_cli/getting-started-cli.html
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the source code used in this chapter is available on GitHub at [https://github.com/PacktPublishing/Practical-Cloud-Native-Java-Development-with-MicroProfile/tree/main/Chapter07](https://github.com/PacktPublishing/Practical-Cloud-Native-Java-Development-with-MicroProfile/tree/main/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying cloud-native applications to Open Liberty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how to deploy a MicroProfile application using
    **Open Liberty**. We have chosen Open Liberty because we are committers on Open
    Liberty, but its focus on being current with the latest releases of MicroProfile,
    performance, and ease of use make it a good option for anyone.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect from the name, Open Liberty is an open source Java runtime
    for building and deploying cloud-native applications. It is designed around the
    idea of components called **features** that can be configured to provide just
    enough runtime for your application needs. This means if your application doesn't
    use or need MicroProfile OpenTracing, then you don't need to configure the MicroProfile
    OpenTracing feature and the runtime will be smaller, faster, and leaner – it'll
    be better sized for what your application needs. Open Liberty has a feature for
    programming APIs such as MicroProfile APIs, Java EE, Jakarta EE, and gRPC. It
    also has features for runtime capabilities such as a feature for integrating with
    OpenID Connect for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Liberty is configured primarily using a simple XML file format referred
    to as a `server.xml` file. XML is used for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that Java has baked-in support for parsing XML is one of the primary
    reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XML models hierarchical configuration very well (unlike a properties format)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Space characters do not affect the semantic interpretation of the file format
    (unlike YAML).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When parsing the configuration file, Open Liberty takes the approach of ignoring
    any configuration it doesn't understand. This has several advantages. It means
    that a `server.xml` file can contain a configuration that isn't valid for the
    release of Open Liberty being used without causing a startup failure. It also
    means a simple typo in the configuration won't prevent the server from starting
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the core responsibilities of the `server.xml` file is to configure which
    features to load. By using the `server.xml` file to configure which features are
    to be used and by ensuring behavior changes are only introduced via new features,
    Open Liberty guarantees that the behavior of the configuration remains unchanged
    from one release to the next. A simple server configuration that enables all the
    MicroProfile APIs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Open Liberty configuration can be centralized in a single `server.xml` file,
    or it can be split up among many configuration files. This both facilitates both
    sharing of configuration and also separates configuration based on the environment
    the server configuration is deployed to. An example of this might be the use of
    an in-memory database in a development environment, but in production, a database
    such as DB2, Oracle, or MariaDB might be used instead. This is facilitated by
    two mechanisms. The first is a `server.xml` file that can explicitly include another
    `server.xml` file. The second is the use of something referred to as `defaults`
    and `overrides`, that are read before and after the main server configuration
    file. The files in these directories are read in alphabetical order, providing
    predictability in how configuration is read. Configuration files can also be parameterized
    using variable replacement syntax. Variables can be defined in the `server.xml`
    file, as Java system properties, or using environment variables. Variables in
    `server.xml` can be defined multiple times and the last definition of the variable
    will be used for variable resolution. A variable might be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It can then be referenced elsewhere like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Variables can also have a default value, which allows configuration to be written
    that will always work while allowing it to be overridden in production:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Variables have different precedents based on where they are defined from, allowing
    them to be easily overridden. The precedence order (later precedence overrides
    previous precedence) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`server.xml` default values'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `bootstrap.properties` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Java system properties
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variables defined in s`erver.xml`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variables defined on server startup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This provides multiple simple ways to change the behavior of Open Liberty based
    on the environment Open Liberty is deployed into.
  prefs: []
  type: TYPE_NORMAL
- en: Open Liberty allows you to package your MicroProfile application as a `WAR`
    file for deployment into the server. The MicroProfile specifications do not have
    an opinion on how an application is packaged and deployed, so Open Liberty reuses
    the `WAR` packaging model from Jakarta EE as the way to package the application.
    This makes a lot of sense because MicroProfile makes use of several Jakarta EE
    programming models, and it makes it easier for a MicroProfile application to make
    use of parts of Jakarta EE that are not in MicroProfile, such as concurrency utilities
    for Jakarta EE. It also allows you to reuse the existing Maven and Gradle build
    tools for packaging a MicroProfile application.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to deploy a `WAR` file into Open Liberty. The first is by
    dropping the WAR file into the `dropins` folder, and the second is through the
    `server.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The primary reason for using the server configuration approach over the `dropins`
    folder is that it allows you to customize how the application is run, for example,
    setting the application's `contextRoot`, configuring classloading, or configuring
    security role bindings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Liberty supports several mechanisms for packaging applications for deployment.
    The simplest is to package the application as a `WAR` file. This is the least
    likely in a cloud-native environment though. Open Liberty also supports packaging
    a server as a `zip` file, an executable `JAR` file, and a Docker container (described
    in the next section). Open Liberty provides plugins for Maven and Gradle that
    make it simple to build applications that will run on Open Liberty. One of the
    features of these plugins is the Open Liberty `dev` mode in Maven, simply add
    the Open Liberty Maven plugin to the `plugin` section of your `pom.xml` as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This configures the plugin to use the 3.3.4 version of the plugin or a more
    recent release if one exists.
  prefs: []
  type: TYPE_NORMAL
- en: When you run the `liberty:dev` Maven goal, the plugin will compile the application,
    downloading any dependencies required to run the application, deploy it into Liberty,
    and run the server with Java debugger support. This allows you to make changes
    to the application in any code editor, whether it is a simple editor such as **vi**
    or a full-fledged IDE such as **IntelliJ IDEA** or **Eclipse IDE**.
  prefs: []
  type: TYPE_NORMAL
- en: Liberty's design makes it very simple to build applications that will run in
    a container environment such as Docker. There is even a `dev` mode for containers
    that can be run using `liberty:devc` for this. The next section will discuss how
    to create a container as the deployment artifact.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing cloud-native applications using Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at how to containerize a MicroProfile application.
    There are two key parts to containerizing anything: the first is the creation
    of an image, and the second is running that image. While Docker was not the first
    product to do containerization, it did popularize it in a way that developers
    and operators could understand. Cloud Foundry was a common early alternative that
    had similar concepts but hid them as internal implementation details rather than
    making them first-class concepts. With Docker, these two concepts were broken
    into two parts, exposed by the `docker build` command used to create the image,
    and the `docker run` command used to run the image. These concepts were further
    expanded to become standardized, meaning there are now multiple alternatives to
    `docker build` and `docker run`.'
  prefs: []
  type: TYPE_NORMAL
- en: The container image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **container image** is the container deployment artifact. A container image
    contains everything required to run the application. This means that a container
    image can be moved from one environment to another with confidence that it will
    run in the same way. The motto is to think, create once, run everywhere; however,
    there are some limitations to this. A container is tied to a CPU architecture,
    so a container designed for x86 CPUs wouldn't run on ARM or Power ones without
    a translation layer for the CPU instructions (such as Rosetta 2, which translates
    Mac x86 instructions to Mac ARM ones to support x86 Mac applications on Macs with
    an M series ARM processor).
  prefs: []
  type: TYPE_NORMAL
- en: A **Dockerfile** is a set of instructions for how to create a container image.
    A Dockerfile starts by declaring a named image that it is based on and then identifies
    a series of steps to add additional content into the container image. A common
    practice might be for it to be based on an image containing an **Operating System**
    (**OS**), a Java image, or an image that pre-packages the application runtime
    such as Open Liberty.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is convenient to think of a container image as a single large file
    containing everything in the image, this is not how a container image works. A
    container image is made up of a series of layers that are identified by an SHA
    hash. This provides three key advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the storage space required to store images. If you have 30 images
    based on a common base image, you only store that common base image once, not
    30 times. While storage space is relatively cheap, if you have a large number
    of applications, file duplication between the containers would soon add up to
    large numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces bandwidth requirements. When you transfer container images to and
    from a **container registry**, you do not need to upload or download the layers
    you already have. It is likely you will have many container images, but one common
    OS image, and a common JVM image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces build time. If the input to a layer hasn't changed, there is no need
    to rebuild it. The input to a layer is any change you made in that line, plus
    the output of the prior line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each layer has a pointer to the layer it was built on top of. When you create
    a container image, it consists of all the layers from the base image and one new
    layer for every single line in the Dockerfile. A simple Dockerfile for packaging
    a simple MicroProfile application might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This creates a container image based on Ubuntu 20.04 with Java SE 11 using the
    OpenJ9 JRE implementation with all Open Liberty features available to it. It then
    copies `server.xml` from the default location in a Maven project and the application
    to the `dropins` folder. This will create an image with all the layers associated
    with the `open-liberty:full-java11-openj9` image, and two layers associated with
    this image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice: using multiple layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of this approach is when you push or pull an image, only layers
    that are not already present are transferred. In the simple MicroProfile example
    stated previously, when you build and push the image, only the layers associated
    with the application and server configuration will be transferred. Think of it
    this way: if you have a base image that is 500 MB in size, and the layers of your
    image are a total of 5 MB, your image would be a total of 505 MB, but when you
    push that back to the container registry, only 5 MB would need to be sent since
    the base image is already there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to some interesting design questions when designing Docker images.
    The purpose of a Docker image is clearly to get it running somewhere, and preferably
    to get that to happen as quickly as possible. This makes it faster to deploy a
    new image production, to scale it up, or in the event of a problem, to replace
    the container with a new one. A simple way to build Docker images is to package
    your application in a single JAR file, add it to the Dockerfile, and then run
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a popular way to create Docker images and works well if the application
    is small, but many applications built this way are not small. Consider the case
    where that `jar` contains Open Liberty, the application code, and some open source
    dependencies. That means that every time the application is modified, a new layer
    containing all that code has to be re-created and deployed. If, on the other hand,
    the application was split up, then a change to the application would require a
    much smaller upload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this example, a change to the application code would only rebuild the last
    layer, meaning the upload will be smaller and distributed faster. A change to
    the open source dependencies would of course result in that layer being rebuilt,
    but those tend to change less frequently than the application. If there are many
    applications that share a common set of libraries (whether open source or not),
    it might make sense to create a named base image that all applications can use.
    This would be especially useful if the container images are commonly run on the
    same host.
  prefs: []
  type: TYPE_NORMAL
- en: A significant thing to understand about the layers is once created, they are
    immutable. That means if you delete a file created in an earlier layer, it doesn't
    remove the files from the image; it just marks them as deleted so they cannot
    be accessed. This means that when transferring the container image around, you
    will copy the bytes for the file, but it won't ever be accessible. If the file
    is contributed to by a base image, this will be unavoidable, but if you control
    the image, then it is something to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As previously stated, a Dockerfile is a series of instructions detailing how
    to create a Docker image. There are several instructions for creating an image.
    The first instruction in all the examples so far is the `FROM` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: FROM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `FROM` instruction defines the container image that is the base of the
    image you are creating. All Dockerfiles are required to start with `FROM`. A Dockerfile
    can have multiple `FROM` lines: these are commonly used to create a multi-stage
    build. An example of this might be if you need some kind of extra tools to build
    your image that you don''t want to be present when you run it. An example of this
    might be `wget` or `curl` for downloading files and `unzip` for expanding a zip
    file. A simple multi-stage build might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the first stage installs `wget` and `unzip`, downloads a file,
    and unzips it. The second stage starts from the base image and then copies the
    extracted files into the new image layer. If a single-stage Dockerfile were created,
    this would have resulted in an image with three additional layers containing the
    binaries for `unzip` and `get`, the `zip` file, and `extract`. The multi-stage
    build only contains `extract`. To do this with a single-stage Docker build is
    less readable and would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This Dockerfile uses a single `RUN` command to run multiple commands to create
    only a single layer and it has to undo each step before the end. The last line
    is required to tidy up files created by `apt`. The multi-stage Dockerfile is much
    simpler. Another common use for multi-stage builds is to use the first stage to
    build the application, and then the second stage for running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: COPY and ADD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `COPY` and `ADD` instructions perform a similar function. `ADD` has a superset
    of the function of `COPY` so it is generally advised to only use `ADD` if you
    need the extended function. The first argument for both instructions specifies
    the source file (or directory) and by default is interpreted to be copying from
    the machine running the build. The command is always relative to the directory
    the build is run from and you cannot use `..` to navigate to the parent directory.
    The use of the `from` argument, as shown in the previous section, redirects the
    copy to be from another container image. The second argument is the location in
    the container the file should be copied to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ADD` command provides some additional features over and above the `COPY`
    command. The first is it allows you to specify a `URL` to download the file from
    as the first argument. The second feature is it will unzip a `tar.gz` file into
    a directory. To go back to the first multi-stage build example, if the output
    was a `tar.gz` file, it would mean it could be simplified to just be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: RUN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `RUN` instruction simply executes one or more commands using the shell
    from the OS layer. This allows you to do pretty much anything you want or need
    to provide the command available in the base OS image. For example, it is uncommon
    for `unzip` or `wget` to be in the base Linux OS images, so those commands will
    fail unless action is taken to install them. Each `RUN` instruction creates a
    new layer, so if you create a file in one `RUN` command and delete it in another,
    due to the immutability of the layers, the file will exist but not be visible
    anymore. For this reason, it is often important to use the `&&` operator to string
    multiple commands together in a single layer. An example of this was shown previously,
    but is repeated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ARG and ENV
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`ARG` defines a build argument that can be specified at build time. The `ARG`
    values are set when running `docker build` using the `build-arg` argument. `ARG`
    can have a default value, in case it isn''t provided at build time. These build
    arguments are not persisted after the build finishes, so they are not available
    at runtime and are not persisted in the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ENV` defines an environment variable that is available both at build and runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these are referenced in the same way, so the key difference is the visibility
    of the value.
  prefs: []
  type: TYPE_NORMAL
- en: ENTRYPOINT and CMD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When running a container, you need something to happen, such as starting the
    Open Liberty server. What happens can be defined by the Dockerfile using the instructions
    `ENTRYPOINT` and `CMD`. The difference between the two instructions is how they
    interact with the `docker run` command. When running a Docker container, any arguments
    after the Docker image name are passed into the container. `CMD` provides a default
    value in case no command-line arguments are provided. `ENTRYPOINT` defines a command
    that will be run and any command-line arguments provided to `docker run` are passed
    in after `ENTRYPOINT`. Both `CMD` and `ENTRYPOINT` have the same syntax. The Open
    Liberty container specifies both of these, so images based on them do not tend
    to specify them.
  prefs: []
  type: TYPE_NORMAL
- en: WORKDIR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `WORKDIR` instruction is used to change the current directory for future
    `RUN`, `CMD`, `COPY`, and `ENTRYPOINT` instructions.
  prefs: []
  type: TYPE_NORMAL
- en: USER
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When building an image, the default user account used for executing commands
    is the `root` one. For some operations, this is reasonable and fair. If doing
    an OS update, you typically need to execute as `root`. However, when running the
    containers using, the `root` account is a clear security issue. The Dockerfile
    has a `USER` instruction that sets the user account used for `RUN` instructions
    as well as the process that executes in the container when it is run. This makes
    it simple to set the account to a non-root account. The Open Liberty images in
    previous examples set `USER` to `1001`, which means that any of the previous examples
    based on it will not run using the `root` account, but the one based on the Java
    image would. One problem with the previous Dockerfile examples is that the `ADD`
    and `COPY` instructions write to the files so they are owned by the `root` user,
    which can cause issues at runtime. This can be resolved by updating the `ADD`
    or `COPY` instructions to change the ownership as they are written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, the `RUN` instruction can be used to execute the `chown` command-line
    tool. This will create a new layer but might be required if the `ADD` or `COPY`
    instruction moves multiple files and only some should have their ownership changed.
  prefs: []
  type: TYPE_NORMAL
- en: Although Dockerfiles are the most used way to create a container image, there
    are alternative ways to build container images. A few examples follow.
  prefs: []
  type: TYPE_NORMAL
- en: Source-to-Image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Source-to-Image** (**S2I**) is a technology that converts an application
    source into a container image. Instead of creating a **Dockerfile**, an **S2I**
    builder ingests the source code, runs the build, and encodes it in a container
    image. This allows the developer to focus on the application code and not the
    creation of the container. By encoding the best practices for building a container
    in an **S2I** builder, it can be reused across applications, making it more likely
    that a set of applications all have well-designed container images. There are
    **S2I** builders for many languages and frameworks, including Open Liberty. **S2I**
    is an open source technology created by Red Hat to help developers adopt OpenShift,
    although it can be, and is, used to create containers that can run anywhere.'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Native Buildpacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`WAR` file.'
  prefs: []
  type: TYPE_NORMAL
- en: Having created a container image, the next step is to run it. While a developer
    might use Docker to run the image on their desktop when running in production,
    the most common way to run container images is with Kubernetes, which we will
    discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying cloud-native applications to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kubernetes** started as a project in Google to allow them to manage software
    at scale. It has since moved to become an open source project managed by the **Cloud
    Native Computing Foundation** (**CNCF**) and has contributors from all over the
    industry. Every major (and most minor) public cloud provider uses Kubernetes to
    manage the deployment of containers. There are also private cloud products such
    as Red Hat OpenShift that provide a distribution of Kubernetes for deployment
    either on-premises or on a public cloud but dedicated to a single company.'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes deployment is known as a **cluster**. To run containers and provide
    a highly available, scalable environment a cluster consists of a control plane
    and a set of key resources that provide it with the ability to run or manage containers,
    scale them, and keep the containers running in the event of any failures. When
    running a container in Kubernetes, the container is placed in a Pod, which is
    then run on a node based on decisions made by the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: A **Pod** provides a shared context for running a set of containers. All the
    containers in a Pod run on the same node. Although you can run multiple containers
    in a Pod, normally a Pod will contain a single application container, and any
    other containers running in the Pod will be sidecars providing some administrative
    or support function to the application container.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a traditional automated operation environment, the automation will describe
    how to set the environment up. When using Kubernetes instead of describing how
    to set up the environment, the description describes the desired end state, and
    it is up to the control plane to decide how to make this happen. The configuration
    for this is provided as one (or, more often, a set of) YAML document(s). The net
    effect of this is when deploying to Kubernetes, you do not describe the deployment
    by defining pods on a node and putting a container into it. Instead, you define
    a **Deployment** that will express the container you want to deploy and how many
    replicas of the container should be created. A simple deployment of a single instance
    of Open Liberty can be done with this YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This YAML can then be deployed using the `kubectl apply` command. This results
    in a single Pod running Open Liberty being deployed. While the container is running
    and could respond to HTTP requests, there is no route for network traffic to get
    to the container. The key to enabling network traffic to reach a deployment is
    the Kubernetes **Service**. The Service defines the port the process in the container
    is listening on and the port it should be accessed via the Kubernetes networking
    stack. A Service for this can be defined using this YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A Service allows other containers running Kubernetes to access it, but it doesn''t
    allow services outside the cluster to access it. There are several options for
    how to expose the container externally, for example, port forwarding, an ingress
    controller, or OpenShift has the concept of **routes**. A route essentially just
    exposes a service externally to the cluster. You can specify the host and path,
    or you can let Kubernetes default it. To expose this Open Liberty server externally,
    you can define a route using this YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These three YAML files have deployed a container and exposed it externally so
    it can be accessed and used. The three YAML files can be placed in a single file
    using `---` as a separator, but there is an alternative option for managing deployment
    than using YAML to configure everything, and that option is to use an Operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'An **Operator** is a way of packaging, deploying, and managing a set of resources
    related to an application. They were originally intended to help manage stateful
    applications where just throwing away and starting a new Pod might result in data
    loss; however, they can also be used to simplify the deployment of applications.
    Operators watch for the definition of a **Custom Resource** that it understands
    and configures the relevant Kubernetes resources to run that application. An Operator
    can do things such as managing the deployment and updating of an application when
    new images become available. Open Liberty provides an Operator that can manage
    the deployment of applications built on Open Liberty. As an example, all the previous
    YAML files can be simply replaced with this YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: MicroProfile Health in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The MicroProfile Health specification allows you to configure support for liveness
    and readiness checks. These probes allow the Kubernetes control plane to understand
    the health of the container and what action to take. An unsuccessful liveness
    probe will trigger the Pod to be recycled as it indicates that a problem has occurred
    that cannot be resolved. A readiness probe, on the other hand, will simply cause
    Kubernetes to stop routing traffic to the Pod. In both cases, you need to have
    multiple instances to ensure that during any outage of one container, the clients
    will remain unaware. To configure these liveness and readiness probes, you ensure
    that the Open Liberty server is configured to run MicroProfile Health:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, when defining the application, configure the liveness and readiness probes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This configures the liveness and readiness probes to make an HTTP `get` request
    to the MicroProfile health endpoints for liveness and readiness. It also configures
    a wait period after the container starts before making the first check. This gives
    the container a chance to run any startup routines before it starts polling to
    determine the status.
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile Config in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MicroProfile Config** provides a way to receive configuration in your application
    that can be provided in the environment. In Kubernetes, this kind of configuration
    is typically stored in a **ConfigMap** or a **Secret**. As discussed previously,
    in [*Chapter 5*](B17377_05_Final_SB_epub.xhtml#_idTextAnchor091), *Enhancing Cloud-Native
    Applications*, a ConfigMap is essentially a set of key/value pairs stored in Kubernetes
    that can be bound into a Pod so it is available to the container. To receive configuration
    in your application from Kubernetes, ensure that the Open Liberty server is configured
    to run MicroProfile Config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many ways to create a ConfigMap, and [*Chapter 5*](B17377_05_Final_SB_epub.xhtml#_idTextAnchor091),
    *Enhancing Cloud-Native Applications*, demonstrated one mechanism. Another way
    to define a ConfigMap is to apply a ConfigMap with the following YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when deploying your application, you can either bind a single environment
    variable from this ConfigMap, or all of them. To bind the value of `example.property.1`
    from the ConfigMap as a variable called `PROP_ONE` to an Open Liberty application,
    you would use the following YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'A ConfigMap could (as in the aforementioned example) contain a lot of properties
    that the container may need to access, instead of binding a single entry, or entries
    one by one, you can bind all the entries. The following YAML would define an application
    with all the values of the ConfigMap bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the more recent features of MicroProfile Config is the concept of configuration
    profiles. The idea is that you can provide the configuration for running the application
    in development, test, and production environments and have MicroProfile Config
    only load the configuration for the desired profile. To configure this, you also
    need to define the config profile. The MicroProfile Config specification says
    that a property in a profile name starts with `%<profile name>`; however, `%`
    isn''t valid in an environment variable name so it is replaced with `_`. Example
    YAML for this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: ConfigMaps in Kubernetes are good for storing data that isn't sensitive, but
    when it comes to storing API keys, credentials, and so on, Kubernetes has an alternative
    concept known as Secrets. Secrets can represent multiple different kinds of Secrets,
    but here we are just going to consider simple key/value pairs. The Kubernetes
    platform provides better protection for Secrets than ConfigMaps, although many
    people prefer to use third-party products for Secret management. It is still good
    to understand how Secrets work because third-party products tend to follow the
    same conventions to access sensitive data from within the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secrets are encoded using base64 encoding, which isn''t fantastic protection.
    Open Liberty allows passwords it loads to be AES encrypted, and provides an API
    for decrypting protected strings, so your base64-encoded secret could be a base64-encoded
    AES encrypted string. However, since you would still need to provide the decryption
    key to Open Liberty and this is not a security hardening book, we will not go
    into further details here. Referencing a single key pair from a secret from deployment
    is done almost identically to referencing from a ConfigMap, but using `secretKeyRef`
    rather than `configMapKeyRef`; for example, with this YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: If you deploy the secret YAML and bind to it as in the aforementioned example,
    your container will have an environment variable called `PROP_ONE` whose value
    is `super secret`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like with a ConfigMap, you can bind all the key/value pairs in a secret
    to the container, and just like with the prior example, it is done in a very similar
    way to ConfigMaps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Secrets can also be bound as files in the container file system and this can
    be preferable for security-sensitive data. When you do this, the secret will be
    defined in the file system and the value of the secret will be the content of
    the file. MicroProfile Config cannot consume secrets bound this way, but it provides
    a way to add additional ConfigSources, allowing you to easily load the configuration.
    The YAML for binding a secret to the file system is to essentially mount it as
    a volume. The following example YAML will result in every key/value in the secret
    `secret.config` being mounted as a file in the file system of the container under
    the directory `/my/secret`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable the injection of the bound Secrets, you need a `META-INF/services/org.eclipse.microprofile.config.spi.ConfigSource`
    on the application classpath will automatically cause it to be loaded. A short
    example ConfigSource that will do this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This config source will load the content of a file with the property name read
    from a well-defined directory. If the file cannot be read, it will act as if the
    property is not defined. Kubernetes will update the file content when the secret
    is updated, which means that updates can be visible to the application automatically
    since this code will reread the file each time the property is read.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss some considerations when using a Service
    Mesh with MicroProfile.
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile and Service Mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying into a Kubernetes cluster, some people choose to make use of
    a Service Mesh. The goal of a Service Mesh is to move certain considerations of
    microservices out of the application code and to place it around the application.
    A Service Mesh can remove some application concerns such as service selection,
    observability, fault tolerance, and, to a certain degree, security. One common
    Service Mesh technology is **Istio**. The way Istio works is by inserting a sidecar
    into the Pod for the containers and all inbound and network traffic is routed
    via that sidecar. This allows the sidecar to perform activities applying access
    control policies, routing requests to downstream services, and applying fault
    tolerance policies such as retrying requests or timing them out. Some of these
    capabilities overlap with some of the MicroProfile capabilities, for example,
    Istio can handle attaching and propagating OpenTracing data with requests. If
    you use **Istio**, you clearly do not need to make use of MicroProfile OpenTracing,
    although using both would complement each other rather than cause conflict.
  prefs: []
  type: TYPE_NORMAL
- en: 'One area where the use of a Service Mesh and MicroProfile can conflict in negative
    ways is fault tolerance. For example, if you configure 5 retries in MicroProfile
    and 5 retries in Istio and they all fail, you will end up with a total of 25 retries.
    As a result, it is common when using a Service Mesh to disable the MicroProfile
    fault tolerance capabilities. This can be done with the environment variable of
    `MP_Fault_Tolerance_NonFallback_Enabled` set to `false`. This will disable all
    the MicroProfile fault tolerance support except for the fallback capability. This
    is because the logic to perform on a failure is intrinsically an application consideration
    and not something that can be extracted into the Service Mesh. This can simply
    be disabled using the following YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This configures the application to have a hardcoded environment variable that
    disables the non-fallback MicroProfile Fault Tolerance behaviors. This could also
    be done with a ConfigMap.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have reviewed the MicroProfile implementation that is used
    in the rest of the book, some best practices for building a container for a MicroProfile
    application, and how to deploy that application into Kubernetes. While the chapter
    isn't an exhaustive review of the capabilities available in Kubernetes, it does
    focus on the particular considerations for deploying MicroProfile applications
    to Kubernetes, and why they interact with Kubernetes services. This chapter should
    have provided you with a good starting point for creating and deploying MicroProfile
    applications using Open Liberty, containers, and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will describe an example application that makes use of MicroProfile
    for a set of microservices deployed in a container to a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
