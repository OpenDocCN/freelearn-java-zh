<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer171">
<h1 class="chapter-number" id="_idParaDest-129"><a id="_idTextAnchor130"/>8</h1>
<h1 id="_idParaDest-130"><a id="_idTextAnchor131"/>Enabling Data Security and Governance</h1>
<p>In the preceding chapters, we learned how to evaluate requirements and analyze and apply various architectural patterns to solve both real-time and batch-based problems. We learned how to choose the optimal technical stack and develop, deploy, and execute the proposed solution. We also discussed various popular architectural patterns for data ingestion. However, any discussion about data architecture is incomplete without mentioning data governance and data security. In this chapter, we will focus on understanding and applying data governance and security in the data layer.</p>
<p>In this chapter, we will first discuss what data governance is, and why it is so important. We will also briefly discuss a few open source data governance tools that are available on the market. Then, we will practically demonstrate a data governance implementation by adding a data governance layer to a data ingestion pipeline. The data ingestion pipeline ID will be developed using Apache NiFi and data governance will be achieved using DataHub. Then, we will discuss the need for data security and the types of solutions that help enable it. Finally, we will discuss a few open source tools available to enable data security.</p>
<p>By the end of the chapter, you will know the definition of and need for a data governance framework. You will also know when data governance is required and all about the data governance framework by the <strong class="bold">Data Governance Institute</strong> (<strong class="bold">DGI</strong>). In addition, you will know how to implement practical data governance using DataHub. Finally, you will know about various solutions and tools available to enable data security.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Introducing data governance – what and why</li>
<li>Practical data governance using DataHub and NiFi</li>
<li>Understanding the need for data security </li>
<li>Solutions and tools available for data security</li>
</ul>
<h1 id="_idParaDest-131"><a id="_idTextAnchor132"/>Technical requirements</h1>
<p>For this chapter, you will need the following:</p>
<ul>
<li>OpenJDK 1.11 installed on your local machine</li>
<li>Docker installed on your local machine</li>
<li>An AWS account </li>
<li>NiFi 1.12.0 installed on your local machine</li>
<li>DataHub installed on your local machine</li>
<li>Prior knowledge of YAML is preferred</li>
</ul>
<p>The code for this chapter can be downloaded from this book’s GitHub repository: <a href="https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08">https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08</a>.</p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/>Introducing data governance – what and why </h1>
<p>First, let’s try to understand what data governance is and what it does. In layman’s terms, <strong class="bold">data governance</strong> is the process <a id="_idIndexMarker855"/>of assigning proper authority and taking decisions regarding data-related matters. According to the DGI, data governance is defined as “<em class="italic">a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models that describe who can take what actions with what information, and when, under what circumstances, using what methods.</em>”</p>
<p>As evident from this definition, it is a practice of creating definitive strategies, policies, and rules that define who can make what decisions or perform actions related to data. It also lays out guidelines about how to make decisions related to data. </p>
<p>Data governance<a id="_idIndexMarker856"/> considers the following aspects:</p>
<ul>
<li>Rules</li>
<li>Enterprise-level organizations</li>
<li>Decision rights and procedures</li>
<li>Accountability</li>
<li>Monitoring and controlling data</li>
</ul>
<p>Now that we have a<a id="_idIndexMarker857"/> basic understanding of data governance, let’s explore what scenarios data governance is recommended in.</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>When to consider data governance</h2>
<p>A formal data governance<a id="_idIndexMarker858"/> framework should be considered in any of the following scenarios:</p>
<ul>
<li>The data in the organization has grown so much and become so complex that traditional data management tools are unable to solve cross-functional data needs.</li>
<li>The horizontally focused business units and teams need cross-functional data generated or maintained by other focused groups. In this case, an enterprise-wide solution (instead of siloed) for data availability and management is required. For example, for B2B sales of Visa, the marketing and accounting departments maintain data in silos. However, the sales department needs both these silos’ data to create accurate sales predictions. So, the data across these different departments should be stored in an enterprise-wide central repository instead of silos. Hence, this enterprise-wide data requires data governance for proper access, use, and management.</li>
<li>Compliance, legal, and <a id="_idIndexMarker859"/>contractual obligations can also call for formal data governance. For example, the <strong class="bold">Health Insurance Portability and Accountability Act</strong> (<strong class="bold">HIPAA</strong>) enforces that all <strong class="bold">protected health information</strong> (<strong class="bold">PHI</strong>) data should be well governed and protected from theft or<a id="_idIndexMarker860"/> unauthorized access.</li>
</ul>
<p>So far, we’ve discussed what data governance is and when we should have a formal data governance framework. Now we will learn about the DGI data governance framework.</p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor135"/>The DGI data governance framework</h2>
<p>The DGI data <a id="_idIndexMarker861"/>governance framework is a logical structure for implementing<a id="_idIndexMarker862"/> proper data governance in an organization to enable better data-related decisions to be made. The following diagram depicts this data governance framework:</p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 8.1 – Data governance framework " height="783" src="image/B17084_08_001.jpg" width="1292"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Data governance framework</p>
<p>In this section, we will discuss the various components of the DGI data governance framework. Let’s take a look at each of the components highlighted in the preceding diagram:</p>
<ol>
<li><strong class="bold">Mission</strong>: The mission <a id="_idIndexMarker863"/>of the DGI framework can be attributed to three primary responsibilities:<ol><li>Define rules</li>
<li>Execute and implement the rules by providing ongoing protection and services to data stakeholders</li>
<li>Deal with scenarios that arise due to non-compliance</li>
</ol></li>
</ol>
<p>This form of mission is very similar to political governance. In the political governance model, the government makes the rules, the executive branch implements the rules, and the judiciary deals with non-compliance or rule breakers. Just like political governance, here, one set of data stakeholders defines the rules. Another set of stakeholders ensures that the rules are followed. Finally, a<a id="_idIndexMarker864"/> third set of stakeholders makes decisions related to non-compliance. Optionally, the organization can develop a vision statement around the mission, which may be used to inspire data stakeholders to envision possibilities and set data-related goals.</p>
<ol>
<li value="2"><strong class="bold">Focus areas</strong>: Mission and<a id="_idIndexMarker865"/> vision lead us to the primary focus areas. We have two primary focus areas. They are as follows:<ul><li>Goals should be <strong class="bold">SMART</strong> – that is, <strong class="bold">Specific, Measurable, Actionable, Relevant, and Timely</strong>. We should remember <a id="_idIndexMarker866"/>the principle of the four P’s while deciding on the goals we want to pursue – that is, programs, projects, professional disciplines, and people. We must ask how these efforts can help our organization in terms of revenue, cost, complexity, and ensuring survival (that is, security, compliance, and privacy).</li>
<li>Metrics should also be SMART. Everyone in data governance should know how to quantify and measure success.</li>
</ul></li>
</ol>
<p>This discussion leads to the question of where we can fund our data governance program. To do so, we must ask the following questions:</p>
<ul>
<li>How can we fund the data governance office?</li>
<li>How can we fund the data architects/analysts to define rules and data?</li>
<li>How can we fund the data stewardship activities?</li>
</ul>
<p>Focus areas are very important when planning formal data governance.</p>
<ol>
<li value="3"><strong class="bold">Data rules and definitions</strong>: In this component, rules, policies, standards, and compliances are set around data. Typical activities may include creating new rules, exploring <a id="_idIndexMarker867"/>existing rules, and addressing gaps and overlaps.</li>
<li><strong class="bold">Decision rights</strong>: This <a id="_idIndexMarker868"/>component implores the question, <em class="italic">Who can make a data-related decision, when, and using what process?</em> The data governance program allows us to store decision rights as metadata for data-related decisions.</li>
<li><strong class="bold">Accountability</strong>: This <a id="_idIndexMarker869"/>component creates accountability for the implementation of a rule or decision once it is made. The data governance program may be <a id="_idIndexMarker870"/>expected to integrate the accountabilities in a day-to-day <strong class="bold">software development life cycle</strong> (<strong class="bold">SDLC</strong>).</li>
<li><strong class="bold">Controls</strong>: Data is the<a id="_idIndexMarker871"/> new gold, so there is a huge security risk involved if there are sensitive data breaches. How do we ensure that <a id="_idIndexMarker872"/>such risks are mitigated and handled properly? They can be handled using controls. Controls can be preventive or reactive. The data governance team is usually tasked with making recommendations for creating these controls at different levels of the control stack (networking/OS/database/application). Sometimes, the data governance team is also asked to modify the existing controls to ensure better data governance.</li>
<li><strong class="bold">Data stakeholders</strong>: Data stakeholders are individuals or groups who either could affect or are affected by the data. Since they have a direct correlation with the data, they are consulted<a id="_idIndexMarker873"/> while taking data decisions. Again, based on the scenario, they may want to be involved in some data-related decisions, should be consulted before decisions are finalized, or be informed once decisions are made.</li>
<li><strong class="bold">Data governance office</strong>: This<a id="_idIndexMarker874"/> facilitates, supports, and runs the data governance program and data stewardship activities. It is the centralized governing body and usually consists of data architects, data analysts, and those working on creating the metadata. It collects and aligns policies, rules, and standards from different stakeholders across the organization and comes up with organizational-level rules and standards. It is responsible for providing centralized <a id="_idIndexMarker875"/>data governance-related communication. It is also responsible for collecting the data governance metrics and publishing reports and success measures to all the data stakeholders.</li>
<li><strong class="bold">Data stewards</strong>: They are part of the data stewardship council, which is responsible for making data-related decisions such as setting policies, specifying standards, or providing<a id="_idIndexMarker876"/> recommendations to the DGO. Based on the size and complexity of the organization, the data stewardship council(s) may have a hierarchy. In data quality-focused governance projects, there can be an optional data quality steward.</li>
<li><strong class="bold">Data processes</strong>: These are the <a id="_idIndexMarker877"/>methods that are used to govern data. The processes should be documented, standardized, and repeatable. They are designed to follow regulatory and compliance requirements for data management, privacy, and security.</li>
</ol>
<p>All these components work in tandem and create a feedback loop, which ensures data governance is continuously improving and up to date.</p>
<p>In this section, we learned about what data governance is, when it should be implemented, and the DGI framework. In the next section, we will provide step-by-step guidelines for implementing data governance.</p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor136"/>Practical data governance using DataHub and NiFi</h1>
<p>In this section, we will<a id="_idIndexMarker878"/> discuss a tool called DataHub and how different data stakeholders and stewards can make use of it to enable better data governance. But first, we will understand the use case and what we are trying to achieve. </p>
<p>In this section, we will build a data governance capability around a data ingestion pipeline. This data ingestion pipeline will fetch any new objects from an S3 location, enrich them, and store the data in a MySQL table. In this particular use case, we are getting telephone recharge or top-up events in an S3 bucket from various sources such as mobile or the web. We are enriching this data and storing it in a MySQL database using an Apache NiFi pipeline.</p>
<p>Apache NiFi is a powerful and reliable drag-and-drop visual tool that allows you to easily process and distribute data. It creates directed graphs to create a workflow or a data pipeline. It consists of the following high-level components so that you can create reliable data routing and transformation capabilities:</p>
<ul>
<li><strong class="bold">FlowFile</strong>: Each data record is <a id="_idIndexMarker879"/>serialized and processed as a FlowFile object in the NiFi pipeline. A FlowFile object consists of  flowfile content and attribute denoting data content and metadata respectively.</li>
<li><strong class="bold">Processor</strong>: This is one of the <a id="_idIndexMarker880"/>basic units of NiFi. This component is primarily responsible for processing the data. It takes a FlowFile as input, processes it, and generates a new one. There are around 300 inbuilt processors. NiFi allows you to develop and deploy additional custom processors using Java.</li>
<li><strong class="bold">Queue</strong>: NiFi follows a staged <a id="_idIndexMarker881"/>event-driven architecture. This means that communication between processors is asynchronous. So, there needs to be a message bus that will hold a FlowFile generated by one processor, before it is picked by the next processor. This message<a id="_idIndexMarker882"/> bus is called the NiFi queue. Additionally, it supports setting up backpressure thresholds, load balancing, and prioritization policies.</li>
<li><strong class="bold">Controller service</strong>: This allows you to share functionality and state across JVM cleanly and consistently. It is<a id="_idIndexMarker883"/> responsible for things such as creating and maintaining database connection pools or distributed caches.</li>
<li><strong class="bold">Processor groups</strong>: When a data flow becomes complex, it makes more sense to group a set of components, such <a id="_idIndexMarker884"/>as processors and queues, into an<a id="_idIndexMarker885"/> encapsulation called a processor group. A complex pipeline may have multiple processor groups connected. Each processor group, in turn, has a data flow inside it.</li>
<li><strong class="bold">Ports</strong>: Processor groups can be<a id="_idIndexMarker886"/> connected using ports. To get the input from an external processor group, input ports are used. To send a processed FlowFile out of the processor group, output ports are used.</li>
</ul>
<p>Now, let’s build the NiFi pipeline <a id="_idIndexMarker887"/>for our use case. Our source is an S3 bucket called <strong class="source-inline">chapter8input</strong>, while our output is a MySQL cluster. We will be receiving S3 objects in the <strong class="source-inline">chapter8input</strong> folder from different sources. Each S3 object will be in JSON format and will contain one phone recharge or top-up (bundle) event. Our data<a id="_idIndexMarker888"/> sink is a MySQL table called <strong class="source-inline">bundle_events</strong>. The <strong class="bold">Data Definition Language</strong> (<strong class="bold">DDL</strong>) of this table is as follows:</p>
<pre class="source-code">CREATE TABLE `dbmaster`.`bundle_events` (
  `customerid` INT NOT NULL,
  `bundleid` INT NOT NULL,
  `timestamp` VARCHAR(45) NOT NULL,
  `source` VARCHAR(45) NULL,
  PRIMARY KEY (`customerid`, `bundleid`, `timestamp`));</pre>
<p>Now, the NiFi pipeline polls the S3 bucket and checks for any change events, such as a JSON file being created or updated. Once a file is uploaded to the S3 bucket, NiFi should fetch the file, enrich it with the source type using the S3 object name, and then write the enriched data to the MySQL <strong class="source-inline">bundle_events</strong> table.</p>
<p>If you don’t have Apache NiFi installed in your system, download and install Apache NiFi-1.12.0. You can download, install, and follow the startup instructions at <a href="https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi">https://github.com/apache/nifi/blob/rel/nifi-1.12.0/nifi-docs/src/main/asciidoc/getting-started.adoc#downloading-and-installing-nifi</a> to do so. Alternatively, you can spin up a NiFi cluster/node on an AWS EC2 instance.</p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor137"/>Creating the NiFi pipeline</h2>
<p>Next, we will discuss how to<a id="_idIndexMarker889"/> build the NiFi pipeline. The NiFi pipeline<a id="_idIndexMarker890"/> that we will build is shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 8.2 – NiFi pipeline to read data from S3 and write to MySQL " height="755" src="image/B17084_08_002.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – NiFi pipeline to read data from S3 and write to MySQL</p>
<p>Let’s try to understand the overall <a id="_idIndexMarker891"/>NiFi pipeline and how we are building it: </p>
<ol>
<li value="1">First, we have used a <strong class="bold">ListS3</strong> processor to capture whether any S3 objects have been inserted or updated in the configured S3 bucket. It lists all the change events. </li>
<li>Then, using the <strong class="bold">SplitRecord</strong> processor, we split the records into individual events. </li>
<li>Using the <strong class="bold">EvaluateJsonPath</strong> processor, we create the <strong class="source-inline">sourceKey</strong> attribute for the FlowFile. </li>
<li>Then, we use <strong class="bold">FetchS3Object</strong> to fetch the S3 object. The <strong class="bold">FetchS3Oject</strong> processor is responsible for reading the actual S3 object. If <strong class="bold">FetchS3Object</strong> successfully reads the file, it sends the S3 object content as a FlowFile to the <strong class="bold">JoltTransformRecord</strong> processor. </li>
<li><strong class="bold">JoltTransformRecord</strong> is used to <a id="_idIndexMarker892"/>enrich the data before the enriched data is sent to be written to MySQL using the <strong class="bold">PutDatabaseRecord</strong> processor. </li>
<li>The success of the <strong class="bold">PutDatabaseRecord</strong> processor is sent to the <strong class="bold">LogSuccessMessage</strong> processor. As shown in the preceding screenshot, all FlowFiles in failure scenarios are sent to the <strong class="bold">LogErrorMessage</strong> processor.</li>
</ol>
<p>Now, let’s configure each of the<a id="_idIndexMarker893"/> NiFi processors present in the NiFi pipeline (shown in <em class="italic">Figure 8.2</em>):</p>
<ul>
<li><strong class="bold">ListS3 processor</strong>: The configuration <a id="_idIndexMarker894"/>of the <strong class="bold">ListS3</strong> processor is shown in the following screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer143">
<img alt="Figure 8.3 – Configuring the ListS3 processor " height="1012" src="image/B17084_08_003.jpg" width="1429"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Configuring the ListS3 processor</p>
<p>As we can see, the <strong class="bold">ListS3</strong> processor is configured to poll and listen for changes in the <strong class="bold">chapter8input</strong> S3 bucket. Apart from the bucket name, we must configure the <strong class="bold">Region</strong>, <strong class="bold">Access Key ID</strong>, and <strong class="bold">Secret Access Key</strong> details for the NiFi instance to connect to the AWS<a id="_idIndexMarker895"/> S3 bucket. Finally, we have configured the Record Writer property, which is set as a controller service of the <strong class="bold">JsonRecordSetWriter</strong> type.</p>
<ul>
<li><strong class="bold">SplitRecord processor</strong>: Next, we will <a id="_idIndexMarker896"/>configure the <strong class="bold">SplitRecord</strong> processor, as shown in the following<a id="_idIndexMarker897"/> screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer144">
<img alt="Figure 8.4 – Configuring the SplitRecord processor " height="680" src="image/B17084_08_004.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Configuring the SplitRecord processor</p>
<p><strong class="bold">SplitRecord</strong> is responsible for splitting a single FlowFile containing multiple write events in S3 into individual<a id="_idIndexMarker898"/> events. Now, each event is metadata for one <strong class="bold">S3Object</strong>. </p>
<ul>
<li><strong class="bold">EvaluateJsonPath processor</strong>: We use the <strong class="bold">EvaluateJsonPath</strong> processor to extract the value of the <strong class="bold">key</strong> column <a id="_idIndexMarker899"/>from the FlowFile content and add it as an attribute to the FlowFile attributes. The configuration of the <strong class="bold">EvaluateJsonPath</strong> processor is shown in the following screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="Figure 8.5 – Configuring the EvaluateJsonPath processor " height="752" src="image/B17084_08_005.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Configuring the EvaluateJsonPath processor</p>
<p>Here, we have configured the <strong class="bold">Destination</strong> property with the <strong class="source-inline">flowfile-attribute</strong> value. This indicates that a new key-value pair will be added to the FlowFile attributes. The <a id="_idIndexMarker900"/>attribute to be added should be provided as a dynamic property, where the name<a id="_idIndexMarker901"/> of the property will be the attribute key and the value of the property will be the attribute value. Here, we have added a property<a id="_idIndexMarker902"/> named <strong class="bold">sourcekey</strong>. The value of this property is the NiFi expression <strong class="source-inline">$.key</strong>. This expression fetches the value of the <strong class="source-inline">key</strong> field from the FlowFile content (which is a JSON). </p>
<ul>
<li><strong class="bold">FetchS3Object processor</strong>: The following <a id="_idIndexMarker903"/>screenshot shows the configuration of the <strong class="bold">FetchS3Object</strong> processor:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer146">
<img alt="Figure 8.6 – Configuring the FetchS3Object processor " height="1063" src="image/B17084_08_006.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Configuring the FetchS3Object processor</p>
<p>As we can see, the <strong class="bold">FetchS3Object</strong> configuration uses the newly added <strong class="bold">sourcekey</strong> attribute. <strong class="source-inline">${sourcekey}</strong> is a NiFi<a id="_idIndexMarker904"/> expression that gets the value of the <strong class="bold">sourcekey</strong> attribute. Apart from this, the S3-related properties such as <strong class="bold">Bucket</strong>, <strong class="bold">Access Key ID</strong>, and <strong class="bold">Secret Access Key</strong> need to be set in this processor. The output of this<a id="_idIndexMarker905"/> processor is the content of our <strong class="bold">S3Object</strong> (which is in JSON format). </p>
<ul>
<li><strong class="bold">JoltTransformRecord processor</strong>: The <strong class="bold">JoltTransform</strong> processor is used to add a new key-value pair to the JSON. The <a id="_idIndexMarker906"/>configuration of the <strong class="bold">JoltTransformRecord</strong> processor is as follows:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="Figure 8.7 – Configuring the JoltTransformRecord processor " height="723" src="image/B17084_08_007.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Configuring the JoltTransformRecord processor</p>
<p>The <strong class="bold">JoltTransformRecord</strong> processor uses Apache Jolt to do JSON transformations. Jolt <a id="_idIndexMarker907"/>takes a JSON called <strong class="bold">Jolt Specification</strong> to transform one JSON into another. In a <strong class="bold">Jolt Specification</strong>, multiple types of operations can be performed, such as <strong class="source-inline">shift</strong>, <strong class="source-inline">chain</strong>, and <strong class="source-inline">default</strong>. For more information about Apache Jolt, go to <a href="https://github.com/bazaarvoice/jolt#jolt">https://github.com/bazaarvoice/jolt#jolt</a>.</p>
<p>Here, we will discuss the Jolt transformation we are using to add a key-value pair to the JSON FlowFile content. We add the <strong class="source-inline">source</strong> key using the <strong class="bold">Jolt Specification</strong> property, as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="Figure 8.8 – Using the Jolt Specification property to add the source key " height="524" src="image/B17084_08_008.jpg" width="1140"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Using the Jolt Specification property to add the source key</p>
<p>As you can see, by setting <strong class="source-inline">operation</strong> to <strong class="source-inline">default</strong> in <strong class="bold">Jolt Specification</strong>, you can add a new key-value pair to <a id="_idIndexMarker908"/>an input JSON. Here, we are adding a <strong class="source-inline">source</strong> key with the dynamic value calculated using the NiFi expression language. <strong class="source-inline">${sourcekey:substringBefore('_')}</strong> is a NiFi expression. This expression returns a substring of the <strong class="source-inline">FlowFile</strong> attribute’s <strong class="bold">sourcekey</strong>, from the beginning of the string till the character before the first occurrence of an underscore (<strong class="source-inline">'_'</strong>). </p>
<ul>
<li><strong class="bold">PutDatabaseRecord processor</strong>: Once this JSON is enriched with the newly added key-value pair, the record is<a id="_idIndexMarker909"/> written to the <a id="_idIndexMarker910"/>MySQL database using the <strong class="bold">PutDatabaseRecord</strong> processor. The following screenshot shows the configuration of the <strong class="bold">PutDatabaseRecord</strong> processor:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer149">
<img alt="Figure 8.9 – Configuring the PutDatabaseRecord processor " height="1035" src="image/B17084_08_009.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Configuring the PutDatabaseRecord processor</p>
<p>As evident from this <a id="_idIndexMarker911"/>configuration, we need to configure two controller services – <strong class="bold">Record Reader</strong> and <strong class="bold">Database Connection Pooling Service</strong>. We configure a simple <strong class="bold">JsonTreeReader</strong> service for <strong class="bold">Record Reader</strong> and a <strong class="bold">DBCPConnectionPool</strong> service for <strong class="bold">Database Connection Pooling Service</strong>. Apart from that, we also need to set <strong class="bold">Database Type</strong> to <strong class="source-inline">MySQL</strong>, <strong class="bold">Statement Type</strong> to <strong class="source-inline">INSERT</strong>, and <strong class="bold">Table Name</strong> to <strong class="source-inline">bundle_events</strong>. Now, let’s see how the <strong class="bold">DBCPConnectionPool</strong> service, called <strong class="bold">MysqlDBCPConnectionPool</strong>, is configured. The following <a id="_idIndexMarker912"/>screenshot shows the configuration of the <strong class="bold">MysqlDBCPConnectionPool</strong> service:</p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="Figure 8.10 – Configuring the DBCPConnectionPool service " height="929" src="image/B17084_08_010.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Configuring the DBCPConnectionPool service</p>
<p>The <strong class="bold">MysqlDBCPConnectionPool</strong> controller service is used to create a JDBC connection pool. To do <a id="_idIndexMarker913"/>so, you must configure the <strong class="bold">Database Connection URL</strong> (the JDBC URL), <strong class="bold">Database Driver Class Name</strong>, and <strong class="bold">Database Driver Location(s)</strong> properties, as shown in the preceding screenshot.</p>
<p>With that, we have built the entire NiFi pipeline to extract data from S3, enrich it, and write it to a MySQL table. You can check out the entire NiFi pipeline by going to <a href="https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml">https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter08/sourcecode/nifi_s3ToMysql.xml</a>.</p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor138"/>Setting up DataHub</h2>
<p>Now, we will add a data<a id="_idIndexMarker914"/> governance layer to our solution. Although <a id="_idIndexMarker915"/>many data governance tools are available, most of them are paid tools. There are several data governance tools available for pay-as-you-go models in the cloud, such as AWS Glue Catalog. However, since our solution will be running on-premises, we will choose a platform-agnostic open source tool. DataHub, developed by LinkedIn, is one such open source tool that comes with a decent set of features. We will use it in this chapter to explain data governance practically. In this section, we will learn how to configure DataHub. In this pipeline, S3 is the source, MySQL is the target, and NiFi is the processing engine. To create data governance, we need to connect to all these systems and extract metadata from them. DataHub does this for us.</p>
<p>Before we begin connecting<a id="_idIndexMarker916"/> these components (of the data pipeline) to DataHub, we need to download and install DataHub in our local Docker environment. The detailed installation <a id="_idIndexMarker917"/>instructions can be found at <a href="https://datahubproject.io/docs/quickstart">https://datahubproject.io/docs/quickstart</a>.</p>
<p>In the next section, we will learn how to connect different data sources to DataHub.</p>
<h3>Adding an ingestion source to DataHub</h3>
<p>To connect a data source or <a id="_idIndexMarker918"/>pipeline component to DataHub, we <a id="_idIndexMarker919"/>must go to the <strong class="bold">Ingestion</strong> tab from the top-right menu bar, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 8.11 – Connecting a new source for metadata ingestion in DataHub " height="578" src="image/B17084_08_011.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Connecting a new source for metadata ingestion in DataHub</p>
<p>Then, we must click the <strong class="bold">Create new source</strong> button to create a new connection to a data source. On clicking<a id="_idIndexMarker920"/> this button, we get the following popup:</p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 8.12 – Choosing the type of data source to create a new connection " height="768" src="image/B17084_08_012.jpg" width="900"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Choosing the type of data source to create a new connection</p>
<p>This popup is a multipage wizard where you can create a new data source connection. First, as shown in the preceding screenshot, you are asked to choose a data source type. For example, if the <a id="_idIndexMarker921"/>data is in a MySQL database, then we would select<a id="_idIndexMarker922"/> MySQL from the various thumbnails present in the wizard. For sources, which are not listed, such as NiFi and S3, we can select the <strong class="bold">Custom</strong> thumbnail. Once you’ve chosen a type, click <strong class="bold">Next</strong>; you will be taken to the second step called <strong class="bold">Configure Recipe</strong>, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 8.13 – Configuring a recipe for the data source " height="518" src="image/B17084_08_013.jpg" width="781"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Configuring a recipe for the data source</p>
<p>In the <strong class="bold">Configure Recipe</strong> step, we have to add some <strong class="source-inline">YAML</strong> code in the space provided. Irrespective of the data source, the YAML has two top-level elements – <strong class="source-inline">source</strong> and <strong class="source-inline">sink</strong>. Again, both <strong class="source-inline">source</strong> and <strong class="source-inline">sink</strong> contain two elements – <strong class="source-inline">type</strong> and <strong class="source-inline">config</strong>. Here, <strong class="source-inline">type</strong> denotes the type of source or sink. For example, in the preceding screenshot, we are configuring NiFi, so the source type is <strong class="source-inline">nifi</strong>. If we had selected MySQL as the type of connection<a id="_idIndexMarker923"/> in the previous step, the source’s <strong class="source-inline">type</strong> in this step would be <strong class="source-inline">mysql</strong>. </p>
<p>There are primarily three types of sinks that are currently available in DataHub, as follows:</p>
<ul>
<li><strong class="bold">Console</strong>: The metadata is <a id="_idIndexMarker924"/>sent to <strong class="source-inline">stdout</strong></li>
<li><strong class="bold">DataHub</strong>: The metadata is sent to DataHub via the GMS REST API</li>
<li><strong class="bold">File</strong>: The metadata is sent to a<a id="_idIndexMarker925"/> configured file</li>
</ul>
<p>Here, we will be using DataHub as the sink since this allows us to do governance and monitoring using DataHub’s capabilities. In this case, our type of sink will be <strong class="source-inline">datahub-rest</strong>. We also<a id="_idIndexMarker926"/> need to specify the GMS REST API’s HTTP address as the base<a id="_idIndexMarker927"/> path. Since our DataHub installation is using Docker, we will use <strong class="source-inline">localhost:9002</strong> as the server IP address (as shown in <em class="italic">Figure 8.13</em>).</p>
<p>Once everything has been added, click <strong class="bold">Next</strong> to go to the <strong class="bold">Schedule Execution</strong> step, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 8.14 – Create an execution schedule " height="1105" src="image/B17084_08_014.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Create an execution schedule</p>
<p>In this step, we set an execution schedule for DataHub to fetch metadata from the source. In this case, we<a id="_idIndexMarker928"/> configured a CRON expression, <strong class="source-inline">0,30 * * * *</strong>, which means it will run every 30 minutes.</p>
<p>Click <strong class="bold">Next</strong> to go to the last step, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 8.15 – The final step of the New Ingestion Source wizard " height="738" src="image/B17084_08_015.jpg" width="1584"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – The final step of the New Ingestion Source wizard</p>
<p>In the final step of the wizard, we<a id="_idIndexMarker929"/> need to enter the name of the ingestion source. This name can be any arbitrary name that helps uniquely identify that source. Finally, we must click <strong class="bold">Done</strong> to add the new data source. The following screenshot shows the added sources:</p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 8.16 – Sources added at a glance " height="575" src="image/B17084_08_016.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Sources added at a glance</p>
<p>As shown in the preceding screenshot, we can monitor how many times a job has run to fetch metadata from<a id="_idIndexMarker930"/> each source and whether the last run was successful or not. All the YAML files that have been used for our use case can be found on GitHub at <a href="https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08">https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter08</a>.</p>
<p>In the next section, we will discuss how we can perform different data governance tasks using this tool.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor139"/>Governance activities</h2>
<p>As discussed in the preceding section, once a data source or pipeline is connected with DataHub, it provides a lot of tools to<a id="_idIndexMarker931"/> enable a data governance model around those sources. The following are a few capabilities that we will explore in this chapter:</p>
<ul>
<li><strong class="bold">Adding a domain</strong>: A domain in DataHub can be any logical grouping; it might be organization-specific. This<a id="_idIndexMarker932"/> helps us analyze the domain-wise resource utilization and other statistics. For example, an organization can use business unit names as domains. In our particular scenario, <strong class="bold">DataServices</strong> is a business unit in the organization, and we create a domain in the name of the business unit. To create a new domain, we can navigate to <strong class="bold">Manage</strong> | <strong class="bold">Domains</strong> and click the <strong class="bold">New Domain</strong> button. Once this button is clicked, a popup dialog will open, as shown in the following screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 8.17 – Creating a new domain " height="981" src="image/B17084_08_017.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Creating a new domain</p>
<p>As we can see, we must <a id="_idIndexMarker933"/>provide a name and description for a domain. You cannot <a id="_idIndexMarker934"/>create two domains with the same name.</p>
<ul>
<li><strong class="bold">Adding a user group</strong>: We can<a id="_idIndexMarker935"/> manage users and groups by going to <strong class="bold">Settings</strong> and selecting the <strong class="bold">Groups</strong> tab. Upon clicking the <strong class="bold">Create group</strong> button, a dialog similar to the following will appear:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer158">
<img alt="Figure 8.18 – Adding a new user group " height="816" src="image/B17084_08_018.jpg" width="1019"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Adding a new user group</p>
<p>As we can see, we must <a id="_idIndexMarker936"/>provide a group name and an optional description. User groups help make any <a id="_idIndexMarker937"/>user part of a group, and accountability, ownership, access policies and rules can be assigned to a group.</p>
<ul>
<li><strong class="bold">Exploring metadata</strong>: This activity in data<a id="_idIndexMarker938"/> governance comes under data definition. Most data governance tools support metadata management. Here, as shown in the following screenshot, the DataHub dashboard provides a summary of the metadata:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="Figure 8.19 – Platform-wise and domain-wise resource summary " height="691" src="image/B17084_08_019.jpg" width="900"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Platform-wise and domain-wise resource summary</p>
<p>The preceding screenshot <a id="_idIndexMarker939"/>shows the various platforms and objects present on each platform. It also shows how many resources are in each domain. For enterprise data<a id="_idIndexMarker940"/> governance, it is important to monitor and understand how many resources are being used by different business units for auditing, tracking, and financial purposes. The following screenshot shows a MySQL table resource and its corresponding data definitions:</p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="Figure 8.20 – Adding descriptions to metadata " height="442" src="image/B17084_08_020.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – Adding descriptions to metadata</p>
<p>As we can see, we can add a description to the metadata. On the other hand, the following screenshot shows how the schema of the table is loaded and seen in DataHub:</p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="Figure 8.21 – Schema of the table  " height="732" src="image/B17084_08_021.jpg" width="1634"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Schema of the table </p>
<p>As we can see, descriptions, terms, and tags can be added and maintained in each column of the <a id="_idIndexMarker941"/>schema. This enables data definition activities for data<a id="_idIndexMarker942"/> governance.</p>
<ul>
<li><strong class="bold">Exploring lineage</strong>: DataHub allows us to explore the lineage of the data. In the data definition of data <a id="_idIndexMarker943"/>governance, apart from maintaining the metadata and descriptions, proper governance must know the origin of the data and how it is being used. This aspect is covered by <strong class="bold">Lineage</strong>, as shown in the following screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="Figure 8.22 – Data lineage " height="880" src="image/B17084_08_022.jpg" width="1439"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – Data lineage</p>
<p>Here, we can see <strong class="bold">1 Downstream</strong> and no upstream events for the <strong class="bold">chapter8input</strong> S3 bucket. We can click on the <strong class="bold">Visualize Lineage</strong> button to see the lineage as a diagram, as<a id="_idIndexMarker944"/> shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="Figure 8.23 – Visualizing data lineage as a workflow " height="516" src="image/B17084_08_023.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – Visualizing data lineage as a workflow</p>
<p>Apart from visual<a id="_idIndexMarker945"/> lineage, you can explore the impact analysis of this resource. This impact analysis helps communicate with the impacted groups in case a change or maintenance event occurs.</p>
<ul>
<li><strong class="bold">Creating accountability by adding a domain and owners to a resource</strong>: We can start creating accountability of resources by attaching a domain and one or multiple <a id="_idIndexMarker946"/>owners to a resource from the landing page of the resource, as shown in the following screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="Figure 8.24 – Assigning a domain and owners " height="814" src="image/B17084_08_024.jpg" width="1560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Assigning a domain and owners</p>
<p>An owner can be a group<a id="_idIndexMarker947"/> or a user. However, it is advisable to assign ownership to a group and add users to that group. When adding an owner, you can choose the type of ownership, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<img alt="Figure 8.25 – Adding an owner " height="761" src="image/B17084_08_025.jpg" width="1032"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Adding an owner</p>
<p>Here, while adding the <strong class="bold">offer_analytics</strong> owner, we set the type of owner to <strong class="bold">Technical Owner</strong>. An owner can be one of three types – <strong class="bold">Technical Owner</strong>, <strong class="bold">Business Owner</strong>, or <strong class="bold">Data Steward</strong>. <strong class="bold">Technical Owner</strong> is accountable for producing, maintaining, and distributing the asset. <strong class="bold">Business Owner</strong> is a domain expert associated <a id="_idIndexMarker948"/>with the asset. Finally, <strong class="bold">Data Steward</strong> is accountable for the governance of the asset.</p>
<ul>
<li><strong class="bold">Setting policies</strong>: DataHub allows<a id="_idIndexMarker949"/> us to set policies. Policies are sets of rules that define privileges over an asset or the DataHub platform. There are two categories of policies – <strong class="bold">Platform</strong> and <strong class="bold">Metadata</strong>. The <strong class="bold">Platform</strong> policy allows us to assign DataHub platform-level privileges to users or groups, while the <strong class="bold">Metadata</strong> policy allows us to assign metadata privileges to users or groups. To create a new policy, navigate to <strong class="bold">Settings</strong> | <strong class="bold">Privileges</strong>. Upon clicking the <strong class="bold">Create New Policy</strong> button, a wizard appears, as shown in the following screenshot:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer166">
<img alt="Figure 8.26 – Creating a View_Analytics policy " height="1298" src="image/B17084_08_026.jpg" width="1493"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – Creating a View_Analytics policy</p>
<p>Here we are creating a<a id="_idIndexMarker950"/> new policy called <strong class="bold">View_Analytics</strong>. We<a id="_idIndexMarker951"/> have chosen <strong class="bold">Platform</strong> as the type of policy. Optionally, we can add a description of the policy. Here, we have added a description that states <strong class="bold">This policy is for viewing analytics only</strong>. </p>
<p>Click <strong class="bold">Next</strong> to go to the <strong class="bold">Configure Privileges</strong> section, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="Figure 8.27 – Configuring policy privileges " height="588" src="image/B17084_08_027.jpg" width="1350"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27 – Configuring policy privileges</p>
<p>Here, we are<a id="_idIndexMarker952"/> configuring the platform privileges for the policy that we are defining. Finally, we must select/specify the users or groups that this<a id="_idIndexMarker953"/> policy will be applied to. The following screenshot shows how we can assign users or groups to a policy:</p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="Figure 8.28 – Assigning users/groups to a policy " height="938" src="image/B17084_08_028.jpg" width="1356"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.28 – Assigning users/groups to a policy</p>
<p>As we can see, the <strong class="bold">View_Analytics</strong> policy is assigned to the <strong class="bold">executives</strong> group. </p>
<ul>
<li><strong class="bold">Visual analytics</strong>: DataHub <a id="_idIndexMarker954"/>also allows users to create and save dashboards for <a id="_idIndexMarker955"/>the analytics required for the data governance of an organization. The following screenshot shows various metrics, such as the most viewed datasets and data governance completeness (how much of the documentation/lineage/schema is well defined) for different entity types:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="Figure 8.29 – Various visual analytics metrics " height="788" src="image/B17084_08_029.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.29 – Various visual analytics metrics</p>
<p>The following screenshot shows some other dashboard charts where we can find the platforms that were used per domain and their count. We can also see the number of entities (datasets or pipeline components) per platform:</p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<img alt="Figure 8.30 – Data Landscape Summary " height="805" src="image/B17084_08_030.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.30 – Data Landscape Summary</p>
<p>Now that we have<a id="_idIndexMarker956"/> discussed and learned about the concepts of data governance <a id="_idIndexMarker957"/>and have practically implemented data governance around a use case, let’s learn about data security.</p>
<h1 id="_idParaDest-139"><a id="_idTextAnchor140"/>Understanding the need for data security</h1>
<p>Before we understand the need for data security, let’s try to<a id="_idIndexMarker958"/> define what data security is. <strong class="bold">Data security</strong> is the process of protecting enterprise data and preventing any data loss from malicious or unauthorized access to data. Data security includes the following tasks:</p>
<ul>
<li>Protecting sensitive data from attacks.</li>
<li>Protecting data and applications from any ransomware attacks.</li>
<li>Protecting against any attacks that can delete, modify, or corrupt corporate data.</li>
<li>Allowing access and control of data to the necessary user within the organization. Again, read-only, write, and delete access is provided to the data based on the role and its use.</li>
</ul>
<p>Some industries may have stringent data security requirements. For example, a US health insurance company needs to ensure PHI data is extremely well protected according to HIPAA standards. Another example is that a financial firm such as Bank Of America has to ensure card and account data is extremely secure because that can cause direct monetary loss. But even if there is no stringent data security requirement in your organization, data <a id="_idIndexMarker959"/>security is essential to prevent data loss and loss of trust of the organization’s customers.</p>
<p>Although data privacy and data security are overlapping terms, it helps to understand the subtle difference between the two. Data privacy ensures that only an authenticated user can access the data, and even if the data is accessed somehow, it should be encrypted or tokenized so that unauthorized users cannot use the data. Data security includes data privacy concerns, but apart from that, it focuses on tackling any malicious activity concerning the data. One simple example is that, to protect PHI data, Cigna Health insurance has encrypted all its sensitive data. This ensures data privacy but not data security. Although hackers may not be able to decipher the encrypted data, they can still delete the data or double encrypt the data so that the data becomes unusable.</p>
<p>Now, let’s discuss why data security is needed.</p>
<p>Every year, the damage that’s caused by data breaches is around $8 billion in the US alone and, on average, each incident causes approximately 25,000 accounts to be compromised. Let’s explore a few of the biggest data breaches that have happened over the last few years:</p>
<ul>
<li>In June 2013, Capital One reported a data breach that affected all 106 million of its accounts. Personal information such as credit history, social security numbers, and bank accounts was compromised.</li>
<li>In June 2021, a massive data breach happened on LinkedIn, wherein 700 million accounts (92% of the total accounts at that time) were affected. User data was posted for sale on the dark web.</li>
<li>In April 2019, Facebook reported an attack that compromised 533 million accounts.</li>
</ul>
<p>As we can see, these breaches not only pose security risks to the compromised data but also damage the reputation and trust <a id="_idIndexMarker960"/>of the company. Now, let’s discuss a few common data security threats, as follows:</p>
<ul>
<li><strong class="bold">Phishing and other social engineering attacks</strong>: Social engineering is a common way to trick or manipulate individuals to gain unauthorized access to corporate data or gather confidential information. Phishing is a common form of social engineering<a id="_idIndexMarker961"/> attack. Here, messages <a id="_idIndexMarker962"/>or emails that appear to be from a trusted source are sent to individuals. They contain malicious links that, when clicked, can give unauthorized access to corporate networks and data.</li>
<li><strong class="bold">Insider threats</strong>: Insider threats are<a id="_idIndexMarker963"/> caused by an employee who inadvertently or intentionally threatens the security of corporate data. They can be of the following types:<ul><li>Malicious actors, who intentionally steal data or cause harm to the organization for personal gain.</li>
<li>Non-malicious actors, who cause threats accidentally or because they are unaware of security standards.</li>
<li>Compromised users, whose systems are compromised by external attackers without their knowledge. Then, attackers perform malicious activities, pretending to be legitimate users.</li>
</ul></li>
<li><strong class="bold">Ransomware</strong>: This is a form of <a id="_idIndexMarker964"/>malware that infects corporate systems and encrypts data so that it becomes useless without a decryption key. Attackers then display a ransom message for payment to release the decryption key.</li>
<li><strong class="bold">SQL injection</strong>: Here, attackers<a id="_idIndexMarker965"/> gain access to databases and their data. Then, they inject unwanted code into seemingly innocent SQL queries to perform unwanted operations, thereby either deleting or corrupting the data.</li>
<li><strong class="bold">Distributed Denial of Service (DDoS) attacks</strong>: A DDoS attack is a malicious attempt to disrupt a web resource such as a web service. It acts by sending a huge volume of dummy calls to<a id="_idIndexMarker966"/> the web server. Due to the extreme volume of service calls at a very short burst, the web server crashes or becomes unresponsive, causing web resource downtime.</li>
<li><strong class="bold">Compromised data in the cloud</strong>: Ensuring security in the cloud is a big challenge. Security is required <a id="_idIndexMarker967"/>while sending data over the network and for data at rest in the cloud.</li>
</ul>
<p>In this section, we discussed <a id="_idIndexMarker968"/>what data security is and the threats that can occur in absence of it by covering some real-world examples. Now, let’s discuss the solutions and tools available for data security.</p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor141"/>Solution and tools available for data security</h1>
<p>In the previous section, we briefly discussed what data security is and why it is needed. We also looked at a few common data security threats. The solutions and tools described here help mitigate or minimize the risk from the threats discussed in the previous section:</p>
<ul>
<li><strong class="bold">Data discovery and classification</strong>: To ensure data security, it is important to discover sensitive information. This<a id="_idIndexMarker969"/> technique uses data discovery to classify data into various security labels (such as confidential, public, and so on). Once classification is done, security policies can be applied to the various classifications of data according to the organization’s needs.</li>
<li><strong class="bold">Firewalls</strong>: This is the first line of defense against any network intrusions. They exclude any undesirable traffic from entering the network. They also help open specific ports to the external network, which gives hackers less of a chance to enter the network.</li>
<li><strong class="bold">Backup and recovery</strong>: This helps organizations protect themselves in case the data is deleted or destroyed.</li>
<li><strong class="bold">Antivirus</strong>: This is used to detect any viruses, Trojans, and rootkits that can steal, modify, or damage your data. They are widely used for personal and corporate data security.</li>
<li><strong class="bold">Intrusion Detection and Prevention Systems</strong> (<strong class="bold">IDS/IPS</strong>): IDS/IPS performs a deep inspection of <a id="_idIndexMarker970"/>packets on network traffic and logs any <a id="_idIndexMarker971"/>malicious activities. These tools help stop DDoS attacks.</li>
<li><strong class="bold">Security Information and Event Management</strong> (<strong class="bold">SIEM</strong>): This analyzes recorded logs from various kinds of <a id="_idIndexMarker972"/>devices such as network devices, servers, and applications, and generates security alert events based on specific criteria and thresholds.</li>
<li><strong class="bold">Data Loss Prevention</strong> (<strong class="bold">DLP</strong>): This <a id="_idIndexMarker973"/>mechanism monitors different devices to make sure sensitive data is not copied, moved, or deleted without proper authorization. It also monitors and logs who is using the data.</li>
<li><strong class="bold">Access control</strong>: This technique allows or denies read, write, or delete access to individual data resources. Access<a id="_idIndexMarker974"/> controls can be implemented using an <strong class="bold">Access Control List</strong> (<strong class="bold">ACL</strong>) or <strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>). Cloud <a id="_idIndexMarker975"/>security systems such are IAM are used to enforce access control on the cloud.</li>
<li><strong class="bold">Security as a Service</strong>: This is modeled on the lines of Software-as-a-Service. Here, service providers provide the <a id="_idIndexMarker976"/>security of corporate infrastructure on a subscription basis. It uses a pay-as-you-go model.</li>
<li><strong class="bold">Data encryption</strong>: Data such as PHI can be very sensitive. If it is lost or leaked, this can cause regulatory issues and<a id="_idIndexMarker977"/> heavy financial losses. To provide data protection on such data, we can either encrypt the data (where the masked data loses all the original data properties, such as size) or tokenize it (where the masked data retains the properties of the original data).</li>
<li><strong class="bold">Physical security</strong>: Finally, physical security is essential to stop unauthorized access to data. Physical security<a id="_idIndexMarker978"/> can be enabled by creating strong security policies and implementing them. Policies such as locking the system whenever you step out, no tailgating, and others can help prevent social engineering attacks and unauthorized access to data.</li>
</ul>
<p>In this section, we learned <a id="_idIndexMarker979"/>about the data security solutions and tools that are available in the industry. Now, let’s summarize what we’ve learned in this chapter.</p>
<h1 id="_idParaDest-141"><a id="_idTextAnchor142"/>Summary</h1>
<p>In this chapter, we learned what data governance is and why is it needed. Then, we briefly discussed the data governance framework. After that, we discussed how to develop a practical data governance solution using DataHub. Next, we learned what data security is and why it is needed. Finally, we briefly discussed the various solutions and tools that are available for ensuring data security.</p>
<p>With that, we have learned how to ingest data using real-time and batch-based pipelines, popular architectural patterns for data ingestion, and data governance and data security. In the next chapter, we will discuss how to publish the data as a service for downstream systems.</p>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer172">
<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/>Section 3 – Enabling Data as a Service</h1>
<p>This section of the book focuses on architecting solutions for Data as a Service. In this part, you will learn how to build various kinds of Enterprise grade <strong class="bold">Data as a Service</strong> (<strong class="bold">DaaS</strong>) solutions and secure and manage them properly. </p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B17084_09.xhtml#_idTextAnchor144"><em class="italic">Chapter 9</em></a><em class="italic">, Exposing MongoDB Data as a Service</em> </li>
<li><a href="B17084_10.xhtml#_idTextAnchor157"><em class="italic">Chapter 10</em></a><em class="italic">, Federated and Scalable DaaS with GraphQL</em></li>
</ul>
</div>
<div>
<div id="_idContainer173">
</div>
</div>
</div>
</body></html>