- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Architecting a Batch Processing Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to architect medium- to low-volume batch-based
    solutions using Spring Batch. We also learned how to profile such data using DataCleaner.
    However, with data growth becoming exponential, most companies have to deal with
    huge volumes of data and analyze it to their advantage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to analyze, profile, and architect a big
    data solution for a batch-based pipeline. Here, we will learn how to choose the
    technology stack and design a data pipeline to create an optimized and cost-efficient
    big data solution. We will also learn how to implement this solution using Java,
    Spark, and various AWS components and test our solution. After that, we will discuss
    how to optimize the solution to be more time and cost-efficient. By the end of
    this chapter, you will know how to architect and implement a data analysis pipeline
    in AWS using S3, Apache Spark (Java), AWS EMR, AWS Lambda, and AWS Athena. You
    will also know how to fine-tune the code for optimized performance, as well as
    how to plan and optimize the cost of implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing the architecture and choosing the right tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the ODL using AWS Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along with this chapter, you’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prior knowledge of Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior knowledge of the basics of Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java 1.8 or above installed on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ Idea community or ultimate edition installed on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found in this book’s GitHub repository: [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05).'
  prefs: []
  type: TYPE_NORMAL
- en: Developing the architecture and choosing the right tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data engineering, after the data has been successfully ingested and stored
    in a data lake or a data warehouse, often, it needs to be mined and stored for
    specific needs in a more sorted and customized form for reporting and analysis.
    In this chapter, we will discuss such a problem where a huge volume of data needs
    to be analyzed and stored in a more customized format for a specific downstream
    audience.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s assume that an e-commerce firm, ABC, wants to analyze various kinds of
    user interaction on its products and determine the top-selling products for each
    category each month. They want to provide incentives to the top-selling products
    in each category. They also want to provide special offers and marketing promotion
    tools to products with top view-to-sale ratios but are not the top-selling products.
    In addition, they want to market seller tools and training, as well as marketing
    services, to the team with the lowest-selling product in each category. Currently,
    ABC stores all user transactions in its transactional databases for a product,
    but there is no monthly data view where information about the top seller, worst
    seller, and the top view-to-sale ratio is available. They want an **Organized
    Data Layer** (**ODL**) to be created so that such analytical queries can easily
    be performed with optimum performance every month.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s analyze the given problem. First, let’s analyze the requirements in terms
    of the four dimensions of data.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will try to answer the question, *what is the velocity of the data?* Here,
    as evident from the requirements, we need to create an ODL with monthly analyzed
    data. Hence, our data will be used after a month, so we have no real-time data
    processing requirement. So, we can safely assume that we are dealing with a batch
    processing problem. However, it will be helpful to know how frequently the data
    arrives, which will help us determine at what frequency we can schedule a batch
    job. So, we must ask the e-commerce firm, *how frequently will source data be
    provided to us?* ABC tells us that the source data will be dropped to us as CSV
    files on a monthly or bi-monthly basis, but never twice daily. This information
    is helpful to us, but that brings other questions to mind.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the most obvious next question that comes to our mind is, *how big is the
    data/file that will be shared once or twice monthly?* Considering that each record
    will be an event on each transaction that any user has made on any product in
    the e-commerce marketplace, the data is likely to be huge. ABC tells us that transactions
    can be either view, cart, or purchase transactions. So, for each action, such
    as viewing a product, adding to the cart, and purchasing a product, there will
    be separate entries in the file. ABC also tells us that the number of products
    and categories is likely to increase in the future. From our guestimate and ABC’s
    internal data, each file sent to us can vary from hundreds of gigabytes to terabytes
    of data. The data that needs to be processed is in the range of hundreds of gigabytes
    and terabytes, which is ideal for big data processing. Our analysis also tells
    that the e-commerce traffic is going to increase over time. These observations
    indicate that this is a big data problem. So, we need to develop a big data solution
    to solve this batch processing problem.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will look at the *variety of the data*. From our previous discussion,
    we know that data is arriving in CSV format. So, this is structured data that
    needs to be analyzed and processed. We will hold the discussion on the variety
    of data for a while as we will be taking that up during the implementation phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next decision that we must make, as architects, is to choose the right
    platform. Should we run this application on-premise or in the cloud? There are
    pros and cons to both. However, there are a few vital points regarding why the
    cloud may be a better choice in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost saving**: Running a big data job with terabytes of data will require
    a very good big data infrastructure to run on-premise. However, these jobs will
    only run once or twice a month. If we choose to create an on-premise environment,
    it doesn’t make sense to spend so many dollars creating a clustered Hadoop infrastructure
    that will only be used once or twice a month, but where the infrastructure needs
    to be maintained and running at all times. The amount of cost and effort involved
    in creating and maintaining such an infrastructure doesn’t justify the utilization.
    This will be much cheaper on the cloud, where you pay only for the resources you
    utilize during the job run. The cloud can give you the choice to only pay for
    what you use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in the cloud, you can choose to keep your Hadoop environment running
    for only 2 hours daily; this way, you only pay for those 2 hours and not for the
    whole day. More importantly, it supports elasticity, which means you can auto-scale
    your number of nodes to a higher or lower number based on your usage. This gives
    you the flexibility to use only the required resource each time. For example,
    if we know that the data (which needs to be processed) will be huge in November
    and the jobs will take up more resources and time in November, we can increase
    the resource capacity for November and bring it down when the volume reduces to
    a normal level. Such capabilities of cloud technologies enable huge cost savings
    on the overall execution of the system (especially **capital expenditure** (**CapEx**)
    costs).
  prefs: []
  type: TYPE_NORMAL
- en: '**Seasonal variation in workloads**: Usually, in an e-commerce site, the activity
    during the holiday season or festival season is high, while at other times, the
    activity is low. User activity directly impacts the size of the file for that
    month. So, we must be able to scale the infrastructure up and down as we need.
    This can easily be achieved in the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future elasticity**: As one of the requirements clearly states that the number
    of products and categories is likely to increase, this means we will need to scale
    up both processing and storage capacities in the future. While such changes require
    a lot of time and resources in on-premise environments, this can easily be achieved
    in the cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of sensitive data**: There is no specific federal or **Protected Health
    Information** (**PHI**) data involved in our use case that needs to be encrypted
    or tokenized before it is stored in the cloud. So, we should be good with legal
    and data security requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we can choose any public cloud platform, for our convenience, we will
    use AWS as our cloud platform in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Architecting the solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have gathered and analyzed our requirements, let’s try building
    the architecture for the solution. To architect this solution, we need to answer
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Where should we store or land the input data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should we process the input data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where and how should we store the output data/ODL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should we provide a querying interface to the ODL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How and when should we schedule a processing job?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see what options we have for storing the input data. We can store the
    data in one of the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S3**: **S3** or **Simple Storage Service** is a very popular object storage
    service. It is also cheap and very reliable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EMR/EC2 attached EBS volumes**: **Elastic Block Storage** (**EBS**) is a
    block storage solution where storage volumes can be attached to any virtual server,
    such as an EC2 instance. For a big data solution, if you use **Elastic Map Reduce**
    (**EMR**), EBS volumes can be attached to each participating EC2 node in that
    EMR cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic File System (EFS)**: EFS is a shared filesystem that’s often attached
    to a NAS server. It is usually used for content repositories, media stores, or
    user home directories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss the different factors to consider before choosing your storage.
  prefs: []
  type: TYPE_NORMAL
- en: Factors that affect your choice of storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cost is an important factor that we need to consider when choosing any cloud
    component. However, let’s look at factors other than cost that affect our choice
    of storage. These factors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance**: Both EBS and EFS can perform faster than S3 in terms of IOPS.
    Although performance is slower in S3, it’s not significantly slower to read the
    data from other storage options. From a performance perspective, an EFS or EBS
    volume will still be preferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Although all three storage options are scalable, S3 has the
    most seamless scalability without any manual effort or interruption. Since scalability
    is one of our important needs as our data grows over time and there is a possibility
    of bigger file sizes in the future (according to the requirements), from this
    perspective, S3 is a clear winner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Life cycle management**: Both S3 and EFS have life cycle management features.
    Suppose you believe that older files (older than a year) need to be archived;
    these programs can seamlessly move to another cheaper storage class, which provides
    seamless archival storage as well as cost savings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless architecture support**: Both S3 and EFS provide serverless architecture
    support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability and robustness**: Again, both S3 and EFS are highly robust
    and available options for storage. In this regard, EBS is not on par with the
    other two storage options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data analytic tool compatibility**: Reading and writing data from an
    S3 or EBS volume is much easier from big data processing engines such as Spark
    and MapReduce. Also, creating external Hive or Athena tables is much easier if
    the data resides in S3 or EBS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, both S3 and EFS seem to be promising options to use. Now, let’s
    look at how crucial cost is in determining cloud storage solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Determining storage based on cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most important tools for any cloud solution architect is a cost
    estimator or pricing calculator. As we are using AWS, we will use AWS Pricing
    Calculator: [https://calculator.aws/#/](https://calculator.aws/#/).'
  prefs: []
  type: TYPE_NORMAL
- en: We will use this tool to compare the cost of storing input data in EFS versus
    S3 storage. In our use case, we’ll assume that we get 2 TB of data per month and
    that we must store monthly data for 3 months before we can archive it. We also
    need to store data for up to 1 year. Let’s see how our cost varies based on our
    choice of storage.
  prefs: []
  type: TYPE_NORMAL
- en: Here, for either kind of storage, we will use **S3 Intelligent-Tiering** (which
    supports automatic life cycle management and reduces cost) to do the calculation.
    It asks for the average data storage per month and the amount stored in the frequent
    access layer, infrequent access layer, and archive layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the average data storage required per month, 2 TB of new data
    per month gets generated for our use case. So, we have 2 TB of data to store in
    the first month, 4 TB of data to store in the second, 6 TB of data to store in
    the third, and so on. So, to calculate the average data, we must add all the storage
    requirements for each month together and divide the result by 12\. The mathematical
    equation for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17084_Formula_5.1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding formula gives us a 13 TB per month calculation. Now, it asks
    us for the percentage stored in the frequent access layer – the layer that we
    will read the data from. The data in the frequent access layer can only be 2 TB
    for each month (which is around 15% of 13 TB). Using these values, we can calculate
    the estimated cost, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – AWS S3 cost estimation tool ](img/B17084_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – AWS S3 cost estimation tool
  prefs: []
  type: TYPE_NORMAL
- en: Using the previously mentioned calculation, ballpark estimates for Amazon S3
    come to 97.48 USD per month on average. However, a similar calculation for Amazon
    EFS would cost 881.92 USD. This depicts that using EFS will be nine times costlier
    than using Amazon S3\.
  prefs: []
  type: TYPE_NORMAL
- en: So, looking at the cost, combined with other parameters, we can safely decide
    on choosing Amazon S3 as our input storage. Based on a similar set of logic and
    calculations, we can store the ODL in S3 as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a discussion about the storage layer for the output data is incomplete
    without deciding on the schema and format of the output files. Based on the requirements,
    we can conclude that the output data should have the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`year`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`month`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`category_id`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`product_id`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tot_sales`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tot_onlyview`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sales_rev`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_by_revenue`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_by_sales`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is advisable to partition the table on a yearly and monthly basis since most
    of the queries on the ODL will be monthly. Now that we have finalized all the
    details of the storage layer, let’s discuss the processing layer of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must look at the options that are available in AWS for processing a
    big data batch job. Primarily, there are two native alternatives. One is running
    Spark on EMR clusters, while the other is running Glue jobs (Glue is a fully managed
    serverless AWS service). AWS Glue allows you to write a script in Scala or Python
    and trigger Glue jobs either through the AWS Management Console or programmatically.
    Since we are interested in implementing the solution in Java, AWS Glue is not
    an option for us. Also, AWS Glue scripts have less portability and a higher learning
    curve. Here, we will stick to Spark on EMR. However, Spark jobs can be run on
    an EMR cluster in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The classical approach of running `spark submit` from the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cloud-specific approach of adding a `spark submit` step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s see how the cost matrix helps us determine which approach we should
    take to submit a Spark job among the two options mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The cost factor in the processing layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first option is to keep the EMR cluster up and running all the time and
    trigger the `spark submit` command using a shell script from a cronjob trigger
    at specific time intervals (very similar to what we would do on an on-premise
    Hadoop cluster). Such a cluster is known as a *persistent EMR cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: The second option is to add an EMR step to run the Spark job while creating
    the cluster and then terminate it once it has run successfully. This kind of EMR
    cluster is known as a *transient EMR cluster*. Let’s see how the cost estimates
    vary for each option.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an EMR cluster, there are three types of nodes: *master node*, *core node*,
    and *task node*. The master node manages the cluster and acts as the NameNode
    and the Jobtracker. The core node acts as the DataNode, as well as the worker
    node, which is responsible for processing the data. TaskNodes are optional, but
    they are required for separate task tracker activities.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to their nature of work, usually, selecting a compute-optimized instance
    works great for the master node, while a mixed instance works best for the core
    nodes. In our calculation, we will use the c4.2xlarge instance type for the master
    node and the m4.4xlarge instance type for the core nodes. If we need four core
    nodes, a persistent EMR cluster would cost us around 780 USD per month. A similar
    configuration on a transient EMR cluster would only cost around 7 USD per month,
    considering the job runs two or three times a month with a job run duration not
    exceeding 2 hours each. As we can see, the second option is nearly 100 times more
    cost-effective. Therefore, we will choose a transient EMR cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s figure out how to create and schedule the transient EMR clusters.
    In our use case, the data arrives in the S3 buckets. Each successful creation
    event in an S3 bucket generates an event that can trigger an AWS Lambda function.
    We can use such a Lambda function to create the transient cluster every time a
    new file lands in the landing zone of the S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the preceding discussion, the following diagram depicts the architecture
    of the solution that’s been proposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Solution architecture ](img/B17084_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Solution architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, the input data lands in the S3 bucket (also
    known as landing zone) as the source data. Here, one source data file arrives
    twice a month. The architecture diagram depicts four steps denoted by numerals.
    Let’s look at these steps in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: A CloudWatch event is generated when an incoming source file is completely written
    in the S3 bucket. This generates a Lambda trigger, which, in turn, invokes a Lambda
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Lambda function receives the creation event records and creates a transient
    EMR cluster with a step configured to run a Spark job to read and process the
    new input file(s).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark job in the EMR step reads and processes the data. Then, it writes
    the transformed output data to the S3 bucket for the ODL layer. Upon successfully
    terminating the Spark step, the transient cluster gets terminated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the data residing in the ODL layer will be exposed as Athena tables that
    can be queried for any analytical purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This gives us a very simple yet powerful architecture to solve such big data
    batch processing problems. The logs and metrics in all the processing and storage
    components will be captured by AWS CloudWatch logs. We can further improve this
    architecture by adding auditing and alert features using CloudWatch logs and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is advisable to use the Parquet format as the output storage format because
    of the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost saving and performance**: Since multiple output columns can potentially
    have low cardinality, the ODL data storage format should be a columnar format,
    which can give cost savings as well as better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Technology compatibility**: Since we are dealing with big data processing,
    and our processing layer is Spark-based, Parquet will be the most suitable data
    format to use for the ODL layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have analyzed the problem and developed a robust, reliable, and
    cost-effective architecture, let’s implement the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step of any implementation is always understanding the source data.
    This is because all our low-level transformation and cleansing will be dependent
    on the variety of the data. In the previous chapter, we used DataCleaner to profile
    the data. However, this time, we are dealing with big data and the cloud. DataCleaner
    may not be a very effective tool for profiling the data if its size runs into
    the terabytes. For our scenario, we will use an AWS cloud-based data profiling
    tool called AWS Glue DataBrew.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the source data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to do data profiling and analysis to understand
    the incoming data (you can find the sample file for this on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05).
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an S3 bucket called `scalabledataarch` using the AWS Management Console
    and upload the sample input data to the S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Creating an S3 bucket and uploading the input file ](img/B17084_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Creating an S3 bucket and uploading the input file
  prefs: []
  type: TYPE_NORMAL
- en: 'From the AWS Management Console, go to the AWS Glue DataBrew service. Click
    on the **DATASET** side tab. Then, click the **Connect new dataset** button. A
    dialog box similar to the one shown in the following screenshot will appear. Select
    **Amazon S3** and then enter the source data path of the S3 bucket. Finally, click
    the **Create Dataset** button to create a new dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Adding a new dataset to AWS Glue DataBrew ](img/B17084_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Adding a new dataset to AWS Glue DataBrew
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a data profiling job using the dataset that we’ve added.
    First, select the newly added dataset and go to the **Data profile overview**
    tab, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – The Data profile overview tab ](img/B17084_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – The Data profile overview tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, click on the **Run data profile** button, which will take you to a **Create
    job** popup. Enter the job name and choose to run the sample using the **Full
    dataset** option, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Creating a data profiling job ](img/B17084_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Creating a data profiling job
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the output location as `scalablearch-dataprof` (our S3 bucket). The output
    files of the data profiling job will be stored here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Configuring the data profiling job ](img/B17084_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Configuring the data profiling job
  prefs: []
  type: TYPE_NORMAL
- en: Then, configure `product_id`, `category_id`, and `brand`,
  prefs: []
  type: TYPE_NORMAL
- en: 'we have configured them accordingly, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Configuring the Correlations widget ](img/B17084_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Configuring the Correlations widget
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must set up the security roles for the data profiling job. Once you’ve
    done this, click the **Create Job** button to create the data profiling job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Setting the security permissions for the data profiling job
    ](img/B17084_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Setting the security permissions for the data profiling job
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can see the newly created data profiling job in the **Profile jobs**
    tab. We can run the data profiling job by clicking the **Run job** button on this
    screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Data profiling job created and listed ](img/B17084_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Data profiling job created and listed
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the job successfully runs, we can go to the dataset, open the **Data lineage**
    tab, and view the data lineage, as well as the time interval before which the
    last successful data profiling job ran:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Lineage of the data profiling job ](img/B17084_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Lineage of the data profiling job
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the report to find missing values, cardinalities, the correlation
    between columns, and the distribution of the data. These metrics help us determine
    whether there are anomalies in the data that need to be cleaned up or if there
    are missing values and if they need to be handled. It also helps us understand
    the quality of the data that we are dealing with. This helps us do proper cleansing
    and transformation so that it doesn’t give us surprises later during our implementation.
    The following screenshot shows some sample metrics that AWS Glue DataBrew displays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Data profile metrics ](img/B17084_05_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Data profile metrics
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see some useful statistics. For example, `event_type` has no noise
    and it has a very low cardinality. It also shows that the data is not uniformly
    distributed by this column.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have analyzed the data, let’s develop the Spark application that
    will process the records.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the Spark application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the analysis in the previous section, we will create the incoming
    record schema string. Then, we will use that schema string to read the incoming
    data, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will calculate the total sales and total views for each product using
    the `count_if` aggregation function of Spark, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create another DataFrame to calculate the total revenue for only the
    purchase events. The following code snippet shows how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will combine the `countAggDf` and `revenueAggDf` DataFrames using a
    `LEFT OUTER JOIN` SparkSQL query, as shown in the following code snippet. The
    null values for `total_sales` for the product that didn’t have a single sale are
    set to `0.0` using the `na.fill()` method of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will apply window functions on the resultant `combinedEnrichedDf` DataFrame
    to derive the columns – that is, `rank_by_revenue` and `rank_by_sales`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is ready and is in the same format as the output. So, we must write
    the transformed data to the output S3 bucket using Parquet format while ensuring
    it’s partitioned by year and month:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The full source code for this application is available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05/sourcecode/EcommerceAnalysis](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05/sourcecode/EcommerceAnalysis).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to deploy and run the Spark application
    on an EMR cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and running the Spark application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have developed the Spark job, let’s try to run it using a transient
    EMR cluster. First, we will create an EMR cluster manually and run the job. To
    create the transient EMR cluster manually, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, build the Spark application JAR file and upload it to an S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the AWS Management Console for AWS EMR. Click the **Create Cluster** button
    to create a new transient cluster manually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the EMR configuration. Make sure that you set **Launch mode** to **Step
    execution**. Make sure that you select **emr-6.4.0** as the value of the **Release**
    field in the **Software configuration** section. Also, for **Add Steps**, choose
    **Spark application** for **Step type**. Leave all the other fields as-is. Your
    configuration should look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Manual transient EMR cluster creation ](img/B17084_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Manual transient EMR cluster creation
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to add the Spark step, click on the **Configure** button. This will make
    a dialog box appear where you can enter various Spark step-related configurations,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Adding a Spark step to the EMR cluster ](img/B17084_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Adding a Spark step to the EMR cluster
  prefs: []
  type: TYPE_NORMAL
- en: Please ensure that you specify the driver class name in the **Spark-submit options**
    area and provide the necessary information in the **Application location*** and
    **Arguments** boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Add** to add the step. Once added, it will look similar to what’s shown
    in the following screenshot. Then, click **Create Cluster**. This will create
    the transient cluster, run the Spark job, and terminate the cluster once the Spark
    job has finished executing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.15 – EMR cluster configuration with an added Spark step ](img/B17084_05_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – EMR cluster configuration with an added Spark step
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it has successfully run, you will see that the job succeeded in the **Steps**
    tab of the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Job monitoring in the EMR cluster ](img/B17084_05_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Job monitoring in the EMR cluster
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Spark errors
  prefs: []
  type: TYPE_NORMAL
- en: A Spark job runs on huge volumes of data and can throw multiple exceptions.
    It can also report multiple stage failures, such as `OutOfMemoryException`, large
    frame errors, throttling errors from multipart files uploaded in AWS S3, and so
    on. Covering all of these is beyond the scope of this book. However, you can refer
    to a very concise Spark troubleshooting guide at [https://docs.qubole.com/en/latest/troubleshooting-guide/spark-ts/troubleshoot-spark.xhtml#troubleshooting-spark-issues](https://docs.qubole.com/en/latest/troubleshooting-guide/spark-ts/troubleshoot-spark.xhtml#troubleshooting-spark-issues)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have deployed and run the Spark application manually, let’s automate
    how Spark jobs are created and run by implementing a Lambda trigger.
  prefs: []
  type: TYPE_NORMAL
- en: Developing and testing a Lambda trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS Lambda functions are fully managed serverless services that help process
    information. They are supported by multiple languages such as Python, JavaScript,
    Java, and so on. Although Python or JavaScript runtimes are faster, we will use
    the Java runtime in this book to implement the solution (since we are focusing
    on Java-based implementations in this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'To write a Lambda function that will react to an S3 event, we must create a
    Java class that implements the `RequestHandler` interface and takes `S3Event`
    as its generic `Input` type, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this class, we must implement the `handleRequest` method of the `RequestHandler`
    interface. In the `handleRequest` method, we loop through each `S3EventNotificationRecord`,
    which denotes a new file being created or updated. We collect all the S3 object
    names attached to this `S3EventNotificationRecord` in `S3ObjectNames`. For each
    distinct S3 object name present in `S3ObjectNames`, we create and launch an AWS
    transient EMR cluster. The following code snippet shows the implementation of
    the `handleRequest` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the implementation of the `createClusterAndRunJob` method.
    This takes two arguments: `inputS3path` and the Lambda logger. This method uses
    AWS SDK to create an `ElasticMapReduce` object. This method uses the `StepConfig` API
    to build a `spark submit` step. Then, it uses all the configuration details, along
    with `SparkSubmitStep`, to configure the `RunJobFlowRequest` object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can submit a request to create and run an EMR cluster using the
    `runJobFlow` method of the `ElasticMapReduce` object, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we developed the Lambda function, let’s deploy, run, and test it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an IAM security role for the Lambda function to trigger the EMR cluster,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Creating a new IAM role ](img/B17084_05_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Creating a new IAM role
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Lambda function using the AWS Management Console. Please provide the
    name and runtime of the function, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Creating an AWS Lambda function ](img/B17084_05_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Creating an AWS Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'While creating the Lambda function, please make sure that you change the default
    execution role to the IAM role you created in *Step 1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Setting the IAM role to an AWS Lambda function ](img/B17084_05_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Setting the IAM role to an AWS Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you must add an S3 trigger for the Lambda function, as shown in the following
    screenshot. Make sure that you enter the proper bucket name and prefix where you
    will push your source files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Creating an S3 event trigger for the Lambda function ](img/B17084_05_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Creating an S3 event trigger for the Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you must build the JAR file locally from the Lambda function we developed
    using our Maven Java project (the full source code for the project can be found
    at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05/sourcecode/S3lambdaTriggerEmr](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter05/sourcecode/S3lambdaTriggerEmr)).
    Once the JAR file has been built, you must upload it using the **Upload from**
    | **.zip or .jar file** option, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Deploying an AWS Lambda JAR file ](img/B17084_05_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Deploying an AWS Lambda JAR file
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can test the whole workflow by placing a new data file in the S3 bucket
    mentioned in the S3 trigger. Once the Lambda function executes, it creates a transient
    EMR cluster where the Spark job will run. You can monitor the metrics of the Lambda
    function from the AWS Management Console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Monitoring the AWS Lambda function ](img/B17084_05_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – Monitoring the AWS Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'You can monitor the Spark application from the AWS EMR management console by
    looking through the **Persistent user interfaces** options in the transient cluster’s
    **Summary** tab, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – EMR Cluster management console ](img/B17084_05_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – EMR Cluster management console
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting a Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, you may have trouble when invoking or executing a Lambda
    function. A very concise guide to troubleshoot all such issues has been published
    by AWS. For more information, check out [https://docs.aws.amazon.com/lambda/latest/dg/lambda-troubleshooting.xhtml](https://docs.aws.amazon.com/lambda/latest/dg/lambda-troubleshooting.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see if we can further optimize the Spark application by monitoring
    the Spark job.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning a Spark job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can investigate the Spark UI to see its **directed acyclic graph** (**DAG**).
    In our case, our DAG looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – DAG of the Spark job ](img/B17084_05_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – DAG of the Spark job
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, both *Stage 5* and *Stage 6* are performing the same job of
    scanning and reading the CSV into a DataFrame. This is because we have a DataFrame
    called `ecommerceEventDf` that is being used to derive two different DataFrames.
    Both derived DataFrames calculate `ecommerceEventDf` separately due to Spark’s
    lazy evaluation technique, which causes the performance to slow down. We can overcome
    this issue by persisting the `ecommerceEventDf` DataFrame, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After making this change, the new DAG will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Optimized DAG of the Spark job ](img/B17084_05_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – Optimized DAG of the Spark job
  prefs: []
  type: TYPE_NORMAL
- en: In the new DAG, there’s a green dot in the `InMemoryTableScan` task. This green
    dot represents the in-memory persistence of the data by Spark so that it doesn’t
    scan the CSV file twice, thus saving processing time. In this use case, it will
    speed up the performance of the Spark job by around 20%.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have implemented and tested our solution, let’s learn how to build
    an Athena table on top of the output folder and enable easy querying of the results.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the ODL using AWS Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to perform data querying on the ODL that
    we have created using our architecture. We will focus on how to set up Athena
    on our output folder to do easy data discovery and querying:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to AWS Athena via the AWS Management Console. Click on **Explore the
    query editor**. First, go to the **Manage settings** form of the **Query editor**
    area and set up an S3 bucket where the query results can be stored. You can create
    an empty bucket for this purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Setting up AWS Athena ](img/B17084_05_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – Setting up AWS Athena
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create an Athena table on top of our S3 output bucket. For this, we
    will create a DDL to create a table called `ecom_odl`, which is a partitioned
    table on the `year` and `month` columns. The DDL of the table can be seen in the
    following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will run this DDL statement in the **Query editor** area of Athena to create
    the table shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Creating an Athena table based on the output data ](img/B17084_05_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – Creating an Athena table based on the output data
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the table has been created, we need to add the partition. We can do this
    by using the `MSCK REPAIR` command (similar to Hive):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon running the previous command, all partitions are auto-discovered from
    the S3 bucket. Now, you can run any query on the `ecom_odl` table and get the
    result. As shown in the following screenshot, we run a sample query to find the
    top three products by revenue for each category in October 2019:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Querying ODL using an Athena table ](img/B17084_05_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – Querying ODL using an Athena table
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully architected, designed, and developed a big data
    batch processing solution and created an interface for the downstream teams to
    query our analyzed data using AWS Athena. Now, let’s summarize what we have learned
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to analyze a problem and identified that it
    was a big data problem. We also learned how to choose a platform and technology
    that will be performance-savvy, optimized, and cost-effective. We learned how
    to use all these factors judiciously to develop a big data batch processing solution
    in the cloud. Then, we learned how to analyze, profile, and draw inferences from
    big data files using AWS Glue DataBrew. After that, we learned how to develop,
    deploy, and run a Spark Java application in the AWS cloud to process a huge volume
    of data and store it in an ODL. We also discussed how to write an AWS Lambda trigger
    function in Java to automate the Spark jobs. Finally, we learned how to expose
    the processed ODL data through an AWS Athena table so that downstream systems
    can easily query and use the ODL data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to develop optimized and cost-effective batch-based
    data processing solutions for different kinds of data volumes and needs, in the
    next chapter, we will learn how to effectively build solutions that help us process
    and store data in real time.
  prefs: []
  type: TYPE_NORMAL
