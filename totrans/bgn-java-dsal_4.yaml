- en: Algorithm Design Paradigms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about hash tables and binary search trees.
    In this chapter, we will explore algorithm design paradigms. These design patterns
    can be seen as the generic methods or approaches that motivate the design of a
    class of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Just as an algorithm is a higher abstraction than a computer program, an algorithm design
    paradigm is an abstraction higher than an algorithm. The choice of an algorithm
    paradigm is an important one when designing an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will focus on the following three algorithm paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: Greedy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By becoming familiar with these higher abstractions, you can make more informed decisions
    when designing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In a previous chapter, we have come across the merge sort and quick sort algorithms,
    which are examples of the divide and conquer paradigm. As the name suggests, both
    of these algorithms *divide* the input into smaller parts, which are then solved
    recursively (*conquer*).
  prefs: []
  type: TYPE_NORMAL
- en: There are obviously more algorithm design paradigms, but these three already
    cover a broad range of problems. Some other paradigms we're not talking about
    in this book are backtracking and prune and search. There are even paradigms focused
    on specific branches of computer science. The sweep line algorithms, in computational geometry,
    is an example of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe greedy, divide and conquer, and dynamic programming algorithm paradigms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze common problems solved by using the described paradigms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the properties of a problem to be solved by each paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve some well-known problems that explain the applicability of each paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Greedy Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithms typically go through a sequence of steps, wherein each step you have
    a set of choices. Greedy algorithms, as the name suggests, follow the heuristic
    of making the locally choice at each step, with the hope of arriving at a global
    optimum. To better understand what we mean by this, let's introduce a problem.
  prefs: []
  type: TYPE_NORMAL
- en: The Activity Selection Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Peter is an energetic guy, and usually has many things to do in a given day.
    However, with the amount of things he wants to do, he is usually unable to do
    them all in a single day. What he usually does after waking up is write up a list
    of activities that he has to do, along with their time span. Then, looking at
    that list, he devises a plan for the day, trying to accommodate as many activities
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being an energetic guy, he usually rushes through this process and finds himself
    doing fewer activities than possible throughout the day. Can you help him maximize
    the amount of activities he can do in a day, given his schedule? An example of
    a schedule for Peter is given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Activity** | **Time Span** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Tidy up his room | 10:00 - 12:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Going to the rock concert | 20:00 - 23:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Play chess at the local club | 17:00 - 19:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Take a shower | 10:00 - 10:30 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Dinner with friends | 19:00 - 20:30 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Play Civilization VI | 21:30 - 23:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Have lunch with friends | 12:30 - 13:30 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Go to the cinema | 20:00 - 22:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Go biking in the park | 17:00 - 19:30 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Go to the beach | 16:00 - 19:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Go to the library | 15:00 - 17:00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: Peter''s schedule'
  prefs: []
  type: TYPE_NORMAL
- en: This is known as the **activity selection problem**. The problem is to schedule
    several competing activities that require exclusive use of a common resource (which
    is Peter, in this case), with the goal of selecting a maximum size set of activities
    that are mutually compatible.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we are trying to find the biggest set of activities that Peter
    can perform in a day. Each activity (*a[i]*) has a start time (*s[i]*) and a finish
    time (*f[i]*). Two activities, *a[i]* and *a[j]*, are considered compatible if
    the intervals (*s[i]*, *f[i]*) and (*s[j]*, *f[j]*) do not overlap, for example,
    *s[i] ≥ f[j]* or *s[j] ≥ f[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at Peter''s schedule, and converting the start and finish times to
    minutes since the start of a day, we can arrive at the following table. To convert
    the times to minutes since the start of the day, we multiply the hours by 60 and
    add the minutes. For example, `activity 1` runs from 10:00 to 12:00\. 10:00 is
    *600 (10*60 + 0)* minutes since the start of the day, and 12:00 is *720 (12*60
    + 0)* minutes since the start of the day:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ai** | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| **si** | 600 | 1200 | 1020 | 600 | 1140 | 1290 | 750 | 1200 | 1020 | 960
    | 900 |'
  prefs: []
  type: TYPE_TB
- en: '| **fi** | 720 | 1380 | 1140 | 630 | 1230 | 1380 | 810 | 1320 | 1170 | 1140
    | 1020 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.2: Start and finish times of peter''s activities'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, the subset *{a[1], a[3], a[5], a[6]}* consists of mutually
    compatible activities, as their times don''t overlap. It is not a maximum subset
    (that is, we can find a set with a larger number of activities), since the subset
    *{a[3], a[4], a[5], a[6], a[7], a[11]}* is larger (note that the order of the
    activities is *a[4]*, *a[7]*, *a[11]*, *a[3]*, *a[5]* and *a[6]*). In fact, it
    is a larger subset of mutually compatible activities. It is not the only one:
    another possible larger subset is *{a[1], a[7], a[11], a[3], a[5], a[6]}*.'
  prefs: []
  type: TYPE_NORMAL
- en: How should we approach this problem to find the maximum size set of activities
    that are mutually compatible? It turns out we should do the greedy choice. What
    this means is that, at each step of the algorithm, from the set of activities
    that we can still perform, we should choose one greedily.
  prefs: []
  type: TYPE_NORMAL
- en: The greedy choice may not be immediate, but you can intuitively think that we
    should select activities that leave Peter available for as many other activities
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have two approaches to resolve this problem, and they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Always choose the activity with the earliest starting time; however, we can
    have the activity with the earliest starting time finishing after all the other
    activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the activity that consumes the least amount of time; however, we can
    have a small activity overlapping two or more non-overlapping activities (for
    example, activities *[1, 4)*, *[3, 5)* and *[4, 8)*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, both of these approaches don't work.
  prefs: []
  type: TYPE_NORMAL
- en: From the set of activities that we are able to choose, we must choose the first
    one to finish, as that is the activity that would leave Peter available for as
    many of the activities that follow it as possible. If we sort activities by finish
    time, we can then always select the first activity we find that is compatible
    with the last activity selected for Peter.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the Activity Selection Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To implement the greedy algorithm in Java to solve the activity selection problem,
    as **described previously**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A possible implementation of the algorithm described to solve the activity
    selection problem is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to [https://goo.gl/xYT2Ho](https://goo.gl/xYT2Ho) to access complete
    code.
  prefs: []
  type: TYPE_NORMAL
- en: After having sorted the activities by finish time, the selection part of the
    algorithm runs in *O(n)* time. Since we can't sort in *O(n)*, the overall complexity
    of this algorithm is bounded by the complexity of the sorting algorithm. As seen
    in previous chapters, we can sort in *O(nlog(n))* so that's the runtime complexity
    of the algorithm we've devised for the activity selection problem. The algorithm
    looks good, but how can we be sure that it always arrives at the optimal solution?
  prefs: []
  type: TYPE_NORMAL
- en: Ingredients of a Greedy Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two basic ingredients every greedy algorithm has in common. They
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimal substructure property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greedy choice property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal Substructure Property
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in solving an optimization problem by using a greedy approach
    is to characterize the structure of an optimal solution. A problem exhibits the
    optimal substructure property if an optimal solution to the problem within it
    contains optimal solutions to subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we can think that the activity selection problem exhibits the optimal
    substructure property in the sense that, if we suppose that a given activity belongs
    to a maximum size set of mutually compatible activities, then we are left to choose
    the maximum size set of mutually compatible activities from the ones that finish
    before this activity starts and that start after this activity finishes. Those
    two sets must also be maximum sets for the compatible activities, so that they
    can showcase the optimal substructure of this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, it is possible to prove that the activity selection problem exhibits
    optimal substructure. Let''s assume we have the set of activities sorted in monotonically
    increasing order of finish time so that the following can be true for activities
    *i* and *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f[i] ≤ f[j] if i ≤ j*'
  prefs: []
  type: TYPE_NORMAL
- en: Whereas *f[i]* is denoting the finish time of activity *i*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we denote by *S[ij]*; the set of activities that start after activity
    *a[i]* finish, and they finish before activity *a[j]* starts. Thus, we wish to
    find a maximum set of mutually compatible activities in *S[ij]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that the set is *A[ij]*, and includes activity *a[k]*. By including
    *a[k]* in an optimal solution, we are left with two subproblems that are finding
    the maximum subset of mutually compatible activities in the set *S[ik]* and set
    *S[kj]*, which can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A[ij] = A[ik] ∪ {a[k]} ∪ A[kj]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the size of the maximum size set of mutually compatible activities in
    *S[ij]* is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*|A[ij]| = |A[ik]| + |A[kj]| + 1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we could find a set *A''[kj]* of mutually compatible activities in *S[kj]*
    where *|A''[kj]| > |A[kj]|*, then we could use *A''[kj]* instead of *A[kj]* in
    the optimal solution to the subproblem for *S[ij]*. But that way, we would have
    something that contradicts the assumption that *A[ij]* is an optimal solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*|A[ik]| + |A''[kj]| + 1 > |A[ik]| + |A[kj]| + 1 = |A[ij]|*'
  prefs: []
  type: TYPE_NORMAL
- en: Greedy Choice Property
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When searching for a possible solution to a problem, we usually consider various solutions,
    which we call the solution space.
  prefs: []
  type: TYPE_NORMAL
- en: When trying to find the best solution to a problem, we're usually interested
    in a global optimum, that is, the optimal solution from the whole set of possible
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: However, the solution space can exhibit other optimums. Namely, we can have
    local optimums, which are optimal solutions in a small neighborhood of possible
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The greedy choice property states that from a local optimum we can reach a global
    optimum, without having to reconsider decisions that have already been made.
  prefs: []
  type: TYPE_NORMAL
- en: In the activity selection problem for Peter, we applied the greedy choice by
    always choosing the activity with the earliest finish time from the set of available
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we can think that this problem exhibits the greedy choice property
    in the sense that if we have a maximum size subset and we replace the activity
    from that set with the earliest finish time with one that finishes even earlier,
    we are always left with a maximum size set, making it safe to always choose the
    one with the earliest finish time.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to prove that the greedy choice is always part of some optimal
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to prove that, for any nonempty subproblem *S[k]*, if *a[m]* is an
    activity in *S[k]* with the earliest finish time, then *a[m]* is included in some
    maximum size subset of mutually compatible activities of *S[k]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prove it, let''s assume the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A[k]* is a maximum size subset of mutually compatible activities in *S[k]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a[j]* is the activity in *A[k]* with the earliest finish time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *a[j] = a[m]*, we are done. If *a[j] != a[m]*, we can try to replace a[j]
    by am in *A[k]*, producing the set *A'[k] = A[k] - {a[k]} ∪ {a[m]}*.
  prefs: []
  type: TYPE_NORMAL
- en: We can safely do that since the activities in *A[k]* are disjoined. *a[j]* is
    the first activity to finish in *A[k]*, and *f[m] <= f[j]*.
  prefs: []
  type: TYPE_NORMAL
- en: Since *|A'[k]| = |A[k]|*, we conclude that *A'[k]* is a maximum size subset
    of mutually compatible activities of *S[k]*, and that it includes *a[m]*.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition usually helps us decide whether a greedy algorithm produces the optimal
    solution, without having to formally prove the optimal substructure and the greedy
    choice properties. We're also going to cover a different paradigm of algorithm
    design in this chapter, which is dynamic programming that also requires problems
    to exhibit the optimal substructure property. If you are not sure if a greedy
    algorithm works for a given problem due to the greedy choice, you can always build
    a dynamic programming solution for it to gain some insight.
  prefs: []
  type: TYPE_NORMAL
- en: Huffman Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To gain more insight about greedy algorithms, let's look at another problem
    that is solvable by a greedy algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Huffman codes represent a way of compressing data effectively. Data is considered to
    be a sequence of characters. Huffman's greedy algorithm uses a table with the
    frequency of each character to build up an optimal way of representing each character
    as a binary string.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, imagine we have a 1,00,000 character data file that we
    wish to store in a compressed fashion. The frequency of each character in the
    data file is given by the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Character** | a | b | c | d | e | f |'
  prefs: []
  type: TYPE_TB
- en: '| **Frequency** | 45,000 | 13,000 | 12,000 | 16,000 | 9,000 | 5,000 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.3: Frequency of each character in a data file'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various ways to represent this information. For the purpose of this
    problem, let''s say that we want to design a binary character code in which each
    character is represented by a unique binary string (which we shall call a **code
    word**). One option is to use a fixed-length code (for example, each character
    is represented by a code word of the same size). If we opt for that, we need three
    bits to represent each the six characters, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Character** | a | b | c | d | e | f |'
  prefs: []
  type: TYPE_TB
- en: '| **Frequency** | 45,000 | 13,000 | 12,000 | 16,000 | 9,000 | 5,000 |'
  prefs: []
  type: TYPE_TB
- en: '| **Code Word** | 000 | 001 | 010 | 011 | 100 | 101 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.4: Code word for each character'
  prefs: []
  type: TYPE_NORMAL
- en: Using this method, we need 3,00,000 bits to code the entire sequence of characters.
    Can we do better?
  prefs: []
  type: TYPE_NORMAL
- en: 'A variable-length code can do a lot better than a fixed-length code. Since
    we want to minimize the size of the compressed sequence of bits, we want to give
    short code words to frequent characters and long code words to infrequent characters.
    A possible code for this character sequence is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Character** | a | b | c | d | e | f |'
  prefs: []
  type: TYPE_TB
- en: '| **Frequency** | 45,000 | 13,000 | 12,000 | 16,000 | 9,000 | 5,000 |'
  prefs: []
  type: TYPE_TB
- en: '| **Code Word** | 0 | 101 | 100 | 111 | 1101 | 1100 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.5: Possible code for character sequence'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of 3,00,000 bits, this code requires only 2,24,000 bits to represent
    the character sequence. Using this code, we save around 28% of space. The code
    we have presented is also an optimal character code for this sequence, as we shall
    see.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Huffman Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start studying an algorithm to solve this problem, we should introduce
    something called **prefix codes**. Prefix codes are codes in which no code word
    is also a prefix of some other code word. As you have seen from the proposed variable-length
    code, we must make sure that no code word is also a prefix of some other code
    word, since we want to concatenate on code words and unambiguously be able to
    decode it afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: For example, using the code shown previously, we encode the string `abc` as
    `0101100`. Since no code word is a prefix of any other, decoding is vastly simplified,
    as we can identify the initial code word, translate it, and repeat the process
    on the remainder of the encoded sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'A convenient (for decoding purposes) representation of prefix codes is a binary
    tree whose leaves are the characters of the original data sequence. For the proposed
    variable-length code, we have the following binary tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59c876aa-4888-44ae-8c40-4db8b7c60f33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: A Representation of prefix codes'
  prefs: []
  type: TYPE_NORMAL
- en: The binary code word for a character is the path from the root to that character,
    following in the binary digits in each edge. Note that each node also holds the
    frequency of characters under its subtree. Such a tree also has some interesting
    properties. A tree for an optimal prefix code has exactly *|C|* leaves, *C* being
    the alphabet from which the characters are drawn. The number of internal nodes
    is exactly *|C| - 1*. We also know that the number of bits necessary to encode
    a particular character in a sequence is equal to the frequency of that character
    multiplied by the depth of the leaf that holds the character. As such, the number
    of bits required to encode the full character sequence is simply the sum of these
    values for all the characters in the alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: If we can build such a tree, we can compute an optimal prefix code. David A.
    Huffman invented a greedy algorithm to construct an optimal prefix code, called
    a **Huffman Code**. Its basic idea is to build the tree in a bottom-up fashion.
    We start with just the leaf, one for each character. We then repeatedly join the
    two least-frequent nodes, until we are left with a single node, which is the root
    of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an Algorithm to Generate Code Words Using Huffman Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement an algorithm capable of building the tree that generates the binary code
    words for the character in a data file using Java:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a priority queue to store nodes with the character frequency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeatedly join the two nodes with the least frequencies until you are left with
    a single node. The source code for this algorithm is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.2: Huffman code. Source class name: Huffman'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/indhgT](https://goo.gl/indhgT) to access the full code.
  prefs: []
  type: TYPE_NORMAL
- en: We make use of a priority queue to store our nodes, making it very efficient
    *(O(logn))* to extract the node with the least frequency. A priority queue is
    like a queue, or a stack, but each element has an additional **priority** associated
    with it. An element with a higher priority is served before an element with lower
    priority. Priority queues are usually implemented using heaps, which usually provide
    *O(1)* time to find the element with a higher priority, *O(logn)* to insert an
    element, and *O(logn)* time to remove the element with a higher priority.
  prefs: []
  type: TYPE_NORMAL
- en: To analyze the running time of Huffman's algorithm, let's break it down into
    steps. We first go through each character in the frequencies map and build a node
    that we later insert in the priority queue. It takes *O(logn)* time to insert
    a node in the priority queue. Since we go through each character, it takes *O(nlogn)*
    to create and initially populate the priority queue. The second for loop executes
    exactly *n-1* times. Each time, we perform two removes from the priority queue,
    each one taking *O(logn)* time. In its whole, the `for` loop takes *O(nlogn)*
    time. We thus have two steps, each taking *O(nlogn)* time, which leaves us at
    a total running time on a set of *n* characters of *O(nlogn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity: Implementing a Greedy Algorithm to Compute Egyptian Fractions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scenario**'
  prefs: []
  type: TYPE_NORMAL
- en: For this activity, we will be building a greedy algorithm to compute Egyptian fractions.
    Every positive fraction can be represented as a sum of unique unit fractions.
    A fraction is a unit fraction if its numerator is one and its denominator is a
    positive integer. For example, *1/3* is a unit fraction. Such a representation,
    for example, a sum of unique unit fractions, is called an Egyptian fraction, since
    it was used by the ancient Egyptians.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Egyptian fraction representation of *2/3* is *1/2 + 1/6*. The
    Egyptian fraction representation of *6/14* is *1/3 + 1/11 + 1/231*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Aim**'
  prefs: []
  type: TYPE_NORMAL
- en: To implement a greedy algorithm to compute Egyptian fractions, as described
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the `build` method of the `EgyptianFractions` class, which returns
    a list of denominators for the Egyptian fraction representation, in increasing
    order, which is available on GitHub at:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/TrainingByPackt/Data-Structures-and-Algorithms-in-Java/blob/master/src/main/java/com/packt/datastructuresandalg/lesson4/activity/egyptian/EgyptianFractions.java](https://github.com/TrainingByPackt/Data-Structures-and-Algorithms-in-Java/blob/master/src/main/java/com/packt/datastructuresandalg/lesson4/activity/egyptian/EgyptianFractions.java)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Assume that the denominator is always larger than the numerator, and that the
    returned denominators always fit in a Long
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To verify that your solution is correct, run `./gradlew test` in the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps for Completion**'
  prefs: []
  type: TYPE_NORMAL
- en: Check whether the numerator divides the denominator without leaving a remainder,
    and that we're left with a single fraction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If not, find the greatest possible unit fraction, subtract it from the original fraction,
    and recur on the remaining fraction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this first section, we introduced the greedy paradigm of algorithm design
    using the activity selection problem as a running example. We introduced the two
    properties a problem must observe to be optimally solved by a greedy algorithm:
    optimal substructure and greedy choice. To gain intuition about the applicability
    of greedy algorithms, we later explored two other problems that are solvable by
    a greedy approach: Huffman coding and Egyptian fractions.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Divide and Conquer Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ab7975d0-4b38-437d-9ff5-8f6c20199874.xhtml), *Sorting Algorithms
    and Fundamental Data Structures*, we introduced, among other sorting algorithms,
    merge and quick sort. A peculiarity of both algorithms is that they divide the
    problem into subproblems that are smaller instances of the same, solve each one
    of the subproblems recursively, and then combine the solutions to the subproblems
    into the solution for the original problem. This strategy forms the basis of the
    divide and conquer paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: The Divide and Conquer Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a divide and conquer approach, we solve a problem recursively, applying
    the following three steps at each level of recursion:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Divide** the problem into more than one subproblems that are smaller instances
    of the same problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conquer** the subproblems by solving them recursively. Eventually, the subproblem
    sizes are small enough for them to be solved in a straightforward manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine** the solutions to the subproblems in the solution for the original
    problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a subproblem is large enough to be solved recursively, we call that the
    **recursive case**. When a subproblem becomes small enough that recursion is no
    longer necessary, we say that we have reached to the **base case**. It is common
    to solve subproblems that are different from the original problem, in addition
    to the subproblems that are smaller instances of the main problem. Solving these
    problems is considered to be part of the combine step.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ab7975d0-4b38-437d-9ff5-8f6c20199874.xhtml), *Sorting Algorithms
    and Fundamental Data Structures*, we saw that the runtime complexity of merge
    sort was *O(nlogn)*. We can also see that the worst-case running time *T(n)* of
    merge sort can be described by the following recurrence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ecc74ee-27be-45ee-950b-fb75990d6893.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These kinds of recurrences arise frequently and characterize divide and conquer algorithms.
    If we generalize the recursion to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e675b4ff-2902-4f87-893f-9aa79d8a6bfb.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *a >= 1, b > 1* and *f(n)* is a given function, we have the recurrence
    for the worst-case running time of a divide and conquer algorithm that creates
    subproblems, each being of size *1/b* of the original problem, and in which the
    combine steps together take *f(n)* time.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to divide and conquer algorithms, it is often easier to come
    up with this recursion, but harder to derive the runtime complexity. Fortunately,
    there are at least three methods to provide **O** bounds for these recurrences:
    the substitution method, the recursion tree method, and the master method. For
    the purpose of this book, we will only be focused on the master method.'
  prefs: []
  type: TYPE_NORMAL
- en: The Master Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The master method provides a way to solve recurrences of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*T(n) = aT(n/b) + f(n)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a >= 1* and *b > 1* are constants, *f(n)* is an asymptotically positive function'
  prefs: []
  type: TYPE_NORMAL
- en: The master method consists of three cases, which will allow you to solve these
    kind of recurrences quite easily. Before we delve into these three cases, it is
    important to note that the recurrence is not actually well defined, since *n/b*
    may not be an integer. Whether we replace it with the floor or ceiling of the
    *n/b* division will not affect the asymptotic behavior of the recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: The big O notation describes the asymptotic upper bound of the growth rate of
    a function. There are also other notations to describe bounds on the growth rate
    of functions.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of the master method, we're interested in the big-theta notation
    (*Ө*) and the big-omega notation (*Ω*).
  prefs: []
  type: TYPE_NORMAL
- en: The big-theta notation describes the asymptotic tight bound of the growth rate.
    It's called tight bound because the running time is nailed within a constant factor
    above and below. It is a tighter bound than *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: When we say that an algorithm is *O(f(n))*, we're saying that the running time
    of the algorithm as *n* gets larger is at most proportional to *f(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: When we say that an algorithm is *Ө(f(n))*, we're saying that the running time
    of the algorithm as *n* gets larger is proportional to *f(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: The big-omega notation describes an asymptotic lower bound of the growth rate.
    When we say that an algorithm is *Ω(f(n))*, we're saying that the running time
    of the algorithm as n gets larger is at least proportional to *f(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having clarified all the necessary notations, we can present the three cases
    to derive the asymptotic bounds from a recurrence of type *T(n) = aT(n/b) + f(n)*
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If f(n) = O(n^(log[b])^(a-∈))* for some constant *∈ > 0*, then *T(n) = θ(n^(log[b])^a)
    = O(n^(log[b])^a)*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *f(n) = θ(n^(log[b])^a)*, then *T(n) = θ(n^(log[b])^a log(n)) = O(n^(log[b])^a
    log(n))*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *f(n) = Ω(n^(log[b])^(a+∈))* for some constant *∈ > 0*, and if *a f(n/b)
    ≤ c f(n)* for some constant *c < 1* and all sufficiently large *n*, then *T(n)
    = T(n) = θ(f(n)) = O(f(n))*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that in all three cases we're comparing the function *f(n)* with the function
    n^(log[b])^a. This function is usually called the **critical exponent**.
  prefs: []
  type: TYPE_NORMAL
- en: We can see intuitively that the larger of the two functions determines the solution
    to the recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: In *case 2*, the functions are of the same size, so we multiply by a logarithmic
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: One other way to see this is that in *case 1*, the work to split and recombine
    a problem is dwarfed by subproblems; in *case 2*, the work to split and recombine
    a problem is comparable to subproblems; and in *case 3*, the work to split and
    recombine a problem dominates the subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: To get some practice on using the master method, let's look at some examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first case, let''s consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12f13cde-6a96-4453-b439-e303f7414a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: For this recurrence, we have *a = 9*, *b = 3*, *f(n) = n*, and thus we have
    *n^(log[b])^a = n^(log[3])**⁹ = θ(n²)*. Since *f(n) = Ω(n^(log[4])**^(3+∈))*,
    *∈* being one, we can apply *case 1* of the master theorem and conclude that *T(n)
    = O(n2)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the second case, let''s consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bcc4829-6d16-4475-a815-dc5f2a9c328d.png)'
  prefs: []
  type: TYPE_IMG
- en: For this recurrence, we have *a = 2*, *b = 2*, *f(n) = 10n*, and thus we have
    *n^(log[b])**⁹ = n^(log[2])**² = O(n)*. Since *f(n) = O(n)*, we can apply *case
    2* of the master theorem and conclude that *T(n) = O(nlogn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the third and final case, let''s consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa684dd6-9968-4e85-8634-3516a4a3d102.png)'
  prefs: []
  type: TYPE_IMG
- en: For this recurrence, we have *a = 3*, *b = 4*, *f(n) = nlog(n)*, and thus we
    have *n^(log[b])**^a = n^(log[4])**³ = O(n^(0.793))*. Since *f(n) = Ω(n^(log[4])**^(3+∈))*,
    *∈* being around *0.2*, we can apply *case 3* as long as the condition holds for
    *f(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: For sufficiently large *n*, we have *af(n/b) = 3(n/4)log(n/4) <= (3/4)nlogn
    (for c = 3/4)*. Consequently, *T(n) = O(nlogn)*.
  prefs: []
  type: TYPE_NORMAL
- en: The Closest Pair of Points Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what characterizes a divide and conquer algorithm and are familiar
    with the master method to derive bounds from recurrences, let's look at a problem
    solvable by a divide and conquer approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem we will be looking at is the problem of finding the closest pair
    of points on a plane. We are given an array of *n* points in the plane, and we
    want to find out the closest pair of points in this array. Recall that the distance
    between two points, *p* and *q*, is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fdfc998-b219-47ee-905f-d62184129d1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our first approach may be to compute the distance between each pair and return
    the smallest, for a runtime complexity of *O(n²)*. The following snippet implements
    this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.3: Brute force for closest pair of points. Source class name: ClosestPairOfPoints'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/FrRW3i](https://goo.gl/FrRW3i) to access this code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed algorithm solves this problem, but we can do better by using a
    divide and conquer approach. The algorithm makes use of a preprocessing step in
    which it sorts the input array by its *x* coordinate. Then, it proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Divides** the array into two halves'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recursively** finds the smallest distances in both subarrays (**conquer**)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Combines** the results by taking the minimum distance from both halves and
    additionally considers pairs so that one point in the pair is from the left subarray
    and another is from the right subarray'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The approach seems straightforward, except for the combine part. After finding
    the minimum distance *d* from both the left and right subarrays, we have an upper
    bound of the minimum distance for this subproblem. Therefore, we only need to
    consider points whose *x* coordinate is closer than *d* to the middle vertical
    line. We can then sort those points by *y* coordinates and find the smallest distance
    in the strip, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74bdf3ed-7e65-4c80-95e5-608f9fdc79c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Method to calculate closest pair of points'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet computes the minimum distance in the strip, considering the
    minimum distance computed so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.4: Computing the minimum distance between pairs of points in the
    middle strip. Source class name: ClosestPairOfPoints'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/PwUrTc](https://goo.gl/PwUrTc) to access the full code.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this code, it seems to have a runtime of *O(n²)*, which doesn't really
    improve our brute force approach. However, it can be proved geometrically that
    for every point in the strip, a maximum of seven points after it need to be checked.
    This reduces the runtime of this step to *O(nlogn)* due to the sorting step.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to [http://people.csail.mit.edu/indyk/6.838-old/handouts/lec17.pdf](http://people.csail.mit.edu/indyk/6.838-old/handouts/lec17.pdf)
    for the geometric proof of the preceding problem.
  prefs: []
  type: TYPE_NORMAL
- en: We can even do better on this step if we sort the initial input by the *y *coordinate
    and keep a relationship between the points on the *x*-sorted array and the *y*-sorted
    array, effectively reducing the runtime of this step to *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.5: Combine Step of the divide and conquer algorithm to find the closest
    pair of points'
  prefs: []
  type: TYPE_NORMAL
- en: 'on a plane. Source class name: ClosestPairOfPoints'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/wyQkBc](https://goo.gl/wyQkBc) to access this code.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed algorithm divides all points into two sets and recursively solves
    both subproblems. After dividing, it finds the strip in *O(n)* time and finds
    the closest points in *O(n)* time (we're assuming the improvement of not requiring
    the sort in this step). Therefore, *T(n)* can be expressed as *T(n) = 2T(n/2)
    + O(n) + O(n) = 2T(n/2) + O(n)*, which is a bound of *O(nlogn)*, being better
    than the brute force approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity: Solving the Maximum Subarray Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scenario**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an algorithm to solve the maximum subarray problem. Find the non-empty,
    contiguous subarray of the input array whose values have the largest sum. You
    can see an example array with the maximum subarray indicated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/599a327f-5d66-4291-ac25-597488b7e4d2.png)'
  prefs: []
  type: TYPE_IMG
- en: The *O(n²)* brute force algorithm that tests all combinations for the start
    and end indices of the subarray is trivial to implement. Try to solve this using
    the divide and conquer algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Aim**'
  prefs: []
  type: TYPE_NORMAL
- en: To design and implement an algorithm to solve the maximum subarray problem with
    a better runtime than the *O(n²)* brute force algorithm, using a divide and conquer
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to implement the `maxSubarray()` method of the `MaximumSubarray` class
    in the source code, which returns the sum of values for the maximum subarray of
    the input array. The code is available on the following path:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/TrainingByPackt/Data-Structures-and-Algorithms-in-Java/blob/master/src/main/java/com/packt/datastructuresandalg/lesson4/activity/maxsubarray/MaximumSubarray.java](https://github.com/TrainingByPackt/Data-Structures-and-Algorithms-in-Java/blob/master/src/main/java/com/packt/datastructuresandalg/lesson4/activity/maxsubarray/MaximumSubarray.java)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Assume that the sum always fits in an int, and that the size of the input array
    is at most 100,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code comes with a test suite for this class, so to verify that your
    solution is correct, run `./gradlew test` in the command line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps for Completion**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The divide and conquer approach suggests that we divide the subarray into two subarrays
    of as equal size as possible. After doing so, we know that a maximum subarray
    must lie in exactly one of following three places:'
  prefs: []
  type: TYPE_NORMAL
- en: Entirely in the left subarray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entirely in the right subarray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crossing the midpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum subarray of the arrays on the left and right is given recursively,
    since those subproblems are smaller instances of the original problem.
  prefs: []
  type: TYPE_NORMAL
- en: Find a maximum subarray that crosses the midpoint.
  prefs: []
  type: TYPE_NORMAL
- en: There exists an even faster dynamic programming algorithm with a runtime of
    *O(n)* to solve the maximum subarray problem. The algorithm is called Kadane's
    algorithm. Dynamic programming will be explored in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: In the second section, we introduced the divide and conquer paradigm of algorithm
    design. We formalized the steps that a divide and conquer algorithm goes through,
    and showed the students how to go from a recurrence relationship to a runtime
    complexity bound using the master theorem. To gain intuition about the applicability
    of divide and conquer algorithms, we explored the problem of finding the closest
    pair of points on a plane.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After greedy and divide and conquer, we will turn our attention to dynamic programming.
    Dynamic programming is an algorithm design paradigm that also attempts to solve
    optimization problems by combining solutions with subproblems. Unlike divide and
    conquer, subproblems need to exhibit optimal substructure for dynamic programming
    to be applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of a Dynamic Programming Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two key ingredients that an optimization problem must have for dynamic
    programming to be applicable: optimal substructure and overlapping subproblems.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimal Substructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Optimal substructure is something we already covered when we introduced greedy
    algorithms. Recall that a problem exhibits optimal substructure, if an optimal
    solution to the problem contains within it, the optimal solutions to the sub-problems.
    There''s a common pattern when trying to discover optimal substructure for a problem
    that can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Show that a solution to the problem consists of making a choice, which leaves
    one or more subproblems to be solved. This choice may not be obvious and it is
    likely that many choices have to be tried (contrary to a greedy approach, in which
    a single optimal choice is made).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supposing that you are given the choice that leads to an optimal solution, determine
    the subproblems that follow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show that the solutions to the subproblems used within an optimal solution to
    the problem must themselves be optimal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, a *cut-and-paste* technique is used here. By supposing that each subproblem
    solution is not optimal, if a non-optimal solution is *cut out* and an optimal
    one is *pasted in*, a better solution to the original problem is produced, contradicting
    the supposition that the original solution to the problem was optimal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlapping Subproblems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another ingredient that optimization problems must have for dynamic programming
    to apply is that the space of subproblems should be **small**. Hence, a recursive
    algorithm for the problem should solve the same subproblems repeatedly. Typically,
    the total number of distinct subproblems is a polynomial in the input size. A
    recursive algorithm is said to have overlapping subproblems if it visits the same
    problem repeatedly. It's therefore typical for dynamic programming algorithms
    to cache solutions to subproblems to avoid re-computation of the same solutions
    over and over.
  prefs: []
  type: TYPE_NORMAL
- en: 0-1 Knapsack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To showcase a dynamic programming solution, exploring the properties of the
    problem that make it solvable by this technique, we shall look at the **0-1 knapsack** problem.
  prefs: []
  type: TYPE_NORMAL
- en: You are given weights and values of *n* items. You must put these items in a
    knapsack of capacity *W* to get the maximum value of the knapsack. You cannot
    break an item. You can either pick it or not pick it (hence the *0-1* property).
  prefs: []
  type: TYPE_NORMAL
- en: In other words, you are given two arrays, *values[0...n-1]* and *weights[0...n-1]*,
    which represent values and weights associated with n items, respectively. You
    want to find the maximum value subset of *values[]* such that the sum of weights
    of the subset is smaller than or equal to *W*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first immediate solution to this problem is to consider all subsets of
    items and calculate the total weight and value of all subsets, considering only
    those whose total weight is smaller than *W*. To consider all subsets of items,
    we can observe that there are two choices for each item: either it is included
    in the optimal subset, or it is not. Hence, the maximum value we can obtain from
    *n* items is the maximum of two values:'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum value obtained by *n-1* items and *W* weight (that is, they don't include
    the *n^(th)* item in the optimal solution)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the *n^(th)* item plus the maximum value obtained by *n-1* items
    and *W* minus the weight of the *n^(th)* item (for example, including the *n^(th)*
    item)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the previous observation, we've shown the optimal substructure property
    for the *0-1* knapsack problem.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the 0-1 Knapsack Problem Using Recursion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To write a code for solving the *0-1* knapsack problem by implementing the recursive approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that this is a recursive top-down approach, thus it repeatedly computes
    the same subproblems for an exponential runtime complexity (2^n). The following
    code snippet solves this problem using a recursive approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.6: Recursive solution for the 0-1 Knapsack problem. Source class
    name: Knapsack'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/RoNb5L](https://goo.gl/RoNb5L) to access this code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram of tree shows the recursion for *n* and *W*, with inputs
    *values[] = {10, 20, 30}* and *weights[] = {1, 1, 1}*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b975d86-dbdc-473f-8dd6-aefd117c7609.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Tree Showing the recursion for N and W'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since problems are evaluated again, this has the overlapping subproblems property.
    When we have a recursive top-down approach to a problem with the overlapping subproblems
    property, we can improve things by modifying the procedure to save the result
    of each subproblem (in an array or hash table). The procedure now first checks
    to see whether it has previously solved the subproblem. If so, it returns the
    saved value; if not, it computes, stores, and returns it. It is said that the
    recursive procedure has been memoized, remembering what results it has computed
    previously. Such an approach is therefore usually called top-down with memoization.
    The following code snippet adapts the previous one to use memoization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.7: Top down with memoization approach for the 0-1 knapsack problem.
    Source class name: Knapsack'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/VDEZ1B](https://goo.gl/VDEZ1B) to access this code.
  prefs: []
  type: TYPE_NORMAL
- en: By applying memoization, we reduce the runtime complexity of the algorithm from
    exponential (*2^n*) to quadratic (*n*W*). It is also possible to solve this problem
    using a bottom-up approach.
  prefs: []
  type: TYPE_NORMAL
- en: Use of a bottom-up approach typically depends on some natural notion of the
    *size* of the subproblems. We must sort the subproblems by size and solve them
    in order, smallest first, so that we're sure that we already have computed the
    solutions for smaller subproblems when we need them for a larger one.
  prefs: []
  type: TYPE_NORMAL
- en: 'A bottom-up approach usually yields the same asymptotic running time as a top-down
    approach, but it is typical for it to have much better constant factors, since
    it has less overhead for procedure calls. A bottom-up approach for solving the
    *0-1* Knapsack problem is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Go to [https://goo.gl/bYyTs8](https://goo.gl/bYyTs8) to access this code.
  prefs: []
  type: TYPE_NORMAL
- en: Longest Common Subsequence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's look at a different problem which is solvable by a dynamic programming
    algorithm. The problem we're now interested in is the longest common subsequence
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between a subsequence and a substring is that a substring is
    a consecutive subsequence. For example, *[a, d, f]* is a subsequence of *[a, b,
    c, d, e, f]*, but not a substring. *[b, c, d]* is a substring and a subsequence
    of *[a, b, c, d, e, f]*.
  prefs: []
  type: TYPE_NORMAL
- en: We're interested in finding similarities between two given sequences by computing
    the **Longest Common Subsequence** (**LCS**) between them. A common subsequence,
    *S[3]*, of two given sequences, *S[1]* and *S[2]*, is a sequence whose elements
    appear in both *S* and *S[2]* in the same order, but not necessarily consecutively.
    This problem is usually applicable when finding DNA similarities of different
    organisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we have two strands, *S[1]* and *S[2]*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[1] = ACCGGTCGAGTGCGCGGAGCCGGCCGAA*'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[2] = GTCGTTCGGAATGCCGTTGCTCTGTAAAA*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the longest common strand between those two, let''s call it S[3], would
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[3] = GTCGTCGGAAGCCGGCCGAA*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad9bd457-e4b4-4cde-b38b-dfe89c9518e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Calculating the longest common subsequence'
  prefs: []
  type: TYPE_NORMAL
- en: This problem is solvable using dynamic programming. If we go for a brute force
    approach, we can enumerate all subsequences of *S[1]* and check each subsequence
    to see if it is also a subsequence of *S[2]*, keeping track of the longest one
    we find.
  prefs: []
  type: TYPE_NORMAL
- en: However, if *|S[1]| = m*, then *S[1]* has *2m* subsequences, making it impractical
    for long sequences. The **LCS** exhibits the optimal-substructure property.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way for this to become evident is to think in terms of prefixes. Let''s
    assume that we have the following two sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X = {x[1], x[2]… x[m]}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = {y[1], y[2]… y[n]}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *Z* be any LCS of *X* and *Y* that can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z = {z[1], z[2]… z[k]}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the following cases are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: If *x[m] = y[n]*, then *z[k] = x[m] = y[n]*, and therefore *Z[k]-1* is a LCS
    of *X[m]-1* and *Y[n]-1*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *x[m] != y[n]*, then *z[k] != x[m]* implies that *Z* is a LCS of *X[m]-1*
    and *Y*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *x[m] != y[n]*, then *z[k] != y[n]* implies that *Z* is a LCS of *X* and
    *Yn-1*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This tells us that an LCS of two sequences contains the LCS of prefixes of
    the two sequences, exhibiting the optimal-substructure property. If we define
    *c[i][j]* to be the length of a LCS of sequences *X[i]* and *Y[j]*, then we can
    arrive at the following recursive formula, which guides us toward the dynamic
    programming solution for the problem that can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/200e5215-3062-41cd-b1ed-5e67d99fe77e.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the recurrence, the property of overlapping subproblems immediately
    pops out.
  prefs: []
  type: TYPE_NORMAL
- en: To find a LCS of *X* and *Y*, we may need to find the LCS's of *X* and *Y[n-1]*
    and *X[m-1]* and *Y*, but each of these have the problem of finding the LCS of
    *X[m-1]* and *Y[n-1]*. Many other subproblems share sub-subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this recurrence, in a bottom-up fashion (where we have solutions for
    subproblems readily computed), we can produce the following dynamic programming
    algorithm to compute the length of the longest common subsequence of two strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Snippet 4.9: Computing the length of the longest common subsequence of two
    Strings using dynamic programming algorithm. Source class name: LongestCommonSubsequence'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://goo.gl/4TdNVQ](https://goo.gl/4TdNVQ) to access this code.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to also compute the longest common subsequence, and not only
    the length of it if we keep track of the **direction** we go in the *c* matrix
    on each step (either up or left), taking into account that we only add a new character
    to the optimal subsequence when *x[i] = y[j]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t cover the solution to this problem in this book. If you are interested
    in it, the Wikipedia page for the LCS problem has a detailed walkthrough on the
    implementation of it: [https://en.wikipedia.org/wiki/Longest_common_subsequence_problem](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity: The Coin Change Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scenario**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this activity, we will be building a dynamic programming algorithm to solve
    the coin change problem. Given a value, *N*, if we want to split it into coins,
    and we have an infinite supply of each of *S={S[1], S[2], …, S[m]}* valued coins,
    in how many ways can we do it? The order of the coins doesn''t matter. For *N
    = 4* and *S = {1, 2, 3}*, there are four solutions: *{1, 1, 1, 1}*, *{1, 1, 2}*,
    *{2, 2}*, and *{1, 3}*, so the result should be four.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aim**'
  prefs: []
  type: TYPE_NORMAL
- en: To solve the coin change problem as described previously using a dynamic programming
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to implement the `ways()` method of the `CoinChange` class, which
    returns the number of ways to produce a given change for amount *N*, given a set
    of coins. It is available on the following path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/TrainingByPackt/Data-Structures-and-Algorithms-in-Java/blob/master/src/main/java/com/packt/datastructuresandalg/lesson4/activity/coinchange/CoinChange.java](https://github.com/TrainingByPackt/Data-Structures-and-Algorithms-in-Java/blob/master/src/main/java/com/packt/datastructuresandalg/lesson4/activity/coinchange/CoinChange.java).'
  prefs: []
  type: TYPE_NORMAL
- en: The source code comes with a test suite for this class, so to verify that your
    solution is correct, run `./gradlew test` in the command line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps for Completion**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When going through a coin *S[m]*, in order to count the number of solutions,
    we can divide the solution into two sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Those that do not contain any coin *S[m]*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Those that contain at least one *S[m]*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *w[i][j]* counts the number of ways to make change for *i* using coins up
    to *S[j]*, then we have the following recursion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/257e69cc-0b87-4387-878c-347011e03777.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this third and final section, we introduced the dynamic programming paradigm
    of algorithm design, using the *0-1* knapsack and the longest common subsequence
    problems as examples. We introduced the two properties a problem must observe
    to be optimally solved by a dynamic programming algorithm: optimal substructure
    and overlapping subproblems, and showed the students how to identify these properties.
    We''ve also seen the differences between a top-down (with memoization) and a bottom-up
    approach in dynamic programming algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed three different algorithm design paradigms.
    We've seen example problems for all of them and discussed how we can identify
    whether problems may be solvable by one of the given paradigms. In the next chapter,
    we will be focusing on some string matching algorithms that use the paradigms
    introduced here.
  prefs: []
  type: TYPE_NORMAL
