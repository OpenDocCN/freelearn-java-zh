<html><head></head><body>
      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Application Optimization – Memory Management and Server Configuration</h1>
            
         </header>
         
         
         <article>
            
            
            <p class="mce-root">We now know how to get information about the performance of our application. From
               high-level execution time to deep container internals, we can determine which part
               of the code is slowing us down.
            </p>
            
            <p>However, this is mainly about our code or stack (the Java EE container). There are
               other criteria that can influence the performance of the same machine (considering
               that the CPU and the memory are fixed).
            </p>
            
            <p>In this chapter, we will investigate the following:</p>
            
            <ul>
               
               <li>How JVM manages the memory and automatically releases unused objects</li>
               
               <li>Compare different options to release the memory that JVM offers</li>
               
               <li>See how the server configuration can also impact the performance</li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Java and the memory</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Java is a high-level language, which means that it is doing a lot of work for you.
               Nowadays, most languages do that (such as Scala, Go, and even recent C++ updates),
               but to understand the memory challenge, we need to go back to the early programming
               days and compare two simple code segments.
            </p>
            
            <p>The first one is a simplified version of our provisioning service, directly taken
               from our quote manager application:
            </p>
            <pre><span>public void </span>refresh() {<br/>    <span>final </span>Client client = ClientBuilder.<span>newClient</span>();<br/>    <span>try </span>{<br/>        <span>final </span>String[] symbols = getSymbols(client);<br/>        for (String symbol : symbols) {<br/>            <span>final </span>Data data = <span>client</span>.target(<span>financialData</span>)<br/>                        .resolveTemplate(<span>"symbol"</span>, symbol)<br/>                        .request(<span>APPLICATION_JSON_TYPE</span>)<br/>                        .get(Data.<span>class</span>);<br/>            <span>quoteService</span>.createOrUpdate(new UpdateRequest(data));<br/>        }<br/>    } <span>finally </span>{<br/>        client.close();<br/>    }<br/>}</pre>
            <p>The variable usage is interesting to observe. With respect to the Java variable scope, <kbd>client</kbd> is available to the complete <kbd>refresh</kbd> method, the <kbd>symbols</kbd> array is available in the <kbd>try</kbd> block. Therefore, the <kbd>for</kbd> loop and <kbd>data</kbd> are only for one iteration of the loop. However, we never really allocate any object
               memory explicitly; we can call <kbd>new</kbd> to reference a constructor but we do not have the memory vision, the pointer, or
               the size.
            </p>
            
            <p>If we compare the same code block to a version where you need to manage the memory,
               it will look as follows:
            </p>
            <pre><span>public void </span>refresh() {<br/>    <span>final </span>Client client = ClientBuilder.<span>newClient</span>();<br/>    <span>try </span>{<br/>        <span>final </span>String[] symbols = getSymbols(client);<br/>        try {<br/>            for (String symbol : symbols) {<br/>                 <span>final </span>Data data = <span>client</span>.target(<span>financialData</span>)<br/>                        .resolveTemplate(<span>"symbol"</span>, symbol)<br/>                        .request(<span>APPLICATION_JSON_TYPE</span>)<br/>                        .get(Data.<span>class</span>);<br/>                try {<br/>                    final UpdateRequest updateRequest = new<br/>                    UpdateRequest(data);<br/>                    try {<br/>                        <span>quoteService</span>.createOrUpdate(updateRequest);<br/>                    } finally {<br/>                        releaseMemory(updateRequest);<br/>                    }<br/>                } finally {<br/>                    releaseMemory(data);<br/>                }<br/>            }<br/>        } finally {<br/>            releaseMemory(symbols);<br/>        }<br/>    } <span>finally </span>{<br/>        client.close();<br/>    }<br/>}</pre>
            <p>The code gets way more complex even if this example assumes that <kbd>client.close()</kbd> handles the releasing, which cannot be true. In fact, each allocated object needs
               to call the <kbd>releaseMemory()</kbd> function to release the allocated structure. It also implies that we should not miss
               any call. Otherwise, we would be leaking memory. The previous code example uses a
               lot of nested <kbd>try</kbd>/<kbd>finally</kbd> to guarantee this.
            </p>
            
            <p>What should we learn from this simple example? We should learn that Java allows the
               developer not to care about memory management in most cases. If you use some native
               integration through JNI, for instance, you may still need to handle it. To ensure
               that the application behaves well and does not leak—which is important for a server
               that is not supposed to be restarted—the JVM provides several solutions for memory
               management. This is transparently done but directly impacts the performance, since
               memory allocation is sensitive for a process and memory deallocation has some challenges.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Garbage collector</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Garbage collector is the name of the part of the JVM that handles the memory. To make
               it very simple, it is the part releasing the memory held by some unused objects and
               reallocating this memory space to new objects.
            </p>
            
            <p>This part is dependent on the JVM you are using, but all the algorithms use the same
               sort of logic. So, it is important to understand how it works in the high level, and
               then you can investigate the specifics of your particular JVM.
            </p>
            
            <p>In the context of this book, we will limit ourselves to the HotSpot JVM (the Oracle
               one).
            </p>
            
            <p>The heap memory is divided into two main spaces: the young generation and the old
               one. The young generation is itself divided into multiple spaces: the Eden and the
               survivors (there are two survivors). Both the generations also have a <em>virtual</em> space that is mainly there to support either garbage collection operations or generation
               resizing.
            </p>
            
            <p>Since Java 8, the permanent space (used until Java 7) has been dropped and replaced
               by metaspace. Its role is to hold the metadata of the application in memory, such
               as the classes (name, annotations, fields, and so on). If you remember the previous
               chapter on how to monitor your applications, you will probably be thinking about the <kbd>jcmd GC.class_stats</kbd> command that gives you information about this space of memory. In terms of the performance,
               it is important to ensure that this space is constant once the JVM is <em>hot</em>. Concretely, it means that once we have executed all the possible code paths of our
               application, we should not see many changes on the memory allocated to that space.
               If you still see it moving significantly after that time, you may have a leak or a
               classloader issue that you'll need to investigate before you continue working on the
               performance.
            </p>
            
            <p>From now on, we will just deal with the heap. It is the part of the memory that you
               need to start with when you start tuning an application for production deployment
               or benchmark. To summarize what we have just talked about, you can visualize the way
               the memory is split with the following diagram:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="168" src="assets/6f100ec7-b72d-4507-a89a-7cec663e56db.png" width="104"/></div>
            
            <p>The global idea is to start filling the <em>first</em> zone (the <strong>Eden</strong> zone of the young generation) with <em>dynamic</em> objects—you can visualize it as request-related objects with a mental model<span>—</span>and once it is full, the garbage collector will move objects still used to the next
               zone (<strong>Survivor 1</strong>, <strong>Survivor 2</strong>, and, finally, the <strong>Tenured</strong> zone). This is a very high-level understanding of the generations. The point behind
               splitting the memory this way is that the garbage collector works on <span>zones </span>smaller than the full memory when it needs to run, and it can apply different algorithms
               to each zone and be more efficient. Keep in mind that the more you go to the old generation,
               the more objects survive the garbage collection cycles and stay in the memory in terms
               of the application runtime.
            </p>
            
            <p>The main difference between the young and old generations is the way the garbage collector
               impacts the application. While working on the young generation and running, it will
               execute what is called a <em>minor collection</em>. This browses through the corresponding zone, is generally fast, and has low impact
               in terms of the application's performance.
            </p>
            
            <p>However, when a collection is executed in the old generation, it is called a <em>major collection</em> and generally blocks the application. This means that your application may not be
               responding during the collection time, which highly affects the performance.
            </p>
            
            <p>Now there are several more detailed ways to delve into the ways the JVM garbage collection
               works, and each of them has an impact on the performance.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Garbage collector algorithms</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Over the years, the garbage collector algorithms have been enhanced and there are
               multiple algorithms available now. They match several sorts of applications and are
               more or less adapted, depending on the product:
            </p>
            
            <ul>
               
               <li><strong>Serial collector</strong>: This is a mono-threaded implementation and the default algorithm for client-side
                  machines (32 bit or single processor).
               </li>
               
               <li><strong>Parallel collector</strong>: The serial collector algorithm adapts to server resources (fast CPU and big memory
                  sizes). The parallel collector is the default one for server machines (&gt;= 2 processors).
               </li>
               
               <li><strong>Parallel compacting collector</strong>: This allows to paralellize the tenured generation processing.
               </li>
               
               <li><strong>Concurrent Mark Sweep (CMS) collector</strong>: With this collector, the tenured generation is processed in parallel with the application.
               </li>
               
               <li><strong>Garbage first collector (G1)</strong>: This collection is concurrently performed with the application that targets the
                  server application we are dealing with in the context of this book. This will be the
                  default collector with Java 9 but is already available with Java 8.
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The serial collector</h1>
            
         </header>
         
         
         <article>
            
            
            <p>To force using the serial collector, you will need to add <span>the following option</span> to your JVM:
            </p>
            <pre>-XX:+UseSerialGC</pre>
            <p>Once this option is added to the JVM, you will have a garbage collector using a single
               thread and potentially lock the application for collection.
            </p>
            
            <p>The first collection will move the still used objects from eden to the first empty
               survivor space. If the used objects are too big, they are directly moved to the tenured
               space. Younger objects of the <em>survivor 1</em> (also known as <em>survivor from</em>) space are then moved to the <em>survivor 2</em> (also known as <em>survivor to</em>) space if there is space; otherwise, they are moved directly to the tenured space.
               Older objects are moved to the tenured space.
            </p>
            
            <p>Once all these moves are done, the eden and survivor spaces (which were full) can
               be freed. Note that the survivor space is then reversed in terms of its role, which
               means that the survivor space is always empty.
            </p>
            
            <p>Here is a way to represent the algorithm:</p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="228" src="assets/7aa913cc-38a3-4fa5-a5fa-cd6361a3718b.jpg" width="198"/></div>
            
            <p>We start from a state where the eden becomes full and the first survivor has some
               objects:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="205" src="assets/8644e415-ee19-42a0-8f12-19f454940308.jpg" width="178"/></div>
            
            <p>In the first phase, the used objects are moved from eden to the second survivor if
               they fit (the two small green blocks) or directly to the tenured space if too big
               (the large block in the end):
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="196" src="assets/7f7d9996-e6ca-4a9b-b963-eb5badeb0f1d.jpg" width="170"/></div>
            
            <p>Now, the same logic is applied to the first survivor space, so we have the small used
               objects moving to the other survivor and the big ones to the tenured space. At this
               stage, you can still have unused objects in the first survivor:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="186" src="assets/f9fd35ad-2702-410b-bae7-5e4b64f14668.jpg" width="160"/></div>
            
            <p>Finally, the last step is to free the unused memory: the eden and first survivor in
               our example. In the next cycle, exactly the same logic will be followed, but the two
               survivor spaces will be reversed.
            </p>
            
            <p>This algorithm mainly concerns how the young generation is managed, but it is not
               sufficient in itself, since it will fill the old one pretty quickly. For this reason,
               a complete cycle requires a second algorithm called the Mark-Sweep-Compact, which
               is applied on the tenured space.
            </p>
            
            <p>As you can guess from its name, the algorithm has three phases:</p>
            
            <ol>
               
               <li><strong>Mark</strong>: In this phase, the collector identifies the still used instances and marks the associated
                  memory space
               </li>
               
               <li><strong>Sweep</strong>: The unmarked memory in the previous step is freed
               </li>
               
               <li><strong>Compact</strong>: The previous step may have created holes in the memory space, so the collector compacts
                  it, ensuring that all the objects are placed side by side for faster access
               </li>
               
            </ol>
            
            <p>You can visualize it—for a single memory zone<span>—</span>with the following diagram:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="74" src="assets/22b758bc-46b0-432f-895c-1e2e7b88e075.png" width="466"/></div>
            
            <p>Let's assume that our initial state is the previous diagram. We have five objects
               filling the space. The first step is then to <em>Mark</em> the still used objects to be able to remove the remaining ones (no more used). It
               can be illustrated with the following diagram:
            </p>
            
            <div class="mce-root CDPAlignCenter CDPAlign"><img height="71" src="assets/8d42ee2c-77a6-446b-8d4b-2f81d5e42b1d.png" width="449"/></div>
            
            <p>The darker blocks are the no-more-used ones in this illustration, and the lighter
               ones are the still-used ones. Now that we have proceeded from the <em>Mark</em> phase, we will execute the <em>Sweep</em> phase and remove the unused blocks:
            </p>
            
            <div class="mce-root CDPAlignCenter CDPAlign"><img height="66" src="assets/711d8745-6257-45f3-8a03-abc8284c5713.png" width="417"/></div>
            
            <p>At this point, we are fine in terms of the memory volume. This means that we've freed
               all the memory we can and we could almost stop the collection here. However, as you
               can see in this diagram, there are some holes in the free space.
            </p>
            
            <p>One issue is that if we need to allocate a big object, then it can be split into multiple
               memory zones and the memory access may become slower. To optimize this, we have the
               last step, that is, <em>Compact</em>:
            </p>
            
            <div class="mce-root CDPAlignCenter CDPAlign"><img height="66" src="assets/bd0e64a1-1b15-4336-abbe-9f407bf2f574.png" width="418"/></div>
            
            <p>After this last step, the memory is optimized (compacted); all the objects are side
               by side and we have the biggest possible available memory zone vacant at the end.
            </p>
            
            <p>For a server (don't forget that we're talking about Java EE in this book), this is
               rarely the fastest garbage collector mode, but the concepts are quite important to
               understand and if you write a Java EE client (using the JAX-RS client API, for instance)
               it can still apply to your final delivery, which will not require a lot of memory
               allocation.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Parallel and parallel compacting collectors</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The parallel collector is close to the serial collector (which is why it is important
               to understand the serial algorithm). The main difference will be how it sets up the
               collection. We saw that, in a serial collection, a single thread is responsible for
               the collection, but with the parallel collector, multiple threads adopt the role.
               This means that you can hope to reduce the <em>stop the world</em> duration (when the garbage collector enforces the application to stop responding
               and do its job) by the number of threads (theoretically).
            </p>
            
            <p>This can be activated/enforced (this is supposed to be the default for a server machine)
               by adding this option <span>on the JVM</span>: <kbd>-XX:+UseParallelGC</kbd>.
            </p>
            
            <p>The parallel collector can be tuned with several JVM options, but here are some you
               may want to customize:
            </p>
            
            <ul>
               
               <li><kbd>-XX:MaxGCPauseMillis=N</kbd>: You configure the value (<kbd>N</kbd>) to the maximum duration you desire for the pauses of the garbage collector. Note
                  that this is just a hint and there is no guarantee that you will obtain the desired
                  result, but it can be interesting as a first attempt. Also keep in mind that it will
                  optimize the garbage collector pause duration (and, maybe, not the throughput of the
                  application, which can be an issue for the server). In the end, you may manually tune
                  the heap size and ratio, but starting to test this JVM option can give you some hints
                  about the way to go.
               </li>
               
               <li><kbd>-XX:GCTimeRatio=N</kbd>: This is also a hint, allowing you to request that no more than <em>1/(1+N)</em> of the application time is spent in garbage collection. This is intended to optimize
                  the throughput of the application<span>—</span>which is generally the case for a server. This is not the easiest configuration to
                  understand, so let's use a small example. If you set <em>N</em> to 19, then 1/20 of the application time (5%) will be allocated to garbage collection.
                  The default is 99 so only 1% of the application time is allocated to garbage collection.
               </li>
               
               <li><kbd>-XX:ParallelGCThreads=N</kbd>: This allows you to configure the number of threads allocated to parallel garbage
                  collection.
               </li>
               
            </ul>
            
            <p>There are no magic values for these flags, but once you have identified the memory
               needs of your application, it is very interesting to tune them a bit to be aligned
               with the application requirements.
            </p>
            
            <p>What is interesting to know is that the garbage collector can adjust the generation
               sizes, depending on the configured ratio to try to respect it. This is done by increasing
               the generation sizes to decrease the collection times. By default, a generation size
               is increased by 20% and decreased by 8%. Here again, you can customize this adjustment
               with some JVM flags.
            </p>
            
            <p>There are lots of flags, and their name and supported values can depend on the JVM
               you use. Since they are not portable flags, you may need to check them against your
               JVM documentation. With the Hotspot, you can do it with the following command:
            </p>
            <pre><strong>java -XX:+PrintFlagsFinal -version</strong></pre>
            <p>This command prints the Java version (just to avoid any error) and also prints the
               JVM flags, allowing you to list them all, which is the part we care about. Once you
               have the output, you just need to filter the flags you want. In our case, we want
               to customize the way the generation sizes are increased/decreased, so we can filter
               the flags with the <kbd>grep</kbd> command (on Unix) using the <kbd>Generation</kbd> keyword:
            </p>
            <pre><strong>$ java -XX:+PrintFlagsFinal -version 2&gt;&amp;amp;amp;1 | grep Generation</strong><br/><strong>    uintx TenuredGenerationSizeIncrement = 20 {product}</strong><br/><strong>    uintx TenuredGenerationSizeSupplement = 80 {product}</strong><br/><strong>    uintx TenuredGenerationSizeSupplementDecay = 2 {product}</strong><br/><strong>     bool UseAdaptiveGenerationSizePolicyAtMajorCollection = true {product}</strong><br/><strong>     bool UseAdaptiveGenerationSizePolicyAtMinorCollection = true {product}</strong><br/><strong>    uintx YoungGenerationSizeIncrement = 20 {product}</strong><br/><strong>    uintx YoungGenerationSizeSupplement = 80 {product}</strong><br/><strong>    uintx YoungGenerationSizeSupplementDecay = 8 {product}</strong></pre>
            <p><kbd>+PrintFlagsFinal</kbd> allows you to list the options, their value (after the equal sign), their type (first
               string), and the flag type (in braces).
            </p>
            
            <div class="packt_tip">If you add other options to the JVM (such as <kbd>-client</kbd> or <kbd>-server</kbd>), it will adjust the values to reflect these flags.
            </div>
            
            <p>As you can see in the previous capture, you can use <kbd>-XX:YoungGenerationSizeIncrement</kbd> to customize the increase percentage of the young generation when needed to respect
               the configured ratio. <kbd>YoungGenerationSizeSupplement</kbd> is a supplement for the young generation size increment used at startup, and the
               decay flag is the decay factor to the supplement value. Indeed, the tenured generation
               has the same sort of configuration.
            </p>
            
            <p>These configurations are very advanced and you must ensure that you can explain why
               you tune them before doing it; otherwise, you can just mess up your JVM configuration
               and make your application behave badly.
            </p>
            
            <p>Finally, parallel GC still uses a single thread for the tenured generation collection.</p>
            
            <p>Now, there is also a compacting parallel collector. It is the same as the parallel
               collector in the young generation, but in the tenured one, the algorithm differs a
               bit. It is close to the <em>Mark</em>/<em>Sweep</em>/<em>Compact</em> algorithm, except it divides the space in more zones to let the collector work in
               parallel on them. Then, the sweep phase is replaced by a <em>summary</em> phase, where the density is verified to request a compaction. Finally, the compaction
               is done in parallel. To use this option, you need to activate another JVM flag: <kbd><span>-XX:+UseParallelOldGC</span></kbd>. This option is supposed to be good for applications with large heaps, which is the
               case with applications handling a lot of concurrent requests and using some caching
               mechanism.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Concurrent Mark Sweep (CMS)</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The goal of the CMS is to reduce GC pauses, enabling you to execute the GC while the
               application is running. For this, it uses multiple threads. The main idea is to be
               able to free some memory before the tenured generation is full, and the GC needs to
               pause the application threads. If it happens too frequently, you will need to adjust
               the application tuning (CMS configuration) to avoid it as much as possible and to
               keep it behaving correctly. You can identify this type of issue checking <kbd>verbose:gc</kbd> output and <kbd>Concurrent Mode Failure</kbd> messages.
            </p>
            
            <p>Compared to the standard <em>Mark</em> phase, the one of the <em>CMS</em> algorithm will pause the application twice: first pause to mark objects directly
               reachable from the root of the memory graph and the second pause to identify the objects
               missed in the concurrent tracing phase. This concurrent tracing phase will take resources
               that the application will not be able to use anymore, and the throughput will potentially
               decrease a bit.
            </p>
            
            <p>A collection mainly has two triggers:</p>
            
            <ul>
               
               <li>A kind of timeout based on the historical statistics of the memory, with a safety
                  bound added to avoid a concurrent mode failure, which is very costly. This safety
                  bound can be controlled based on a percentage, customizable with the <kbd>-XX:CMSIncrementalSafetyFactory=N</kbd> <span>flag. </span></li>
               
               <li>Based on the remaining size on the tenured space. This trigger can be controlled with <kbd>-XX:CMSInitiatingOccupancyFaction=N</kbd>, <em>N</em> being the percentage of the tenured space that should trigger a collection if it
                  is full. By default, it is 92% (N=92).
               </li>
               
            </ul>
            
            <p>When investigating long GC pauses, you may need to tune the safety factor and, potentially,
               the occupancy fraction options to see if triggering the GC earlier <span>(or later)</span> can help.
            </p>
            
            <p>One last thing to note about this algorithm is that there is no compaction, so the
               memory access can become slower with time.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Garbage First (G1)</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The Garbage First collector is the most recent implementation. It is a server-side
               implementation that tends to decrease the pauses and concurrently works with the application.
               It introduces a new way of visualizing the heap.
            </p>
            
            <p>The heap is divided into constant-sized regions. The G1 starts to mark the regions
               concurrently. After this phase, it knows which regions are almost empty and then starts
               collecting the memory from these regions, allowing the G1 to get a lot of memory pretty 
               quickly and without much effort. This is where the name of this algorithm comes from.
               Then, in the compact phase, G1 can copy objects from multiple regions to a single
               region to ensure that it stays efficient.
            </p>
            
            <p>An important thing to know about the G1 implementation is that it is based on statistics
               and can have a few glitches in the execution, even if the model is quite accurate
               in practice.
            </p>
            
            <p>The fact that G1 splits the heap in regions also means that it is intended for applications
               using a lot of memory (Oracle claims more than 6 GB) and requiring small pause times
               (less than 0.5 seconds). This implies that switching from the CMS to the G1 is not
               always worth it; you should ensure that you meet one of these criteria before switching
               to the G1:
            </p>
            
            <ul>
               
               <li>~50% of the heap is used by live data</li>
               
               <li>The statistics are not that accurate with the CMS algorithm (if the allocation rate
                  varies a lot)
               </li>
               
               <li>You have identified long GC pauses</li>
               
            </ul>
            
            <p>The G1 also has several JVM flags that you can use to customize the behavior the garbage
               collector should take. For instance, <kbd>-XX:MaxGCPauseMillis</kbd> sets the pause duration that you accept in milliseconds (statistically once again)
               and <kbd>-XX:ConcGCThreads=N</kbd> defines the concurrent number of threads used to mark the regions (recommended setting
               is about 25% of the parallel GC thread count). Default settings are intended for the
               most common use cases but you may need to refine the configuration to adapt it to
               your application and the way it uses (or reuses) memory.
            </p>
            
            <p>Here again, activating the G1 collector needs its own JVM flag: <kbd>-XX:UseG1GC</kbd>.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Common memory settings</h1>
            
         </header>
         
         
         <article>
            
            
            <p>We saw how the memory was collected and that there are a lot of tuning options (once
               again, don't forget to check your particular JVM options). However, there are some
               very common memory settings that you will want to customize before finely tuning the
               collector. Here is a small table with these memory settings:
            </p>
            
            <table>
               
               <tbody>
                  
                  <tr>
                     
                     <td><strong>Option</strong></td>
                     
                     <td><strong>Description</strong></td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td><kbd>-Xmx&lt;size&gt;</kbd></td>
                     
                     <td>The maximum memory size allocated to the heap, for example, <kbd>-Xmx1g</kbd> to allow the heap to grow until 1GB
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td><kbd>-Xms&lt;size&gt;</kbd></td>
                     
                     <td>The starting memory size allocated to the heap, <span>for example,</span> <kbd>-Xms512m</kbd> <span>to allocate 512 MB to the heap</span></td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td><kbd>-Xss&lt;size&gt;</kbd></td>
                     
                     <td>The stack size (can avoid <kbd>StackOverflowError</kbd>)
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td><kbd><span>-XX:SurvivorRatio=N</span></kbd></td>
                     
                     <td>Ration between eden and survivor spaces</td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td><kbd>-XX:MinHeapFreeRatio=N</kbd> and <kbd>-XX:MaxHeapFreeRatio=N</kbd></td>
                     
                     <td>The ratio to trigger a heap resize, the min flag will trigger a heap increase, and
                        the max flag will trigger a heap size decreasing
                     </td>
                     
                  </tr>
                  
               </tbody>
               
            </table>
            
            <p>With all that we saw previously, it means that tuning the JVM memory can lead to a
               big set of options/flags, but don't be afraid, it is just a matter of taking control
               over its application. Here is a complete flag set example:
            </p>
            <pre class="oac_no_warn">-Xms24G -Xmx24G -XX:+UseG1GC -XX:MaxGCPauseMillis=150 -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4 -XX:InitiatingHeapOccupancyPercent=70 ....</pre>
            <p>This command is a common server memory configuration for a server with a lot of available
               memory for the application(s) you deploy:
            </p>
            
            <ul>
               
               <li>Allocate 24 Gigabytes for the heap (fixed since the min and max memory size are set
                  to the same size).
               </li>
               
               <li>Enforce the JVM to use the G1 collector (accurate, as we use more than 6 GB of heap)
                  and customizes G1 configuration. It defines a targeted max GC pause of 150 ms and
                  requests G1 to use 16 parallel threads for the memory collection and four threads
                  to mark regions once collected.
               </li>
               
               <li>Finally, the collection cycles will start if the heap is occupied at 70%.</li>
               
            </ul>
            
            <p>This is not a bad setting to start with for a server (you can increase the number
               of threads a bit, but not too much, as they can be used at the same time your application
               is running); increase the max GC pause if you can accept it.
            </p>
            
            <p>The parameter you will probably tune the most is the heap size (24 GB in the previous
               example), depending on the requirements of your application.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Debugging GC behavior</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Now that we know how to tune the memory quite finely, we need to know what our application
               does, to be able to adjust the configuration. To do so, the JVM provides several ad
               hoc tools.
            </p>
            
            <p>The most common and, probably, the most useful tool for the memory is the <kbd>-verbose:gc</kbd> option that you can pass launching your JVM. It will output memory information. If
               we activate it on our quote application, you will quickly see these sorts of lines:
            </p>
            <pre>[GC (Allocation Failure) 41320K-&gt;14967K(153600K), 0.0115487 secs]<br/>[GC (Metadata GC Threshold) 44213K-&gt;20306K(153600K), 0.0195955 secs]<br/>[Full GC (Metadata GC Threshold) 20306K-&gt;13993K(139264K), 0.0596210 secs]<br/>[GC (Allocation Failure) 77481K-&gt;23444K(171008K), 0.0081158 secs]<br/>[GC (Metadata GC Threshold) 69337K-&gt;23658K(207360K), 0.0094964 secs]<br/>[Full GC (Metadata GC Threshold) 23658K-&gt;20885K(248320K), 0.0792653 secs]<br/>[GC (Allocation Failure) 144789K-&gt;27923K(252416K), 0.0078509 secs]<br/>[GC (Allocation Failure) 155923K-&gt;36753K(252416K), 0.0174981 secs]<br/>[GC (Metadata GC Threshold) 68430K-&gt;37173K(275968K), 0.0146621 secs]<br/>[Full GC (Metadata GC Threshold) 37173K-&gt;31086K(321536K), 0.1868723 secs]</pre>
            <p>We can distinguish two kinds of lines corresponding to different generation zone collection:</p>
            
            <ul>
               
               <li><strong>GC</strong>: This is a young generation collection
               </li>
               
               <li><strong>Full GC</strong>: This is a tenured generation collection
               </li>
               
            </ul>
            
            <p>Each collection is associated with a reason:</p>
            
            <ul>
               
               <li><strong>Allocation failure</strong>: The GC is asked to run because no more memory is available
               </li>
               
               <li><strong>Metadata GC threshold</strong>: The metaspace threshold is reached
               </li>
               
            </ul>
            
            <p>Then, you can see the resizing of the memory; for instance, <kbd>41320K-&gt;14967K(153600K)</kbd> means that the used memory is resized from ~41M to ~15M. The number in the parentheses
               is the available space (~150 MB here).
            </p>
            
            <p>Don't be afraid to see the GC running often and even the full GC lines. While their
               executions are fast and you see that the memory size is acceptable, it is not an issue
               at all. However, if the execution is long and the memory stays high even after the
               collection, then you will need to tune the GC or update the application to ensure
               it comes back to something fast and reliable.
            </p>
            
            <div class="packt_tip">You can activate this output at runtime through JMX if you go on the <kbd>java.lang:type=Memory</kbd> MBean and set <span>the </span><kbd>Verbose</kbd> attribute to <kbd>true</kbd>.
            </div>
            
            <p>If you want even more details about the GC, you can add the <kbd><span>-XX:+PrintGCDetails</span></kbd> option and the lines will be more verbose:
            </p>
            <pre>[GC (Allocation Failure) [PSYoungGen: 145920K-&gt;16360K(162304K)] 177095K-&gt;54877K(323072K), 0.0152172 secs] [Times: user=0.03 sys=0.01, real=0.02 secs]</pre>
            <p>We can recognize the previous information but there is new data as well:</p>
            
            <ul>
               
               <li><kbd>PSYoungGen</kbd>: This is the collector type; this value means minor GC
               </li>
               
               <li>The resizing after (<kbd>X-&gt;Y(Z)</kbd>) shows the young generation size
               </li>
               
               <li>The last resizing is the full heap one</li>
               
               <li>Finally, the duration is expressed in terms of user time, system time, and real time</li>
               
            </ul>
            
            <p>If you want to save all this information (log lines) in a file, you can add a flag
               with a file path and the JVM will dump this output in the file instead of the console:
            </p>
            <pre><strong>-Xloggc:/path/to/output.log  -XX:+PrintGCDetails</strong></pre>
            <p>This will allow you to analyze the GC behavior <em>offline</em>. There are some tools able to parse this output and let you visualize it directly.
               One of them is <em>GCViewer</em>, which you can find at <a href="https://github.com/chewiebug/GCViewer/wiki/Changelog">https://github.com/chewiebug/GCViewer/wiki/Changelog</a>. Once the JAR is downloaded, you can run it with Java directly:
            </p>
            <pre><strong>java -jar gcviewer-1.35.jar</strong></pre>
            <p class="mce-root">Then, just open the <kbd>output.log</kbd> file in the interface and you should visualize your GC as follows:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img src="assets/78dc1415-597d-4a0f-a1a4-9ccb6022da73.png"/></div>
            
            <p>This tool has two main interesting features:</p>
            
            <ul>
               
               <li>The temporal graph deduced from the log output</li>
               
               <li>The statistics tabs (on the right) showing the statistic summary of the GC, and the
                  number and duration of the pauses
               </li>
               
            </ul>
            
            <p>This last information can let you validate the behavior of your application once you
               add some caching or some background task parallelly executed with the main application.
            </p>
            
            <p>It will let you ensure the impact of one part of the application on the other in terms
               of memory and potentially find a performance issue if the memory is too impacted by
               this new feature.
            </p>
            
            <p>Here is the legend associated with the colors of the graph:</p>
            
            <div class="CDPAlignCenter CDPAlign"><img src="assets/7c9c01cd-e77b-49df-966c-336bdf0f583d.png"/></div>
            
            <p>We find most of the information we talked about in the previous part and, particularly,
               the collections, generations, and so on. Note that the <em>Y</em> axis can be read with two units (time and memory size) depending on which graph you
               are looking at. You will likely filter the printed graphs to see it clearly, but you
               will find all the information you need to see when your GC is running.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Heap dump</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Sometimes, you will need to get a heap dump to investigate what is in memory and why
               the GC runs so often or for so long. To do so, the JDK provides a tool called <kbd>jmap</kbd> that allows you to take a dump from the Java PID:
            </p>
            <pre><strong>jmap -dump:format=b,file=/path/to/dump.hprof &lt;PID&gt;</strong></pre>
            <p>This command will stop the application (a bit like a <em>stop the world</em> pause) and write all the instances into the configured file. Opening the output with
               <kbd>jvisualvm</kbd> will enable you to investigate the instances, and, in particular, <span>the number of instances and </span><span>the corresponding allocating size.</span></p>
            
            <p>Here is what it can look like:</p>
            
            <div class="CDPAlignCenter CDPAlign"><img src="assets/831907f8-7b00-40d1-a064-641beb3e487a.png"/></div>
            
            <p>Normally, you should mainly see JVM classes such as <kbd>char[]</kbd>, <kbd>String</kbd>, <kbd>Map</kbd>, and so on.
            </p>
            
            <p>If you double-click on a type, you will see the instances. For example, on <kbd>String</kbd> for our quote manager dump, we will have a view like this one:
            </p>
            
            <p class="mce-root"/>
            
            <div class="CDPAlignCenter CDPAlign"><img height="434" src="assets/ac3cc129-cce2-4737-a950-d6539a5ca24c.png" width="307"/></div>
            
            <p>Here, we visualize the <kbd>String</kbd> instances related to the request (the request URL or a subpart of it, such as the
               protocol, the host, or the port). So we can deduce that the instances are related
               to the request and its parsing. This is perfectly normal, but if you start seeing
               a lot of your own classes <span>in the classes, view</span>, and/or a few classes but with a huge allocation size, you can double-click on these
               types and get the instance details to identify why it happened.
            </p>
            
            <p>If we double-click on <kbd>JsonQuotePage</kbd>, we will see something like this:
            </p>
            
            <div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/2c887942-c264-422b-8209-8072abd044ce.png"/></div>
            
            <p>The reference of the instance on the left side (in the list of instances) doesn't
               help us much, but the detailed view (on the right side) of a single instance shows
               the actual data structure with its values.
            </p>
            
            <p>As a reminder, our POJO page model is the following one:</p>
            <pre><span>@JsonbPropertyOrder</span>({<span>"total"</span>, <span>"items"</span>})<br/><span>public class </span>JsonQuotePage {<br/>    <span>private long </span><span>total</span>;<br/>    <span>private </span>List&lt;JsonQuote&gt; <span>items</span>;<br/><br/>    // getters/setters<br/>}</pre>
            <p>Our structure (class) has two fields: <kbd>total</kbd> and <kbd>items</kbd>, and we can find these two entries in the detailed view of the instance we obtained
               from the dump. As shown in the screenshot, we can even browse the <kbd>items</kbd> values. This will let you identify whether there is something particular with the
               values and whether you will need to customize the query you use to retrieve the data.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Java EE and the resources</h1>
            
         </header>
         
         
         <article>
            
            
            <p>With Java EE, you will likely have a lot of volatile objects <span>(with a short lifespan) </span>that are bound to a request. If we take <kbd>QuoteResource</kbd>, we will first allocate our JSON model, then our entity model, and so on. All these
               instances will be needed for the request only and nothing more. Thus, the garbage
               collector will quickly collect them. The garbage collector is quite good for such
               dynamic applications. However, it doesn't mean that we do not have long living instances.
               Even without an application cache, the server caches a lot of metadata to ensure that
               it works and runs fast. One example is the CDI container, which will keep the metadata
               of all the beans in the memory to make sure that it can create them when requested
               by the application. This takes memory and will never be released until the application
               is not deployed. This means that by tuning the memory of your application, you will
               also need to ensure that you tune the memory of the server. As already explained earlier,
               in performance tuning, the application is not only composed of your code but also
               the server code; otherwise, you will miss most of the logic and you can't be accurate
               tuning your application.
            </p>
            
            <p>In general, the server adds some long live instances to ensure that the server layer
               does not slow your application down. However, there is a particular kind of logic
               that the server implements for you that directly impacts the performance of your application,
               and that you will need to watch out for: the resources.
            </p>
            
            <p>What we call resources in an application server is generally what is managed by the
               server. It is often something you want to be able to configure outside the application
               even if Java EE 6 introduced some <kbd>@XXXDefinition</kbd> annotations to be able to do it from the application itself. The goal is to inject
               a preconfigured—and adapted to the current environment<span>—</span>instance. It is also often connected to something external or directly related to
               the machine resources. Since the next chapter will be about threading, here, we will
               focus on the external resources.
            </p>
            
            <p>On top of the list of the most used resources, we find <kbd>DataSource</kbd>. This represents a connection to a database such as MySQL in our quote application.
               To cite a few other well-known resources before digging into the <kbd>DataSource</kbd> case, you can encounter the following:
            </p>
            
            <ul>
               
               <li>Concurrent resources (<kbd>ManagedExecutorService</kbd>) to handle concurrency. We will deal with them in the next chapter.
               </li>
               
               <li><kbd>DataSource</kbd>, which connects to the databases.
               </li>
               
               <li>The JMS resources, such as <kbd>ConnectionFactory</kbd>, which connects to a broker, and <kbd>Queue</kbd> and <kbd>Topic</kbd>, which represent a destination.
               </li>
               
               <li>The Java mail <kbd>Session</kbd>, which allows you to interact with a mail box (often used to send mails from the
                  application).
               </li>
               
               <li>The resource adapters, which provide a way of handling inputs/outputs in an EE fashion
                  (integrated with security and transactions).
               </li>
               
            </ul>
            
            <p>All these resources are configurable in the server<span>—</span>most of them from the application<span>—</span>and allow the operation team (or the person responsible for the deployment) to tune
               the resource the application is using. It also keeps the developer from having to
               care about the configuration and ensure that the deployment will be customizable enough.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">DataSource</h1>
            
         </header>
         
         
         <article>
            
            
            <p>To illustrate it, we will refer to our <kbd>DataSource</kbd> example. In the first chapter, we configured a pool and a data source. The data source
               was just a name in the JNDI that the application was using to find the data source
               and later uses it in its JPA layer. This is pretty much what a data source is: a connection
               factory. What is crucial here is the way the connections are managed.
            </p>
            
            <p>Most of the time, the production data sources are remote processes or, at least, require
               a network connection. If you remember our MySQL connection URL, we used the following:
            </p>
            <pre>jdbc:mysql://localhost:3306/quote_manager</pre>
            <p>This is a very common <em>development</em> configuration and it connects to <kbd>localhost</kbd><em>.</em> Thus, the network cost is the machine local loop cost, which is generally optimized
               to be very fast. In real deployment, you will more likely connect to the following:
            </p>
            <pre>jdbc:mysql://application.mysql.company.com:3306/quote_manager</pre>
            <p>This URL will mark a remote host, and obtaining a connection will imply some real
               latency. Indeed, it will not be as slow as an internet connection, where the network
               has a lot of switches, routers, and hops. However, going over the network always implies
               some milliseconds of latency.
            </p>
            
            <p>What is interesting with data sources (it is also the case for JMS resources in general)
               is that the protocol associated with the connection is a connected protocol. Understand
               that once the connection is obtained, sending a command is faster, as you don't have
               to create another connection.
            </p>
            
            <p>If you ran the quote manager application, you would probably get some warning like
               this one:
            </p>
            <pre>Sat Sep 30 17:06:25 CEST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</pre>
            <p>This warning is issued by the MySQL driver and encourages you to use SSL for the network
               communication. This is mainly for security reasons, but the SSL connection is even
               slower. You can check out the HTTP2 protocol (introduced in Servlet 4.0 release, included
               in Java EE 8) that introduces the push protocol to avoid doing too many connections
               from the browser over SSL, because it was starting to be really slow with modern applications
               having a lot of web resources.
            </p>
            
            <p>This means that reusing the same connection over and over is always a good idea because
               it will cut down the actual connection step and will just let the application issue
               commands and do its business.
            </p>
            
            <p>To solve this <em>connection</em> issue, the server configures the data source as a connection pool. A pool is a set
               of reusable connections. Each server has its own pool and its own configuration, which
               are more or less advanced but all share the same logic:
            </p>
            
            <ul>
               
               <li><strong>Some size configuration</strong>: The number of connections to keep in the memory while the server is idled (min),
                  the number of connections that can be created (max), how long a connection can be
                  awaited, and so on
               </li>
               
               <li><strong>Some eviction configuration</strong>: The way to determine a connection should be dropped from the pool
               </li>
               
            </ul>
            
            <p>The eviction is very important as the pool is related to long running connections.
               You, therefore, need to be able to remove transparently the corrupted connections.
               For instance, the MySQL server drops a connection after 8 hours by default. So, if
               you run your application for 8 hours and 1 minute, all the requests will start failing
               if you don't have any eviction. For a data source, this is generally done with <kbd>ValidationQuery</kbd>, which is a SQL query executed from time to time. If this fails, it is considered
               as invalidating the connection and removing it from the pool. The <em>time to time</em> can generally be configured too, and can mainly mean three moments: <em>before the connection is shared and given to the application</em>, <em>when the connection is given back to the pool</em> (the application is done with it), or <em>in a background thread when the connection is idled</em>.
            </p>
            
            <p>The choice is a trade-off between <em>do you accept a few requests failing because the connection was not yet evicted by
                  the background thread</em> and <em>do you accept it to be a bit slower because the connection is validated every time</em>. You can even mix all these validation types. A very important point here is to make
               sure that the validation query is very fast to execute. Never use <kbd>SELECT * from CUSTOMER</kbd>, which can return back thousands (or more) lines. Always use a constant result set
               query. For MySQL, it is recommended that you use <kbd>SELECT 1</kbd>, which will just return <kbd>1</kbd>. Each database has this kind of query, so check it out and ensure that it is well
               configured in your pool.
            </p>
            
            <p>You will indeed find other configurations depending on the pool you rely on, but they
               are less impacting on the performance, so we will not detail them here.
            </p>
            
            <p>One crucial thing about the connection pools<span>—</span>data source ones or not<span>—</span>is to make sure you are not waiting for them. As suggested earlier, if you are waiting
               for connections, then you are slower than you should be, as the pool should avoid
               this case. This means that when it comes to the performance, you don't have to hesitate
               and should just ask your pool to never wait for a connection. This parameter is often
               called <em>Max Wait Time</em>. Setting it to 0 or something very small can cause your application to start failing
               with exceptions saying that the application was not able to retrieve a connection
               in the configured time. This is exactly what we want! This means that the pool is
               too small for the application's needs and, thus, you need to increase the pool size.
               Alternatively, you can also decrease the load sent to the server. It doesn't mean
               you shouldn't configure any wait time in production. This is a parameter that can
               ensure you respond to your clients, but it will potentially increase your response
               time. If it is acceptable, don't hesitate to increase the value once you've tuned
               the application, but don't do it before ensuring that you are not using a wrongly
               configured pool.
            </p>
            
            <p>If your pool size is too small, you will likely see it in the stack traces or in the
               monitoring you set up with the invocation, such as <kbd>allocateConnection</kbd> or <kbd>getConnection</kbd>, depending on your server.
            </p>
            
            <p>In terms of size tuning, it is tempting to simply set as many connections as possible,
               but you need to keep in mind a few important points:
            </p>
            
            <ul>
               
               <li>The database connection count can be limited. Never allocate all of them to your application
                  cluster, and ensure that you can keep at least a few for maintenance purposes. For
                  instance, if your MySQL allows 152 connections (default <kbd>max_connections</kbd> of MySQL is 151 and <kbd>mysqld</kbd> allows <kbd>max_connections+1</kbd> actual connections), then your application<span>—</span>as a cluster<span>—</span>can use 140 connections.
               </li>
               
               <li>The pool handling requires you to deal with concurrency. Your application will request
                  connections from all its threads and then the container pool will need to give different
                  connections to all the threads to ensure that there is no data corruption of cross-thread
                  connection usage. On the other hand, eviction will also happen at the same time the
                  application is running and will potentially lock the pool (it is close to the GC algorithm
                  problem). The smaller the work needed to be done on the pool, the better it will behave
                  and the less it will impact the application.
               </li>
               
            </ul>
            
            <p>This means that not over adjusting the pool is also important. It is generally a heuristic
               you should do based on your application and the targeted SLA. If you have no idea
               about the size you must use, you can start setting the maximum size of the pool to
               25% of your maximum concurrent threads (HTTP pool size, for instance) and then increase
               it until you don't get an error anymore, with a maximum wait time of 0.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Java EE and pools</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Java EE has a lot of pool types involved in its stack. Here is a summary of the ones
               you may want to have a look at while tuning your application. Each time, the tuning
               logic will be based on the same logic: no wait time and start with a medium size in
               order not to over allocate resources and impact the application too much.
            </p>
            
            <table>
               
               <tbody>
                  
                  <tr>
                     
                     <td><strong>Pool Type</strong></td>
                     
                     <td><strong>Description</strong></td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>The <kbd>DataSource</kbd> pool
                     </td>
                     
                     <td>This handles and recycles database connections.</td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>The <kbd>ConnectionFactory</kbd> pool
                     </td>
                     
                     <td>This handles JMX connections to the broker.</td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>The <kbd>@Stateless</kbd> pool
                     </td>
                     
                     <td>This handles the recycling of stateless instances. Today, it is relevant when stateless
                        instances are used as a poor man throttling implementation (max instance = max concurrency)
                        or when the instances access some thread of unsafe resources, which is very expensive
                        to instantiate. It can be seen as the Java EE pool API for application needs.
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>The HTTP thread pool</td>
                     
                     <td>These threads are used to server the requests. They often have some sibling threads
                        to accept the connection (selector threads).
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>The managed thread pool</td>
                     
                     <td>This is usable by the application; these thread pools are used to execute custom tasks.
                        They are often used in reactive programming to inherit from EE features in a reactive
                        stack, such as RxJava.
                     </td>
                     
                  </tr>
                  
                  <tr>
                     
                     <td>The Resource Adapter / JCA Connector pool</td>
                     
                     <td>The JCA specification defines multiple pools: the instance pools that are configured
                        in a dependent way but share the same principle as the other pools, and the thread
                        pool using <kbd>WorkManager</kbd>, which is the pre EE concurrency utility way to have a user thread pool injected
                        by the server into the application (connector here). Applications rarely use JCA connectors
                        today, but if you have inherited one, ensure that it is well tuned and integrated
                        with your server.
                     </td>
                     
                  </tr>
                  
               </tbody>
               
            </table>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Java EE and HTTP pools</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Even if Java EE containers are more and more commonly used for daemons and standalone
               applications, most of the developed applications are still web applications and, therefore,
               use HTTP either as clients of another server or as the server themselves.
            </p>
            
            <div class="packt_infobox"><span>In the last sentence, <em>Java EE containers</em> include the wide embedded varieties of containers such as TomEE Embedded, Apache
                  Meecrowave, WildFly Swarm, and so on, and is not limited to standalone containers
                  or full profile servers.</span></div>
            
            <p>This means that the Java EE configuration will have to deal with HTTP configuration.
               It needs to be handled at multiple levels (networks, HTTP caching, and so on) but
               also in multiple layers (server/HTTP connector, client connection pooling, SSL tuning,
               and so on).
            </p>
            
            <p>We will delve into more details about this in <a href="cdbfd25b-1b43-4b9d-b45c-78586b39ebb5.xhtml">Chapter 5</a>, <em>Scale Up: Threading and Implications</em> for the pooling - related configuration and in <a href="8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml">Chapter 6</a>, <span><em>Be Lazy, Cache Your Data</em> </span>for the caching configuration.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Java EE implicit features</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The Java EE philosophy has always been <em>to work out of the box</em>. However, it may have some impact on the performance as, sometimes, features are
               activated without your application requiring them at all. If you know that you don't
               need a feature, don't hesitate to disable it.
            </p>
            
            <p>For instance, in <kbd>persistence.xml</kbd><em>,</em> we disabled bean validation integration adding this line:
            </p>
            <pre>&lt;validation-mode&gt;NONE&lt;/validation-mode&gt;</pre>
            <p>This avoids the JPA provider adding the bean validation listeners it uses to validate
               the entities and, thus, saves some CPU cycles without impacting the application. If
               you check the chain of our application, we use JAX-RS (has a bean validation integration)
               and then JPA (has another bean validation integration).
            </p>
            
            <p>The rule for validation can be to always validate the data when it is entering the
               system (JAX-RS for us) and not validate them internally. In fact, it is redundant
               in terms of application logic. This is why it is fine to disable it at the JPA layer,
               for instance.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Summary</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In this chapter, we saw how memory is managed with Java, and how to influence it to
               optimize and adapt it to your application requirements. We also saw how the Java EE
               server-provided resources help you save time, as they not only enable you to skip
               reconnection time between usages but also imply a dedicated tuning to not abuse the
               server memory and CPU.
            </p>
            
            <p>The idea behind this chapter is to ensure that you have the keys and knowledge to
               be able to investigate any issue with memory and to be able to tune the memory and
               resources without being lost or using some random numbers.
            </p>
            
            <p>Also, this part is probably the most unportable one and will be related to the JVM
               (for the memory) and server (for the resources) you'll use in your deployment. All
               the concepts will still apply but the way you tune them can differ, since this is
               not something standardized yet—even if the JVM tuning does not change as much as server
               configurations. However, don't hesitate to check out your JVM or server documentation
               and make sure to have read it before entering into a benchmark phase in order not
               to lose time in testing options you don't know upfront.
            </p>
            
            <p>In the next chapter, we will see how to make the most of Java EE concurrent programming
               and how it is linked to the Java EE threading model to ensure that your application
               can scale. The memory and the CPU are the two most central resources a server uses
               on a machine: we just saw the memory resource, and we will now deal with the CPU through
               the threading study.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   </body></html>