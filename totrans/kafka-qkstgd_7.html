<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">KSQL</h1>
                </header>
            
            <article>
                
<p><span>In previous chapters, we wrote Java code to manipulate data streams with Kafka, and we also we built several Java processors for Kafka and Kafka Streams. In this chapter, we will use KSQL to achieve the same results.</span></p>
<p>This chapter covers the following topics:</p>
<ul>
<li>KSQL in a nutshell</li>
<li>Running KSQL</li>
<li>Using the KSQL CLI</li>
<li>Processing data with KSQL</li>
<li>Writing to a topic</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">KSQL in a nutshell</h1>
                </header>
            
            <article>
                
<p>With Kafka Connect, we can build clients in several programming languages: JVM (Java, Clojure, Scala), C/C++, C#, Python, Go, Erlang, Ruby, Node.js, Perl, PHP, Rust, and Swift. In addition to this, if your programming language is not listed, you can use the Kafka REST proxy. But the Kafka authors realized that all programmers, especially data engineers, can all talk the same language: <strong><span>Structured Query Language</span></strong> (<strong><span>SQL</span></strong>). So, they decided to create an abstraction layer on Kafka Streams in which they could manipulate and query streams using SQL.</p>
<p>KSQL is a SQL engine for Apache Kafka. It allows writing SQL sentences to analyze data streams in real time. Remember that a stream is an unbounded data structure, so we don't know where it begins, and we are constantly receiving new data. Therefore, KSQL queries usually keep generating results until you stop them.</p>
<p class="mce-root"/>
<p>KSQL runs over Kafka Streams. To run queries over a data stream, the queries are parsed, analyzed, and then a Kafka Streams topology is built and executed, just as we did at the end of each <kbd>process()</kbd> method when running Kafka Streams applications. KSQL has mapped  the Kafka Streams concepts <span>one to one</span>, for example, tables, joins, streams, windowing functions, and so on. </p>
<p>KSQL runs on KSQL servers. So if we need more capacity, we can run one or more instances of KSQL servers. Internally, all the KSQL instances work together, sending and receiving information through a dedicated and private topic called <kbd>_confluent-ksql-default__command_topic</kbd>.</p>
<p>As with all Kafka technologies, we can also interact with KSQL through a REST API. Also, KSQL, has its own fancy <strong><span>command-line interface</span></strong> (<strong><span>CLI</span></strong>). If you want to read more about KSQL read the online documentation at the following URL: <a href="https://docs.confluent.io/current/ksql/docs/index.html">https://docs.confluent.io/current/ksql/docs/index.html</a><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running KSQL</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, KSQL is shipped with the Confluent Platform. When we start the Confluent Platform, automatically at the end it starts a KSQL server, as shown in the <em>Figure 7 .1</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/90352a54-1b1d-4e12-994e-9b7fae1ccfdd.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7.1: Confluent Platform startup</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To start the KSQL server alone (not recommendable), we can use the <kbd>ksql-server-start</kbd> command. Just type <kbd>./ksql</kbd> from the bin directory, as shown in <em>Figure 7.2</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3dede300-c03e-4ea1-bb21-427859df15e8.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.2: KSQL CLI start screen</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the KSQL CLI</h1>
                </header>
            
            <article>
                
<p>The KSQL CLI is a command prompt to interact with KSQL; it is very similar to the one that comes with relational databases such as MariaDB or MySQL. To see all the possible commands, type help and a list with the options will be displayed.</p>
<p>At the moment, we have not informed KSQL of anything. We must declare that something is a table or a stream. We will use the information produced from previous chapters with the producers that write JSON information to the <kbd>healthchecks</kbd> topic.</p>
<p>If you remember, the data looks like this:</p>
<pre><strong>{"event":"HEALTH_CHECK","factory":"Lake Anyaport","serialNumber":"EW05-HV36","type":"WIND","status":"STARTING","lastStartedAt":"2018-09-17T11:05:26.094+0000","temperature":62.0,"ipAddress":"15.185.195.90"}</strong><br/><strong>{"event":"HEALTH_CHECK","factory":"Candelariohaven","serialNumber":"BO58-SB28","type":"SOLAR","status":"STARTING","lastStartedAt":"2018-08-16T04:00:00.179+0000","temperature":75.0,"ipAddress":"151.157.164.162"}</strong><br/><strong>{"event":"HEALTH_CHECK","factory":"Ramonaview","serialNumber":"DV03-ZT93","type":"SOLAR","status":"RUNNING","lastStartedAt":"2018-07-12T10:16:39.091+0000","temperature":70.0,"ipAddress":"173.141.90.85"}</strong><br/><strong>...</strong></pre>
<p>KSQL can read JSON data, and can also read data in Avro format. To declare a stream from the <kbd>healthchecks</kbd> topic, we use the following command:</p>
<pre><strong>ksql&gt;  CREATE STREAM healthchecks (event string, factory string, serialNumber string, type string, status string, lastStartedAt string, temperature double, ipAddress string) WITH (kafka_topic='healthchecks', value_format='json');</strong></pre>
<p>The output is similar to this:</p>
<pre><strong>Message</strong><br/><strong>----------------------------</strong><br/><strong>Stream created and running</strong><br/><strong>----------------------------</strong></pre>
<p>To review the structure of an existing <kbd>STREAM</kbd>, we can use the <kbd>DESCRIBE</kbd> command, which is shown here and that tells us the data types and their structure:</p>
<pre><strong>ksql&gt; DESCRIBE healthchecks;</strong></pre>
<p><span>The output is similar to the following:</span></p>
<pre><strong>Name          : HEALTHCHECKS</strong><br/><strong>Field         | Type</strong><br/><strong>-------------------------------------------</strong><br/><strong>ROWTIME       | BIGINT           (system)</strong><br/><strong>ROWKEY        | VARCHAR(STRING)  (system)</strong><br/><strong>EVENT         | VARCHAR(STRING)</strong><br/><strong>FACTORY       | VARCHAR(STRING)</strong><br/><strong>SERIALNUMBER  | VARCHAR(STRING)</strong><br/><strong>TYPE          | VARCHAR(STRING)</strong><br/><strong>STATUS        | VARCHAR(STRING)</strong><br/><strong>LASTSTARTEDAT | VARCHAR(STRING)</strong><br/><strong>TEMPERATURE   | DOUBLE</strong><br/><strong>IPADDRESS     | VARCHAR(STRING)</strong></pre>
<p>Note that at the beginning, two extra fields are shown: <kbd>ROWTIME</kbd> (the message timestamp) and <kbd>ROWKEY</kbd> (the message key).</p>
<p class="mce-root"/>
<p>When we created the stream, we declared that the Kafka topic is <kbd>healthchecks</kbd>. So, if we execute the <kbd>SELECT</kbd> command, we obtain a list of the events that are in the topic to which our stream points in real time (remember to run a producer to obtain fresh data). The command is as follows:</p>
<pre><strong>ksql&gt; select * from healthchecks;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>1532598615943 | null | HEALTH_CHECK | Carliefort | FM41-RE80 | WIND | STARTING | 2017-08-13T09:37:21.681+0000 | 46.0 | 228.247.233.14</strong><br/><strong>1532598616454 | null | HEALTH_CHECK | East Waldo | HN72-EB29 | WIND | RUNNING | 2017-10-31T14:20:13.929+0000 | 3.0 | 223.5.127.146</strong><br/><strong>1532598616961 | null | HEALTH_CHECK | New Cooper | MM04-TZ21 | SOLAR | SHUTTING_DOWN | 2017-08-21T21:10:31.190+0000 | 23.0 | 233.143.140.46</strong><br/><strong>1532598617463 | null | HEALTH_CHECK | Mannmouth | XM02-PQ43 | GEOTHERMAL | RUNNING | 2017-09-08T10:44:56.005+0000 | 73.0 | 221.96.17.237</strong><br/><strong>1532598617968 | null | HEALTH_CHECK | Elvisfort | WP70-RY81 | NUCLEAR | RUNNING | 2017-09-07T02:40:18.917+0000 | 49.0 | 182.94.17.58</strong><br/><strong>1532598618475 | null | HEALTH_CHECK | Larkinstad | XD75-FY56 | GEOTHERMAL | STARTING | 2017-09-06T08:48:14.139+0000 | 35.0 | 105.236.9.137</strong><br/><strong>1532598618979 | null | HEALTH_CHECK | Nakiaton | BA85-FY32 | SOLAR | RUNNING | 2017-08-15T04:10:02.590+0000 | 32.0 | 185.210.26.215</strong><br/><strong>1532598619483 | null | HEALTH_CHECK | North Brady | NO31-LM78 | HYDROELECTRIC | RUNNING | 2017-10-05T12:12:52.940+0000 | 5.0 | 17.48.190.21</strong><br/><strong>1532598619989 | null | HEALTH_CHECK | North Josianemouth | GT17-TZ11 | SOLAR | SHUTTING_DOWN | 2017-08-29T16:57:23.000+0000 | 6.0 | 99.202.136.163</strong></pre>
<p>The <kbd>SELECT</kbd> command shows the data from the Kafka topic declared in the stream. The query never stops, so it will run till you stop it. New records are printed as new lines, as new events are produced in the topic. To stop a query, type <em>Ctrl<span> </span></em><span>+</span><em> C</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Processing data with KSQL</h1>
                </header>
            
            <article>
                
<p>In previous chapters, we took the data from the <kbd>healthchecks</kbd> topic, calculated the <kbd>uptimes</kbd> of the machines, and pushed this data into a topic called <kbd>uptimes</kbd>. Now, we are going to do this with KSQL.</p>
<p>At the time of writing, KSQL does not yet have a function to compare two dates, so we have the following two options:</p>
<ul>
<li>Code a <strong>u</strong><span><strong>ser-defined function</strong> </span>(<strong><span>UDF</span></strong>) for KSQL <span>in Java</span></li>
<li>Use the existing functions to make our calculation</li>
</ul>
<p>As creating a new UDF is out of scope for now, let's go for the second option: use the existing functions to make our calculation.</p>
<p>The first step is to parse the startup time using the <kbd>STRINGTOTIMESTAMP</kbd> function, shown as follows (remember that we declared the date in string format, because KSQL doesn't yet have a <kbd>DATE</kbd> type):</p>
<pre><strong>ksql&gt; SELECT event, factory, serialNumber, type, status, lastStartedAt, temperature, ipAddress, STRINGTOTIMESTAMP(lastStartedAt,'yyyy-MM-dd''T''HH:mm:ss.SSSZ') FROM healthchecks;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>HEALTH_CHECK | Ezekielfurt | AW90-DQ16 | HYDROELECTRIC | RUNNING | 2017-09-28T21:00:45.683+0000 | 7.0 | 89.87.184.250 | 1532168445683</strong><br/><strong>HEALTH_CHECK | Icieville | WB52-WC16 | WIND | SHUTTING_DOWN | 2017-10-31T22:38:26.783+0000 | 15.0 | 40.23.168.167 | 1532025506783</strong><br/><strong>HEALTH_CHECK | McClurehaven | QP68-WX17 | GEOTHERMAL | RUNNING | 2017-11-12T23:16:27.105+0000 | 76.0 | 252.213.150.75 | 1532064587105</strong><br/><strong>HEALTH_CHECK | East Maudshire | DO15-BB56 | NUCLEAR | STARTING | 2017-10-14T03:04:00.399+0000 | 51.0 | 93.202.28.134 | 1532486240399</strong><br/><strong>HEALTH_CHECK | South Johnhaven | EE06-EX06 | HYDROELECTRIC | RUNNING | 2017-09-06T20:14:27.438+0000 | 91.0 | 244.254.181.218 | 1532264867438</strong></pre>
<p>The next step is to compare these dates to the current date. In KSQL, at the moment, there is no function to get today's date either, so let's use the <kbd>STRINGTOTIMESTAMP</kbd> function to parse today's date, shown as follows:</p>
<pre><strong>ksql&gt; SELECT serialNumber, STRINGTOTIMESTAMP(lastStartedAt,'yyyy-MM-dd''T''HH:mm:ss.SSSZ'), STRINGTOTIMESTAMP('2017-11-18','yyyy-MM-dd') FROM healthchecks;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>FE79-DN10 | 1532050647607 | 1510984800000</strong><br/><strong>XE79-WP47 | 1532971000830 | 1510984800000</strong><br/><strong>MP03-XC09 | 1532260107928 | 1510984800000</strong><br/><strong>SO48-QF28 | 1532223768121 | 1510984800000</strong><br/><strong>OC25-AB61 | 1532541923073 | 1510984800000</strong><br/><strong>AL60-XM70 | 1532932441768 | 1510984800000</strong></pre>
<p>Now, let's compare these two dates and calculate the number of days between them, shown as follows (1 day = 86,400 seconds = 24 hours x 60 minutes x 60 seconds, 1 second = 1,000 milliseconds):</p>
<p class="mce-root"/>
<p class="mce-root"/>
<pre><strong>ksql&gt; SELECT serialNumber, (STRINGTOTIMESTAMP('2017-11-18','yyyy-MM-dd''T''HH:mm:ss.SSSZ')-STRINGTOTIMESTAMP(lastStartedAt,'yyyy-MM-dd'))/86400/1000 FROM healthchecks;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>EH92-AQ09 | 39</strong><br/><strong>BB09-XG98 | 42</strong><br/><strong>LE94-BT50 | 21</strong><br/><strong>GO25-IE91 | 97</strong><br/><strong>WD93-HP20 | 22</strong><br/><strong>JX48-KN03 | 12</strong><br/><strong>EC84-DD11 | 73</strong><br/><strong>SF06-UB22 | 47</strong><br/><strong>IU77-VQ89 | 18</strong><br/><strong>NM80-ZY31 | 5</strong><br/><strong>TR64-TI21 | 51</strong><br/><strong>ZQ13-GI11 | 80</strong><br/><strong>II04-MB66 | 48</strong></pre>
<p>Perfect, now we have calculated the uptime for every machine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing to a topic</h1>
                </header>
            
            <article>
                
<p>So far, we have processed the data and printed the results in real time. To send these results to another topic, we use a <kbd>CREATE</kbd> command modality, where it is specified from a <kbd>SELECT</kbd>.</p>
<p>Let's start by writing the uptime as a string and writing the data in a comma-delimited format, shown as follows (remember that KSQL supports comma-delimited, JSON, and Avro formats). At the moment, it's enough because we're only writing one value:</p>
<pre><strong>ksql&gt; CREATE STREAM uptimes WITH (kafka_topic='uptimes', value_format='delimited') AS SELECT CAST((STRINGTOTIMESTAMP('2017-11-18','yyyy-MM-dd''T''HH:mm:ss.SSSZ')-STRINGTOTIMESTAMP(lastStartedAt,'yyyy-MM-dd'))/86400/1000 AS string) AS uptime FROM healthchecks;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>Message</strong><br/><strong>----------------------------</strong><br/><strong>Stream created and running</strong><br/><strong>----------------------------</strong></pre>
<p>Our query is running in the background. To see it is running, we could use a console consumer of the <kbd>uptimes</kbd> topic, shown as follows:</p>
<pre><strong>$ ./kafka-console-consumer --bootstrap-server localhost:9092 --topic uptimes --property print.key=true</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>null  39</strong><br/><strong>null  42</strong><br/><strong>null  21</strong></pre>
<p>The results are correct; however, we forgot to use the machine serial number as the message key. To do this, we have to rebuild our query and our stream.</p>
<p>The first step is to use the <kbd>show queries</kbd> command, shown here:</p>
<pre><strong>ksql&gt; show queries;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong> Query ID       | Kafka Topic | Query String</strong><br/><strong>-------------------------------------------------------------------------------</strong><br/><strong>CSAS_UPTIMES_0 | UPTIMES     | CREATE STREAM uptimes WITH (kafka_topic='uptimes', value_format='delimited') AS SELECT CAST((STRINGTOTIMESTAMP('2017-11-18','yyyy-MM-dd''T''HH:mm:ss.SSSZ')-STRINGTOTIMESTAMP(lastStartedAt,'yyyy-MM-dd'))/86400/1000 AS string) AS uptime FROM healthchecks;</strong><br/><strong>-------------------------------------------------------------------------------</strong><br/><strong>For detailed information on a Query run: EXPLAIN &lt;Query ID&gt;;</strong></pre>
<p>With the <kbd>Query ID</kbd>, use the <kbd>terminate &lt;ID&gt;</kbd> <span>command, </span>shown as follows:</p>
<pre><strong>ksql&gt; terminate CSAS_UPTIMES_0;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>Message</strong><br/><strong>-------------------</strong><br/><strong>Query terminated.</strong><br/><strong>-------------------</strong></pre>
<p>To delete the stream, use the <kbd>DROP STREAM</kbd> command, shown as follows:</p>
<pre><strong>ksql&gt; DROP STREAM uptimes;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>Message</strong><br/><strong>------------------------------</strong><br/><strong>Source UPTIMES was dropped.</strong><br/><strong>------------------------------</strong></pre>
<p>To write the events key correctly, we must use the <kbd>PARTITION BY</kbd> <span>clause. </span>First, we regenerate our stream with a partial calculation, shown as follows:</p>
<pre><strong>ksql&gt; CREATE STREAM healthchecks_processed AS SELECT serialNumber, CAST((STRINGTOTIMESTAMP('2017-11-18','yyyy-MM-dd''T''HH:mm:ss.SSSZ')-STRINGTOTIMESTAMP(lastStartedAt,'yyyy-MM-dd'))/86400/1000 AS string) AS uptime FROM healthchecks;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>Message</strong><br/><strong>----------------------------</strong><br/><strong>Stream created and running</strong><br/><strong>----------------------------</strong></pre>
<p>This stream has two fields (<kbd>serialNumber</kbd> and <kbd>uptime</kbd>). To write these calculated values to a topic, we use <kbd>CREATE STREAM</kbd>, <kbd>AS SELECT</kbd> as follows:</p>
<pre><strong>ksql&gt; CREATE STREAM uptimes WITH (kafka_topic='uptimes', value_format='delimited') AS SELECT * FROM healthchecks_processed;</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>Message</strong><br/><strong>----------------------------</strong><br/><strong>Stream created and running</strong><br/><strong>----------------------------</strong></pre>
<p>Finally, run a console consumer to show the results, demonstrated as follows:</p>
<pre><strong>$ ./bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic uptimes --property print.key=true</strong></pre>
<p><span>The output is similar to this:</span></p>
<pre><strong>EW05-HV36   33</strong><br/><strong>BO58-SB28   20</strong><br/><strong>DV03-ZT93   46</strong><br/><strong>...</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, close the KSQL CLI (<em>Ctrl</em> +<em> C</em> and close the command window). As the queries are still running in KSQL, you still see the outputs in the console consumer window.</p>
<p>Congratulations, you have built a Kafka Streams application with a few KSQL commands.</p>
<p>To unveil all the power of KSQL, it is important to review the official documentation at the following address: </p>
<p><a href="https://docs.confluent.io/current/ksql/docs/tutorials/index.html">https://docs.confluent.io/current/ksql/docs/tutorials/index.html</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>KSQL is still very new, but the product has gained adoption among developers. We all hope it continues to be extended to support more data formats (Protobuffers, Thrift, and so on) and more functions (more UDFs, such as geolocation and IoT, that are quite useful).</p>
<p>So again, congratulations! In this chapter, we did the same as in the previous ones, but without writing a single line of Java code. This makes KSQL the preferred tool for people who are not programmers, but are dedicated to data analysis.</p>


            </article>

            
        </section>
    </body></html>