- en: '*Chapter 2*: JIT, HotSpot, and GraalJIT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about C1 and C2 compilers and the kind of
    code optimizations and de-optimizations that C2 compilers perform at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will deep dive into the C2 just-in-time compilation and
    introduce Graal's just-in-time compilation. **Just-In-Time** (**JIT**) compilation
    is one of the key innovations that enabled Java to compete with traditional **ahead-of-time**
    (**AOT**) compilers. As we learned in the previous chapter, JIT compilation evolved
    with C2 compilers in JVM. The C2 JIT compiler constantly profiles code execution
    and applies various optimizations and de-optimizations at runtime to compile/recompile
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will be a hands-on session, where we will take a sample code and
    analyze how the C2 JIT compiler works and introduce Graal JIT.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how the JIT compiler works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how code is optimized by JIT by identifying HotSpots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use profiling tools to demonstrate how the JIT compiler works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how GraalVM JIT works on top of JVM JIT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a clear understanding of the internal
    workings of the JIT compiler and how GraalVM extends it further. We will be using
    sample Java code and profiling tools such as JITWatch to gain a deeper understanding
    of how JIT works.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow the instructions given in this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: All the source code referred to in this chapter can be downloaded from [https://github.com/PacktPublishing/Supercharge-Your-Applications-with-GraalVM/tree/main/Chapter02](https://github.com/PacktPublishing/Supercharge-Your-Applications-with-GraalVM/tree/main/Chapter02).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git ([https://github.com/git-guides/install-git](https://github.com/git-guides/install-git))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven ([https://maven.apache.org/install.html](https://maven.apache.org/install.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenSDK ([https://openjdk.java.net/](https://openjdk.java.net/)) and JavaFX
    ([https://openjfx.io/](https://openjfx.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JITWatch ([https://www.jrebel.com/blog/understanding-java-jit-with-jitwatch#:~:text=JITWatch%20is%20a%20log%20analyser,to%20the%20Adopt%20OpenJDK%20initiative](https://www.jrebel.com/blog/understanding-java-jit-with-jitwatch#:~:text=JITWatch%20is%20a%20log%20analyser,to%20the%20Adopt%20OpenJDK%20initiative))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Code in Action video for this chapter can be found at [https://bit.ly/3w7uWlu](https://bit.ly/3w7uWlu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will set up all the prerequisite tools and environments
    that are required to follow on with the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenJDK Java
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can install OpenJDK from [https://openjdk.java.net/install/](https://openjdk.java.net/install/).
    This URL has detailed instructions on how to install OpenJDK. We also require
    JavaFX. Please refer to [https://openjfx.io/](https://openjfx.io/) for more details
    on how to install JavaFX.
  prefs: []
  type: TYPE_NORMAL
- en: Installing JITWatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JITWatch is one of the most widely used log analysis and visualization tools
    for understanding the behavior of the JIT compiler. This is also widely used in
    analyzing the code and identifying opportunities for better performance tuning.
  prefs: []
  type: TYPE_NORMAL
- en: JITWatch is an active open source project hosted at [https://github.com/AdoptOpenJDK/jitwatch](https://github.com/AdoptOpenJDK/jitwatch).
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical commands for installing JITWatch are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Taking a deep dive into HotSpot and the C2 JIT compiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we walked through the evolution of JVM and how the
    C2 JIT compiler evolved. In this section, we will dig deeper into the JVM C2 JIT
    compiler. Using sample code, we will go through the optimizations that the JIT
    compiler performs at runtime. To appreciate the Graal JIT compiler, it is very
    important to understand how the C2 JIT compiler works.
  prefs: []
  type: TYPE_NORMAL
- en: Profile-guided optimization is the key principle for JIT compilers. While AOT
    compilers can optimize the static code, most of the time, that is just not good
    enough. It's important to understand the runtime characteristics of the application
    to identify opportunities for optimization. JVM has a built-in profiler that dynamically
    instruments the application to profile some key parameters and to identify opportunities
    for optimizations. Once identified, it will compile that code to the native language
    and switch from running the interpreted code to faster-compiled code. The optimizations
    are based on profiling and educated assumptions that are made by JVM. If any of
    these assumptions are incorrect, JVM will de-optimize and switch back to running
    interpreted code. This is called **Mixed Mode Execution**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a flow of how JVM performs the profile-guided optimizations
    and switches between different modes of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – JIT compilation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – JIT compilation
  prefs: []
  type: TYPE_NORMAL
- en: The Java source code (`.java`) is compiled to bytecode (`.class`), which is
    an intermediate representation of the code. JVM starts to run the bytecode with
    the inbuilt interpreter. The interpreter uses a bytecode to machine code mapping,
    converts the bytecode instructions to machine code one statement at a time, and
    then executes it.
  prefs: []
  type: TYPE_NORMAL
- en: 'While JVM executes these instructions, it also tracks the number of times a
    method is invoked. When the method invocation count of a particular method exceeds
    the compiler threshold, it spins off a compiler to compile that method on a separate
    compilation thread. There are two types of compilers that JVM uses to compile
    the code: C1 (client) and C2 (server) JIT compilers. The compiled code is stored
    in the code cache, so that the next time that method is invoked, JVM will execute
    the code from the code cache instead of interpreting it. JIT compilers perform
    various optimizations to the code, and hence, over time, the application performs
    better. The rest of this section will walk through these various components in
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Code cache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code cache is an area in JVM where JVM stores the compiled native methods
    (also referred to as `nmethod`). The code cache is set to a static size and it
    might become full after a while. Once the code cache is full, JVM cannot compile
    or store any more code. It is very important to tune the code cache for optimum
    performance. Four key parameters help us to fine-tune JVM performance, with the
    optimum code cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-XX:InitialCodeCacheSize`: The initial size of the code cache. The default
    size is 160 KB (varies based on the JVM version).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:ReservedCodeCacheSize`: The maximum size the code cache can grow to. The
    default size is 32/48 MB. When the code cache reaches this limit, JVM will throw
    a warning: `CodeCache is full. Compiler has been disabled.`. JVM offers the `UseCodeCacheFlushing`
    option to flush the code cache when the code cache is full. The code cache is
    also flushed when the compiled code is not hot enough (when the counter is less
    than the compiler threshold).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:CodeCacheExpansionSize`: This is the expansion size when it scales up.
    Its default value is 32/64 KB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:+PrintCodeCache`: This option can be used to monitor the usage of the
    code cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since Java 9, JVM segments the code cache into three segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-XX:NonNMethodCodeHeapSize` flag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:ProfiledCodeHeapSize` flag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:NonProfiledCodeHeapSize` flag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiler threshold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The compilation threshold is a factor that helps JVM decide when to perform
    JIT compilations. When JVM detects that a method execution has reached a compilation
    threshold, JVM will instigate the appropriate compiler to perform the compilation
    (more on this later in this section, where we will walk through the various types
    of JIT compilers and tiered compilation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Deciding the compilation threshold is based on two key variables. These variables
    come with a default value for each JVM, but can also be changed with appropriate
    command-line arguments. These variables are very critical in tuning the performance
    of JVM and should be used carefully. These two variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Method invocation counter**: This counts the number of times a particular
    method is invoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop counter**: This refers to the number of times a particular loop has
    completed execution (what is referred to as branching back). Sometimes, this is
    also referred to as Backedge Threshold or Backedge Counter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JVM profiles these two variables at runtime and, on this basis, decides whether
    that method/loop needs to be compiled. When a compilation threshold is reached,
    JVM spins off a compilation thread to compile that particular method/loop.
  prefs: []
  type: TYPE_NORMAL
- en: The compilation threshold can be changed using the `-XX:CompilationThreshold=N`
    flag as an argument while executing the code. The default value of `N` is `1500`
    for the client compiler and `10000` for the server compiler.
  prefs: []
  type: TYPE_NORMAL
- en: On-stack replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The methods that reach the compilation threshold are compiled by the JIT compilers,
    and the next time the method is called, the compiled machine code is called. This
    improves performance over time. However, in cases of long-running loops that reach
    the loop counter threshold (Backedge Threshold), the compilation thread initiates
    code compilation. Once the code that is in the loop is compiled, the execution
    is stopped and resumed with the compiled code frame. This process is called **On-Stack
    Replacement (OSR)**. Let's look at the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet just discusses how OSR works. To keep it simple,
    the code simply shows a long-running loop where we are just calculating the total
    number of times the loop runs. In this case, the `main()` method is never entered,
    so even after the compilation threshold is reached and the code is compiled, the
    compiled code cannot be used as the interpreter continues to execute, unless the
    code is replaced. This is where OSR helps in optimizing such code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following flowchart shows how the OSR works in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – OSR flowchart'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – OSR flowchart
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: The interpreter starts executing the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the compiler threshold is reached, JVM spins off a compiler thread to compile
    the method. In the meantime, the interpreter continues to execute the statement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the compilation thread comes back with the compiled code (compilation frame),
    JVM checks whether the interpreter is still executing the code. If the interpreter
    is still executing the code, it will pause and perform OSR, and execution starts
    with the compiled code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we run this code with the `-XX:PrintCompilation` flag on, this is the
    output that shows that JVM performed OSR (the % attribute indicates that it performed
    OSR):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – OSR log screenshot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – OSR log screenshot
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the next section to understand the log format in detail.
  prefs: []
  type: TYPE_NORMAL
- en: XX:+PrintCompilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`XX:+PrintCompilation` is a very powerful argument that can be passed to understand
    how the JIT compilers are kicking in and optimizing the code. Before we run our
    code with this argument, let''s first understand the output format.'
  prefs: []
  type: TYPE_NORMAL
- en: '`XX:+PrintCompilation` produces a log list of parameters separated by blank
    spaces in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example snapshot of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Print compilation log format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Print compilation log format
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at what these parameters mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Timestamp`: This is the time in milliseconds since JVM started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CompilationID`: This is an internal identification number used by JVM in the
    compilation queue. This will not necessarily be in a sequence, as there are background
    compilation threads that might reserve some of the IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Flags`: The compiler flags are very important parameters that are logged.
    This suggests what attributes are applied by the compiler. JVM prints a comma-separated
    string of five possible characters to indicate five different attributes that
    are applied to the compiler. If none of the attributes are applied, it is shown
    as a blank string. The five attributes are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a. `%` character. OSR is explained earlier in this section. This attribute suggests
    that an OSR compilation is triggered as the method is looping over a large loop.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. `!` character. This indicates that the method has an exception handler.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. `s` character. This indicates that the method is synchronized.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. `b` character. This indicates that the compilation occurred in blocking mode.
    This means that the compilation did not happen in the background.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e. `n` character. This indicates that the code is compiled to the native method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Tier`: This indicates which tier of compilation is performed. Refer to the
    *Tiered compilation* section for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MethodName`: This column lists the method that is being compiled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MethodSize`: This is the size of the method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Deoptimization performed`: This shows any de-optimizations that may be performed.
    We will discuss this in detail in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tiered compilation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we briefly covered compilation tiers/levels. In this
    section, we will go into more details. Client compilers kick in early when the
    compiler threshold is reached. The server compiler kicks in based on the profiling.
    The recent versions of JVM use a combination of both compilers for optimum performance.
    However, the user can specifically use one of the compilers with the `-client`,
    `-server`, or `-d64` arguments. The default behavior of JVM is to use tiered compilation,
    which is the most optimum JIT compilation. With tiered compilation, the code is
    first compiled by the client compiler and, based on the profiling, if the code
    gets hotter (hence the name HotSpot), the server compiler kicks in and recompiles
    the code. This process was explained in the previous section by means of the flowchart.
  prefs: []
  type: TYPE_NORMAL
- en: Tiered compilation brings in more optimization as the code gets complicated
    and runs for longer. There are instances where JIT compilation runs more optimally
    and faster than AOT compilation. While AOT compilation brings in optimization,
    during the build phase, it does not have the intelligence to optimize/deoptimize
    itself based on runtime profiling. Runtime profiling, optimizing, and deoptimizing
    are the key advantages of JIT compilation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three versions of JIT compilers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-client` argument:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`long` or `double` variables. This version of the compiler can be explicitly
    invoked using the `-server` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-d64` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server compilers are up to 4x slower in compiling than client compilers. However,
    they do generate a faster running application (up to 2x).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five tiers/levels of compilation levels as listed next. A compilation
    log can be used to find which method was compiled to what level, by means of compilation
    print:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 0 – Interpreted code**: This the standard interpreter mode, where the
    JIT is still not activated. The JIT gets activated based on the compilation threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 1 – Simple C1 compiled code**: This is a basic no-profile compilation
    of the code. The compiled code will not have any instrumentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 2 – Limited C1 compiled code**: In this level, basic counters are instrumented.
    The counter will help JVM decide to move to the next level, L2\. Sometimes, when
    the C2 compiler is busy, JVM will use this level as an intermediate before promotion
    to Level 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 3 – Full C1 compiled code**: In this level, the code is fully instrumented
    and profiled. This detailed profiling will help decide further optimization with
    L4\. This level adds up to 25-30% of overhead to the compiler and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 4 – C2 compiled code**: This is the most optimized compilation of the
    code, where all the optimization is applied. However, while profiling, if JVM
    finds that the context of optimization has changed, it will deoptimize and replace
    the code with L0 or L1 (for trivial methods).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now look at how the Java HotSpot compiler performs tiered compilation.
    The following diagram shows the various tiers and flow patterns of compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Tiered compilation patterns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – Tiered compilation patterns
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand what each flow indicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: This is the normal pattern of how JVM works. All the code starts with
    L0 and escalates to L3 when the compilation threshold is reached. At L3, the code
    is compiled with complete detailed profiling instrumentation. The code is then
    profiled at runtime, when it reaches the threshold, and then the code is re-compiled
    with the C2 compiler (L4), with maximum optimization. C2 compilers require detailed
    data regarding the control flow so as to take decisions concerning optimization.
    Later in this section, we will walk through all the optimizations that the C2
    compiler (JIT) performs. It is possible, however, that the optimizations are invalid,
    due to changes in the flows or the context of optimization. In this case, JVM
    will deoptimize and bring it back to L0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B: C2 Busy**: C2 compilation is performed on a separate compilation thread
    and the compilation activities are queued. When the compilation threads are all
    busy, JVM will not follow the normal flow, as this may affect the overall performance
    of the application. Instead, JVM will escalate to L2, where at least the counters
    are instrumented, and at a later point, when the code reaches the higher threshold,
    it will escalate to L3 and L4\. At any point, JVM can deoptimize or invalidate
    the compiled code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C: Trivial Code**: Sometimes, JVM will compile the code to L3 and realize
    that the code does not require any optimization, as it is very straightforward/simple,
    based on the profiling. In this case, it will bring it down to L1\. That way,
    the execution of the code will be faster. The more we instrument the code, the
    more overhead we are putting on the execution. It is typically observed that L3
    adds anywhere between 20-30% overhead to execution, due to instrumentation code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can look at how JVM behaves using the `-XX:+PrintCompilation` option. Here
    is an example of a normal flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For this code, when we execute `java` with `-XX:+PrintCompilation`, the following
    log is generated on the console. The log can be redirected to a log file using
    the `+LogCompilation` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Log showing tiered compilation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – Log showing tiered compilation
  prefs: []
  type: TYPE_NORMAL
- en: 'In this screenshot, you can see how the `main()` method moves from L0->L3->L4,
    which is the normal flow (A). As JVM performs optimizations and de-optimizations,
    jumping between these various levels of compilation, it reaches the most optimum,
    stable point. This is one of the greatest advantages that JIT compilers have over
    AOT compilers. The JIT compiler uses the runtime behavior to optimize the code
    execution (not just the semantic/static code optimizations). If you run this with
    JITWatch, we can see a clearer representation. The following screenshot shows
    the JITWatch tool compile chain when we run it by the `Sample.java` snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – JITWatch tiered compilation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – JITWatch tiered compilation
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous screenshot shows that `Sample::main()` is compiled with the C1-L3
    compiler. `Sample::Sample()` (default constructor) is inlined and `Sample::performOperation()`
    is also inlined into `Sample::main()`. `Sample::performAnotherOperation()` is
    also compiled. This is the first level of optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows how various compilers are run on each of the
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – JITWatch tiered compilation of main()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – JITWatch tiered compilation of main()
  prefs: []
  type: TYPE_NORMAL
- en: 'This screenshot shows how the `main()` method is optimized. Since the `main()`
    method has a long loop, OSR has occurred two times: once when the C1 compiled
    code is replaced, and the second time when the C2 compiled code is replaced. In
    each case, it has performed inlining. You can see what optimizations the C1 and
    C2 compilers performed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – JITWatch tiered compilation of main() – OSR-L3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 – JITWatch tiered compilation of main() – OSR-L3
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous screenshot, we can see that `Sample::performAnotherOperation()`
    is compiled and `Sample::performOperation()` is inlined into `Sample::main()`.
    The following screenshot shows further optimization that is performed by inlining
    `Sample:performAnotherOperation()` into `Sample::performOperation()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – JITWatch tiered compilation of main() – OSR-L4'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – JITWatch tiered compilation of main() – OSR-L4
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at how the JIT compiler optimizes the S`ample::performAnotherOperation()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – JITWatch tiered compilation of performAnotherOperation()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – JITWatch tiered compilation of performAnotherOperation()
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the previous screenshot, `Sample::performAnotherOperation()`
    has gone through various optimizations and OSRs, as it runs a long loop. The code
    is inlined into `Sample::performOperation()` as it hits the compiler threshold.
    The following screenshots reveal how `Sample::performAnotherOperation()` is compiled
    and inlined into `Sample::performOperation()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at how the JIT compiler compiles the `Sample::performOperation()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – JITWatch tiered compilation of performOperation()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – JITWatch tiered compilation of performOperation()
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the C1 compilation chain view for the `performOperation()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – JITWatch tiered compilation of performOperation() – C1 compilation
    chain view'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – JITWatch tiered compilation of performOperation() – C1 compilation
    chain view
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous screenshot shows that `Sample::performAnotherOperation()` is compiled
    as it hits the compiler threshold, and the following screenshot shows that the
    compiled code is inlined into `Sample::performOperation()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – JITWatch tiered compilation of performOperation() – C2 compilation
    chain view'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 – JITWatch tiered compilation of performOperation() – C2 compilation
    chain view
  prefs: []
  type: TYPE_NORMAL
- en: JITWatch can be used to gain detailed understanding of how the C1 an C2 compilers
    behave, and how the optimizations are performed. This helps in reflecting on the
    application code, and proactively updating the source code for better runtime
    performance. To get a better understanding of how the C2 compiler optimizes the
    code, let's now look at the various types of code optimizations that JVM applies
    during compilation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the optimizations performed by JIT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will cover the various optimization techniques that the JIT compilers
    employ at the various levels of compilation.
  prefs: []
  type: TYPE_NORMAL
- en: Inlining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calling a method is an expensive operation for JVM. When the program calls a
    method, JVM has to create a new stack frame for that method, copy all the values
    into the stack frame, and execute the code. Once the method completes, the stack
    frame has to be managed post-execution. One of the best practices in object-oriented
    programming is to access the object members through access methods (getters and
    setters).
  prefs: []
  type: TYPE_NORMAL
- en: Inlining is one of the most effective optimizations performed by JVM. JVM replaces
    the method call with the actual content of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run our previous code with the following command, we can see how JVM
    performs inlining:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the call to the `performOperation()` method is replaced with
    the content of the inline `main()` method. After inlining effectively, the `main()`
    method will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Inlining can be disabled using the `-XX:-Inline` flag.
  prefs: []
  type: TYPE_NORMAL
- en: JVM decides to inline the code, based on the number of calls made to the method
    and the size of the method, and if the method is frequently called (hot), and
    the size of the method is <325 bytes. Methods that are <35 bytes are inlined by
    default. These numbers can be changed with the `-XX:+MaxFreqInlineSize` and `-XX:+MaxInlineSize`
    flags from the command line, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Monomorphic, bimorphic, and megamorphic dispatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polymorphism is one of the key object-oriented concepts that provides a way
    to dynamically load classes based on a context, and the behavior is decided dynamically.
    Interfaces and inheritance are two of the most widely used polymorphism implementations.
    However, this comes with a performance overhead, as JVM loads the class/interface
    implementations dynamically. Inlining the implementations becomes a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: One of the things that JVM profiles is the number of times a particular implementation
    is called and how many derived class/interface implementations really exist for
    a given base class or interface. If the profiler identifies only one implementation,
    then it's called monomorphic. If it finds two, then it's called bimorphic, and
    megamorphic means there are multiple implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the profiling, the JIT compiler identifies which specific derived
    class object (or interface implementation) is used and takes the decision on inlining
    that specific implementation, to overcome the performance overheads of polymorphism.
    Monomorphic and bimorphic are easy to inline. The JIT profiler tracks the execution
    paths and identifies in which context which implementation is used and performs
    inlining. Megamorphic implementations are complex to inline. The following code
    snippet shows polymorphism. We will use this code to understand the performance
    overhead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we have defined an interface called `Shape`, and we have
    three implementations of the interface, namely, `Circle`, `Square`, and `Triangle`.
    And, we are using a switch to initialize the right class. There are two optimization
    scenarios here:'
  prefs: []
  type: TYPE_NORMAL
- en: If the JIT knows that a particular implementation is used, it optimizes the
    code and might perform an inline. This is called a monomorphic dispatch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If, let's say, the decision is to be taken based on a particular variable or
    a configuration, JIT will profile, which is the most optimistic assumption it
    can take, and only those classes and inline them, and may also use an uncommon
    trap. In case the implementation class that is used is different from what is
    assumed, the JIT will deoptimize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dead code elimination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The JIT compiler identifies the code that is never executed or not required
    while profiling. This is called dead code, and the JIT compiler eliminates this
    from the execution. Modern IDEs identify dead code; this is purely based on the
    static code analysis performed. JIT compilers not only eliminate such trivial
    code, but also eliminate the code based on the control flow at runtime. Dead code
    elimination is one of the most effective ways to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the following code as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the `calculateSomething()` method has some logic. Let's look at
    the previous code snippet. The `finalTotalValue` variable is initialized and later,
    the total is calculated by calling the `calculateValue()` method in a loop, but
    assuming that `finalTotalValue` is never used after calculation. The initialization
    code, the array heap allocation code, and the loop that calls the `calculateValue()`
    method, are all dead code. JIT understands this at runtime and removes it completely.
  prefs: []
  type: TYPE_NORMAL
- en: JIT takes these decisions based on profiling and whether the code is reachable.
    It might remove unnecessary `if` statements (especially null checks; if the object
    is never seen to be null – this technique is sometimes referred to as Null Check
    Elimination). It will replace this with what is called "uncommon trap" code. If
    this execution ever reaches this trap code, it will then deoptimize.
  prefs: []
  type: TYPE_NORMAL
- en: Another case where "uncommon trap" code is placed by eliminating the code is
    by predicting branches. Based on the profiling, JIT assumes and predicts a branch
    code (`if`, `switch`, and so on) that may never be executed, and eliminates that
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Common Subexpression Elimination is another effective technique that JIT uses
    to eliminate code. In this technique, an intermediate subexpression is removed
    to save the number of instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Later, in the *Escape analysis* section, we will also see some code elimination
    techniques, based on the escape analysis that JIT performs.
  prefs: []
  type: TYPE_NORMAL
- en: Loop optimization – Loop unrolling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loop unrolling is another effective optimization technique. This is more effective
    in smaller loop body and large number of iterations. The technique involves looking
    to reduce the iterations in the loop by replacing the code. Here is a very simple
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be rolled into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the JIT compiler decides to reduce the iterations by 1/4, by
    calling `somefunction()` four times instead of once. This has a significant performance
    improvement, as the number of jump statements goes down by 1/4\. Of course, the
    decision on four is taken based on the size of the array so that the array reference
    does not go out of bounds.
  prefs: []
  type: TYPE_NORMAL
- en: Escape analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Escape analysis is one of the most advanced optimizations that the JIT compiler
    performs. This is controlled with the `-XX:+DoEscapeAnalysis` flag from the command
    line. This is enabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we went through the various memory areas in the *Memory
    subsystem* section. Heap and stack are two of the most important memory areas.
    The heap memory area is accessible across various threads in JVM. A heap is not
    thread-safe. When multiple threads access the data that is stored in the heap,
    it is recommended to write thread-safe code by obtaining synchronization locks.
    This blocks the other threads from accessing the same data. This has performance
    implications.
  prefs: []
  type: TYPE_NORMAL
- en: Stack memory is thread-safe, as it is allocated for that particular method call.
    Only the method thread has access to this area, hence there is no need to worry
    about obtaining synchronization locks or the absence of blocking threads.
  prefs: []
  type: TYPE_NORMAL
- en: The JIT compiler performs a detailed analysis of the code to identify code where
    we are allocating variables in the heap, but only using them in a specific method
    thread, and takes decisions on allocating these variables to the "stack area"
    instead of the "heap area." This is one of the most complex optimizations that
    JIT compilers perform and has a huge impact on performance. JIT might decide to
    store the variables in PC registers for even faster access.
  prefs: []
  type: TYPE_NORMAL
- en: JIT also looks for the use of `synchronized` and tracks. If it's called by a
    single thread, JIT decides to ignore `synchronized`. This has a significant impact
    on performance. `StringBuffer` is one of the objects that is thread-safe and has
    a lot of synchronized methods. If an instance of `StringBuffer` is not used outside
    a single method, JIT decides to ignore `synchronized`. This technique is sometimes
    referred to as "lock elision."
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where a synchronized lock cannot be ignored, the JIT compiler looks
    to combine the `synchronized` blocks. This technique is known as lock coarsening.
    This technique looks for subsequent `synchronized` blocks. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, two subsequent synchronized blocks are trying to obtain a lock
    on the same class. The JIT compiler will combine these two blocks into one.
  prefs: []
  type: TYPE_NORMAL
- en: 'JIT performs a similar analysis for variables that are created with a loop
    and never used outside the loop. There is a very sophisticated technique called
    "scalar replacement," where the JIT profiles for objects that are created, but
    only a few member variables are used in the object that is not used. JIT will
    decide to stop creating the objects and replace them with the member variables
    directly. Here is a very simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `StateStoring` class is a simple class, where we are storing the state of
    an object with two members – `state_variable_1` and `state_variable_2`. JIT profiles
    this for various iterations and checks whether this object was created and never
    used outside the scope. It might decide not to even create the object, instead
    replacing the object getters and setters with actual scalars (local variables).
    That way, entire object creation and destruction (which is a very expensive process)
    can be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a more advanced example, and this time let''s see how JITWatch shows
    the escape analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, the `allocateObjects()` method is creating an array (on
    the heap) and adding that value to the array. The `dummyInt` variable's scope
    is limited to the `for` loop in the `allocateObjects()` method. There is no need
    to have these objects created in the heap. After performing the escape analysis,
    JIT identifies that these variables can be put in a stack frame instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following JITWatch screenshot demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – JITWatch escape analysis – 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 – JITWatch escape analysis – 1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this screenshot, the bytecode that allocates `dummyInt` is struck off to
    indicate that heap allocation for that variable is not required:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – JITWatch escape analysis – 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 – JITWatch escape analysis – 2
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot shows the optimization that is performed by C2/Level
    4, where it removes allocation of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Deoptimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at various optimization techniques that the
    JIT compiler performs. The JIT compiler optimizes the code with some assumptions
    that it makes, based on the profiling. Sometimes, these assumptions may be not
    correct in a different context. When JIT stumbles upon these scenarios, it deoptimizes
    the code and goes back to using an interpreter to run the code. This is called
    Deoptimization and has an impact on performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two scenarios where Deoptimization occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: When the code is "non-entrant"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the code is "zombie"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's understand these scenarios with the help of examples.
  prefs: []
  type: TYPE_NORMAL
- en: Non-entrant code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two cases where the code becomes non-entrant:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-XX:+PrintCompilation` flag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sample.java`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Tiered compilation escalation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16878_Figure_2.17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 – Tiered compilation escalation
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see the tiered compilation in action (the
    third column shows the tier number) and the optimization that is done.
  prefs: []
  type: TYPE_NORMAL
- en: Zombie code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, some objects are created in the heap of the code that is marked
    as non-entrant. Once the GC reclaims all these objects, then JVM will mark the
    methods of those classes as zombie code. JVM then removes this compiled zombie
    code from the code cache. As we discussed in the *Taking a deep dive into hotspot
    and the C2 JIT* section, it's very important to keep the code cache optimum, as
    this has a significant impact on performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in tiered compilation, deoptimization is performed when any of the
    assumptions that the Java JIT made is challenged by the control flow at runtime.
    In the next section, we will briefly introduce the Graal JIT compiler, and how
    it plugs into JVM.
  prefs: []
  type: TYPE_NORMAL
- en: Graal JIT and the JVM Compiler Interface (JVMCI)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, as we walked through the various features and advancements
    that JIT compilers underwent, it is very clear that C2 is very sophisticated.
    However, C2 compiler implementation has its downsides. C2 is implemented in the
    C/C++ language. While C/C++ is fast, it is not type-safe and it does not have
    garbage collection. Hence, the code becomes very complex. C2 implementation is
    very complex, as it has become more and more complex to change the code for new
    enhancements and bug fixes.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, Java has matured to run as fast as C/C++ in many cases. Java
    is type-safe with garbage collection. Java is simpler and easier to manage than
    C/C++. The key advantages of Java are its exception handling capabilities, memory
    management, better IDE/profiling, and tooling support. The JIT compiler is nothing
    but a program that takes in a bytecode, `byte[]`, optimizes it, compiles it, and
    returns an array of machine code, `byte[]`. This can easily be implemented in
    Java. What we need is a JVM interface that can provide the protocol for implementing
    the custom compiler logic. This will help open up JVM for the new implementations
    of JIT compilers.
  prefs: []
  type: TYPE_NORMAL
- en: JDK enhancement proposal JEP243 ([https://openjdk.java.net/jeps/243](https://openjdk.java.net/jeps/243))
    is a proposal to provide a compiler interface that will enable writing a compiler
    in Java and extending JVM to use it dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'JEP243 was added in Java 9\. This is one of the most significant enhancements
    to JVM. JVMCI is an implementation of JEP243\. JVMCI provides the required extensibility
    to write our own JIT compilers. JVMCI provides the API that is required to implement
    custom compilers and configure JVM to call these custom compiler implementations.
    The JVMCI API provides the following capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to VM data structures, which is required to optimize the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing the compiled code following optimization and deoptimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Callbacks from JVM to execute the compilation at runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A JVMCI can be executed with the following command-line flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Graal is an implementation of JVMCI, which brings all the key features and
    optimizations that are required for a modern Java runtime. Graal is wholly implemented
    in Java. Graal is much more than just a JIT compiler. Here is a quick comparison
    between the Graal JIT and the JVM JIT (C2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16878_Table_2.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next chapter will go into more detail on Graal architecture, and [*Chapter
    4*](B16878_04_Final_SK_ePub.xhtml#_idTextAnchor077), *Graal Just-In-Time Compiler*,
    will go deeper into how Graal JIT works, and how it builds on top of Java JIT
    and brings in more advanced optimizations and support for Polyglot.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went into a lot of detail on how the JIT compiler works
    and discussed the tiered compilation patterns that JVM uses to optimize the code.
    We also walked through various optimization techniques with a number of sample
    code examples. This provided a good understanding of the internal workings of
    JVM.
  prefs: []
  type: TYPE_NORMAL
- en: JVMCI provides the extensibility to build custom JIT compilers on JVM. Graal
    JIT is an implementation of JVMCI.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provided the basis for understanding how JVM works, and how JIT
    compilation optimizes the code at runtime. This is key in understanding how the
    Graal JIT compiler works.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand how the Graal VM architecture is built
    on the JVM architecture, and how it extends it to support Polyglot.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a code cache?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the various flags that can be used to optimize a code cache?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the compiler threshold?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is on-stack replacement?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is tiered compilation? What are the various patterns of tiered compilation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is inlining?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is monomorphic dispatch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is loop unrolling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is escape analysis?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Deoptimization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is JVMCI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Introduction to JVM Languages* ([https://www.packtpub.com/product/introduction-to-jvm-languages/9781787127944](https://www.packtpub.com/product/introduction-to-jvm-languages/9781787127944))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java SDK documentation ([https://docs.oracle.com](https://docs.oracle.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GraalVM documentation ([https://docs.oracle.com/en/graalvm/enterprise/19/guide/overview/compiler.html](https://docs.oracle.com/en/graalvm/enterprise/19/guide/overview/compiler.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JITWatch documentation ([https://github.com/AdoptOpenJDK/jitwatch](https://github.com/AdoptOpenJDK/jitwatch))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
