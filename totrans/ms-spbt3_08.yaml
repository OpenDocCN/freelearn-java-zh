- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Exploring Event-Driven Systems with Kafka
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kafka 探索事件驱动系统
- en: 'In this chapter, we will delve into the mechanics of creating an event-driven
    system using Kafka and Spring Boot. Here, we’ll discover how to configure Kafka
    and ZooKeeper on your computer using Docker, laying the foundation for developing
    microservices that can seamlessly communicate through events. You’ll get hands-on
    experience with building two Spring Boot applications: one for generating events
    and the other for consuming them, simulating the functions of a sender and receiver
    in a messaging framework.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨使用 Kafka 和 Spring Boot 创建事件驱动系统的机制。在这里，我们将发现如何使用 Docker 在您的计算机上配置
    Kafka 和 ZooKeeper，为开发能够通过事件无缝通信的微服务奠定基础。您将获得实际操作经验，构建两个 Spring Boot 应用程序：一个用于生成事件，另一个用于消费事件，模拟消息框架中的发送者和接收者的功能。
- en: The ultimate aim of this chapter is to equip you with the skills to design,
    deploy, and monitor an **event-driven architecture** (**EDA**) that harnesses
    the capabilities of Kafka combined with the simplicity of Spring Boot. This knowledge
    is not crucial for your progress in this book’s journey but invaluable in real-world
    scenarios where scalable and responsive systems are not just preferred but expected.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最终目标是让您掌握设计、部署和监控利用 Kafka 功能并结合 Spring Boot 简单性的 **事件驱动架构**（**EDA**）的技能。这些知识对于您在本书旅程中的进步并非至关重要，但在现实世界中，可扩展和响应性系统不仅受到青睐，而且被视为期望。
- en: Mastering these principles and tools is essential for creating applications
    that are adaptable, scalable, and capable of meeting the evolving demands of contemporary
    software environments. By the conclusion of this chapter, you will have an event-driven
    setup on your local machine, boosting your confidence to tackle more complex systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握这些原则和工具对于创建能够适应、可扩展并能满足当代软件环境不断变化需求的程序至关重要。在本章结束时，您将在本地机器上拥有一个事件驱动设置，这将增强您处理更复杂系统的信心。
- en: 'The following are the main topics of this chapter that you’ll explore:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该章节的主要主题，您将进行探索：
- en: Introduction to event-driven architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件驱动架构简介
- en: Setting up Kafka and ZooKeeper for local development
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地开发中设置 Kafka 和 ZooKeeper
- en: Building an event-driven application with Spring Boot messaging
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spring Boot 消息构建事件驱动应用程序
- en: Monitoring event-driven systems
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控事件驱动系统
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, we are going to need to configure some settings on our local
    machines:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们需要在我们的本地机器上配置一些设置：
- en: '**Java Development Kit 17** (**JDK 17**)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java 开发工具包 17**（**JDK 17**）'
- en: A modern **integrated development environment** (**IDE**); I recommend IntelliJ
    IDEA
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个现代 **集成开发环境**（**IDE**）；我推荐 IntelliJ IDEA
- en: 'You can clone all repositories related to [*Chapter 8*](B18400_08.xhtml#_idTextAnchor233)
    from the GitHub repository here: [https://github.com/PacktPublishing/Mastering-Spring-Boot-3.0/](https://github.com/PacktPublishing/Mastering-Spring-Boot-3.0/)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以从以下 GitHub 仓库克隆与[*第 8 章*](B18400_08.xhtml#_idTextAnchor233)相关的所有仓库：[https://github.com/PacktPublishing/Mastering-Spring-Boot-3.0/](https://github.com/PacktPublishing/Mastering-Spring-Boot-3.0/)
- en: Docker Desktop
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Desktop
- en: Introduction to event-driven architecture
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件驱动架构简介
- en: '**Event-driven architecture**, also known as **EDA**, is a design approach
    widely used in software development. It focuses more on triggering actions based
    on events than following a strict step-by-step process. In EDA, when a specific
    event occurs, the system reacts promptly by carrying out the action or series
    of actions. This method differs from models that rely on request-response patterns
    and offers a more dynamic and real-time system behavior.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件驱动架构**，也称为 **EDA**，是一种在软件开发中广泛使用的架构方法。它更侧重于根据事件触发动作，而不是遵循严格的步骤流程。在 EDA
    中，当发生特定事件时，系统会迅速做出反应，执行相应的动作或一系列动作。这种方法与依赖于请求-响应模式的其他模型不同，它提供了一个更动态和实时的系统行为。'
- en: EDA is significant in the era we’re living in where data is constantly being
    generated and updated. The ability to promptly respond to changes is invaluable
    in such a fast-paced environment. EDA empowers businesses to seize opportunities
    and address challenges swiftly compared to conventional systems. This agility
    is particularly crucial in industries such as finance, real-time analytics, the
    **Internet of Things** (**IoT**), and other areas where rapid changes occur frequently
    and the timeliness of information holds importance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个数据不断生成和更新的时代，EDA具有重要意义。能够迅速响应变化的能力在这样的快节奏环境中是无价的。与传统的系统相比，EDA使企业能够迅速抓住机会和应对挑战。这种敏捷性在金融、实时分析、**物联网**（**IoT**）和其他快速变化频繁、信息时效性重要的领域尤为重要。
- en: 'Moving to EDA can significantly change how a company functions, offering the
    following benefits:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 转向EDA可以显著改变公司的运作方式，带来以下好处：
- en: '**Responsiveness**: By handling events in real time, event-driven systems offer
    immediate feedback or action, which is crucial for time-sensitive tasks.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应性**：通过实时处理事件，事件驱动系统提供即时的反馈或行动，这对于时间敏感的任务至关重要。'
- en: '**Scalability**: Event-driven setups can manage a number of events without
    causing delays in processing. This scalability is important for businesses dealing
    with increasing data volume and complexity.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：事件驱动的设置可以在不造成处理延迟的情况下管理大量事件。这种可扩展性对于处理数据量不断增加和复杂性的企业来说非常重要。'
- en: '**Flexibility**: As components in EDA are loosely connected, they can be updated
    or replaced independently without impacting the system. This flexibility makes
    upgrades and the integration of features simpler.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：由于EDA中的组件松散连接，它们可以独立更新或替换，而不会影响系统。这种灵活性使得升级和功能的集成更加简单。'
- en: '**Efficiency**: Minimizing the need for checking for new data through polling
    or querying reduces resource consumption, improving overall system efficiency.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：通过减少通过轮询或查询检查新数据的需求，可以降低资源消耗，从而提高整体系统效率。'
- en: '**Enhanced user experience**: In applications requiring real-time information,
    such as gaming and live updates, EDA contributes to providing a dynamic user experience.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的用户体验**：在需要实时信息的应用中，如游戏和实时更新，EDA有助于提供动态的用户体验。'
- en: These benefits highlight why many organizations are moving toward EDA to meet
    the demands of modern technological challenges.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优势突显了为什么许多组织正在转向EDA以满足现代技术挑战的需求。
- en: In EDA, we need a **message broker**. A message broker helps us to distribute
    the message between the components. In this chapter, we will use Apache Kafka
    as a message broker. Kafka is an open source stream-processing platform. It was
    initially developed by LinkedIn and later donated to the Apache Software Foundation.
    Kafka primarily functions as a message broker adept at handling substantial data
    volumes efficiently.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在EDA中，我们需要一个**消息代理**。消息代理帮助我们分发组件之间的消息。在本章中，我们将使用Apache Kafka作为消息代理。Kafka是一个开源的流处理平台。它最初由LinkedIn开发，后来捐赠给了Apache软件基金会。Kafka主要作为一个高效的、擅长处理大量数据的消息代理。
- en: Its design features facilitate durable message storage and high-throughput event
    processing for effective EDA implementations. This platform allows distributed
    data streams to be consumed in time, making it an optimal solution for applications
    requiring extensive data-processing and transfer capabilities.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其设计特性促进了持久消息存储和高效的事件处理，以实现有效的EDA实现。这个平台允许分布式数据流及时被消费，使其成为需要广泛数据处理和传输能力的应用的理想解决方案。
- en: With Kafka, developers can seamlessly transfer data between components of an
    event-driven system, ensuring the preservation of event integrity and order even
    in complex transaction scenarios. This feature positions Kafka as a component
    in the architecture of many modern high-performance applications that rely on
    real-time data processing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kafka，开发者可以在事件驱动系统的组件之间无缝传输数据，确保即使在复杂的交易场景中也能保持事件完整性和顺序。这一特性使Kafka成为许多现代高性能应用架构中的组件，这些应用依赖于实时数据处理。
- en: Now that we have a grasp of what EDA entails and the benefits it brings, along
    with understanding Kafka’s role in such systems, we will go through the process
    of setting up Kafka on Docker. This setup creates a controlled and reproducible
    environment for the exploration of Kafka’s capabilities within EDA. Our aim is
    to equip you with the tools and knowledge to deploy Kafka efficiently, enabling
    you to harness the potential of real-time data processing in your projects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 EDA 的含义及其带来的好处，以及理解了 Kafka 在此类系统中的作用，我们将通过在 Docker 上设置 Kafka 的过程。这种设置创建了一个受控且可重复的环境，用于探索
    Kafka 在 EDA 中的功能。我们的目标是为你提供部署 Kafka 的工具和知识，使你能够利用实时数据处理在项目中的潜力。
- en: By mastering the deployment of Kafka using Docker, you will acquire the experience
    essential for comprehending and managing the intricacies of event-driven systems.
    This hands-on approach not only reinforces theoretical concepts but also readies
    you to effectively handle real-world applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握使用 Docker 部署 Kafka 的技能，你将获得理解和管理事件驱动系统复杂性的必要经验。这种动手实践方法不仅加强了理论概念，而且使你能够有效地处理现实世界中的应用。
- en: Setting up Kafka and ZooKeeper for local development
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为本地开发设置 Kafka 和 ZooKeeper
- en: '**Kafka** plays a role in an event-driven system, facilitating smooth communication
    among different components. It enables services to communicate through message
    exchange, like how people use messaging apps to stay connected. This architecture
    promotes the development of scalable applications by allowing various parts of
    the system to function autonomously and respond promptly to events. We will also
    mention Kafka and its role in the *Understanding Kafka brokers and their role
    in event-driven systems* section in more detail.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kafka** 在事件驱动系统中扮演着角色，促进不同组件之间的顺畅通信。它使服务能够通过消息交换进行通信，就像人们使用即时通讯应用保持联系一样。这种架构通过允许系统的各个部分独立运行并迅速响应事件，促进了可扩展应用程序的开发。我们还将更详细地提及
    Kafka 以及其在 *理解 Kafka 代理及其在事件驱动系统中的作用* 部分中的作用。'
- en: However, Kafka doesn’t work alone; it collaborates with **ZooKeeper**, which
    serves as its overseer. ZooKeeper monitors Kafka’s brokers to ensure they’re functioning.
    Think of it as having a coordinator who assigns tasks and ensures operations.
    ZooKeeper is essential for managing the background processes that uphold Kafka’s
    stability and reliability during peak loads.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kafka 并不单独工作；它与 **ZooKeeper** 协作，ZooKeeper 作为其监管者。ZooKeeper 监控 Kafka 的代理以确保它们正常运行。把它想象成一个协调员，分配任务并确保操作。ZooKeeper
    对于管理在高峰负载期间维持 Kafka 的稳定性和可靠性的后台进程至关重要。
- en: After talking about the components we need, I will also mention the installation.
    We will use Docker as we did in previous chapters. Docker simplifies the setup
    of Kafka and ZooKeeper on your machine. It provides a portable version of the
    entire configuration that you can easily launch whenever needed, hassle-free.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了我们需要组件之后，我还会提到安装。我们将使用 Docker，就像我们在前面的章节中所做的那样。Docker 简化了 Kafka 和 ZooKeeper
    在你机器上的设置。它提供了一个易于启动的便携式配置版本，你可以在需要时轻松启动，无需麻烦。
- en: This method of setting up Kafka and ZooKeeper isn’t just for convenience; it’s
    also about ensuring that you can explore, create, and test your event-driven systems
    without having to worry about intricate installation procedures or variations
    between setups. As we delve into the steps of setting up Kafka and ZooKeeper using
    Docker, remember that this forms the groundwork. You’re establishing an adaptable
    infrastructure for your applications—one that will facilitate effective communication
    and seamless scalability. Let’s proceed and get your local development environment
    ready for EDA.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置 Kafka 和 ZooKeeper 的方法不仅是为了方便；它还关乎确保你可以在不担心复杂的安装程序或设置之间的差异的情况下探索、创建和测试你的事件驱动系统。当我们深入研究使用
    Docker 设置 Kafka 和 ZooKeeper 的步骤时，请记住，这构成了基础。你正在为应用程序建立一个适应性强的基础设施——这将促进有效的通信和无缝的可扩展性。让我们继续前进，为你的本地开发环境准备
    EDA。
- en: Understanding Kafka brokers and their role in event-driven systems
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Kafka 代理及其在事件驱动系统中的作用
- en: In the changing world of EDA, **Kafka brokers** serve as efficient hubs carefully
    managing the reception routing and delivery of messages to their designated destinations.
    Within the Kafka ecosystem, a Kafka broker plays a role as part of a group of
    brokers that work together to oversee message traffic. In simple terms, imagine
    these brokers as diligent postal workers handling the messages from producers
    and organizing them into topics similar to specific mailboxes or addresses. These
    topics can be divided into sections to facilitate scalable message processing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在不断变化的 EDA 世界中，**Kafka 代理**作为高效的枢纽，精心管理消息的接收、路由和交付到指定的目的地。在 Kafka 生态系统中，Kafka
    代理作为一组协同工作以监控消息流量的代理的一部分发挥作用。简单来说，想象这些代理就像勤奋的邮递员，处理生产者的消息并将它们组织成类似特定邮箱或地址的主题。这些主题可以分成多个分区，以促进可扩展的消息处理。
- en: Let’s see how a Kafka cluster works in *Figure 8**.1*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在*图 8.1*中 Kafka 集群是如何工作的。
- en: '![Figure 8.1: Kafka cluster architecture](img/B18400_08_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1：Kafka 集群架构](img/B18400_08_01.jpg)'
- en: 'Figure 8.1: Kafka cluster architecture'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：Kafka 集群架构
- en: In this diagram, you can see how Kafka organizes its workflow. Producers are
    the sources that send data to the Kafka system. They push messages into the Kafka
    cluster, which consists of multiple brokers (**Broker 1**, **Broker 2**, and **Broker
    3**). These brokers store the messages and make them available for consumption.
    ZooKeeper acts as the manager of this cluster, keeping track of the state of brokers
    and performing other coordination tasks. Consumer groups, labeled **Group-A**
    and **Group-B**, pull messages from the brokers depending on their needs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，你可以看到 Kafka 如何组织其工作流程。生产者是向 Kafka 系统发送数据的来源。它们将消息推送到 Kafka 集群，该集群由多个代理（**代理
    1**、**代理 2**和**代理 3**）组成。这些代理存储消息并使其可供消费。ZooKeeper 作为该集群的管理者，跟踪代理的状态并执行其他协调任务。消费者组，标记为**Group-A**和**Group-B**，根据需要从代理中拉取消息。
- en: The true magic of Kafka brokers lies in their adeptness at managing these topic
    sections. When a message arrives, the broker determines where to place it within
    a section based on criteria such as its importance level. This method ensures
    a distribution of messages and groups similar ones (those sharing common attributes)
    in one section. This partitioning process is essential for distributing workloads
    and enables consumer applications to process messages concurrently for more streamlined
    data handling.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 代理的真正魔法在于它们在管理这些主题分区方面的熟练程度。当一条消息到达时，代理根据其重要性等级等标准确定将其放置在分区内的位置。这种方法确保了消息的分布，并在一个分区中将相似的消息（具有共同属性的）分组。这种分区过程对于分配工作负载至关重要，并使消费者应用程序能够并发处理消息，从而实现更高效的数据处理。
- en: Furthermore, another critical function of Kafka brokers is ensuring message
    duplication across the Kafka system, safeguarding against data loss in case of
    broker malfunctions. This duplication process acts as a safety measure by creating
    copies of sections across different brokers. If a broker goes offline, another
    can step in, smoothly keeping the system strong and flexible.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Kafka 代理的另一个关键功能是确保 Kafka 系统中的消息重复，以防代理故障导致数据丢失。这种重复过程通过在不同代理之间创建分区副本作为安全措施。如果一个代理离线，另一个可以介入，平滑地保持系统的强大和灵活。
- en: Brokers are skilled at storing and providing messages for consumers. They use
    offsets to track which messages consumers have read, allowing consumers to resume
    right where they left off in the message stream. This ensures that every message
    is handled and gives consumers the flexibility to manage messages at their own
    pace.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 代理擅长存储和为消费者提供消息。他们使用偏移量来跟踪消费者已读取的消息，使消费者能够从消息流中上次离开的地方继续读取。这确保了每条消息都被处理，并给消费者提供了按自己的节奏管理消息的灵活性。
- en: The orchestration of messages in a Kafka cluster, overseen by brokers, is a
    process that combines efficiency with reliability. This intricate coordination
    carried out by brokers enables event-driven systems to function efficiently, managing
    large amounts of data with precision. By utilizing the features of Kafka brokers,
    developers can create systems that are not only scalable and resilient but also
    capable of processing messages swiftly and accurately to meet the demands of today’s
    fast-paced digital landscape.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群中由代理监督的消息编排是一个结合了效率和可靠性的过程。由代理执行的这种复杂协调使得事件驱动系统能够高效运行，精确地管理大量数据。通过利用Kafka代理的功能，开发者可以创建不仅可扩展和弹性好，而且能够快速、准确地处理消息以满足当今快节奏数字景观需求的系统。
- en: As we further explore the aspects of setting up and using Kafka, the role of
    brokers as the foundation for reliable and efficient message distribution becomes
    increasingly clear. Their ability to handle and direct messages serves as the
    core of any EDA, ensuring that information is delivered accurately to its intended
    destination on time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进一步探索设置和使用Kafka的方面，代理作为可靠和高效消息分发基础的作用变得越来越明显。它们处理和引导消息的能力是任何事件驱动架构（EDA）的核心，确保信息能够准时准确地送达目的地。
- en: Running Kafka and ZooKeeper with Docker
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Docker运行Kafka和ZooKeeper
- en: Running Kafka and ZooKeeper on your computer through Docker can be a game-changer
    for developers. It streamlines what was once a setup process into something simple
    and easy to handle. Docker containers serve as transportable spaces that can be
    swiftly initiated, halted, and deleted, making them ideal for development and
    testing purposes. This arrangement enables you to recreate a production-level
    environment on your machine without the need for setup or specialized hardware.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Docker在您的计算机上运行Kafka和ZooKeeper，对于开发者来说可能是一个颠覆性的改变。它将曾经繁琐的设置过程简化为简单易操作的过程。Docker容器作为可携带的空间，可以迅速启动、停止和删除，非常适合开发和测试目的。这种安排使您能够在机器上重新创建生产级环境，而无需设置或专用硬件。
- en: 'You will be familiar with Docker Compose since we have used it in almost all
    the previous chapters. We will use Docker Compose to run both services with a
    single command. Here’s a simple `docker-compose.yml` file example that sets up
    Kafka and ZooKeeper:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在几乎所有前面的章节中都使用了Docker Compose，您将熟悉它。我们将使用Docker Compose通过单个命令运行这两个服务。以下是一个简单的`docker-compose.yml`文件示例，用于设置Kafka和ZooKeeper：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `docker-compose.yml` file is like a recipe that tells Docker exactly how
    to run your Kafka and ZooKeeper containers. It tells Docker which images to use,
    how the containers should talk to each other on a network, which ports to open,
    and what environment variables to set. In this file, we have told Docker to run
    ZooKeeper on port `2181` and Kafka on port `9092`. Using this file, we streamline
    the whole process, making it as easy as pressing a button to get your setup running.
    It’s a brilliant tool for developers, cutting down on the manual steps and letting
    you focus on the fun part—building and experimenting with your event-driven applications.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-compose.yml`文件就像一个食谱，告诉Docker如何运行您的Kafka和ZooKeeper容器。它告诉Docker使用哪些镜像，容器如何在网络上相互通信，哪些端口需要打开，以及需要设置哪些环境变量。在这个文件中，我们告诉Docker在端口`2181`上运行ZooKeeper，在端口`9092`上运行Kafka。使用这个文件，我们简化了整个过程，使其像按按钮一样简单，以启动您的设置。这是一个出色的开发者工具，减少了手动步骤，让您专注于有趣的部分——构建和实验您的事件驱动应用程序。'
- en: 'Save this file as `docker-compose.yml` and run it using this command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将此文件保存为`docker-compose.yml`，并使用以下命令运行：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command pulls the necessary Docker images, creates the containers, and
    starts Kafka and ZooKeeper in detached mode, leaving them running in the background.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令拉取必要的Docker镜像，创建容器，并以分离模式启动Kafka和ZooKeeper，使它们在后台运行。
- en: By following these steps, you’ve just set up a robust, scalable messaging backbone
    for your applications to build upon. This foundation not only supports the development
    of event-driven systems but also paves the way for experimenting with Kafka’s
    powerful features in a controlled local environment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，您已经为您的应用程序搭建了一个强大、可扩展的消息骨干，以便在此基础上构建。这个基础不仅支持事件驱动系统的开发，还为在受控的本地环境中实验Kafka的强大功能铺平了道路。
- en: Finishing our exploration of configuring Kafka using Docker, it’s evident how
    this pairing removes the obstacles in running Kafka on your computer. Docker’s
    container magic has turned what might have been a laborious task into a straightforward
    process, allowing you to concentrate more on the creative aspects of developing
    applications rather than getting caught up in setup intricacies. This simplified
    setup isn’t only about convenience; it’s also about democratizing technology and
    simplifying its management, empowering developers to experiment and innovate with
    EDA without dealing with overly complicated configurations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 完成我们对使用 Docker 配置 Kafka 的探索后，很明显，这种组合消除了在您的计算机上运行 Kafka 的障碍。Docker 的容器魔法将可能是一项繁重的工作转变为一个简单的过程，让您能够更多地关注应用程序开发的创造性方面，而不是陷入设置的复杂性中。这种简化的设置不仅关乎便利，还关乎技术民主化和管理的简化，赋予开发者无需处理过于复杂的配置即可进行实验和创新
    EDA 的能力。
- en: As we shift from the aspects of setting up Kafka and ZooKeeper to delving into
    the exciting realm of constructing an event-driven application using Spring Boot
    messaging, we’re transitioning from laying the groundwork for infrastructure to
    engaging in the artistry of application design. In this section, you’ll witness
    firsthand how your Kafka setup empowers you as we walk you through the creation
    of applications that generate and consume messages with Spring Boot. This is where
    abstract concepts materialize into creations allowing you to fully leverage the
    capabilities of event-driven systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从设置 Kafka 和 ZooKeeper 的方面转向探索使用 Spring Boot 消息构建事件驱动应用程序的激动人心领域，我们正从基础设施建设的基石转向应用程序设计的艺术。在本节中，您将亲身体验到您的
    Kafka 设置如何赋予您力量，因为我们将引导您创建使用 Spring Boot 生成和消费消息的应用程序。这正是抽象概念转化为创造的地方，让您能够充分利用事件驱动系统的功能。
- en: Building an event-driven application with Spring Boot messaging
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spring Boot 消息构建事件驱动应用程序
- en: Crafting an event-driven application using Spring Boot involves building a system
    that’s responsive, scalable, and equipped to handle the complexities of modern
    software requirements. Essentially, an event-driven application responds to events,
    ranging from user interactions to messages from external systems. This methodology
    enables components of your application to interact and operate independently,
    enhancing flexibility and efficiency. With Spring Boot, setting up such an application
    is made easier due to its philosophy of convention over configuration and the
    array of tools it provides from the start.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spring Boot 构建事件驱动应用程序涉及构建一个响应迅速、可扩展并能处理现代软件需求复杂性的系统。本质上，事件驱动应用程序响应的事件范围从用户交互到外部系统的消息。这种方法使您的应用程序组件能够独立交互和操作，从而提高灵活性和效率。由于
    Spring Boot 的约定优于配置的哲学以及它从一开始就提供的工具集，使用 Spring Boot 设置此类应用程序变得更加容易。
- en: Throughout this journey, we will take a hands-on approach by introducing two
    Spring Boot projects—one will focus on generating events while the other will
    concentrate on consuming them. This segregation mirrors real-life scenarios where
    producers and consumers are often located in systems or microservices highlighting
    the decentralized nature of contemporary applications. By working on these projects,
    you will gain experience in configuring a producer for sending messages and a
    consumer for reacting to those messages, within the context of Spring Boot and
    Kafka. This method not only strengthens your comprehension of event-driven systems
    but also equips you with the resources needed to create and enhance your own scalable
    applications.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个旅程中，我们将通过介绍两个 Spring Boot 项目来采取动手实践的方法——一个将专注于生成事件，另一个将专注于消费它们。这种分离反映了现实生活中的场景，其中生产者和消费者通常位于系统或微服务中，突出了当代应用程序的去中心化特性。通过参与这些项目，您将获得配置用于发送消息的生产者和用于响应这些消息的消费者在
    Spring Boot 和 Kafka 上下文中的经验。这种方法不仅加强了您对事件驱动系统的理解，还为您创建和改进可扩展的应用程序提供了所需的资源。
- en: As we move forward, we’ll dive into the details of creating a Spring Boot project
    for Kafka integration. This will establish the foundation for our event-based
    applications, walking you through the process of configuring a Spring Boot project
    to send and receive messages using Kafka. You’ll gain insights into the settings,
    libraries, and initial code structures required to kick start the implementation.
    Here is where our theoretical ideas transform into executable code. So, let’s
    get started and embark on this journey of developing robust interactive applications
    with Spring Boot and Kafka.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，我们将深入了解创建 Kafka 集成的 Spring Boot 项目的细节。这将为我们的事件驱动应用程序奠定基础，引导您通过配置 Spring
    Boot 项目使用 Kafka 发送和接收消息的过程。您将了解启动实现所需的设置、库和初始代码结构。在这里，我们的理论想法将转化为可执行代码。所以，让我们开始这段旅程，用
    Spring Boot 和 Kafka 开发强大的交互式应用程序。
- en: Creating a Spring Boot project for Kafka integration
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于 Kafka 集成的 Spring Boot 项目
- en: Starting a project in Spring Boot that is specifically tailored for integrating
    with Kafka is the practical step toward unlocking the capabilities of event-driven
    applications. This step combines the ease and adaptability of Spring Boot with
    the messaging features of Kafka, allowing developers to create scalable and agile
    applications. Through this integration, we are establishing a base that facilitates
    communication and the management of large data volumes and operations in a distributed
    setting. The objective is to establish a framework that addresses message production
    and consumption requirements while also seamlessly expanding as the application
    evolves.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spring Boot 中启动一个专门针对 Kafka 集成的项目是解锁事件驱动应用程序功能的具体步骤。这一步骤结合了 Spring Boot 的便捷性和适应性以及
    Kafka 的消息功能，使开发者能够创建可扩展和敏捷的应用程序。通过这次集成，我们正在建立一个基础，它有助于在分布式环境中进行通信和管理大量数据和操作。目标是建立一个框架，该框架不仅满足消息生产和消费需求，而且随着应用程序的发展能够无缝扩展。
- en: We will need two different projects to demonstrate the consumer and producer.
    So, you will need to follow the same steps twice to create the two projects. But
    it would be better to choose a different name when entering the project metadata
    in *step 2*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两个不同的项目来演示消费者和生成者。因此，您需要重复执行相同的步骤两次来创建这两个项目。但在 *步骤 2* 中输入项目元数据时，最好选择不同的名称。
- en: In *Figure 8**.2*, we can see how our applications will communicate with each
    other.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 8.2* 中，我们可以看到我们的应用程序将如何相互通信。
- en: '![Figure 8.2: How our apps communicate with each other](img/B18400_08_02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2：我们的应用程序如何相互通信](img/B18400_08_02.jpg)'
- en: 'Figure 8.2: How our apps communicate with each other'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：我们的应用程序如何相互通信
- en: As you can see in *Figure 8**.2*, there is no direct call between the producer
    application and the consumer application. The producer application sends a message
    to Kafka and Kafka publishes this message to the consumer application.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在 *图 8.2* 中所见，生产者应用程序和消费者应用程序之间没有直接的调用。生产者应用程序向 Kafka 发送消息，Kafka 将此消息发布给消费者应用程序。
- en: 'Here is a step-by-step guide to creating a Spring Boot project:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是创建 Spring Boot 项目的分步指南：
- en: Navigate to Spring Initializr ([https://start.spring.io/](https://start.spring.io/))
    to bootstrap your project. It’s an online tool that lets you generate a Spring
    Boot project with your chosen dependencies quickly.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 Spring Initializr ([https://start.spring.io/](https://start.spring.io/))
    以启动您的项目。这是一个在线工具，允许您快速生成带有所选依赖项的 Spring Boot 项目。
- en: Enter your project’s metadata, such as **Group**, **Artifact**, and **Description**.
    Give different names for consumer and producer projects. Choose either **Maven**
    or **Gradle** as your build tool according to your preference. In our example,
    we will use **Gradle**.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入您项目的元数据，例如 **组**、**工件**和**描述**。为消费者和生成者项目提供不同的名称。根据您的偏好选择 **Maven** 或 **Gradle**
    作为构建工具。在我们的示例中，我们将使用 **Gradle**。
- en: Select your dependencies. For a Kafka project, you need to add `Spring Web`
    for the producer project. This dependency includes the necessary libraries to
    integrate Kafka with Spring Boot.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您的依赖项。对于 Kafka 项目，您需要为生产者项目添加 `Spring Web`。这个依赖项包括将 Kafka 与 Spring Boot 集成的必要库。
- en: Generate the project. Once you’ve filled in all the details and selected your
    dependencies, click on **Generate** to download your project template.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成项目。一旦您填写了所有详细信息并选择了依赖项，请点击 **生成** 下载您的项目模板。
- en: In *Figure 8**.3*, we can see which dependencies we need and how to configure
    Spring Initializr.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在**图8.3**中，我们可以看到我们需要哪些依赖项以及如何配置Spring Initializr。
- en: '![Figure 8.3: Screenshot of Spring Initialzr](img/B18400_08_03.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3：Spring Initializr的截图](img/B18400_08_03.jpg)'
- en: 'Figure 8.3: Screenshot of Spring Initialzr'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：Spring Initializr的截图
- en: Extract the downloaded ZIP file and open the project in your favorite IDE, such
    as IntelliJ IDEA, Eclipse, or VS Code.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取下载的ZIP文件，并在您最喜欢的IDE中打开项目，例如IntelliJ IDEA、Eclipse或VS Code。
- en: 'Update the `application.properties` file using the following line. Use different
    ports for consumer and publisher projects:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下行更新`application.properties`文件。为消费者和发布者项目使用不同的端口：
- en: '[PRE2]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When integrating Kafka with a Spring Boot project, a key component is **Spring
    Kafka**, which is added by Spring Initializr as Spring for Apache Kafka. This
    library simplifies the handling of messaging solutions based on Kafka by providing
    a user abstraction. It streamlines the process of sending and receiving messages
    between your Spring Boot application and Kafka brokers. By abstracting the complexities
    of producer and consumer configurations, it enables you to focus on implementing
    business logic rather than dealing with repetitive code for message handling.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当将Kafka与Spring Boot项目集成时，一个关键组件是**Spring Kafka**，这是由Spring Initializr作为Apache
    Kafka的Spring版本添加的。这个库通过提供用户抽象简化了基于Kafka的消息解决方案的处理。它简化了在Spring Boot应用和Kafka代理之间发送和接收消息的过程。通过抽象生产者和消费者配置的复杂性，它使您能够专注于实现业务逻辑，而不是处理消息处理的重性代码。
- en: With your Spring Boot project configured and essential Kafka integration dependencies
    in place, you are now ready to delve into the details of producing and consuming
    messages. This setup serves as a starting point for exploring communication and
    EDAs, offering an effective approach to managing data flow in your applications.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置好Spring Boot项目并放置了必要的Kafka集成依赖项后，您现在可以深入了解生产和消费消息的细节。这个设置是探索通信和EDAs的起点，为在应用中管理数据流提供了一种有效的方法。
- en: Moving on to building the producer application in the next subsection marks
    a shift from setup to implementation. Here, we will guide you through setting
    up a Kafka producer within your Spring Boot project. This is where all your foundational
    work begins to take shape, allowing you to send messages to Kafka topics and kickstart
    the communication process for any event-driven system. Get ready to translate
    theory into action and witness how your application can engage with Kafka.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 进入下一小节构建生产者应用标志着从设置到实现的转变。在这里，我们将指导您在Spring Boot项目中设置Kafka生产者。这是您所有基础工作开始成形的地方，让您能够向Kafka主题发送消息，并启动任何事件驱动系统的通信过程。准备好将理论转化为实践，并见证您的应用如何与Kafka交互。
- en: Building the producer application
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建生产者应用
- en: Creating the producer application is like establishing a broadcasting hub within
    your event-based framework, where your Spring Boot setup is all set to dispatch
    messages out to the world—or, precisely, to a Kafka topic. This stage holds importance
    as it marks the beginning of information flow within your system, ensuring that
    data reaches its intended destination at the right moment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 创建生产者应用就像在基于事件框架内建立一个广播中心，您的Spring Boot设置已经准备好向世界——或者更确切地说，向Kafka主题——发送消息。这一阶段非常重要，因为它标志着系统内信息流的开始，确保数据在正确的时间到达预定的目的地。
- en: Creating a Kafka producer in Spring Boot involves a few straightforward steps.
    First, you need to configure your application to connect to Kafka. This is done
    in the `application.properties` file in your producer Spring Boot project. You’ll
    specify details such as the Kafka server’s address and the default topic to which
    you want to send messages.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spring Boot中创建Kafka生产者涉及几个简单的步骤。首先，您需要配置您的应用以连接到Kafka。这是在您的生产者Spring Boot项目的`application.properties`文件中完成的。您将指定有关Kafka服务器地址和您想要发送消息的默认主题的详细信息。
- en: 'Here’s how we will implement a Kafka producer in a Spring Boot application:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何在Spring Boot应用中实现Kafka生产者的方式：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this code, `KafkaTemplate` is a Spring-provided class that simplifies sending
    messages to a Kafka topic. We inject this template into our `MessageProducer`
    service and use its `send` method to publish messages. The `send` method takes
    two parameters—the name of the topic and the message itself.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，`KafkaTemplate`是一个Spring提供的类，它简化了向Kafka主题发送消息的过程。我们将此模板注入到`MessageProducer`服务中，并使用其`send`方法发布消息。`send`方法接受两个参数——主题的名称和消息本身。
- en: 'To ensure your producer application can successfully send messages to Kafka,
    you’ll need to add some configurations to your `application.properties` file:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您的生产者应用程序能够成功地向Kafka发送消息，您需要在`application.properties`文件中添加一些配置：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These configurations help Spring Boot identify the location of your Kafka server
    (Bootstrap servers) and how to convert the messages into a format for transmission
    over the network (key serializer and value serializer). Serialization involves
    converting your message, in this case, a string, into a format that can be transmitted
    over the network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置帮助Spring Boot识别您的Kafka服务器位置（引导服务器）以及如何将消息转换为通过网络传输的格式（键序列化和值序列化）。序列化涉及将您的消息（在这种情况下，是一个字符串）转换为可以在网络上传输的格式。
- en: By setting up and configuring your Kafka producer, you have taken a step toward
    developing an event-driven application. This configuration allows your application
    to initiate conversations within your distributed system by sending out messages
    that other parts of your system can respond to and handle.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置和配置您的Kafka生产者，您已经朝着开发事件驱动应用程序迈出一步。此配置允许您的应用程序通过发送其他系统部分可以响应和处理的消息，在分布式系统中发起对话。
- en: 'Moving forward, let’s shift our focus to the counterpart of this interaction:
    building the consumer application. This involves creating listeners that anticipate
    and react to messages dispatched by our producer. It plays a role in closing the
    communication loop within our EDA, transforming our system into a dynamic network
    of services capable of responding to real-time data. Let’s proceed with our exploration
    and uncover how we can unleash the potential of event-driven applications.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将注意力转向这种交互的对应物：构建消费者应用程序。这包括创建监听器，以预测和响应由我们的生产者发送的消息。它在我们的EDA内部关闭通信循环中发挥作用，将我们的系统转变为一个能够对实时数据进行响应的动态服务网络。让我们继续我们的探索，揭示我们如何释放事件驱动应用程序的潜力。
- en: Building the consumer application
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建消费者应用程序
- en: Once we’ve got our broadcasting station set up using our producer application,
    it’s time to tune to the correct frequency by developing the consumer application.
    This step ensures that the messages sent out by the producer aren’t just lost
    in space but are actually received, understood, and put into action. In our event-driven
    structure, the consumer application acts like a listener in a crowd catching signals
    meant for it and handling them accordingly. By incorporating a Kafka consumer
    into a Spring Boot application, we establish an element that eagerly waits for
    messages and is prepared to process them as soon as they come through. This ability
    plays a role in creating systems that are truly interactive and can respond promptly
    to changes and events in real time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用生产者应用程序设置了广播站，就是时候通过开发消费者应用程序调整到正确的频率。这一步骤确保生产者发送的消息不会在太空中丢失，而是真正被接收、理解并付诸行动。在我们的事件驱动结构中，消费者应用程序就像人群中捕捉到针对它的信号并相应处理的听众。通过将Kafka消费者集成到Spring
    Boot应用程序中，我们建立了一个渴望等待消息并准备好在消息到来时立即处理它们的元素。这种能力在创建真正交互式并能实时迅速响应变化和事件的系统中发挥作用。
- en: To set up a Kafka consumer in Spring Boot, you first need to configure your
    application to listen to the Kafka topics of interest. This involves specifying
    in the `application.properties` file where your Kafka server is located and which
    topics your application should subscribe to.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spring Boot中设置Kafka消费者之前，您首先需要配置应用程序以监听感兴趣的Kafka主题。这涉及到在`application.properties`文件中指定您的Kafka服务器位置以及应用程序应订阅哪些主题。
- en: 'Here’s how we will implement a Kafka consumer in our Spring Boot application:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在我们的Spring Boot应用程序中实现Kafka消费者的方法：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this snippet, the `@KafkaListener` annotation marks the `listen` method as
    a listener for messages on `messageTopic`. The `groupId` is used by Kafka to group
    consumers that should be considered as a single unit. This setup allows your application
    to automatically pick up and process messages from the specified topic.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，`@KafkaListener`注解将`listen`方法标记为`messageTopic`上的消息监听器。`groupId`由Kafka用于将消费者分组，这些消费者应被视为一个单一单元。这种设置允许您的应用程序自动从指定的主题中获取并处理消息。
- en: 'To make sure your consumer application consumes messages efficiently, add the
    following configurations to your `application.properties` file:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您的消费者应用程序高效地消费消息，请将以下配置添加到您的`application.properties`文件中：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These configurations make sure your user connects to the Kafka server (Bootstrap
    servers) and properly decodes the messages it receives (key deserializer and value
    deserializer). The `auto-offset-reset` option guides Kafka on where to begin reading
    messages if there’s no offset for your users group; by setting it to `earliest`,
    our application will start to consume from the beginning of the event topic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置确保您的用户连接到 Kafka 服务器（引导服务器）并正确解码它接收到的消息（键反序列化和值反序列化）。`auto-offset-reset`
    选项指导 Kafka 在您的用户组没有偏移量时从何处开始读取消息；将其设置为 `earliest`，我们的应用程序将开始从事件主题的开始消费。
- en: Once your consumer application is active, your event-driven system is now fully
    operational, capable of both sending and receiving messages through the Kafka
    messaging pipeline. This two-way communication framework lays the foundation for
    scalable applications that can handle real-time data streams and respond promptly
    to events as they occur.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的消费者应用程序处于活动状态，您的事件驱动系统现在完全运行，能够通过 Kafka 消息管道发送和接收消息。这种双向通信框架为可扩展的应用程序奠定了基础，这些应用程序可以处理实时数据流并迅速对事件做出响应。
- en: Looking forward, the next critical step involves testing both the producer and
    consumer applications to ensure their integration. This phase bridges theory with
    practice, allowing you to witness the outcomes of your efforts. Testing serves
    not only to verify individual component functionalities but also to validate the
    overall responsiveness and efficiency of the system. Let’s progress by initiating
    tests on our event-driven applications, ensuring they’re primed to manage any
    challenges that may arise.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，下一个关键步骤涉及测试生产者和消费者应用程序以确保它们的集成。这一阶段将理论与实践相结合，让您见证您努力的成果。测试不仅用于验证单个组件的功能，还用于验证系统的整体响应性和效率。让我们通过在我们的事件驱动应用程序上启动测试来前进，确保它们准备好应对可能出现的任何挑战。
- en: Testing the whole stack – bringing your event-driven architecture to life
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试整个堆栈 – 使您的事件驱动架构变为现实
- en: After configuring our event-based system using Kafka, Spring Boot, and Docker,
    we reach a pivotal moment as we test the entire setup to witness our system in
    operation. This critical stage confirms that our separate elements, the producer
    and consumer applications, are properly set up and communicating as intended while
    also ensuring that Kafka, managed by Docker, effectively transmits messages between
    them. This testing phase represents the culmination of our work, allowing us to
    directly observe the dynamic exchange of messages that serves as the core of any
    event-driven system.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Kafka、Spring Boot 和 Docker 配置我们的基于事件的系统后，我们达到了一个关键的时刻，即测试整个设置以见证我们的系统运行。这一关键阶段确认了我们的各个元素，即生产者和消费者应用程序，已正确设置并按预期进行通信，同时确保由
    Docker 管理的 Kafka 有效地在它们之间传输消息。这一测试阶段标志着我们工作的完成，使我们能够直接观察作为任何事件驱动系统核心的消息动态交换。
- en: 'Here are the instructions to run the whole stack:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是运行整个堆栈的说明：
- en: '`docker-compose.yml` file that defines the Kafka and ZooKeeper services and
    run the following:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 Kafka 和 ZooKeeper 服务并运行以下 `docker-compose.yml` 文件：
- en: '[PRE7]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: server.port=8282
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: server.port=8282
- en: '[PRE8]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`8181`, by setting this in its `application.properties` file:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在 `application.properties` 文件中设置 `8181`：
- en: '[PRE9]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Use your IDE or the `Gradle` command as with the producer to start the consumer
    application.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用您的 IDE 或与生产者相同的 `Gradle` 命令来启动消费者应用程序。
- en: '`GET` requests to the producer’s message-triggering endpoint:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向生产者的消息触发端点发送 `GET` 请求：
- en: '[PRE10]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Replace `hello-world` with any string you wish to send as a message. Trigger
    a few different messages to test various scenarios.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 `hello-world` 替换为您希望发送的消息的任何字符串。触发几个不同的消息以测试各种场景。
- en: '**Observe the consumer’s log**: Switch to the console or log output of your
    consumer application. You should see the messages logged as they are consumed,
    indicating successful communication from the producer, through Kafka, to the consumer.
    The output will be as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**观察消费者的日志**：切换到您的消费者应用程序的控制台或日志输出。您应该看到随着消费而记录的消息，表明从生产者通过 Kafka 到消费者的通信成功。输出将如下所示：'
- en: '[PRE11]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Successfully running the test stack and observing the flow of messages from
    the producer to the consumer via Kafka is an invaluable experience because it
    showcases the power and flexibility of EDAs. This hands-on testing not only increases
    your understanding of integrating Kafka with Spring Boot applications but also
    highlights the importance of seamless communication in distributed systems. As
    you’ve seen, Docker plays a pivotal role in simplifying the setup for development
    and testing environments. After this practical experience, you are ready to delve
    into sophisticated and scalable event-driven applications, which are requested
    in modern software developments.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 成功运行测试栈并观察通过Kafka从生产者到消费者的消息流是一种宝贵的经验，因为它展示了事件驱动架构（EDA）的强大和灵活性。这种动手测试不仅增加了你对将Kafka与Spring
    Boot应用程序集成的理解，还强调了在分布式系统中无缝通信的重要性。正如你所看到的，Docker在简化开发和测试环境的设置中扮演着关键角色。经过这次实践经验，你将准备好深入研究复杂和可扩展的事件驱动应用程序，这在现代软件开发中是必需的。
- en: Now, with a fully functional event-driven application in hand, it’s time to
    look ahead. The next step is ensuring our application not only runs but succeeds
    under various conditions. This means diving into monitoring—a vital component
    of any application’s life cycle. In the upcoming section, we’ll explore how to
    keep a keen eye on our application’s performance and how to swiftly address any
    issues that arise. This knowledge will help not only in maintaining the health
    of our application but also in optimizing its efficiency and reliability. So,
    let’s move forward, ready to tackle these new challenges with confidence.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们手头有一个功能齐全的事件驱动应用程序，是时候向前看了。下一步是确保我们的应用程序不仅在各种条件下运行，而且能够成功。这意味着深入监控——这是任何应用程序生命周期中的关键组成部分。在接下来的部分，我们将探讨如何密切关注应用程序的性能，以及如何迅速解决出现的任何问题。这些知识不仅有助于维护应用程序的健康，还有助于优化其效率和可靠性。因此，让我们继续前进，自信地应对这些新的挑战。
- en: Monitoring event-driven systems
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控事件驱动系统
- en: In the dynamic world of event-driven systems, where applications communicate
    through a constant flow of messages, monitoring plays a crucial role in ensuring
    everything runs smoothly. Just as a busy airport needs air traffic control to
    keep planes moving safely and efficiently, an EDA relies on monitoring to maintain
    the health and performance of its components. This oversight is vital for spotting
    when things go wrong and understanding the overall system behavior under various
    loads and conditions. It enables developers and operations teams to make informed
    decisions, optimize performance, and prevent issues before they impact users.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件驱动系统的动态世界中，应用程序通过持续的流量进行通信，监控在确保一切顺利运行中扮演着关键角色。正如繁忙的机场需要空中交通管制来确保飞机安全高效地移动一样，事件驱动架构（EDA）依赖于监控来维护其组件的健康和性能。这种监督对于在问题发生时及时发现和理解系统在各种负载和条件下的整体行为至关重要。它使开发者和运维团队能够做出明智的决策，优化性能，并在问题影响用户之前预防问题。
- en: For applications built with Kafka and Spring Boot, a robust set of monitoring
    tools and techniques is essential for keeping an eye on the system’s pulse. At
    its core, Kafka is designed to handle high volumes of data, making monitoring
    aspects such as message throughput, broker health, and consumer lag imperative.
    Tools such as Apache Kafka’s JMX metrics and external utilities such as Prometheus
    and Grafana offer deep insights into Kafka’s performance. These tools can track
    everything from the number of messages being processed to the time it takes to
    travel through the system.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用Kafka和Spring Boot构建的应用，一套强大的监控工具和技术对于监控系统的脉搏至关重要。在核心上，Kafka被设计来处理大量数据，因此监控如消息吞吐量、代理健康和消费者延迟等特性变得至关重要。Apache
    Kafka的JMX指标和像Prometheus和Grafana这样的外部工具提供了对Kafka性能的深入洞察。这些工具可以追踪从正在处理的消息数量到消息穿越系统所需时间的所有方面。
- en: As monitoring the Spring Boot application was covered in the *Spring Boot Actuator
    with Prometheus and Grafana* section of [*Chapter 7*](B18400_07.xhtml#_idTextAnchor213),
    it won’t be covered here. We will only focus on monitoring Kafka in this section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在[*第7章*](B18400_07.xhtml#_idTextAnchor213)的[*“Spring Boot Actuator with Prometheus
    and Grafana”*](https://wiki.example.org/spring_boot_actuator_with_prometheus_and_grafana)部分已经涵盖了Spring
    Boot应用的监控，这里将不再重复。本节我们将专注于Kafka的监控。
- en: Monitoring your Kafka infrastructure
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控你的Kafka基础设施
- en: Monitoring your Kafka setup is like using a tool to closely examine the core
    functions of your event-driven system. It’s all about getting a view of how well
    your Kafka environment is running, which is crucial for identifying problems,
    optimizing resource usage, and ensuring messages are delivered on time and reliably.
    Given Kafka’s role in managing data streams and event processing, any issues or
    inefficiencies can impact the entire system. Therefore, establishing a monitoring
    system isn’t just helpful; it’s necessary for maintaining a strong and efficient
    architecture.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 Kafka 设置就像使用工具来仔细检查你的事件驱动系统的核心功能。这全部关乎了解你的 Kafka 环境运行状况，这对于识别问题、优化资源使用以及确保消息及时可靠地传递至关重要。鉴于
    Kafka 在管理数据流和事件处理中的作用，任何问题或低效都可能影响整个系统。因此，建立监控系统不仅是有帮助的，而且是维护强大高效架构的必要条件。
- en: 'Here are the key metrics to monitor in Kafka:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 Kafka 中需要监控的关键指标：
- en: '**Broker metrics**: These include the number of active brokers in your cluster
    and their health status. Monitoring the CPU, memory usage, and disk I/O of each
    broker helps in identifying resource bottlenecks.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理指标**：这些包括集群中活跃代理的数量及其健康状况。监控每个代理的 CPU、内存使用和磁盘 I/O 帮助识别资源瓶颈。'
- en: '**Topic metrics**: Important metrics here include message in-rate, message
    out-rate, and the size of topics. Keeping an eye on these can help in understanding
    the flow of data and spotting any unusual patterns.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题指标**：这里的重要指标包括消息输入率、消息输出率和主题大小。关注这些指标有助于理解数据流并发现任何异常模式。'
- en: '**Consumer metrics**: Consumer lag, which indicates how far behind a consumer
    group is in processing messages, is critical for ensuring data is processed in
    a timely manner. Additionally, monitoring the number of active consumers can help
    with detecting issues with consumer scalability and performance.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者指标**：消费者延迟，表示消费者组在处理消息方面落后多少，对于确保数据及时处理至关重要。此外，监控活跃消费者的数量有助于检测消费者可扩展性和性能问题。'
- en: '**Producer metrics**: Monitoring the rate of produced messages, along with
    error rates, can highlight issues in data generation or submission to Kafka topics.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者指标**：监控生产消息的速率以及错误率，可以突出显示数据生成或提交到 Kafka 主题中的问题。'
- en: We will use Kafka Manager (now known as **CMAK**, or **Cluster Manager for Apache
    Kafka**) to monitor our Kafka server. Running CMAK in the same Docker Compose
    file as your Kafka and ZooKeeper setup is convenient for managing and monitoring
    your Kafka cluster locally.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Kafka Manager（现称为 **CMAK**，或 **Apache Kafka 集群管理器**）来监控我们的 Kafka 服务器。在包含
    Kafka 和 ZooKeeper 设置的同一 Docker Compose 文件中运行 CMAK，便于本地管理和监控 Kafka 集群。
- en: Using CMAK to monitor the Kafka server
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CMAK 监控 Kafka 服务器
- en: 'Here’s how you can include CMAK in your Docker Compose setup and get it running
    on your local machine:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您如何在 Docker Compose 设置中包含 CMAK 并在本地机器上运行它的方法：
- en: 'To include CMAK in your existing Docker Compose setup, you’ll need to add a
    new service definition for it. Open your `docker-compose.yml` file and append
    the following service definition:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在现有的 Docker Compose 设置中包含 CMAK，您需要为其添加一个新的服务定义。打开您的 `docker-compose.yml` 文件，并附加以下服务定义：
- en: '[PRE12]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We have simply introduced the `kafka-manager` image in our `docker-compose.yml`
    file—CMAK depends on ZooKeeper and Kafka since it needs to monitor their performance,
    and it will serve on port `9000`.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经在 `docker-compose.yml` 文件中简单地引入了 `kafka-manager` 镜像——CMAK 依赖于 ZooKeeper
    和 Kafka，因为它需要监控它们的性能，并且它将在端口 `9000` 上提供服务。
- en: 'With your `docker-compose.yml` file updated, launch the services by running
    the following command in the terminal, in the directory containing your Docker
    Compose file:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新了 `docker-compose.yml` 文件后，通过在包含 Docker Compose 文件的目录中的终端运行以下命令来启动服务：
- en: '[PRE13]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once all the services are up and running, open a web browser and go to `http://localhost:9000`.
    You should be greeted with the Kafka Manager (CMAK) interface.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有服务都启动并运行，打开网页浏览器并访问 `http://localhost:9000`。你应该会看到 Kafka Manager（CMAK）界面。
- en: To start monitoring your Kafka cluster with Kafka Manager, you’ll need to add
    your cluster to the Kafka Manager UI.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要使用 Kafka Manager 开始监控 Kafka 集群，您需要将您的集群添加到 Kafka Manager UI 中。
- en: Click on the **Add** **Cluster** button.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **添加** **集群** 按钮。
- en: Fill in the cluster information. For `zookeeper:2181` if you’re running everything
    locally, and use the default ZooKeeper setup from your Docker Compose file. Note
    that since Kafka Manager is running in the same Docker network created by Docker
    Compose, it can resolve the ZooKeeper hostname directly.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写集群信息。如果您在本地运行一切，请使用`zookeeper:2181`，并使用Docker Compose文件中的默认ZooKeeper设置。请注意，由于Kafka
    Manager运行在由Docker Compose创建的同一Docker网络中，它可以直接解析ZooKeeper主机名。
- en: In *Figure 8**.4*, we can see how we can fill the form in by using the **Add
    Cluster** screen of Kafka Manager.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在**图8.4**中，我们可以看到如何通过使用Kafka Manager的**添加集群**界面来填写表格。
- en: '![Figure 8.4: Screenshot of the Add Cluster screen in the Kafka Manager application](img/B18400_08_04.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4：Kafka Manager应用程序中添加集群屏幕的截图](img/B18400_08_04.jpg)'
- en: 'Figure 8.4: Screenshot of the Add Cluster screen in the Kafka Manager application'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：Kafka Manager应用程序中添加集群屏幕的截图
- en: Save your cluster configuration.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存您的集群配置。
- en: Now that your Kafka cluster is added to Kafka Manager, you can explore various
    metrics and configurations, such as topic creation, topic listing, and consumer
    groups.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的Kafka集群已添加到Kafka Manager中，您可以探索各种指标和配置，例如主题创建、主题列表和消费者组。
- en: '![Figure 8.5: Kafka Manager screen for our topic](img/B18400_08_05.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：我们的主题的Kafka Manager屏幕](img/B18400_08_05.jpg)'
- en: 'Figure 8.5: Kafka Manager screen for our topic'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：我们的主题的Kafka Manager屏幕
- en: In *Figure 8**.5*, you can see a screenshot of the CMAK dashboard, which gives
    information about a specific Kafka topic called `messageTopic`. The dashboard
    provides an overview including details on the topic’s replication factor, the
    number of partitions, and the total sum of partition offsets representing the
    total message count in the topic. Additionally, it offers controls to manage the
    topic, such as options to delete the topic, add partitions, or modify the topic’s
    configuration. The dashboard also presents insights into how partitions are distributed
    across brokers with metrics such as **Preferred Replicas %** and flags any skewed
    or under-replicated partitions, which are crucial for diagnosing and maintaining
    optimal health and balance within the Kafka cluster.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图8.5**中，您可以看到CMAK仪表板的截图，该仪表板提供了关于名为`messageTopic`的特定Kafka主题的信息。仪表板提供了一个概述，包括主题的复制因子、分区数量以及表示主题中总消息数的分区偏移总和的详细信息。此外，它还提供了管理主题的控件，例如删除主题、添加分区或修改主题配置的选项。仪表板还展示了分区如何在代理之间分布的见解，包括**首选副本百分比**等指标，并标记任何倾斜或低副本分区，这对于诊断和维护Kafka集群中的最佳健康和平衡至关重要。
- en: This setup allows you to manage and monitor your Kafka cluster locally with
    ease, providing a powerful interface for handling Kafka configurations and observing
    cluster performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置使您能够轻松地本地管理和监控您的Kafka集群，提供了一个强大的界面来处理Kafka配置和观察集群性能。
- en: Implementing a monitoring strategy that covers these key metrics and leveraging
    tools such as Kafka Manager can help you better understand your Kafka infrastructure.
    This not only aids in proactive maintenance and optimization but also prepares
    you to react swiftly and effectively to any issues that arise.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 实施涵盖这些关键指标的监控策略，并利用Kafka Manager等工具，可以帮助您更好地理解您的Kafka基础设施。这不仅有助于主动维护和优化，还能让您准备好迅速有效地应对任何出现的问题。
- en: In a nutshell, effectively monitoring Kafka is essential for an event-driven
    system. It’s important to keep an eye on key metrics such as broker health, partition
    balance, message flow, and consumer lag. Tools such as CMAK, Prometheus, and Grafana
    not only simplify these tasks but also provide in-depth visibility and analysis
    to turn raw data into actionable insights. By monitoring, potential issues can
    be spotted and addressed before they become major problems, ensuring the smooth
    operation of the Kafka messaging pipeline.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，有效地监控Kafka对于事件驱动系统至关重要。关注关键指标，如代理健康、分区平衡、消息流和消费者延迟非常重要。CMAK、Prometheus和Grafana等工具不仅简化了这些任务，还提供了深入可见性和分析，将原始数据转化为可操作的见解。通过监控，可以在问题成为大问题之前发现并解决，确保Kafka消息管道的平稳运行。
- en: A monitored event-driven system is equipped to handle the complexities of modern
    data streams and workload requirements. It ensures that every part of the system
    functions reliably, maintaining the performance needed for today’s applications.
    Ultimately, the strength of the systems lies in paying attention to operational
    details—where monitoring isn’t just a routine but a vital aspect of system health
    and longevity.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个被监控的事件驱动系统能够处理现代数据流和负载需求的复杂性。它确保系统的每个部分都能可靠地运行，保持今天应用程序所需的性能。最终，系统的强大之处在于关注运营细节——在这里，监控不仅仅是一项常规工作，而是系统健康和长寿的一个关键方面。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As we wrap up this chapter, let’s take a moment to look back on the journey
    we’ve shared. We’ve dived into the world of Kafka and Spring Boot, putting together
    each piece of our event-driven system. Here’s what we accomplished:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的结束，让我们花一点时间回顾一下我们共同走过的旅程。我们深入到Kafka和Spring Boot的世界，组装起我们事件驱动系统的每一块。以下是我们的成就：
- en: '**Setting up Kafka and ZooKeeper**: We set up Kafka and ZooKeeper on our local
    machines using Docker, creating a robust backbone for our messaging system.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置Kafka和ZooKeeper**：我们使用Docker在我们的本地机器上设置了Kafka和ZooKeeper，为我们的消息系统创建了一个强大的骨干。'
- en: '**Building Spring Boot applications**: We built two Spring Boot applications
    from scratch, one as an event producer and the other as a consumer, learning how
    they work together to form a responsive EDA.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建Spring Boot应用程序**：我们从零开始构建了两个Spring Boot应用程序，一个作为事件生产者，另一个作为消费者，学习了它们如何协同工作形成一个响应式的EDA。'
- en: '**Monitoring the Kafka infrastructure**: We learned the importance of monitoring
    our Kafka infrastructure, using tools such as CMAK to keep a watchful eye on the
    health and performance of our system.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控Kafka基础设施**：我们学习了监控Kafka基础设施的重要性，使用CMAK等工具密切关注系统的健康和性能。'
- en: The insights explored in this chapter aren’t just theoretical; they translate
    into abilities that you can promptly utilize in real-world scenarios. These competencies
    are essential for ensuring your systems function and remain resilient, empowering
    them to adapt to the ever-changing data landscape with agility. The capability
    to set up, integrate, and manage systems is indispensable in today’s rapidly evolving
    tech arena.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨的见解不仅仅是理论上的；它们可以迅速转化为你在现实场景中可以立即利用的能力。这些能力对于确保你的系统运行并保持弹性至关重要，使它们能够敏捷地适应不断变化的数据景观。在当今快速发展的技术领域，设置、集成和管理系统的能力是不可或缺的。
- en: By continuing your learning journey with us, you’re not just acquiring tools
    for your skillset; you’re improving your development workflow, making it more
    seamless and effective. You’re also strengthening the durability and manageability
    of your applications, providing an edge in the competitive technology sector.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过继续与我们一同学习，你不仅获得了技能集的工具；你还改进了你的开发工作流程，使其更加流畅和高效。你还在增强应用程序的耐用性和可管理性，为竞争激烈的技术领域提供优势。
- en: As we move forward to the next chapter, we’ll delve into the details of advanced
    Spring Boot features that enhance your development process. You’ll discover the
    art of aspect-oriented programming for organizing code, leverage the Feign client
    for seamless HTTP API integration, and harness the capabilities of Spring Boot’s
    sophisticated auto-configuration features. The next chapter focuses on simplifying
    your tasks as a developer, making them more efficient and productive. Let’s move
    ahead together and expand our knowledge further.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们迈向下一章，我们将深入了解增强开发过程的Spring Boot高级功能。你将发现组织代码的面向方面编程的艺术，利用Feign客户端实现无缝的HTTP
    API集成，并利用Spring Boot复杂的自动配置功能。下一章的重点是简化作为开发者的任务，使它们更加高效和富有成效。让我们携手前进，进一步扩展我们的知识。
