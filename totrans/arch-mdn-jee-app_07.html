<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>As we have seen in the previous chapter, Continuous Delivery pipelines allow developers to ship software with constant velocity and quality. In order to meet this quality, automated software tests are required. Engineers that work on features want to be sure that everything works as expected. This is even more the case when the software project advances, changes, and potentially breaks existing behavior. Developers need to be sure that no unwanted side-effects are introduced.</p>
<p>Ideally, the software tests contained in a build pipeline are sufficient, without further manual verification, to deploy to production.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>The requirements of software tests</li>
<li>Different test levels and scopes</li>
<li>Unit, component, integration, system, and performance tests</li>
<li>How to run test scenarios locally</li>
<li>How to craft maintainable tests</li>
<li>Required test technology</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The necessity of tests</h1>
                </header>
            
            <article>
                
<p>Tests are necessary to be able to rely on that a certain functionality behaves in a certain way later in production. In all kinds of manufacturing businesses, tests are a natural part of the process. A car has countless parts that need to be tested independently as well as interdependently. Nobody wants to drive a car which has its first test run on a real street with the actual customer.</p>
<p>Tests simulate production behavior and verify components in a safe environment. Manufactured parts that break during test runs are something positive;â€‰they have just pointed out potential errors and nothing more than time and materials is lost. Parts that break in production can cause more harm.</p>
<p>The same is true for software tests. Test failures are something positive, at worst they used up some time and effort, at best they prevent potential bugs from going to production.</p>
<p>As seen previously, tests need to run with the least required human interaction possible. Humans are good at thinking about reasonable test cases and crafting creative test scenarios. Computers, however, are better at executing them. Verifying complex tests is something computers also do well, after they have been given clear verification instructions. With software growing more complex over time, the effort of manually verifying behavior gets bigger and bigger and more prone to error over time. Computers perform better and more reliably at repetitive tasks.</p>
<p>Reliable automated software tests are a prerequisite of moving fast. Automated tests can be executed many times, verifying the whole application. Builds run many times a day, execute all tests every time - <span>even if only minor changes were introduced -</span> and enable verified versions to go to production. This would not be feasible with tests executed by humans.</p>
<p>Automated tests increase the reliability of and confidence in the Continuous Delivery process. For Continuous Deployment, that is, going directly to production, sufficient, automated test scenarios are absolutely required. When all commits are potential candidates for production deployment, all software behavior must <span>be</span> adequately verified upfront. Without this automated verification, Continuous Deployment wouldn't be possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Requirements of well-crafted tests</h1>
                </header>
            
            <article>
                
<p>Today's software world has agreed that tests are crucial to working software. But what makes a good software test? What software components do we have to test? And, more importantly, how can we develop well-crafted tests?</p>
<p>In general, tests should fulfill the following requirements:</p>
<ul>
<li>Predictability</li>
<li>Isolation</li>
<li>Reliability</li>
<li>Fast execution</li>
<li>Automation</li>
<li>Maintainability</li>
</ul>
<p>The following describes these requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predictability</h1>
                </header>
            
            <article>
                
<p>First of all, software tests have to be stable, predictable, and reproducible. The same project circumstances must predictably produce the same test case outcomes, that is, passing or failing. Tests that sometimes pass and sometimes fail are not helpful at all. They either distract developers by providing false positive results or suppress actual bugs with false negative outcomes.</p>
<p>Circumstances that need to be taken into account are, among others, the current time, time zones, locales, randomly generated data, and concurrent execution of other tests that could interfere. The test scenarios should be predictably and explicitly set up, so that these circumstances have no influence on the outcome. If the tested functionality is in fact influenced by these factors, this is a sign that there is a need for additional test scenarios, considering different configurations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Isolation</h1>
                </header>
            
            <article>
                
<p>The requirement of predictability also relates to isolation. Test cases have to run self-sufficiently, without affecting other tests. Changing and maintaining test cases should also have no impact on other test scenarios.</p>
<p>Besides leveraging predictability and maintainability, isolating tests also has an impact on the reproducibility of errors. Complex test scenarios may contain a lot of concerns and responsibilities that can make it hard to find the root causes of failing tests. Isolated tests with smaller scope, however, limit the possibilities of causes and enable developers to find bugs faster.</p>
<p>The several test scopes an enterprise project usually has, which we will see later in this chapter, also come with several test isolation layers. Tests with a small scope, such as unit tests, run more isolated than, for example, end-to-end tests. It certainly makes sense to write test cases in different scopes, which implies different test isolation layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reliability</h1>
                </header>
            
            <article>
                
<p>Ideally, software tests of a project reliably test all functionality. The mantra should be that software that passes the tests is ready for production usage. This is of course a goal to strive for, for example by continuous improvement.</p>
<p>Using Continuous Delivery and especially Continuous Deployment requires a reliable and sufficient test harness. The software tests are the ultimate quality barrier before production deployment.</p>
<p>Reliable tests that pass should not require any further interaction. Therefore, they should not output verbose logs if the overall execution was successful. While a detailed explanation of what happened during execution is very helpful in failing tests, it becomes distracting in passing runs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fast execution</h1>
                </header>
            
            <article>
                
<p>As said before, tests are required to execute quickly. Fast running tests are a necessity for development pipelines providing fast feedback. Especially with the number of tests growing over time by applying continuous improvement, the only way to keep the pipeline effective is to keep test execution time low.</p>
<p>Typically, test execution spends the most time in starting up the test technology. Integration tests in particular, which use an embedded container, consume a lot of startup time. The time spent performing the actual test is in most cases not such a big issue.</p>
<p>Tests that consume a lot of time contradict the idea of continuous improvement of quality. The more test cases and scenarios that are added to the project, the longer the overall test execution and the slower is the feedback. Especially with the challenges of a fast-moving world, software tests need to perform as fast as possible. The rest of this chapter will show you how we can achieve this goal, particularly in regard to end-to-end test scenarios.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automation</h1>
                </header>
            
            <article>
                
<p>Automation is a prerequisite for fast feedback. Continuous Delivery pipeline steps should run with the least human intervention as possible. The same is true for test scenarios. Executing software tests and validating their results should run completely and reliably without human interaction.</p>
<p>The test cases define the functionality's expected behavior and validate the outcome against it. The test will then reliably pass without additional notice or fail with a detailed explanation. Passing tests should not require any further human interaction.</p>
<p>A scenarios with huge or complex test data in particular represent a certain challenge in automating test cases. In order to deal with this issue, engineers should craft test cases in a maintainable way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maintainability</h1>
                </header>
            
            <article>
                
<p>Developing test cases is one thing. Keeping efficient test cases with good coverage when functionality changes is another thing. The challenge with having poorly-crafted test scenarios is that as soon as production functionality changes, the tests need to change as well, requiring a lot of time and effort.</p>
<p>Crafting test cases requires the same attention and effort as production code. Experience shows that without this effort put in, tests contain a lot of duplication and multiple responsibilities. In the same way as for production code, test code requires refactoring.</p>
<p>It should be possible to change or extend test scenarios without much effort required. In particular the test data that changes needs to be represented effectively.</p>
<p>Maintainable tests are a prerequisite for enterprise projects that have proper test coverage and yet are flexible for changes in their business logic. Being able to adapt in a fast-moving world requires adjustable test scenarios as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What to test</h1>
                </header>
            
            <article>
                
<p>Before we go into the topic of how to craft effective, fast, reliable, automated, and maintainable test cases, let's have a look at what assets to test. There are tests on a code layer as well as end-to-end tests. Code layer tests are based on the project's source code and are usually executed during development and build time, whereas end-to-end tests, of all kinds, operate on a running application.</p>
<p>Depending on the test scopes, which we will get to know in the next section, there are different layers of tests, whether tests operate on classes, multiple components, enterprise applications, or whole environments. In all cases the test object needs to be isolated from external concerns. The nature of tests is that they verify certain behavior under specific conditions. The environment surrounding the test object, such as the test cases as well as used components, have to interact with the test object accordingly. The test case will therefore control the test object. This not only includes tests at code level, but also end-to-end tests with external systems being simulated and mocked away.</p>
<p>Most importantly, software tests should verify business behavior. The specified use cases all have to perform certain logic that has to be tested before production deployment. Software tests should therefore verify that the application fulfills the business requirements. Special and corner cases need to be covered as well as negative tests.</p>
<p>For example, testing authentication functionality not only needs to verify that a user can log in with the correct credentials, but also that they can not log in using wrong the credentials. A corner case of this example would be to verify that the authentication component notifies a user whose password is about to expire as soon as he logs in successfully.</p>
<p>Besides business behavior, technical aspects and cross-cutting components also need to be tested. Accessed databases and external systems and the form of the communication is required to be verified on both ends in order to guarantee a working team. These concerns are best tested in end-to-end tests.</p>
<p>In all cases the test object should not be modified during the test, but work in the way as it will in production. This is crucial for crafting reliable tests that will not alter their behavior later on. For code level tests, this only requires that the contents of all involved components are the same. For end-to-end tests, this includes the whole enterprise application as well as the installation and configuration of the application's runtime.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Definition of test scopes</h1>
                </header>
            
            <article>
                
<p>There are several test scopes and responsibilities to consider. The following will introduce the different scopes the rest of this chapter will cover.</p>
<p>Certain namings, such as <em>integation tests</em>, are used ambiguously in various enterprise projects. This sub-chapter defines consistent test scope names that are used for the rest of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit tests</h1>
                </header>
            
            <article>
                
<p>Unit tests verify the behavior of individual units of an application. A unit test usually represents a single class, in some cases a few interdependent classes.</p>
<p>Unit tests operate on code level. They are usually executed in the IDE during development as well as part of the build process before the application is packaged. Unit tests have the shortest execution time of all test scopes. They only execute limited functionality that can be instantiated easily on code level. Potential dependencies of the units are simulated using mocks or dummy classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Component tests</h1>
                </header>
            
            <article>
                
<p>Component tests verify the behavior of a coherent component. They span more than just an individual unit, but still operate on code level. Component tests aim to integrate several components together, which verify the interdependent behavior without setting up container environments.</p>
<p>The scope of component tests is to provide more integration than unit tests without running the application in potentially slow, simulated environments. Similar to unit tests, they use mocking functionality to delimit and simulate test boundaries. An embedded or remote enterprise container is not required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration tests</h1>
                </header>
            
            <article>
                
<p>There is a lot of disagreement as to what integration tests represent and how they are designed. The aimed integration can happen on various levels.</p>
<p>I will use the term as it is quite widely used in the Java ecosystem and as it is represented in the Maven conventions. Integration tests run on code level, providing integration of several units and components, and usually run some more-or-less complex testing framework. This is the main distinction from component tests here.</p>
<p>Integration tests have a similar scope as component tests also integrate several units; however, the focus is on the integration. This integration is more technology than business related. For example, managed beans can make use of CDI injection to acquire certain dependencies using qualifiers or CDI producers. Developers need to verify whether the CDI <em>plumbing</em> has been done properly, that is, the correct annotations have been used, without necessarily deploying the application to a server.</p>
<p>Testing frameworks start up an embedded runtime that will build up several components and run code level tests against them.</p>
<p>Component tests, however, solely focus on the business logic and are limited to simple dependencies that are easily resolvable without sophisticated containers. In general, component tests are preferable for testing business use cases since they contain less moving parts and will run faster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System tests</h1>
                </header>
            
            <article>
                
<p>The term system tests is sometimes also used ambiguously. In this context, the term covers all test cases that run the application or the system as a whole, verifying use cases in an end-to-end manner. Sometimes the terms acceptance or integration tests are used respectively. However, this book consistently uses the term system tests to refer to end-to-end tests.</p>
<p>System tests are quite important to verify that a deployed application works as expected, including both business logic and technical concerns. Whereas the majority of business logic should already be covered by unit and component tests, system tests verify that the overall behavior, including all external systems, is met. This includes how functionality is integrated and interacts within the system landscape.</p>
<p>For an application to provide value, it is not sufficient to only include business logic, but also how that logic is accessed. This needs to be verified in an end-to-end manner.</p>
<p>Since this book is targeted for backend applications, UI level tests are not considered here; this includes UI end-to-end tests as well as UI reactiveness tests. Developers typically develop UI tests using test technology such as <strong>Arquillian Graphene</strong>. The system test approaches described in this chapter are applicable to UI level tests as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance tests</h1>
                </header>
            
            <article>
                
<p>Performance tests verify the non-functional aspect of how a system performs in terms of responsiveness and correct behavior under certain workloads.</p>
<p>It needs to be ensured that an application can provide business value, not only under laboratory conditions but also in production. In production the load on the system can vary heavily, depending on the nature of the application and its use cases. Applications that are publicly available also run the risk of becoming the target of denial of service attacks.</p>
<p>Performance tests are a helpful tool to detect potential performance issues that are caused by the application. This includes, for example, resource leaks, misconfiguration, deadlock situations, or missing timeouts. Putting the application under simulated workload will bring these issues to light.</p>
<p>However, as we will see in <a href="">Chapter 9</a>, <em>Monitoring, Performance, and Logging</em>, performance tests aren't necessarily helpful to predict production responsiveness or tune an application's performance. They should be used as a barrier against obvious mistakes, providing fast feedback.</p>
<p>For the rest of this book, I will use the term performance tests to describe performance as well as load or stress tests that put the application under performance load.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stress tests</h1>
                </header>
            
            <article>
                
<p>Similar to performance tests, stress tests aim to put the system under a certain stress to verify correct behavior in abnormal situations. Whereas performance tests mainly target the application's performance in terms of responsibility and stability, stress tests can cover all aspects and attempts that try to bring the system down.</p>
<p>This includes invalid calls, neglecting communication contracts, or random, unexpected events from the environment. This list of tests is non-exhaustive here and beyond the scope of this book.</p>
<p>However, to give a few examples, stress test may verify against misuse of HTTP connections, such as SYN flooding, DDoS attacks in general, unexpected shutdowns of infrastructure, or further, so-called fuzz or monkey testing.</p>
<p>Creating a sophisticated test harness containing a lot of stress tests would practically be beyond the scope of most projects. However, for enterprise projects it makes sense to include a few reasonable stress tests that match the used environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing tests</h1>
                </header>
            
            <article>
                
<p>After the motivations, requirements, and different scopes, let's have a closer look at how to craft test cases in Java Enterprise projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit tests</h1>
                </header>
            
            <article>
                
<p>Unit tests verify the behavior of individual units of an application. In a Java EE application, this usually regards single entity, boundary, and control classes.</p>
<p>In order to unit test a single class, no exhaustive test case should be required. Ideally, instantiating the test object and setting up minimum dependencies should be sufficient to be able to invoke and verify its business functionality.</p>
<p>Modern Java EE supports this approach. Java EE components, such as EJBs as well as CDI managed beans are testable in a straightforward way by simply instantiating the classes. As we saw previously, modern enterprise components represent plain Java objects, including annotations, without extending or implementing technically motivated superclasses or interfaces, so-called no-interface views.</p>
<p>This allows tests to instantiate EJB or CDI classes and to wire them up as required. Used delegates such as injected controls that are irrelevant to the test case are mocked away. By doing so, we define the boundaries of the test case, what should be tested, and what is not relevant. Mocked delegates enable to verify the test object interaction.</p>
<p>A mock object simulates behavior of an actual instance of its type. Calling methods on mocks usually only returns dummy or mock values. Test objects are not aware that they communicate with a mock object. The behavior of mocks, as well as the verification of invoked methods, is controlled within the test scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article>
                
<p>Let's start with a <span>unit test of</span> a Java EE core component. The <kbd>CarManufacturer</kbd> boundary executes certain business logic and invokes a <kbd>CarFactory</kbd> delegate control:</p>
<pre style="padding-left: 60px">@Stateless
public class CarManufacturer {
    @Inject
    CarFactory carFactory;

    @PersistenceContext
    EntityManager entityManager;

    public Car manufactureCar(Specification spec) {
        Car car = carFactory.createCar(spec);
        entityManager.merge(car);
        return car;
    }
}</pre>
<p>Since the EJB boundary is a plain Java class, it can be instantiated and set up in a unit test. The most commonly used Java unit test technology is <strong>JUnit</strong> together with <strong>Mockito</strong> for mocking. The following code snippet shows the car manufacturer test case, instantiating the boundary test object and using Mockito to mock away used delegates:</p>
<pre>import org.junit.Before;
import org.junit.Test;
import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.*;

public class CarManufacturerTest {

    <strong>private CarManufacturer testObject;</strong>

    <strong>@Before</strong>
    public void setUp() {
        testObject = new CarManufacturer();
        testObject.carFactory = mock(CarFactory.class);
        testObject.entityManager = mock(EntityManager.class);
    }

    <strong>@Test</strong>
    public void test() {
        Specification spec = ...
        Car car = ...

        when(testObject.entityManager.merge(any())).then(a -&gt; a.getArgument(0));
        when(testObject.carFactory.createCar(any())).thenReturn(car);

        assertThat(<strong>testObject.manufactureCar(spec)</strong>).isEqualTo(car);

        verify(testObject.carFactory).createCar(spec);
        verify(testObject.entityManager).merge(car);
    }
}</pre>
<p>The JUnit framework instantiates the <kbd>CarManufacturerTest</kbd> test class once during the test execution.</p>
<p>The <kbd>@Before</kbd> method, <kbd>setUp()</kbd> here, is executed every time before a <kbd>@Test</kbd> method runs. Similarly, methods annotated with <kbd>@After</kbd> run after every test run. The <kbd>@BeforeClass</kbd> and <kbd>@AfterClass</kbd> methods, however, are only executed once per test class, before and after the execution, respectively.</p>
<p>Mockito methods, such as <kbd>mock()</kbd>, <kbd>when()</kbd>, or <kbd>verify()</kbd> are used to create, set up, and verify mocking behavior, respectively. Mock objects are instructed to behave in a certain way. After the test execution, they can verify whether certain functionality has been called on them.</p>
<p>This is an admittedly easy example, but it contains the essence of unit testing core components. No further custom test runner, neither an embedded container is required to verify the boundary's behavior. As opposed to custom runners, the JUnit framework can run unit tests at a very high rate. Hundreds of examples like these will be executed on modern hardware in no time. The startup time is short and the rest is just Java code execution, with a tiny overhead from the testing framework.</p>
<p>Some readers may have noticed the package-private visibility on the <kbd>CarManufacturer</kbd> class. This is due to providing better testability in order to be able to set the delegate on instantiated classes. Test classes that reside in the same package as the boundary are able to modify its dependencies. However, engineers might argue that this violates the encapsulation of the boundary. Theoretically they're right, but no caller will be able to modify the references once the components run in an enterprise container. The referenced object is not the actual delegate, but a proxy thereof, hence the CDI implementation can prevent misuse. It certainly is possible to inject the mock object using reflection or by using constructor-based injection. However, field-based injection together with directly setting the dependencies in the test cases provides better readability with the same production behavior. As of today, many enterprise projects have agreed upon using field dependency injection with package-private visibility.</p>
<p>Another discussion is whether to use custom JUnit runners such as <kbd>MockitoJUnitRunner</kbd> together with custom mocking annotations or a plain setup approach, as shown previously. The following code snippet shows a more dense example using a custom runner:</p>
<pre>import org.junit.runner.RunWith;
import org.mockito.InjectMocks;
import org.mockito.Mock;
import org.mockito.junit.MockitoJUnitRunner;

<strong>@RunWith(MockitoJUnitRunner.class)</strong>
public class CarManufacturerTest {

    <strong>@InjectMocks</strong>
    private CarManufacturer testObject;

    <strong>@Mock</strong>
    private CarFactory carFactory;

    <strong>@Mock</strong>
    private EntityManager entityManager;

    @Test
    public void test() {
        ...
        when(carFactory.createCar(any())).thenReturn(car);
        ...
        verify(carFactory).createCar(spec);
    }
}</pre>
<p>Using the custom Mockito runner allows developers to configure tests with less code as well as to define injections with private visibility in the service class. Using a plain approach, as shown previously, provides more flexibility for complex mock scenarios. However, which method to use in order to run and define Mockito mocks is indeed a question of taste.</p>
<p>Parameterized tests is an additional JUnit functionality to define test cases that are similar in the scenario, but differ in input and output data. The <kbd>manufactureCar()</kbd> method could be tested with a variety of input data, resulting in a slightly different outcome. Parameterized test cases enable to develop these scenarios more productively. The following code snippet shows an example of such test cases:</p>
<pre>import org.junit.runners.Parameterized;

<strong>@RunWith(Parameterized.class)</strong>
public class CarManufacturerMassTest {

    private CarManufacturer testObject;

    <strong>@Parameterized.Parameter(0)
    public Color chassisColor;</strong>

    <strong>@Parameterized.Parameter(1)
    public EngineType engineType;</strong>

    @Before
    public void setUp() {
        testObject = new CarManufacturer();
        testObject.carFactory = mock(CarFactory.class);
        ...
    }

    @Test
    public void test() {
        // chassisColor &amp; engineType
        ...
    }

    <strong>@Parameterized.Parameters(name = "chassis: {0}, engine type: {1}")</strong>
    public <strong>static Collection&lt;Object[]&gt; testData()</strong> {
        return Arrays.asList(
                new Object[]{Color.RED, EngineType.DIESEL, ...},
                new Object[]{Color.BLACK, EngineType.DIESEL, ...}
        );
    }
}</pre>
<p>Parameterized test classes are instantiated and executed following the data in the <kbd>@Parameters</kbd> test data method. Each element in the returned collection results in a separate test execution. The test class populates its parameter properties and continues the text execution as usual. The test data contains test input parameters as well as expected values.</p>
<p>The benefit of this parameterized approach is that it enables developers to add new test cases by simply adding another line within the <kbd>testData()</kbd> method. The preceding example shows the combination of a parameterized unit test using mocks. That combination is only possible using a plain Mockito approach, as described previously, instead of using <kbd>MockitoJUnitRunner</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technology</h1>
                </header>
            
            <article>
                
<p>These examples use JUnit 4 which, at the time of writing, is the most used unit testing framework version. Mockito is used to mock objects and it provides sufficient flexibility for the majority of use cases. In order to assert conditions, these examples use <strong>AssertJ</strong> as the test matching library. It provides functionality to verify the state of objects using productive method-chaining invocations.</p>
<p>These technologies serve as examples for the required test aspects. The point here, however, is not to dictate certain functionalities rather it is to showcase specific, reasonable choices for these test requirements. Other technology that provides similar functionality and benefits is equally advisable.</p>
<p>A typical example of another widely-used technology is Hamcrest matchers as a test matching library or the less frequently used <strong>TestNG</strong> unit test framework.</p>
<p>By the time you're reading this, JUnit version 5 will have emerged, which provides some additional functionalities, especially regarding to dynamic tests. Dynamic tests have similar motivations as parameterized tests, by programmatically and dynamically defining test cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Component tests</h1>
                </header>
            
            <article>
                
<p>Component tests verify the behavior of a coherent component. They provide more integration than unit tests without running the application in simulated environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>The behavior of coherent functionality represented by several interdependent classes needs to be verified in order to test the integration thereof. Component tests should run as fast as unit tests by still isolating functionality, that is, testing coherent units. Therefore they aim to provide fast feedback by integrating yet more logic than just unit tests. Component tests verify business use cases, from the boundary down to involved controls.</p>
<p>Code level component tests are possible since the vast majority of managed beans use quite straightforward delegates. The injected types are in most cases directly resolvable without interfaces or qualifiers and could practically be instantiated and injected, even without embedded containers. This enables component tests to be implemented using the same testing frameworks as unit tests. Required delegates and mocks are set up as part of the test case. The test scenario we want to show starts from the beginning of a business use case down to injected controls.</p>
<p>The following examples will examine how to implement component tests with some basic code quality practices, that help writing maintainable tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article>
                
<p>Imagine the whole manufacture car use case shown in the previous example in the <em>Unit tests</em> section, needs to be tested. A car is created, using a delegate <kbd>CarFactory</kbd>, and then is persisted into the database. Testing the persistence layer is out of this test scope, therefore the entity manager will be mocked away.</p>
<p>The following code snippet shows the component test to the manufacture car use case:</p>
<pre>public class ManufactureCarTest {

    private CarManufacturer carManufacturer;

    @Before
    public void setUp() {
        carManufacturer = <strong>new CarManufacturer();</strong>
        carManufacturer.carFactory = <strong>new CarFactory();</strong>
        carManufacturer.entityManager = mock(EntityManager.class);
    }

    @Test
    public void test() {
        when(carManufacturer.entityManager.merge(any())).then(a -&gt; a.getArgument(0));

        Specification spec = ...
        Car expected = ...

        assertThat(carManufacturer.manufactureCar(spec)).isEqualTo(expected);
        verify(carManufacturer.entityManager).merge(any(Car.class));
    }
}</pre>
<p>The preceding example is quite similar to the previous ones, with the exception that <kbd>CarFactory</kbd> is instantiated, using the actual business logic. The mocks, which represent the boundaries of the test case, verify correct behavior.</p>
<p>However, while this approach works for straightforward use cases, it is somewhat naive in regard to more sophisticated real-world scenarios. The boundaries of the test case are as seen in the test class, for the <kbd>CarFactory</kbd> delegate to be self-sufficient and not inject further controls. Of course, all interdependent units that are part of a component test can define delegates. Depending on the nature of the test and the use case, these nested delegates also need to be instantiated or mocked away.</p>
<p>This will eventually lead to a lot of effort required in setting up the test case. We could make use of test framework functionality such as Mockito annotations here. Doing so, the test case injects all classes that are involved in the test case. Developers specify which of them will be instantiated or mocked away, respectively. Mockito provides functionality to resolve references, which is sufficient for the majority of use cases.</p>
<p>The following code snippet shows a component test of a similar scenario, this time using a <kbd>CarFactory</kbd> delegate that has an <kbd>AssemblyLine</kbd> and <kbd>Automation</kbd> as nested dependencies. These are mocked away in the test case:</p>
<pre style="padding-left: 30px">@RunWith(MockitoJUnitRunner.class)
public class ManufactureCarTest {

    <strong>@InjectMocks
    private CarManufacturer carManufacturer;</strong>

    <strong>@InjectMocks
    private CarFactory carFactory;</strong>

    @Mock
    private EntityManager entityManager;

    @Mock
    private AssemblyLine assemblyLine;

    @Mock
    private Automation automation;

    @Before
    public void setUp() {
        carManufacturer.carFactory = carFactory;

        // setup required mock behavior such as ...
        when(assemblyLine.assemble()).thenReturn(...);
    }

    @Test
    public void test() {
        Specification spec = ...
        Car expected = ...

        assertThat(carManufacturer.manufactureCar(spec)).isEqualTo(expected);
        verify(carManufacturer.entityManager).merge(any(Car.class));
    }
}</pre>
<p>The <kbd>@InjectMocks</kbd> functionality of Mockito attempts to resolve object references with mock objects injected as <kbd>@Mock</kbd> in the test case. The references are set using reflection. If boundaries or controls define new delegates, they need to be defined at least as a <kbd>@Mock</kbd> object in the test cases to prevent <kbd>NullPointerException</kbd>. However, this approach only partially improves the situation since it leads to a lot of dependencies being defined in the test class.</p>
<p>An enterprise project with a growing number of component tests introduces a lot of verbosity and duplication if it follows only this approach.</p>
<p>To make the test code less verbose and restrict this duplication, we could introduce a test superclass for a specific use case scenario. That superclass would contain all <kbd>@Mock</kbd> and <kbd>@InjectMock</kbd> definitions, setting up required dependencies, delegates, and mocks. However, such test superclasses also contain a lot of implicit logic, which delegates are defined and being used somewhere in the extended test cases. This approach leads to test cases that are tightly coupled to commonly used superclasses, eventually leading to implicitly coupling the test cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Delegating test components</h1>
                </header>
            
            <article>
                
<p>It is more advisable to use delegation rather than extension.</p>
<p>Mocking and verification logic that depends on the used components is delegated to separate test objects. The delegates thus encapsulate and manage this logic individually.</p>
<p>The following code snippet shows the test case using components that define the car manufacture and car factory dependencies:</p>
<pre>public class ManufactureCarTest {

    <strong>private CarManufacturerComponent carManufacturer;
    private CarFactoryComponent carFactory;</strong>

    @Before
    public void setUp() {
        carFactory = <strong>new CarFactoryComponent();</strong>
        carManufacturer = <strong>new CarManufacturerComponent(carFactory);</strong>
    }

    @Test
    public void test() {
        Specification spec = ...
        Car expected = ...

        assertThat(carManufacturer.manufactureCar(spec)).isEqualTo(expected);

        <strong>carManufacturer.verifyManufacture(expected);
        carFactory.verifyCarCreation(spec);</strong>
    }
}</pre>
<p>The <kbd>Component</kbd> test dependencies specify the declared dependencies and mocks including the setup and verification behavior for our test cases. The idea is to define components that are reusable within multiple component tests, wiring up similar logic.</p>
<p>The following code snippet shows the definition of <kbd>CarManufacturerComponent</kbd>:</p>
<pre>public class CarManufacturerComponent <strong>extends CarManufacturer</strong> {

    public CarManufacturerComponent(CarFactoryComponent carFactoryComponent) {
        <strong>entityManager = mock(EntityManager.class);
        carFactory = carFactoryComponent;</strong>
    }

    public void verifyManufacture(Car car) {
        verify(entityManager).merge(car);
    }
}</pre>
<p>The class resides in the same package as the <kbd>CarManufacturer</kbd> class, but under the test sources. It can subclass the boundary to add mocking and verification logic. In this example, it is dependent on the <kbd>CarFactory</kbd> component, that also provides additional test logic:</p>
<pre>public class CarFactoryComponent <strong>extends CarFactory</strong> {

    public CarFactoryComponent() {
        <strong>automation = mock(Automation.class);
        assemblyLine = mock(AssemblyLine.class);</strong>
        when(automation.isAutomated()).thenReturn(true);
    }

    public void verifyCarCreation(Specification spec) {
        verify(assemblyLine).assemble(spec);
        verify(automation).isAutomated();
    }
}</pre>
<p>These components serve as reusable test objects that wire up certain dependencies and configure mocking behavior, accordingly. They can be reused within multiple component tests and enhanced without affecting usages.</p>
<p>These examples aim to give an idea of what is possible in order to write maintainable tests. For components being reused, more refactoring approaches should be considered, for example, using a builder-pattern like configuration to satisfy different situations. The <em>Maintaining test data and scenarios</em> section in this chapter contains more about how to write maintainable test code.</p>
<p>The benefit of component tests is that they run as fast as unit tests and yet verify more complex integration logic. The complex logic is tackled by delegation and encapsulation, increasing maintainability. The code and overhead required to setup is limited.</p>
<p>It makes sense to verify coherent business logic using component tests. Use case invocations are tested on a business level with technical low-level aspects being mocked away.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technology</h1>
                </header>
            
            <article>
                
<p>These examples again demonstrate plain JUnit and Mockito test approaches. With some code quality practices, it's certainly possible to write maintainable, dense test cases with limited configuration overhead.</p>
<p>Component tests implemented as shown previously are a practical approach to wire up components that use straightforward dependency injection. If the production code makes use of CDI producers and qualifiers, the injection logic of the test components will change accordingly.</p>
<p>Component tests aim to verify the business use case behavior of coherent units. They usually don't verify the technical wiring. It's advisable to use integration tests in order to verify whether CDI injection was used correctly, for example, in terms of custom qualifiers, producers, or scopes.</p>
<p>However, there are test technologies that provide dependency injection into test cases. Examples for these are <strong>CDI-Unit</strong> or the more sophisticated <strong>Aquillian Testing Framework</strong>. Test cases using these frameworks run in containers, either embedded or even remotely, and are able to further verify the integration of components.</p>
<p>Sophisticated test frameworks certainly provide test cases that are closer to the enterprise application, but also come with the challenge of slow application startups. The containers are usually executed and configured in every test case, typically taking a few hundred milliseconds or more. This does not sound that much but quickly adds up as more tests arrive.</p>
<p>For component tests that aim to solely verify business behavior, faster, and lightweight approaches like the one presented, are therefore preferable. With their fast nature, component tests as well as unit tests are per default executed during the project build. They should be the default way how to verify application business logic.</p>
<p>The following shows code level integration tests that make use of simulated containers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration tests</h1>
                </header>
            
            <article>
                
<p>Component tests verify coherent business logic in isolated and fast tests. Sophisticated Java EE integration behavior, such as injection, custom qualifiers, CDI events, or scopes are not covered by these tests.</p>
<p>Integration tests aim to verify the technical collaboration of components within the enterprise system. This covers concerns such as configuration of Java EE core components, communication with external systems, or persistence. Are the Java EE components annotated correctly? Does the JSON-B mapping produce the desired JSON format? Is the JPA ORM mapping defined properly?</p>
<p>The idea behind code level integration tests is to provide faster feedback by verifying correct integration without the need to build and deploy the application to a test environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Embedded containers</h1>
                </header>
            
            <article>
                
<p>Since unit test technology is not aware of Java EE specifics, integration tests need more sophisticated test functionality in the form of containers. There are several technologies available that start up an embedded container and make parts of the application available.</p>
<p>An example for this is CDI-Unit. It provides functionality to run test cases in a CDI container, further enabling developers to enhance and modify its configuration. CDI-Unit scans the dependencies of tested objects and configures them accordingly. Required mocks and test specific behavior are defined in a declarative approach. A managed bean such as the car manufacturer boundary is set up within the test case, with all required dependencies and mocks.</p>
<p>This approach detects configuration errors, such as missing CDI annotations. The following code snippet shows a car manufacture test, similar to the component test before, that instantiates the boundary:</p>
<pre>import org.jglue.cdiunit.CdiRunner;

<strong>@RunWith(CdiRunner.class)</strong>
public class ManufactureCarIT {

    <strong>@Inject
    CarManufacturer carManufacturer;</strong>

    @Mock
    EntityManager entityManager;

    @Before
    public void setUp() {
        carManufacturer.entityManager = entityManager;
    }

    @Test
    public void test() {
        Specification spec = ...
        Car expected = ...

        assertThat(carManufacturer.manufactureCar(spec)).isEqualTo(expected);
        verify(entityManager).merge(expected);
    }
}</pre>
<p>The custom JUnit runner detects beans injected into the test case and resolves them accordingly. Since CDI-Unit only supports the CDI standard and not the full Java EE API, the test explicitly mocks and sets the entity manager. All other used controls, such as the car factory, automation, and assembly line, are instantiated and injected, accordingly.</p>
<p>CDI-Unit tests can be enhanced to serve more sophisticated scenarios. It's possible to produce beans that are being used within the test scope.</p>
<p>However, this technology certainly has its limits. CDI-Unit is helpful to quickly verify configuration and collaboration of managed beans.</p>
<p>Another, more sophisticated technology for testing applications is Arquillian. Arquillian bundles integration test cases into deployable archives, manages enterprise containers, either embedded or remotely, and deploys, executes, and verifies the test archives. It makes it possible to enhance test cases with custom test behavior depending on the scenario.</p>
<p>The advantage of Arquillian is that it supports containers with full Java EE support. This enables integration tests to operate in more production-near scenarios.</p>
<p>The following code snippet shows a simple example of deploying the car manufacturer boundary to an embedded enterprise container managed by Arquillian:</p>
<pre>import org.jboss.arquillian.container.test.api.Deployment;
import org.jboss.arquillian.junit.Arquillian;
import org.jboss.shrinkwrap.api.ShrinkWrap;
import org.jboss.shrinkwrap.api.asset.EmptyAsset;
import org.jboss.shrinkwrap.api.spec.WebArchive;

<strong>@RunWith(Arquillian.class)</strong>
public class ManufactureCarIT {

    <strong>@Inject
    CarManufacturer carManufacturer;</strong>

    <strong>@Deployment</strong>
    public static WebArchive createDeployment() {
        return ShrinkWrap.create(WebArchive.class)
                .addClasses(CarManufacturer.class)
                // ... add other required dependencies
                .addAsWebInfResource(EmptyAsset.INSTANCE, "beans.xml");
    }

    @Test
    public void test() {
        Specification spec = ...
        Car expected = ...

        assertThat(carManufacturer.manufactureCar(spec)).isEqualTo(expected);
    }
}</pre>
<p>This test case will create a dynamic web archive that ships the boundary and required delegates and deploys it into an embedded container. The test itself can inject and call methods on the specific components.</p>
<p>The container does not necessarily have to run in an embedded way, it can also be a managed or remote container. Containers that run for longer than just the test execution avoid the container startup time and execute tests much more quickly.</p>
<p>Executing these integration tests will take a comparatively long time, but operate closer to production environments. Misconfigured managed beans will be detected during development before the application is shipped. The flexibility and customization of Arquillian, by including custom bean definitions that reside in the test scope, enables pragmatic test scenarios.</p>
<p>However, this example only slightly touches the functionality of embedded container tests. Test frameworks such as Arquillian can be used for validating the integration of container configuration, communication, persistence, and UI. In the rest of this chapter, we will see some shortcomings of integration tests that operate on simulated or embedded environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Embedded databases</h1>
                </header>
            
            <article>
                
<p>Mapping persistence of domain entities is usually defined using JPA annotations. Validating this mapping before an actual server deployment prevents careless mistakes and saves time.</p>
<p>In order to verify a correct database mapping, a database is required. Besides using deployed environment database instances, embedded databases provide similar verification with fast feedback. Embedded container tests running on frameworks such as Arquillian can be used to access this functionality. However, for basic verification it's not necessary for the application to run inside a container.</p>
<p>JPA ships with the possibility to run standalone, in any Java SE environment. We can make use of this and write test cases that wire up the JPA configuration and connect against an embedded or local database.</p>
<p>Imagine a car part that is manufactured and assembled in the car manufacture. The car part domain entity is mapped with JPA as follows:</p>
<pre>@Entity
@Table(name = "car_parts")
public class CarPart {

    @Id
    @GeneratedValue
    private long id;

    @Basic(optional = false)
    private String order;

    @Enumerated(STRING)
    @Basic(optional = false)
    private PartType type;

    ...
}</pre>
<p>In order to verify correct persistence, a test entity bean should at least be persisted and reloaded from the database. The following code snippet shows an integration test that sets up a standalone JPA persistence:</p>
<pre>import javax.persistence.EntityTransaction;
import javax.persistence.Persistence;

public class CarPartIT {

    <strong>private EntityManager entityManager;
    private EntityTransaction transaction;</strong>

    @Before
    public void setUp() {
        <strong>entityManager = Persistence.createEntityManagerFactory("it").createEntityManager();
        transaction = entityManager.getTransaction();</strong>
    }

    @Test
    public void test() {
        <strong>transaction.begin();</strong>

        CarPart part = new CarPart();
        part.setOrder("123");
        part.setType(PartType.CHASSIS);
        <strong>entityManager.merge(part);</strong>

        <strong>transaction.commit();</strong>
    }
}</pre>
<p>Since the persistence runs standalone, there is no container taking care of handling transactions. The test case does this programmatically, as well as setting up the entity manager, using the persistence unit <kbd>it</kbd>. The persistence unit is configured in test scope <kbd>persistence.xml</kbd>. For this test purpose it's sufficient to configure a resource local transactional unit:</p>
<pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;persistence version="2.2" 
        
        xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/persistence
        http://xmlns.jcp.org/xml/ns/persistence/persistence_2_2.xsd"&gt;

    &lt;persistence-unit name="<strong>it</strong>" transaction-type="RESOURCE_LOCAL"&gt;
        <strong>&lt;class&gt;com.example.cars.entity.CarPart&lt;/class&gt;</strong>

        &lt;exclude-unlisted-classes&gt;true&lt;/exclude-unlisted-classes&gt;
        &lt;properties&gt;
            &lt;property name="javax.persistence.jdbc.url" value="jdbc:derby:./it;create=true"/&gt;
            &lt;property name="javax.persistence.jdbc.driver" value="org.apache.derby.jdbc.EmbeddedDriver"/&gt;
            &lt;property name="javax.persistence.schema-generation.database.action" value="drop-and-create"/&gt;
        &lt;/properties&gt;
    &lt;/persistence-unit&gt;
&lt;/persistence&gt;</pre>
<p>The involved entity classes such as <kbd>CarPart</kbd> have to be specified explicitly, since there is no container that takes care of annotation scanning. The JDBC configuration points to an embedded database, in this case <strong>Apache Derby</strong>.</p>
<p>The enterprise project does not include the Java EE implementation, only the API. Therefore, an JPA implementation, such as <strong>EclipseLink</strong>, is added as a test dependency, together with the Derby database.</p>
<p>This integration test provides faster feedback for configuration mismatches and careless mistakes by validating the persistence mapping locally. For example, the shown test case would fail because the <kbd>order</kbd> property of the <kbd>CarPart</kbd> type isn't able to be mapped, since <kbd>order</kbd> is a reserved SQL keyword. The solution to this is to change the column mapping, for example, by renaming the column with <kbd>@Column(name = "part_order")</kbd>.</p>
<p>This is a typical example of mistakes developers make while configuring the persistence. Preventing these errors, that otherwise won't be detected before deployment time, provides faster feedback and saves time and effort.</p>
<p>Of course, this approach will not find all database related integration mismatches. There is no container being used and persistence errors, for example, related to concurrent transactions, won't be found before fully-fledged system tests are executed. Still, it provides a helpful first verification in the pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running integration tests</h1>
                </header>
            
            <article>
                
<p>Attentive readers may have noticed the naming convention of integration tests ending with <kbd>IT</kbd> for integration test. This naming emerged from a Maven naming convention, excluding test classes, that don't match the <kbd>Test</kbd> naming pattern, <span>in the</span> <em>test</em> <span>phase</span>. Classes ending with <kbd>IT</kbd> will be run by a different life cycle plugin.</p>
<p>This approach supports developers in crafting effective development pipelines, since code level integration tests shouldn't necessarily run in the first build step depending on the time they take. With the example of Maven, the <strong>Failsafe Plugin</strong> runs integration tests, using the command <kbd>mvn failsafe:integration-test failsafe:verify</kbd>, after the project has been built.</p>
<p>The IDE, of course, supports both running <kbd>Test</kbd> named tests as well as other naming conventions.</p>
<p>Gradle doesn't take this naming structure into account. In order to achieve the same goal, Gradle projects would use multiple sets of test sources that are executed separately.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code level integration tests versus system tests</h1>
                </header>
            
            <article>
                
<p>Code level tests, such as unit, component, or integration tests, provide fast feedback during development. They enable developers to verify whether the business logic works as expected for isolated components and the overall configuration is sane.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shortcomings of integration tests</h1>
                </header>
            
            <article>
                
<p>However, in order to verify the application's production behavior, these tests are not sufficient. There will be differences in technology, configuration, or runtime that eventually lead to gaps in the test cases. Examples are enterprise containers with different versions, mismatches in the bean configuration once the whole application is deployed, different database implementations, or differences in JSON serialization.</p>
<p>Ultimately, the application runs in production. It makes a lot of sense to verify the behavior in environments that are equivalent to production.</p>
<p>Certainly, it is advisable to craft several test scopes to both have tests with isolated scope and faster feedback as well as integrational tests. The shortcoming of code level integration tests is that they often take a great amount of time.</p>
<p>In my projects in the past, integration tests running containers such as Arquillian, usually were responsible for the vast majority of the build time, resulting in build with 10 minutes and more. This greatly slows down the Continuous Delivery pipeline, resulting in slow feedback and fewer builds. An attempt to solve this shortcoming is to use remote or managed containers in Arquillian tests. They will run with a longer life cycle than the test run and eliminate the startup times.</p>
<p>Code level integration tests are a helpful way to quickly verify application configuration, what cannot be tested using unit or component tests. They are not ideal for testing business logic.</p>
<p>Integration tests that deploy the whole application on simulated environments, such as embedded containers, provide certain value, but are not sufficient to verify production behavior since they are not equivalent to production. No matter whether on code level or simulated environments, integration tests tend to slow down the overall pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shortcomings of system tests</h1>
                </header>
            
            <article>
                
<p>System tests which test an application that is deployed to a production-like environment in an end-to-end fashion provide the most representative verification. Since they run in a later step in the Continuous Delivery pipeline, they provide slower feedback. Test cases, such as verifying the JSON mappings of an HTTP endpoint, will take longer before they provide feedback to engineers.</p>
<p>Tackling and maintaining complex test scenarios is an aspect <span>that takes quite some time and effort</span>. Enterprise applications require definition and maintenance of test data and configuration, <span>especially when many external systems are involved</span>. For example, an end-to-end test that verifies creating a car in the car manufacture application requires external concerns such as the assembly line to be set up as well as test data. Managing these scenarios involves a certain effort.</p>
<p>End-to-end tests attempt to use external systems and databases similarly to production. This introduces the challenge to handle unavailable or erroneous environments. External systems or databases that are unavailable cause the tests to fail; however, the application is not responsible for this test failure. This scenario violates the requirement of predictability, that tests should not depend on external factors that provide false positives. Therefore, it's advisable that system tests mock away external systems that <span>are not part of the application under test. Doing this enables to construct predictable end-to-end tests. The Sub-chapter</span> <em>System tests</em> <span>covers how to implement this approach.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conclusion</h1>
                </header>
            
            <article>
                
<p>Code level unit and component tests verify isolated, specific business logic. They provide immediate feedback and prevent careless mistakes. Component tests, in particular, cover the integration of business related software units.</p>
<p>The delimitation of component tests is that they run without a simulated container, setting up the test cases in a programmatic fashion. Integration tests rely on inversion of control, similar to application containers that wire up components with less developer effort involved. However, crafting maintainable test cases using a programmatic approach with unit test technology ultimately leads to more effective tests. We will see in the <em>Maintaining test data and scenarios</em> section in this chapter, what methods support us in crafting productive test cases.</p>
<p>Integration tests verify the technical integration as well as configuration of application components. Their feedback is certainly faster than deploying the application as part of the pipeline. However, integration tests do not provide sufficient verification compared to production.</p>
<p>They are a good fit to provide a first basic barrier against common errors and careless mistakes. Since starting up integration tests usually takes quite some time, it makes a lot of sense to run a limited number of them. Ideally test frameworks such as Arquillian deploy to managed or remote containers that keep running beyond a single test case.</p>
<p>System tests verify the application's behavior in the most production-like fashion. They provide the ultimate feedback, whether the whole enterprise application works as expected, including business as well as technical aspects. In order to construct predictable test scenarios, it's important to consider external concerns, such as databases and external systems.</p>
<p>Crafting test cases, especially complex test scenarios, takes a lot of time and effort. The question is where does it make the most sense to spend this effort on? In order to test business logic, and especially coherent components, it's advisable to use component tests. Integration tests don't provide ultimate verification, but still take certain time and effort. It makes sense to use a few of them for fast integration feedback, but not to test business logic. Developers may also find ways to reuse created scenarios in several test scopes, for example both integration tests and system tests.</p>
<p>The overall goal should be to minimize the time and effort spent to craft and maintain test cases, to minimize the overall pipeline execution time and to maximize the application verification coverage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System tests</h1>
                </header>
            
            <article>
                
<p>System tests run against a deployed enterprise application. The application contains the same code, configuration, and runtime as in production. The test cases use external communication, such as HTTP, to initiate use cases. They verify that the overall outcome, such as HTTP responses, database state, or communication with external systems, matches the expectations of the application.</p>
<p>System tests answer the question what to test with: the application that runs in the same way as in production, excluding external concerns, accessed using its regular interfaces. External concerns will be simulated, ensuring predictable tests and enabling verification of the communication. It depends on the scenario whether a used database is seen as part of the application and used similarly, or mocked away as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing test scenarios</h1>
                </header>
            
            <article>
                
<p>System test scenarios can easily get quite complex, involving several concerns and obfuscating the actual use case that is to be tested.</p>
<p>In order to manage the complexity of scenarios it makes sense to first craft the procedure of the test case without writing actual code. Defining the required steps in comments, or even on paper first, provides a good overview of what the test scenario is about. Implementing the actual test case afterwards with regard to reasonable abstraction layers will result in more maintainable test cases, with potentially reusable functionality. We will cover this approach in an example later in this sub-chapter.</p>
<p>It's important to take test data into consideration. The more responsibilities a scenario holds, the more complex it will be to define and maintain the test data. It makes sense to put some effort into test data functionality that is being used commonly in the test cases. Depending on the nature of the application and its domain, it may even make sense to define a specific engineer role for this. Providing reusable functionality that is usable effectively can provide some relief; however, it may still be necessary to at least define and document common test data and scenarios.</p>
<p>Ultimately it doesn't help to ignore the complexity of test data. If the application domain does include sophisticated scenarios, ignoring this situation by leaving out certain test cases or postponing test scenarios until production doesn't improve the application's quality.</p>
<p>In order to craft predictable isolated test cases, the scenario should run as stateless and self-sufficient as possible. Test cases should have a starting point similar to production and not rely on a certain state of the system. They should consider other potential tests and usages running simultaneously.</p>
<p>For example, creating a new car should not make assumptions about the number of existing cars. The test case should not verify that the list of all cars is empty before the creation or that it only contains the created car afterwards. It rather verifies that the created car is part of the list at all.</p>
<p>For the same reason it should be avoided that system tests have an impact on the environment life cycle. In situations that involve external systems, it's necessary to control the behavior of the mocked systems. If possible, these cases should be limited in order to enable to execute other scenarios concurrently.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simulating external concerns</h1>
                </header>
            
            <article>
                
<p>System test scenarios use external systems in the same way as in production. However, similar to mock objects in unit and component tests, system tests simulate and mock away external systems. In this way, potential issues that the application isn't responsible for are eliminated. System tests run in dedicated environments, for example, provided by container orchestration frameworks. The test object is the sole application, deployed, executed and configured in the same way as in production.</p>
<p>Simulated external systems are configured to provide the expected behavior once accessed by the application. Similar to a mock object, they verify correct communication depending on the use case.</p>
<p>For the majority of use cases, used databases would not be mocked away. The test scenario can manage and populate database contents as part of the test life cycle, if required. If the database system contains a lot of concerns external to the application, for example containing a lot of database code or representing a search engine, it may make sense to mock and simulate this behavior.</p>
<p>Container orchestration strongly supports these efforts by abstracting systems as services. Pod images can be replaced by other implementations without affecting the tested application. The mocked services can be accessed and configured from within the running system test, defining behavior and external test data.</p>
<div class="CDPAlignCenter CDPAlign"><img height="280" width="332" src="assets/abb98818-2655-46ba-9880-823c439a14f8.png"/></div>
<p>The dotted line illustrates the control and verification of the mocked system as part of the test scenario. The running application will use the external service as usual, with the difference that this service is in fact, backed by a mock.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing system tests</h1>
                </header>
            
            <article>
                
<p>System tests run as a step within the Continuous Delivery pipeline. They connect against a running application on a test environment, invoke business use cases, and verify the overall outcome.</p>
<p>System test cases usually don't impact the application's life cycle. The application is deployed upfront as part of the CD pipeline. If required, the system tests control the state and behavior of external mocks and contents of databases.</p>
<p>Generally speaking, it makes sense to develop system tests as separate build projects without any code dependency to the project. Since system tests access the application from the outside there should be no implications on how the system is being used. System tests are developed against the application's endpoint contracts. Similarly, the system tests should not use classes or functionality that is part of the application, such as using the application's JSON mapping classes. Defining technology and system access from the outside as separate build projects prevents unwanted side effects caused by existing functionality. The system test project can reside besides the application project in the same repository.</p>
<p>The following example will construct a system test from a top-down approach, defining test scenarios and appropriate abstraction layers.</p>
<p>The business use cases of the car manufacture application are accessed via HTTP. They involve external systems and database accesses. In order to verify the creation of a car, the system test will connect against the running application, as a real-world use case would.</p>
<p>In order to manage the test scenario, the case is crafted using logical steps with comments as placeholders first, and then implemented in several abstraction layers:</p>
<pre>public class CarCreationTest {

    @Test
    public void testCarCreation() {

        // verify car 1234 is not included in list of cars

        // create car
        //   with ID 1234,
        //   diesel engine
        //   and red color

        // verify car 1234 has
        //   diesel engine
        //   and red color

        // verify car 1234 is included in list of cars

        // verify assembly line instruction for car 1234
    }
}</pre>
<p>These comments represent the logical steps that are executed and verified when testing creation of a car. They are related to the business rather than the technical implementation.</p>
<p>We realize these comments in private methods, or better, own delegates. The delegates encapsulate technical concerns, as well as potential life cycle behavior:</p>
<p>We define <kbd>CarManufacturer</kbd> and <kbd>AssemblyLine</kbd> delegates that abstract the access and behavior of the applications and delegates. They are defined as part of the system test and have no relation to or knowledge of the managed beans with the same name in the application code. The system test project code is defined independently. It could also be implemented using a different technology, only depending on the communication interface of the application.</p>
<p>The following code snippet shows the integration of the delegates. The car creation system test only contains business logic relevant to implementation, with the delegates realizing the actual invocations. This leverages readable as well as maintainable test cases. Similar system tests will reuse the delegate functionality:</p>
<pre>import javax.ws.rs.core.GenericType;

public class CarCreationTest {

    <strong>private CarManufacturer carManufacturer;
    private AssemblyLine assemblyLine;</strong>

    @Before
    public void setUp() {
        <strong>carManufacturer = new CarManufacturer();
        assemblyLine = new AssemblyLine();</strong>

        <strong>carManufacturer.verifyRunning();
        assemblyLine.initBehavior();</strong>
    }

    @Test
    public void testCarCreation() {
        String id = "X123A345";
        EngineType engine = EngineType.DIESEL;
        Color color = Color.RED;

        verifyCarNotExistent(id);

        String carId = <strong>carManufacturer.createCar(id, engine, color)</strong>;
        assertThat(carId).isEqualTo(id);

        verifyCar(id, engine, color);

        verifyCarExistent(id);

        <strong>assemblyLine.verifyInstructions(id);</strong>
    }

    private void verifyCarExistent(String id) {
        List&lt;Car&gt; cars = <strong>carManufacturer.getCarList()</strong>;
        if (cars.stream().noneMatch(c -&gt; c.getId().equals(id)))
            fail("Car with ID '" + id + "' not existent");
    }

    private void verifyCarNotExistent(String id) {
        List&lt;Car&gt; cars = <strong>carManufacturer.getCarList()</strong>;
        if (cars.stream().anyMatch(c -&gt; c.getId().equals(id)))
            fail("Car with ID '" + id + "' existed before");
    }

    private void verifyCar(String carId, EngineType engine, Color color) {
        Car car = <strong>carManufacturer.getCar(carId)</strong>;
        assertThat(car.getEngine()).isEqualTo(engine);
        assertThat(car.getColor()).isEqualTo(color);
    }
}</pre>
<p>This serves as a basic example for an application system test. The delegates such as <kbd>CarManufacturer</kbd> handle the lower-level communication and validation:</p>
<pre>public class CarManufacturer {

    private static final int STARTUP_TIMEOUT = 30;
    private static final String CARS_URI = "http://test.car-manufacture.example.com/" +
            "car-manufacture/resources/cars";

    private WebTarget carsTarget;
    private Client client;

    public CarManufacturer() {
        client = ClientBuilder.newClient();
        carsTarget = client.target(URI.create(CARS_URI));
    }

    public <strong>void verifyRunning()</strong> {
        long timeout = System.currentTimeMillis() + STARTUP_TIMEOUT * 1000;

        while (!isSuccessful(carsTarget.request().head())) {
            // waiting until STARTUP_TIMEOUT, then fail
            ...
        }
    }

    private boolean isSuccessful(Response response) {
        return response.getStatusInfo().getFamily() == Response.Status.Family.SUCCESSFUL;
    }

    public <strong>Car getCar(String carId)</strong> {
        Response response = carsTarget.path(carId).request(APPLICATION_JSON_TYPE).get();
        assertStatus(response, Response.Status.OK);
        return response.readEntity(Car.class);
    }

    public <strong>List&lt;Car&gt; getCarList()</strong> {
        Response response = carsTarget.request(APPLICATION_JSON_TYPE).get();
        assertStatus(response, Response.Status.OK);
        return response.readEntity(new GenericType&lt;List&lt;Car&gt;&gt;() {
        });
    }

    public <strong>String createCar(String id, EngineType engine, Color color)</strong> {
        JsonObject json = Json.createObjectBuilder()
                .add("identifier", id)
                .add("engine-type", engine.name())
                .add("color", color.name());

        Response response = carsTarget.request()
                .post(Entity.json(json));

        assertStatus(response, Response.Status.CREATED);

        return extractId(response.getLocation());
    }

    private void assertStatus(Response response, Response.Status expectedStatus) {
        assertThat(response.getStatus()).isEqualTo(expectedStatus.getStatusCode());
    }

    ...
}</pre>
<p>The test delegate is configured against the car manufacture test environment. This configuration could be made configurable, for example, by a Java system property or environment variable in order to make the test reusable against several environments.</p>
<p>If the delegate needs to hook up into the test case life cycle, it can be defined as a JUnit 4 rule or JUnit 5 extension model.</p>
<p>This example connects against a running car manufacture application via HTTP. It can create and read cars, mapping and verifying the responses. The reader may have noted how the delegate encapsulates communication internals, such as HTTP URLs, status codes, or JSON mapping. Its public interface only comprises classes that are relevant to the business domain of the test scenario, such as <kbd>Car</kbd> or <kbd>EngineType</kbd>. The domain entity types used in system tests don't have to match the ones defined in the application. For reasons of simplicity, system tests can use different, simpler types that are sufficient for the given scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying and controlling external mocks</h1>
                </header>
            
            <article>
                
<p>We just saw how to connect a system test against a running enterprise application. But how can we control and manipulate the external system that is used inside the application's use case?</p>
<p>An external system can be mocked away using mock server technology such as <strong>WireMock</strong>. WireMock runs as a standalone web server, which is configured to answer specific requests accordingly. It acts like a code level test mock object, that stubs and verifies behavior.</p>
<p>The benefit of using container orchestration frameworks for system tests is that services can be easily replaced by mock servers. The external system's infrastructure as code configuration for the system test environment can contain a WireMock Docker image, which is executed instead of the actual system.</p>
<p>The following code snippet shows an example Kubernetes configuration for the assembly line system, using a WireMock Docker image in the running pods:</p>
<pre>---
kind: <strong>Service</strong>
apiVersion: v1
metadata:
  name: <strong>assembly-line</strong>
  namespace: <strong>systemtest</strong>
spec:
  selector:
    app: assembly-line
  ports:
    - port: 8080
---
kind: <strong>Deployment</strong>
apiVersion: apps/v1beta1
metadata:
  name: <strong>assembly-line</strong>
  namespace: <strong>systemtest</strong>
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: assembly-line
    spec:
      containers:
      - name: assembly-line
        image: <strong>docker.example.com/wiremock:2.6</strong>
      restartPolicy: Always
---</pre>
<p>The system test connects against this service, using an administration URL to set up and modify the mock server's behavior.</p>
<p>The following code snippet shows an implementation of the <kbd>AssemblyLine</kbd> test delegate, using the WireMock API to control the service:</p>
<pre>import static com.github.tomakehurst.wiremock.client.ResponseDefinitionBuilder.okForJson;
import static com.github.tomakehurst.wiremock.client.WireMock.*;
import static java.util.Collections.singletonMap;

public class AssemblyLine {

    public void initBehavior() {
        configureFor("http://test.assembly.example.com", 80);
        resetAllRequests();

        stubFor(get(urlPathMatching("/assembly-line/processes/[0-9A-Z]+"))
                .willReturn(okForJson(singletonMap("status", "IN_PROGRESS"))));

        stubFor(post(urlPathMatching("/assembly-line/processes"))
                .willReturn(status(202)));
    }

    public void verifyInstructions(String id) {
        verify(postRequestedFor(urlEqualTo("/assembly-line/processes/" + id))
                .withRequestBody(carProcessBody()));
    }

    ...
}</pre>
<p>The initial behavior instructs the WireMock instance to answer HTTP requests appropriately. The behavior can also be modified during the test case, if more complex processes and conversations have to be represented.</p>
<p>If a more sophisticated test scenario involves asynchronous communication such as long-running processes, the test cases can use polling to wait for verifications.</p>
<p>The defined car manufacturer and assembly line delegates can be reused within multiple test scenarios. Some cases might require to run system tests mutually exclusively.</p>
<p>In the <em>Maintaining test data and scenarios</em> section, we will see what further methods and approaches support developers in writing maintainable test cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance tests</h1>
                </header>
            
            <article>
                
<p>Performance tests verify the non-functional requirement of how a system performs in terms of responsiveness. They don't verify business logic, they verify the application's technology, implementation, and configuration.</p>
<p>In production systems the load on the systems can vary heavily. This is especially true for applications that are publicly available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>Similar to tests that verify business behavior, it can be helpful to test upfront whether an application, or component thereof, is likely to meet their performance expectations in production. The motivation is to prevent major performance drops, potentially caused by introduced errors.</p>
<p>It's important to consider the application logic when constructing performance test scenarios. Some invocations perform more expensive processes than others. Generally it makes sense to construct performance tests after realistic production scenarios, in regard to the frequency and nature of requests.</p>
<p>For example, the ratio of guests browsing an online shop to customers actually performing purchase transactions should somehow reflect the real world.</p>
<p>However, it also makes sense to construct tests that excessively perform expensive invocations, to detect potential issues that emerge when the system is under stress.</p>
<p><span>In</span> <a href="">Chapter 9</a><span>,</span> <em>Monitoring, Performance, and Logging</em><span>, w</span>e will see why performance tests on environments other than production are a poor tool to explore the application's limits and potential bottlenecks. Instead of putting great effort into crafting sophisticated performance test scenarios, it makes more sense to invest into technical insights into production systems.</p>
<p>Still, we will see a few techniques of how to craft simple load tests that put a simulated application under pressure to find evident issues.</p>
<p>A reasonable attempt is to simulate usual load, ramp up the number of concurrent users, and explore at which point the application becomes unresponsive. If the responsiveness breaks sooner than from an earlier test run, this could indicate an issue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Key performance indicators</h1>
                </header>
            
            <article>
                
<p>Key performance indicators give information about the application's responsiveness during normal behavior as well as under simulated workload. There are several indicators that can be gathered that directly affect the user, such as the response time or the error rate. These gauges represent the system's state and will provide insights about its behavior under performance tests. The indicated values will change depending on the number of concurrent users as well as the test scenario.</p>
<p>A first interesting insight is the application's response time - the time it takes to respond to a client's request including all transmissions. It directly affects the quality of the offered service. If the response time falls below a certain threshold, timeouts may occur that cancel and fail the request. The latency time is the time it takes until the server receives the first byte of the request. It mainly depends on the network setup.</p>
<p>During performance tests, it's especially interesting to see how the response time and latency time change compared to their average. When increasing the load on an application, at some point the application will become unresponsive. This unresponsiveness can originate from all kinds of reasons. For example, available connections or threads may be consumed, timeouts can occur, or database optimistic locking may fail. The request error rate represents the ratio of failed requests.</p>
<p>The number of concurrent users or load size in a specific interval of time affects the performance metrics of the application and needs to be considered in the test results. A higher number of users will put the system under more stress, depending <span>on the nature of the request.</span> This number is related to the number of concurrent transactions, technical transactions in this case, that indicate how many transactions the application can handle at a time.</p>
<p>The CPU and memory utilization provide insights about the application's resources. Whereas the values don't necessarily say much about the application's health, they represent the trend of resource consumption during load simulation.</p>
<p>Similarly, the overall throughput indicates the total amount of data that the server transmits to the connected users at any point in time.</p>
<p>The key performance indicators provide insights about the application's responsiveness. They help gather experience and especially trends during development. This experience can be used to verify future application versions. Especially after making changes in technology, implementation, or configuration, performance tests can indicate a potential performance impact.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing performance tests</h1>
                </header>
            
            <article>
                
<p>It makes sense to design performance test scenarios that are close to the real world. Performance test technology should support scenarios that not only ramp up a big amount of users, but simulate user behavior. Typical behavior could be, for example, a user visits the home page, logs in, follows a link to an article, adds the article to their shopping cart, and performs the purchase.</p>
<p>There are several performance test technologies available. At the time of writing, the arguably most used ones are <strong>Gatling</strong> and <strong>Apache JMeter</strong>.</p>
<p>Apache JMeter executes test scenarios that put applications under load and generates reports from the test execution. It uses XML-based configuration, supports multiple or custom communication protocols and can be used to replay recorded load test scenarios. Apache JMeter defines test plans that contain compositions of so-called samplers and logic controllers. They are used to define test scenarios that simulate user behavior. JMeter is distributed, using a master/slave architecture and can be used to generate load from several directions. It ships a graphical UI which is is used to edit the test plan configuration. Command-line tools execute the tests locally or on a Continuous Integration server.</p>
<p>Gatling provides a similar performance test solution, but it defines test scenarios programmatically written in Scala. It therefore provides a lot of flexibility in defining test scenarios, behavior of virtual users, and how the test progresses. Gatling can also record and reuse user behavior. Since the tests are defined programmatically, there are a lot of flexible solutions possible, such as dynamically feeding cases from external sources. The so-called checks and assertions are used to verify whether a single test request or the whole test case was successful.</p>
<p>Unlike JMeter, Gatling runs on a single host, not distributed.</p>
<p>The following code snippet shows the definition of a simple Gatling simulation in Scala:</p>
<pre>import io.gatling.core.Predef._
import io.gatling.core.structure.ScenarioBuilder
import io.gatling.http.Predef._
import io.gatling.http.protocol.HttpProtocolBuilder
import scala.concurrent.duration._

class CarCreationSimulation <strong>extends Simulation</strong> {

  val httpConf: HttpProtocolBuilder = http
    .baseURL("http://test.car-manufacture.example.com/car-manufacture/resources")
    .acceptHeader("*/*")

  val scn: ScenarioBuilder = <strong>scenario("create_car")</strong>
    .exec(http("request_1")
      .get("/cars"))
    .exec(http("request_1")
      .post("/cars")
      .body(StringBody("""{"id": "X123A234", "color": "RED", "engine": "DIESEL"}""")).asJSON
      .check(header("Location").saveAs("locationHeader")))
    .exec(http("request_1")
      .get("${locationHeader}"))

  pause(1 second)

  setUp(
    scn.inject(<strong>rampUsersPerSec(10).to(20).during(10 seconds)</strong>)
  ).protocols(httpConf)
    .constantPauses

}</pre>
<p>The <kbd>create_car</kbd> scenario involves three client requests, which retrieve all cars, create a car, and follow the created resource. The scenarios configure multiple virtual users. The number of users starts at <kbd>10</kbd> and is ramped up to <kbd>20</kbd> users within <kbd>10</kbd> seconds runtime.</p>
<p>The simulation is triggered via the command line and executed against a running environment. Gatling provides test results in HTML files. The following code snippet shows the Gatling HTML output of the test example run:</p>
<div class="CDPAlignCenter CDPAlign"><img height="368" width="458" src="assets/87e5c982-56ad-4d5a-9fcf-65516195e3c2.png"/></div>
<p>This example gives an idea of what is possible with Gatling tests.</p>
<p>Since performance tests should reflect somewhat realistic user scenarios, it makes sense to reuse existing system test scenarios for performance tests. Besides programmatically defining user behavior, pre-recorded test runs can be used to feed in data from external sources such as web server log files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Insights</h1>
                </header>
            
            <article>
                
<p>The point in executing performance tests is less a <em>green</em> or <em>red</em> outcome than the insights gathered from the runs. The test reports and the application's behavior are collected during the test runs. These collections enable us to gain experience and discover trends in performance.</p>
<p>While the performance tests can be executed standalone, they ideally run continuously as part of a Continuous Delivery pipeline. It's already helpful to gain these insights without needing to affect the outcome of the pipeline step. After some metrics have been gathered, engineers can consider defining a performance run as failed if the measured performance indicated a major drop from the usual expectations.</p>
<p>This matches the idea of continuous improvement or in this case avoiding responsiveness deterioration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running tests locally</h1>
                </header>
            
            <article>
                
<p>The previous chapter covered development workflows and Continuous Delivery pipelines. It's crucial for modern enterprise applications to define an effective pipeline. However, while the CI server takes care of all build, test, and deploy steps, software engineers are still required to build and test on their local environments.</p>
<p>Continuous Delivery pipelines with proper tests sufficiently verify that enterprise applications work as expected. However, the shortcoming with only relying on the pipeline is that engineers receive feedback later and only after they have pushed their changes to the central repository. Whereas this is the idea behind Continuous Integration, developers still want certainty in their changes before committing them.</p>
<p>Committing changes that contain careless mistakes disturbs other team members by unnecessarily breaking the build. Errors that are easy to detect can be prevented by verifying the commit locally. This is certainly doable in code level tests, such as unit, component, and integration tests, which run on local environments as well. Performing code level tests before committing prevents the majority of mistakes.</p>
<p>When developing technical or cross-cutting concerns, such as interceptors or JAX-RS JSON mapping, engineers also want feedback before committing the changes to the pipeline. As mentioned before, system tests against actually running applications provide the most realistic verification.</p>
<p>For local environments, developers could write sophisticated integration tests, running on embedded containers, to receive faster feedback. However, as we saw previously, this requires quite some time and effort and still does not reliably cover all situations.</p>
<p>Using container technologies enables engineers to run the same software images on multiple environments, including locally. There are Docker installations available for the major operating systems. Local machines can run Docker containers in the same way as in production, setting up custom configuration or wiring up their own networks, if required.</p>
<p>This enables us to run fully-fledged system tests on local environments as well. Whereas this step doesn't necessarily have to be performed during development, it's helpful for developers that want to verify integrational behavior.</p>
<p>Developers can perform build and test steps locally, similar to the Continuous Delivery pipeline. Running steps via the command line greatly facilitates this approach. Docker <kbd>run</kbd> commands enable us to dynamically configure volumes, networks, or environment variables based on the local host.</p>
<p>In order to automate the process, the separate build, deploy, and test commands are combined into shell scripts.</p>
<p>The following code snippet shows one example of a Bash script that performs several steps. Bash scripts can be run on Windows as well, via Unix-console emulators:</p>
<pre><strong>#!/bin/bash
set -e
cd hello-cloud/

# build
mvn package
docker build -t hello-cloud .

# deploy
docker run -d \
  --name hello-cloud-st \
  -p 8080:8080 \
  -v $(pwd)/config/local/application.properties:/opt/config/application.properties \
  hello-cloud

# system tests
cd ../hello-cloud-st/
mvn test

# stopping environment
docker stop hello-cloud-st</strong></pre>
<p>The <em>hello-cloud</em> application is contained in the <kbd>hello-cloud/</kbd> subfolder and built with Maven and Docker. The Docker <kbd>run</kbd> command configures a custom properties file. This is similar to the orchestration configuration example shown in <a href="a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml">Chapter 5</a>, <em>Container and Cloud Environments with Java EE</em>.</p>
<p>The <kbd>hello-cloud-st/</kbd> directory contains system tests that connect against a running application. In order to direct the system test to the local environment, the <em>hosts</em> configuration of the local machine can be adapted. The Maven test run executes the system tests.</p>
<p>This approach enables developers to verify behavior in fully-fledged system tests that are executed in the Continuous Delivery pipelines as well as locally, if required.</p>
<p>If the system test scenario requires several external systems, they are equally run as Docker containers, similar to the test environment. Applications that run in container orchestration environments use logical service names to resolve external systems. The same is possible for natively running Docker containers, that are part of custom Docker networks. Docker resolves container names in containers running in the same network.</p>
<p>This approach is used to run all kinds of services locally and is especially useful to run mock servers.</p>
<p>The following snippet shows an example of the idea of running a local test environment:</p>
<pre><strong>#!/bin/bash
# previous steps omitted

docker run -d \
  --name assembly-line \
  -p 8181:8080 \
  docker.example.com/wiremock:2.6

docker run -d \
  --name car-manufacture-st \
  -p 8080:8080 \
  car-manufacture

# ...</strong></pre>
<p>Similar to the system test example, the WireMock server will be configured as part of the test case. The local environment needs to ensure that hostnames point to the corresponding localhost containers.</p>
<p>For more complex setups, it makes sense to run the services in a container orchestration cluster as well. There are local installation options for Kubernetes or OpenShift available. The container orchestration abstracts cluster nodes. It therefore makes no difference for infrastructure as code definitions, whether a cluster runs locally, as a single node, in a server environment on-premises, or in the cloud.</p>
<p>This enables engineers to use the very same definitions that are used in the test environments. Running a local Kubernetes installation simplifies the shell scripts to a few <kbd>kubectl</kbd> commands.</p>
<p>If installing Kubernetes or OpenShift locally is too oversized, orchestration alternatives such as Docker Compose can be used as lightweight alternatives. Docker Compose also defines multi-container environments and their configuration in infrastructure as code files - executable by a single command. It provides similar benefits as Kubernetes. Arquillian Cube is another sophisticated way of orchestrating and running Docker containers.</p>
<p>Automating steps locally via scripts, highly increases the developer's productivity. Running system tests on local machines benefits engineers by providing faster feedback with less disruption.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maintaining test data and scenarios</h1>
                </header>
            
            <article>
                
<p>Test cases verify that the application will behave as expected when deployed to production. The tests also ensure that the expectations are still met when new features are developed.</p>
<p>However, it's not sufficient to define test scenarios and test data only once. Business logic will evolve and change over time and test cases need to adapt.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importance of maintainable tests</h1>
                </header>
            
            <article>
                
<p>Both for writing and managing test cases, it's crucial to create maintainable test code. Over time the number of test cases will increase. In order to stay productive during development, some time and effort needs to be spent on the test code quality.</p>
<p>For production code, every engineer agrees that code quality is an important requirement. Since t<span>ests are not part of the application that is running in production they are</span> often treated differently. Experience shows that developers rarely invest time and effort in test code quality. <span>However, the quality of test cases has a huge impact on the developer's productivity.</span></p>
<p>There are some signs that indicate poorly written tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Signs of lack of test quality</h1>
                </header>
            
            <article>
                
<p>Generally speaking, development time that is overly spent in test code rather than in production code can be a sign of poorly designed or crafted tests. A feature that is being implemented or changed will cause some tests to fail. How fast can the test code adapt? How many occurrences of test data or functionality are there that need to be changed? How easy is it to add test cases to the existing code base?</p>
<p>Failing tests that are being <kbd>@Ignored</kbd> for more than a very short period of time are also an indicator of a potential flaw in test quality. If the test case is logically still relevant, it needs to be stabilized and fixed. If it becomes obsolete, it should be deleted. However, tests should never be deleted in order to save time and effort that would be necessary to fix them when the test scenarios would logically still be relevant.</p>
<p>Copy and pasting test code should also be an alarming signal. This practice is sadly quite common in enterprise projects, especially when test scenarios slightly differ in their behavior. Copy and pasting violates the <strong>don't repeat yourself</strong> (<strong>DRY</strong>) principle and introduces a lot of duplication which makes future changes expensive.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test code quality</h1>
                </header>
            
            <article>
                
<p>While production code quality is important for keeping a constant development velocity, test code quality is so as well. Tests, however, are mostly not treated in the same way. Experience shows that enterprise projects rarely invest time and effort into refactoring tests.</p>
<p>In general the same practices for high code quality apply for test code as they do for live code. Certain principles are especially important for tests.</p>
<p>First of all, the DRY principle certainly has its importance. On code level this means to avoid repeating definitions, test procedures, and code duplication that contains just minor differences.</p>
<p>For test data, the same principle applies. Experience shows that multiple test case scenarios that use similar test data tempt developers to use copy and pasting. However, doing so will lead to an unmaintainable code base, once changes in the test data have to be made.</p>
<p>The same is true for assertions and mock verifications. Assert statements and verifications that are applied one by one directly in the test method, similarly lead to duplication and challenges with maintenance.</p>
<p>Typically the biggest issue in test code quality is missing abstraction layers. Test cases too often contain different aspects and responsibilities. They mix business with technical concerns.</p>
<p>Let me give an example of a poorly written system test in pseudo code:</p>
<pre>@Test
public void testCarCreation() {
    id = "X123A345"
    engine = EngineType.DIESEL
    color = Color.RED

    // verify car X123A345 not existent
    response = carsTarget.request().get()
    assertThat(response.status).is(OK)
    cars = response.readEntity(List&lt;Car&gt;)
    if (cars.stream().anyMatch(c -&gt; c.getId().equals(id)))
        fail("Car with ID '" + id + "' existed before")

    // create car X123A345
    JsonObject json = Json.createObjectBuilder()
            .add("identifier", id)
            .add("engine-type", engine.name())
            .add("color", color.name())

    response = carsTarget.request().post(Entity.json(json))
    assertThat(response.status).is(CREATED)
    assertThat(response.header(LOCATION)).contains(id)

    // verify car X123A345
    response = carsTarget.path(id).request().get()
    assertThat(response.status).is(OK)
    car = response.readEntity(Car)
    assertThat(car.engine).is(engine)
    assertThat(car.color).is(color)

    // verify car X123A345 existent

    // ... similar invocations as before

    if (cars.stream().noneMatch(c -&gt; c.getId().equals(id)))
        fail("Car with ID '" + id + "' not existent");
}</pre>
<p>Readers might have noticed that it requires quite some effort to comprehend the test case. The inline comments provide some help, but comments like these are in general rather a sign of poorly constructed code.</p>
<p>The example, however, is similar to the system test example crafted previously.</p>
<p>The challenge with test cases like these is not only that they're harder to comprehend. Mixing multiple concerns, both technically and business motivated into a single class, or even a single method, introduces duplication and rules out maintainability. What if the payload of the car manufacture service changes? What if the logical flow of the test case changes? What if new test cases with similar flow but different data need to be written? Do developers copy and paste all code and modify the few aspects then? Or what if the overall communication changes from HTTP to another protocol?</p>
<p>For test cases, the most important code quality principles are to apply proper abstraction layers together with delegation.</p>
<p>Developers need to ask themselves which concerns this test scenario has. There is the test logical flow, verifying the creation of a car with required steps. There is the communication part, involving HTTP invocations and JSON mapping. There might be an external system involved, maybe represented as a mock server that needs to be controlled. And there are assertions and verifications to be performed on these different aspects.</p>
<p>This is the reason why we crafted the previous system test example with several components, all of them concerning different responsibilities. There should be one component for accessing the application under test, including all communication implementation details required. In the previous example, this was the responsibility of the car manufacturer delegate.</p>
<p>Similar to the assembly line delegate, it makes sense to add one component for every mock system involved. These components encapsulate configuration, control, and verification behavior of the mock servers.</p>
<p>Verifications that are made on test business level are advisably outsourced as well, either into private methods or delegates depending on the situation. The test delegates can then again encapsulate logic into more abstraction layers, if required by the technology or the test case.</p>
<p>All of these delegate classes and methods become single points of responsibility. They are reused within all similar test cases. Potential changes only affect the points of responsibility, leaving other parts of the test cases unaffected.</p>
<p>This requires the definition of clear interfaces between the components that don't leak the implementation details. For this reason it makes sense, especially for the system test scope, to have a dedicated, simple model representation. This model can be implemented simply and straightforward with potentially less type safety than the production code.</p>
<p>A reasonable green field approach, similar to the previous system test example, is to start with writing comments and continuously replacing them with delegates while going down the abstraction layers. This starts with what the test logically executes first, implementation details second. Following that approach naturally avoids mixing business and technical test concerns. It also enables simpler integration of test technology that supports writing tests productively, such as <strong>Cucumber-JVM</strong> or <strong>FitNesse</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test technology support</h1>
                </header>
            
            <article>
                
<p>Some test technology also support crafting maintainable tests. AssertJ, for example, provides the possibility to create custom assertions. In our test case the car needs to verify the correct engine and color encapsulated into car specifications. Custom assertions can decrease overall duplication in the test scope.</p>
<p>The following shows a custom <kbd>AssertJ</kbd> assertion for verifying a car:</p>
<pre>import org.assertj.core.api.AbstractAssert;

public class CarAssert <strong>extends AbstractAssert&lt;CarAssert, Car&gt;</strong> {

    public CarAssert(Car actual) {
        super(actual, CarAssert.class);
    }

    public static CarAssert assertThat(Car actual) {
        return new CarAssert(actual);
    }

    public <strong>CarAssert isEnvironmentalFriendly()</strong> {
        isNotNull();

        if (actual.getSpecification().getEngine() != EngineType.ELECTRIC) {
            failWithMessage("Expected car with environmental friendly engine but was &lt;%s&gt;",
                    actual.getEngine());
        }

        return this;
    }

    public <strong>CarAssert satisfies(Specification spec)</strong> {
        ...
    }

    public <strong>CarAssert hasColor(Color color)</strong> {
        isNotNull();

        if (!Objects.equals(actual.getColor(), color)) {
            failWithMessage("Expected car's color to be &lt;%s&gt; but was &lt;%s&gt;",
                    color, actual.getColor());
        }

        return this;
    }

    public <strong>CarAssert hasEngine(EngineType type)</strong> {
        ...
    }
}</pre>
<p>The assertion is then usable within the test scope. The correct static import of the <kbd>CarAssert</kbd> class has to be chosen for the <kbd>assertThat()</kbd> method:</p>
<pre>assertThat(car)
        .hasColor(Color.BLACK)
        .isEnvironmentalFriendly();</pre>
<p>The examples in this chapter showed tests written mainly with Java, JUnit, and Mockito, with the exception of embedded application containers and Gatling. There are dozens of other test technologies that uses different frameworks as well as dynamic JVM languages.</p>
<p>A famous example of this the <strong>Spock Testing Framework</strong> which uses Groovy. The motivation behind this technology was to write leaner, more maintainable tests. Since dynamic JVM languages such as Groovy or Scala are less verbose than plain Java, this idea sounds reasonable.</p>
<p>Test frameworks, such as Spock, indeed result in test cases that require minimal code. They make use of dynamic JVM language features such as less-constraint method names such as <kbd>def "car X123A234 should be created"()</kbd>. Spock testing also provides clear readability with low effort.</p>
<p>However, readable tests are achievable with all test technologies if test code quality is minded. Maintainability, in particular, is rather a question of well-crafted test cases and proper abstraction layers than of the technology being used. Once test cases become quite complex, the impact of the technology on maintainability becomes less relevant.</p>
<p>When choosing test technology, the team's familiarity with the technology should also be considered. At the time of writing, enterprise Java developers are usually less familiar with dynamic JVM languages.</p>
<p>However, the test code quality should be more important than the used technology. Applying good practices of software engineering to tests should be considered as mandatory, using other test frameworks as optional. Refactoring test cases frequently increases the maintainability and reusability of test components and ultimately the quality of the software project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Tests are required to verify software functionality in simulated environments. Software tests should run predictably, isolated, reliably, fast, and in an automated way. In order to enable productive project life cycles, it's important to keep tests maintainable.</p>
<p>Unit tests verify the behavior of individual units of an application, mostly single entity, boundary, or control classes. Component tests verify the behavior of coherent components. Integration tests fulfill the need to verify the interaction of the Java EE components. Database integration tests use embedded databases together with standalone JPA to verify the persistence mapping. System tests verify deployed applications that run on actual environments. Container orchestration heavily supports running system test environments with potential mock applications.</p>
<p>In order to verify functionality before it is pushed to the central repository, engineers need the ability to run tests on their local environment. Changes that contain careless mistakes disturb other teammates by unnecessarily breaking the build. Docker, Docker Compose, and Kubernetes can run on local environments as well, enabling developers to verify behavior upfront. It's advisable to craft simple automation scripts that include the required steps.</p>
<p>In order to achieve a constant development velocity, it's required to develop maintainable test cases. In general, test code should have a similar quality to production code. This includes refactoring, proper abstraction layers, and software quality in general.</p>
<p>These approaches are, in fact, more helpful than introducing sophisticated test frameworks using dynamic JVM languages. Whereas frameworks such as Spock certainly enable readable, minimal test cases, the benefits of using proper practices of software craftsmanship have a more positive impact on the overall test code quality, especially once test scenarios become complex. No matter what testing technology is being used, software engineers are advised to mind the test code quality in order to keep test cases maintainable.</p>
<p>The following chapter will cover the topic of distributed systems and microservices architectures.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>