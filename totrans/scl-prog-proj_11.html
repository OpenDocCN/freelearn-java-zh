<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Batch and Streaming Analytics</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we introduced Spark and obtained BTC/USD transaction data from<span> </span><a href="http://www.bitstamp.net">www.bitstamp.net</a>. Using that data, we can now perform some analysis on it.</p>
<p>First, we are going to query this data using a notebook tool named Apache Zeppelin. After that, we will write a program that receives the live transactions from<span> </span><a href="https://www.bitstamp.net/">https://www.bitstamp.net/</a><span> </span>and sends them to a Kafka topic as they arrive.</p>
<p>Finally, we will use Zeppelin again to run some streaming analytics queries on the data coming to the Kafka topic.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Introduction to Zeppelin</li>
<li class="h1">Analyzing transactions with Zeppelin</li>
<li class="h1">Introducing Apache Kafka</li>
<li class="h1">Streaming transactions to Kafka</li>
<li class="h1">Introducing Spark<span> </span>Streaming</li>
<li class="h1">Analyzing Streaming transactions with Zeppelin</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Zeppelin</h1>
                </header>
            
            <article>
                
<p>Apache Zeppelin is an open source software offering a web interface to create notebooks.<br/>
In a notebook, you can inject some data, execute snippets of code to perform analysis on the data, and then visualize it.</p>
<p class="mce-root">Zeppelin is a collaborative tool; several users can use it simultaneously. You can share notebooks and define the role of each user. You would typically define two different roles:</p>
<ul>
<li class="mce-root">The writer, usually a developer, can edit all the paragraphs and create forms.</li>
<li class="mce-root">The end user does not know much about the technical implementation details. He will just want to change some values in a form and then look at the effect on the results. The results can be a table or a graph, and can be exported to CSV.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Zeppelin</h1>
                </header>
            
            <article>
                
<p>Before installing Zeppelin, you need to have Java and Spark installed on your machine.</p>
<p class="mce-root">Follow these steps to install Zeppelin:</p>
<ol>
<li class="mce-root">Download the binary from: <a href="http://zeppelin.apache.org/download.html">http://zeppelin.apache.org/download.html</a>.</li>
<li class="mce-root"><span>Explode the</span><span> </span><kbd>.tgz</kbd><span> </span><span>file using your favorite program</span></li>
</ol>
<p class="mce-root"><span>That's it. Zeppelin is installed and configured with default settings. The next step is to start it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Starting Zeppelin</h1>
                </header>
            
            <article>
                
<p class="mce-root">To start the Zeppelin daemon process, run the following command in the Terminal:</p>
<ul>
<li class="mce-root">Linux and macOS:</li>
</ul>
<pre class="mce-root" style="padding-left: 60px"><strong> &lt;downloadPath&gt;/zeppelin-0.8.0-bin-all/bin/zeppelin-daemon.sh start</strong></pre>
<ul>
<li class="mce-root">Windows:</li>
</ul>
<pre class="mce-root" style="padding-left: 60px"><strong>&lt;downloadPath&gt;/zeppelin-0.8.0-bin-all/bin/zepplin.cmd start</strong></pre>
<p>Zeppelin is now running and ready to accept requests on<span> </span><a href="http://localhost:8080">http://localhost:8080</a>.</p>
<p>Open the URL in your favorite browser. You should see this page:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/765efbd0-b62f-4e64-9acf-7d7b0ac45600.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing Zeppelin</h1>
                </header>
            
            <article>
                
<p>Let's create a new notebook to test if our installation is working correctly.</p>
<p>From the<span> </span><a href="http://localhost:8080">http://localhost:8080</a><span> </span>home page, click on<span> </span><span class="packt_screen">Create New Note</span><span> </span>and set<span> </span><kbd>Demo</kbd><span> </span>as a name in the popup:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d5686f6-7f3e-4bab-a385-af87777a33c2.png" style="width:32.17em;height:23.67em;"/></p>
<p>As mentioned in the window, the default interpreter will be Spark, and this is what we want. Click on<span> </span><span class="packt_screen">Create</span><span> </span>now.</p>
<p>You just created your first notebook. You should see the following in your browser:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cd21ed57-f511-4700-8c70-aabea16b0e7b.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structure of a notebook</h1>
                </header>
            
            <article>
                
<p class="mce-root">A notebook is a document in which you can add<span> </span><strong>paragraphs</strong>. Each paragraph can use a different<span> </span><strong>interpreter</strong>, which interacts with a specific framework or language.</p>
<p class="mce-root">In each paragraph, there are two sections:</p>
<ul>
<li class="mce-root">The top one is an editor, into which you can type some source code and run it</li>
<li class="mce-root">The bottom one displays the results</li>
</ul>
<p class="mce-root">If, for example, you choose to use a Spark interpreter, all the lines of code you write in the paragraph will be interpreted (like the REPL seen in<span> </span><a href="db4d7854-92ff-43c8-a87b-2d605bf88d1b.xhtml" target="_blank">Chapter 1</a>,<span> </span><em>Writing Your First Program</em>). All variables defined will be kept in memory and shared with all the other paragraphs of the notebook. Similarly to the Scala REPL, when you execute it, the output of the execution along with the types of the variables defined will be printed in the result section.</p>
<p class="mce-root">Zeppelin is installed by default with interpreters like Spark, Python, Cassandra, Angular, HDFS, Groovy, and JDBC.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing a paragraph</h1>
                </header>
            
            <article>
                
<p>Well, our first notebook is completely empty! We can reuse the example used in<span> </span><a href="0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml" target="_blank">Chapter 10</a>, <em>Fetching and Persisting Bitcoin Market Data</em>, in the section<span> </span><em>Exploring Spark's API with the Scala console</em>. If you remember, we created <kbd>Dataset</kbd><span> </span>from a sequence of strings. Enter the following in the notebook:</p>
<pre>val dsString = Seq("1", "2", "3").toDS()<br/>dsString.show()</pre>
<p>Then hit<span> </span><em>Shift</em><span> </span>+<span> </span><em>Enter</em><span> </span>(or click on the play triangle of the UI).</p>
<p>The interpreter runs and you should see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/67ae70c1-f3e9-42f9-9385-087a1b9c22d8.png" style="width:55.33em;height:33.83em;"/></p>
<p>The notebook created the dataset from the sequence and printed in the result section of the paragraph.</p>
<p>Notice that in the previous chapter, when we executed this code from the Scala console, we had to create <kbd>SparkSession</kbd><span> </span>and had to add some imports. When we use the Spark interpreter in Zeppelin, all the<span> </span>implicits<span> </span>and imports are done automatically and <kbd>SparkSession</kbd><span> </span>is created for us.</p>
<p><kbd>SparkSession</kbd><span> </span>is exposed with the variable name<span> </span><kbd>spark</kbd>. You can, for instance, get the Spark version with the following code in a paragraph:</p>
<pre>spark.version</pre>
<p>After having run the paragraph, you should see the version printed in the result section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/573f1cad-9491-45e0-8cee-bd1ae4a8b863.png" style="width:48.75em;height:33.83em;"/></p>
<p>At that point, we tested the installation and it is working properly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawing charts</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to create a basic<span> </span><kbd>Dataset</kbd><span> </span>and draw a chart of its data.</p>
<p>In a new paragraph, add the following:</p>
<pre>case class Demo(id: String, data: Int)<br/>val data = List(<br/>   Demo("a",1),<br/>   Demo("a",2),<br/>   Demo("b",8),<br/>   Demo("c",4))<br/>val dataDS = data.toDS()<br/>dataDS.createOrReplaceTempView("demoView")</pre>
<p>We define a<span> </span><kbd>Demo</kbd><span> </span>class with<span> </span><kbd>id</kbd><span> </span>and a<span> </span><kbd>data</kbd><span> </span>property, then we create a list of different<span> </span><kbd>Demo</kbd><span> </span>objects and convert it into <kbd>Dataset</kbd>.</p>
<p>From the<span> </span><kbd>dataDS</kbd> dataset ,we call the<span> </span><kbd>.createOrReplaceTempView("demoView")</kbd> method. This function is going to register the dataset as a temporary view. With this view defined, we can use SQL to query this dataset. We can try it by adding in a new paragraph, as follows:</p>
<pre>%sql<br/>select * from demoView</pre>
<p>The newly created paragraph starts with<span> </span><kbd>%sql</kbd>. This defines the interpreter that we use. In our case, this is the Spark SQL interpreter.</p>
<p>The query selects all the columns from<span> </span><kbd>demoView</kbd>. After you hit<span> </span><em>Shift</em> +<span> </span><em>Enter</em>, the following table will be shown:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/45161a31-1db6-4730-bd04-f17c97cc92f3.png" style="width:47.42em;height:37.83em;"/></p>
<p>As you can see, the SQL interpreter shows the result of the query by displaying a table in the result section of the paragraph.</p>
<p>Notice the menu at the top of the table. There are multiple ways to represent the data—bar chart, pie chart, area chart, line chart, and scatter chart.</p>
<p>Click on the bar chart. The notebook now looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3cbf9992-902a-46de-841d-d804651de9e8.png" style="width:48.75em;height:21.92em;"/></p>
<p>It might seem odd that we don't have data available. Actually, we need to configure the chart to get it working. Click on<span> </span><span class="packt_screen">settings</span><span>, </span>then define which column is the value and on which one you want to aggregate the data.</p>
<p>Drag the<span> </span><kbd>id</kbd><span> </span>tag to the<span> </span><span class="packt_screen">groups</span><span> </span>box and<span> </span><kbd>data</kbd><span> </span>to the<span> </span><span class="packt_screen">values</span><span> </span>box. As we choose to group the IDs, <kbd>SUM</kbd><span> </span>of<span> </span><kbd>data</kbd><span> </span>is performed. The configuration and the updated chart should look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5bad433c-ef81-4b33-8833-a42750cf47e8.png" style="width:47.67em;height:52.58em;"/></p>
<p>The chart is now correct. The sum of all the<span> </span><kbd>id</kbd><span> </span><kbd>a</kbd><span> </span>is<span> </span><kbd>3</kbd>,<span> </span><kbd>8</kbd><span> </span>for the<span> </span><kbd>b</kbd><span> </span><kbd>id</kbd><span> </span>and<span> </span><kbd>4</kbd><span> </span>for the<span> </span><kbd>c</kbd><span> </span><kbd>id</kbd>.</p>
<p>We are now familiar enough with Zeppelin to perform analytics on the Bitcoin transactions data that we produced in the previous chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing transactions with Zeppelin</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we wrote a program that saves BTC/USD transactions to Parquet files. In this section, we are going to use Zeppelin and Spark to read those files and draw some charts.</p>
<p>If you came directly to this chapter, you first need to set up the<span> </span><kbd>bitcoin-analyser</kbd><span> </span>project, as explained in<span> </span><a href="0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml">Chapter 10</a>,<span> </span><em>Fetching and Persisting Bitcoin Market Data</em>.</p>
<p>Then you can either:</p>
<ul>
<li>
<p>Run<span> </span><kbd>BatchProducerAppIntelliJ</kbd>. This will save the last 24 hours of transactions in the<span> </span><kbd>data</kbd><span> </span>folder of the project directory, then save new transactions every hour.</p>
</li>
<li>
<p>Use the sample transaction data that is committed in GitHub. You will have to check out this project:<span> <a href="https://github.com/PacktPublishing/Scala-Programming-Projects">https://github.com/PacktPublishing/Scala-Programming-Projects</a></span>.</p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawing our first chart</h1>
                </header>
            
            <article>
                
<p>With these Parquet files ready, create a new notebook in Zeppelin and name it<span> </span><kbd>Batch analytics</kbd>. Then, in the first cell, type the following:</p>
<pre>val transactions = spark.read.parquet("&lt;rootProjectPath&gt;/Scala-Programming-Projects/bitcoin-analyser/data/transactions")<br/>z.show(transactions.sort($"timestamp"))</pre>
<p>The first line creates <kbd>DataFrame</kbd><span> </span>from the transactions files. You need to replace the absolute path in the<span> </span><kbd>parquet</kbd><span> </span>function with the path to your Parquet files. The second line uses the special<span> </span><kbd>z</kbd><span> </span>variable to show the content of<span> </span><kbd>DataFrame</kbd><span> </span>in a table. This<span> </span><kbd>z</kbd><span> </span>variable is automatically provided in all notebooks. Its type is<span> </span><kbd>ZeppelinContext</kbd>, and it allows you to interact with the Zeppelin renderer and interpreter.</p>
<p>Execute the cell with<span> </span><em>Shift</em><span> </span>+<span> </span><em>Enter</em>. You should see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/65ccb66a-9c04-49f3-88a0-7a9815fedc81.png"/></p>
<p>There is a warning message saying that the output is truncated. This is because we retrieved too much data. If we then attempt to draw a chart, some data will be missing.</p>
<p>The solution to this issue could be to change Zeppelin's settings and increase the limit. But if we were to do that, the browser would have to keep a lot of data in memory, and the notebook's file when saved to disk would be large as well.</p>
<p>A better solution is to aggregate the data. We cannot show all the transactions that happened in a day on a chart, but if we can aggregate them with a window of<span> </span><kbd>20 minutes</kbd>, that will reduce the number of data points to display. Create a new cell with the following code:</p>
<pre>val <strong>group</strong> = transactions.groupBy(window($"timestamp", "20 minutes"))<br/><br/>val <strong>tmpAgg</strong> = group.agg(<br/>  count("tid").as("count"), <br/>  avg("price").as("avgPrice"),<br/>  stddev("price").as("stddevPrice"),<br/>  last("price").as("lastPrice"),<br/>  sum("amount").as("sumAmount"))<br/><br/>val <strong>aggregate</strong> = tmpAgg.select("window.start", "count", "avgPrice", "lastPrice", "stddevPrice", "sumAmount").sort("start").cache()<br/><br/>z.show(aggregate)</pre>
<p>Run this new cell. You should see the following output:</p>
<pre><strong>group</strong>: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [window: struct&lt;start: timestamp, end: timestamp&gt;], value: [timestamp: timestamp, tid: int ... 4 more fields], type: GroupBy] <br/><br/><strong>tmpAgg</strong>: org.apache.spark.sql.DataFrame = [window: struct&lt;start: timestamp, end: timestamp&gt;, count: bigint ... 4 more fields] <br/><br/><strong>aggregate</strong>: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: timestamp, count: bigint ... 4 more fields]</pre>
<p>Here is a further description for the preceding code:</p>
<p class="mce-root"/>
<ul>
<li>
<p>First, we group our transactions with a window of<span> </span><kbd>20 minutes</kbd>. In the output, you can see that the type of<span> </span><kbd>group</kbd><span> </span>is<span> </span><kbd>RelationalGroupedDataset</kbd>. This is an intermediate type on which we must call an aggregation method to produce another<span> </span><kbd>DataFrame</kbd>.</p>
</li>
<li>
<p>Then we call the<span> </span><kbd>agg</kbd><span> </span>method on<span> </span><kbd>group</kbd><span> </span>to compute several aggregations in one go. The<span> </span><kbd>agg</kbd><span> </span>method takes several<span> </span><kbd>Column</kbd><span> </span>arguments as <kbd>vararg</kbd>. The<span> </span><kbd>Column</kbd><span> </span>objects we pass are obtained by calling various aggregation functions from the <kbd>org.apache.spark.sql.functions</kbd> object. Each column is renamed so that we can refer to it easily later on.</p>
</li>
<li>
<p>The resulting variable<span> </span><kbd>tmpAgg</kbd><span> </span>is <kbd>DataFrame</kbd><span> </span>which has a column<span> </span><kbd>window</kbd><span> </span>of type<span> </span><kbd>struct</kbd>. <kbd>struct</kbd><span> </span>nests several columns. In our case, it has a column<span> </span><kbd>start</kbd><span> </span>and a column<span> </span><kbd>end</kbd><span> </span>of type<span> </span><kbd>timestamp</kbd>.<span> </span><kbd>tmpAgg</kbd><span> </span>also has all the columns containing the aggregations—<kbd>count</kbd>,<span> </span><kbd>avgPrice</kbd>, and <kbd>sumAmount</kbd>.</p>
</li>
<li>
<p>After that, we select only the columns that we are interested in and then assign the resulting<span> </span><kbd>DataFrame</kbd><span> </span>in a variable<span> </span><kbd>aggregate</kbd>. Notice that we can refer to the nested columns of the window column with the<span> </span><kbd>“.”</kbd><span> </span>notation. Here we select all rows apart from<span> </span><kbd>window.end</kbd>. We then<span> </span><kbd>sort</kbd><span> </span>the<span> </span><kbd>DataFrame</kbd><span> </span>by ascending<span> </span><kbd>start</kbd><span> </span>time so that we can use<span> </span><kbd>start</kbd><span> </span>as the<span> </span><em>x</em><span> </span>axis<span> </span>of our future chart. Finally, we<span> </span><kbd>cache</kbd><span> </span>the<span> </span><kbd>DataFrame</kbd><span> </span>so that Spark will not have to reprocess it when we create other cells with different charts.</p>
</li>
</ul>
<p>Underneath the output, Zeppelin displays a table, but this time it does not give us any warning. It is able to load all the data without truncation. We can, therefore, draw a chart:</p>
<ul>
<li>
<p>Click on the line chart button</p>
</li>
<li>
<p><span>Drag and drop the</span><span> </span><kbd>start</kbd><span> </span><span>column in the</span><span> </span><span>section</span></p>
</li>
<li>Drag and drop<span> </span><kbd>avgPrice</kbd><span> </span><span>and</span><span> </span><kbd>lastPrice</kbd><span> </span><span>in the</span><span> </span><span class="packt_screen">values</span><span> </span><span>section</span></li>
</ul>
<p>You should see something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d7f27a93-4652-4491-9064-297ecfcf2506.png" style="width:42.58em;height:42.83em;"/></p>
<p>We can see the evolution of the average price and of the last price in 20-minute increments.</p>
<p>If you hover the mouse on the chart, Zeppelin displays the information of the corresponding data point:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3eb66130-e99b-49a2-8ccd-93e824f6a5b9.png" style="width:27.75em;height:18.08em;"/></p>
<p>Feel free to try different types of charts, with different<span> </span><em>values</em><span> </span>for the<span> </span><em>y </em>axis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawing more charts</h1>
                </header>
            
            <article>
                
<p>Since our<span> </span><kbd>aggregate DataFrame</kbd><span> </span>is available in scope, we can create new cells to plot different charts. Create two new cells with the following code:</p>
<pre>%spark<br/>z.show(aggregate)</pre>
<p>Then, on each new cell, click on the line chart button and drag the<span> </span><kbd>start</kbd><span> </span>column to the<span> </span><span class="packt_screen">keys</span><span> </span>section. After that:</p>
<ol>
<li>
<p>In the first chart, drag and drop<span> </span><kbd>sumAmount</kbd><span> </span>in the<span> </span><span class="packt_screen">values</span><span> </span>section. The chart shows the evolution of the volume exchanged.</p>
</li>
<li>
<p>In the second chart, drag and drop<span> </span><kbd>stddevPrice</kbd><span> </span>in the<span> </span><span class="packt_screen">values</span><span> </span>section. The chart shows the evolution of the standard deviation.</p>
</li>
<li>
<p>Click on<span> </span><span class="packt_screen">settings</span><span> </span>on both cells to hide the settings.</p>
</li>
<li>Click on<span> </span><span class="packt_screen">Hide editor </span>on both cells.</li>
</ol>
<p>You should obtain something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c9121c11-a18f-456a-b412-cde98bbeba92.png"/></p>
<p>We can observe spikes at around the same time—when there is a spike in volume exchanged, the standard deviation spikes as well. This is because many large transactions move the price significantly, which increases the standard deviation.</p>
<p>You now have the basic building blocks to run your own analysis. With the power of the<span> </span><kbd>Dataset</kbd><span> </span>API, you could drill down to a specific period using<span> </span><kbd>filter</kbd> and then plot the evolution of a moving average over different time spans.</p>
<p>The only problem with our notebook is that we would only get new transactions every hour. The<span> </span><kbd>BatchProducerApp</kbd><span> </span>that we wrote in the previous chapter does not produce transactions more frequently, and if you try to call the REST API every few seconds, you will get blacklisted by the Bitstamp server. The preferred way of getting live transactions is to use a WebSocket API.</p>
<p>For solving this, in the next section, we are going to build an application called<span> </span><kbd>StreaminProducerApp</kbd><span> </span>that will push live transactions to a Kafka topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Apache Kafka</h1>
                </header>
            
            <article>
                
<p>In the previous<span> </span>section,<span> </span><em>Introducing Lambda Architecture</em>, we mentioned that Kafka is used for stream processing. Apache Kafka is a high throughput distributed messaging system. It allows decoupling the data coming in with the data going out.</p>
<p>It means that multiple systems (<strong>producers</strong>) can send messages to Kafka. Kafka will then deliver these messages out to the<span> </span><strong>consumers</strong><span> </span>registered.</p>
<p class="mce-root">Kafka is distributed, resilient, and fault-tolerant, and has a very low latency. Kafka can scale horizontally by adding more machines to the system. It is written in Scala and Java.</p>
<p class="mce-root">Kafka is broadly used; Airbnb, Netflix, Uber, and LinkedIn use this technology.</p>
<p class="mce-root">The purpose of this chapter is not to become an expert in Kafka, but rather to familiarize you with the fundamentals of this technology. By the end of the chapter, you will be able to understand the use case developed in this chapter—streaming bitcoin transactions in a Lambda architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Topics, partitions, and offsets</h1>
                </header>
            
            <article>
                
<p>In order to process the messages exchanged between the producers and the consumer, Kafka defines three main components—topics, partitions, and offsets.</p>
<p class="mce-root">A<span> </span><strong>topic</strong><span> </span>groups messages of the same type for streaming. It has a name and a number of partitions. You can have as many topics as you want. As Kafka can be distributed on multiple nodes, it needs a way to materialize the stream of messages on these different nodes. This is why the message stream (topic) is split into multiple<span> </span><strong>partitions</strong>. Each partition contains a portion of the messages sent to a topic.</p>
<p class="mce-root">Each node of the Kafka cluster manages several partitions. A given partition is assigned to several nodes. This avoids losing data if a node is lost, and allows a higher throughput. By default, Kafka uses the hash code of the message to assign it to a partition. You can define a key for a message to control this behavior.</p>
<p class="mce-root">In a partition, the order of the message is guaranteed, and once a message is written on it, it cannot be changed. The messages are<span> </span><strong>immutable</strong>.<br/>
<br/>
A topic can be consumed by zero to many<span> </span><strong>consumer</strong><span> </span>processes. A key feature of Kafka is that each consumer can consume the stream at his own pace: a producer can be sending message 120, while one consumer is processing message 40, and another one is processing message 100.</p>
<p class="mce-root">This asymmetry is made possible by storing the messages on disk. Kafka keeps the messages for a limited amount of time; the default setting is one week.</p>
<p class="mce-root">Internally, Kafka uses IDs to keep track of the messages and a sequence number to generate these IDs. It maintains one unique sequence per partition. This sequence number is called an<span> </span><strong>offset</strong>. An offset only has a meaning for a specific partition.</p>
<p class="mce-root"><br/>
Each node of the Kafka cluster runs a process called a<span> </span><strong>broker. </strong>Each broker manages several topics with one or more partitions.<br/>
<br/>
Let's summarize everything with an example. We can define a topic named<span> </span><kbd>shapes</kbd><span> </span>with a number of partitions equal to two. This topic receives messages, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c0ac8b7b-69e6-47fd-9d1c-62480007107d.png" style="width:38.83em;height:8.17em;"/></p>
<p>Let's say we have three nodes in the cluster. The representation of the brokers, partitions, offsets, and messages would be the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58ec2d48-b82b-4421-9885-563af723c6e5.png" style="width:31.50em;height:39.83em;"/></p>
<p>Notice that as we defined only two partitions and we have three machines, one of the machines is not going to be used.</p>
<p>Another option available when you define a partition is the number of replicas. To be resilient, Kafka replicates the data in multiple brokers, so if one broker is failing, the data can be retrieved from another one.</p>
<p>You should now be more familiar with the fundamentals of the Kafka architecture. We will now spend a little bit of time on two other components: the producer and the consumer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Producing data into Kafka</h1>
                </header>
            
            <article>
                
<p>The component to send messages to a topic is called a<span> </span><strong>producer</strong><span> </span>in Kafka. The responsibility of a producer is to automatically select a partition through a broker to write messages. In case of failure, the producer should automatically recover.</p>
<p>The partition selection is based on a key. The producer will take care of sending all messages with the same key to the same partition. If there is no key provided with the message, the producer load balances the messages using a round-robin algorithm.</p>
<p>You can configure the producer with the level of acknowledgment you want to receive. There are three levels:</p>
<ul>
<li><kbd>acks=0</kbd>: The producer sends the data and forgets it; no acknowledgment is done. There is no guarantee, and messages could be lost.</li>
<li><kbd>acks=1</kbd>: The producer waits for an acknowledgment of the first replicas. You can be sure that no data will be lost as long as the broker that acknowledged does not crash.</li>
<li><kbd>acks=all</kbd>: The producer waits for an acknowledgment of all the replicas. You can be sure that no data will be lost even if one broker crashes.</li>
</ul>
<p>Of course, if you want an acknowledgment of all the replicas, you might expect longer latencies. Acknowledgment of the first replica (<kbd>ack=1</kbd>) is a good compromise between safety and latency.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consuming data from Kafka</h1>
                </header>
            
            <article>
                
<p>To read messages from a topic, you need to run a <strong>consumer</strong>. As with the producer, the consumer will automatically select the broker to read from and will recover in case of failure.</p>
<p>The consumer reads the messages from all partitions. Within a partition, it is guaranteed to receive messages in the same order as they were produced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consumer group</h1>
                </header>
            
            <article>
                
<p>You can have multiple consumers for the same topic in a<span> </span><strong>consumer group</strong>. If you have the same number of consumers and partitions in a group, each consumer will read only one partition. This allows parallelizing the consumption. If you have more consumers than partitions, the first consumer takes the partition and the rest of the consumers are in waiting mode. They will consume only if a consumer reading on a partition is failing.</p>
<p>Until now, we only have one group of consumers reading from partitions. In a typical system, you would have many consumer groups. For example, in the case of a Bitcoin transaction, we could have a consumer group reading the messages to perform analytics on it, and another group for a user interface that shows a feed of all the transactions. The latency between the two cases are not the same, and we don't want to have a dependency between each use case. For that purpose, Kafka uses the notion of groups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Offset management</h1>
                </header>
            
            <article>
                
<p>Another important concept is that when a consumer reads a message, it will automatically inform Kafka of the offset that was read. This way, if a consumer dies, Kafka knows the offset of the last message read. When the consumer restarts, it can send the next message to it. As for the producer, we can decide when to commit offsets. There are three options :</p>
<ul>
<li>At most once; as soon as the message is received, the offset is committed.</li>
<li>At least once; the offset is committed after the message has been processed.</li>
<li>Exactly once; the offset is committed after the message has been processed, and there are additional constraints on the producer—it must not resend messages in case of network failures. The producer must have idempotent and transactional capabilities, which were introduced in Kafka 0.11</li>
</ul>
<p>The most commonly used is the<span> </span><em>at least<span> </span>once</em><span> </span>option. If the processing of the message is failing, you can reprocess it, but you might occasionally receive the same message multiple times. In the case of<span> </span><em>at most once</em>, if anything goes wrong during the process, the message will be lost.</p>
<p>OK, enough theor. We have learned about topics, partitions, offsets, consumers, and producers. The last piece of missing knowledge is a simple question—how do I connect my producer or consumer to Kafka?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connecting to Kafka</h1>
                </header>
            
            <article>
                
<p>We introduced in the section<span> </span><em>Topics, partitions, and offsets</em><span> </span>the notion of brokers. Brokers are deployed on multiple machines, and all the brokers form what we call a Kafka cluster.</p>
<p>If you want to connect to a Kafka cluster, all you need to know is the address of one of the brokers. All the brokers know about all the metadata of the cluster—brokers, partitions, and topics. Internally, Kafka uses a product named<span> </span><strong>Zookeeper</strong>. This allows the sharing of all this metadata between brokers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming transactions to Kafka</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to write a program that produces a stream of the BTC/USD transactions that happen in real time. Our program will:</p>
<ol>
<li>
<p>Subscribe to the WebSocket API of Bitstamp to get a stream of transactions in JSON format.</p>
</li>
<li>
<p>For each transaction coming in the stream, it will:</p>
<ul>
<li>
<p>Deserialize it</p>
</li>
<li>
<p>Convert it to the same<span> </span><kbd>Transaction</kbd><span> </span>case class that we used in<span> </span><kbd>BatchProducer</kbd><span> </span>in the previous chapter</p>
</li>
<li>
<p>Serialize it</p>
</li>
<li>
<p>Send it to a Kafka topic</p>
</li>
</ul>
</li>
</ol>
<p>In the next section, we will use Zeppelin again with Spark Streaming to query the data streamed to the Kafka topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subscribing with Pusher</h1>
                </header>
            
            <article>
                
<p>Go to Bitstamp's WebSocket API for live transactions:<span> </span><a href="https://www.bitstamp.net/websocket/">https://www.bitstamp.net/websocket/</a>.<a href="https://www.bitstamp.net/websocket/"/></p>
<p>You will see that this API uses a tool called Pusher channels for real-time WebSocket streaming. The API documentation provides a Pusher Key that we need to use to receive live transactions.</p>
<p><strong>Pusher</strong><span> </span>channels is a hosted solution for delivering a stream of messages using a publish/subscribe pattern. You can find out more on their website:<span> </span><a href="https://pusher.com/features">https://pusher.com/features</a>.</p>
<p>Let's try to use Pusher to receive some live BTC/USD transactions. Open the project<span> </span><kbd>bitcoin-analyser</kbd>, start a new Scala console, and type the following:</p>
<pre><span>import </span>com.pusher.client.<span>Pusher<br/></span><span>import </span>com.pusher.client.channel.<span>SubscriptionEventListener<br/></span><span><br/></span><span>val </span>pusher = <span>new </span><span>Pusher</span>(<span>"de504dc5763aeef9ff52"</span>)<br/>pusher.<span>connect</span>()<br/><span>val </span>channel = pusher.<span>subscribe</span>(<span>"live_trades"</span>)<br/><br/>channel.<span>bind</span>(<span>"trade"</span>, <span>new </span><span>SubscriptionEventListener</span>() {<br/>  <span>override def </span><span>onEvent</span>(<span>channel</span>: <span>String</span>, <span>event</span>: <span>String</span>, <span>data</span>: <span>String</span>): <br/><span>    Unit </span>= {<br/>      <span>println</span>(<span>s"Received event: </span><span>$</span><span>event</span><span> with data: </span><span>$</span><span>data</span><span>"</span>)<br/>  }<br/>})</pre>
<p>Let's have a look in detail:</p>
<ol>
<li>
<p>In the first line, we create the Pusher client with the key that was specified in Bitstamp's documentation.</p>
</li>
<li>
<p>Then we connect to the remote Pusher server and subscribe to the<span> </span><kbd>"live_trades"</kbd><span> </span>channel. We obtain an object of type<span> </span><kbd>channel</kbd>.</p>
</li>
<li>
<p>Finally, we use the<span> </span><kbd>channel</kbd><span> </span>to register (<kbd>bind</kbd>) a callback function that will be called every time the<span> </span><kbd>channel</kbd><span> </span>receives a new event with the name<span> </span><kbd>trade</kbd>.</p>
</li>
</ol>
<p>After a few seconds, you should see some trades being printed:</p>
<pre>Received event: trade with data: {"amount": 0.001, "buy_order_id": 2165113017, "sell_order_id": 2165112803, "amount_str": "0.00100000", "price_str": "6433.53", "timestamp": "1537390248", "price": 6433.5299999999997, "type": 0, "id": 74263342}<br/>Received event: trade with data: {"amount": 0.0089460000000000008, "buy_order_id": 2165113493, "sell_order_id": 2165113459, "amount_str": "0.00894600", "price_str": "6433.42", "timestamp": "1537390255", "price": 6433.4200000000001, "type": 0, "id": 74263344}<br/>(...)</pre>
<p>The data is in JSON format, with a schema that conforms to what was defined in Bitstamp’s WebSocket documentation.</p>
<p>With these few lines, we can write the first building block of our application. Create a new object, <kbd>StreamingProducerApp</kbd>, in the package<span> </span><kbd>coinyser</kbd><span> </span>in<span> </span><kbd>src/main/scala</kbd><span> </span>with the following content:</p>
<pre><span>package </span>coinyser<br/><br/>import java.sql.Timestamp<br/>import java.text.SimpleDateFormat<br/>import java.util.TimeZone<br/><br/><span>import </span>cats.effect.IO<br/>import com.fasterxml.jackson.databind.ObjectMapper<br/>import com.fasterxml.jackson.module.scala.DefaultScalaModule<br/><span>import </span>com.pusher.client.<span>Client<br/></span><span>import </span>com.pusher.client.channel.<span>SubscriptionEventListener<br/></span><span>import </span>com.typesafe.scalalogging.<span>StrictLogging<br/></span><span><br/></span><span>object </span>StreamingProducer <span>extends </span><span>StrictLogging </span>{<br/><br/>  <span>def </span><span>subscribe</span>(<span>pusher</span>: <span>Client</span>)(<span>onTradeReceived</span>: <span>String </span>=&gt; <span>Unit</span>):   <br/><span>   IO</span>[<span>Unit</span>] =<br/>      <span>for </span>{<br/>        _ &lt;- <span>IO</span>(<span>pusher</span>.<span>connect</span>())<br/>        channel &lt;- <span>IO</span>(<span>pusher</span>.<span>subscribe</span>(<span>"live_trades"</span>))<br/><br/>        _ &lt;- <span>IO</span>(channel.<span>bind</span>(<span>"trade"</span>, <span>new </span><span>SubscriptionEventListener</span>() {<br/>          <span>override def </span><span>onEvent</span>(<span>channel</span>: <span>String</span>, <span>event</span>: <span>String</span>, <span>data</span>: <br/>            <span>String</span>): <span>Unit </span>= {<br/>              <span>logger</span>.<span>info</span>(<span>s"Received event: </span><span>$</span><span>event</span><span> with data: </span><span>$</span><span>data</span><span>"</span>)<br/>                <span>onTradeReceived</span>(<span>data</span>)<br/>           }<br/>         }))<br/>      } <span>yield </span>()<br/>}</pre>
<p>Our function<span> </span><kbd>subscribe</kbd><span> </span>takes a<span> </span><kbd>Pusher</kbd><span> </span>instance (of type<span> </span><kbd>Client</kbd>) and a callback function, <kbd>onTradeReceived</kbd>, and returns <kbd>IO[Unit]</kbd>. When <kbd>IO</kbd><span> </span>is run, it will call<span> </span><kbd>onTradeReceived</kbd><span> </span>each time a new trade is received. The implementation is similar to the few lines that we typed into the console. It basically wraps every side-effecting function in an<span> </span><kbd>IO</kbd>.</p>
<div class="packt_infobox">For the sake of conciseness and readability, we have not exposed the details of this function's unit test. You can check it out in the GitHub repository.<br/>
For writing the test, we had to create a<span> </span><kbd>FakePusher</kbd><span> </span>class that implements a few methods of the<span> </span><kbd>Client</kbd><span> </span>interface.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deserializing live transactions</h1>
                </header>
            
            <article>
                
<p>The JSON payload that we receive when we subscribe to live transactions is slightly different from the one we had in the REST endpoint for fetching batch transactions.</p>
<p>We are going to need to deserialize it to a case class before we can transform it to the same<span> </span><kbd>Transaction</kbd><span> </span>case class that we used in the previous chapter. For this, first create a new case class, <kbd>coinyser.WebsocketTransaction</kbd><span> </span>in<span> </span><kbd>src/main/scala</kbd>:</p>
<pre><span>package </span>coinyser<br/><br/><span>case class </span><span>WebsocketTransaction</span>(<span>amount</span>: <span>Double</span>,<br/>                                <span>buy_order_id</span>: <span>Long</span>,<br/>                                <span>sell_order_id</span>: <span>Long</span>,<br/>                                <span>amount_str</span>: <span>String</span>,<br/>                                <span>price_str</span>: <span>String</span>,<br/>                                <span>timestamp</span>: <span>String</span>,<br/>                                <span>price</span>: <span>Double</span>,<br/>                                <span>`type`</span>: <span>Int</span>,<br/>                                <span>id</span>: <span>Int</span>)</pre>
<p>The names and types of the attributes correspond to the JSON attributes.</p>
<p>After that, we can write a unit test for a new function, <kbd>deserializeWebsocketTransaction</kbd>. Create a new class, <kbd>coinyser.StreamingProducerSpec</kbd>, in<span> </span><kbd>src/test/scala</kbd>:</p>
<pre>package coinyser<br/><br/>import java.sql.Timestamp<br/>import coinyser.StreamingProducerSpec._<br/>import org.scalactic.TypeCheckedTripleEquals<br/>import org.scalatest.{Matchers, WordSpec}<br/><br/><span>class </span><span>StreamingProducerSpec </span><span>extends </span><span>WordSpec </span><span>with </span><span>Matchers </span><span>with </span><span>TypeCheckedTripleEquals </span>{<br/>  <span>"StreamingProducer.deserializeWebsocketTransaction" </span><span>should </span>{<br/>    <span>"deserialize a valid String to a WebsocketTransaction" </span><span>in </span>{<br/>      <span>val </span>str =<br/>        <span>"""{"amount": 0.045318270000000001, "buy_order_id": 1969499130,<br/></span><span>          |"sell_order_id": 1969495276, "amount_str": "0.04531827",<br/></span><span>          |"price_str": "6339.73", "timestamp": "1533797395",<br/></span><span>          |"price": 6339.7299999999996, "type": 0, "id": <br/>          71826763}"""</span>.<span>stripMargin<br/></span><span>      </span>StreamingProducer.<span>deserializeWebsocketTransaction</span>(str) <span>should<br/></span><span>        ===</span>(<span>SampleWebsocketTransaction</span>)<br/>    }<br/>  }<br/>}<br/><br/><span>object </span>StreamingProducerSpec {<br/>  <span>val </span><span>SampleWebsocketTransaction </span>= <span>WebsocketTransaction</span>(<br/>    <span>amount </span>= <span>0.04531827</span>, <span>buy_order_id </span>= <span>1969499130</span>, <span>sell_order_id </span>= <br/>    <span>1969495276</span>, <span>amount_str </span>= <span>"0.04531827"</span>, <span>price_str </span>= <span>"6339.73"</span>,     <br/>    <span>timestamp </span>= <span>"1533797395"</span>, <span>price </span>= <span>6339.73</span>, <span>`type` </span>= <span>0</span>, <span>id </span>=     <br/>    <span>71826763</span>)<br/>}</pre>
<p>The test is straightforward—we define a sample JSON string, call the function under<span> </span><kbd>test</kbd>, and make sure the deserialized object<span> </span><kbd>SampleWebsocketTransaction</kbd><span> </span>contains the same values.</p>
<p>Now we need to implement the function. Add a new<span> </span><kbd>val<span> </span>mapper: ObjectMapper</kbd><span> </span>and a new function<span> </span><kbd>deserializeWebsocketTransaction</kbd><span> </span>to the<span> </span><kbd>StreamingProducer</kbd><span> </span>object:</p>
<pre><span>package </span>coinyser<br/><br/><span>import </span>java.sql.<span>Timestamp<br/></span><span>import </span>java.text.<span>SimpleDateFormat<br/></span><span>import </span>java.util.<span>TimeZone<br/></span><span><br/></span><span>import </span>cats.effect.IO<br/><span>import </span>com.fasterxml.jackson.databind.<span>ObjectMapper<br/></span><span>import </span>com.fasterxml.jackson.module.scala.DefaultScalaModule<br/><span>import </span>com.pusher.client.<span>Client<br/></span><span>import </span>com.pusher.client.channel.<span>SubscriptionEventListener<br/></span><span>import </span>com.typesafe.scalalogging.<span>StrictLogging<br/></span><span><br/></span><span>object </span>StreamingProducer <span>extends </span><span>StrictLogging </span>{<br/>  <br/>  <span>def </span><span>subscribe</span>(<span>pusher</span>: <span>Client</span>)(<span>onTradeReceived</span>: <span>String </span>=&gt; <span>Unit</span>): <br/><span>  IO</span>[<span>Unit</span>] =<br/>    ...<br/><br/>  <span>val </span><span>mapper</span>: <span>ObjectMapper </span>= {<br/>    <span>val </span>m = <span>new </span><span>ObjectMapper</span>()<br/>    m.<span>registerModule</span>(DefaultScalaModule)<br/>    <span>val </span>sdf = <span>new </span><span>SimpleDateFormat</span>(<span>"yyyy-MM-dd HH:mm:ss"</span>)<span><br/></span><span>    </span>sdf.<span>setTimeZone</span>(<span>TimeZone</span>.<span>getTimeZone</span>(<span>"UTC"</span>))<br/>    m.<span>setDateFormat</span>(sdf)<br/>  }<br/><br/>  <span>def </span><span>deserializeWebsocketTransaction</span>(<span>s</span>: <span>String</span>): <span>WebsocketTransaction <br/></span>   = {<br/>        <span>mapper</span>.<span>readValue</span>(<span>s</span>, <span>classOf</span>[<span>WebsocketTransaction</span>])<br/>  }<br/>}</pre>
<p>For this part of the project, we use the<span> </span><strong>Jackson</strong><span> </span>Java library to deserialize/serialize JSON objects. It is the library that is used under the hood by Spark when it reads/writes dataframe from/to JSON. Hence, it is available without adding any more dependencies.</p>
<p>We define a constant, <kbd>mapper: ObjectMapper</kbd>, which is the entry point of Jackson for serializing/deserializing classes. We configure it to write timestamps in a format that is compatible with what Spark can parse. This will be necessary later on when we read the Kafka topic using Spark. Then the function’s implementation calls<span> </span><kbd>readValue</kbd><span> </span>to deserialize the JSON into <kbd>WebsocketTransaction</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting to transaction and serializing</h1>
                </header>
            
            <article>
                
<p>We are able to listen to live transactions and deserialize them to objects of type<span> </span><kbd>WebsocketTransaction</kbd>. The next steps are:</p>
<ul>
<li>
<p>Convert these<span> </span><kbd>WebsocketTransaction</kbd><span> </span>objects into the same case class, <kbd>Transaction</kbd>, that we defined in the previous chapter.</p>
</li>
<li>
<p>Send these<span> </span><kbd>Transaction</kbd><span> </span>objects to a Kafka topic. But for this, they need to be serialized first. The simplest way is to serialize to JSON.</p>
</li>
</ul>
<p>As usual, we start by writing tests. Add the following tests to<span> </span><kbd>StreamingProducerSpec</kbd>:</p>
<pre><span>class </span><span>StreamingProducerSpec </span><span>extends </span><span>WordSpec </span><span>with </span><span>Matchers </span><span>with </span><span>TypeCheckedTripleEquals </span>{<br/><br/>  <span>"StreamingProducer.deserializeWebsocketTransaction" </span><span>should </span>{...}<br/><br/>  <span>"StreamingProducer.convertTransaction" </span><span>should </span>{<br/>    <span>"convert a WebSocketTransaction to a Transaction" </span><span>in </span>{  <br/>      StreamingProducer.<span>convertWsTransaction<br/></span>      (<span>SampleWebsocketTransaction</span>) <span>should<br/></span><span>        ===</span>(<span>SampleTransaction</span>)<br/>    }<br/>  }<br/><br/>  <span>"StreamingProducer.serializeTransaction" </span><span>should </span>{<br/>    <span>"serialize a Transaction to a String" </span><span>in </span>{<br/>      StreamingProducer.<span>serializeTransaction</span>(<span>SampleTransaction</span>) <span>should<br/></span><span>        ===</span>(<span>SampleJsonTransaction</span>)<br/>    }<br/>  }<br/>}<br/><br/><span>object </span>StreamingProducerSpec {<br/>  <span>val </span><span>SampleWebsocketTransaction </span>= <span>WebsocketTransaction</span>(...)<br/><br/>  <span>val </span><span>SampleTransaction </span>= <span>Transaction</span>(<br/>    <span>timestamp </span>= <span>new </span><span>Timestamp</span>(<span>1533797395000L</span>), <span>tid </span>= <span>71826763</span>,<br/>    <span>price </span>= <span>6339.73</span>, <span>sell </span>= <span>false</span>, <span>amount </span>= <span>0.04531827</span>)<br/><br/>  <span>val </span><span>SampleJsonTransaction </span>=<br/>    <span>"""{"timestamp":"2018-08-09 06:49:55",<br/></span><span>      |"date":"2018-08-09","tid":71826763,"price":6339.73,"sell":false,<br/></span><span>      |"amount":0.04531827}"""</span>.<span>stripMargin</span><span><br/></span>}</pre>
<p>The test for<span> </span><kbd>convertWsTransaction</kbd><span> </span>checks that, once converted,<span> </span><kbd>SampleWebsocketTransaction</kbd><span> </span>is the same as<span> </span><kbd>SampleTransaction</kbd>.</p>
<p>The test for<span> </span><kbd>serializeTransaction</kbd><span> </span>checks that, once serialized,<span> </span><kbd>SampleTransaction</kbd><span> </span>is the same as<span> </span><kbd>SampleJsonTransaction</kbd>.</p>
<p>The implementation of these two functions is straightforward. Add the following definitions in<span> </span><kbd>StreamingProducer</kbd>:</p>
<pre><span>object </span>StreamingProducer <span>extends </span><span>StrictLogging </span>{<br/><br/>  <span>def </span><span>subscribe</span>(<span>pusher</span>: <span>Client</span>)(<span>onTradeReceived</span>: <span>String </span>=&gt; <span>Unit</span>): <br/><span>    IO</span>[<span>Unit</span>] = ...<br/><br/>  <span>val </span><span>mapper</span>: <span>ObjectMapper </span>= {...}<br/><br/>  <span>def </span><span>deserializeWebsocketTransaction</span>(<span>s</span>: <span>String</span>): <span>WebsocketTransaction <br/></span>   = {...}<br/><br/>  <span>def </span><strong><span>convertWsTransaction</span></strong>(<span>wsTx</span>: <span>WebsocketTransaction</span>): <span>Transaction </span>=<br/>    <span>Transaction</span>(<br/>      <span>timestamp </span>= <span>new </span><span>Timestamp</span>(<span>wsTx</span>.<span>timestamp</span>.<span>toLong </span>* <span>1000</span>), <span>tid </span>= <br/>        <span>wsTx</span>.<span>id</span>, <span>price </span>= <span>wsTx</span>.<span>price</span>, <span>sell </span>= <span>wsTx</span>.<span>`type` </span>== <span>1</span>, <span>amount </span>= <br/>        <span>wsTx</span>.<span>amount</span>)<br/><br/>  <span>def </span><strong><span>serializeTransaction</span></strong>(<span>tx</span>: <span>Transaction</span>): <span>String </span>= <br/>    <span>mapper</span>.<span>writeValueAsString</span>(<span>tx</span>)<br/>}</pre>
<p>In<span> </span><kbd>convertWsTransaction</kbd>, we have to multiply the timestamp by 1,000 to get the time in milliseconds. The other attributes are just copied.</p>
<p>In<span> </span><kbd>serializeTransaction</kbd>, we reuse the<span> </span><kbd>mapper</kbd><span> </span>object to serialize a<span> </span><kbd>Transaction</kbd><span> </span>object to JSON.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together</h1>
                </header>
            
            <article>
                
<p>We now have all the building blocks to create our application. Create a new object, <kbd>coinyser.StreamingProducerApp</kbd>, and type the following code:</p>
<pre><span>package </span>coinyser<br/><br/><span>import </span>cats.effect.{ExitCode, IO, <span>IOApp</span>}<br/><span>import </span>com.pusher.client.<span>Pusher<br/></span><span>import </span>StreamingProducer._<br/><span>import </span>org.apache.kafka.clients.producer.{<span>KafkaProducer</span>, <span>ProducerRecord</span>}<br/><span>import </span>scala.collection.JavaConversions._<br/><br/><span>object </span>StreamingProducerApp <span>extends </span><span>IOApp </span>{<br/>  <span>val </span><span>topic </span>= <span>"transactions"<br/></span><span><br/></span><span>  </span><span>val </span><span>pusher </span>= <span>new </span><span>Pusher</span>(<span>"de504dc5763aeef9ff52"</span>)<br/><br/>  <span>val </span><span>props </span>= <span>Map</span>(<br/>    <span>"bootstrap.servers" </span><span>-&gt; </span><span>"localhost:9092"</span>,<br/>    <span>"key.serializer" </span><span>-&gt; <br/>    </span><span>"org.apache.kafka.common.serialization.IntegerSerializer"</span>,<br/>    <span>"value.serializer" </span><span>-&gt; <br/>    </span><span>"org.apache.kafka.common.serialization.StringSerializer"</span>)<br/><br/>  <span>def </span><span>run</span>(<span>args</span>: <span>List</span>[<span>String</span>]): <span>IO</span>[<span>ExitCode</span>] = {<br/>    <span>val </span>kafkaProducer = <span>new </span><span>KafkaProducer</span>[<span>Int</span>, <span>String</span>](<span>props</span>)<br/><br/>    <span>subscribe</span>(<span>pusher</span>) { wsTx =&gt;<br/>      <span>val </span>tx = <span>convertWsTransaction</span>(<span>deserializeWebsocket<br/>      Transaction</span>(wsTx))<br/>      <span>val </span>jsonTx = <span>serializeTransaction</span>(tx)<br/>      kafkaProducer.<span>send</span>(<span>new </span><span>ProducerRecord</span>(<span>topic</span>, tx.<span>tid</span>, jsonTx))<br/>    }.<span>flatMap</span>(_ =&gt; IO.<span>never</span>)<br/>  }<br/>}</pre>
<p>Our object extends<span> </span><kbd>cats.IOApp</kbd>, and as such we have to implement a<span> </span><kbd>run</kbd><span> </span>function that returns<span> </span><kbd>IO[ExitCode]</kbd>.</p>
<p>In the<span> </span><kbd>run</kbd><span> </span>function, we first create <kbd>KafkaProducer[Int, String]</kbd>. This Java class from the Kafka client library will allow us to send messages to a topic. The first type parameter is the type of the message's key. The key of our message will be the<span> </span><kbd>tid</kbd><span> </span>attribute in the<span> </span><kbd>Transaction</kbd><span> </span>case class, which is of type<span> </span><kbd>Int</kbd>. The second type parameter is the type of the message itself. In our case, we use<span> </span><kbd>String</kbd>, because we are going to serialize our messages to JSON. If storage space was a concern, we could have used <kbd>Array[Byte]</kbd><span> </span>and a binary serialization format such as Avro.</p>
<p>The<span> </span><kbd>props Map</kbd><span> </span>passed to construct<span> </span><kbd>KafkaProducer</kbd><span> </span>contains various configuration options for interacting with the Kafka cluster. In our program, we pass the minimum set of properties and leave the others with default values, but there are many more fine-tuning options. You can find out more here:<span> </span><a href="http://kafka.apache.org/documentation.html#producerconfigs">http://kafka.apache.org/documentation.html#producerconfigs</a>.</p>
<p>Then we call the<span> </span><kbd>StreamingProducer.subscribe</kbd><span> </span>function that we implemented earlier, and pass a callback function that will be called each time we receive a new transaction. This anonymous function will:</p>
<ol>
<li>
<p>Deserialize the JSON into<span> </span><kbd>WebsocketTransaction</kbd>.</p>
</li>
<li>
<p>Convert <kbd>WebsocketTransaction</kbd><span> </span>into <kbd>Transaction</kbd>.</p>
</li>
<li>
<p>Serialize <kbd>Transaction</kbd><span> </span>to JSON.</p>
</li>
<li>
<p>Send the JSON transaction to the Kafka topic using<span> </span><kbd>kafkaProducer.send</kbd>. For this, we have to create <kbd>ProducerRecord</kbd>, which contains the<span> </span><kbd>topic</kbd><span> </span>name, the key, and the content of the message.</p>
</li>
</ol>
<p>The<span> </span><kbd>subscribe</kbd><span> </span>function returns<span> </span><kbd>IO[Unit]</kbd>. This will start a background thread and complete immediately when we run it. But we do not want to stop the main thread immediately; we need to keep our program running forever. This is why we<span> </span><kbd>flatMap</kbd><span> </span>it and return<span> </span><kbd>IO.never</kbd>, which will keep the main thread running until we kill the process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running StreamingProducerApp</h1>
                </header>
            
            <article>
                
<p>Before running our application, we need to start a Kafka cluster. For the sake of simplicity, we are just going to start a single broker on your workstation. If you wish to set up a multinode cluster, please refer to the Kafka documentation:</p>
<ol>
<li>Download Kafka 0.10.2.2 from this URL:<span> </span><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz">https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz</a>. We use the version 1.1.1 because we have tested it with<span> </span>the<kbd>spark-sql-kafka-0.10</kbd> library at the time of writing. You could use a later version of Kafka, but it is not guaranteed that an old Kafka client can always communicate with a more recent broker.</li>
<li>Open a console and then decompress the package in your favorite directory:<br/>
<kbd>tar<span> </span>xfvz<span> </span>kafka*.tgz</kbd>.</li>
<li>Go to the installation directory and start Zookeeper:</li>
</ol>
<pre style="padding-left: 60px">cd kafka_2.11-1.1.1<br/>bin/zookeeper-server-start.sh config/zookeeper.properties</pre>
<p class="mce-root" style="padding-left: 60px">You should see a lot of log output. The last line should contain<span> </span><kbd>INFO binding to port 0.0.0.0/0.0.0.0:2181</kbd>.</p>
<ol start="4">
<li>Open a new console, and start the Kafka server:</li>
</ol>
<pre style="padding-left: 60px">cd kafka_2.11-1.1.1<br/>bin/kafka-server-start.sh config/server.properties</pre>
<p style="padding-left: 60px">You should see some logs present in the last line.</p>
<ol start="5">
<li>Once Kafka has started, open a new console and run the following commands:</li>
</ol>
<pre style="padding-left: 60px">cd kafka_2.11-1.1.1<br/>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic transactions –from-beginning</pre>
<p style="padding-left: 60px">This command starts a console consumer that listens to the<span> </span><kbd>transactions</kbd><span> </span>topic. Any message sent to this topic will be printed on the console. This will allow us to test that our program works as expected.</p>
<ol start="6">
<li>Now run<span> </span><kbd>StreamingProducerApp</kbd><span> </span>in IntelliJ. After a few seconds, you should get an output similar to this in IntelliJ:</li>
</ol>
<pre style="padding-left: 60px">18/09/22 17:30:41 INFO StreamingProducer$: Received event: trade with data: {"amount": 0.019958119999999999, "buy_order_id": 2180038294, "sell_order_id": 2180031836, "amount_str": "0.01995812", "price_str": "6673.66", "timestamp": "1537633840", "price": 6673.6599999999999, "type": 0, "id": 74611373}</pre>
<p style="padding-left: 60px">Our application received some messages from the WebSocket API and sent them to the Kafka topic transactions.</p>
<ol start="7">
<li>If you then go back to the console on which you started the console consumer, you should see new<span> </span><kbd>Transaction</kbd><span> </span>serialized objects being printed in real time:</li>
</ol>
<pre style="padding-left: 60px">{"timestamp":"2018-09-22 16:30:40","date":"2018-09-22","tid":74611373,"price":6673.66,"sell":false,"amount":0.01995812}</pre>
<p>This indicates that our streaming application works as expected. It listens to the Pusher channel to receive BTC/USD transactions and sends everything it receives to the Kafka topic<span> </span><kbd>transactions</kbd>. Now that we have some data going to a Kafka topic, we can use Spark Streaming to run some analytics queries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Spark Streaming</h1>
                </header>
            
            <article>
                
<p>In<span> </span><a href="0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml"/><a href="0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml" target="_blank">Chapter</a><span> </span><a href="0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml" target="_blank">10</a>,<em> Fetching and Persisting Bitcoin Market Data</em>, we used Spark to save transactions in a batch mode. The batch mode is fine when you have to perform an analysis on a bunch of data all at once.</p>
<p>But in some cases, you might need to process data as it is entering into the system. For example, in a trading system, you might want to analyze all the transactions done by the broker to detect fraudulent transactions. You could perform this analysis in batch mode after the market is closed; but in this case, you can only act after the fact.</p>
<p>Spark Streaming allows you to consume a streaming source (file, socket, and Kafka topic) by dividing the input data into many micro-batches. Each micro-batch is an RDD that can then be processed by the<span> </span><strong>Spark Engine</strong>. Spark divides the input data using a time window. So if you define a time window of 10 seconds, then Spark Streaming will create and process a new RDD every 10 seconds:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f316f1f7-bd8f-4a80-852f-85472c21d2ce.png" style="width:44.33em;height:8.25em;"/></p>
<p>Going back to our fraud detection system, by using Spark Streaming we could detect a pattern of a fraudulent transaction as it arises and immediately act on the broker to limit the damage.</p>
<p>In the previous chapter, you learned that Spark offers two APIs for processing batch data—RDD and<span> </span><kbd>Dataset</kbd>. RDD is the original and core API, and<span> </span><kbd>Dataset</kbd><span> </span>is the more recent one that allows it to perform SQL queries and optimize the execution plan automatically. Similarly, Spark Streaming offers two APIs to process streams of data:</p>
<ul>
<li><strong>Discretized Stream</strong><span> </span>(<strong>DStream</strong>) is basically a continuous series of RDDs. Each RDD in a DStream contains data from a certain time interval. You can get more information about it here:<span> </span><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a>.</li>
<li><strong>Structured streaming</strong><span> </span>is more recent. It allows you to use the same methods as the Dataset API. The difference is that in <kbd>Dataset</kbd><span> </span>you manipulate an unbound table that grows as new input data arrives. You can get more information about it here:<span> </span><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a>.</li>
</ul>
<p>In the next section, we are going to use Zeppelin to run a Spark-structured streaming query on the BTC/USD transaction data that we previously produced in a Kafka topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing streaming transactions with Zeppelin</h1>
                </header>
            
            <article>
                
<p>At this point, if you have not done so yet, you should start the following processes on your machine. Please refer to the previous sections if you are not sure how to start them:</p>
<ul>
<li>Zookeeper</li>
<li>Kafka broker</li>
<li>The<span> </span><kbd>StreamingProducerApp</kbd><span> </span>consuming live BTC/USD transactions and pushing them to a Kafka topic named<span> </span><kbd>transactions</kbd>.</li>
<li>Apache Zeppelin</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading transactions from Kafka</h1>
                </header>
            
            <article>
                
<p>Using your browser, create a new Zeppelin Notebook named<span> </span><kbd>Streaming</kbd>, and  then type the following code in the first cell:</p>
<pre>case class Transaction(timestamp: java.sql.Timestamp,<br/>                       date: String,<br/>                       tid: Int,<br/>                       price: Double,<br/>                       sell: Boolean,<br/>                       amount: Double)<br/>val schema = Seq.empty[Transaction].toDS().schema</pre>
<p>Execute the cell to define the<span> </span><kbd>Transaction</kbd><span> </span>class and the variable<span> </span><kbd>schema</kbd>. We have to redefine the<span> </span><kbd>Transaction</kbd><span> </span>case class in Zeppelin because Zeppelin does not have access to the classes of our IntelliJ project, <kbd>bitcoin-analyser</kbd>. We could have instead packaged a<span> </span><kbd>.jar</kbd><span> </span>file and added it into Zeppelin's dependency settings, but as this is the only class we need, we found it easier to redefine it. The<span> </span><kbd>schema</kbd><span> </span>variable is of type<span> </span><kbd>DataType</kbd>. We will use it in the following paragraph to deserialize the JSON transactions that we will consume from the Kafka topic.</p>
<p>Then create a new cell underneath with the following code:</p>
<pre>val dfStream = {<br/>  spark.readStream.format("kafka")<br/>  .option("kafka.bootstrap.servers", "localhost:9092")<br/>  .option("startingoffsets", "latest")<br/>  .option("subscribe", "transactions")<br/>  .load()<br/>  .select(<br/>    from_json(col("value").cast("string"), schema)<br/>      .alias("v")).select("v.*").as[Transaction]<br/>}</pre>
<p>This creates a new variable<span> </span><kbd>dfStream</kbd><span> </span>of type<span> </span><kbd>DataFrame</kbd>. For creating this, we called:</p>
<ul>
<li>The method<span> </span><kbd>readStream</kbd><span> </span>on the<span> </span><kbd>spark: SparkSession</kbd><span> </span>object. It returns an object of the <kbd>DataStreamReader</kbd> type that we can configure further.</li>
<li>The methods<span> </span><kbd>format("kafka")</kbd><span> </span>and<span> </span><kbd>option("kafka.bootstrap.servers", "localhost:9092")</kbd><span> </span>specify that we want to read data from Kafka and point to the broker localhost on port<span> </span><kbd>9092</kbd>.</li>
<li><kbd>option("startingoffsets", "latest")</kbd><span> </span>indicates that we only want to consume the data from the latest offset. Other options are<span> </span><kbd>"earliest"</kbd>, or a JSON string specifying a starting offset for each<span> </span><kbd>TopicPartition</kbd>.</li>
<li><kbd>option("subscribe", "transaction")</kbd><span> </span>specifies that we want to listen to the topic<span> </span><kbd>transactions</kbd>. This is the topic name that we used in our<span> </span><kbd>StreamingProducerApp</kbd>.</li>
<li>The call to<span> </span><kbd>load()</kbd><span> </span>returns <kbd>DataFrame</kbd>. But at this stage, it only contains one<span> </span><kbd>value</kbd><span> </span>column with the raw JSON.</li>
<li>We then deserialize the JSON using the<span> </span><kbd>from_json</kbd><span> </span>function and the<span> </span><kbd>schema</kbd><span> </span>object that we created in the previous paragraph. This returns a single column of type<span> </span><kbd>struct</kbd>. In order to have all the columns at the root level, we rename the column using<span> </span><kbd>alias("v")</kbd>,<span> </span>and select all columns inside it using<span> </span><kbd>select("v.*")</kbd>.</li>
</ul>
<p>When you run the paragraph, you will get the following output:</p>
<pre>dfStream: org.apache.spark.sql.DataFrame = [timestamp: timestamp, date: string ... 4 more fields]</pre>
<p>At this point, we have <kbd>DataFrame</kbd><span> </span>with all the columns that we need to run further analysis on right? Let's try to display it. Create a new paragraph with this code and run it:</p>
<pre>z.show(dfStream)</pre>
<p>You should see the following error:</p>
<pre>java.lang.RuntimeException: java.lang.reflect.InvocationTargetException at org.apache.zeppelin.spark.SparkZeppelinContext.showData(SparkZeppelinContext.java:112) at org.apache.zeppelin.interpreter.BaseZeppelinContext.show(BaseZeppelinContext.java:238) at org.apache.zeppelin.interpreter.BaseZeppelinContext.show(BaseZeppelinContext.java:224) ... 52 elided Caused by: java.lang.reflect.InvocationTargetException: org.apache.spark.sql.AnalysisException: <strong>Queries with streaming sources must be executed with writeStream.start();;</strong></pre>
<p>The problem here is that there is a missing writing step. Even though the type<span> </span><kbd>DataFrame</kbd><span> </span>is exactly the same as the one we obtained in the<span> </span><kbd>Batch</kbd><span> </span>notebook, we cannot use it in exactly the same way. We have created a streaming<span> </span><kbd>DataFrame</kbd><span> </span>that can consume and transform messages from Kafka, but what is it supposed to do with these messages?</p>
<p>In the Spark-structured streaming world, the action methods such as<span> </span><kbd>show</kbd>,<span> </span><kbd>collect</kbd>, or<span> </span><kbd>take</kbd><span> </span>cannot be used. You have to tell Spark where to write the data it consumes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing to an in-memory sink</h1>
                </header>
            
            <article>
                
<p>A Spark structured streaming process has three types of components:</p>
<ul>
<li>The<span> </span><strong>input source</strong><span> </span>is specified with the<span> </span><kbd>format(source: String)</kbd><span> </span>method on<span> </span><kbd>DataStreamReader</kbd>. This source can be a file, a Kafka topic, a network socket, or a constant rate. Once configured with<span> </span><kbd>option</kbd>, the call to<span> </span><kbd>load()</kbd><span> </span>returns a<span> </span><kbd>DataFrame</kbd>.</li>
<li><strong>Operations</strong><span> </span>are the classic<span> </span><kbd>DataFrame</kbd>/<kbd>Dataset</kbd><span> </span>transformations, such as<span> </span><kbd>map</kbd>,<span> </span><kbd>filter</kbd>,<span> </span><kbd>flatMap</kbd>, and <kbd>reduce</kbd>. They take <kbd>Dataset</kbd><span> </span>as input and return another transformed<span> </span><kbd>Dataset</kbd><span> </span>with the recorded transformation.</li>
<li>The<span> </span><strong>output sink</strong><span> </span>writes the transformed data. For specifying the sink, we must first obtain <kbd>DataStreamWriter</kbd><span> </span>by calling the <kbd>writeStream</kbd> method on<span> </span><kbd>Dataset</kbd>,<span> </span>and then configure it. For this, we have to call the <kbd>format(source: String)</kbd> method on<span> </span><kbd>DataStreamWriter</kbd>. The output sink can be a file, a Kafka topic, or <kbd>foreach</kbd><span> </span>that takes a callback. For debugging purposes, there is also a console sink and a memory sink.</li>
</ul>
<p>Going back to our streaming transactions, the error that we obtained after calling<span> </span><kbd>z.show(dfStream)</kbd><span> </span>indicated that we were missing a sink for our<span> </span><kbd>DataFrame</kbd>. To remediate this, add a new paragraph with the following code:</p>
<pre>val query = {<br/>  dfStream<br/>    .writeStream<br/>    .format("memory")        <br/>    .queryName("transactionsStream")<br/>    .outputMode("append")<br/>    .start()<br/>}</pre>
<p>This code configures a memory sink for our<span> </span><kbd>DataFrame</kbd>. This sink creates an in-memory Spark table whose name is given by<span> </span><kbd>queryName("transactionsStream")</kbd>. A table named<span> </span><kbd>transactionsStream</kbd><span> </span>will be updated every time a new micro-batch of transactions is processed.</p>
<p>There are several strategies for writing to a streaming sink, specified by<span> </span><kbd>outputMode(outputMode: String)</kbd>:</p>
<ul>
<li><kbd>"append"</kbd><span> </span>means that only the new rows will be written to the sink.</li>
<li><kbd>"complete"</kbd><span> </span>will write all the rows every time there is an update.</li>
<li><kbd>"update"</kbd><span> </span>will write all the rows that were updated. This is useful when you perform some aggregation; for instance, counting the number of messages. You would want the new count to be updated every time there is a new row coming in.</li>
</ul>
<p>Once our<span> </span><kbd>DataStreamWriter</kbd><span> </span>is configured, we call the<span> </span><kbd>start()</kbd> method. This will start the whole workflow in the background. It is only from this point that the data starts to be consumed from the Kafka topic and gets written to the in-memory table. All the previous operations were lazy and were just configuring the workflow.</p>
<p>Run the paragraph now, and you will see an output which looks like this:</p>
<pre>query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3d9dc86a</pre>
<p>This<span> </span><kbd>StreamingQuery</kbd><span> </span>object is a handle to the streaming query running in the background. You can use it to monitor the progress of the streaming workflow, get some information about its execution plan, or<span> </span><kbd>stop</kbd><span> </span>it altogether. Feel free to explore the API of<span> </span><kbd>StreamingQuery</kbd><span> </span>and try to call its methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drawing a scatter chart</h1>
                </header>
            
            <article>
                
<p>Since we have a running<span> </span><kbd>StreamingQuery</kbd><span> </span>that writes to an in-memory table called<span> </span><kbd>transactionsStream</kbd>, we can display the data contained in this table with the following paragraph:</p>
<pre>z.show(spark.table("transactionsStream").sort("timestamp"))</pre>
<p class="mce-root">Run this paragraph, and if you have some transactions coming into your Kafka topic, you should see a table which looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f8da5a58-b286-4807-baad-ebf627065be1.png"/></p>
<p>You can then click on the scatter chart button. If you want to draw the evolution of the price, you can drag and drop the<span> </span><kbd>timestamp</kbd><span> </span>in the<span> </span><em>x</em><span> </span>axis<span> </span>and the<span> </span><kbd>price</kbd><span> </span>in the<span> </span><em>y</em><span> </span>axis. You should see something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/10b668f8-f57c-4304-9815-eaf7bb93a653.png"/></p>
<p>If you want to refresh the chart with the latest transactions, you can just re-rerun the paragraph.</p>
<p>This is pretty good, but if you let the query run for some time, you might end up with quite a lot of transactions. Ultimately, you will reach the limit of what Zeppelin can handle and you will get the same error, <kbd>OUTPUT IS TRUNCATED</kbd>, that we had in the section<span> </span><em>Drawing our first chart</em>.</p>
<p>To remediate this, we have to group the transactions using a time window, in the same way that we did for the batch transactions that we read from<span> </span><kbd>parquet</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Aggregating streaming transactions</h1>
                </header>
            
            <article>
                
<p>Add a new paragraph with the following code:</p>
<pre>val aggDfStream = { <br/>  dfStream<br/>    .withWatermark("timestamp", "1 second")<br/>    .groupBy(window($"timestamp", "10 seconds").as("window"))<br/>    .agg(<br/>      count($"tid").as("count"), <br/>      avg("price").as("avgPrice"),<br/>      stddev("price").as("stddevPrice"),<br/>      last("price").as("lastPrice"),<br/>      sum("amount").as("sumAmount")<br/>    )<br/>    .select("window.start", "count", "avgPrice", "lastPrice", <br/>    "stddevPrice", "sumAmount")<br/>}</pre>
<p>This code is very similar to the one we wrote in the section<span> </span><em>Drawing our first chart</em>. The only difference is the call to<span> </span><kbd>withWatermark</kbd>, but the rest of the code is the same. This is one of the main benefits of using Spark-structured streaming—we can reuse the same code for transforming batch datasets and streaming<span> </span>datasets.</p>
<p>Watermarking is mandatory when we aggregate streaming datasets. In a few words, we need to tell Spark how long it will wait to receive late data before discarding it, and what timestamp column it should use to measure the time between two rows.</p>
<p>As Spark is distributed, it can potentially consume the Kafka topic using several consumers, each of them consuming a different partition of the topic. This means that Spark Streaming could potentially process transactions out of order. In our case, we have just one Kafka broker and do not expect a high volume of transactions; hence, we use a low watermark of one second. Watermarking is a quite complex subject. You can find out more on the Spark website:<span> </span><a href="https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time">https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time</a>.</p>
<p>Once you have run this new paragraph, you will have a new<span> </span><kbd>Dataframe</kbd>, <kbd>aggDfStream</kbd>, which will aggregate transactions in 10-second windows. But before we can draw a chart of this aggregated data, we need to create a query to connect an in-memory sink. Create a new paragraph with this code:</p>
<pre><span>val </span>aggQuery = {<br/>  aggDfStream<br/>    .<span>writeStream<br/></span><span>    </span>.<span>format</span>(<span>"memory"</span>)<br/>    .<span>queryName</span>(<span>"aggregateStream"</span>)<br/>    .<span>outputMode</span>(<span>"append"</span>)<br/>    .<span>start</span>()<br/>}</pre>
<p class="mce-root">It is nearly the same as the one we wrote in the section<span> </span><em>Drawing a scatter chart</em>. We just used<span> </span><kbd>aggDfStream</kbd><span> </span>instead of<span> </span><kbd>df</kbd>, and the output table name is now called<span> </span><kbd>aggregateStream</kbd>.</p>
<p>Finally, add a new paragraph to display the data contained in the<span> </span><kbd>aggregateStream</kbd> table:</p>
<pre>z.show(spark.<span>table</span>(<span>"aggregateStream"</span>).<span>sort</span>(<span>"start"</span>)</pre>
<p>You will need to wait at least 30 seconds after having run <kbd>aggQuery</kbd><span> </span>to get some aggregated transaction data. Wait a bit, then run the paragraph. After that, click on the line chart button, then drag and drop the<span> </span><kbd>start</kbd><span> </span>column in the<span> </span><span class="packt_screen">keys</span><span> </span>section and<span> </span><kbd>avgPrice</kbd><span> </span>in the<span> </span><span class="packt_screen">values</span><span> </span>section. You should see a chart that looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/99fb5115-e696-40e8-b68f-b12278399905.png"/></p>
<p>If you re-run the paragraph after 10 seconds or more, you should see it being updated with new transactions.</p>
<p>It turns out that this<span> </span><kbd>aggregateStream</kbd><span> </span><kbd>DataFrame</kbd><span> </span>has exactly the same columns as the<span> </span><kbd>aggregate</kbd><span> </span><kbd>DataFrame</kbd><span> </span>that we created in the section<span> </span><em>Drawing our first chart</em>. The difference between them is that<span> </span><kbd>aggregate</kbd><span> </span>is built using the historical batch data coming from Parquet files, and<span> </span><kbd>aggregateStream</kbd><span> </span>is built using the live data coming from Kafka. They are actually complementary—<kbd>aggregate</kbd><span> </span>has got all the transactions from the last hours or days, while<span> </span><kbd>aggregateStream</kbd><span> </span>has got all the transactions from the point in time in which we started the<span> </span><kbd>aggQuery</kbd>. If you want to draw a chart containing all the data up to the latest live transaction, you can simply use <kbd>union</kbd><span> </span>of both<span> </span>dataframes, adequately filtered so that time windows are not duplicated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned how to use Zeppelin to query parquet files and display some charts. Then, you developed a small program to stream transaction data from a WebSocket to a Kafka topic. Finally, you used Spark Streaming inside Zeppelin to query the data arriving in the Kafka topic in real time.</p>
<p>With all these building blocks in place, you have all the tools to analyze the Bitcoin transaction data in more detail. You could let the<span> </span><kbd>BatchProducerApp</kbd><span> </span>run for several days or weeks to get some historical data. With the help of Zeppelin and Spark, you could then try to detect patterns and come up with a trading strategy. Finally, you could then use a Spark Streaming flow to detect in real time when some trading signal arises and perform a transaction automatically.</p>
<p>We have produced streaming data on only one topic, but it would be quite straightforward to add other topics covering other currency pairs, such as BTC/EUR or BTC/ETH. You could also create another program that fetches data from another cryptocurrency exchange. This would enable you to create Spark Streaming queries that detect arbitrage opportunities (when the price of a product is cheaper in one market than in another).</p>
<p>The building blocks we implemented in this chapter can be also used in a Lambda architecture.<span> </span>A Lambda<span> </span>architecture is a data processing architecture designed to handle large volumes of data by using both batch processing and streaming methods. Many Lambda architectures involve having different code bases for the batch and streaming layers, but with Spark, this negative point can be greatly reduced. You can find out more about Lambda architectures on this website:<span> </span><a href="http://lambda-architecture.net/">http://lambda-architecture.net/</a>.</p>
<p>This completes the final chapter in our book. We hope you enjoyed reading it as much as we enjoyed writing it. You will be empowered by the ins and outs of various concepts of Scala.<span> Scala is a language that is well designed and is not an end by himself, this is the basement to build more interesting concepts, like the type theory and for sure the category theory, I encourage your curiosity to look for more concepts to continually improve the readability and the quality of your code. You will also be able to apply it to solve a variety of real-world problems.</span></p>


            </article>

            
        </section>
    </body></html>