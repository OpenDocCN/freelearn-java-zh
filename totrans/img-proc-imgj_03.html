<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Advanced Image Processing with ImageJ"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Advanced Image Processing with ImageJ</h1></div></div></div><p>The previous chapter showed you how to load and view images in ImageJ and how to make basic alterations to image intensity and pixel values. This chapter will deal with the techniques used to preprocess images. We will prepare them for image analysis and measurements. This chapter will apply some of the techniques we examined in the earlier chapters. We will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Correcting images</li><li class="listitem" style="list-style-type: disc">Z-stack processing</li><li class="listitem" style="list-style-type: disc">Time series processing</li><li class="listitem" style="list-style-type: disc">Image and stack calculations</li></ul></div><div class="section" title="Correcting images"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Correcting images</h1></div></div></div><p>In order to analyze <a id="id80" class="indexterm"/>images, we sometimes need to correct the problems that were present during acquisition. Problems such as noise, uneven illumination, and background fluorescence can cause many issues during image analysis. I will provide a little technical background on the sources of these problems and then follow this up with how they can be corrected in ImageJ.</p><div class="section" title="Technical background"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>Technical background</h2></div></div></div><p>Of the many sources of <a id="id81" class="indexterm"/>noise that exist in imaging, a few can be corrected with correct acquisition settings. Others are inherent in the electronics and physical properties of the camera, and cannot be easily fixed. I will first deal with the source of noise that can be remedied with optimizing acquisition: <span class="strong"><strong>Shot</strong></span> or <span class="strong"><strong>Poisson</strong></span> noise. Next, we will look at <span class="strong"><strong>Electronic</strong></span> or <span class="strong"><strong>Dark</strong></span> noise.</p></div><div class="section" title="Correcting Shot noise"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Correcting Shot noise</h2></div></div></div><p>Shot noise is <a id="id82" class="indexterm"/>caused by the physical properties of light; light can be seen as packages of light or photons. The number of photons that are collected by each photo-detector site on the camera determines the final pixel intensity. If only a few photons hit the <a id="id83" class="indexterm"/>detector at any time, the differences in the number of photons <a id="id84" class="indexterm"/>could be large. This is called a Poisson process, and the signal-to-noise ratio can be expressed as follows:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_17.jpg" alt="Correcting Shot noise"/></div><p>This means that the <a id="id85" class="indexterm"/>
<span class="strong"><strong>signal-to-noise ratio</strong></span> (<span class="strong"><strong>SNR</strong></span>) will get larger as the number of photons (<span class="emphasis"><em>N</em></span>) increases. By increasing the exposure time or the illumination intensity, the number of photons per pixel and the SNR will increase. A low SNR cannot be fixed with processing techniques in software.</p></div><div class="section" title="Correcting dark noise"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Correcting dark noise</h2></div></div></div><p>Another source of <a id="id86" class="indexterm"/>noise is called dark noise or dark current. This <a id="id87" class="indexterm"/>source of noise comes from the electronics in the camera and can be visualized by taking an image with the camera without illumination. In digital consumer cameras, exposing the image while the lens is completely covered can easily do this. You can even try it with the camera of your phone. Just cover the lens tightly and take a picture (make sure the flash is disabled!). As an example, the following figure shows a small region of an image taken by two different cameras, both with the same settings. The left-hand image is a small region of an image taken with a Sony α6000 (2014), while the image on the right-hand side is from a Canon EOS 550D (2010). The orange bar is there to delineate them:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_01.jpg" alt="Correcting dark noise"/></div><p>The intensities of each image were equalized to show the pattern, and only the green channel is shown in this example. The settings for both cameras were as follows: 1/10sec exposure, ƒ5.6, and ISO 200. It is clear from these images that the noise level from the electronics is quite different between the two camera sensors. Note that most scientific cameras, especially cooled <span class="strong"><strong>EM-CCD</strong></span> (<span class="strong"><strong>Electron-Multiplying CCD</strong></span>) cameras, have far lower levels of <a id="id88" class="indexterm"/>electronic noise. This allows some EM-CCD cameras to detect single photons and even count them.</p><p>In order for the <a id="id89" class="indexterm"/>subtraction of the dark noise signal to work, the exposure duration needs to be identical to the exposure time during the acquisition to get the same level of dark noise. The duration of the exposure is directly linked to the amount of noise. A longer exposure results in more dark noise. This type of noise can be easily fixed in ImageJ using the image calculator that will be introduced a little later in this chapter.</p><p>To determine the noise level of your own camera, take a picture with the lens covered (make sure it is completely blocked from all light). Ideally, you should do this with your camera capturing images as <a id="id90" class="indexterm"/>RAW files. When a camera acquires images as JPEG files, the camera already performs some noise reduction on the image. If you can only capture images in JPEG, check to see whether there is an option to switch off the noise reduction. Now, open the image in ImageJ, as was illustrated in the previous chapter, and follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select your darkshot image window by clicking on it, making it active. In ImageJ, most commands will operate on the active image, or the last opened image. By clicking on an image window, that image becomes the active image.</li><li class="listitem">In order to determine the noise level, we can select an area that we want to measure. We will create a rectangle by specifying it by entering the specific values. To do this, go to <span class="strong"><strong>Edit</strong></span> | <span class="strong"><strong>Selection</strong></span> | <span class="strong"><strong>Specify…</strong></span> and select the <span class="strong"><strong>Centered</strong></span> checkbox, before entering <code class="literal">512</code> for the width and height. For the <span class="strong"><strong>X coordinate</strong></span> and <span class="strong"><strong>Y coordinate</strong></span>, enter half the width and height of your image (indicated in the image subtitle) and click on <span class="strong"><strong>OK</strong></span>.</li><li class="listitem">Make sure that the measurements are set to standard deviation. This can be done by going to <span class="strong"><strong>Analyze</strong></span> | <span class="strong"><strong>Set Measurements</strong></span> and selecting the <span class="strong"><strong>Standard deviation</strong></span> checkbox. Selecting other parameters for measurement is fine, and in the output below <span class="strong"><strong>Area</strong></span>, <span class="strong"><strong>Mean gray value</strong></span> and <span class="strong"><strong>Min &amp; max gray value</strong></span> were also selected. For this exercise, the <span class="strong"><strong>Standard deviation</strong></span> option is the only relevant parameter that is required.</li><li class="listitem">Perform the measurement by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>M</em></span> or by going to <span class="strong"><strong>Analyze</strong></span> | <span class="strong"><strong>Measure</strong></span>. You can measure regions immediately after placing them, or you can add them to the <span class="strong"><strong>ROI Manager</strong></span> (see the next chapter for more details) before measuring them.</li></ol></div><p>The results should now be <a id="id91" class="indexterm"/>visible in a new window labeled <span class="strong"><strong>Results</strong></span>. Depending on which parameters you selected, the results in this window might deviate from <a id="id92" class="indexterm"/>the one shown here (I have included area and the minimum and maximum values):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_02.jpg" alt="Correcting dark noise"/></div><p>The first line contains the results from the α6000 camera, and the second line contains the results from the EOS 550D camera. The area is identical (<span class="emphasis"><em>512 x 512 = 262144 pixels</em></span>) for both measurements, but the standard deviation (a measure of the noise) is lower for the first camera by a factor of 6.3. Also, the mean of the first camera is closer to 0, as you would expect the value to be when there is no light hitting the sensor.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>Cameras can have pixels that no longer work (<span class="strong"><strong>dead pixels</strong></span>). Dead pixels will show as black pixels in bright areas and always occur at the same location. The opposite can also happen. Very bright pixels in dark areas are called <span class="strong"><strong>hot pixels</strong></span>. Hot pixels do not have to occur at the same location every time and are more common with very long exposure times. For EMCCD cameras, there is another source of bright pixels, which is caused by cosmic rays hitting the image sensor. These events are relatively common in long time series, and present themselves as very bright regions for only a single frame. The removal of dead and hot pixels follows the same methodology as the dark noise removal.</p></div></div><p>For most type of exposures, these levels of noise are so small that they do not cause degradation of your pictures. A picture at the beach with the sun in the sky will not require correction. With the <a id="id93" class="indexterm"/>enormous amount of light that is detected, the electronic noise is drowned out completely. However, one field of image acquisition where <a id="id94" class="indexterm"/>dark noise is a substantial factor is in the field of astrophotography or night-time photography. Whenever long exposures are required for image acquisition, the electronic noise becomes a substantial factor that can degrade your image.</p><p>To reduce the effects of sensor noise in low-light conditions, you need to change the way you acquire your image slightly: instead of a single exposure, you need multiple exposures in quick succession. Some cameras support this automatically, using names such as handheld twilight (Sony) or multi-frame noise reduction (Pentax, Olympus etc.). In this mode you take 2 or more pictures in rapid succession and the final image is an average of the series of images. You can also do something like this in ImageJ by using the following procedure:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the multiple images that you acquired in succession (make sure there are no other images opened!)</li><li class="listitem">Select <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Stacks</strong></span> | <span class="strong"><strong>Images to Stack</strong></span> from the menu. You will now have a single window where every slice represents one image that you took.</li><li class="listitem">Create the noise-reduction image by selecting <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Stacks</strong></span> | <span class="strong"><strong>Z Project…</strong></span> from the menu and use Average Intensity as the projection type.</li></ol></div><p>A thing to keep in mind is the following: when anything moves between the individual exposures, this method will not provide good results. It is possible to correct for simple shifts, but this only works in the simplest of cases.</p></div><div class="section" title="Uneven illumination – background subtraction"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/>Uneven illumination – background subtraction</h2></div></div></div><p>When an <a id="id95" class="indexterm"/>image is acquired under difficult lighting conditions, it can sometimes occur that the illumination is not even across the image sensor. This effect of uneven illumination is something that can easily be corrected in ImageJ. To show how this is done, we will take an image acquired with brightfield illumination <a id="id96" class="indexterm"/>on an inverted microscope using <span class="strong"><strong>Differential Interference Contrast</strong></span> (<span class="strong"><strong>DIC</strong></span>) optics.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>DIC images provide contrast by looking at the difference in thickness of your specimen. A single light wave is split into two separate rays that are slightly separated but parallel and with the same phase. When one ray goes through an object with higher density than the parallel ray, the waves will shift out of phase. When they are recombined, the out-of-phase rays will partially cancel each other out (interference). This results in less light on the camera pixel, making the pixel darker. For cells, the strongest interference can be found close to the membrane of the cell. One ray will pass through the cell, while the parallel ray will pass through the water outside the cell.</p></div></div><p>The image shows the effect of uneven illumination. The left-hand side of the frame is darker than the middle, and the gradient runs along the frame in a slightly diagonal direction. It is also clear that the field is not going in one direction. The middle is the brightest and the two edges, left and right, are darker:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_03.jpg" alt="Uneven illumination – background subtraction"/></div><p>As a first <a id="id97" class="indexterm"/>attempt, we will use the background subtraction method to see whether this will fix the problem. To do so, we need to go to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Subtract Background…</strong></span> and use the following settings:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_04.jpg" alt="Uneven illumination – background subtraction"/></div><p>After applying the background subtraction, the image is altered, but the effect of the uneven background is still not fixed. The image is actually a little darker on the left-bottom side, and also the middle did not decrease in intensity that much (see the left-hand-side image). Note <a id="id98" class="indexterm"/>that when the <span class="strong"><strong>Light background</strong></span> option was selected (see the right-hand-side image), there is a strong over compensation on both the left and right-hand sides. Not only is the contrast reduced on those sides, but also the illumination is now more uneven than it was before the correction:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_05.jpg" alt="Uneven illumination – background subtraction"/></div><p>Disabling the <span class="strong"><strong>Sliding paraboloid</strong></span> option also caused artifacts that were even more artificial and incorrect. The problem with this type of background subtraction is that it assumes a homogeneously decreasing change in background. This means that the changes in background should be smooth and go from high to low in a single direction (left to right, diagonal, and so on.). However, DIC images such as this one have a tendency to have a background that has more of a U-shape: high at the edges and low in the middle, or vice versa. Therefore, this method is unsuitable for this type of image, and other methods need to be explored to fix this problem.</p><p>Next, we will try to eliminate the background using a method called pseudo-flatfield correction. This method is based on filtering the image using a Gaussian filter that blurs the details. This filter will capture the uneven illumination and separate it from the objects in the frame. The basis of how these filters work will be discussed in the next chapter in more detail. Let's create the background image that we will use to correct the uneven illumination. You need to perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we want to <a id="id99" class="indexterm"/>duplicate the image so that we keep the original image for subtraction. To do so, we will go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Duplicate…</strong></span> or use <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>D</em></span> and name the duplicate image <code class="literal">background</code>.</li><li class="listitem">To create a Gaussian low-pass filter, we will select the background image and go to <span class="strong"><strong>Process | Filters | Gaussian Blur…</strong></span>, entering a value of <code class="literal">150</code> for the sigma (radius). When you check the preview checkbox, you will see that the image will look like it is defocused. You can see that the objects can no longer be distinguished, and what is left is the diagonal background illumination.</li><li class="listitem">We can now subtract this background from the original image to correct the uneven illumination. To do this, we will start the image calculator by going to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Image Calculator…</strong></span> from the menu. Then, we will select the original image as <span class="strong"><strong>Image1</strong></span> and the background image as <span class="strong"><strong>Image2</strong></span>. Set the operation to <span class="strong"><strong>Subtract</strong></span> and check the <span class="strong"><strong>Create new window</strong></span> and <span class="strong"><strong>32-bit (float) result</strong></span> checkboxes. The following image shows the effect of the subtraction and how it corrected the uneven illumination:<div class="mediaobject"><img src="graphics/Insert_image_4909_03_06.jpg" alt="Uneven illumination – background subtraction"/></div></li></ol></div></div><div class="section" title="Image normalization"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Image normalization</h2></div></div></div><p>To enhance the <a id="id100" class="indexterm"/>contrast of an underexposed image, you can go to <span class="strong"><strong>Process | Enhance Contrast… </strong></span>option, and select the <span class="strong"><strong>Normalize</strong></span> checkbox. This stretches the gray values over the entire range of an 8-bit or 16-bit image. It does not work on RGB images. The following image shows the effect of the normalization, with the original image on the left-hand side and the normalized image on the right-hand side:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_07.jpg" alt="Image normalization"/></div><p>This also works for stacks or time series, where the normalization can be done for each frame separately. A similar effect can be obtained using the <span class="strong"><strong>Auto</strong></span> option in the <span class="strong"><strong>Brightness/Contrast</strong></span> <a id="id101" class="indexterm"/>window, as described in the previous chapter. Note that the normalization is applied to the image and modifies the pixel values irreversibly. If the signal should not change over time, this should not pose a big problem for measurements. However, for intensity changes over time, this method will distort or remove the changes.</p></div><div class="section" title="Bleach correction"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"/>Bleach correction</h2></div></div></div><p>When imaging <a id="id102" class="indexterm"/>fluorescence, the illumination can cause the bleaching of the <a id="id103" class="indexterm"/>fluorophore under investigation. This effect is well established and is related to the intensity of the excitation light. To avoid this effect, it is better to use a long exposure with low intensity light. However, this may not always be possible. The amount of bleaching is related to the intensity at the beginning and decreases in an exponential fashion. To see whether an image series is affected by bleaching, we can make a quick measurement on the entire image for each frame to see what the mean intensity is. Note that if there are changes in illumination or background signal in individual frames, the results might not look like a smooth curve. To make a quick measurement, press <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>A</em></span> to select the entire frame and then press <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>M</em></span> to measure the intensity. Repeat the measurement for each frame and plot the mean intensity values against the frame number (or time, if you know the interval) in your favorite graphics program. In this case, I used <span class="strong"><strong>MATLAB</strong></span> to create the plot, although you could also create the plot by selecting <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Stacks</strong></span> | <span class="strong"><strong>Plot Z-axis Profile</strong></span> from the menu in ImageJ. Here is an example of a bleaching curve:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_08.jpg" alt="Bleach correction"/></div><p>This collection of points seems to follow a trend that is close to either a straight line or an exponential curve, although the trend in the first 2 seconds seems more exponential than linear.</p><p>In order to perform <a id="id104" class="indexterm"/>bleach correction, you can select the correction plugin in <a id="id105" class="indexterm"/>Fiji by going to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Adjust</strong></span> | <span class="strong"><strong>Bleach Correction</strong></span>. There are three methods for correction:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Simple ratio</li><li class="listitem" style="list-style-type: disc">Exponential fit</li><li class="listitem" style="list-style-type: disc">Histogram matching</li></ul></div><p>Simple ratio is the best method if the decrease in intensity does not follow a regular shape, such as an exponential decay function. For most fluorescence imaging, this method yields good results and can be combined with fluorescence measurements. The histogram matching method performs better with noisy images, but is less suitable for intensity measurements.</p><p>Since our trend looked more like an exponential decay model, we selected the second method, which fits a single exponential function to the data:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_09.jpg" alt="Bleach correction"/></div><p>This plot was generated using the parameters obtained by the <span class="strong"><strong>bleach correction</strong></span> command and entering them in MATLAB. The ImageJ command itself also generates a plot with the data and the fitted curve. However, the axes are labeled <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>. So, for the purpose of clarity, I have recreated the plot with the correct labels. The red line indicates the fitted function, which matches the curve with an R<sup>2</sup> value of .9954 (a very good fit). The model consists of three parameters, labeled <span class="emphasis"><em>a</em></span>, <span class="emphasis"><em>b</em></span>, and <span class="emphasis"><em>c</em></span>. The value of <span class="emphasis"><em>a</em></span> indicates how much above the asymptote, indicated by the <span class="emphasis"><em>c</em></span> value, the first point lies. The asymptote is the value to which this exponential curve will go when given infinite time. The value of <span class="emphasis"><em>b</em></span> indicates the rate at which the curve decays. If you want to know the time it takes to lose half of the initial <a id="id106" class="indexterm"/>fluorescence, you can use the following formula, using <a id="id107" class="indexterm"/>the <span class="emphasis"><em>b</em></span> value from the fitting:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_18.jpg" alt="Bleach correction"/></div><p>The preceding formula gives you the half-time of the fluorescence loss. Note that the <span class="emphasis"><em>b</em></span> parameter for the fit is expressed in frames and not time. So, when using the preceding formula, you need to multiply the result with your frame interval to get the value in seconds (or minutes). In the graph shown earlier, the half-time is 30.587 seconds (using the formula with a <span class="emphasis"><em>b</em></span> value of 0.0028327 and a frame interval of 0.125 seconds).</p></div></div></div>
<div class="section" title="Stack processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Stack processing</h1></div></div></div><p>ImageJ is very suitable <a id="id108" class="indexterm"/>to process information that has more than two dimensions: data acquired at different Z-levels or at different time points. We have already seen an example of stack processing in the section on noise correction. The next section will deal with time series consisting of frames. However, first, we will explore more options when dealing with image stacks containing slices (Z-stacks).</p><div class="section" title="Processing Z-stacks"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/>Processing Z-stacks</h2></div></div></div><p>Z-stacks are series of <a id="id109" class="indexterm"/>2D images that were acquired at different heights or distances. In a microscope, this is done by moving the objective or the stage up or down and acquiring an image at specific intervals. In <span class="strong"><strong>Magnetic </strong></span><a id="id110" class="indexterm"/>
<span class="strong"><strong>Resonance Imaging</strong></span> (<span class="strong"><strong>MRI</strong></span>), this is <a id="id111" class="indexterm"/>done by moving the patient through the center of the scanner. The scanner then creates an image for each position using radio pulses that create fluctuations in the magnetic field. These fluctuations can be measured by the detector in an MRI machine. This results in a single slice that can be combined into a single file. Some of the processing that you may want to perform on this type of image involves creating projections or 3D renders of the volume. We will first examine the projections that you can create. Then, you will understand why you would create them.</p><div class="section" title="Stack projections"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/>Stack projections</h3></div></div></div><p>We have already seen an <a id="id112" class="indexterm"/>example of a Z-projection in the section on noise cancellation. In the previous section, we used the projection to create an average intensity for each pixel over the frames. For images that contain slices (Z-information), an average projection is usually not the most useful projection. However, there are other Z-projections available in ImageJ that are more applicable for Z-stacks. The following sections will deal with some examples of these projections.</p><div class="section" title="Maximum projection"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec01"/>Maximum projection</h4></div></div></div><p>A maximum projection <a id="id113" class="indexterm"/>uses the maximum intensity of each pixel across the slices. If a stack has 20 slices, then each pixel will contain the maximum value across the 20 slices. This type of projection can be helpful to reduce the third dimension of a Z-stack in order to create a two-dimensional representation of the data. This type of projection essentially flattens the image. When used on fluorescence images with sparse signal (few bright pixels) at the same location, this projection has the effect of showing all the objects in a single frame. It is also useful for fluorescent images that have thin objects that are in focus in different slices at different positions. By flattening the Z-stack, all the in-focus parts will be visible in one continuous shape. You can visualize this as a flight of stairs. Each step has a different Z-position, but if you would flatten the steps (assuming that the steps do not overlap), you would get a rectangular board. If you have an image that is not sparse, then this projection would be of little use. To demonstrate this projection, open the <span class="strong"><strong>Confocal Series</strong></span> image from the sample images. Go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Stacks</strong></span> | <span class="strong"><strong>Z Project…</strong></span> and choose <span class="strong"><strong>Max Intensity</strong></span> as the <a id="id114" class="indexterm"/>projection type. The following image shows the result of this projection:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_10.jpg" alt="Maximum projection"/></div><p>As is visible in the preceding image, the maximum intensity projection shows the entire shape of the cell, but some information was lost. Specifically, the small details in the first frames are drowned out by the intense pixels from the middle of the volume. For some representations, this is fine. For some Z-stacks, the rough shape of the total volume is unclear in the individual slices, but the maximum projection shows the general shape.</p><p>To demonstrate this effect, we will open the <span class="strong"><strong>Bat Cochlea Volume</strong></span> image by going to <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Open Samples</strong></span>. Looking at a few slices from this volume gives very little information about the shape of this <a id="id115" class="indexterm"/>sensory organ (the numbers indicate the slice number):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_11.jpg" alt="Maximum projection"/></div><p>When we create a maximum intensity projection, the general shape of this organ becomes much more obvious: it is shaped like a twisted spiral (the cochlea is the shell-shaped cavity that is used for hearing). When you also open the <span class="strong"><strong>Bat Cochlea Renderings</strong></span> image by going to <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Open Samples</strong></span>, you see the 3D rendering of the volume:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_12.jpg" alt="Maximum projection"/></div><p>The image on the left-hand side shows the maximum projection of the Z-stack, while the images on the right-hand side show the rendered volumes. From this image, it is clear that the maximum projection provides more information than the individual slices. However, some of the details are still lost in the process. In particular the top-left part of the cochlea is very unclear in the maximum projection. The start of the spiral is obscured because of the loop behind it.</p><p>Fiji has an option that allows you to create a maximum projection that contains more information: depth coding. Depth coding assigns a color to the Z-location of a pixel, resulting in different colors for different slices. To do so in Fiji, go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Hyperstacks</strong></span> | <span class="strong"><strong>Temporal-Color Code</strong></span> and select <span class="strong"><strong>Grays</strong></span> for the LUT. This results in the middle representation in the preceding image (not exactly, but very similar). Note that you will get a message that the slices and frames were swapped. This is done because this plugin is designed for time series and not Z-stacks. The right image in the preceding figure is a 3D rendering <a id="id116" class="indexterm"/>of the volume, which will be covered in the next section.</p></div></div><div class="section" title="Volume viewing and rendering"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/>Volume viewing and rendering</h3></div></div></div><p>When images are <a id="id117" class="indexterm"/>acquired over a range of depths, the goal <a id="id118" class="indexterm"/>is usually to view this collection of images as a 3D volume. Another useful viewing aspect is to take a 3-dimensional volume, cut it along the <span class="emphasis"><em>z</em></span> axis, and view the volume from the side. This latter view cannot be obtained from the two-dimensional images. For this example, we will use two different stacks from the sample images. Let's start with viewing the volume of an MRI stack. To open the image, go to <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Open Samples</strong></span> and select <span class="strong"><strong>MRI stack</strong></span>. This is an MRI stack where every slice is at a different level through the head (the numbers indicate the slice number):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_13.jpg" alt="Volume viewing and rendering"/></div><p>The eyes are clearly visible in slice number 6 as two dark orbs on the top of the image. Slice 11 shows the brain within the skull, and frame 16 shows the ventricles as black holes in the middle of the head. Slice 26 shows the top of the head. The area is much smaller indicating that the crown of the head is being reached.</p><p>A lot of information is already present in the slices. The eyes can clearly be seen, as well as the sinuses and the nose (slice 1). The information in these slices is not complete, however. We lack shape along the <span class="emphasis"><em>z</em></span> axis. To view the three-dimensional shape of this volume, we can use a viewer that comes with Fiji. Go to <span class="strong"><strong>Plugins</strong></span> | <span class="strong"><strong>Volume Viewer</strong></span> with the MRI stack selected. If you have the standard ImageJ, the <span class="strong"><strong>Volume Viewer</strong></span> plugin can be downloaded and installed from the plugins page. Then, the following window will open:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_14.jpg" alt="Volume viewing and rendering"/></div><p>This is <span class="strong"><strong>Volume </strong></span><a id="id119" class="indexterm"/>
<span class="strong"><strong>Viewer</strong></span>, which is included in Fiji and available as a plugin for ImageJ. On the left-hand side, there are three images that show different views of the volume: an <span class="strong"><strong>xy</strong></span> slice (this is the top view for this stack), a <span class="strong"><strong>yz</strong></span> slice (this is a side view for this stack), and finally, an <span class="strong"><strong>xz</strong></span> slice (this is a front view for this stack). The large image in the middle <a id="id120" class="indexterm"/>of the viewer is the current selected view, in this case, the <span class="emphasis"><em>xz</em></span> slice view. The position of this slice is indicated in the overview images on the left-hand side by the cyan (<span class="emphasis"><em>xy</em></span>) and green (<span class="strong"><strong>yz</strong></span>) lines. Note that I adjusted the <span class="strong"><strong>z-Aspect</strong></span> by entering <code class="literal">5</code> and pressing <span class="emphasis"><em>Enter</em></span> instead of the value of <code class="literal">1</code> based on the current calibration. The volume, otherwise, looks very squashed. The squashed appearance is caused by the fact that this image was not calibrated. Each voxel (a contraction of volume <a id="id121" class="indexterm"/>and pixel) is 1 x 1 x 1, without a unit. A typical value for the voxel size in MRI images is 1.5 x 1.5 x 3.0 mm, which can be set using the image properties as described in the previous chapter. We can now change the view by selecting the view buttons at the bottom of the viewer. The <span class="strong"><strong>yz</strong></span> button will give us a side view of this volume. It is also possible to rotate the volume by clicking and dragging the mouse.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>The size of the volume viewer window has a minimum size of ±1024 x 768 pixels. This can mean that depending on the pixel dimensions of your monitor, some of the controls might fall off the screen. For most modern displays, this should not be an issue. However, for some small screens or beamers, this can be a problem.</p></div></div><p>Next, we will look at a different type of image: a Z-stack of a fly brain using fluorescent imaging. To open the image, go to <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Open Samples</strong></span> and select the <span class="strong"><strong>Fly Brain</strong></span> image. The Z-stack will open, and you can go through the slices:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_15.jpg" alt="Volume viewing and rendering"/></div><p>The first slice contains no bright pixels, but as you move through the stack, the brain of the fruit fly starts to show <a id="id122" class="indexterm"/>defined features. This stack shows the brain <a id="id123" class="indexterm"/>from a (rotated) front view in contrast to the MRI stack, which was displayed as a top view. We will use the volume viewer to examine the entire volume and use it to create a short movie of the volume turning. To start, select the <span class="strong"><strong>Fly Brain</strong></span> stack and go to <span class="strong"><strong>Plugins</strong></span> | <span class="strong"><strong>Volume Viewer</strong></span>. The initial image will be a slice view, but for this example, we want to switch to a different mode. Select <span class="strong"><strong>Volume (4)</strong></span> mode using the selector at the top of the viewer. We will set the interpolation to <span class="strong"><strong>Tricubic sharp (3)</strong></span> using the drop-down selector. On the right-hand side of the viewer, we will modify the transfer function to <span class="strong"><strong>2D Grad</strong></span> to create a slightly more pleasing view. Next, we will set the rotation for <span class="emphasis"><em>X</em></span>, <span class="emphasis"><em>Y</em></span>, and <span class="emphasis"><em>Z</em></span> at the bottom of the volume viewer to -90, 30, and 180 respectively. This will provide a side view of the brain.</p><p>By pressing the <span class="strong"><strong>Snapshot</strong></span> button (top right) in the viewer, we will get a picture of the current view. Next, we will increase the value for the <span class="emphasis"><em>Y</em></span> rotation with 10-degree increments and take a snapshot every time until you have reached 210 degrees. We now made snapshots of the brain from one side of the brain to the other side (180 degrees). To turn this into an animation, all we have to do is go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Stacks</strong></span> | <span class="strong"><strong>Images to Stack</strong></span>. If you close the original stack, you only have to press OK in the dialog. Otherwise, you would have to enter <code class="literal">Volume_Viewer</code> in the <span class="strong"><strong>Title contains</strong></span> field. You will now have a stack that can be played and saved as a movie for presentation purposes. For this example, we used increments of 10 degrees for the rotation, which gives an adequate result. However, if you take smaller increments, the result will look much smoother. Feel free to modify the angles at which you view the volume for different results as well as experiment with the other settings available within the volume viewer.</p><p>The volume viewer is <a id="id124" class="indexterm"/>a very powerful function in ImageJ <a id="id125" class="indexterm"/>that allows for the investigation and visualization of 3D objects. Use the <span class="strong"><strong>Slice (0)</strong></span> mode to examine the volume as a cross-section and the <span class="strong"><strong>Volume (4)</strong></span> mode to see a solid model.</p></div></div><div class="section" title="Processing time series"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"/>Processing time series</h2></div></div></div><p>Time series <a id="id126" class="indexterm"/>consists of images acquired over time, usually with a fixed interval. Movies can also be seen as time series with a fixed interval of 24 or 25 frames per second (<span class="strong"><strong>fps</strong></span>). Processing of time series mostly focuses on two areas: fluctuations in <a id="id127" class="indexterm"/>intensity over time and background reduction and normalization. Fluctuations in intensity have been covered in the previous section where we looked at bleach correction. In the following section, we will look at ways to normalize the time series data.</p></div><div class="section" title="Normalizing time series data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec30"/>Normalizing time series data</h2></div></div></div><p>Normalizing <a id="id128" class="indexterm"/>time series data will help in further analysis by <a id="id129" class="indexterm"/>providing a correction for the baseline <a id="id130" class="indexterm"/>intensity. Many times, the goal of time series is to look at changes in intensity or movement over time. Normalizing will yield cleaner time series data relative to the resting or baseline state. A very simple normalization is to calculate ΔF over F0 (dFF0). The basis for this metric is that the baseline fluorescence can be different between time series, but the changes in intensity <span class="emphasis"><em>relative to the baseline</em></span> are similar. It is calculated using the following formula:</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_19.jpg" alt="Normalizing time series data"/></div><p>The numerator is the difference between the current frame (Ft) and the baseline (F0). The baseline is the average of the first <span class="emphasis"><em>n</em></span> frames. A value of <span class="emphasis"><em>dFF0</em></span> larger than 1 indicates the signal increased relative to the baseline, while a value less than 1 indicates a decrease relative to the baseline. It is possible to perform this calculation only on the measured values of a time series (in Excel or MATLAB), but you can also transform the time series directly. I will now show you how to do this in ImageJ using the Z projection, image duplication, and image calculator.</p><p>To get started, we will open the <code class="literal">timeseries_events.tif</code> image, which is available in the online resources with this book. This is a time series of vesicles in a cell that are transported and fuse when the cell is stimulated with electrodes. It contains two channels: one with a red fluorescent marker and the other with a green fluorescent marker. The red marker is fluorescent at all times until the vesicle fuses, at which point it disappears. The green marker is not fluorescent while the cargo is within the vesicle, but as soon as it fuses, it becomes bright. To start the processing, we first want to split the channels into two different time series. To do this, select the time series and go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Color</strong></span> | <span class="strong"><strong>Split Channels</strong></span> to generate two time series: one for each channel. We will select the green channel, which was labeled <code class="literal">C1_timeseries_events.tif</code>, using the <span class="strong"><strong>split channels</strong></span> command.</p><p>We can now start with the first step in the creation of the <code class="literal">dFF0</code> time series: creating the baseline frame. We will go to <span class="strong"><strong>Image</strong></span> | <span class="strong"><strong>Stacks</strong></span> | <span class="strong"><strong>Z Project…</strong></span> and set the method to <span class="strong"><strong>Average Intensity</strong></span> and <span class="strong"><strong>Stop slice:</strong></span> to <code class="literal">5</code>. What we do here is create an average of the first five frames. This effectively reduces the noise in the individual frames by averaging it out while leaving the <a id="id131" class="indexterm"/>bright objects present in the first frames intact. Let's rename the resulting image to make it easier to identify later on. Right-click on the average image and select <span class="strong"><strong>Rename…</strong></span> from the context menu. Rename the image to <code class="literal">F0</code> so that it will be easy to select later on.</p><p>For the next step, we <a id="id132" class="indexterm"/>will create the ΔF image. As explained at the beginning of this section, this image is the raw image minus the baseline image. To get this <a id="id133" class="indexterm"/>image, we will use the image calculator by going to <span class="strong"><strong>Process</strong></span> | <span class="strong"><strong>Image Calculator</strong></span> from the <span class="strong"><strong>ImageJ</strong></span> menu. Select the original time series as <code class="literal">Image1</code> and the <span class="strong"><strong>F0</strong></span> image as <code class="literal">Image2</code>. Then, set the method to <span class="strong"><strong>Subtract</strong></span>. Make sure that the <span class="strong"><strong>Create new window</strong></span> option is selected.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>The order of the images is very important when one of them is a stack or time series and the other is a single frame. The stack always needs to be set at the <code class="literal">Image1</code> position if you wish to modify each slice or frame. For subtraction, this is usually obvious, but for multiplication, the order of the operation would not be important from a mathematical perspective (<span class="emphasis"><em>A × B equals B × A</em></span>). However, if you place the time series or stack on <code class="literal">Image2</code> and the single frame on <code class="literal">Image1</code>, only the current slice or frame is used for the calculation!</p></div></div><p>We now have the ΔF stack, so let's rename it to make it easier. Right-click on the new time series, select <span class="strong"><strong>Rename…</strong></span>, and enter <code class="literal">deltaF</code> as the new name.</p><p>Now, we can create the final time series that is normalized to the baseline. Note that the <code class="literal">deltaF</code> series by itself already provides an improvement over the original time series as it is corrected for the initial static background. To create the <span class="emphasis"><em>dFF0</em></span> image, we will use the Image Calculator again. This time, we will select <code class="literal">deltaF</code> as <code class="literal">Image1</code> and <code class="literal">F0</code> as <code class="literal">Image2</code> and the <span class="strong"><strong>Divide</strong></span> operation. Select the <span class="strong"><strong>Create new window</strong></span> and <span class="strong"><strong>32-bit (float) result</strong></span> options.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>This time, the 32-bit result option is useful. As we noted earlier, in the definition of the calculation, we expected the results to be between 0 and infinity. This is denoted as <span class="emphasis"><em>[0, ∞]</em></span> in mathematical notation. This means that that any value, including 0 and infinity, are within the range of possible values. When this option is not selected during the calculation, all the values below 1 will be rounded to 0, and information about these events are lost. Note that for the example used here, the events we wish to see will have a value larger than 1. So, in this particular case, it is not crucial.</p></div></div><p>The new image is now the <a id="id134" class="indexterm"/>dFF0 image, which has been corrected for the baseline and normalized to the initial baseline intensity. The following image <a id="id135" class="indexterm"/>shows the effect of this <a id="id136" class="indexterm"/>normalization (second row), compared to the original images (first row):</p><div class="mediaobject"><img src="graphics/Insert_image_4909_03_16.jpg" alt="Normalizing time series data"/></div><p>The clearest difference that can be seen is that the images before frame <span class="strong"><strong>300</strong></span> are nearly black, indicating that nothing is happening relative to the baseline situation. At frame <span class="strong"><strong>300</strong></span> and beyond, the increase in signal at different locations is very clear, indicating that the signal has increased in these locations.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Summary</h1></div></div></div><p>In this chapter, we investigated the processing of different types of images. We looked at different sources of noise that can corrupt images and degrade their quality. You learned how to apply different corrections to the images to fix these problems. We then looked at processing steps specifically aimed at Z-stacks and time series.</p><p>In the next chapter, we will see how to separate pixels into different groups and how to clean up and filter the result for further processing.</p></div></body></html>