- en: Reactive Messaging with Quarkus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the nuts and bolts of **SmallRye Reactive
    Messaging**, which can be used in Quarkus to implement the Eclipse MicroProfile
    Reactive Messaging specification. By the end of this chapter, you will have a
    solid development model for your data streaming applications and know how to connect
    to streaming platforms such as **Apache Kafka** and **ActiveMQ**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Reactive Messaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming messages with Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming messages with **Advanced Message Queuing Protocol** (**AMQP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the source code for the project in this chapter on GitHub at [https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Reactive Messaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reactive Streams is an initiative that aims to provide a standard for exchanging
    data streams across an asynchronous boundary. At the same time, it guarantees
    that the receiving side is not forced to buffer arbitrary amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several available implementations of Reactive Stream, and we have
    already learned how you can feature Reactive Programming in Vert.x. In this chapter,
    we will complement our knowledge using the SmallRye Reactive Messaging implementation
    to show you how we can integrate it with a streaming platform such as Apache Kafka
    or a messaging broker such as ActiveMQ with minimal configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: To familiarize ourselves with MicroProfile Reactive Messaging, we need proper
    knowledge of some key concepts. First of all, MicroProfile Reactive Messaging
    is a specification that uses CDI beans to drive the flow of messages toward some
    specific channels.
  prefs: []
  type: TYPE_NORMAL
- en: A message is a basic interface that contains a payload to be streamed. The `Message`
    interface is parameterized in order to describe the type of payload it contains.
    Additionally, a `Message` interface contains attributes and metadata that are
    specific to the broker that's used for message exchange (for example, Kafka or
    AMQ).
  prefs: []
  type: TYPE_NORMAL
- en: 'A channel, on the other hand, is a string indicating which source or destination
    of messages is used. There are two types of channels:'
  prefs: []
  type: TYPE_NORMAL
- en: Internal channels are local to the application and are used to implement a multi-step
    process for messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remote channels are connected to remote brokers (such as Apache Kafka or AMQ)
    through connectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since MicroProfile Reactive Messaging is fully governed by the CDI model, two
    core annotations are used to indicate whether a method is a producer or a consumer
    of messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@Incoming`**:** This annotation is used on a method to indicate that it consumes
    messages from the specified channel. The name of the channel is added to the annotation
    as an attribute. Here is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The effect of placing this annotation on a method is that the method will be
    called each time a message is sent to that channel. From the user''s perspective,
    it is totally transparent about whether the incoming message arrives from a collocated
    CDI bean or a remote broker. However, you may decide to make it clear that the
    method consumes a specific kind of message, such as `KafkaMessage` (which inherits
    from `Message`). Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`@Outgoing`: This annotation indicates that a method publishes messages to
    a channel. In much the same way, the name of the channel is stated in the annotation''s
    attribute:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Within the method annotated with `@Outgoing`, we return a concrete implementation
    of the `Message` interface.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that you can only annotate a single method with `@Outgoing` for one
    channel. If you attempt to use the same channel in more than one `@Outgoing` annotated
    method, an error will be emitted at deployment time.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also annotate a method with both `@Incoming` and `@Outgoing` so that
    it behaves like a **message processor**, which transforms the content of the message
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding examples, we can see that messages flow from an `@Outgoing`
    stream producer to an `@Incoming` stream consumer and Reactive Messaging transparently
    connects the two endpoints. In order to decouple `Producer` and `Consumer` messages,
    you can add a component such as Apache Kafka by using the connectors provided
    by MicroProfile API. In the next section, we will introduce our first example
    of Reactive Messaging using Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming messages with Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka ([https://kafka.apache.org/](https://kafka.apache.org/)) is a distributed
    data streaming platform that can be used to publish, subscribe, store, and process
    streams of data from multiple sources in real time at amazing speeds.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka can be plugged into streaming data pipelines that distribute data
    between systems, and also into the systems and applications that consume that
    data. Since Apache Kafka reduces the need for point-to-point integrations for
    data sharing, it is a perfect fit for a range of use cases where high throughput
    and scalability are vital.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, once you combine Kafka with Kubernetes, you attain all the benefits
    of Kafka, as well as the advantages of Kubernetes, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability and high availability**: You can easily scale up and down resources
    with Kubernetes, which means you can automatically determine the pool of resources
    that Apache Kafka will share with other applications while guaranteeing the high
    availability of your Kafka cluster at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portability**: By running Kafka with Kubernetes, your cluster of Kafka nodes
    can span across on-site and public, private, or hybrid clouds, even using different
    operating systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To manage the Kafka environment, you need a piece of software called **ZooKeeper**,
    which manages naming and configuring data in order to provide flexible and robust
    synchronization within distributed systems. ZooKeeper controls the status of the
    Kafka cluster nodes and also keeps track of Kafka topics, partitions, and all
    the Kafka services you need. We won't cover the details of ZooKeeper administration
    in this chapter, although it's worth mentioning its role as you will need to get
    to grips with it in order to land a Kafka Administrator job.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate Apache Kafka and MicroProfile Streaming's powerful combination
    on Quarkus, we will design a simple application that simulates a stock trading
    ticker that's updated in real time by purchases and sales. Get ready and open
    for business!
  prefs: []
  type: TYPE_NORMAL
- en: Composing our stock trading application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with the architecture of our stock trading application. To set
    up an application that has a minimal level of complexity, we will create the following
    channels:'
  prefs: []
  type: TYPE_NORMAL
- en: An **outgoing** producer that's bound to the **"stock-quote"** channel, where
    messages containing stock orders will be written into a topic named ****"stocks"****
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **incoming** consumer that's bound to the **"stocks"** channel, which read
    messages that are available in the **"stocks"** topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **outgoing** producer that's bound to the **"in-memory-stream"** channel,
    which broadcasts the new stock quote to all the available subscribers internally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **incoming** consumer that's bound to the **"in-memory-stream"** channel,
    which reads the new stock quote and sends it as SSE to clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the basic stream of messages that will be used
    in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e02a25f9-1583-4127-bde7-748e0f12d06e.png)'
  prefs: []
  type: TYPE_IMG
- en: The example application can be found in the `Chapter10/kafka` folder of this
    book's GitHub repository. We recommend importing the project into your IDE before
    you move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the `pom.xml` file of this project, we have included the
    following extension so that we can stream messages to the Apache Kafka server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Before we dive into the code, we need to fulfill some requirements to run Kafka
    in a container. As we mentioned previously, Kafka needs ZooKeeper to manage its
    cluster, so we need to start both services. A practical solution you can use in
    the development or test environment is to use **Docker Compose**, which is a tool
    that's used to manage and synchronize multiple container applications in a single
    configuration file written in YAML format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation of Docker Compose is detailed on its documentation page ([https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)),
    but for a Linux machine, you can install a stable release of it with the following
    shell command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you''re done, apply for the right permission on the `docker-compose` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can verify the installed version, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we're done with our preliminary requirements, it's time to add some
    lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: Coding bean classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first class we will add is `QuoteGenerated`, which is an `ApplicationScoped`
    CDI bean that produces random quotes for a company every two seconds. Here''s
    the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This class produces messages that will be written to Kafka through the `"stock-quote"`
    channel. The message contains the stock order that is randomly generated through
    three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The type of order (sale/purchase)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The company name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of shares purchased/sold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the end of the day, the `generate` method will produce a message that contains
    a JSON string, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So that we have a better understanding of the accessory components, here is
    the `Company` enumeration, which contains the following set of companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need the core part of the `Operation` class, which is a Java POJO that
    holds the data of each stock order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, a brief Wall Street 101: each stock order will determine a change in the
    quotation of a company. Simply put, by selling stocks, the price of a company
    will decrease, while a buy order will make the stock more demanded, which means
    that the price will rise. The number of shares sold/purchased will eventually
    determine how much the price goes up and down.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `QuoteConverter` class will do the job of converting a stock
    order into a new quotation for the `Company` involved in the transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `init` method of this class simply bootstraps the initial quotation of every
    `Company` with some random values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `newQuote` method is at the heart of our transaction system. By reading
    the operation data contained in the JSON file, a new quote is generated using
    a basic algorithm: for any 25 stocks that are transacted, there will be one point''s
    impact on the value of the stock. The returned JSON string wraps the `Quote` class,
    which is broadcasted to all the matching subscribers of the `"in-memory-stream"`
    channel by means of the `@Broadcast` annotation being on top of the method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of completeness, we''ll also include the `Quote` Java class, which
    will be sent as JSON to the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Within our example, we have the following subscriber for the `"in-memory-stream"`
    channel, where `Quote` is published:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`QuoteEndpoint` is our REST endpoint. Within this, we are using the `@Channel`
    qualifier to inject the `"in-memory-stream"` channel into the bean. That''s exactly
    the point where the reactive world (governed by streams) unifies with the imperative
    world (the CDI bean, which executes code in sequence). Simply put, this is where
    our bean is able to retrieve channels that are managed by Reactive Messaging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the preceding components need a broker, which is where we publish the stock
    quotes and read them. Here is the `application.properties` file, which keeps all
    of these pieces together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first block is related to the Kafka destination, also known as a **sink**
    in streaming parlance, and is where we write the stock quote produced by `QuoteGenerator`.
    To replicate the data across the node of the classes, it is necessary to serialize
    its content. Byte streams are the standard language that the OS uses for I/O.
    In our case, since the data is in JSON format, we use `StringSerializer`.
  prefs: []
  type: TYPE_NORMAL
- en: In the second block, we configure the source topic and connector, where we read
    the stock quote as a JSON serialized stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all we need to do is add a client application that is able to capture
    the SSE and display the text of it in a nicely formatted table of data. For the
    sake of brevity, we will just add the core JavaScript function that collects the
    SSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will be included in the `index.html` page, which can be
    found in the source code of this chapter. Let''s see it in action! Before building
    the application, start the Kafka/ZooKeeper containers with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The Docker Compose tool will search for the `docker-compose.yaml` file, which
    is in the root directory of this example. Here, we have configured the Kafka and
    ZooKeeper containers so that they start. A successful bootstrap will produce the
    following output at the bottom of your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that the Kafka and ZooKeeper containers are up and running by
    executing the `docker ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will display the following active processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, bootstrap the application as usual with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The welcome page of the application (available at `http://localhost:8080`)
    will show the stock quotes ticker in action, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b47c63f-1243-4a6d-8287-ff3d5c75ba5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Each company in the list will start with an N/A quote until a random operation
    is executed on it. In the end, you will see that the preceding page is updated
    every two seconds, which is what we configured in the `QuoteGenerator` class.
    Pretty cool, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: 'When you''re done with this example, stop all the running containers with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the `docker-compose` process terminates, the preceding command will display
    a list of all the container layers that have been stopped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, verify that the Kafka and ZooKeeper containers have been stopped by executing
    the `docker ps` command once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command shouldn't produce any output, which means that no pending
    Docker process is running.
  prefs: []
  type: TYPE_NORMAL
- en: We've just got started with the Docker Compose tool. Now, let's move on and
    deploy the full application stack on OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming messages to Kafka in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete our next challenge, we strongly recommend using the latest OpenShift
    4.X version. As a matter of fact, in order to orchestrate multiple services such
    as Kafka and ZooKeeper, it is much simpler to use OpenShift release 4 as it is
    built on top of the concept of **Operators**. A Kubernetes Operator is a piece
    of software running in a Pod on the cluster, which introduces new object types
    through **Custom Resource Definitions** (**CRDs**). A CRD is nothing but an extension
    mechanism in Kubernetes that lets you define interfaces for a user; for example,
    you can define a CRD for a Kafka server, which provides a simpler way for us to
    configure and run it in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Operators already have a common directory ([https://operatorhub.io/](https://operatorhub.io/)),
    where you can find an existing Operator or add your own.
  prefs: []
  type: TYPE_NORMAL
- en: You can evaluate OpenShift 4 by going to [https://www.openshift.com/trial/](https://www.openshift.com/trial/).
    There, you can find several alternatives for evaluating OpenShift, either in the
    cloud or on your own machine. In this chapter, we are assuming that you have already
    completed the signup procedure and that you have OpenShift 4 up and running.
  prefs: []
  type: TYPE_NORMAL
- en: For the next project, please refer to the `Chapter10/kafka-openshift` directory,
    where you will find the stock trade application configured for OpenShift and the
    YAML files for setting up and configuring the Kafka cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kafka on OpenShift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest way to install and manage an Apache Kafka cluster on the OpenShift
    cluster is via the **Strimzi** project ([https://strimzi.io/](https://strimzi.io/)),
    which can be installed as an OpenShift Operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a new OpenShift project named `kafka-demo`. You can either
    create it from the admin console or using the `oc` command-line utility, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned output will confirm that the project namespace has been created
    in your virtual address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The server name will be different in your case, depending on the account name
    you chose when you signed in.
  prefs: []
  type: TYPE_NORMAL
- en: 'We recommend continuing from the OpenShift web-console. From the left-hand
    Administrator panel, select OperatorHub, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7eeb4369-78cc-41d0-a8e0-6c461bf12678.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The OperatorHub Catalog will show up in the main OpenShift dashboard. Select
    the **Strimzi** Operator, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d98b877c-6503-4e14-83f3-f12931abd19a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, in the following UI, choose to Install the Operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a434ae1e-ced9-405b-b7b9-a1f8ad0676ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, you will be able to choose whether you want to install the Operator in
    all the available namespaces of your cluster or just in a specific project. Since
    we won''t be using this Operator in other projects, just check the A specific
    namespace on the cluster option and pick up your project. Your selection should
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6928559a-0d2a-4f78-a26e-768bf838c3f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a few seconds, in the main panel, you will be notified that the Operator
    has been installed, along with all its provided APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d01d9dd-0e17-4636-ba5d-255a89f0e6ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that you have the Strimzi Operator available, installing the Kafka cluster
    will be a piece of cake! Within the `Chapter10/kafka-openshift/strimzi` folder,
    you will find the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kafka-cluster-descriptor.yaml`: This file contains a Kafka cluster definition
    based on the Strimzi Operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kafka-topic-queue-descriptor.yaml`: This file defines a resource (a Kafka
    topic) that we need to configure in our Kafka cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install both of them with the `oc` command. Let''s start from the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, wait a few seconds until the Kafka cluster is up and running. You can
    check the status of your Pods in the current project with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, wait until all the Pods are running, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful cluster setup will be made up of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Three Kafka cluster nodes in a running state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three ZooKeeper cluster nodes also in a running state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The name of the cluster (`my-kafka`) has been assigned within the `kafka-cluster-descriptor.yaml`
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s move on by adding a queue named `stock`, as defined in the `kafka-topic-queue-descriptor.yaml`
    folder. You can create it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want some insights into the Kafka cluster, you can check whether the
    topic is available. To do that, log in to any of the available Kafka nodes with
    `oc rsh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'By doing this, you will have access to the Terminal of that container. From
    there, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The minimal output you will see in the console is `stocks`, which is our topic
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To connect to the Kafka cluster, we won''t use an IP address or a Pod name
    (which will vary upon restarts). Instead, we will use the service name, which
    will let you reach the cluster through an alias. You can check the available service
    names with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command will be restricted to the `name` column.
    In our case, it will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The service name we are interested in is `my-kafka-kafka-bootstrap`, which we
    will add to our Quarkus project soon.
  prefs: []
  type: TYPE_NORMAL
- en: Shaping up our project for native cloud execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run our project on OpenShift, we will apply some minimal changes to the
    configuration file so that we can reach the Kafka service name we have just determined.
    In the following code, we have highlighted the changes that we have to apply to
    the `application.properties` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, in the preceding configuration, we have used the `bootstrap.servers`
    property to specify the list of Kafka servers (`host:port`).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple servers can be added in the configuration by using a comma to separate
    each entry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the source code of this example, you will also find that all POJO classes
    that have been serialized in the messaging stream are annotated with `@io.quarkus.runtime.annotations.RegisterForReflection`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As a matter of fact, when building a native executable, GraalVM does some assumptions
    to remove all the classes, methods, and fields that are not used directly in your
    code. The elements that are used via reflection are not part of the call tree,
    so they are candidates when it comes to eliminating them from the native executable.
    Since JSON libraries heavily rely on reflection to perform their job, we must
    explicitly tell GraalVM not to exclude them by using the `@RegisterForReflection`
    annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the small change we have applied in order to publish it to the cloud.
    Now, build and deploy the native application with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Please note that you can find the preceding script, that is, `deploy-openshift.sh`,
    in the `Chapter10/kafka-openshift` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the preceding script has been executed, verify that the `quarkus-kafka`
    Pod is up and running by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will confirm this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the route address as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The route will be under the `HOST/PORT` column output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to reach your application with just a click, head to the Administration
    console and select Networking | Routes. Then, click on the Route Location, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/802028dc-c9bd-4ca8-8832-550a3ca1ac8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the configured timeout for emitting quotes elapses, you will see the stock
    trading application in action on OpenShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f3994e6-0532-489d-b0a2-29653bbdc1da.png)'
  prefs: []
  type: TYPE_IMG
- en: We've come to the end of our glorious journey of Apache Kafka Streaming! In
    the next section, we will learn how to approach another candidate solution for
    streaming messaging, which is based on the AMQP protocol instead.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming messages with AMQP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have only just found Quarkus after years in the Java Enterprise community,
    you will already be familiar with message brokers, which are used to allow different
    Java applications to communicate using JMS as a standard protocol. Although JMS
    is a robust and mature solution for implementing a messaging system, one of the
    main limitations of it is that it's focused exclusively on Java. In the microservices
    world, it's fairly common to use different languages to compose the overall system
    architecture, so a platform-independent solution is required. In this context,
    AMQP offers a set of advantages that make it a perfect fit for implementing the
    Reactive Streams API when it comes to microservices in a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following are some of the main features of the AMQP protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides a platform-independent wire-level messaging protocol that allows
    interoperability across multiple languages and platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a wire-level protocol, by which data is sent across the network as a stream
    of bytes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can achieve high performance while working at low-level byte streams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports long-lived messaging and classic message queues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports distribution patterns such as round-robin (where the load is equally
    distributed across servers) and store and forward (where messages are stored at
    the sender side in a persistence store and are then forwarded to the receiver
    side).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides support for transactions (across message destination), as well as
    distributed transactions using common standards (XA, X/Open, MS DTC).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides support for data encryption using SASL and TLS protocols.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows us to control the message flow with metadata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides flow control of messages to control backpressure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To let our applications interact with AMQP, we need a broker that supports this
    protocol. A commonly adopted solution in the Java Enterprise is **Apache Artemis
    ActiveMQ** ([https://activemq.apache.org/components/artemis/](https://activemq.apache.org/components/artemis/)),
    which is compatible with Java Enterprise **Message-Oriented Middleware** (**MOM**)
    as well. In the next section, we will learn how to start it and configure it in
    our stock quotes application.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the AMQP broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to kick-start our application as quickly as possible, we will use a
    Docker Compose script. This will download a suitable version of the message broker
    and set some environment variables that are needed so that we can reach the broker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply start the `docker-compose.yaml` file contained in the `amqp` folder
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If the startup is successful, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that the Kafka and ZooKeeper containers are up and running by
    executing the `docker ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will display the following active processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s configure our application so that it can use ActiveMQ. You will
    find the updated application in the `Chapter10/amqp` folder of this book''s GitHub
    repository. First of all, we need to replace Kafka''s Reactive Messaging dependency
    with the AMQP Reactive Messaging dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of application configuration, some changes need to be made to our
    `application.properties` file. First of all, we need to include the username and
    password that we have set in `docker-compose.yaml` (`quarkus/quarkus`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to configure the AMQP connector so that we can write to the `stock-quote`
    queue, by specifying that the queue is durable (for example, persists to disk
    and survives a broker restart):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Conversely, we need to configure the AMQP connector so that it can read from
    the `stocks` queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can bootstrap the application as usual with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The welcome page of the application (available at `http://localhost:8080`)
    will show the stock quotes ticker in action, which now uses ActiveMQ as its broker.
    As you can see, a minimal adjustment has been made to the UI (just the title),
    but it accurately masks the changes that were made under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ef0a228-fb93-4f34-94fe-41939a735c63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can find out more about this process by logging in to the AMQ management
    console, which is available at `http://localhost:8161/console`. Once you''ve logged
    in with your configured credentials (`quarkus/quarkus`), you can check that the
    destination queue has been created in the list of available addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30854435-e1e8-475b-a47c-0950985f5de4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By selecting the `stocks` destination, you can check any further details in
    the main panel of the management console, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02058696-2aea-4437-a369-14cf21332b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When you''re done, stop all the running containers with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will display a list of all the container layers that
    have been stopped, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, verify that the ActiveMQ containers have been stopped by executing the
    `docker ps` command once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command shouldn't produce any output. Now, let's test the same
    application stack in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming messages to AMQ in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last thing we will do is deploy the Quarkus application to the cloud while
    using AMQ as a messaging broker. For this purpose, we will plug the ActiveMQ Docker
    image that we tested previously into OpenShift (more details about this image
    can be found on GitHub at [https://github.com/vromero/activemq-artemis-docker](https://github.com/vromero/activemq-artemis-docker)).
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, create a new project named `amq-demo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will confirm that the project namespace has been created in your
    virtual address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, deploy the AMQ server to your project with the following command, which
    will set the username and password so that you can access the broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Take note of the `RESTORE_CONFIGURATION=true` environment variable. This is
    needed since OpenShift automounts empty volumes in all the declared volumes. Since
    this behavior impacts the `/etc` folder of this image, which is where the configuration
    is stored, we need to set the `RESTORE_CONFIGURATION` environment variable to
    `true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the `new-app` command, the following output will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the status of the Pods with the `oc` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output confirms that the `artemis` Pod is in a running state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s check the service name, which will be `artemis`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that the output that''s returned matches the output shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s go for the final kill: we will deploy the application located in
    the `Chapter10/amqp-openshift` directory. Within this folder, you will find the
    stock trade application, which has been configured to stream messages on AMQ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the updated `application.properties` file, which contains the AMQ username
    and password, along with the host and the port where the service runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will deploy the application contained in the same folder (`Chapter10/amqp-openshift`)
    into OpenShift. For convenience, you can simply run the `deploy-openshift.sh`
    script, which can be found in the same directory. Here is the content of the script,
    which should be pretty familiar to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, check that the `quarkus-amq` Pod is in a running state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The output that you''ll receive confirms this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can verify that the application works by clicking on the route address.
    Just go to the Networking | Routes path in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8e60ed7-837e-45d8-abc0-2c278f101eec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output will be pretty much the same, except for the route name, which celebrates
    your last achievement in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51eed0a9-3623-488e-ad76-d2c252a90e98.png)'
  prefs: []
  type: TYPE_IMG
- en: That's all, folks!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use CDI beans to produce, consume, and process
    messages using the Reactive Messaging specification. We also learned how to bootstrap
    and configure Apache Kafka and Active MQ's broker so that it acts as a distributed
    streaming platform for our CDI Beans. To put our new skills in place, we created
    an example stock trade application, which was initially run in development mode
    and then deployed as a native image on top of OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have reached the end of this book, where we learned about the story
    of the gradual renewal of the Java Enterprise application, from monolith to native
    microservices running in the cloud. It was an exciting journey that certainly
    set a milestone, but this isn't the end of our hard work – it's just the end of
    this story.
  prefs: []
  type: TYPE_NORMAL
