- en: Reactive Messaging with Quarkus
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Quarkus 进行响应式消息传递
- en: In this chapter, we will learn about the nuts and bolts of **SmallRye Reactive
    Messaging**, which can be used in Quarkus to implement the Eclipse MicroProfile
    Reactive Messaging specification. By the end of this chapter, you will have a
    solid development model for your data streaming applications and know how to connect
    to streaming platforms such as **Apache Kafka** and **ActiveMQ**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习 SmallRye 响应式消息传递的细节，它可以在 Quarkus 中用于实现 Eclipse MicroProfile 响应式消息传递规范。到本章结束时，您将拥有一个坚实的数据流应用程序开发模型，并了解如何连接到
    Apache Kafka 和 ActiveMQ 等流平台。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Getting started with Reactive Messaging
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用响应式消息传递
- en: Streaming messages with Apache Kafka
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Kafka 流式传输消息
- en: Streaming messages with **Advanced Message Queuing Protocol** (**AMQP**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **高级消息队列协议**（**AMQP**）流式传输消息
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the source code for the project in this chapter on GitHub at [https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上的本章中找到项目的源代码：[https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10)。
- en: Getting started with Reactive Messaging
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用响应式消息传递
- en: Reactive Streams is an initiative that aims to provide a standard for exchanging
    data streams across an asynchronous boundary. At the same time, it guarantees
    that the receiving side is not forced to buffer arbitrary amounts of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 响应式流是一个旨在提供跨异步边界交换数据流标准的倡议。同时，它保证接收方不会被强制缓冲任意数量的数据。
- en: There are several available implementations of Reactive Stream, and we have
    already learned how you can feature Reactive Programming in Vert.x. In this chapter,
    we will complement our knowledge using the SmallRye Reactive Messaging implementation
    to show you how we can integrate it with a streaming platform such as Apache Kafka
    or a messaging broker such as ActiveMQ with minimal configuration changes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可用的响应式流实现，我们已经学习了如何在 Vert.x 中实现响应式编程。在本章中，我们将使用 SmallRye 响应式消息传递实现来补充我们的知识，向您展示如何通过最小配置更改将其与
    Apache Kafka 或 ActiveMQ 等流平台或消息代理集成。
- en: To familiarize ourselves with MicroProfile Reactive Messaging, we need proper
    knowledge of some key concepts. First of all, MicroProfile Reactive Messaging
    is a specification that uses CDI beans to drive the flow of messages toward some
    specific channels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉 MicroProfile 响应式消息传递，我们需要了解一些关键概念。首先，MicroProfile 响应式消息传递是一个使用 CDI 容器来驱动消息流向特定通道的规范。
- en: A message is a basic interface that contains a payload to be streamed. The `Message`
    interface is parameterized in order to describe the type of payload it contains.
    Additionally, a `Message` interface contains attributes and metadata that are
    specific to the broker that's used for message exchange (for example, Kafka or
    AMQ).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 消息是一个基本接口，包含要流式传输的有效负载。`Message` 接口是参数化的，以便描述它包含的有效负载类型。此外，`Message` 接口还包含特定于用于消息交换的代理（例如
    Kafka 或 AMQ）的属性和元数据。
- en: 'A channel, on the other hand, is a string indicating which source or destination
    of messages is used. There are two types of channels:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通道是一个字符串，指示使用哪个消息的源或目的地。有两种类型的通道：
- en: Internal channels are local to the application and are used to implement a multi-step
    process for messages.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部通道位于应用程序内部，用于实现消息的多步骤处理过程。
- en: Remote channels are connected to remote brokers (such as Apache Kafka or AMQ)
    through connectors.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 远程通道通过连接器连接到远程代理（例如 Apache Kafka 或 AMQ）。
- en: 'Since MicroProfile Reactive Messaging is fully governed by the CDI model, two
    core annotations are used to indicate whether a method is a producer or a consumer
    of messages:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 MicroProfile 响应式消息传递完全受 CDI 模型管理，因此使用两个核心注解来指示方法是否是消息的生产者或消费者：
- en: '`@Incoming`**:** This annotation is used on a method to indicate that it consumes
    messages from the specified channel. The name of the channel is added to the annotation
    as an attribute. Here is an example:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@Incoming`**:** 该注解用于方法上，表示它从指定的通道消费消息。通道的名称作为属性添加到注解中。以下是一个示例：'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The effect of placing this annotation on a method is that the method will be
    called each time a message is sent to that channel. From the user''s perspective,
    it is totally transparent about whether the incoming message arrives from a collocated
    CDI bean or a remote broker. However, you may decide to make it clear that the
    method consumes a specific kind of message, such as `KafkaMessage` (which inherits
    from `Message`). Here is an example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将此注解放置在方法上的效果是，每次向该通道发送消息时都会调用该方法。从用户的角度来看，它对传入消息是否来自本地 CDI 实例或远程代理是完全透明的。然而，您可能决定明确指出该方法消费特定类型的消息，例如
    `KafkaMessage`（它继承自 `Message`）。以下是一个示例：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`@Outgoing`: This annotation indicates that a method publishes messages to
    a channel. In much the same way, the name of the channel is stated in the annotation''s
    attribute:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@Outgoing`：这个注解表示一个方法向一个通道发布消息。在许多方面，通道的名称都在注解的属性中声明：'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Within the method annotated with `@Outgoing`, we return a concrete implementation
    of the `Message` interface.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在注解了 `@Outgoing` 的方法中，我们返回 `Message` 接口的具体实现。
- en: Be aware that you can only annotate a single method with `@Outgoing` for one
    channel. If you attempt to use the same channel in more than one `@Outgoing` annotated
    method, an error will be emitted at deployment time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您只能为单个通道使用 `@Outgoing` 注解一个方法。如果您尝试在多个 `@Outgoing` 注解的方法中使用相同的通道，则在部署时将发出错误。
- en: 'You can also annotate a method with both `@Incoming` and `@Outgoing` so that
    it behaves like a **message processor**, which transforms the content of the message
    data:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `@Incoming` 和 `@Outgoing` 同时注解一个方法，使其表现得像一个**消息处理器**，它转换消息数据的内容：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From the preceding examples, we can see that messages flow from an `@Outgoing`
    stream producer to an `@Incoming` stream consumer and Reactive Messaging transparently
    connects the two endpoints. In order to decouple `Producer` and `Consumer` messages,
    you can add a component such as Apache Kafka by using the connectors provided
    by MicroProfile API. In the next section, we will introduce our first example
    of Reactive Messaging using Apache Kafka.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的示例中，我们可以看到消息从 `@Outgoing` 流生产者流向 `@Incoming` 流消费者，并且反应式消息透明地连接了这两个端点。为了解耦
    `Producer` 和 `Consumer` 消息，您可以使用 MicroProfile API 提供的连接器添加一个组件，如 Apache Kafka。在下一节中，我们将介绍使用
    Apache Kafka 的第一个反应式消息示例。
- en: Streaming messages with Apache Kafka
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Kafka 进行流消息
- en: Apache Kafka ([https://kafka.apache.org/](https://kafka.apache.org/)) is a distributed
    data streaming platform that can be used to publish, subscribe, store, and process
    streams of data from multiple sources in real time at amazing speeds.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka ([https://kafka.apache.org/](https://kafka.apache.org/)) 是一个分布式数据流平台，可以用于以惊人的速度实时发布、订阅、存储和处理来自多个源的数据流。
- en: Apache Kafka can be plugged into streaming data pipelines that distribute data
    between systems, and also into the systems and applications that consume that
    data. Since Apache Kafka reduces the need for point-to-point integrations for
    data sharing, it is a perfect fit for a range of use cases where high throughput
    and scalability are vital.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 可以集成到流数据管道中，这些管道在系统之间分配数据，也可以集成到消费这些数据的系统和应用程序中。由于 Apache Kafka
    减少了数据共享的点对点集成需求，因此它非常适合需要高吞吐量和可扩展性的各种用例。
- en: 'Additionally, once you combine Kafka with Kubernetes, you attain all the benefits
    of Kafka, as well as the advantages of Kubernetes, such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦将 Kafka 与 Kubernetes 结合使用，您将获得 Kafka 的所有优势，以及 Kubernetes 的以下优势：
- en: '**Scalability and high availability**: You can easily scale up and down resources
    with Kubernetes, which means you can automatically determine the pool of resources
    that Apache Kafka will share with other applications while guaranteeing the high
    availability of your Kafka cluster at the same time.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和高可用性**：您可以使用 Kubernetes 轻松地进行资源的扩展和缩减，这意味着您可以在保证 Kafka 集群高可用性的同时，自动确定
    Apache Kafka 将与其他应用程序共享的资源池。'
- en: '**Portability**: By running Kafka with Kubernetes, your cluster of Kafka nodes
    can span across on-site and public, private, or hybrid clouds, even using different
    operating systems.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：通过在 Kubernetes 上运行 Kafka，您的 Kafka 节点集群可以跨越本地和公有、私有或混合云，甚至可以使用不同的操作系统。'
- en: To manage the Kafka environment, you need a piece of software called **ZooKeeper**,
    which manages naming and configuring data in order to provide flexible and robust
    synchronization within distributed systems. ZooKeeper controls the status of the
    Kafka cluster nodes and also keeps track of Kafka topics, partitions, and all
    the Kafka services you need. We won't cover the details of ZooKeeper administration
    in this chapter, although it's worth mentioning its role as you will need to get
    to grips with it in order to land a Kafka Administrator job.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要管理 Kafka 环境，你需要一个名为 **ZooKeeper** 的软件，它负责命名和配置数据，以便在分布式系统中提供灵活且强大的同步。ZooKeeper
    控制着 Kafka 集群节点的状态，并跟踪 Kafka 主题、分区以及你需要的所有 Kafka 服务。虽然在本章中不会详细介绍 ZooKeeper 的管理细节，但值得一提的是它在
    Kafka 管理员职位中的角色，因为你需要掌握它才能胜任这份工作。
- en: To demonstrate Apache Kafka and MicroProfile Streaming's powerful combination
    on Quarkus, we will design a simple application that simulates a stock trading
    ticker that's updated in real time by purchases and sales. Get ready and open
    for business!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 Apache Kafka 和 MicroProfile Streaming 在 Quarkus 上的强大组合，我们将设计一个简单的应用程序，该程序模拟实时更新的股票交易行情，通过购买和销售进行更新。准备好并开始营业吧！
- en: Composing our stock trading application
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组装我们的股票交易应用程序
- en: 'Let''s start with the architecture of our stock trading application. To set
    up an application that has a minimal level of complexity, we will create the following
    channels:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的股票交易应用程序的架构开始。为了设置一个具有最小复杂度的应用程序，我们将创建以下通道：
- en: An **outgoing** producer that's bound to the **"stock-quote"** channel, where
    messages containing stock orders will be written into a topic named ****"stocks"****
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个绑定到 **"stock-quote"** 通道的 **流出** 生产者，其中包含股票订单的消息将被写入名为 ****"stocks"**** 的主题
- en: An **incoming** consumer that's bound to the **"stocks"** channel, which read
    messages that are available in the **"stocks"** topic
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个绑定到 **"stocks"** 通道的 **流入** 消费者，它读取 **"stocks"** 主题中可用的消息
- en: An **outgoing** producer that's bound to the **"in-memory-stream"** channel,
    which broadcasts the new stock quote to all the available subscribers internally
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个绑定到 **"in-memory-stream"** 通道的 **流出** 生产者，它将新的股票报价广播给所有内部可用的订阅者
- en: An **incoming** consumer that's bound to the **"in-memory-stream"** channel,
    which reads the new stock quote and sends it as SSE to clients
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个绑定到 **"in-memory-stream"** 通道的 **流入** 消费者，它读取新的股票报价并将其作为 SSE 发送给客户端
- en: 'The following diagram depicts the basic stream of messages that will be used
    in our example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了我们将用于示例的基本消息流：
- en: '![](img/e02a25f9-1583-4127-bde7-748e0f12d06e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e02a25f9-1583-4127-bde7-748e0f12d06e.png)'
- en: The example application can be found in the `Chapter10/kafka` folder of this
    book's GitHub repository. We recommend importing the project into your IDE before
    you move on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 示例应用程序可以在本书 GitHub 存储库的 `Chapter10/kafka` 文件夹中找到。我们建议在继续之前将项目导入到您的 IDE 中。
- en: 'As you can see from the `pom.xml` file of this project, we have included the
    following extension so that we can stream messages to the Apache Kafka server:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从本项目的 `pom.xml` 文件中看到的，我们已包含以下扩展，以便我们可以将消息流式传输到 Apache Kafka 服务器：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Before we dive into the code, we need to fulfill some requirements to run Kafka
    in a container. As we mentioned previously, Kafka needs ZooKeeper to manage its
    cluster, so we need to start both services. A practical solution you can use in
    the development or test environment is to use **Docker Compose**, which is a tool
    that's used to manage and synchronize multiple container applications in a single
    configuration file written in YAML format.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，我们需要满足一些要求才能在容器中运行 Kafka。正如我们之前提到的，Kafka 需要 ZooKeeper 来管理其集群，因此我们需要启动这两个服务。在开发或测试环境中，你可以使用一个实用的解决方案，即
    **Docker Compose**，这是一个用于管理和同步单个 YAML 格式配置文件中多个容器应用的工具。
- en: 'The installation of Docker Compose is detailed on its documentation page ([https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)),
    but for a Linux machine, you can install a stable release of it with the following
    shell command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Compose 的安装细节可以在其文档页面上找到 ([https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/))，但对于
    Linux 机器，你可以使用以下 shell 命令安装其稳定版本：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When you''re done, apply for the right permission on the `docker-compose` tool:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，请对 `docker-compose` 工具申请适当的权限：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, you can verify the installed version, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以按照以下方式验证已安装的版本：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should see the following output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we're done with our preliminary requirements, it's time to add some
    lines of code!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们完成了初步需求，是时候添加一些代码行了！
- en: Coding bean classes
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码 bean 类
- en: 'The first class we will add is `QuoteGenerated`, which is an `ApplicationScoped`
    CDI bean that produces random quotes for a company every two seconds. Here''s
    the code for this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将添加的第一个类是 `QuoteGenerated`，它是一个 `ApplicationScoped` CDI bean，每两秒为一家公司生成随机报价。以下是这个类的代码：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This class produces messages that will be written to Kafka through the `"stock-quote"`
    channel. The message contains the stock order that is randomly generated through
    three parameters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类通过 `"stock-quote"` 通道生成将被写入 Kafka 的消息。消息包含通过三个参数随机生成的股票订单：
- en: The type of order (sale/purchase)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订单类型（卖出/买入）
- en: The company name
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司名称
- en: The number of shares purchased/sold
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卖出/买入的股票数量
- en: 'At the end of the day, the `generate` method will produce a message that contains
    a JSON string, similar to the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到了最后，`generate` 方法将生成一个包含 JSON 字符串的消息，类似于以下内容：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So that we have a better understanding of the accessory components, here is
    the `Company` enumeration, which contains the following set of companies:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解辅助组件，以下是 `Company` 枚举，其中包含以下公司集合：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We also need the core part of the `Operation` class, which is a Java POJO that
    holds the data of each stock order:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要 `Operation` 类的核心部分，它是一个 Java POJO，用于存储每个股票订单的数据：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, a brief Wall Street 101: each stock order will determine a change in the
    quotation of a company. Simply put, by selling stocks, the price of a company
    will decrease, while a buy order will make the stock more demanded, which means
    that the price will rise. The number of shares sold/purchased will eventually
    determine how much the price goes up and down.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，简要介绍一下华尔街 101：每个股票订单将决定一家公司的报价变化。简单来说，通过卖出股票，公司的价格会下降，而买入订单会使股票需求增加，这意味着价格会上涨。卖出的/买入的股票数量最终将决定价格上升和下降的幅度。
- en: 'The following `QuoteConverter` class will do the job of converting a stock
    order into a new quotation for the `Company` involved in the transaction:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 `QuoteConverter` 类将负责将股票订单转换为交易涉及的 `Company` 的新报价：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `init` method of this class simply bootstraps the initial quotation of every
    `Company` with some random values.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的 `init` 方法只是用一些随机值启动每个 `Company` 的初始报价。
- en: 'The `newQuote` method is at the heart of our transaction system. By reading
    the operation data contained in the JSON file, a new quote is generated using
    a basic algorithm: for any 25 stocks that are transacted, there will be one point''s
    impact on the value of the stock. The returned JSON string wraps the `Quote` class,
    which is broadcasted to all the matching subscribers of the `"in-memory-stream"`
    channel by means of the `@Broadcast` annotation being on top of the method.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`newQuote` 方法是我们交易系统的核心。通过读取 JSON 文件中包含的操作数据，使用基本算法生成一个新的报价：对于任何交易的 25 只股票，将对股票的价值产生一个点的影响。返回的
    JSON 字符串封装了 `Quote` 类，通过方法顶部的 `@Broadcast` 注解，将消息广播到 `"in-memory-stream"` 通道的所有匹配订阅者。'
- en: 'For the sake of completeness, we''ll also include the `Quote` Java class, which
    will be sent as JSON to the client:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们还将包括 `Quote` Java 类，它将被作为 JSON 发送到客户端：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Within our example, we have the following subscriber for the `"in-memory-stream"`
    channel, where `Quote` is published:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们有一个 `"in-memory-stream"` 通道的以下订阅者，其中 `Quote` 被发布：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`QuoteEndpoint` is our REST endpoint. Within this, we are using the `@Channel`
    qualifier to inject the `"in-memory-stream"` channel into the bean. That''s exactly
    the point where the reactive world (governed by streams) unifies with the imperative
    world (the CDI bean, which executes code in sequence). Simply put, this is where
    our bean is able to retrieve channels that are managed by Reactive Messaging.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`QuoteEndpoint` 是我们的 REST 端点。在这个端点中，我们使用 `@Channel` 限定符将 `"in-memory-stream"`
    通道注入到 bean 中。这正是反应式世界（由流控制）与命令式世界（CDI bean，按顺序执行代码）统一的地方。简单来说，这就是我们的 bean 能够检索由反应式消息管理的通道的地方。'
- en: 'All the preceding components need a broker, which is where we publish the stock
    quotes and read them. Here is the `application.properties` file, which keeps all
    of these pieces together:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的先前组件都需要一个经纪人，这是发布股票报价并读取它们的地方。以下是 `application.properties` 文件，它将这些组件组合在一起：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first block is related to the Kafka destination, also known as a **sink**
    in streaming parlance, and is where we write the stock quote produced by `QuoteGenerator`.
    To replicate the data across the node of the classes, it is necessary to serialize
    its content. Byte streams are the standard language that the OS uses for I/O.
    In our case, since the data is in JSON format, we use `StringSerializer`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个块与Kafka目标相关，在流式通信中通常称为**sink**，这是我们写入由`QuoteGenerator`产生的股票报价的地方。为了在类的节点之间复制数据，有必要序列化其内容。字节流是操作系统用于I/O的标准语言。在我们的情况下，由于数据是JSON格式，我们使用`StringSerializer`。
- en: In the second block, we configure the source topic and connector, where we read
    the stock quote as a JSON serialized stream.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个块中，我们配置了源主题和连接器，其中我们以JSON序列化流的形式读取股票报价。
- en: 'Now, all we need to do is add a client application that is able to capture
    the SSE and display the text of it in a nicely formatted table of data. For the
    sake of brevity, we will just add the core JavaScript function that collects the
    SSE:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要添加一个能够捕获SSE并将其文本以格式化的表格形式显示的客户端应用程序。为了简洁起见，我们只添加收集SSE的核心JavaScript函数：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code will be included in the `index.html` page, which can be
    found in the source code of this chapter. Let''s see it in action! Before building
    the application, start the Kafka/ZooKeeper containers with the following command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将被包含在`index.html`页面中，该页面位于本章的源代码中。让我们看看它的实际效果！在构建应用程序之前，使用以下命令启动Kafka/ZooKeeper容器：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Docker Compose tool will search for the `docker-compose.yaml` file, which
    is in the root directory of this example. Here, we have configured the Kafka and
    ZooKeeper containers so that they start. A successful bootstrap will produce the
    following output at the bottom of your console:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Compose工具将搜索根目录中的`docker-compose.yaml`文件。在这里，我们已配置Kafka和ZooKeeper容器以便它们启动。成功的启动将在控制台底部产生以下输出：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can verify that the Kafka and ZooKeeper containers are up and running by
    executing the `docker ps` command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过执行`docker ps`命令来验证Kafka和ZooKeeper容器是否正在运行：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding command will display the following active processes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将显示以下活动进程：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, bootstrap the application as usual with the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下命令以常规方式启动应用程序：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The welcome page of the application (available at `http://localhost:8080`)
    will show the stock quotes ticker in action, as shown in the following screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的欢迎页面（可在`http://localhost:8080`访问）将显示正在运行的股票报价行情，如下截图所示：
- en: '![](img/4b47c63f-1243-4a6d-8287-ff3d5c75ba5e.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b47c63f-1243-4a6d-8287-ff3d5c75ba5e.png)'
- en: Each company in the list will start with an N/A quote until a random operation
    is executed on it. In the end, you will see that the preceding page is updated
    every two seconds, which is what we configured in the `QuoteGenerator` class.
    Pretty cool, isn't it?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的每家公司都将开始于一个N/A报价，直到对其执行随机操作。最后，你会看到前面的页面每两秒更新一次，这是我们配置在`QuoteGenerator`类中的。很酷，不是吗？
- en: 'When you''re done with this example, stop all the running containers with the
    following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成这个示例后，使用以下命令停止所有正在运行的容器：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once the `docker-compose` process terminates, the preceding command will display
    a list of all the container layers that have been stopped:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`docker-compose`进程终止，上述命令将显示所有已停止的容器层列表：
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, verify that the Kafka and ZooKeeper containers have been stopped by executing
    the `docker ps` command once more:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过再次执行`docker ps`命令来验证Kafka和ZooKeeper容器是否已停止：
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding command shouldn't produce any output, which means that no pending
    Docker process is running.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令不应产生任何输出，这意味着没有挂起的Docker进程正在运行。
- en: We've just got started with the Docker Compose tool. Now, let's move on and
    deploy the full application stack on OpenShift.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开始使用Docker Compose工具。现在，让我们继续并在OpenShift上部署完整的应用程序堆栈。
- en: Streaming messages to Kafka in the cloud
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云中向Kafka发送流消息
- en: To complete our next challenge, we strongly recommend using the latest OpenShift
    4.X version. As a matter of fact, in order to orchestrate multiple services such
    as Kafka and ZooKeeper, it is much simpler to use OpenShift release 4 as it is
    built on top of the concept of **Operators**. A Kubernetes Operator is a piece
    of software running in a Pod on the cluster, which introduces new object types
    through **Custom Resource Definitions** (**CRDs**). A CRD is nothing but an extension
    mechanism in Kubernetes that lets you define interfaces for a user; for example,
    you can define a CRD for a Kafka server, which provides a simpler way for us to
    configure and run it in our cluster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们接下来的挑战，我们强烈建议使用最新的OpenShift 4.X版本。事实上，为了编排多个服务，如Kafka和ZooKeeper，使用基于**Operator**概念的OpenShift
    4版本要简单得多。Kubernetes Operator是在集群上的Pod中运行的软件片段，它通过**自定义资源定义**（**CRDs**）引入新的对象类型。CRD不过是Kubernetes中的一个扩展机制，它允许您为用户定义接口；例如，您可以定义一个用于Kafka服务器的CRD，这为我们提供了一个更简单的方法来配置和在我们的集群中运行它。
- en: In addition, Operators already have a common directory ([https://operatorhub.io/](https://operatorhub.io/)),
    where you can find an existing Operator or add your own.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Operator已经有一个公共目录([https://operatorhub.io/](https://operatorhub.io/))，您可以在其中找到现有的Operator或添加您自己的。
- en: You can evaluate OpenShift 4 by going to [https://www.openshift.com/trial/](https://www.openshift.com/trial/).
    There, you can find several alternatives for evaluating OpenShift, either in the
    cloud or on your own machine. In this chapter, we are assuming that you have already
    completed the signup procedure and that you have OpenShift 4 up and running.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问[https://www.openshift.com/trial/](https://www.openshift.com/trial/)来评估OpenShift
    4。在那里，您可以找到评估OpenShift的几种选择，无论是在云中还是在您的机器上。在本章中，我们假设您已经完成了注册流程，并且OpenShift 4已经启动并运行。
- en: For the next project, please refer to the `Chapter10/kafka-openshift` directory,
    where you will find the stock trade application configured for OpenShift and the
    YAML files for setting up and configuring the Kafka cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个项目，请参考`Chapter10/kafka-openshift`目录，在那里您将找到为OpenShift配置的股票交易应用程序以及设置和配置Kafka集群的YAML文件。
- en: Installing Kafka on OpenShift
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenShift上安装Kafka
- en: The simplest way to install and manage an Apache Kafka cluster on the OpenShift
    cluster is via the **Strimzi** project ([https://strimzi.io/](https://strimzi.io/)),
    which can be installed as an OpenShift Operator.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenShift集群上安装和管理工作Apache Kafka集群的最简单方法是使用**Strimzi**项目([https://strimzi.io/](https://strimzi.io/))，该项目可以作为OpenShift
    Operator安装。
- en: 'Start by creating a new OpenShift project named `kafka-demo`. You can either
    create it from the admin console or using the `oc` command-line utility, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个名为`kafka-demo`的新OpenShift项目。您可以从管理员控制台创建它，或者使用`oc`命令行工具，如下所示：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The returned output will confirm that the project namespace has been created
    in your virtual address:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的输出将确认项目命名空间已创建在您的虚拟地址中：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The server name will be different in your case, depending on the account name
    you chose when you signed in.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器名称将根据您在登录时选择的账户名称而有所不同。
- en: 'We recommend continuing from the OpenShift web-console. From the left-hand
    Administrator panel, select OperatorHub, as shown in the following screenshot:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议从OpenShift web-console继续操作。从左侧管理员面板中选择OperatorHub，如下截图所示：
- en: '![](img/7eeb4369-78cc-41d0-a8e0-6c461bf12678.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7eeb4369-78cc-41d0-a8e0-6c461bf12678.png)'
- en: 'The OperatorHub Catalog will show up in the main OpenShift dashboard. Select
    the **Strimzi** Operator, as shown in the following screenshot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OperatorHub目录将在主OpenShift仪表板中显示。选择**Strimzi** Operator，如下截图所示：
- en: '![](img/d98b877c-6503-4e14-83f3-f12931abd19a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d98b877c-6503-4e14-83f3-f12931abd19a.png)'
- en: 'Then, in the following UI, choose to Install the Operator:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在以下UI中，选择安装Operator：
- en: '![](img/a434ae1e-ced9-405b-b7b9-a1f8ad0676ca.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a434ae1e-ced9-405b-b7b9-a1f8ad0676ca.png)'
- en: 'Next, you will be able to choose whether you want to install the Operator in
    all the available namespaces of your cluster or just in a specific project. Since
    we won''t be using this Operator in other projects, just check the A specific
    namespace on the cluster option and pick up your project. Your selection should
    look as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将能够选择是否要在集群的所有可用命名空间中安装Operator，或者只在特定的项目中安装。由于我们不会在其他项目中使用此Operator，只需选中“集群上的特定命名空间”选项并选择您的项目。您的选择应如下所示：
- en: '![](img/6928559a-0d2a-4f78-a26e-768bf838c3f0.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6928559a-0d2a-4f78-a26e-768bf838c3f0.png)'
- en: 'After a few seconds, in the main panel, you will be notified that the Operator
    has been installed, along with all its provided APIs:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，在主面板中，您将收到通知，Operator已安装，以及所有提供的API：
- en: '![](img/4d01d9dd-0e17-4636-ba5d-255a89f0e6ff.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d01d9dd-0e17-4636-ba5d-255a89f0e6ff.png)'
- en: 'Now that you have the Strimzi Operator available, installing the Kafka cluster
    will be a piece of cake! Within the `Chapter10/kafka-openshift/strimzi` folder,
    you will find the following files:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了Strimzi Operator，安装Kafka集群将变得轻而易举！在`Chapter10/kafka-openshift/strimzi`文件夹中，您将找到以下文件：
- en: '`kafka-cluster-descriptor.yaml`: This file contains a Kafka cluster definition
    based on the Strimzi Operator.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kafka-cluster-descriptor.yaml`：此文件包含基于Strimzi Operator的Kafka集群定义。'
- en: '`kafka-topic-queue-descriptor.yaml`: This file defines a resource (a Kafka
    topic) that we need to configure in our Kafka cluster.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kafka-topic-queue-descriptor.yaml`：此文件定义了一个资源（一个Kafka主题），我们需要在我们的Kafka集群中进行配置。'
- en: 'You can install both of them with the `oc` command. Let''s start from the cluster:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`oc`命令安装它们两个。让我们从集群开始：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, wait a few seconds until the Kafka cluster is up and running. You can
    check the status of your Pods in the current project with the following command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，等待几秒钟，直到Kafka集群启动并运行。您可以使用以下命令检查当前项目中Pod的状态：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, wait until all the Pods are running, as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，等待直到所有Pod都在运行，如下所示：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'A successful cluster setup will be made up of the following components:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个成功的集群设置将包括以下组件：
- en: Three Kafka cluster nodes in a running state
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个Kafka集群节点处于运行状态
- en: Three ZooKeeper cluster nodes also in a running state
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个ZooKeeper集群节点也处于运行状态
- en: 'The name of the cluster (`my-kafka`) has been assigned within the `kafka-cluster-descriptor.yaml`
    file, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的名称（`my-kafka`）已经在`kafka-cluster-descriptor.yaml`文件中指定，如下所示：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let''s move on by adding a queue named `stock`, as defined in the `kafka-topic-queue-descriptor.yaml`
    folder. You can create it with the following command:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过添加一个名为`stock`的队列来继续，该队列在`kafka-topic-queue-descriptor.yaml`文件夹中定义。您可以使用以下命令创建它：
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You will see the following output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you want some insights into the Kafka cluster, you can check whether the
    topic is available. To do that, log in to any of the available Kafka nodes with
    `oc rsh`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想对Kafka集群有一些了解，您可以检查主题是否可用。为此，使用`oc rsh`登录到任何可用的Kafka节点：
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'By doing this, you will have access to the Terminal of that container. From
    there, execute the following command:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，您将能够访问该容器的终端。从那里，执行以下命令：
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The minimal output you will see in the console is `stocks`, which is our topic
    name:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台中最小的输出是`stocks`，这是我们的主题名称：
- en: '[PRE37]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To connect to the Kafka cluster, we won''t use an IP address or a Pod name
    (which will vary upon restarts). Instead, we will use the service name, which
    will let you reach the cluster through an alias. You can check the available service
    names with the following command:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到Kafka集群，我们不会使用IP地址或Pod名称（这些名称在重启后会发生变化）。相反，我们将使用服务名称，这将让您通过别名访问集群。您可以使用以下命令检查可用的服务名称：
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output of the preceding command will be restricted to the `name` column.
    In our case, it will look as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出将仅限于`name`列。在我们的情况下，它将如下所示：
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The service name we are interested in is `my-kafka-kafka-bootstrap`, which we
    will add to our Quarkus project soon.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的服务名称是`my-kafka-kafka-bootstrap`，我们很快将其添加到我们的Quarkus项目中。
- en: Shaping up our project for native cloud execution
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为原生云执行准备我们的项目
- en: 'To run our project on OpenShift, we will apply some minimal changes to the
    configuration file so that we can reach the Kafka service name we have just determined.
    In the following code, we have highlighted the changes that we have to apply to
    the `application.properties` file:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要在OpenShift上运行我们的项目，我们将对配置文件进行一些最小更改，以便我们可以访问我们刚刚确定的Kafka服务名称。在以下代码中，我们已突出显示必须应用到`application.properties`文件中的更改：
- en: '[PRE40]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, in the preceding configuration, we have used the `bootstrap.servers`
    property to specify the list of Kafka servers (`host:port`).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在先前的配置中，我们使用了`bootstrap.servers`属性来指定Kafka服务器列表（`host:port`）。
- en: Multiple servers can be added in the configuration by using a comma to separate
    each entry.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置中，您可以通过逗号分隔每个条目来添加多个服务器。
- en: 'Within the source code of this example, you will also find that all POJO classes
    that have been serialized in the messaging stream are annotated with `@io.quarkus.runtime.annotations.RegisterForReflection`,
    as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例的源代码中，你也会发现所有在消息流中序列化的 POJO 类都标注了 `@io.quarkus.runtime.annotations.RegisterForReflection`，如下所示：
- en: '[PRE41]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As a matter of fact, when building a native executable, GraalVM does some assumptions
    to remove all the classes, methods, and fields that are not used directly in your
    code. The elements that are used via reflection are not part of the call tree,
    so they are candidates when it comes to eliminating them from the native executable.
    Since JSON libraries heavily rely on reflection to perform their job, we must
    explicitly tell GraalVM not to exclude them by using the `@RegisterForReflection`
    annotation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在构建原生可执行文件时，GraalVM 会做一些假设以删除所有未直接在代码中使用的类、方法和字段。通过反射使用到的元素不是调用树的一部分，因此它们是消除原生可执行文件时的候选元素。由于
    JSON 库严重依赖反射来完成其工作，我们必须使用 `@RegisterForReflection` 注解明确告诉 GraalVM 不要排除它们。
- en: 'That''s the small change we have applied in order to publish it to the cloud.
    Now, build and deploy the native application with the following commands:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们为了将其发布到云端所做的微小更改。现在，使用以下命令构建和部署原生应用程序：
- en: '[PRE42]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Please note that you can find the preceding script, that is, `deploy-openshift.sh`,
    in the `Chapter10/kafka-openshift` folder.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以找到前面的脚本，即 `deploy-openshift.sh`，在 `Chapter10/kafka-openshift` 文件夹中。
- en: 'Once the preceding script has been executed, verify that the `quarkus-kafka`
    Pod is up and running by using the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行了前面的脚本，请使用以下命令验证 `quarkus-kafka` Pod 是否已启动并运行：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output will confirm this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将确认这一点：
- en: '[PRE44]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You can check the route address as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下方式检查路由地址：
- en: '[PRE45]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The route will be under the `HOST/PORT` column output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 路由将在 `HOST/PORT` 列输出：
- en: '[PRE46]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If you want to reach your application with just a click, head to the Administration
    console and select Networking | Routes. Then, click on the Route Location, as
    shown in the following screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想通过点击就能访问你的应用程序，请转到管理控制台并选择网络 | 路由。然后，点击路由位置，如下面的截图所示：
- en: '![](img/802028dc-c9bd-4ca8-8832-550a3ca1ac8c.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/802028dc-c9bd-4ca8-8832-550a3ca1ac8c.png)'
- en: 'Once the configured timeout for emitting quotes elapses, you will see the stock
    trading application in action on OpenShift:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置的输出引言超时时间结束，你将在 OpenShift 上看到股票交易应用程序的实际运行情况：
- en: '![](img/9f3994e6-0532-489d-b0a2-29653bbdc1da.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9f3994e6-0532-489d-b0a2-29653bbdc1da.png)'
- en: We've come to the end of our glorious journey of Apache Kafka Streaming! In
    the next section, we will learn how to approach another candidate solution for
    streaming messaging, which is based on the AMQP protocol instead.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达了 Apache Kafka 流的辉煌旅程的终点！在下一节中，我们将学习如何接近另一个候选解决方案，即基于 AMQP 协议的流消息。
- en: Streaming messages with AMQP
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AMQP 进行消息流
- en: If you have only just found Quarkus after years in the Java Enterprise community,
    you will already be familiar with message brokers, which are used to allow different
    Java applications to communicate using JMS as a standard protocol. Although JMS
    is a robust and mature solution for implementing a messaging system, one of the
    main limitations of it is that it's focused exclusively on Java. In the microservices
    world, it's fairly common to use different languages to compose the overall system
    architecture, so a platform-independent solution is required. In this context,
    AMQP offers a set of advantages that make it a perfect fit for implementing the
    Reactive Streams API when it comes to microservices in a distributed system.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是在多年的 Java 企业社区之后刚刚发现 Quarkus，你将已经熟悉消息代理，这些代理用于允许不同的 Java 应用程序使用 JMS 作为标准协议进行通信。尽管
    JMS 是实现消息系统的健壮和成熟的解决方案，但它的主要局限性之一是它仅专注于 Java。在微服务世界中，使用不同的语言来构建整体系统架构相当常见，因此需要一个平台无关的解决方案。在这种情况下，AMQP
    提供了一系列优势，使其成为在分布式系统中的微服务实现反应式流 API 的完美选择。
- en: 'In a nutshell, the following are some of the main features of the AMQP protocol:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，以下是一些 AMQP 协议的主要功能：
- en: It provides a platform-independent wire-level messaging protocol that allows
    interoperability across multiple languages and platforms.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一个平台无关的底层消息协议，允许跨多种语言和平台进行互操作性。
- en: It is a wire-level protocol, by which data is sent across the network as a stream
    of bytes.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个底层协议，数据作为字节流通过网络发送。
- en: It can achieve high performance while working at low-level byte streams.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在低级别字节流工作的情况下可以实现高性能。
- en: It supports long-lived messaging and classic message queues.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持长连接消息和经典消息队列。
- en: It supports distribution patterns such as round-robin (where the load is equally
    distributed across servers) and store and forward (where messages are stored at
    the sender side in a persistence store and are then forwarded to the receiver
    side).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持如轮询（负载在服务器之间均匀分配）和存储转发（消息在发送方侧存储在持久存储中，然后转发到接收方侧）等分发模式。
- en: It provides support for transactions (across message destination), as well as
    distributed transactions using common standards (XA, X/Open, MS DTC).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持事务（跨消息目的地），以及使用通用标准（XA、X/Open、MS DTC）的分布式事务。
- en: It provides support for data encryption using SASL and TLS protocols.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持使用SASL和TLS协议进行数据加密。
- en: It allows us to control the message flow with metadata.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许我们通过元数据控制消息流。
- en: It provides flow control of messages to control backpressure.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供消息流控制以控制背压。
- en: To let our applications interact with AMQP, we need a broker that supports this
    protocol. A commonly adopted solution in the Java Enterprise is **Apache Artemis
    ActiveMQ** ([https://activemq.apache.org/components/artemis/](https://activemq.apache.org/components/artemis/)),
    which is compatible with Java Enterprise **Message-Oriented Middleware** (**MOM**)
    as well. In the next section, we will learn how to start it and configure it in
    our stock quotes application.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的应用程序与AMQP交互，我们需要一个支持此协议的代理。Java企业中常用的解决方案是**Apache Artemis ActiveMQ**
    ([https://activemq.apache.org/components/artemis/](https://activemq.apache.org/components/artemis/))，它也与Java企业**面向消息的中间件**（**MOM**）兼容。在下一节中，我们将学习如何在我们的股票报价应用程序中启动和配置它。
- en: Configuring the AMQP broker
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置AMQP代理
- en: In order to kick-start our application as quickly as possible, we will use a
    Docker Compose script. This will download a suitable version of the message broker
    and set some environment variables that are needed so that we can reach the broker.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能快地启动我们的应用程序，我们将使用Docker Compose脚本。这将下载一个合适的消息代理版本，并设置一些必要的环境变量，以便我们可以访问代理。
- en: 'Simply start the `docker-compose.yaml` file contained in the `amqp` folder
    with the following command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 只需使用以下命令启动`amqp`文件夹中的`docker-compose.yaml`文件：
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If the startup is successful, you should see the following output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启动成功，你应该会看到以下输出：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can verify that the Kafka and ZooKeeper containers are up and running by
    executing the `docker ps` command:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过执行`docker ps`命令来验证Kafka和ZooKeeper容器是否正在运行：
- en: '[PRE49]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding command will display the following active processes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将显示以下活动进程：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, let''s configure our application so that it can use ActiveMQ. You will
    find the updated application in the `Chapter10/amqp` folder of this book''s GitHub
    repository. First of all, we need to replace Kafka''s Reactive Messaging dependency
    with the AMQP Reactive Messaging dependency:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们配置我们的应用程序，使其可以使用ActiveMQ。你可以在本书GitHub仓库的`Chapter10/amqp`文件夹中找到更新后的应用程序。首先，我们需要将Kafka的响应式消息传递依赖项替换为AMQP响应式消息传递依赖项：
- en: '[PRE51]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In terms of application configuration, some changes need to be made to our
    `application.properties` file. First of all, we need to include the username and
    password that we have set in `docker-compose.yaml` (`quarkus/quarkus`):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用配置方面，我们需要对`application.properties`文件进行一些修改。首先，我们需要包含在`docker-compose.yaml`中设置的用户名和密码（`quarkus/quarkus`）：
- en: '[PRE52]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, we need to configure the AMQP connector so that we can write to the `stock-quote`
    queue, by specifying that the queue is durable (for example, persists to disk
    and survives a broker restart):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要配置AMQP连接器，以便我们可以写入`stock-quote`队列，通过指定队列是持久的（例如，持久化到磁盘并在代理重启后存活）：
- en: '[PRE53]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Conversely, we need to configure the AMQP connector so that it can read from
    the `stocks` queue:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要配置AMQP连接器，以便它可以读取`stocks`队列：
- en: '[PRE54]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we can bootstrap the application as usual with the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令像往常一样引导应用程序：
- en: '[PRE55]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The welcome page of the application (available at `http://localhost:8080`)
    will show the stock quotes ticker in action, which now uses ActiveMQ as its broker.
    As you can see, a minimal adjustment has been made to the UI (just the title),
    but it accurately masks the changes that were made under the hood:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的欢迎页面（可在`http://localhost:8080`访问）将显示股票报价的实时行情，现在它使用ActiveMQ作为其代理。正如你所见，UI（仅标题）进行了最小调整，但它准确地掩盖了底下的变化：
- en: '![](img/0ef0a228-fb93-4f34-94fe-41939a735c63.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ef0a228-fb93-4f34-94fe-41939a735c63.png)'
- en: 'You can find out more about this process by logging in to the AMQ management
    console, which is available at `http://localhost:8161/console`. Once you''ve logged
    in with your configured credentials (`quarkus/quarkus`), you can check that the
    destination queue has been created in the list of available addresses:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过登录到 AMQ 管理控制台了解更多关于此过程的信息，该控制台位于 `http://localhost:8161/console`。一旦您使用配置的凭据（`quarkus/quarkus`）登录，您可以在可用地址列表中检查是否已创建目标队列：
- en: '![](img/30854435-e1e8-475b-a47c-0950985f5de4.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30854435-e1e8-475b-a47c-0950985f5de4.png)'
- en: 'By selecting the `stocks` destination, you can check any further details in
    the main panel of the management console, as shown in the following screenshot:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择 `stocks` 目标，您可以在管理控制台的主面板中检查任何进一步的详细信息，如下面的截图所示：
- en: '![](img/02058696-2aea-4437-a369-14cf21332b8e.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02058696-2aea-4437-a369-14cf21332b8e.png)'
- en: 'When you''re done, stop all the running containers with the following command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，使用以下命令停止所有正在运行的容器：
- en: '[PRE56]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preceding command will display a list of all the container layers that
    have been stopped, like so:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将显示已停止的所有容器层的列表，如下所示：
- en: '[PRE57]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then, verify that the ActiveMQ containers have been stopped by executing the
    `docker ps` command once more:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，再次执行 `docker ps` 命令以验证 ActiveMQ 容器已被停止：
- en: '[PRE58]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The preceding command shouldn't produce any output. Now, let's test the same
    application stack in the cloud.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令不应产生任何输出。现在，让我们在云端测试相同的应用程序堆栈。
- en: Streaming messages to AMQ in the cloud
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云端向 AMQ 流式传输消息
- en: The last thing we will do is deploy the Quarkus application to the cloud while
    using AMQ as a messaging broker. For this purpose, we will plug the ActiveMQ Docker
    image that we tested previously into OpenShift (more details about this image
    can be found on GitHub at [https://github.com/vromero/activemq-artemis-docker](https://github.com/vromero/activemq-artemis-docker)).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的最后一件事是在使用 AMQ 作为消息代理的同时将 Quarkus 应用程序部署到云端。为此，我们将之前测试过的 ActiveMQ Docker
    镜像插入到 OpenShift 中（有关此镜像的更多详细信息可在 GitHub 上找到：[https://github.com/vromero/activemq-artemis-docker](https://github.com/vromero/activemq-artemis-docker))。
- en: 'First of all, create a new project named `amq-demo`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个名为 `amq-demo` 的新项目：
- en: '[PRE59]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output will confirm that the project namespace has been created in your
    virtual address:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将确认项目命名空间已创建在您的虚拟地址中：
- en: '[PRE60]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, deploy the AMQ server to your project with the following command, which
    will set the username and password so that you can access the broker:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令将 AMQ 服务器部署到您的项目中，这将设置用户名和密码，以便您可以访问代理：
- en: '[PRE61]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Take note of the `RESTORE_CONFIGURATION=true` environment variable. This is
    needed since OpenShift automounts empty volumes in all the declared volumes. Since
    this behavior impacts the `/etc` folder of this image, which is where the configuration
    is stored, we need to set the `RESTORE_CONFIGURATION` environment variable to
    `true`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `RESTORE_CONFIGURATION=true` 环境变量。这是必需的，因为 OpenShift 自动挂载所有声明的空卷。由于此行为影响此镜像的
    `/etc` 文件夹，而配置存储在该文件夹中，因此我们需要将 `RESTORE_CONFIGURATION` 环境变量设置为 `true`。
- en: 'After executing the `new-app` command, the following output will be displayed:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 `new-app` 命令后，将显示以下输出：
- en: '[PRE62]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You can check the status of the Pods with the `oc` command:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `oc` 命令检查 Pods 的状态：
- en: '[PRE63]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The following output confirms that the `artemis` Pod is in a running state:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出确认 `artemis` Pod 正在运行状态：
- en: '[PRE64]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, let''s check the service name, which will be `artemis`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查服务名称，它将是 `artemis`：
- en: '[PRE65]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Check that the output that''s returned matches the output shown here:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 确认返回的输出与这里显示的输出匹配：
- en: '[PRE66]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, let''s go for the final kill: we will deploy the application located in
    the `Chapter10/amqp-openshift` directory. Within this folder, you will find the
    stock trade application, which has been configured to stream messages on AMQ.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行最后的终结操作：我们将部署位于 `Chapter10/amqp-openshift` 目录中的应用程序。在这个文件夹中，您将找到配置为在
    AMQ 上流式传输消息的股票交易应用程序。
- en: 'Here is the updated `application.properties` file, which contains the AMQ username
    and password, along with the host and the port where the service runs:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是更新后的 `application.properties` 文件，其中包含 AMQ 用户名和密码，以及服务运行的主机和端口：
- en: '[PRE67]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we will deploy the application contained in the same folder (`Chapter10/amqp-openshift`)
    into OpenShift. For convenience, you can simply run the `deploy-openshift.sh`
    script, which can be found in the same directory. Here is the content of the script,
    which should be pretty familiar to you:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将部署与同一文件夹（`Chapter10/amqp-openshift`）中的应用程序到OpenShift。为了方便，你可以简单地运行`deploy-openshift.sh`脚本，该脚本位于同一目录下。以下是脚本的内容，这应该对你来说相当熟悉：
- en: '[PRE68]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Then, check that the `quarkus-amq` Pod is in a running state:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，检查`quarkus-amq` Pod是否处于运行状态：
- en: '[PRE69]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output that you''ll receive confirms this:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 你将收到的输出确认了这一点：
- en: '[PRE70]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, you can verify that the application works by clicking on the route address.
    Just go to the Networking | Routes path in the console:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过点击路由地址来验证应用程序是否工作。只需在控制台中的“网络 | 路由”路径下进行操作：
- en: '![](img/a8e60ed7-837e-45d8-abc0-2c278f101eec.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a8e60ed7-837e-45d8-abc0-2c278f101eec.png)'
- en: 'The output will be pretty much the same, except for the route name, which celebrates
    your last achievement in this book:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将几乎相同，只是路由名称会庆祝你在本书中的最后一次成就：
- en: '![](img/51eed0a9-3623-488e-ad76-d2c252a90e98.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/51eed0a9-3623-488e-ad76-d2c252a90e98.png)'
- en: That's all, folks!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，朋友们！
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to use CDI beans to produce, consume, and process
    messages using the Reactive Messaging specification. We also learned how to bootstrap
    and configure Apache Kafka and Active MQ's broker so that it acts as a distributed
    streaming platform for our CDI Beans. To put our new skills in place, we created
    an example stock trade application, which was initially run in development mode
    and then deployed as a native image on top of OpenShift.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用CDI Bean根据反应式消息规范来产生、消费和处理消息。我们还学习了如何启动和配置Apache Kafka和Active
    MQ的代理，使其作为我们的CDI Bean的分布式流平台。为了将我们的新技能付诸实践，我们创建了一个示例股票交易应用程序，该应用程序最初以开发模式运行，然后作为原生镜像部署到OpenShift上。
- en: Now, we have reached the end of this book, where we learned about the story
    of the gradual renewal of the Java Enterprise application, from monolith to native
    microservices running in the cloud. It was an exciting journey that certainly
    set a milestone, but this isn't the end of our hard work – it's just the end of
    this story.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经到达了本书的结尾，在这里我们了解了Java企业应用程序从单体到在云中运行的本地微服务的逐步更新故事。这是一次激动人心的旅程，确实设立了一个里程碑，但这并不是我们辛勤工作的结束——这只是这个故事的一个结束。
