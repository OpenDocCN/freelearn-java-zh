- en: Chapter 3. Multithreading and Reactive Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lesson, we will look at an approach to support a high performance of
    an application by programmatically splitting the task between several workers.
    That was how the pyramids were built 4,500 years ago, and this method has not
    failed to deliver since then. But there is a limitation on how many laborers can
    be brought to work on the same project. The shared resources provide a ceiling
    to how much the workforce can be increased, whether the resources are counted
    in square feet and gallons (as the living quarters and water in the time of the
    pyramids) or in gigabytes and gigahertz (as the memory and processing power of
    a computer).
  prefs: []
  type: TYPE_NORMAL
- en: Allocation, usage, and limitations of a living space and computer memory are
    very similar. However, we perceive the processing power of the human workforce
    and CPU quite differently. Historians tell us that thousands of ancient Egyptians
    worked on cutting and moving massive stone blocks at the same time. We do not
    have any problem understanding what they mean even if we know that these workers
    rotated all the time, some of them resting or attending to other matters temporarily
    and then coming back to replace the ones who have finished their annual assignment,
    others died or got injured and were replaced by the new recruits.
  prefs: []
  type: TYPE_NORMAL
- en: But in case of computer data processing, when we hear about working threads
    executing at the same time, we automatically assume that they literally do what
    they are programmed to do in parallel. Only after we look under the hood of such
    a system we realize that such parallel processing is possible only when the threads
    are executed each by a different CPU. Otherwise, they time share the same processing
    power, and we perceive them working at the same time only because the time slots
    they use are very short--a fraction of the time units we have used in our everyday
    life. When the threads share the same resource, in computer science we say they
    do it concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: In this lesson, we will discuss the ways to increase Java application performance
    by using the workers (threads) that process data concurrently. We will show how
    to use threads effectively by pooling them, how to synchronize the concurrently
    accessed data, how to monitor and tune worker threads at runtime, and how to take
    advantage of the reactive programming concept.
  prefs: []
  type: TYPE_NORMAL
- en: But before doing that, let's revisit the basics of creating and running multiple
    threads in the same Java process.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are principally two ways to create worker threads--by extending the `java.lang.Thread`
    class and by implementing the `java.lang.Runnable` interface. While extending
    the `java.lang.Thread` class, we are not required to implement anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `MyThread` class inherits the `name` property with an automatically generated
    value and the `start()` method. We can run this method and check the `name`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code, the result will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the generated `name` is `Thread-0`. If we created another thread
    in the same Java process, the `name` would be `Thread-1` and so on. The `start()`
    method does nothing. The source code shows that it calls the `run()` method if
    such a method is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add any other method to the `MyThread` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `calculateAverageSqrt()` method calculates the average square root of the
    first 99,999 integers and assigns the result to a property that can be accessed
    anytime. The following code demonstrates how we can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this brings up the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you would expect, the `calculateAverageSqrt()` method blocks until the calculations
    are completed. It was executed in the main thread without it taking advantage
    of multithreading. To do this, we move the functionality in the `run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we call the `start()` method again, as in the first example and expect
    the result to be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the output of this code may surprise you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that the main thread accessed (and printed) the `t1.getResult()`
    function before the new `t1` thread finished its calculations. We can experiment
    and change the implementation of the `run()` method to see if the `t1.getResult()`
    function can get a partial result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we run the `demo_thread_03()` method again, the result remains
    the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It takes time to create a new thread and get it going. Meanwhile, the `main`
    thread calls the `t1.getResult()` function immediately, thus getting no results
    yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give the new (child) thread time to complete the calculations, we add the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You have noticed that we have paused the main thread by 100 ms and added printing
    of the current thread name, to illustrate what we mean by `main` thread, the name
    that is assigned automatically to the thread that executes the `main()` method.
    The output of the previous code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The delay of 100 ms was enough for the `t1` thread to finish the calculations.
    That was the first of two ways of creating threads for multithreaded calculation.
    The second way is to implement the `Runnable` interface. It may be the only way
    possible if the class that does calculations already extends some other class
    and you cannot or don''t want to use composition for some reasons. The `Runnable`
    interface is a functional interface (has only one abstract method) with the `run()`
    method that has to be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement this interface in the `MyRunnable` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It has the same functionality as the `Thread01` class earlier plus we have added
    id that allows identifying the thread if necessary since the `Runnable` interface
    does not have the built-in `getName()` method like the `Thread` class has.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if we execute this class without pausing the `main` thread, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now add the pause as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is exactly the same as the one produced by the `Thread01` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/3_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'All the previous examples stored the generated result in the class property.
    But it is not always the case. Typically, the worker thread either passes its
    value to another thread or stores it in a database or somewhere else externally.
    In such a case, one can take advantage of the `Runnable` interface being a functional
    interface and pass the necessary processing function into a new thread as a lambda
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is going to be exactly the same, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Depending on the preferred style, you can re-arrange the code and isolate the
    lambda expression in a variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can put the lambda expression in a separate method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is going to be the same, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prerequisites](img/03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the basic understanding of threads creation in place, we can now return
    to the discussion about using the multithreading for building a high-performance
    application. In other words, after we understand the abilities and resources needed
    for each worker, we can now talk about logistics of bringing in many of them for
    such a big-scale project as the Great Pyramid of Giza.
  prefs: []
  type: TYPE_NORMAL
- en: To write code that manages the life cycle of worker threads and their access
    to the shared resources is possible, but it is quite the same from one application
    to another. That's why, after several releases of Java, the thread management
    plumbing became part of the standard JDK library as the `java.util.concurrent`
    package. This package has a wealth of interfaces and classes that support multithreading
    and concurrency. We will discuss how to use most of this functionality in the
    subsequent sections, while talking about thread pools, threads monitoring, thread
    synchronization, and the related subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Thread Pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look into the `Executor` interfaces and their implementations
    provided in the `java.util.concurrent` package. They encapsulate thread management
    and minimize the time an application developer spends on the writing code related
    to threads' life cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three `Executor` interfaces defined in the `java.util.concurrent`
    package. The first is the base `Executor` interface has only one `void execute(Runnable
    r)` method in it. It basically replaces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: However, we can also avoid a new thread creation by getting it from a pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second is the `ExecutorService` interface extends `Executor` and adds the
    following groups of methods that manage the life cycle of the worker threads and
    of the executor itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '`submit()`: Place in the queue for the execution of an object of the interface
    `Runnable` or interface `Callable` (allows the worker thread to return a value);
    return object of `Future` interface, which can be used to access the value returned
    by the `Callable` and to manage the status of the worker thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invokeAll()`: Place in the queue for the execution of a collection of interface
    `Callable` objects return, list of `Future` objects when all the worker threads
    are complete (there is also an overloaded `invokeAll()` method with timeout)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invokeAny()`: Place in the queue for the execution of a collection of interface
    `Callable` objects; return one `Future` object of any of the worker threads, which
    has completed (there is also an overloaded `invokeAny()` method with timeout)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Methods that manage the worker threads status and the service itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '`shutdown()`: This prevents new worker threads from being submitted to the
    service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isShutdown()`: This checks whether the shutdown of the executor was initiated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`awaitTermination(long timeout, TimeUnit timeUnit)`: This waits until all worker
    threads have completed execution after a shutdown request, or the timeout occurs,
    or the current thread is interrupted, whichever happens first'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isTerminated()`: This checks whether all the worker threads have completed
    after the shutdown was initiated; it never returns `true` unless either `shutdown()`
    or `shutdownNow()` was called first'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shutdownNow()`: This interrupts each worker thread that is not completed;
    a worker thread should be written so that it checks its own status (using `Thread.currentThread().isInterrupted()`,
    for example) periodically and gracefully shuts down on its own; otherwise, it
    will continue running even after `shutdownNow()` was called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third interface is `ScheduledExecutorService` that extends `ExecutorService`
    and adds methods that allow scheduling of the execution (one-time and periodic
    one) of the worker threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pool-based implementation of `ExecutorService` can be created using the `java.util.concurrent.ThreadPoolExecutor`
    or `java.util.concurrent.ScheduledThreadPoolExecutor` class. There is also a `java.util.concurrent.Executors`
    factory class that covers most of the practical cases. So, before writing a custom
    code for worker threads pool creation, we highly recommend looking into using
    the following factory methods of the `java.util.concurrent.Executors` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`newSingleThreadExecutor()`: This creates an `ExecutorService` (pool) instance
    that executes worker threads sequentially'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newFixedThreadPool()`: This creates a thread pool that reuses a fixed number
    of worker threads; if a new task is submitted when all the worker threads are
    still executing, it will be set into the queue until a worker thread is available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newCachedThreadPool()`: This creates a thread pool that adds a new thread
    as needed, unless there is an idle thread created before; threads that have been
    idle for sixty seconds are removed from the cache'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newScheduledThreadPool()`: This creates a thread pool of a fixed size that
    can schedule commands to run after a given delay, or to execute periodically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newSingleThreadScheduledExecutor()`: This creates a single-threaded executor
    that can schedule commands to run after a given delay, or to execute periodically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newWorkStealingThreadPool()`: This creates a thread pool that uses the same
    work-stealing mechanism used by `ForkJoinPool`, which is particularly useful in
    case the worker threads generate other threads, such as in recursive algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these methods has an overloaded version that allows passing in a `ThreadFactory`
    that is used to create a new thread when needed. Let's see how it all works in
    a code sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `MyRunnable02` class that implements `Runnable`—our future
    worker threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice the important difference of this implementation from the previous examples--the
    `takeWhile(i -> !Thread.currentThread().isInterrupted())` operation allows the
    stream flowing as long as the thread worker status is not set to interrupted,
    which happens when the `shutdownNow()` method is called. As soon as the predicate
    of the `takeWhile()` returns `false` (the worker thread is interrupted), the thread
    stops producing the result (just ignores the current `result` value). In a real
    system, it would equate to skipping storing `result` value in the database, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting here that using the `interrupted()` status method for checking
    the thread status in the preceding code may lead to inconsistent results. Since
    the `interrupted()` method returns the correct state value and then clears the
    thread state, the second call to this method (or the call to the method `isInterrupted()`
    after the call to the method `interrupted()`) always returns `false`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is not the case in this code, we would like to mention here a mistake
    some developers make while implementing `try/catch` block in a worker thread.
    For example, if the worker needs to pause and wait for an interrupt signal, the
    code often looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can show how to execute the earlier `MyRunnable02` class with a cached
    pool implementation of the `ExecutiveService` pool (other types of thread pool
    are used similarly). First, we create the pool, submit three instances of the
    `MyRunnable02` class for execution and shut down the pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run these lines, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Pools](img/03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: No surprises here! The `isShutdown()` method returns a `false` value before
    the `shutdown()` method is called and a `true` value afterward. The `isTerminated()`
    method returns a `false` value, because none of the worker threads has completed
    yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test the `shutdown()` method by adding the following code after it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will now have the following message (the screenshot would be either
    too big for this page or not readable when fitting):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As expected, after the `shutdown()` method is called, no more worker threads
    can be added to the pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see what we can do after the shutdown was initiated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printRunningThreadIds()` method looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Pools](img/03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that 100 ms was enough for each worker thread to complete the calculations.
    (Notice, if you try to reproduce this data on your computer, the results might
    be slightly different because of the difference in performance, so you would need
    to adjust the timeout.)
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have decreased the wait time to 75 ms, the output became as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Pools](img/03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The 75 ms on our computer was not enough to let all the threads complete, so
    they were interrupted by `shutdownNow()` and their partial results were ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now remove the check of the interrupted status in the `MyRunnable01`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Without the check, even if we decrease the timeout to 1 ms, the result will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Pools](img/03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is because the worker threads have never noticed that somebody tried to
    interrupt them and completed their assigned calculations. This last test demonstrates
    the importance of watching for the interrupted state in a work thread in order
    to avoid many possible problems, namely, data corruption and memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: The demonstrated cached pool works fine and poses no problem if the worker threads
    perform short tasks and their number cannot grow excessively large. If you need
    to have more control over the max number of worker threads running at any time,
    use the fixed size thread pool. We will discuss how to choose the pool size in
    one of the following sections of this lesson.
  prefs: []
  type: TYPE_NORMAL
- en: The single-thread pool is a good fit for executing tasks in a certain order
    or in the case when each of them requires so many resources that cannot be executed
    in parallel with another. Yet another case for using a single-thread execution
    would be for workers that modify the same data, but the data cannot be protected
    from the parallel access another way. The thread synchronization will be discussed
    in more detail in one of the following sections of this lesson, too.
  prefs: []
  type: TYPE_NORMAL
- en: In our sample code, so far we have only included the `execute()` method of the
    `Executor` interface. We will demonstrate the other methods of the `ExecutorService`
    pool in the following section while discussing threads monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: And the last remark in this section. The worker threads are not required to
    be objects of the same class. They may represent completely different functionality
    and still be managed by one pool.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to monitor threads, programmatically and using the external
    tools. We have already seen how the result of a worker calculation could be checked.
    Let''s revisit that code. We will also slightly modify our worker implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For the worker thread identification, instead of custom ID, we now use the
    thread name assigned automatically at the time of the execution (that is why we
    assign the `name` property in the `run()` method that is called in the context
    of the execution when the thread acquires its name). The new class `MyRunnable03`
    can be used like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `shutdown()` method contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the preceding code, the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the result on your computer is different, try to increase the input value
    to the `sleepMs()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to get information about the application worker threads is by using
    the `Future` interface. We can access this interface using the `submit()` method
    of the `ExecutorService` pool, instead of the `execute()`, `invokeAll()`, or `invokeAny()`
    methods. This code shows how to use the `submit()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printFuture()` method has the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sleepMs()` method contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We prefer this implementation instead of the traditional `Thread.sleep()` because
    it is explicit about the time units used.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we execute the previous code, the result will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `printFuture()` method has blocked the main thread execution until the
    first thread has completed. Meanwhile, the second thread has completed too. If
    we call the `printFuture()` method after the `shutdown()` method, both the threads
    would complete by that time already because we have set a wait time of 1 second
    (see the `pool.awaitTermination()` method), which is enough for them to finish
    their job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you think it is not much information from a threads monitoring point of
    view, the `java.util.concurrent` package provides more capabilities via the `Callable`
    interface. It is a functional interface that allows returning any object (containing
    results of the worker thread calculations) via the `Future` object using `ExecutiveService`
    methods--`submit()`, `invokeAll()`, and `invokeAny()`. For example, we can create
    a class that contains the result of a worker thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We have included the name of the worker thread too for monitoring which thread
    generated the result that is presented. The class that implements the `Callable`
    interface may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the code that uses the `MyCallable01` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printResult()` method contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The earlier output shows, as in the previous examples, that the `printResult()`
    method waits until the first of the worker threads finishes, so the second thread
    manages to finish its job at the same time. The advantage of using `Callable`,
    as you can see, is that we can retrieve the actual result from a `Future` object,
    if we need it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage of the `invokeAll()` and `invokeAny()` methods looks similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printResults()` method is using the `printResult()` method, which you
    already know:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the preceding code, the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is no more waiting for the worker thread completing the
    job. That is so because the `invokeAll()` method returns the collection of the
    `Future` object after all the jobs have completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `invokeAny()` method behaves similarly. If we run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following will be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These are the basic techniques for monitoring the threads programmatically,
    but one can easily extend our examples to cover more complicated cases tailored
    to the needs of a specific application. In [Lesson 5](ch05.xhtml "Chapter 5. Making
    Use of New APIs to Improve Your Code"), *Making Use of New APIs to Improve Your
    Code*, we will also discuss another way to programmatically monitor worker threads
    using the `java.util.concurrent.CompletableFuture` class introduced in JDK 8 and
    extended in JDK 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'If necessary, it is possible to get information not only about the application
    worker threads, but also about all other threads in the JVM process using the
    `java.lang.Thread` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call this method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We took advantage of the `toString()` method of the `Thread` class that prints
    only the thread name, priority, and the thread group it belongs to. And we see
    the two application threads we have created explicitly (in addition to the `main`
    thread) in the list under the names `pool-1-thread-1` and `pool-1-thread-2`. But
    if we call the `printAllThreads()` method after calling the `shutdown()` method,
    the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We do not see the `pool-1-thread-1` and `pool-1-thread-2` threads in the list
    anymore because the `ExecutorService` pool has been shut down.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could easily add the stack trace information pulled from the same map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: However, that would take too much space on the book page. In [Lesson 5](ch05.xhtml
    "Chapter 5. Making Use of New APIs to Improve Your Code"), *Making Use of New
    APIs to Improve Your Code*, while presenting new Java capabilities that came with
    JDK 9, we will also discuss a better way to access a stack trace via the `java.lang.StackWalker`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Thread` class object has several other methods that provide information
    about the thread, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dumpStack()`: This prints a stack trace to the standard error stream'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enumerate(Thread[] arr)`: This copies active threads in the current thread''s
    thread group and its subgroups into the specified array `arr`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getId()`: This provides the thread''s ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getState()`: This reads the state of the thread; the possible values from
    `enum Thread.State` can be one of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NEW`: This is the thread that has not yet started'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUNNABLE`: This is the thread that is currently being executed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BLOCKED`: This is the thread that is blocked waiting for a monitor lock to
    be released'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WAITING`: This is the thread that is waiting for an interrupt signal'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TIMED_WAITING`: This is the thread that is waiting for an interrupt signal
    up to a specified waiting time'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TERMINATED`: This is the thread that has exited'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`holdsLock(Object obj)`: This indicates whether the thread holds the monitor
    lock on the specified object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interrupted()` or `isInterrupted()`: This indicates whether the thread has
    been interrupted (received an interrupt signal, meaning that the flag interrupted
    was set to `true`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isAlive()`: This indicates whether the thread is alive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isDaemon()`: This indicates whether the thread is a daemon thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `java.lang.management` package provides similar capabilities for monitoring
    threads. Let''s run this code snippet, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'For better presentation, we took advantage of having thread IDs listed and,
    as you could see previously, have sorted the output by ID. If we call the `printThreadsInfo()`
    method before the `shutdown()` method the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, if we call the `printThreadsInfo()` method after the `shutdown()`
    method, the output will not include our worker threads anymore, exactly as in
    the case of using the `Thread` class API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring Threads](img/03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `java.lang.management.ThreadMXBean` interface provides a lot of other useful
    data about threads. You can refer to the official API on the Oracle website about
    this interface for more information check this link: [https://docs.oracle.com/javase/8/docs/api/index.html?java/lang/management/ThreadMXBean.html](https://docs.oracle.com/javase/8/docs/api/index.html?java/lang/management/ThreadMXBean.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In the list of threads mentioned earlier, you may have noticed the `Monitor
    Ctrl-Break` thread. This thread provides another way to monitor the threads in
    the JVM process. Pressing the *Ctrl* and *Break* keys on Windows causes the JVM
    to print a thread dump to the application's standard output. On Oracle Solaris
    or Linux operating systems, the same effect has the combination of the *Ctrl*
    key and the backslash *\*. This brings us to the external tools for thread monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you don''t have access to the source code or prefer to use the external
    tools for the threads monitoring, there are several diagnostic utilities available
    with the JDK installation. In the following list, we mention only the tools that
    allow for thread monitoring and describe only this capability of the listed tools
    (although they have other extensive functionality too):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `jcmd` utility sends diagnostic command requests to the JVM on the same
    machine using the JVM process ID or the name of the main class: `jcmd <process
    id/main class> <command> [options]`, where the `Thread.print` option prints the
    stack traces of all the threads in the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The JConsole monitoring tool uses the built-in JMX instrumentation in the JVM
    to provide information about the performance and resource consumption of running
    applications. It has a thread tab pane that shows thread usage over time, the
    current number of live threads, the highest number of live threads since the JVM
    started. It is possible to select the thread and its name, state, and stack trace,
    as well as, for a blocked thread, the synchronizer that the thread is waiting
    to acquire, and the thread owning the lock. Use the **Deadlock Detection** button
    to identify the deadlock. The command to run the tool is `jconsole <process id>`
    or (for remote application) `jconsole <hostname>:<port>` , where `port` is the
    port number specified with the JVM start command that enabled the JMX agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `jdb` utility is an example command line debugger. It can be attached to
    the JVM process and allows you to examine threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `jstack` command line utility can be attached to the JVM process and print
    the stack traces of all threads, including JVM internal threads, and optionally
    native stack frames. It allows you to detect deadlocks too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Java Flight Recorder** (**JFR**) provides information about the Java process,
    including threads waiting for locks, garbage collections, and so on. It also allows
    getting thread dumps, which are similar to the one generated by the `Thread.print`
    diagnostic command or by using the jstack tool. It is possible to set up **Java
    Mission Control** (**JMC**) to dump a flight recording if a condition is met.
    JMC UI contains information about threads, lock contention, and other latencies.
    Although JFR is a commercial feature, it is free for developer desktops/laptops,
    and for evaluation purposes in test, development, and production environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find more details about these and other diagnostic tools in the official
    Oracle documentation at [https://docs.oracle.com/javase/9/troubleshoot/diagnostic-tools.htm](https://docs.oracle.com/javase/9/troubleshoot/diagnostic-tools.htm).
  prefs: []
  type: TYPE_NORMAL
- en: Sizing Thread Pool Executors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our examples, we have used a cached thread pool that creates a new thread
    as needed or, if available, reuses the thread already used, but which completed
    its job and returned to the pool for a new assignment. We did not worry about
    too many threads created because our demo application had two worker threads at
    the most and they were quite short lived.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in the case where an application does not have a fixed limit of the worker
    threads it might need or there is no good way to predict how much memory a thread
    may take or how long it can execute, setting a ceiling on the worker thread count
    prevents an unexpected degradation of the application performance, running out
    of memory or depletion of any other resources the worker threads use. If the thread
    behavior is extremely unpredictable, a single thread pool might be the only solution,
    with an option of using a custom thread pool executor (more about this last option
    is explained later). But in most of the cases, a fixed-size thread pool executor
    is a good practical compromise between the application needs and the code complexity.
    Depending on the specific requirements, such an executor might be one of these
    three flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward, fixed-sized `ExecutorService.newFixedThreadPool(int nThreads)`
    pool that does not grow beyond the specified size, but does not adopt either
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several `ExecutorService.newScheduledThreadPool(int nThreads)` pools that allow
    scheduling different groups of threads with a different delay or cycle of execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ExecutorService.newWorkStealingPool(int parallelism)` that adapts to the specified
    number of CPUs, which you may set higher or smaller than the actual CPUs count
    on your computer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the fixed size in any of the preceding pools too low may deprive the
    application of the chance to utilize the available resources effectively. So,
    before selecting the pool size, it is advisable to spend some time on monitoring
    it and tuning JVM (see how to do it in one of the sections of this lesson) with
    the goal of the identification of the idiosyncrasy of the application behavior.
    In fact, the cycle deploy-monitor-tune-adjust has to be repeated throughout the
    application life cycle in order to accommodate and take advantage of the changes
    that happened in the code or the executing environment.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter you take into account is the number of CPUs in your system,
    so the thread pool size can be at least as big as the CPU's count. Then, you can
    monitor the application and see how much time each thread engages the CPU and
    how much of the time it uses other resources (such as I/O operations). If the
    time spent not using the CPU is comparable with the total executing time of the
    thread, then you can increase the pool size by **time not using CPU/total executing
    time**. But that is in the case that another resource (disk or database) is not
    a subject of contention between the threads. If the latter is the case, then you
    can use that resource instead of the CPU as the delineating factor.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the worker threads of your application are not too big or too long
    executing and belong to the mainstream population of the typical working threads
    that complete their job in a reasonably short period of time, you can increase
    the pool size by adding the (rounded up) ratio of the desired response time and
    the time a thread uses CPU or another most contentious resource. This means that,
    with the same desired response time, the less a thread uses CPU or another concurrently
    accessed resource, the bigger the pool size should be. If the contentious resource
    has its own ability to improve concurrent access (like a connection pool in the
    database), consider utilizing that feature first.
  prefs: []
  type: TYPE_NORMAL
- en: If the required number of threads running at the same time changes at runtime
    under the different circumstances, you can make the pool size dynamic and create
    a new pool with a new size (shutting down the old pool after all its threads have
    completed). The recalculation of the size of a new pool might be necessary also
    after you add to remove the available resources. You can use `Runtime.getRuntime().availableProcessors()`
    to programmatically adjust the pool size based on the current count of the available
    CPUs, for example.
  prefs: []
  type: TYPE_NORMAL
- en: If none of the ready-to-use thread pool executor implementations that come with
    the JDK suit the needs of a particular application, before writing the thread
    managing code from scratch, try to use the `java.util.concurrent.ThreadPoolExecutor`
    class first. It has several overloaded constructors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you an idea of its capabilities, here is the constructor with the biggest
    number of options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The earlier mentioned parameters are (quoting from the JavaDoc):'
  prefs: []
  type: TYPE_NORMAL
- en: '`corePoolSize`: This is the number of threads to keep in the pool, even if
    they are idle unless `allowCoreThreadTimeOut` is set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maximumPoolSize`: This is the maximum number of threads to allow in the pool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keepAliveTime`: When the number of threads is greater than the core, this
    is the maximum time that excess idle threads will wait for new tasks before terminating'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unit`: This is the time unit for the `keepAliveTime` argument'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workQueue`: This is the queue to use for holding tasks before they are executed,
    this queue will hold only the `Runnable` tasks submitted by the execute method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threadFactory`: This is the factory to use when the executor creates a new
    thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handler`: This is the handler to use when the execution is blocked because
    the thread bounds and queue capacities are reached'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the previous constructor parameters except the `workQueue` parameter
    can also be set via the corresponding setter after the object of the `ThreadPoolExecutor`
    method has been created, thus allowing more flexibility in dynamic adjustment
    of the existing pool characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Thread Synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have collected enough people and resources such as food, water, and tools
    for the pyramid building. We have divided people into teams and assigned each
    team a task. A number (a pool) of people are living in the nearby village on a
    standby mode, ready to replace the ones that got sick or injured on their assignment.
    We adjusted the workforce count so that there are only a few people who will remain
    idle in the village. We rotate the teams through the work-rest cycle to keep the
    project going at maximum speed. We monitored the process and have adjusted the
    number of teams and the flow of supplies they need so that there are no visible
    delays and there is steady measurable progress in the project as a whole. Yet,
    there are many moving parts overall and various small and big unexpected incidents
    and problems happen all the time.
  prefs: []
  type: TYPE_NORMAL
- en: To make sure that the workers and teams do not step on each other and that there
    is some kind of traffic regulation so that the next technological step does not
    start until the previous one is finished, the main architect sends his representatives
    to all the critical points of the construction site. These representatives make
    sure that the tasks are executed with the expected quality and in the prescribed
    order. They have the power to stop the next team from starting their job until
    the previous team has not finished yet. They act like traffic cops or the locks
    that can shut down the access to the workplace or allow it, if/when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The job these representatives are doing can be defined in the modern language
    as a coordination or synchronization of actions of the executing units. Without
    it, the results of the efforts of the thousands of workers would be unpredictable.
    The big picture from ten thousand feet would look smooth and harmonious, as the
    farmers' fields from the windows of an airplane. But without closer inspection
    and attention to the critical details, this perfect looking picture may bring
    a poor harvest, if any.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in the quiet electronic space of the multithreaded execution environment,
    the working threads have to be synchronized if they share access to the same working
    place. For example, let''s create the following class-worker for a thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it sequentially adds 1, 2, 3, 4, 5 (so, that the resulting
    total is expected to be 15) to the static property of the `Demo04Synchronization`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In the earlier code, while the main thread pauses for 100 ms the first time,
    the thread `t1` brings the value of the variable result to 15, then the thread
    `t2` adds another 15 to get the total of 30\. Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Synchronization](img/03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we remove the first pause of 100 ms, the threads will work concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Synchronization](img/03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final result is still 30\. We feel good about this code and deploy it to
    production as a well-tested code. However, if we increase the number of additions
    from 5 to 250, for example, the result becomes unstable and changes from run to
    run. Here is the first run (we commented out the printout in each thread in order
    to save space):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Synchronization](img/03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And here is the output of another run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Synchronization](img/03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It demonstrates the fact that the `Demo04Synchronization.result += i` operation
    is not atomic. This means it consists of several steps, reading the value from
    the `result` property, adding a value to it, assigning the resulting sum back
    to the `result` property. This allows the following scenario, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Both the threads have read the current value of `result` (so each of the threads
    has a copy of the same original `result` value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each thread adds another integer to the same original one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thread assigns the sum to the `result` property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second thread assigns its sum to the `result` property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the second thread did not know about the addition the first
    thread made and has overwritten the value assigned to the `result` property by
    the first thread. But such thread interleaving does not happen every time. It
    is just a game of chance. That's why we did not see such an effect with five numbers
    only. But the probability of this happening increases with the growth of the number
    of concurrent actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar thing could happen during the pyramid building too. The second team
    could start doing something before the first team has finished their task. We
    definitely need a **synchronizer** and it comes with a `synchronized` keyword.
    Using it, we can create a method (an architect representative) in the `Demo04Synchronization`
    class that will control access to the `result` property and add to it this keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have to modify the `run()` method in the worker thread too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output now shows the same final number for every run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Synchronization](img/03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `synchronized` keyword tells JVM that only one thread at a time is allowed
    to enter this method. All the other threads will wait until the current visitor
    of the method exits from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same effect could be achieved by adding the `synchronized` keyword to a
    block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The difference is that the block synchronization requires an object--a class
    object in the case of static property synchronization (as in our case) or any
    other object in the case of an instance property synchronization. Each object
    has an intrinsic lock or monitor lock, often referred to simply as a monitor.
    Once a thread acquires a lock on an object, no other thread can acquire it on
    the same object until the first thread releases the lock after normal exit from
    the locked code or if the code throws an exception.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, in the case of a synchronized method, an object (the one to which the
    method belongs) is used for locking, too. It just happens behind the scene automatically
    and does not require the programmer to use an object's lock explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you do not have access to the `main` class code (as in the example
    earlier) you can keep the `result` property public and add a synchronized method
    to the worker thread (instead of the class as we have done):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the object of the `MyRunnable05` worker class provides its intrinsic
    lock by default. This means, you need to use the same object of the `MyRunnable05`
    class for all the threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thread Synchronization](img/03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One can argue that this last implementation is preferable because it allocates
    the responsibility of the synchronization with the thread (and the author of its
    code) and not with the shared resource. This way the need for synchronization
    changes along with the thread implementation evolution, provided that the client
    code (that uses the same or different objects for the threads) can be changed
    as needed as well.
  prefs: []
  type: TYPE_NORMAL
- en: There is another possible concurrency issue that may happen in some operating
    systems. Depending on how the thread caching is implemented, a thread might preserve
    a local copy of the property `result` and not update it after another thread has
    changed its value. By adding the `volatile` keyword to the shared (between threads)
    property guarantees that its current value will be always read from the main memory,
    so each thread will see the updates done by the other threads. In our previous
    examples, we just set the `Demo04Synchronization` class property as `private static
    volatile int result`, add a synchronized `incrementResult()` method to the same
    class or to the thread and do not worry anymore about threads stepping on each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: The described thread synchronization is usually sufficient for the mainstream
    application. But the higher performance and highly concurrent processing often
    require looking closer into the thread dump, which typically shows that method
    synchronization is more efficient than block synchronization. Naturally, it also
    depends on the size of the method and the block. Since all the other threads that
    try to access the synchronized method or block are going to stop execution until
    the current visitor of the method or block exits it, it is possible that despite
    the overhead a small synchronized block yields better performance than the big
    synchronized method.
  prefs: []
  type: TYPE_NORMAL
- en: 'For some applications, the behavior of the default intrinsic lock, which just
    blocks until the lock is released, maybe not well suited. If that is the case,
    consider using locks from the `java.util.concurrent.locks` package. The access
    control based on locks from that package has several differences if compared with
    using the default intrinsic lock. These differences may be advantageous for your
    application or provide the unnecessary complication, but it''s important to know
    them, so you can make an informed decision:'
  prefs: []
  type: TYPE_NORMAL
- en: The synchronized fragment of code does not need to belong to one method; it
    can span several methods, delineated by the calls to the `lock()` and `unlock()`
    methods (invoked on the object that implements the `Lock` interface)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While creating an object of the `Lock` interface called `ReentrantLock`, it
    is possible to pass into the constructor a `fair` flag that makes the lock able
    to grant an access to the longest-waiting thread first, which helps to avoid starvation
    (when the low priority thread never can get access to the lock)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows a thread to test whether the lock is accessible before committing to
    be blocked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows interrupting a thread waiting for the lock, so it does not remain blocked
    indefinitely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can implement the `Lock` interface yourself with whatever features you need
    for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A typical pattern of usage of the `Lock` interface looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `finally` block. It is the way to guarantee that the `lock` is released
    eventually. Otherwise, the code inside the `try-catch` block can throw an exception
    and the lock is never released.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the `lock()` and `unlock()` methods, the `Lock` interface has
    the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lockInterruptibly()`: This acquires the lock unless the current thread is
    interrupted. Similar to the `lock()` method, this method blocks while waiting
    until the lock is acquired, in difference to the `lock()` method, if another thread
    interrupts the waiting thread, this method throws the `InterruptedException` exception'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tryLock()`: This acquires the lock immediately if it is free at the time of
    invocation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tryLock(long time, TimeUnit unit)`: This acquires the lock if it is free within
    the given waiting time and the current thread has not been interrupted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newCondition()`: This returns a new `Condition` instance that is bound to
    this `Lock` instance, after acquiring the lock, the thread can release it (calling
    the `await()` method on the `Condition` object) until some other thread calls
    `signal()` or `signalAll()` on the same `Condition` object, it is also possible
    to specify the timeout period (by using an overloaded `await()` method), so the
    thread will resume after the timeout if there was no signal received, see the
    `Condition` API for more details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scope of this book does not allow us to show all the possibilities for thread
    synchronization provided in the `java.util.concurrent.locks` package. It would
    take several lessons to describe all of them. But even from this short description,
    you can see that one would be hard pressed to find a synchronization problem that
    cannot be solved using the `java.util.concurrent.locks` package.
  prefs: []
  type: TYPE_NORMAL
- en: The synchronization of a method or block of code makes sense when several lines
    of code have to be isolated as an atomic (all or nothing) operation. But in the
    case of a simple assignment to a variable or increment/decrement of a number (as
    in our earlier examples), there is a much better way to synchronize this operation
    by using classes from the `java.util.concurrent.atomic` package that support lock-free
    thread-safe programming on a single variable. The variety of classes covers all
    the numbers and even arrays and reference types such as `AtomicBoolean`, `AtomicInteger`,
    `AtomicIntegerArray`, `AtomicReference`, and `AtomicReferenceArray`.
  prefs: []
  type: TYPE_NORMAL
- en: There are 16 classes in total. Depending on the value type, each of them allows
    a full imaginable range of operations, that is, `set()`, `get()`, `addAndGet()`,
    `compareAndSet()`, `incrementAndGet()`, `decrementAndGet()`, and many others.
    Each operation is implemented much more efficiently than the same operations implemented
    with the `synchronized` keyword. And there is no need for the `volatile` keyword
    because it uses it under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: If the concurrently accessed resource is a collection, the `java.util.concurrent`
    package offers a variety of thread-safe implementations that perform better than
    synchronized `HashMap`, `Hashtable`, `HashSet`, `Vector`, and `ArrayList` (if
    we compare the corresponding `ConcurrentHashMap`, `CopyOnWriteArrayList`, and
    `CopyOnWriteHashSet`). The traditional synchronized collections lock the whole
    collection while concurrent collections use such advanced techniques such as lock
    stripping to achieve thread safety. The concurrent collections especially shine
    with more reading and fewer updates and they are much more scalable than synchronized
    collections. But if the size of your shared collection is small and writes dominate,
    the advantage of concurrent collections is not as obvious.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning JVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each pyramid building, as any big project, goes through the same life cycle
    of design, planning, execution, and delivery. And throughout each of these phases,
    a continuous tuning is going on, a complex project is called so for a reason.
    A software system is not different in this respect. We design, plan and build
    it, then change and tune continuously. If we are lucky, then the new changes do
    not go too far back to the initial stages and do not require changing the design.
    To hedge against such drastic steps, we use prototypes (if the waterfall model
    is used) or iterative delivery (if the agile process is adopted) for early detection
    of possible problems. Like young parents, we are always on alert, monitoring the
    progress of our child, day and night.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned already in one of the previous sections, there are several
    diagnostic tools that come with each JDK 9 installation or can be used in addition
    to them for monitoring your Java application. The full list of these tools (and
    the recommendations how to create a custom tool, if needed) can be found in official
    Java SE documentation on the Oracle site: [https://docs.oracle.com/javase/9/troubleshoot/diagnostic-tools.htm](https://docs.oracle.com/javase/9/troubleshoot/diagnostic-tools.htm).'
  prefs: []
  type: TYPE_NORMAL
- en: Using these tools one identifies the bottleneck of the application and addresses
    it either programmatically or by tuning the JVM itself or both. The biggest gain
    usually comes with the good design decisions and from using certain programming
    techniques and frameworks, some of which we have described in other sections.
    In this section, we are going to look at the options available after all possible
    code changes are applied or when changing code is not an option, so all we can
    do is to tune JVM itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the effort depends on the results of the application profiling
    and the nonfunctional requirements for:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency, or how responsive the application is to the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput, or how much work the application is doing in a given unit of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory footprint, or how much memory the application requires
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The improvements in one of them often are possible only at the expense of the
    one or both of the others. The decrease in the memory consumption may bring down
    the throughput and latency, while the decrease in latency typically can be achieved
    only via the increase in memory footprint unless you can bring in faster CPUs
    thus improving all three characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Application profiling may show that one particular operation keeps allocating
    a lot of memory in the loop. If you have an access to the code, you can try to
    optimize this section of the code and thus ease the pressure on JVM. Alternatively,
    it may show that there is an I/O or another interaction with a low device is involved,
    and there is nothing you can do in the code to improve it.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the goal of the application and JVM tuning requires establishing metrics.
    For example, it is well known already that the traditional measure of latency
    as the average response time hides more than it reveals about the performance.
    The better latency metrics would be the maximum response time in conjunction with
    99% best response time. For throughput, a good metrics would be the number of
    transactions per a unit of time. Often the inverse of this metrics (time per transaction)
    closely reflects latency. For the memory footprint, the maximum allocated memory
    (under the load) allows for the hardware planning and setting guards against the
    dreaded `OutOfMemoryError` exception. Avoiding full (stop-the-world) garbage collection
    cycle would be ideal. In practice, though, it would be good enough if **Full GC**
    happens not often, does not visibly affect the performance and ends up with approximately
    the same heap size after several cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, such simplicity of the requirements does happen in practice.
    Real life brings more questions all the time as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Can the target latency (response time) be ever exceeded?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If yes, how often and by how much?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long can the period of the poor response time last?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who/what measures the latency in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the target performance the peak performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected peak load?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long is the expected peak load going to last?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only after all these and similar questions are answered and the metrics (that
    reflect the nonfunctional requirements) are established, we can start tweaking
    the code, running it and profiling again and again, then tweaking the code and
    repeating the cycle. This activity has to consume most of the efforts because
    tuning of the JVM itself can bring only the fraction of the performance improvements
    by comparison with the performance gained by the code changes.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, several passes of the JVM tuning must happen early in order to
    avoid wasting of the efforts and trying to force the code in the not well-configured
    environment. The JVM configuration has to be as generous as possible for the code
    to take advantage of all the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, select garbage collector from the four that JVM 9 supports, which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serial collector**: This uses a single thread to perform all the garbage
    collection work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel collector**: This uses multiple threads to speed up garbage collection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent Mark Sweep (CMS) collector**: This uses shorter garbage collection
    pauses at the expense of taking more of the processor time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Garbage-First (G1) collector**: This is intended for multiprocessor machines
    with a large memory, but meets garbage collection pause-time goals with high probability,
    while achieving high throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official Oracle documentation ([https://docs.oracle.com/javase/9/gctuning/available-collectors.htm](https://docs.oracle.com/javase/9/gctuning/available-collectors.htm))
    provides the following initial guidelines for the garbage collection selection:'
  prefs: []
  type: TYPE_NORMAL
- en: If the application has a small dataset (up to approximately 100 MB), then select
    the serial collector with the `-XX:+UseSerialGC` option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the application will be run on a single processor and there are no pause-time
    requirements, then select the serial collector with the `-XX:+UseSerialGC` option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If (a) peak application performance is the first priority and (b) there are
    no pause-time requirements or pauses of one second or longer are acceptable, then
    let the VM select the collector or select the parallel collector with `-XX:+UseParallelGC`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the response time is more important than the overall throughput and garbage
    collection pauses must be kept shorter than approximately one second, then select
    a concurrent collector with `-XX:+UseG1GC or -XX:+UseConcMarkSweepGC`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But if you do not have particular preferences yet, let the JVM select garbage
    collector until you learn more about your application's needs. In JDK 9, the G1
    is selected by default on certain platforms, and it is a good start if the hardware
    you use has enough resources.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle also recommends using G1 with its default settings, then later playing
    with a different pause-time goal using the `-XX:MaxGCPauseMillis` option and maximum
    Java heap size using the `-Xmx` option. Increasing either the pause-time goal
    or the heap size typically leads to a higher throughput. The latency is affected
    by the change of the pause-time goal too.
  prefs: []
  type: TYPE_NORMAL
- en: 'While tuning the GC, it is beneficial to keep the `-Xlog:gc*=debug` logging
    option. It provides many useful details about garbage collection activity. The
    first goal of JVM tuning is to decrease the number of full heap GC cycles (Full
    GC) because they are very resource consuming and thus may slow down the application.
    It is caused by too high occupancy of the old generation area. In the log, it
    is identified by the words `Pause Full (Allocation Failure)`. The following are
    the possible steps to reduce chances of Full GC:'
  prefs: []
  type: TYPE_NORMAL
- en: Bring up the size of the heap using `-Xmx`. But make sure it does not exceed
    the physical size of RAM. Better yet, leave some RAM space for other applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase the number of concurrent marking threads explicitly using `-XX:ConcGCThreads`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the humongous objects take too much of the heap (watch for **gc+heap=info**
    logging that shows the number next to humongous regions) try to increase the region
    size using `-XX: G1HeapRegionSize`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watch the GC log and modify the code so that almost all the objects created
    by your application are not moved beyond the young generation (dying young).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add or change one option at a time, so you can understand the causes of the
    change in the JVM's behavior clearly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These few steps will help you go and create a trial-and-error cycle that will
    bring you a better understanding of the platform you are using, the needs of your
    application, and the sensitivity of the JVM and the selected GC to different options.
    Equipped with this knowledge, you will then be able to meet the nonfunctional
    performance requirements whether by changing the code, tuning the JVM, or reconfiguring
    the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After several false starts and a few disastrous disruptions, followed by heroic
    recoveries, the process of pyramid building took shape and ancient builders were
    able to complete a few projects. The final shape sometimes did not look exactly
    as envisioned (the first pyramids have ended up bent), but, nevertheless, the
    pyramids still decorate the desert today. The experience was passed from generation
    to generation, and the design and the process were tuned well enough to produce
    something magnificent and pleasant to look at more than 4,000 years later.
  prefs: []
  type: TYPE_NORMAL
- en: The software practices also change over time, albeit we have had only some 70
    years since Mr. Turing wrote the first modern program. In the beginning, when
    there were only a handful of programmers in the world, a computer program used
    to be a continuous list of instructions. Functional programming (pushing a function
    around like a first-class citizen) was introduced very early too but has not become
    a mainstream. Instead, the **GOTO** instruction allowed you to roll code in a
    spaghetti bowl. Structural programming followed, then object-oriented programming,
    with functional programming moving along and even thriving in certain areas. Asynchronous
    processing of the events generated by the pressed keys became routine for many
    programmers. JavaScript tried to use all of the best practices and gained a lot
    of power, even if at the expense of programmers' frustration during the debugging
    (fun) phase. Finally, with thread pools and lambda expressions being part of JDK
    SE, adding reactive streams API to JDK 9 made Java part of the family that allows
    reactive programming with asynchronous data streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be fair, we were able to process data asynchronously even without this new
    API--by spinning worker threads and using thread pools and callables (as we described
    in the previous sections) or by passing the callbacks (even if lost once in a
    while in the maze of the one who-calls-whom). But, after writing such a code a
    few times, one notices that most of such code is just a plumbing that can be wrapped
    inside a framework that can significantly simplify asynchronous processing. That''s
    how the Reactive Streams initiative ([http://www.reactive-streams.org](http://www.reactive-streams.org))
    came to be created and the scope of the effort is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The scope of Reactive Streams is to find a minimal set of interfaces, methods
    and protocols that will describe the necessary operations and entities to achieve
    the goal--asynchronous streams of data with non-blocking back pressure.
  prefs: []
  type: TYPE_NORMAL
- en: The term **non-blocking back pressure** is an important one because it identifies
    one of the problems of the existed asynchronous processing--coordination of the
    speed rate of the incoming data with the ability of the system to process them
    without the need of stopping (blocking) the data input. The solution would still
    include some back pressure by informing the source that the consumer has difficulty
    in keeping up with the input, but the new framework should react to the change
    of the rate of the incoming data in a more flexible manner than just blocking
    the flow, thus the name **reactive**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Reactive Streams API consists of the five interfaces included in the class
    which are `java.util.concurrent.Flow`, `Publisher`, `Subscriber`, `Subscription`,
    and `Processor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: A `Flow.Subscriber` object becomes a subscriber of the data produced by the
    object of `Flow.Publisher` after the object of `Flow.Subscriber` is passed as
    a parameter into the `subscribe()` method. The publisher (object of `Flow.Publisher`)
    calls the subscriber's `onSubscribe()` method and passes as a parameter a `Flow.Subsctiption`
    object. Now, the subscriber can request `numberOffItems` of data from the publisher
    by calling the subscription's `request()` method. That is the way to implement
    the pull model when a subscriber decides when to request another item for processing.
    The subscriber can unsubscribe from the publisher services by calling the `cancel()`
    subscription method.
  prefs: []
  type: TYPE_NORMAL
- en: In return (or without any request, if the implementer has decided to do so,
    that would be a push model), the publisher can pass to the subscriber a new item
    by calling the subscriber's `onNext()` method. The publisher can also tell the
    subscriber that the item production has encountered a problem (by calling the
    subscriber's `onError()` method) or that no more data will be coming (by calling
    the subscriber's `onComplete()` method).
  prefs: []
  type: TYPE_NORMAL
- en: The `Flow.Processor` interface describes an entity that can act as both a subscriber
    and a publisher. It allows creating chains (pipelines) of such processors, so
    a subscriber can receive an item from a publisher, tweak it, and then pass the
    result to the next subscriber.
  prefs: []
  type: TYPE_NORMAL
- en: This is the minimal set of interfaces the Reactive Streams initiative has defined
    (and it is a part of JDK 9 now) in support of the asynchronous data streams with
    non-blocking back pressure. As you can see, it allows the subscriber and publisher
    to talk to each other and coordinate, if need be, the rate of incoming data, thus
    making possible a variety of solutions for the back pressure problem we discussed
    in the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to implement these interfaces. Currently, in JDK 9, there
    is only one example of implementation of one of the interfaces--the `SubmissionPublisher`
    class implements `Flow.Publisher`. But several other libraries already exist that
    implemented Reactive Streams API: RxJava, Reactor, Akka Streams, and Vert.x are
    among the most known. We will use RxJava 2.1.3 in our examples. You can find the
    RxJava 2.x API on [http://reactivex.io](http://reactivex.io) under the name ReactiveX,
    which stands for Reactive Extension.'
  prefs: []
  type: TYPE_NORMAL
- en: While doing that, we would also like to address the difference between the streams
    of the `java.util.stream` package and Reactive Streams (as implemented in RxJava,
    for example). It is possible to write very similar code using any of the streams.
    Let's look at an example. Here is a program that iterates over five integers,
    selects even numbers only (2 and 4), transforms each of them (takes a square root
    of each of the selected numbers) and then calculates an average of the two square
    roots. It is based on the traditional `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the similarity. It is possible to implement the same functionality
    using any of the streams. For example, here is a method that iterates over five
    integers, selects even numbers only (2 and 4, in this case), transforms each of
    them (takes a square root of each of the even numbers) and then calculates an
    average of the two square roots. It is based on the traditional `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this program, the result will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reactive Programming](img/03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The same functionality (with the same output) can be implemented using the
    package `java.util.stream` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The same functionality can be implemented with RxJava:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: RxJava is based on the `Observable` object (which plays the role of `Publisher`)
    and `Observer` that subscribes to the `Observable` and waits for data to be emitted.
    Each item of the emitted data (on the way from the `Observable` to the `Observer`)
    can be processed by the operations chained in a fluent style (see the previous
    code). Each operation takes a lambda expression. The operation functionality is
    obvious from its name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite being able to behave similarly to the streams, an `Observable` has
    significantly different capabilities. For example, a stream, once closed, cannot
    be reopened, while an `Observable` can be reused. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we use `Observable` twice--for average value calculation
    and for the summing all the square roots of the even numbers. The output is as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reactive Programming](img/03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we do not want `Observable` to run twice, we can cache its data, by adding
    the `.cache()` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the previous code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reactive Programming](img/03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the second usage of the same `Observable` took advantage of
    the cached data, thus allowing for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another `Observable` advantage is that the exception can be caught by `Observer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The `subscribe()` method is overloaded and allows to pass in one, two, or three
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is to be used in case of success
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second is to be used in case of an exception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third is to be called after all the data is processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Observable` model also allows more control over multithreaded processing.
    Using `.parallel()` in the streams does not allow you to specify the thread pool
    to be used. But, in RxJava, you can set the type of pool you prefer using the
    method `subscribeOn()` in `Observable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The `subscribeOn()` method tells `Observable` on which thread to put the data.
    The `Schedulers` class has methods that generate thread pools dealing mostly with
    I/O operations (as in our example), or heavy on computation (method `computation()`),
    or creating a new thread for each unit of work (method `newThread()`), and several
    others, including passing in a custom thread pool (method `from(Executor executor)`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The format of this book does not allow us to describe all the richness of RxJava
    API and other Reactive Streams implementations. Their main thrust is reflected
    in Reactive Manifesto ([http://www.reactivemanifesto.org/](http://www.reactivemanifesto.org/))
    that describes Reactive Systems as a new generation of high performing software
    solutions. Built on asynchronous message-driven processes and Reactive Streams,
    such systems are able to demonstrate the qualities declared in the Reactive Manifesto:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticity**: This has the ability to expand and contract as needed based
    on the load'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better responsiveness**: Here, the processing can be parallelized using asynchronous
    calls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience**: Here, the system is broken into multiple (loosely coupled via
    messages) components, thus facilitating flexible replication, containment, and
    isolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing code for Reactive Systems using Reactive Streams for implementing the
    previously mentioned qualities constitutes reactive programming. The typical application
    of such systems today is microservices, which is described in the next lesson.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lesson, we have discussed the ways to improve Java application performance
    by using multithreading. We described how to decrease an overhead of creating
    the threads using thread pools and various types of such pools suited for different
    processing requirements. We also brought up the considerations used for selecting
    the pool size and how to synchronize threads so that they do not interfere with
    each other and yield the best performance results. We pointed out that every decision
    on the performance improvements has to be made and tested through direct monitoring
    of the application, and we discussed the possible options for such monitoring
    programmatically and using various external tools. The final step, the JVM tuning,
    can be done via Java tool flags that we listed and commented in the corresponding
    section. Yet more gains in Java application performance might be achieved by adopting
    the concept of reactive programming, which we presented as the strong contender
    among most effective moves toward highly scalable and highly performing Java applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next lesson, we will talk about adding more workers by splitting the
    application into several microservices, each deployed independently and each using
    multiple threads and reactive programming for better performance, response, scalability,
    and fault-tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name the method that calculates the average square root of the first 99,999
    integers and assigns the result to a property that can be accessed anytime.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which of the following methods creates a thread pool of a fixed size that can
    schedule commands to run after a given delay, or to execute periodically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`newscheduledThreadPool()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`newWorkStealingThreadPool()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`newSingleThreadScheduledExecutor()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`newFixedThreadPool()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether True or False: One can take advantage of the `Runnable` interface
    being a functional interface and pass the necessary processing function into a
    new thread as a lambda expression.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the `__________` method is called, no more worker threads can be added
    to the pool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`shutdownNow()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`shutdown()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`isShutdown()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`isShutdownComplete()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ________ is based on the `Observable` object, which plays the role of a Publisher.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
