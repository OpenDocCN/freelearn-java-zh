- en: Build a Fraud Detection System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建欺诈检测系统
- en: In this chapter, we are going to develop an algorithm based on the Gaussian
    Distribution function using Spark ML. We will apply the algorithm to detect fraud
    in transactions data. This kind of algorithm can be applied toward building robust
    fraud detection solutions for financial institutions, such as banks, which handle
    great quantities of online transactions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Spark ML开发一个基于高斯分布函数的算法。我们将将该算法应用于检测交易数据中的欺诈行为。这种算法可以应用于构建金融机构（如银行）的稳健欺诈检测解决方案，这些金融机构处理大量的在线交易。
- en: At the heart of the Gaussian Distribution, the function is the notion of an
    **anomaly**. The fraud detection problem is only a classification task but in
    a very narrow sense. It is a balanced supervised learning problem. The term *balanced*
    refers to the fact that the positives in the dataset are of a small number in
    relation to the negatives. On the other hand, an anomaly detection problem is
    typically not balanced. The dataset contains a significantly small number of anomalies
    (positives) in relation to the negatives. The fraud detection problem is a prime
    example of an anomaly detection problem. This is a problem where the dataset has
    a small number of outliers or data points whose values depart considerably from
    normal, to-be-expected values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯分布的核心，函数是**异常**的概念。欺诈检测问题不仅仅是一个分类任务，而是一个非常狭窄意义上的分类任务。它是一个平衡的监督学习问题。术语“平衡”指的是数据集中的正样本相对于负样本数量较少。另一方面，异常检测问题通常是不平衡的。数据集中相对于负样本，异常（正样本）的数量显著较少。欺诈检测问题是一个典型的异常检测问题。这是一个数据集中有少量异常值或数据点，其值与正常、预期的值相差很大的问题。
- en: The overarching learning objective of this chapter is to implement a Scala solution
    that will predict fraud in financial transactions. We will lean on the Spark ML
    library's APIs and its supporting libraries in order to build a fraud detection
    prediction application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要学习目标是实现一个Scala解决方案，该解决方案将预测金融交易中的欺诈行为。我们将依赖Spark ML库的API及其支持库来构建欺诈检测预测应用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Fraud detection problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测问题
- en: Project overview—problem formulation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目概述—问题定义
- en: Getting started
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始
- en: Implementation steps
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施步骤
- en: Fraud detection problem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欺诈检测问题
- en: The fraud detection problem is not a supervised learning problem. We have an
    unbalanced class situation in our fraud detection scenario. What do we have to
    say about the importance of the F1 score in relation to the target variable? First,
    the target variable is a binary label. The F1 score is relevant to our fraud detection
    problem because we have an unbalanced class, where one class is practically more important
    than the other. What do we mean by that? The bottom line of the fraud detection
    classification process concerns whether a certain instance is fraudulent, and
    getting the classifier to classify or label this instance correctly as fraudulent.
    The emphasis is not on labeling an instance as non-fraudulent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测问题不是一个监督学习问题。在我们的欺诈检测场景中，我们有一个不平衡的类别情况。关于目标变量的F1分数的重要性，我们有什么要说的吗？首先，目标变量是一个二进制标签。F1分数与我们的欺诈检测问题相关，因为我们有一个不平衡的类别，其中一个类别实际上比另一个更重要。我们这是什么意思？欺诈检测分类过程的底线是确定某个实例是否欺诈，让分类器正确地将该实例分类或标记为欺诈。重点不是将实例标记为非欺诈。
- en: 'To reiterate, there are two classes in our fraud detection problem:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们的欺诈检测问题中有两个类别：
- en: Fraudulent
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈
- en: Non-fraudulent
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非欺诈
- en: That said, we will now look at the dataset that this implementation depends
    upon
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们现在将查看这个实现所依赖的数据集
- en: Fraud detection dataset at a glance
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欺诈检测数据集概览
- en: Download the dataset from the `ModernScalaProjects_Code` download folder.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ModernScalaProjects_Code`下载文件夹中下载数据集。
- en: 'Here is what the dataset looks like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个数据集的样子：
- en: '![](img/6e314c67-5cea-472f-aa67-d37a7e5da734.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6e314c67-5cea-472f-aa67-d37a7e5da734.jpg)'
- en: The dataset that our fraud detection system is built on
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们欺诈检测系统建立的数据集
- en: The Gaussian Distribution function is the basis for our algorithm.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布函数是我们算法的基础。
- en: So, is the F1 score important? Yes. The F1 score cannot be ignored (in the case
    of a balanced class situation, the F1 score is not necessarily important). It
    is the measure of an ML binary classification's process accuracy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，F1 分数重要吗？是的。F1 分数不容忽视（在类平衡的情况下，F1 分数不一定重要）。它是衡量机器学习二分类过程准确性的指标。
- en: There is an F1 score for each class (one for fraudulent and the other for non-fraudulent). Therefore,
    if we want to compute the F1 score, we need to ensure that the F1 score is associated
    with the fraudulent class.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别都有一个 F1 分数（一个用于欺诈，另一个用于非欺诈）。因此，如果我们想计算 F1 分数，我们需要确保 F1 分数与欺诈类别相关联。
- en: Fraud detection in the context of ML is a classification technique method that
    allows us to build models that attempt to detect outliers. Flagging outliers leads
    us to do what is needed to address fraud. For example, if I swiped my card in
    Portland, Maine, on a whale watching vacation in a location more than 1,000 miles
    away from where I live, it is possible that an underlying fraud detection algorithm
    associated with my credit card will flag **fraud**. The distance, in this case,
    led the algorithm to claim that the said transaction at a certain seafood establishment
    place on Maine's waterfront was fake. This is a simple use case. There are other
    financial transactions that this algorithm is trained to monitor and flag fraud.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，欺诈检测是一种分类技术方法，它允许我们构建试图检测异常值的模型。标记异常值引导我们采取应对欺诈所需的措施。例如，如果我在我居住地超过
    1,000 英里远的缅因州波特兰用我的卡刷了一次，那可能意味着与我信用卡关联的潜在欺诈检测算法会标记**欺诈**。在这种情况下，距离导致算法声称在缅因州水边的某个海鲜场所进行的交易是假的。这是一个简单的用例。还有其他金融交易，该算法被训练来监控并标记欺诈。
- en: For example, imagine a situation where Kate lost her card, and some random person
    picked that card up on the street (let us assume that Kate wasn't aware that she
    had lost her card until a day later) and tried to fill up his truck's gas tank
    with about $50 worth of gas. Even though this transaction was carried out, assuming
    that this person that attempted to use her card somehow got past the zip code
    check, Kate's credit card's fraud detection ML algorithm will flag as a suspicious
    transaction. Most likely, the algorithm will simply kick in and cause the transaction
    to fail or, even if that does not happen, she would get a call from the credit
    card company asking her where she used the card recently. In this case scenario,
    she got that call from the credit card company because the fraud detection algorithm
    flagged that particular transaction as suspicious, a fraudulent incident that
    requires the credit card company to take action.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下凯特丢失了她的卡，某个随机人在街上捡到了那张卡（让我们假设凯特直到一天后才意识到她丢失了卡）并试图用大约 50 美元的汽油填满他的卡车油箱。尽管这笔交易已经完成，假设试图使用她卡的人设法通过了邮政编码检查，凯特的信用卡欺诈检测机器学习算法将标记为可疑交易。很可能会触发算法并导致交易失败，或者即使没有发生这种情况，她也会接到信用卡公司的电话，询问她最近在哪里使用了卡。在这种情况下，她接到信用卡公司的电话是因为欺诈检测算法标记了该笔交易为可疑，这是一起需要信用卡公司采取行动的欺诈事件。
- en: A fraud detection system deals with massive amounts of data. A fraud detection
    classifier, as described in this chapter, will sift through a dataset of transaction
    data and process it. Spark's streaming capabilities allow us to detect outliers,
    samples in our dataset whose values do not fall within a normal, anticipated range
    of values. Detecting such values and generating a set of predictions flagging
    fraud is the emphasis of the fraud detection problem described in this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测系统处理大量数据。本章中描述的欺诈检测分类器将筛选交易数据集并对其进行处理。Spark 的流处理能力使我们能够检测异常值，即我们数据集中那些值不在正常、预期的值范围内的样本。检测这些值并生成一组标记欺诈的预测是该章节所述欺诈检测问题的重点。
- en: We will evaluate the performance of the algorithm and look at whether it flags
    or tags non-fraudulent samples as fraudulent or fraudulent samples as fraudulent,
    by computing metrics, such as precision, recall, and a harmonic mean of the precision
    and recall, known as the F1 score.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过计算指标来评估算法的性能，并查看它是否将非欺诈样本标记为欺诈或欺诈样本标记为欺诈，这些指标包括精确度、召回率以及精确度和召回率的调和平均值，即
    F1 分数。
- en: Precision, recall, and the F1 score
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度、召回率和 F1 分数
- en: 'The following are important:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是很重要的：
- en: The F1 measure
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1 度量
- en: Error term
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误差项
- en: 'In terms of a mathematical function, an F1 score can be defined mathematically
    as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学函数的角度来看，F1 分数可以数学上定义为：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will talk a little about the F1 score or measure. The error term is denoted
    by the Epsilon symbol **(ε)**. Central to all this is the labeled input points
    in our unbalanced dataset. We are going to optimize the Epsilon parameter. How
    exactly do we do this? Let''s first find the best possible F1 score. What is an
    Epsilon? In statistics, it is an **error term**. A measurement may deviate from
    its expected value. For example, it be the mean height of all males in a certain
    population. What is its purpose? We may denote an arbitrarily small, positive
    number as **ε**. Before computing the Epsilon, let''s persist the testing dataframe. We
    have the following tasks cut out for us:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要讨论 F1 分数或度量。误差项用 Epsilon 符号 **(ε)** 表示。所有这一切的核心是我们不平衡数据集中的标记输入点。我们将优化 Epsilon
    参数。我们究竟如何做呢？首先，让我们找到最佳的 F1 分数。什么是 Epsilon？在统计学中，它是一个 **误差项**。一个测量值可能偏离其预期值。例如，它可能是某个特定人口中所有男性的平均身高。它的目的是什么？我们可以将任意小的正数表示为
    **ε**。在计算 Epsilon 之前，让我们保存测试数据框。我们面前有以下任务：
- en: Write a function to help us calculate the best Epsilon and the best F1 score
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一个函数帮助我们计算最佳的 Epsilon 和最佳的 F1 分数
- en: Understand what the F1 score is
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 F1 分数的含义
- en: The maximum possible value that the F1 measure can take is 1. It denotes the
    level of correctness of the classification process by the classifier, that is,
    the proportion of samples or instances that are classified with a high degree
    of correctness or precision. It also tells us how robust (or not) the classifier
    is—whether the classifier missed classifying only a small number of samples or
    more. The F1 score is like a balanced mean between the precision and recall.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: F1 度量可能达到的最大值是 1。它表示分类器分类过程的正确程度，即被分类为高度正确或精确的样本或实例的比例。它还告诉我们分类器有多稳健（或不是）——分类器是否只错过分类少量样本或更多。F1
    分数是精确率和召回率之间的平衡平均值。
- en: F1 becomes more important in an unbalanced class situation. It is a more practical
    measure than **accuracy**. Even though accuracy is more intuitive, and because
    of the fact that both false positives and false negatives are considered, a weighted
    score such as F1 becomes more meaningful in understanding the degree of correctness
    of the classifier.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡的类别情况下，F1 分数变得更加重要。它比 **准确率** 更为实用。尽管准确率更直观，而且由于错误肯定和错误否定都被考虑在内，一个加权分数如
    F1 在理解分类器的正确程度时更有意义。
- en: Feature selection
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: Carefully selecting features is a crucial step in the formulation of a fraud
    detection program. Selecting many features or features, that does not contribute
    in a meaningful way, may impact the performance or skew predictions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定欺诈检测程序时，仔细选择特征是一个关键步骤。选择许多特征或对分类没有实质性贡献的特征可能会影响性能或扭曲预测。
- en: 'So, if we wish to flag fraudulent transactions, we should start small and build
    a system with two features only that we deem to meaningfully contribute to the
    classification. We choose two features that we represent in the dataset as columns
    of double values in a comma-separated file. These features are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们希望标记欺诈交易，我们应该从小规模开始，构建一个只包含我们认为对分类有实质性贡献的两个特征的系统。我们在数据集中选择这两个特征，作为逗号分隔文件中双值列的表示。这些特征如下：
- en: '**Transaction**: Money spent on buying a certain commodity or service'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交易**：购买某种商品或服务所花费的金钱'
- en: '**Distance**: A geographical distance from the address of the cardholder on
    file, or a general distance outside of the perimeter defined by the zip code of
    the cardholder'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离**：从持卡人文件地址的地理距离，或持卡人邮编定义的外围以外的通用距离'
- en: That said, the goal for our fraud detection algorithm is that, with the feature
    selection process in place, we want to process the data, crunch all of the data
    points in our dataset, and flag potential fraud. This is a good place to bring
    in the Gaussian Distribution function, the basis for how we implement our fraud
    detection model. We need to talk a little more about this equation. It will help
    us understand exactly what our algorithm does and why it does what it does. In
    the next section, we will talk about the Gaussian Distribution function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们欺诈检测算法的目标是，在特征选择过程到位的情况下，我们想要处理数据，处理数据集中的所有数据点，并标记潜在的欺诈。这是一个引入高斯分布函数的好地方，这是我们实施欺诈检测模型的基础。我们需要更多地讨论这个方程。它将帮助我们理解我们的算法到底做了什么以及为什么这样做。在下一节中，我们将讨论高斯分布函数。
- en: The Gaussian Distribution function
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯分布函数
- en: 'The Gaussian Distribution function is also known as a **bell curve** or a **normal
    distribution curve**. The bell curve has the following characteristics:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布函数也被称为**钟形曲线**或**正态分布曲线**。钟形曲线具有以下特点：
- en: This kind of distribution (of data) is said to be a continuous distribution.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种分布（数据）被称为连续分布。
- en: Data is spread out in this curve so that it converges around the bell portion
    (the highest point) of the curve, rather than to the left or the right. This center
    at the highest point is also the mean of the curve.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据在这个曲线上分布，使其围绕曲线的钟形部分（最高点）收敛，而不是向左或向右。这个最高点处的中心也是曲线的均值。
- en: The highest point on the bell curve corresponds to the highest probability of
    an occurrence and, as the curve tapers off, the probability of occurrences slides
    down to positions on either side of the curve on its slopes.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态分布曲线的最高点对应着发生事件最高概率的位置，随着曲线变窄，发生事件的概率逐渐下降到曲线两侧的斜率位置。
- en: Because of this property, the bell curve is also known as a normal distribution
    curve. All it needs are the standard deviation and the (population) mean.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这一特性，钟形曲线也被称为正态分布曲线。它只需要标准差和（总体）均值。
- en: In a normal distribution, the three statistics, mean, mode and median, all bear
    the same value.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在正态分布中，均值、众数和中位数这三个统计量都具有相同的值。
- en: 'The normal distribution curve is plotted with values of probability densities
    (a.k.a. normal frequencies). Referring to the figure present in *Project overview—problem
    formulation* section, the following are the meanings of the symbols in the equation:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态分布曲线是用概率密度值（也称为正常频率）绘制的。参考*项目概述—问题定义*部分中的图，以下是对方程中符号的含义：
- en: µ = Mean of the population
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: µ = 总体均值
- en: σ = Standard deviation
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: σ = 标准差
- en: x = Plotted on the *x* axis, this represents a continuous random variable
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x = 绘制在 *x* 轴上，代表一个连续的随机变量
- en: e = Natural logarithmic base, with a value of 2.71
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: e = 自然对数的底数，值为 2.71
- en: π = 3.1415
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: π = 3.1415
- en: The mean is simply a net value that is equal to the sum of all values of the
    data points, divided by the number of data points.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值是一个净值，等于所有数据点值的总和除以数据点的数量。
- en: It is to be noted that *y* is nothing but `f(x)` or `p(x)`, the values of which
    are plotted on the *y* axis of the bell curve.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要注意的是，*y* 实际上就是 `f(x)` 或 `p(x)`，其值在钟形曲线的 *y* 轴上绘制。
- en: 'The following diagram illustrates a bell curve:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了钟形曲线：
- en: '![](img/42d2d95d-7717-4897-862c-42ab143de09f.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/42d2d95d-7717-4897-862c-42ab143de09f.jpg)'
- en: Bell curve
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 钟形曲线
- en: Non-fraudulent data comprises the bulk of our data. Such data is clustered in
    or close to the peak of the bell curve. In general terms, the top of the bell
    curve represents an event or a data point with the highest probability of occurrence.
    The tapering edges of the curve are where anomalies or outliers indicating fraud
    are found.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 非欺诈数据构成了我们数据的大部分。这类数据聚集在或接近钟形曲线的峰值。一般来说，钟形曲线的顶部代表发生概率最高的事件或数据点。曲线的渐变边缘是发现异常或欺诈指示性异常的地方。
- en: 'We mentioned fraud detection as a classification problem in a sense. It falls
    under the banner of anomaly detection. The table that follows describes the fundamental
    differences between a classification task and an anomaly detection task:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，我们提到欺诈检测作为一个分类问题。它属于异常检测的范畴。下表描述了分类任务和异常检测任务之间的基本区别：
- en: '![](img/24625414-7a53-44f6-95dc-15fd02b34a56.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/24625414-7a53-44f6-95dc-15fd02b34a56.jpg)'
- en: Classification versus anomaly identification task
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与异常识别任务
- en: 'Looking at the preceding table, it is clear that the reasons that stand out
    to justify the use of an anomaly identification system are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的表格中可以看出，以下是一些突出理由，以证明使用异常识别系统的合理性：
- en: Samples that may be anomalous in one dataset may not be anomalous when they
    are new, incoming samples in another dataset of financial transactions.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个数据集中可能异常的样本，在它们作为另一个金融交易数据集的新 incoming 样本时可能不会异常。
- en: On the other hand, consider a breast cancer sample in an experimental unit of
    breast cancer samples that are classified as **malignant**. If the same sample
    is an incoming sample to experimental unit 2, the result of the classification
    will be the same.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，考虑一个被归类为**恶性**的乳腺癌样本在乳腺癌样本实验单元中。如果相同的样本是实验单元 2 的 incoming 样本，分类结果将相同。
- en: Where does Spark fit in all this?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 在所有这些中处于什么位置？
- en: Whether you run Spark locally or in an environment where you have a cluster
    operating several distributed nodes, Spark will ramp up. In a local Spark environment,
    Spark will treat CPU cores as resources in a cluster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你在本地运行 Spark 还是处于一个拥有多个分布式节点集群的环境中，Spark 都会启动。在本地 Spark 环境中，Spark 会将 CPU 核心视为集群中的资源。
- en: The Gaussian Distribution algorithm is worth looking into. Let's see what our
    approach should be in the following section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布算法值得研究。让我们看看在下一节中我们的方法应该是什么。
- en: Fraud detection approach
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欺诈检测方法
- en: 'The following diagram illustrates the high-level architecture for our fraud
    detection pipeline:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了我们的欺诈检测管道的高级架构：
- en: '![](img/065ef341-edbd-48f7-83fd-8348c1fdd32a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/065ef341-edbd-48f7-83fd-8348c1fdd32a.png)'
- en: High-level architecture for the fraud detection pipeline
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测管道的高级架构
- en: 'The following is a quick overview of the fraud detection process:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对欺诈检测过程的快速概述：
- en: First, we compute statistics on the training set, which serves as a cross-validation
    dataset. We are interested in the mean and standard deviation from the statistical
    numbers.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们计算训练集的统计数据，该训练集作为交叉验证数据集。我们对统计数据中的平均值和标准差感兴趣。
- en: Next, we want to compute the net **probability density function** (**PDF**)
    for each sample in our cross-validation set.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们希望计算交叉验证集中每个样本的净**概率密度函数**（**PDF**）。
- en: We derive the net probability density as a product of the individual probability
    densities.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将净概率密度作为单个概率密度的乘积得出。
- en: Inside the algorithm, we compare a PDF value with an **Error Term** value to
    determine whether that sample represents an outlier, a potential **Fraud** transaction.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在算法内部，我们比较 PDF 值与**误差项**值，以确定该样本是否代表一个异常值，一个潜在的**欺诈**交易。
- en: We optimize our classification process by having the algorithm executed on a
    cross-validation dataset.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过在交叉验证数据集上执行算法来优化我们的分类过程。
- en: As part of the optimization process, our algorithm computes the best possible
    value for the **Error Term**, an optimal value for the **Error Term** (denoted
    by an Epsilon) corresponding to a computed value of the **F1** score that is the
    highest. The algorithm, after repeated iterations, will come up with this highest
    score of the Epsilon.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为优化过程的一部分，我们的算法计算**误差项**的最佳可能值，一个对应于计算出的最高**F1**分数的**误差项**（用 Epsilon 表示）的最佳值。经过多次迭代后，算法将得出这个最高的
    Epsilon 分数。
- en: A look at the dataset tells us that the most transaction data points fall within
    the range of 55-105 dollars. These transactions occurred within a radius of two
    to seven miles.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察数据集告诉我们，大多数交易数据点都位于 55-105 美元的范围内。这些交易发生在半径为两到七英里的范围内。
- en: Spark will run this fraud detection program and extract a certain number of
    potential **Fraud** data points, for example, a good split dataset to use.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 将运行这个欺诈检测程序并提取一定数量的潜在**欺诈**数据点，例如，一个良好的分割数据集来使用。
- en: 'A breakup of the dataset could be as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集的划分可能如下：
- en: 65% as training examples to train the model on
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 65% 作为训练示例，用于训练模型
- en: 35% as a cross-validation set with instances of potential fraud in it
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 35% 作为包含潜在欺诈实例的交叉验证集
- en: Evaluating the performance of our fraud detection algorithm is not done by the
    accuracy metric. The reason lies in the fact that if only a handful of samples
    exist that should be flagged as fraudulent, the algorithm that successfully flagged
    non-fraudulent samples may fail to flag the ones that are indeed potentially fraudulent.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估我们的欺诈检测算法的性能不是通过准确度指标来完成的。原因在于，如果只有少数样本应该被标记为欺诈，那么成功标记非欺诈样本的算法可能无法标记那些确实可能是欺诈的样本。
- en: Instead, we will compute the precision and recall metrics and, consequently,
    the F1 measure as a way to evaluate the performance of the fraud detection classifier.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相反，我们将计算精确度和召回率指标，从而计算F1度量，作为评估欺诈检测分类器性能的一种方式。
- en: Now we will look at an overview of our project, where we formulate the problem
    at hand, mostly in mathematical terms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将概述我们的项目，其中我们将手头的问题用数学术语表述。
- en: Project overview – problem formulation
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目概述 - 问题表述
- en: 'Here is a helpful flowchart formulates the fraud detection problem at hand:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有用的流程图，概述了当前手头的欺诈检测问题：
- en: '![](img/36e52fd5-e8f2-4634-aac4-6c074b1f698a.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36e52fd5-e8f2-4634-aac4-6c074b1f698a.jpg)'
- en: Fraud detection flowchart
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测流程图
- en: That said, let's get started. We do so by setting up an implementation infrastructure
    first.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们开始吧。我们首先设置一个实现基础设施。
- en: Getting started
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: In this section, we will talk about setting up an implementation infrastructure
    or using the existing infrastructure from previous chapters. The following upgrades
    to your infrastructure are optional but recommended.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论设置实现基础设施或使用前几章中现有的基础设施。以下是对您基础设施的升级，这些升级是可选的但推荐使用的。
- en: 'Starting in [Chapter 3](dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml), *Stock
    Price Predictions*, we set up the **Hortonworks Development Platform** (**HDP**)
    Sandbox as a virtual machine. That said, three kinds of (isolated) HDP Sandbox
    deployments are possible. Of the three, we will only talk about two of them and
    those are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第3章](dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml)开始，*股票价格预测*，我们设置了**Hortonworks开发平台**（**HDP**）Sandbox作为虚拟机。话虽如此，有三种（隔离的）HDP
    Sandbox部署方式。在这三种中，我们只会讨论两种，它们是：
- en: '**Virtual machine environment (with Hypervisor) for Sandbox deployment:** HDP
    Sandbox running in an Oracle VirtualBox virtual machine.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有虚拟机管理程序的虚拟机环境（用于Sandbox部署）**：HDP Sandbox在Oracle VirtualBox虚拟机中运行。'
- en: '**A cloud-based environment for Sandbox deployment:** This option is attractive
    for users that have host machine memory limitations. The Sandbox runs in the cloud
    as opposed to a virtual machine that runs on your host machine.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于云的Sandbox部署环境**：此选项对主机内存有限制的用户来说很有吸引力。Sandbox在云中运行，而不是在您的宿主机上运行的虚拟机。'
- en: 'With that opening point made, you can always run the fraud detection system
    code on the Spark shell. You have two options here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出这个观点之后，你总是可以在Spark shell上运行欺诈检测系统代码。这里你有两个选择：
- en: Use **Simple Build Tool** (**SBT**) to build and deploy your application in
    your Spark environment
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**简单构建工具**（**SBT**）在Spark环境中构建和部署您的应用程序
- en: Open a Spark shell, develop interactively, and run it inside the shell
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开Spark shell，进行交互式开发，并在shell中运行它
- en: 'Last, but not least, you need the following software to simply launch the Spark
    shell and develop locally:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，你需要以下软件来简单地启动Spark shell并在本地开发：
- en: Spark 2.3
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.3
- en: Scala 2.11.12
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.12
- en: SBT 1.0.4
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBT 1.0.4
- en: IntelliJ IDEA 2018.1.5 Community Edition
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IntelliJ IDEA 2018.1.5社区版
- en: At least 16 GB of RAM; 32 GB is even better.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少16 GB的RAM；32 GB更好。
- en: Please refer back to the *Setting up prerequisite software* section in [Chapter
    1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict the Class of a Flower
    from the Iris Dataset*. That sets us up with Java, Scala, and Spark, allowing
    us to use the Spark shell for interactive development.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第1章](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml)中的*设置先决软件*部分，*从Iris数据集预测花的类别*。这为我们设置了Java、Scala和Spark，使我们能够使用Spark
    shell进行交互式开发。
- en: In the next section, we will explain how to set up the Hortonworks Sandbox deployment
    on the Microsoft Azure Cloud, thereby proceeding to the implementation part.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释如何在Microsoft Azure云上设置Hortonworks Sandbox部署，从而进入实现部分。
- en: Setting up Hortonworks Sandbox in the cloud
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云中设置Hortonworks Sandbox
- en: You might ask, why Microsoft Azure? Like any popular cloud services provider
    out there (and Google Compute Cloud is another solid offering), Azure prides itself
    as a set of robust cloud services that lets individual users and organizations
    develop and provision their applications on the cloud.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问，为什么是Microsoft Azure？像任何流行的云服务提供商一样（Google Compute Cloud也是另一个不错的选择），Azure自豪地提供了一套强大的云服务，允许个人用户和组织在云上开发和部署他们的应用程序。
- en: Creating your Azure free account, and signing in
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您的Azure免费账户并登录
- en: 'The following are the steps to create an account:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为创建账户所需的步骤：
- en: To get started, head over to the [https://azure.microsoft.com/en-us/](https://azure.microsoft.com/en-us/)
    web address. Click on the Start free button. Doing so will take you to the account
    login screen. If you do not have an account, set one up. This process only gives
    you a new Microsoft Azure account, not an actual cloud account; at least not yet.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始使用，请访问以下网址：[https://azure.microsoft.com/en-us/](https://azure.microsoft.com/en-us/)。点击“开始免费使用”按钮。这样做将带您进入账户登录界面。如果您还没有账户，请创建一个。这个过程只会给您一个新的Microsoft
    Azure账户，而不是一个实际的云账户；至少目前还不是。
- en: Enter a password that you would like to choose with your new Microsoft Azure
    Cloud account.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入您希望与您的新Microsoft Azure云账户一起选择的密码。
- en: Next, head over to your email account, and verify your email address by entering
    the security code.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，转到您的电子邮件账户，通过输入安全码来验证您的电子邮件地址。
- en: If everything went well, your new Azure account is ready to use.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一切顺利，您的新Azure账户即可使用。
- en: The next step is accessing the Azure Marketplace. In this marketplace, we will
    proceed with further steps, such as deployment. For now, let's locate the marketplace.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是访问Azure Marketplace。在这个市场中，我们将进行进一步的步骤，例如部署。现在，让我们找到市场。
- en: The Azure Marketplace
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure Marketplace
- en: 'The following are the steps involved:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下涉及到的步骤：
- en: 'Head over to Azure Marketplace:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到Azure Marketplace：
- en: '![](img/a105cc3d-59a2-4601-a9ea-a0bc27b68e72.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a105cc3d-59a2-4601-a9ea-a0bc27b68e72.jpg)'
- en: Looking for Azure Marketplace
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找Azure Marketplace
- en: 'After clicking on Azure Marketplace, type in `Hortwonworks` in the search box
    on the right, as shown in the following screenshot:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在点击Azure Marketplace后，在右侧的搜索框中输入“Hortwonworks”，如下面的截图所示：
- en: '![](img/f4c527e1-8eb2-4aca-b6b9-35a61dc39dc1.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4c527e1-8eb2-4aca-b6b9-35a61dc39dc1.jpg)'
- en: Searching for the Hortonworks Data Platform link
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索Hortonworks数据平台链接
- en: Click on the Hortonworks Data Platform link, as shown in the preceding screenshot.
    This takes you to the HDP Sandbox page.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 点击前面的截图所示的Hortonworks数据平台链接。这会将您带到HDP沙盒页面。
- en: The HDP Sandbox home page
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDP沙盒主页
- en: 'On the HDP page, here is what you can expect to find:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在HDP页面上，您可以期待以下内容：
- en: Sign in to the Microsoft Azure Marketplace portal.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到Microsoft Azure Marketplace门户。
- en: Kick off the Sandbox creation process, and the follow-through steps.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动沙盒创建过程，并执行后续步骤。
- en: After the Sandbox creation has finished, deployment should follow. All the subsequent
    steps will be explained as we work through this.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在沙盒创建完成后，应该进行部署。所有后续步骤将在我们进行过程中进行解释。
- en: 'Please perform the following steps to meet the preceding expectations:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请执行以下步骤以满足前面的期望：
- en: 'For now, click on the GET IT NOW button, as shown in the following screenshot:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，请点击以下截图所示的“GET IT NOW”按钮：
- en: '![](img/8111ec04-79d3-4679-9858-ec1503014bd9.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8111ec04-79d3-4679-9858-ec1503014bd9.jpg)'
- en: The HDP GET IT NOW page
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: HDP GET IT NOW 页面
- en: 'After clicking on the GET IT NOW blue button, the next thing likely to happen
    is a dialog asking you to sign in to Microsoft Azure:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“GET IT NOW”蓝色按钮后，接下来可能发生的事情是一个对话框，要求您登录到Microsoft Azure：
- en: '![](img/c16dbbda-e67d-4fa4-bab4-d14a3e341625.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c16dbbda-e67d-4fa4-bab4-d14a3e341625.jpg)'
- en: Sign-in page
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 登录页面
- en: 'The sign-in process takes you to another page, a page that lists a form where
    you need to enter details, such as your name, work email, job role, country/region,
    and phone number. You will be redirected to the Azure portal, as shown in the
    following screenshot:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录过程将带您进入另一个页面，一个列出您需要输入详细信息的表单的页面，例如您的姓名、工作电子邮件、工作角色、国家/地区和电话号码。您将被重定向到Azure门户，如下面的截图所示：
- en: '![](img/d5bc5154-adc2-45f5-9d8e-16d021ac4bad.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5bc5154-adc2-45f5-9d8e-16d021ac4bad.jpg)'
- en: The Welcome to Microsoft Azure portal page
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到Microsoft Azure门户页面
- en: On the welcome screen of your portal, you can take the tour if you like, or
    simply click Maybe later and get down to business. Let's get on with the business
    of getting the Sandbox deployed on Azure.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的门户欢迎屏幕上，如果您愿意，可以参加游览，或者简单地点击“稍后考虑”并开始工作。让我们继续在Azure上部署沙盒的业务。
- en: 'The next step is to locate the blue Create button, as shown in the following
    screenshot:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是找到下面的截图所示的蓝色创建按钮：
- en: '![](img/54a09a37-be44-4144-807f-2e63895701b3.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54a09a37-be44-4144-807f-2e63895701b3.jpg)'
- en: Create button screenshot
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 创建按钮截图
- en: 'Now, we will finally get started on the Sandbox deployment process. Note that
    clicking on the Create button does not actually start the deployment process right
    away. The first order of business is creating a virtual machine:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将开始Sandbox部署过程。请注意，点击创建按钮并不会立即启动部署过程。首要任务是创建一个虚拟机：
- en: '![](img/f7451809-8065-46bc-9533-2cd30bef2244.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7451809-8065-46bc-9533-2cd30bef2244.jpg)'
- en: The creation of virtual machines screenshot
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机创建截图
- en: Let's get started on a virtual machine now.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始虚拟机的部署。
- en: Implementation objectives
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现目标
- en: 'The following implementation objectives cover the steps required to implement
    the Gaussian Distribution algorithm. We will perform the preliminary steps, such
    as **Exploratory Data Analysis** (**EDA**) once, and develop the implementation
    code. The breakdown is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实现目标涵盖了实现高斯分布算法所需的步骤。我们将执行初步步骤，例如**探索性数据分析**（**EDA**），然后开发实现代码。具体如下：
- en: Get the breast cancer dataset from the **UCI Machine Learning Repository**.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**UCI机器学习仓库**获取乳腺癌数据集。
- en: Carry out the preliminary EDA in the Sandbox Zeppelin Notebook environment (or
    Spark shell), and run a statistical analysis.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Sandbox Zeppelin Notebook环境（或Spark shell）中执行初步的EDA，并进行统计分析。
- en: Develop the pipeline incrementally in your local Spark shell, or in a Zeppelin
    Notebook on your host machine, managed virtual machine, or your virtual machine
    on the Azure Cloud. Or simply run your Spark fraud detection application as an
    SBT application and deploy it in Spark by creating an Uber JAR using `spark-submit`.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的本地Spark shell中逐步开发管道，或者在主机机器上的Zeppelin笔记本、托管虚拟机或Azure云上的虚拟机上，或者简单地作为SBT应用程序运行你的Spark欺诈检测应用程序，并通过创建Uber
    JAR使用`spark-submit`来部署它。
- en: 'Flesh out your code in IntelliJ. What this means is:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ中完善你的代码。这意味着：
- en: Do not forget to wire up all the necessary dependencies in the `build.sbt` file.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要忘记在`build.sbt`文件中连接所有必要的依赖。
- en: Interpret the classification process, because you want to know how well the
    classifier performed, how close the predicted values are to those in the original
    dataset, and so on.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释分类过程，因为你想要知道分类器表现如何，预测值与原始数据集中的值有多接近，等等。
- en: Implementation steps
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现步骤
- en: A good way to start is to download the skeleton SBT project archive file from
    the `ModernScalaProjects_Code` folder.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的开始是从`ModernScalaProjects_Code`文件夹下载SBT项目存档文件。
- en: 'The step-by-step instructions are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: EDA on the testing (cross-validation) dataset.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试（交叉验证）数据集上进行EDA。
- en: Calculate the probability densities.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算概率密度。
- en: Generate a fraud detection model.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成欺诈检测模型。
- en: 'Generate scores that measure the accuracy of the model:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成衡量模型准确性的分数：
- en: Compute the best F1 score
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最佳F1分数
- en: Compute the best error term
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最佳误差项
- en: Calculate outliers by repeatedly having the model generate predictions over
    each value of error term in a range.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过让模型在误差项的范围内的每个值上重复生成预测来计算异常值。
- en: We will create a `FraudDetection` trait now.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个`FraudDetection`特质。
- en: Create the FraudDetection trait
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建FraudDetection特质
- en: 'In an empty `FraudDetectionPipeline.scala` file, add in the following imports.
    These are imports that we need for `Logging`, `Feature Vector` creation, `DataFrame`
    and `SparkSession` respectively:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个空的`FraudDetectionPipeline.scala`文件中，添加以下导入。这些是我们需要的`Logging`、`特征向量`创建、`DataFrame`和`SparkSession`的导入：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is an all-important trait, holding a method for `SparkSession` creation
    and other code. The code from classes that extend from this trait can share one
    instance of a `SparkSession`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个至关重要的特质，包含用于创建`SparkSession`和其他代码的方法。从这个特质扩展的类可以共享一个`SparkSession`的实例：
- en: '[PRE2]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we need the path to the testing dataset, meant for cross-validation,
    which is crucial to our classification:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要测试数据集的路径，用于交叉验证，这对我们的分类至关重要：
- en: '[PRE3]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The entry point to programming Spark with the `Dataset` and `DataFrame` API
    is the `SparkSession`, which creates `SparkSession` for our fraud detection pipeline,
    as shown in the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Dataset`和`DataFrame` API编程Spark的入口点是`SparkSession`，它为我们的欺诈检测管道创建`SparkSession`，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These two statements turn off `INFO` statements. Feel free to turn them on,
    as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个语句关闭了`INFO`语句。你可以随意打开它们，如下所示：
- en: '[PRE5]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The path to the dataset file is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件的路径如下：
- en: '[PRE6]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create convenience tuples for holding the name of the `features` vector column
    and the `label` column names `val fdTrainSet_EDA = ("summary","fdEdaFeaturesVectors")` are
    as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 创建方便的元组来保存`features`向量列和`label`列名称`val fdTrainSet_EDA = ("summary","fdEdaFeaturesVectors")`如下：
- en: '[PRE7]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This method allows us to transform our cross-validation dataset into a `DataFrame`.
    It takes in the training `Dataset` and outputs `DataFrame`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法允许我们将交叉验证数据集转换为`DataFrame`。它接受训练`Dataset`并输出`DataFrame`：
- en: '[PRE8]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create `DataFrame` by transforming an array of a tuple of `Feature Vectors`
    and `Label`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过转换一个包含`Feature Vectors`和`Label`元组的数组来创建`DataFrame`：
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This `package` statement is required. Place this file in a package of your
    choice:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`package`语句是必需的。将此文件放置在你选择的包中：
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following imports are required as we are going to pass around features
    as serializable vectors; therefore, we need `DenseVector`. The other imports are
    self-explanatory. For example, we cannot do without `DataFrame`, our fundamental
    unit of abstraction of data in Spark:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下导入是必需的，因为我们将要传递可序列化的特征向量；因此，我们需要`DenseVector`。其他导入都是不言自明的。例如，在Spark中，我们无法没有`DataFrame`，这是我们数据抽象的基本单元：
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The purpose of this program is to develop a data pipeline for detecting outliers,
    so-called data anomalies representing data points that point to fraud:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序的目的是为了开发一个检测异常值的数据管道，所谓数据异常是指指向欺诈的数据点：
- en: '[PRE12]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The pipeline program entry point is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 管道程序入口点如下：
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, convert raw data from a training dataset to `DataFrame`. The test data
    resides in the `testing.csv` file that is right under the root of our SBT project
    folder. The training data contains two columns holding double values. The first
    column contains `Cost Data` and the second column contains `Distance`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将训练数据集的原始数据转换为`DataFrame`。测试数据位于我们的SBT项目文件夹根目录下的`testing.csv`文件中。训练数据包含两个包含双精度值的列。第一列包含`Cost
    Data`，第二列包含`Distance`：
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The raw training set `DataFrame` (`rawTrainsSetForEda`) we just obtained is
    meant for EDA. For example, we could inspect it for missing values or characters
    that don''t belong. That said, we will inspect all the rows of the dataset by
    running the `show()` command:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚获得的原始训练集`DataFrame`(`rawTrainsSetForEda`)是用于EDA的。例如，我们可以通过运行`show()`命令检查缺失值或不属于的字符。话虽如此，我们将通过运行`show()`命令检查数据集的所有行：
- en: '[PRE15]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This builds a `DataFrame` for a testing dataset. Its main purpose is cross-validation,
    an important ML technique that is explained back in the theory section:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这为测试数据集构建了一个`DataFrame`。其主要目的是交叉验证，这是一种重要的机器学习技术，在理论部分已有解释：
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Display the new `DataFrame` test set:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新的`DataFrame`测试集：
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, the `summary` method is used to display the results of the EDA, including
    standard deviation, mean, and variances. These are required in our fraud detection
    classification task:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`summary`方法显示EDA的结果，包括标准差、平均值和方差。这些是我们欺诈检测分类任务所必需的：
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, persist the training set dataframe with the default storage level ``(`MEMORY_AND_DISK`)``.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用默认存储级别`(`MEMORY_AND_DISK`)`持久化训练集数据框。
- en: 'Now, let''s display the summary of that Spark extracted for us:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们显示Spark为我们提取的摘要：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Extract the `summary` row containing `"mean"` for both columns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 提取包含`"mean"`的`summary`行，对于两个列：
- en: '[PRE20]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Display the new `DataFrame`—a one-row `DataFrame`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新的`DataFrame`——一个单行`DataFrame`：
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, convert `DataFrame` to an array of rows. Issuing a `map` function call
    on this array extracts the `"mean"` row into this array containing a single tuple
    containing the mean values of both the `Cost` and `Distance` values. The result
    is a `"Mean Pairs"` array containing a tuple of strings:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将`DataFrame`转换为行数组。在这个数组上发出`map`函数调用将`"mean"`行提取到包含单个元组的数组中，该元组包含`Cost`和`Distance`值的平均值。结果是包含字符串元组的`"Mean
    Pairs"`数组：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Pull out both values of `mean` from the `"Mean Pairs"` tuple:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从`"Mean Pairs"`元组中提取两个`mean`值：
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we want to issue the `where` query to extract just the values of standard
    deviation
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要发出`where`查询，仅提取标准差的值
- en: 'from the training dataset EDA statistics `DataFrame`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练数据集EDA统计`DataFrame`中：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Display the content of this standard `DataFrame` deviations:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 显示此标准`DataFrame`偏差的内容：
- en: '[PRE25]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We have `DataFrame` containing two standard deviation values. Extract these
    values into an array of tuples. This array contains just one tuple holding two
    string values of standard deviation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含两个标准差值的`DataFrame`。将这些值提取到一个元组数组中。这个数组只包含一个元组，包含两个表示标准差的字符串值：
- en: '[PRE26]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Extract the standard deviation values from the enclosing tuple. First, we need
    the standard deviation value for the transaction feature:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从包围的元组中提取标准差值。首先，我们需要交易特征的方差值：
- en: '[PRE27]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we want to extract the standard deviation of the distance feature:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要提取距离特征的方差：
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s build the following tuple pair for making a `broadcast` variable:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建以下元组对以创建`broadcast`变量：
- en: '[PRE29]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let''s wrap the preceding tuple pair in `DenseVector`. Why are we doing
    this? Simple. We need a vector to send down into the cluster as a `broadcast`
    variable. Create an array of transaction vectors containing both mean and standard
    deviation values. What we want inside the transaction vector looks like this `[Transaction
    Mean, Transaction SD]`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们将前面的元组对包裹在`DenseVector`中。我们为什么要这样做？很简单。我们需要一个向量将作为`broadcast`变量发送到集群中。创建一个包含均值和标准差值的交易向量数组。我们希望在交易向量中看到的内容如下
    `[交易均值, 交易标准差]`:'
- en: '[PRE30]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s display the vector. Scala offers an elegant way to display the content
    of a collection structure:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示这个向量。Scala提供了一种优雅的方式来显示集合结构的内容：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create an array of distance vectors containing mean and standard deviation.
    And since we need a second vector, it looks like this `[Distance Mean, Distance
    SD)`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '创建一个包含均值和标准差值的距离向量数组。由于我们需要第二个向量，它看起来像这样 `[距离均值, 距离标准差)`:'
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Display the standard deviation vector:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 显示标准差向量：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is now time to broadcast the following into all the nodes in your Spark
    cluster:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将以下内容广播到你的Spark集群中的所有节点：
- en: Mean vectors
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值向量
- en: Standard deviation vectors
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准差向量
- en: Broadcasting mean and standard deviation vectors
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播均值和标准差向量
- en: 'The `sparkContext` variable provides a `broadcast` method:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`sparkContext`变量提供了一个`broadcast`方法：'
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Everything we did up until now was in preparation to calculate the PDF, a value
    that denotes the probability of fraud. That said, we will see how to calculate
    a PDF in the next section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所做的一切都是为了计算PDF，这是一个表示欺诈概率的值。话虽如此，我们将在下一节中看到如何计算PDF。
- en: Calculating PDFs
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算PDFs
- en: For each sample in the testing dataset, a PDF value is to be computed. Therefore,
    we will iterate over the entire dataset, and pass each feature vector into the
    `probabilityDensity` function. That function should, for each sample, compute
    the `probabilityDensity` value of type `Double`. Ultimately, we build an entire
    dataset containing PDF values, all of type `Double`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试数据集中的每个样本，都需要计算一个PDF值。因此，我们将遍历整个数据集，并将每个特征向量传递到`probabilityDensity`函数中。该函数应该对每个样本计算类型为`Double`的`probabilityDensity`值。最终，我们构建了一个包含PDF值的整个数据集，所有类型为`Double`。
- en: 'The `testingDF` dataframe contains two columns:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`testingDF`数据框包含两列：'
- en: '`feature` vector column'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`特征向量列`'
- en: '`label`'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`标签`'
- en: Therefore, inside `map`, each `labelledFeatureVectorRow.getAs()` returns the
    feature vector.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在`map`中，每个`labelledFeatureVectorRow.getAs()`返回特征向量。
- en: Next, extract the features vector out of the testing dataframe. For each sample
    in the testing dataset, a PDF value is to be computed. Therefore, we will iterate
    over the entire dataset, and pass each feature vector into the `probabilityDensity`
    function. That function should, for each sample, compute the `probabilityDensity`
    value of type `Double`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从测试数据框中提取特征向量。对于测试数据集中的每个样本，都需要计算一个PDF值。因此，我们将遍历整个数据集，并将每个特征向量传递到`probabilityDensity`函数中。该函数应该对每个样本计算类型为`Double`的`probabilityDensity`值。
- en: Ultimately, we build an entire dataset containing probability density function
    values, all of type `Double`. The following are two datasets that are extracted
    from the EDA dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们构建了一个包含概率密度函数值的整个数据集，所有类型为`Double`。以下是从EDA数据集中提取的两个数据集。
- en: 'The first dataset shows `mean`, while the second one shows standard deviation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个数据集显示`均值`，而第二个数据集显示`标准差`:'
- en: '[PRE35]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We need `implicits` here to account for the fact that an implicit encoder is
    needed to convert `DataFrame` into `Dataset` holding doubles. These `implicits`
    are provided to us by our `SparkSession` instance `session`, as shown here:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里需要`implicits`来考虑这样一个事实，即需要一个隐式编码器将`DataFrame`转换为包含双精度值的`Dataset`。这些`implicits`由我们的`SparkSession`实例`session`提供，如下所示：
- en: '[PRE36]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Iterate over the testing dataset, and for each feature vector row inside it.
    Apply a method to calculate a probability density value. A dataset of doubles
    is returned that has with in it double values of computed probabilities:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历测试数据集，并对其中的每个特征向量行应用一种方法来计算概率密度值。返回一个包含计算概率的双值数据集：
- en: '[PRE37]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Display the dataset of probability values, as shown here:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 显示概率值的数据集，如下所示：
- en: '[PRE38]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: F1 score
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1分数
- en: Since our fraudulent class is the important one, we are going to need the following
    to help us choose a classifier that has the best F1 score as follows
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的欺诈类别是重要的，我们需要以下内容来帮助我们选择具有最佳F1分数的分类器，如下所示
- en: Labeled data is the test `DataFrame`—`` `testingDf` ``
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签数据是测试`DataFrame`——`testingDf`
- en: PDF—a product of probabilities computed in the `probabilityDensity` function
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDF——`probabilityDensity`函数中计算的概率的乘积
- en: 'Keeping in mind that we need labeled data (points) to arrive at the best F1
    score, the following background information is helpful as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们需要标记数据（点）来达到最佳的F1分数，以下背景信息是有帮助的如下：
- en: What is the role of cross-validation? To understand cross-validation, we revisit
    the validation process, where a subset of the samples from the training set is
    used to train the model. Cross-validation is an improvement over validation, because
    of the fact that there are more observations available for the model to be fitted.
    Cross-validation becomes an attractive option because we are now able to pick
    from several models.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证的作用是什么？为了理解交叉验证，我们回顾一下验证过程，其中使用训练集样本的一个子集来训练模型。由于模型有更多的观测值可供拟合，交叉验证是对验证的一种改进。因为现在我们能够从多个模型中进行选择，交叉验证变得更有吸引力。
- en: 'What are the prerequisites before calculating the Epsilon and the F1 score?
    Those prerequisites are:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算Epsilon和F1分数之前需要哪些先决条件？这些先决条件如下：
- en: The step size = *the maximum of probability density - the minimum probability
    density / 500*
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长 = *概率密度最大值 - 概率密度最小值 / 500*
- en: To arrive at the maximum value of probability, we need calculate the probability
    density value for each sample in the testing dataset and then derive the maximum
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了达到概率的最大值，我们需要计算测试数据集中每个样本的概率密度值，然后得出最大值
- en: 'Labeled data points and features in the form of vectors:'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以向量形式表示的标记数据点和特征：
- en: '![](img/5a4bcc95-97b3-4a21-abec-4d3ccce167ec.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5a4bcc95-97b3-4a21-abec-4d3ccce167ec.png)'
- en: Labeled data points and features in the form of vectors
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 以向量形式表示的标记数据点和特征
- en: 'For each labeled data point, one PDF value needs to be computed. This requires
    prior knowledge of the following statistics:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标记的数据点，需要计算一个PDF值。这需要以下统计数据的先验知识：
- en: Mean cost, mean distance
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均成本，平均距离
- en: The standard cost deviation and, the standard deviation of the instance
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准成本偏差和实例的标准偏差
- en: The data point itself
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点本身
- en: Each labeled data point (or feature) has two mean values and two standard deviations
    value. One labeled data point (feature) has one cost value and one distance value.
    We will take both mean and standard deviation into account and calculate the PDF
    for that labeled data point.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记的数据点（或特征）有两个平均值和两个标准偏差值。一个标记的数据点（特征）有一个成本值和一个距离值。我们将考虑平均值和标准偏差，并计算该标记数据点的PDF。
- en: It turns out that each feature pair returns a probability density value pair,
    of course. We take the product of both probabilities in this pair and return a
    combined probability value as the PDF for that particular feature row.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，每个特征对返回一个概率密度值对，当然。我们取这对概率的乘积，并返回一个组合概率值作为该特定特征行的PDF。
- en: 'We have enough information to calculate the combined probability value:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有足够的信息来计算组合概率值：
- en: '[PRE39]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The strategy is to convert the labeled feature passed in into an array and then
    invoke a `map` operation on it. We want the probability density for each data
    point. Each data point is like this, `{ feature 1, feature 2}`, where `feature
    1` has one mean, and one standard deviation, and `feature 2` has the same. To
    do this, we need to apply the mean and standard deviation of the entire dataset.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是将传入的标记特征转换为数组，然后在该数组上调用`map`操作。我们想要每个数据点的概率密度。每个数据点像这样，`{特征1, 特征2}`，其中`特征1`有一个平均值和一个标准偏差，`特征2`也有相同的。为了做到这一点，我们需要应用整个数据集的平均值和标准偏差。
- en: 'Inside the PDF, write the following code. Define an inner function inside called
    `featureDoubles`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在PDF内部，写下以下代码。在内部定义一个名为`featureDoubles`的内部函数：
- en: '[PRE40]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Inside the inner function, place the following code. The idea is to assemble
    a list of tuples that looks like this:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部函数中，放置以下代码。思路是组装一个看起来像这样的元组列表：
- en: '[PRE41]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the first tuple, `93.47397393` is the transaction feature value of the first row,
    `79.98437516250003` is the mean of all the transactions, and `18.879` is the standard
    deviation of all the transactions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个元组中，`93.47397393`是第一行的交易特征值，`79.98437516250003`是所有交易的均值，`18.879`是所有交易的标准差。
- en: In the second tuple, `6.075334279` is the distance feature value of the first
    row, `5.13..` is the mean of all the distances, and `1.9488924384002693` is the
    standard deviation of all the distances.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个元组中，`6.075334279`是第一行的距离特征值，`5.13..`是所有距离的均值，`1.9488924384002693`是所有距离的标准差。
- en: The goal then is to calculate the PDF of each data point. Since there are two
    features, there are two PDFs per data point. The combined probability then is
    the product of the two.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是计算每个数据点的PDF。由于有两个特征，每个数据点有两个PDF。因此，联合概率是这两个概率的乘积。
- en: 'What we first want is a tuple containing our testing `DataFrame features` in
    the form of an `Array[Double]`, a transaction `DataFrame` containing mean and
    standard deviation in the form of an `Array[Double]`, and a distance `DataFrame`
    containing mean and standard deviation in the form of `Array[Double]`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先希望得到一个包含我们的测试`DataFrame features`的元组，形式为`Array[Double]`，一个包含均值和标准差的交易`DataFrame`，形式为`Array[Double]`，以及一个包含均值和标准差的距离`DataFrame`，形式为`Array[Double]`：
- en: '[PRE42]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The inner function, `featureDoubles`, is done. Let's define a variable called
    `pdF`, representing a list of probability density tuples.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 内部函数`featureDoubles`已完成。让我们定义一个名为`pdF`的变量，表示概率密度元组的列表。
- en: 'The finished function, `featureDoubles`, looks like this:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的函数`featureDoubles`看起来像这样：
- en: '[PRE43]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we need a PDF calculator that we call `pDfCalculator`. `pDfCalculator` is
    a name that represents a `List` of three tuples per row in the test dataset; each
    tuple contains three doubles. What we want inside each tuple looks like this:
    a transaction value, a transaction mean, and a transaction sd. Since there is
    a second tuple, the second tuple looks like this: (distance value, distance mean,
    and distance sd). When `map` is invoked, each tuple in turn inside the list (of
    tuples) is operated upon. Three values inside the tuple are there for a reason.
    All three are needed to calculate the probability density of one feature, as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个PDF计算器，我们称之为`pDfCalculator`。`pDfCalculator`是一个表示测试数据集中每行三个元组的`List`的名称；每个元组包含三个双精度浮点数。我们希望每个元组内部看起来像这样：一个交易值，一个交易均值和一个交易标准差。由于存在第二个元组，第二个元组看起来像这样：（距离值，距离均值和距离标准差）。当`map`被调用时，列表（元组列表）中的每个元组依次被操作。元组内的三个值都有其存在的理由。所有三个都是计算一个特征的概率密度的必要条件，如下所示：
- en: '[PRE44]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the next line of code, we will carry out a `map` operation. Inside the `map` operation,
    we will apply a function that returns `probabilityDensityValue`. To do this, we
    will fall back on the `NormalDistribution` class in the Apache Commons Math library.
    The constructor to the `NormalDistribution` class requires mean and standard deviation
    and the data point itself. There are two features in a single feature row. That
    feature row contains two columns—`Transaction` and `Distance`. Therefore, `map`
    will successively calculate the probability density value for both data points,
    a `Transaction` data point, and a `Distance` data point, respectively:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一行代码中，我们将执行一个`map`操作。在`map`操作中，我们将应用一个返回`probabilityDensityValue`的函数。为此，我们将求助于Apache
    Commons Math库中的`NormalDistribution`类。`NormalDistribution`类的构造函数需要均值、标准差以及数据点本身。一个特征行中包含两个特征。那个特征行包含两列——`Transaction`和`Distance`。因此，`map`将依次计算两个数据点的概率密度值，一个`Transaction`数据点和一个`Distance`数据点：
- en: '[PRE45]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `probabilityDensity` function in its final form looks something like this:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最终形式的`probabilityDensity`函数看起来像这样：
- en: '[PRE46]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Finally, we want the `probabilityDensity` function to return the probability
    density value computed by `val probabilityDensityValue`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望`probabilityDensity`函数返回由`val probabilityDensityValue`计算出的概率密度值。
- en: With the probability density calculation behind us, we will now shift our attention
    to calculating the best error term. The error term is denoted by the Greek letter,
    Epsilon.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成概率密度计算后，我们现在将注意力转向计算最佳误差项。误差项用希腊字母Epsilon表示。
- en: Calculating the best error term and best F1 score
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算最佳误差项和最佳F1分数
- en: 'In this section, we will write a function to calculate:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写一个函数来计算：
- en: The best error term (also known as the Epsilon)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳误差项（也称为 Epsilon）
- en: The best F1 score
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳 F1 分数
- en: 'We start by defining a function called `errorTermCalc`. What parameters does
    it require? It is apparent that we need two parameters:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个名为 `errorTermCalc` 的函数。它需要哪些参数？很明显，我们需要两个参数：
- en: Our cross-validation dataset—`DataFrame`
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的交叉验证数据集——`DataFrame`
- en: '`DataFrame`, containing probability densities'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含概率密度的 `DataFrame`
- en: There you go. We now have a function called `errorTermCalc` that takes two parameters
    and returns both the best error term and the best F1.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们现在有一个名为 `errorTermCalc` 的函数，它接受两个参数并返回最佳误差项和最佳 F1。
- en: Why are these numbers important? To answer that question, we want to detect
    the outliers first. These are labeled data points that indicate fraud. Generating
    a new dataframe that classifies labeled data points as either fraudulent or not
    is the first step before we get down to calculating the best error term and best
    F1.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这些数字很重要？为了回答这个问题，我们首先想要检测异常值。这些标记数据点表明欺诈。在计算最佳误差项和最佳 F1 之前，首先生成一个新的数据框，将标记数据点分类为欺诈或非欺诈是第一步。
- en: 'These are:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是：
- en: Smallest of all the PDFs—`pDfMin`
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 PDF 中的最小值——`pDfMin`
- en: Largest of all the PDFs—`pDFMax`
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 PDF 中的最大值——`pDFMax`
- en: The algorithm inside the code starts by assigning a baseline value, `pdFMin`,
    to the best error term. It then loops through to `pdfMax` in terms of a carefully
    selected step size. Remember, we want the best F1 score and the best way to do
    that is to assign `0`, the worst value that any F1 score can potentially take.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的算法首先将基线值 `pdFMin` 分配给最佳误差项。然后，它通过一个仔细选择的步长循环到 `pdfMax`。记住，我们想要最佳 F1 分数，而做到这一点的方法是将
    `0` 分配给任何 F1 分数可能具有的最差值。
- en: 'The algorithm then works its way through the range of PDF values and arrives
    at the final values of both the best error term and the best F1 score, respectively.
    Basically, these final values are obtained starting with the following primary
    checks:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 算法随后通过 PDF 值的范围，并分别得到最佳误差项和最佳 F1 分数的最终值。基本上，这些最终值是通过以下主要检查获得的：
- en: Is an intermediate calculated value of the best F1 score greater than 0?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最佳 F1 分数的中间计算值是否大于 0？
- en: Is the value of any error term less than the probability density value for a
    certain labeled data point?
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何误差项的值是否小于某个标记数据点的概率密度值？
- en: Remember, there is a probability density for every data point; so we are cycling
    through the entire (cross-) validation dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每个数据点都有一个概率密度；因此，我们正在遍历整个（交叉）验证数据集。
- en: If the test in primary check 1 passes, the best F1 at that point is updated
    to that intermediate calculated value of the best F1 score. PDF step-wise values
    are compared to the predefined error term. If the PDF is less than the predefined
    Epsilon, that data point becomes a predicted fraudulent value.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果主要检查 1 中的测试通过，则该点的最佳 F1 更新为最佳 F1 分数的中间计算值。逐步比较 PDF 值与预定义的误差项。如果 PDF 小于预定义的
    Epsilon，则该数据点成为预测的欺诈值。
- en: 'The definition for the `errorTermCalc` function looks like this:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`errorTermCalc` 函数的定义如下：'
- en: '[PRE47]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We will start fleshing out of details inside the curly braces of the new function.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始详细阐述新函数大括号内的细节。
- en: Maximum and minimum values of a probability density
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率密度的最大值和最小值
- en: 'Here is how we extract the smallest and largest values of the probability density:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何提取概率密度的最小值和最大值的：
- en: '[PRE48]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We need a reasonable, carefully selected step size for the error term. That
    is what we will be doing in the next step.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个合理、仔细选择的误差项步长。这就是我们在下一步将要做的。
- en: Step size for best error term calculation
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳误差项计算步骤大小
- en: 'Now, let''s define a `step` size to calculate the best Epsilon:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个 `step` 大小来计算最佳的 Epsilon：
- en: '[PRE49]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We need a loop for the algorithm to step through and calculate the `labelAndPredictions`
    dataframe at each `step` size value of the error term. This will also help us
    to find the best F1.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个循环，以便算法可以遍历并计算误差项的每个 `step` 大小值下的 `labelAndPredictions` 数据框。这也有助于我们找到最佳
    F1。
- en: A loop to generate the best F1 and the best error term
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成最佳 F1 和最佳误差项的循环
- en: 'Let''s find the best F1 for different Epsilon values:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到不同 Epsilon 值的最佳 F1：
- en: '[PRE50]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Broadcast the error term into Spark. Create the `broadcast` variable first:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 将误差项广播到 Spark。首先创建 `broadcast` 变量：
- en: '[PRE51]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Generate predictions here. If the `Double` value in the probability densities
    dataframe happens to be less than the `broadCastedErrorTerm`, that value is flagged
    as `fraud`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里生成预测。如果概率密度数据框中的`Double`值小于`broadCastedErrorTerm`，则该值被标记为`fraud`。
- en: 'It is possible you may run into the following error:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 可能你会遇到以下错误：
- en: '`Unable to find encoder for type stored in a Dataset. Primitive types (Int,
    String, etc) and Product types (case classes) are supported by importing spark.implicits._
    Support for serializing other types will be added in future releases`.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`Unable to find encoder for type stored in a Dataset. Primitive types (Int,
    String, etc) and Product types (case classes) are supported by importing spark.implicits._
    Support for serializing other types will be added in future releases`。'
- en: 'To get around this problem, we add the following `import` statement:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们添加了以下`import`语句：
- en: '[PRE52]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: For data of a certain datatype to be put away in a new `DataFrame`, Spark wants
    you to pass in appropriate `Encoders`. With that out of the way, let's get down
    to generating predictions.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将特定数据类型的数据放入新的`DataFrame`中，Spark要求你传入适当的`Encoders`。处理完这些之后，让我们开始生成预测。
- en: Generating predictions – outliers that represent fraud
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成预测 - 代表欺诈的异常值
- en: 'We start by transforming the `probabilityDensities` dataframe from before:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将之前的`probabilityDensities`数据框进行转换：
- en: '[PRE53]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, let''s create a new dataframe with two dataframes—the testing dataframe,
    and the final predictions dataframe. Drop the `"label"` column in the testing
    dataframe and do a cross-join with the `finalpreds` dataframe. Do not forget to
    persist the new dataframe with the default storage level `(MEMORY_AND_DISK)`:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个新的数据框，包含两个数据框 - 测试数据框和最终预测数据框。在测试数据框中删除`"label"`列，并与`finalpreds`数据框进行交叉连接。别忘了用默认存储级别`(MEMORY_AND_DISK)`持久化新的数据框：
- en: '[PRE54]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Next, we want to generate the best error term and the best F1 measure.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要生成最佳误差项和最佳F1度量。
- en: Generating the best error term and best F1 measure
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成最佳误差项和最佳F1度量
- en: 'In this section, we want to come up with the number of false positives, the
    number of true positives, and the number of false negatives. First, we want to
    know how many false positives there are:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们想要找出假阳性的数量、真阳性的数量和假阴性的数量。首先，我们想知道有多少假阳性：
- en: '[PRE55]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we want to know how many true positives there are:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想知道有多少真阳性：
- en: '[PRE56]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We also want to know how many false negatives exist:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想知道有多少假阴性存在：
- en: '[PRE57]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Now that we have `fNs`, `tPs`, and `fPs`, we can calculate the `precision` and
    `recall` metrics.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`fNs`、`tPs`和`fPs`，我们可以计算`precision`和`recall`指标。
- en: Preparing to compute precision and recall
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备计算精确度和召回率
- en: Here are the lines of code that implement a simple mathematical equation to
    come up with the precision and recall scores.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是实现一个简单的数学方程的代码行，用于计算精确度和召回率。
- en: 'Let''s calculate `precision` now:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算`precision`：
- en: '[PRE58]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Followed by a calculation of `recall`:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着计算`recall`：
- en: '[PRE59]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We have both `precision` and `recall`. This gives us what we need to calculate
    the F1 score or the `f1Measure`, as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们既有`precision`也有`recall`。这为我们提供了计算F1分数或`f1Measure`所需的内容：
- en: '[PRE60]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, let''s determine `bestErrorTermValue` and `bestF1measure`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们确定`bestErrorTermValue`和`bestF1measure`：
- en: '[PRE61]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We are almost done with the calculation of the best error term (Epsilon) and
    the best F1 measure.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了最佳误差项（Epsilon）和最佳F1度量的计算。
- en: In the next step, we will summarize what we just did to generate the best Epsilon
    and the best error term.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将总结我们刚刚如何生成最佳Epsilon和最佳误差项。
- en: A recap of how we looped through a ranger of Epsilons, the best error term,
    and the best F1 measure
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾我们如何遍历一系列Epsilons，最佳误差项和最佳F1度量
- en: 'We implemented a loop just prior to arriving here. Here is a breakdown of those
    steps in pseudocode:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在到达这里之前，我们实现了一个循环。以下是这些步骤的伪代码：
- en: '[PRE62]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In the preceding `Step 3`, we derived the`labelsAndPredictions` dataframe.
    In `Step 4`, we set out to calculate the following:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的`Step 3`中，我们推导出了`labelsAndPredictions`数据框。在`Step 4`中，我们着手计算以下内容：
- en: False positives
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性
- en: False negatives
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性
- en: True positives
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性
- en: 'In the next section, we will implement the method called `positivesNegatives`
    to calculate false positives, false negatives, and true positives. Here is a representation
    of the `evalScores` method function, where the algorithm does a lot of processing:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现名为`positivesNegatives`的方法来计算假阳性、假阴性和真阳性。以下是`evalScores`方法函数的表示，其中算法进行了大量的处理：
- en: '[PRE63]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Up until this point, we talked about wanting to calculate the best error term
    and the best F1 measure. Both of those metrics need computed values of precision
    and recall, which, in turn, depend on computed values of fPs, fNs, and tPs. That
    brings us to the next task of creating a function that calculates these numbers.
    That is the focus of the next step.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了想要计算最佳误差项和最佳F1度量。这两个指标都需要计算精确度和召回率的值，而这些值反过来又依赖于fPs、fNs和tPs的计算值。这引出了下一个任务，即创建一个计算这些数值的函数。这就是下一步的重点。
- en: Function to calculate false positives
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算假正例的函数
- en: In this section, we write a `positivesNegatives` function that takes in the
    `labelsAndPredictions` dataframe from `Step 3` and spits out either false positives,
    false negatives, or true positives, depending on what we want.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们编写了一个`positivesNegatives`函数，该函数接受来自第3步的`labelsAndPredictions`数据框，并输出假正例、假负例或真正例，具体取决于我们想要什么。
- en: 'It also takes in two other parameters:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 它还接受两个其他参数：
- en: 'A target label that can take the following values:'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目标标签，可以取以下值：
- en: A value of `1.0` for true positives
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真正例的值为 `1.0`
- en: A value of `0.0` for false positives
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假正例的值为 `0.0`
- en: A value of `1.0` for false negatives
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假负例的值为 `1.0`
- en: 'A final predicted value that can take the following values:'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最终的预测值，可以取以下值：
- en: A value of `1.0` for true positives
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真正例的值为 `1.0`
- en: A value of `1.0` for false positives
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假正例的值为 `1.0`
- en: A value of `0.0` for false negatives
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假负例的值为 `0.0`
- en: 'Accordingly, here is one method that calculates all three: true positives,
    false positives, and false negatives:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里有一个计算所有三个值（真正例、假正例和假负例）的方法：
- en: '[PRE64]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The body of the method is a single line of code that calculates a `Double`
    value, of course:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主体是一行代码，计算一个`Double`值，当然：
- en: '[PRE65]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The finished method looks like this:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的方法看起来是这样的：
- en: '[PRE66]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This completes the implementation of the fraud detection system. In the next
    section, we will summarize what this chapter has accomplished.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了欺诈检测系统的实现。在下一节中，我们将总结本章所取得的成果。
- en: Summary
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Fraud detection is not a supervised learning problem. We did not use the random
    forests algorithm, decision trees, or **logistic regression** (**LR**). Instead,
    we leveraged what is known as a Gaussian Distribution equation to build an algorithm
    that performed classification, which is really an anomaly detection or identification
    task. The importance of picking an appropriate Epsilon (error term) to enable
    the algorithm to find the anomalous samples cannot be overestimated. Otherwise,
    the algorithm could go off the mark and label non-fraudulent examples as anomalies
    or outliers that indicate a fraudulent transaction. The point is, tweaking the
    Epsilon parameter does help with a better fraud detection process.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测不是一个监督学习问题。我们没有使用随机森林算法、决策树或**逻辑回归**（**LR**）。相反，我们利用了所谓的高斯分布方程来构建一个执行分类的算法，这实际上是一个异常检测或识别任务。选择一个合适的Epsilon（误差项）以使算法能够找到异常样本的重要性不容小觑。否则，算法可能会走偏，将非欺诈示例标记为异常或异常值，这些异常值表明存在欺诈交易。关键是，调整Epsilon参数确实有助于更好的欺诈检测过程。
- en: A good part of the computational power required was devoted to finding the so-called
    best Epsilon. Computing the best Epsilon was one key part. The other part, of
    course, was the algorithm itself. This is where Spark helped out a lot. The Spark
    ecosystem provided us with a powerful environment, letting us write code that
    parallelizes and orchestrates our data analytics efficiently in a distributed
    manner.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的计算能力的大部分都用于寻找所谓的最佳Epsilon。计算最佳Epsilon是关键的一部分。当然，另一部分是算法本身。这正是Spark大显身手的地方。Spark生态系统为我们提供了一个强大的环境，让我们能够以分布式的方式高效地并行化和编排我们的数据分析代码。
- en: In the next chapter, we will carry out  data analysis tasks on flight performance
    data.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将对飞行性能数据进行数据分析任务。
- en: Questions
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following are questions that will consolidate and deepen your knowledge
    of fraud detection:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些将巩固和深化你对欺诈检测知识的问题：
- en: What is a Gaussian Distribution?
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯分布是什么？
- en: The algorithm in our fraud detection system requires something really important
    to be fed into it, before generating probabilities? What is that?
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的欺诈检测系统中，算法在生成概率之前需要输入一些非常重要的事情，那是什么？
- en: Why is the selection of an error term (Epsilon) such a big deal in detecting
    outliers and identifying the correct false positives and false negatives?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在检测异常值和识别正确的假阳性和假阴性时，选择一个误差项（Epsilon）如此重要？
- en: Why is fraud detection not exactly a classification problem?
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么欺诈检测并不完全是一个分类问题？
- en: Fraud detection is essentially an anomaly identification problem. Can you name
    two properties that define anomaly identification?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欺诈检测本质上是一个异常识别问题。你能说出定义异常识别的两个属性吗？
- en: Can you think of other applications that can leverage anomaly identification
    or outlier detection?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到其他可以利用异常识别或异常值检测的应用吗？
- en: Why is cross-validation so important?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么交叉验证如此重要？
- en: Why is our fraud detection problem not a supervised learning problem?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们的欺诈检测问题不是一个监督学习问题？
- en: Can you name a couple of ways to optimize the Gaussian Distribution algorithm?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出优化高斯分布算法的几种方法吗？
- en: Sometimes, our results may not be satisfactory because the algorithm failed
    to identify certain samples as a fraud. What could we do better?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时候，我们的结果可能不尽如人意，因为算法未能识别某些样本为欺诈。我们还能做些什么来改进？
- en: It is time to move to the last section, where we invite readers to further enrich
    their learning journey by referring to the resources indicated.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进入最后一部分了，我们将邀请读者通过参考所指示的资源来进一步丰富他们的学习之旅。
- en: Further reading
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: PayPal's data platforms carry out real-time decision making that prevents fraud.
    Their systems crunch several petabytes every single day. Check out [https://qcon.ai/](https://qcon.ai/)
    for a recent conference on AI and ML. Study their use cases and learn how companies
    such as PayPal leverage the latest advances in AI and ML to help combat fraud.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: PayPal 的数据平台执行实时决策，以防止欺诈。他们的系统每天处理数个PB的数据。查看 [https://qcon.ai/](https://qcon.ai/)
    了解最近的 AI 和 ML 会议。研究他们的用例，了解像 PayPal 这样的公司如何利用 AI 和 ML 的最新进展来帮助打击欺诈。
- en: Explore how Kafka can work with Spark to bring near real-time fraud detection
    to your fraud detection procedures.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 探索 Kafka 如何与 Spark 合作，将近乎实时的欺诈检测带到您的欺诈检测流程中。
- en: We are all familiar with Airbnb ([https://www.airbnb.com/trust](https://www.airbnb.com/trust)).
    Find out how Airbnb's trust and safety team is combating fraud, while they protect
    and grow their business model that is so critically based on trust.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都熟悉 Airbnb ([https://www.airbnb.com/trust](https://www.airbnb.com/trust))。了解
    Airbnb 的信任和安全团队是如何在保护并扩大其基于信任的商业模式的同时，打击欺诈的。
