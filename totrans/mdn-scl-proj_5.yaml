- en: Build a Fraud Detection System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to develop an algorithm based on the Gaussian
    Distribution function using Spark ML. We will apply the algorithm to detect fraud
    in transactions data. This kind of algorithm can be applied toward building robust
    fraud detection solutions for financial institutions, such as banks, which handle
    great quantities of online transactions.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of the Gaussian Distribution, the function is the notion of an
    **anomaly**. The fraud detection problem is only a classification task but in
    a very narrow sense. It is a balanced supervised learning problem. The term *balanced*
    refers to the fact that the positives in the dataset are of a small number in
    relation to the negatives. On the other hand, an anomaly detection problem is
    typically not balanced. The dataset contains a significantly small number of anomalies
    (positives) in relation to the negatives. The fraud detection problem is a prime
    example of an anomaly detection problem. This is a problem where the dataset has
    a small number of outliers or data points whose values depart considerably from
    normal, to-be-expected values.
  prefs: []
  type: TYPE_NORMAL
- en: The overarching learning objective of this chapter is to implement a Scala solution
    that will predict fraud in financial transactions. We will lean on the Spark ML
    library's APIs and its supporting libraries in order to build a fraud detection
    prediction application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project overview—problem formulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud detection problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fraud detection problem is not a supervised learning problem. We have an
    unbalanced class situation in our fraud detection scenario. What do we have to
    say about the importance of the F1 score in relation to the target variable? First,
    the target variable is a binary label. The F1 score is relevant to our fraud detection
    problem because we have an unbalanced class, where one class is practically more important
    than the other. What do we mean by that? The bottom line of the fraud detection
    classification process concerns whether a certain instance is fraudulent, and
    getting the classifier to classify or label this instance correctly as fraudulent.
    The emphasis is not on labeling an instance as non-fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reiterate, there are two classes in our fraud detection problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraudulent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-fraudulent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That said, we will now look at the dataset that this implementation depends
    upon
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection dataset at a glance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Download the dataset from the `ModernScalaProjects_Code` download folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e314c67-5cea-472f-aa67-d37a7e5da734.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The dataset that our fraud detection system is built on
  prefs: []
  type: TYPE_NORMAL
- en: The Gaussian Distribution function is the basis for our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: So, is the F1 score important? Yes. The F1 score cannot be ignored (in the case
    of a balanced class situation, the F1 score is not necessarily important). It
    is the measure of an ML binary classification's process accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: There is an F1 score for each class (one for fraudulent and the other for non-fraudulent). Therefore,
    if we want to compute the F1 score, we need to ensure that the F1 score is associated
    with the fraudulent class.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection in the context of ML is a classification technique method that
    allows us to build models that attempt to detect outliers. Flagging outliers leads
    us to do what is needed to address fraud. For example, if I swiped my card in
    Portland, Maine, on a whale watching vacation in a location more than 1,000 miles
    away from where I live, it is possible that an underlying fraud detection algorithm
    associated with my credit card will flag **fraud**. The distance, in this case,
    led the algorithm to claim that the said transaction at a certain seafood establishment
    place on Maine's waterfront was fake. This is a simple use case. There are other
    financial transactions that this algorithm is trained to monitor and flag fraud.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine a situation where Kate lost her card, and some random person
    picked that card up on the street (let us assume that Kate wasn't aware that she
    had lost her card until a day later) and tried to fill up his truck's gas tank
    with about $50 worth of gas. Even though this transaction was carried out, assuming
    that this person that attempted to use her card somehow got past the zip code
    check, Kate's credit card's fraud detection ML algorithm will flag as a suspicious
    transaction. Most likely, the algorithm will simply kick in and cause the transaction
    to fail or, even if that does not happen, she would get a call from the credit
    card company asking her where she used the card recently. In this case scenario,
    she got that call from the credit card company because the fraud detection algorithm
    flagged that particular transaction as suspicious, a fraudulent incident that
    requires the credit card company to take action.
  prefs: []
  type: TYPE_NORMAL
- en: A fraud detection system deals with massive amounts of data. A fraud detection
    classifier, as described in this chapter, will sift through a dataset of transaction
    data and process it. Spark's streaming capabilities allow us to detect outliers,
    samples in our dataset whose values do not fall within a normal, anticipated range
    of values. Detecting such values and generating a set of predictions flagging
    fraud is the emphasis of the fraud detection problem described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will evaluate the performance of the algorithm and look at whether it flags
    or tags non-fraudulent samples as fraudulent or fraudulent samples as fraudulent,
    by computing metrics, such as precision, recall, and a harmonic mean of the precision
    and recall, known as the F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Precision, recall, and the F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are important:'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of a mathematical function, an F1 score can be defined mathematically
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will talk a little about the F1 score or measure. The error term is denoted
    by the Epsilon symbol **(ε)**. Central to all this is the labeled input points
    in our unbalanced dataset. We are going to optimize the Epsilon parameter. How
    exactly do we do this? Let''s first find the best possible F1 score. What is an
    Epsilon? In statistics, it is an **error term**. A measurement may deviate from
    its expected value. For example, it be the mean height of all males in a certain
    population. What is its purpose? We may denote an arbitrarily small, positive
    number as **ε**. Before computing the Epsilon, let''s persist the testing dataframe. We
    have the following tasks cut out for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a function to help us calculate the best Epsilon and the best F1 score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand what the F1 score is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum possible value that the F1 measure can take is 1. It denotes the
    level of correctness of the classification process by the classifier, that is,
    the proportion of samples or instances that are classified with a high degree
    of correctness or precision. It also tells us how robust (or not) the classifier
    is—whether the classifier missed classifying only a small number of samples or
    more. The F1 score is like a balanced mean between the precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: F1 becomes more important in an unbalanced class situation. It is a more practical
    measure than **accuracy**. Even though accuracy is more intuitive, and because
    of the fact that both false positives and false negatives are considered, a weighted
    score such as F1 becomes more meaningful in understanding the degree of correctness
    of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Carefully selecting features is a crucial step in the formulation of a fraud
    detection program. Selecting many features or features, that does not contribute
    in a meaningful way, may impact the performance or skew predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we wish to flag fraudulent transactions, we should start small and build
    a system with two features only that we deem to meaningfully contribute to the
    classification. We choose two features that we represent in the dataset as columns
    of double values in a comma-separated file. These features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transaction**: Money spent on buying a certain commodity or service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance**: A geographical distance from the address of the cardholder on
    file, or a general distance outside of the perimeter defined by the zip code of
    the cardholder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That said, the goal for our fraud detection algorithm is that, with the feature
    selection process in place, we want to process the data, crunch all of the data
    points in our dataset, and flag potential fraud. This is a good place to bring
    in the Gaussian Distribution function, the basis for how we implement our fraud
    detection model. We need to talk a little more about this equation. It will help
    us understand exactly what our algorithm does and why it does what it does. In
    the next section, we will talk about the Gaussian Distribution function.
  prefs: []
  type: TYPE_NORMAL
- en: The Gaussian Distribution function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Gaussian Distribution function is also known as a **bell curve** or a **normal
    distribution curve**. The bell curve has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: This kind of distribution (of data) is said to be a continuous distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is spread out in this curve so that it converges around the bell portion
    (the highest point) of the curve, rather than to the left or the right. This center
    at the highest point is also the mean of the curve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The highest point on the bell curve corresponds to the highest probability of
    an occurrence and, as the curve tapers off, the probability of occurrences slides
    down to positions on either side of the curve on its slopes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of this property, the bell curve is also known as a normal distribution
    curve. All it needs are the standard deviation and the (population) mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a normal distribution, the three statistics, mean, mode and median, all bear
    the same value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The normal distribution curve is plotted with values of probability densities
    (a.k.a. normal frequencies). Referring to the figure present in *Project overview—problem
    formulation* section, the following are the meanings of the symbols in the equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: µ = Mean of the population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: σ = Standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x = Plotted on the *x* axis, this represents a continuous random variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: e = Natural logarithmic base, with a value of 2.71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: π = 3.1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean is simply a net value that is equal to the sum of all values of the
    data points, divided by the number of data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is to be noted that *y* is nothing but `f(x)` or `p(x)`, the values of which
    are plotted on the *y* axis of the bell curve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates a bell curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42d2d95d-7717-4897-862c-42ab143de09f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Bell curve
  prefs: []
  type: TYPE_NORMAL
- en: Non-fraudulent data comprises the bulk of our data. Such data is clustered in
    or close to the peak of the bell curve. In general terms, the top of the bell
    curve represents an event or a data point with the highest probability of occurrence.
    The tapering edges of the curve are where anomalies or outliers indicating fraud
    are found.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned fraud detection as a classification problem in a sense. It falls
    under the banner of anomaly detection. The table that follows describes the fundamental
    differences between a classification task and an anomaly detection task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24625414-7a53-44f6-95dc-15fd02b34a56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Classification versus anomaly identification task
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the preceding table, it is clear that the reasons that stand out
    to justify the use of an anomaly identification system are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Samples that may be anomalous in one dataset may not be anomalous when they
    are new, incoming samples in another dataset of financial transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, consider a breast cancer sample in an experimental unit of
    breast cancer samples that are classified as **malignant**. If the same sample
    is an incoming sample to experimental unit 2, the result of the classification
    will be the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where does Spark fit in all this?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you run Spark locally or in an environment where you have a cluster
    operating several distributed nodes, Spark will ramp up. In a local Spark environment,
    Spark will treat CPU cores as resources in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Gaussian Distribution algorithm is worth looking into. Let's see what our
    approach should be in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the high-level architecture for our fraud
    detection pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/065ef341-edbd-48f7-83fd-8348c1fdd32a.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level architecture for the fraud detection pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a quick overview of the fraud detection process:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we compute statistics on the training set, which serves as a cross-validation
    dataset. We are interested in the mean and standard deviation from the statistical
    numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we want to compute the net **probability density function** (**PDF**)
    for each sample in our cross-validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We derive the net probability density as a product of the individual probability
    densities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the algorithm, we compare a PDF value with an **Error Term** value to
    determine whether that sample represents an outlier, a potential **Fraud** transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We optimize our classification process by having the algorithm executed on a
    cross-validation dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As part of the optimization process, our algorithm computes the best possible
    value for the **Error Term**, an optimal value for the **Error Term** (denoted
    by an Epsilon) corresponding to a computed value of the **F1** score that is the
    highest. The algorithm, after repeated iterations, will come up with this highest
    score of the Epsilon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A look at the dataset tells us that the most transaction data points fall within
    the range of 55-105 dollars. These transactions occurred within a radius of two
    to seven miles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark will run this fraud detection program and extract a certain number of
    potential **Fraud** data points, for example, a good split dataset to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A breakup of the dataset could be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 65% as training examples to train the model on
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 35% as a cross-validation set with instances of potential fraud in it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of our fraud detection algorithm is not done by the
    accuracy metric. The reason lies in the fact that if only a handful of samples
    exist that should be flagged as fraudulent, the algorithm that successfully flagged
    non-fraudulent samples may fail to flag the ones that are indeed potentially fraudulent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead, we will compute the precision and recall metrics and, consequently,
    the F1 measure as a way to evaluate the performance of the fraud detection classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we will look at an overview of our project, where we formulate the problem
    at hand, mostly in mathematical terms.
  prefs: []
  type: TYPE_NORMAL
- en: Project overview – problem formulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a helpful flowchart formulates the fraud detection problem at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36e52fd5-e8f2-4634-aac4-6c074b1f698a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fraud detection flowchart
  prefs: []
  type: TYPE_NORMAL
- en: That said, let's get started. We do so by setting up an implementation infrastructure
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will talk about setting up an implementation infrastructure
    or using the existing infrastructure from previous chapters. The following upgrades
    to your infrastructure are optional but recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting in [Chapter 3](dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml), *Stock
    Price Predictions*, we set up the **Hortonworks Development Platform** (**HDP**)
    Sandbox as a virtual machine. That said, three kinds of (isolated) HDP Sandbox
    deployments are possible. Of the three, we will only talk about two of them and
    those are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual machine environment (with Hypervisor) for Sandbox deployment:** HDP
    Sandbox running in an Oracle VirtualBox virtual machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A cloud-based environment for Sandbox deployment:** This option is attractive
    for users that have host machine memory limitations. The Sandbox runs in the cloud
    as opposed to a virtual machine that runs on your host machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that opening point made, you can always run the fraud detection system
    code on the Spark shell. You have two options here:'
  prefs: []
  type: TYPE_NORMAL
- en: Use **Simple Build Tool** (**SBT**) to build and deploy your application in
    your Spark environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open a Spark shell, develop interactively, and run it inside the shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Last, but not least, you need the following software to simply launch the Spark
    shell and develop locally:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala 2.11.12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBT 1.0.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ IDEA 2018.1.5 Community Edition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least 16 GB of RAM; 32 GB is even better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer back to the *Setting up prerequisite software* section in [Chapter
    1](4d645e21-43b1-49dd-99ad-4059360bfc15.xhtml), *Predict the Class of a Flower
    from the Iris Dataset*. That sets us up with Java, Scala, and Spark, allowing
    us to use the Spark shell for interactive development.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain how to set up the Hortonworks Sandbox deployment
    on the Microsoft Azure Cloud, thereby proceeding to the implementation part.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Hortonworks Sandbox in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might ask, why Microsoft Azure? Like any popular cloud services provider
    out there (and Google Compute Cloud is another solid offering), Azure prides itself
    as a set of robust cloud services that lets individual users and organizations
    develop and provision their applications on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your Azure free account, and signing in
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps to create an account:'
  prefs: []
  type: TYPE_NORMAL
- en: To get started, head over to the [https://azure.microsoft.com/en-us/](https://azure.microsoft.com/en-us/)
    web address. Click on the Start free button. Doing so will take you to the account
    login screen. If you do not have an account, set one up. This process only gives
    you a new Microsoft Azure account, not an actual cloud account; at least not yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a password that you would like to choose with your new Microsoft Azure
    Cloud account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, head over to your email account, and verify your email address by entering
    the security code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If everything went well, your new Azure account is ready to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is accessing the Azure Marketplace. In this marketplace, we will
    proceed with further steps, such as deployment. For now, let's locate the marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: The Azure Marketplace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Head over to Azure Marketplace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a105cc3d-59a2-4601-a9ea-a0bc27b68e72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looking for Azure Marketplace
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on Azure Marketplace, type in `Hortwonworks` in the search box
    on the right, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f4c527e1-8eb2-4aca-b6b9-35a61dc39dc1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Searching for the Hortonworks Data Platform link
  prefs: []
  type: TYPE_NORMAL
- en: Click on the Hortonworks Data Platform link, as shown in the preceding screenshot.
    This takes you to the HDP Sandbox page.
  prefs: []
  type: TYPE_NORMAL
- en: The HDP Sandbox home page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the HDP page, here is what you can expect to find:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign in to the Microsoft Azure Marketplace portal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kick off the Sandbox creation process, and the follow-through steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the Sandbox creation has finished, deployment should follow. All the subsequent
    steps will be explained as we work through this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please perform the following steps to meet the preceding expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, click on the GET IT NOW button, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8111ec04-79d3-4679-9858-ec1503014bd9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The HDP GET IT NOW page
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on the GET IT NOW blue button, the next thing likely to happen
    is a dialog asking you to sign in to Microsoft Azure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c16dbbda-e67d-4fa4-bab4-d14a3e341625.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sign-in page
  prefs: []
  type: TYPE_NORMAL
- en: 'The sign-in process takes you to another page, a page that lists a form where
    you need to enter details, such as your name, work email, job role, country/region,
    and phone number. You will be redirected to the Azure portal, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d5bc5154-adc2-45f5-9d8e-16d021ac4bad.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Welcome to Microsoft Azure portal page
  prefs: []
  type: TYPE_NORMAL
- en: On the welcome screen of your portal, you can take the tour if you like, or
    simply click Maybe later and get down to business. Let's get on with the business
    of getting the Sandbox deployed on Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to locate the blue Create button, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/54a09a37-be44-4144-807f-2e63895701b3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Create button screenshot
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will finally get started on the Sandbox deployment process. Note that
    clicking on the Create button does not actually start the deployment process right
    away. The first order of business is creating a virtual machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f7451809-8065-46bc-9533-2cd30bef2244.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The creation of virtual machines screenshot
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started on a virtual machine now.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following implementation objectives cover the steps required to implement
    the Gaussian Distribution algorithm. We will perform the preliminary steps, such
    as **Exploratory Data Analysis** (**EDA**) once, and develop the implementation
    code. The breakdown is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the breast cancer dataset from the **UCI Machine Learning Repository**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Carry out the preliminary EDA in the Sandbox Zeppelin Notebook environment (or
    Spark shell), and run a statistical analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop the pipeline incrementally in your local Spark shell, or in a Zeppelin
    Notebook on your host machine, managed virtual machine, or your virtual machine
    on the Azure Cloud. Or simply run your Spark fraud detection application as an
    SBT application and deploy it in Spark by creating an Uber JAR using `spark-submit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Flesh out your code in IntelliJ. What this means is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do not forget to wire up all the necessary dependencies in the `build.sbt` file.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the classification process, because you want to know how well the
    classifier performed, how close the predicted values are to those in the original
    dataset, and so on.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good way to start is to download the skeleton SBT project archive file from
    the `ModernScalaProjects_Code` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The step-by-step instructions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: EDA on the testing (cross-validation) dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the probability densities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a fraud detection model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate scores that measure the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the best F1 score
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the best error term
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate outliers by repeatedly having the model generate predictions over
    each value of error term in a range.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will create a `FraudDetection` trait now.
  prefs: []
  type: TYPE_NORMAL
- en: Create the FraudDetection trait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an empty `FraudDetectionPipeline.scala` file, add in the following imports.
    These are imports that we need for `Logging`, `Feature Vector` creation, `DataFrame`
    and `SparkSession` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an all-important trait, holding a method for `SparkSession` creation
    and other code. The code from classes that extend from this trait can share one
    instance of a `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need the path to the testing dataset, meant for cross-validation,
    which is crucial to our classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The entry point to programming Spark with the `Dataset` and `DataFrame` API
    is the `SparkSession`, which creates `SparkSession` for our fraud detection pipeline,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'These two statements turn off `INFO` statements. Feel free to turn them on,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The path to the dataset file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create convenience tuples for holding the name of the `features` vector column
    and the `label` column names `val fdTrainSet_EDA = ("summary","fdEdaFeaturesVectors")` are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This method allows us to transform our cross-validation dataset into a `DataFrame`.
    It takes in the training `Dataset` and outputs `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `DataFrame` by transforming an array of a tuple of `Feature Vectors`
    and `Label`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This `package` statement is required. Place this file in a package of your
    choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following imports are required as we are going to pass around features
    as serializable vectors; therefore, we need `DenseVector`. The other imports are
    self-explanatory. For example, we cannot do without `DataFrame`, our fundamental
    unit of abstraction of data in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The purpose of this program is to develop a data pipeline for detecting outliers,
    so-called data anomalies representing data points that point to fraud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The pipeline program entry point is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, convert raw data from a training dataset to `DataFrame`. The test data
    resides in the `testing.csv` file that is right under the root of our SBT project
    folder. The training data contains two columns holding double values. The first
    column contains `Cost Data` and the second column contains `Distance`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The raw training set `DataFrame` (`rawTrainsSetForEda`) we just obtained is
    meant for EDA. For example, we could inspect it for missing values or characters
    that don''t belong. That said, we will inspect all the rows of the dataset by
    running the `show()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This builds a `DataFrame` for a testing dataset. Its main purpose is cross-validation,
    an important ML technique that is explained back in the theory section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the new `DataFrame` test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the `summary` method is used to display the results of the EDA, including
    standard deviation, mean, and variances. These are required in our fraud detection
    classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, persist the training set dataframe with the default storage level ``(`MEMORY_AND_DISK`)``.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s display the summary of that Spark extracted for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the `summary` row containing `"mean"` for both columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the new `DataFrame`—a one-row `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, convert `DataFrame` to an array of rows. Issuing a `map` function call
    on this array extracts the `"mean"` row into this array containing a single tuple
    containing the mean values of both the `Cost` and `Distance` values. The result
    is a `"Mean Pairs"` array containing a tuple of strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull out both values of `mean` from the `"Mean Pairs"` tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, we want to issue the `where` query to extract just the values of standard
    deviation
  prefs: []
  type: TYPE_NORMAL
- en: 'from the training dataset EDA statistics `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the content of this standard `DataFrame` deviations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We have `DataFrame` containing two standard deviation values. Extract these
    values into an array of tuples. This array contains just one tuple holding two
    string values of standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the standard deviation values from the enclosing tuple. First, we need
    the standard deviation value for the transaction feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we want to extract the standard deviation of the distance feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build the following tuple pair for making a `broadcast` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s wrap the preceding tuple pair in `DenseVector`. Why are we doing
    this? Simple. We need a vector to send down into the cluster as a `broadcast`
    variable. Create an array of transaction vectors containing both mean and standard
    deviation values. What we want inside the transaction vector looks like this `[Transaction
    Mean, Transaction SD]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s display the vector. Scala offers an elegant way to display the content
    of a collection structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an array of distance vectors containing mean and standard deviation.
    And since we need a second vector, it looks like this `[Distance Mean, Distance
    SD)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the standard deviation vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now time to broadcast the following into all the nodes in your Spark
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard deviation vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broadcasting mean and standard deviation vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sparkContext` variable provides a `broadcast` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Everything we did up until now was in preparation to calculate the PDF, a value
    that denotes the probability of fraud. That said, we will see how to calculate
    a PDF in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating PDFs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For each sample in the testing dataset, a PDF value is to be computed. Therefore,
    we will iterate over the entire dataset, and pass each feature vector into the
    `probabilityDensity` function. That function should, for each sample, compute
    the `probabilityDensity` value of type `Double`. Ultimately, we build an entire
    dataset containing PDF values, all of type `Double`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `testingDF` dataframe contains two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature` vector column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, inside `map`, each `labelledFeatureVectorRow.getAs()` returns the
    feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Next, extract the features vector out of the testing dataframe. For each sample
    in the testing dataset, a PDF value is to be computed. Therefore, we will iterate
    over the entire dataset, and pass each feature vector into the `probabilityDensity`
    function. That function should, for each sample, compute the `probabilityDensity`
    value of type `Double`.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, we build an entire dataset containing probability density function
    values, all of type `Double`. The following are two datasets that are extracted
    from the EDA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first dataset shows `mean`, while the second one shows standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We need `implicits` here to account for the fact that an implicit encoder is
    needed to convert `DataFrame` into `Dataset` holding doubles. These `implicits`
    are provided to us by our `SparkSession` instance `session`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate over the testing dataset, and for each feature vector row inside it.
    Apply a method to calculate a probability density value. A dataset of doubles
    is returned that has with in it double values of computed probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the dataset of probability values, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since our fraudulent class is the important one, we are going to need the following
    to help us choose a classifier that has the best F1 score as follows
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data is the test `DataFrame`—`` `testingDf` ``
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDF—a product of probabilities computed in the `probabilityDensity` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keeping in mind that we need labeled data (points) to arrive at the best F1
    score, the following background information is helpful as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of cross-validation? To understand cross-validation, we revisit
    the validation process, where a subset of the samples from the training set is
    used to train the model. Cross-validation is an improvement over validation, because
    of the fact that there are more observations available for the model to be fitted.
    Cross-validation becomes an attractive option because we are now able to pick
    from several models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What are the prerequisites before calculating the Epsilon and the F1 score?
    Those prerequisites are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The step size = *the maximum of probability density - the minimum probability
    density / 500*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To arrive at the maximum value of probability, we need calculate the probability
    density value for each sample in the testing dataset and then derive the maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labeled data points and features in the form of vectors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5a4bcc95-97b3-4a21-abec-4d3ccce167ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Labeled data points and features in the form of vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'For each labeled data point, one PDF value needs to be computed. This requires
    prior knowledge of the following statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean cost, mean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard cost deviation and, the standard deviation of the instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data point itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each labeled data point (or feature) has two mean values and two standard deviations
    value. One labeled data point (feature) has one cost value and one distance value.
    We will take both mean and standard deviation into account and calculate the PDF
    for that labeled data point.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that each feature pair returns a probability density value pair,
    of course. We take the product of both probabilities in this pair and return a
    combined probability value as the PDF for that particular feature row.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have enough information to calculate the combined probability value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The strategy is to convert the labeled feature passed in into an array and then
    invoke a `map` operation on it. We want the probability density for each data
    point. Each data point is like this, `{ feature 1, feature 2}`, where `feature
    1` has one mean, and one standard deviation, and `feature 2` has the same. To
    do this, we need to apply the mean and standard deviation of the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the PDF, write the following code. Define an inner function inside called
    `featureDoubles`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the inner function, place the following code. The idea is to assemble
    a list of tuples that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the first tuple, `93.47397393` is the transaction feature value of the first row,
    `79.98437516250003` is the mean of all the transactions, and `18.879` is the standard
    deviation of all the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: In the second tuple, `6.075334279` is the distance feature value of the first
    row, `5.13..` is the mean of all the distances, and `1.9488924384002693` is the
    standard deviation of all the distances.
  prefs: []
  type: TYPE_NORMAL
- en: The goal then is to calculate the PDF of each data point. Since there are two
    features, there are two PDFs per data point. The combined probability then is
    the product of the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we first want is a tuple containing our testing `DataFrame features` in
    the form of an `Array[Double]`, a transaction `DataFrame` containing mean and
    standard deviation in the form of an `Array[Double]`, and a distance `DataFrame`
    containing mean and standard deviation in the form of `Array[Double]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The inner function, `featureDoubles`, is done. Let's define a variable called
    `pdF`, representing a list of probability density tuples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The finished function, `featureDoubles`, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need a PDF calculator that we call `pDfCalculator`. `pDfCalculator` is
    a name that represents a `List` of three tuples per row in the test dataset; each
    tuple contains three doubles. What we want inside each tuple looks like this:
    a transaction value, a transaction mean, and a transaction sd. Since there is
    a second tuple, the second tuple looks like this: (distance value, distance mean,
    and distance sd). When `map` is invoked, each tuple in turn inside the list (of
    tuples) is operated upon. Three values inside the tuple are there for a reason.
    All three are needed to calculate the probability density of one feature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next line of code, we will carry out a `map` operation. Inside the `map` operation,
    we will apply a function that returns `probabilityDensityValue`. To do this, we
    will fall back on the `NormalDistribution` class in the Apache Commons Math library.
    The constructor to the `NormalDistribution` class requires mean and standard deviation
    and the data point itself. There are two features in a single feature row. That
    feature row contains two columns—`Transaction` and `Distance`. Therefore, `map`
    will successively calculate the probability density value for both data points,
    a `Transaction` data point, and a `Distance` data point, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `probabilityDensity` function in its final form looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we want the `probabilityDensity` function to return the probability
    density value computed by `val probabilityDensityValue`.
  prefs: []
  type: TYPE_NORMAL
- en: With the probability density calculation behind us, we will now shift our attention
    to calculating the best error term. The error term is denoted by the Greek letter,
    Epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the best error term and best F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will write a function to calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: The best error term (also known as the Epsilon)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best F1 score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start by defining a function called `errorTermCalc`. What parameters does
    it require? It is apparent that we need two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Our cross-validation dataset—`DataFrame`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`, containing probability densities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There you go. We now have a function called `errorTermCalc` that takes two parameters
    and returns both the best error term and the best F1.
  prefs: []
  type: TYPE_NORMAL
- en: Why are these numbers important? To answer that question, we want to detect
    the outliers first. These are labeled data points that indicate fraud. Generating
    a new dataframe that classifies labeled data points as either fraudulent or not
    is the first step before we get down to calculating the best error term and best
    F1.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are:'
  prefs: []
  type: TYPE_NORMAL
- en: Smallest of all the PDFs—`pDfMin`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Largest of all the PDFs—`pDFMax`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm inside the code starts by assigning a baseline value, `pdFMin`,
    to the best error term. It then loops through to `pdfMax` in terms of a carefully
    selected step size. Remember, we want the best F1 score and the best way to do
    that is to assign `0`, the worst value that any F1 score can potentially take.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm then works its way through the range of PDF values and arrives
    at the final values of both the best error term and the best F1 score, respectively.
    Basically, these final values are obtained starting with the following primary
    checks:'
  prefs: []
  type: TYPE_NORMAL
- en: Is an intermediate calculated value of the best F1 score greater than 0?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the value of any error term less than the probability density value for a
    certain labeled data point?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember, there is a probability density for every data point; so we are cycling
    through the entire (cross-) validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If the test in primary check 1 passes, the best F1 at that point is updated
    to that intermediate calculated value of the best F1 score. PDF step-wise values
    are compared to the predefined error term. If the PDF is less than the predefined
    Epsilon, that data point becomes a predicted fraudulent value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition for the `errorTermCalc` function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We will start fleshing out of details inside the curly braces of the new function.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum and minimum values of a probability density
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how we extract the smallest and largest values of the probability density:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We need a reasonable, carefully selected step size for the error term. That
    is what we will be doing in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Step size for best error term calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s define a `step` size to calculate the best Epsilon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We need a loop for the algorithm to step through and calculate the `labelAndPredictions`
    dataframe at each `step` size value of the error term. This will also help us
    to find the best F1.
  prefs: []
  type: TYPE_NORMAL
- en: A loop to generate the best F1 and the best error term
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s find the best F1 for different Epsilon values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Broadcast the error term into Spark. Create the `broadcast` variable first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Generate predictions here. If the `Double` value in the probability densities
    dataframe happens to be less than the `broadCastedErrorTerm`, that value is flagged
    as `fraud`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible you may run into the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Unable to find encoder for type stored in a Dataset. Primitive types (Int,
    String, etc) and Product types (case classes) are supported by importing spark.implicits._
    Support for serializing other types will be added in future releases`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get around this problem, we add the following `import` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: For data of a certain datatype to be put away in a new `DataFrame`, Spark wants
    you to pass in appropriate `Encoders`. With that out of the way, let's get down
    to generating predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions – outliers that represent fraud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by transforming the `probabilityDensities` dataframe from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a new dataframe with two dataframes—the testing dataframe,
    and the final predictions dataframe. Drop the `"label"` column in the testing
    dataframe and do a cross-join with the `finalpreds` dataframe. Do not forget to
    persist the new dataframe with the default storage level `(MEMORY_AND_DISK)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Next, we want to generate the best error term and the best F1 measure.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the best error term and best F1 measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we want to come up with the number of false positives, the
    number of true positives, and the number of false negatives. First, we want to
    know how many false positives there are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to know how many true positives there are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to know how many false negatives exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have `fNs`, `tPs`, and `fPs`, we can calculate the `precision` and
    `recall` metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to compute precision and recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are the lines of code that implement a simple mathematical equation to
    come up with the precision and recall scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate `precision` now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Followed by a calculation of `recall`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We have both `precision` and `recall`. This gives us what we need to calculate
    the F1 score or the `f1Measure`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s determine `bestErrorTermValue` and `bestF1measure`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We are almost done with the calculation of the best error term (Epsilon) and
    the best F1 measure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will summarize what we just did to generate the best Epsilon
    and the best error term.
  prefs: []
  type: TYPE_NORMAL
- en: A recap of how we looped through a ranger of Epsilons, the best error term,
    and the best F1 measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We implemented a loop just prior to arriving here. Here is a breakdown of those
    steps in pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding `Step 3`, we derived the`labelsAndPredictions` dataframe.
    In `Step 4`, we set out to calculate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: False positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False negatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next section, we will implement the method called `positivesNegatives`
    to calculate false positives, false negatives, and true positives. Here is a representation
    of the `evalScores` method function, where the algorithm does a lot of processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Up until this point, we talked about wanting to calculate the best error term
    and the best F1 measure. Both of those metrics need computed values of precision
    and recall, which, in turn, depend on computed values of fPs, fNs, and tPs. That
    brings us to the next task of creating a function that calculates these numbers.
    That is the focus of the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Function to calculate false positives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we write a `positivesNegatives` function that takes in the
    `labelsAndPredictions` dataframe from `Step 3` and spits out either false positives,
    false negatives, or true positives, depending on what we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also takes in two other parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A target label that can take the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `1.0` for true positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `0.0` for false positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `1.0` for false negatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A final predicted value that can take the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `1.0` for true positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `1.0` for false positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `0.0` for false negatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accordingly, here is one method that calculates all three: true positives,
    false positives, and false negatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The body of the method is a single line of code that calculates a `Double`
    value, of course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The finished method looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: This completes the implementation of the fraud detection system. In the next
    section, we will summarize what this chapter has accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fraud detection is not a supervised learning problem. We did not use the random
    forests algorithm, decision trees, or **logistic regression** (**LR**). Instead,
    we leveraged what is known as a Gaussian Distribution equation to build an algorithm
    that performed classification, which is really an anomaly detection or identification
    task. The importance of picking an appropriate Epsilon (error term) to enable
    the algorithm to find the anomalous samples cannot be overestimated. Otherwise,
    the algorithm could go off the mark and label non-fraudulent examples as anomalies
    or outliers that indicate a fraudulent transaction. The point is, tweaking the
    Epsilon parameter does help with a better fraud detection process.
  prefs: []
  type: TYPE_NORMAL
- en: A good part of the computational power required was devoted to finding the so-called
    best Epsilon. Computing the best Epsilon was one key part. The other part, of
    course, was the algorithm itself. This is where Spark helped out a lot. The Spark
    ecosystem provided us with a powerful environment, letting us write code that
    parallelizes and orchestrates our data analytics efficiently in a distributed
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will carry out  data analysis tasks on flight performance
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are questions that will consolidate and deepen your knowledge
    of fraud detection:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a Gaussian Distribution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm in our fraud detection system requires something really important
    to be fed into it, before generating probabilities? What is that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the selection of an error term (Epsilon) such a big deal in detecting
    outliers and identifying the correct false positives and false negatives?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is fraud detection not exactly a classification problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fraud detection is essentially an anomaly identification problem. Can you name
    two properties that define anomaly identification?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think of other applications that can leverage anomaly identification
    or outlier detection?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is cross-validation so important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is our fraud detection problem not a supervised learning problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name a couple of ways to optimize the Gaussian Distribution algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes, our results may not be satisfactory because the algorithm failed
    to identify certain samples as a fraud. What could we do better?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is time to move to the last section, where we invite readers to further enrich
    their learning journey by referring to the resources indicated.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PayPal's data platforms carry out real-time decision making that prevents fraud.
    Their systems crunch several petabytes every single day. Check out [https://qcon.ai/](https://qcon.ai/)
    for a recent conference on AI and ML. Study their use cases and learn how companies
    such as PayPal leverage the latest advances in AI and ML to help combat fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Explore how Kafka can work with Spark to bring near real-time fraud detection
    to your fraud detection procedures.
  prefs: []
  type: TYPE_NORMAL
- en: We are all familiar with Airbnb ([https://www.airbnb.com/trust](https://www.airbnb.com/trust)).
    Find out how Airbnb's trust and safety team is combating fraud, while they protect
    and grow their business model that is so critically based on trust.
  prefs: []
  type: TYPE_NORMAL
