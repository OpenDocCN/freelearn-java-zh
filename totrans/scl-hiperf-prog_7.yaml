- en: Chapter 7. Architecting for Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 为性能而架构
- en: 'We have come a long way in our exploration of Scala and various techniques
    to write performant code. In this final chapter, we look at more open-ended topics.
    The final topics are largely applicable beyond Scala and the JVM. We dive into
    various tools and practices to improve the architecture and the design of an application.
    In this chapter, we explore the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索Scala和各种编写高性能代码的技术方面，我们已经取得了长足的进步。在本章的最后，我们探讨了一些更开放的话题。最后的话题在很大程度上适用于Scala和JVM之外。我们深入研究了各种工具和实践，以提高应用程序的架构和设计。在本章中，我们探讨了以下主题：
- en: Conflict-free replicated data types (CRDTs)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无冲突复制数据类型（CRDTs）
- en: Throughput and latency impact of queueing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列的吞吐量和延迟影响
- en: The Free monad
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Free monad
- en: Distributed automated traders
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式自动化交易员
- en: 'Thanks to our hard work, MVT is thriving. The sales department is signing contracts
    like there is no tomorrow, and the sales bell is ringing from sunrise to sunset.
    The order book is able to handle more orders, and as a result of the increase
    in traffic, another product offered by MVT is incurring performance issues: the
    automated trading system. The automated trader receives orders from the order
    book and applies various trading strategies in real time to automatically place
    orders on behalf of the customers. As the order book is processing an order of
    magnitude of more trade orders, the automated trading system is unable to keep
    up, and, therefore, cannot efficiently apply its strategies. Several big customers
    recently lost a lot of money due to bad decisions made by the algorithm and the
    high latency of execution. The engineering team needs to solve this performance
    issue. Alice, your technical lead, has tasked you with finding a solution and
    preventing the company from losing newly-acquired customers.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了我们的辛勤工作，MVT正在蓬勃发展。销售部门正在签订合同，仿佛明天就是世界末日，销售铃声从日出响到日落。订单簿能够处理更多的订单，由于流量的增加，MVT提供的另一产品出现了性能问题：自动化交易系统。自动化交易员从订单簿接收订单，并实时应用各种交易策略，代表客户自动下单。由于订单簿正在处理数量级的交易订单，自动化交易系统无法跟上，因此无法有效地应用其策略。最近，一些大客户由于算法决策失误和执行延迟高而损失了大量资金。工程团队需要解决这个性能问题。你的技术负责人Alice要求你找到解决方案，防止公司失去新获得的客户。
- en: In the previous chapter, we studied and took advantage of concurrency. We learned
    how to design code to leverage the power of multicore hardware. The automated
    trader is already optimized to run concurrent code and utilize all the CPU resources
    on the machine. The truth is, there is only so much one machine can handle, even
    with several cores. To scale the system and keep up with the traffic coming from
    the order book, we will have to start implementing a distributed system.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了并发，并利用了并发。我们学习了如何设计代码以利用多核硬件的强大功能。自动化交易器已经优化以运行并发代码并利用机器上的所有CPU资源。事实是，即使是几个核心，一台机器也只能处理这么多。为了扩展系统并跟上订单簿带来的流量，我们不得不开始实施分布式系统。
- en: A glimpse into distributed architectures
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式架构的概览
- en: Distributed computing is a rich topic, and we cannot pretend to address it entirely
    in a single chapter. This short section gives a brief and incomplete description
    of distributed computing. We will try to give you an overview of the paradigm
    and point to some of the main benefits and challenges of distributed systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算是一个丰富的主题，我们无法假装在一章中完全解决它。本节提供了一个简短且不完整的分布式计算描述。我们将尝试为您概述这个范式，并指出分布式系统的一些主要优势和挑战。
- en: The idea behind distributed computing is to design a system involving several
    components, which runs on different machines and communicates with each other
    (for example, over a network) to achieve a task or provide a service. A distributed
    system can involve components of different natures, each component providing a
    specific service and participating in the realization of the task. For example,
    a web server can be deployed to receive HTTP requests. To service a request, the
    web server may communicate over the network to query an authentication service
    to validate credentials and a database server in order to store and retrieve data
    and complete the request. Together, the web server, the authentication service,
    and the database form a distributed system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算背后的理念是设计一个涉及多个组件的系统，这些组件运行在不同的机器上，并通过彼此（例如，通过网络）进行通信以完成一个任务或提供一项服务。分布式系统可以涉及不同性质的组件，每个组件提供特定的服务并参与任务的实现。例如，可以将Web服务器部署来接收HTTP请求。为了处理请求，Web服务器可能通过网络查询认证服务以验证凭证，并查询数据库服务器以存储和检索数据并完成请求。Web服务器、认证服务和数据库共同构成了一个分布式系统。
- en: A distributed system can also involve several instances of the same component.
    These instances form a cluster of nodes, and they can be used to divide the work
    among them. This topology allows a system to scale out and support a higher load
    by adding more instances to the cluster. As an example, if a web server is able
    to handle 20,000 requests per second, it may be possible to run a cluster of three
    identical servers to handle 60,000 requests per second (assuming that your architecture
    allows your application to scale linearly). Distributed clusters also help achieve
    high availability. If one of the nodes crashes, the others are still up and able
    to fulfill requests while the crashed instance is restarted or recovers. As there
    is no single-point of failure, there is no interruption of service.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统也可能涉及相同组件的多个实例。这些实例形成一个节点集群，并且可以用来在他们之间分配工作。这种拓扑结构允许系统通过向集群添加更多实例来扩展并支持更高的负载。例如，如果一个Web服务器能够每秒处理20,000个请求，那么可能可以通过运行三个相同的Web服务器集群来处理每秒60,000个请求（假设您的架构允许您的应用程序线性扩展）。分布式集群还有助于实现高可用性。如果一个节点崩溃，其他节点仍然可以运行并能够满足请求，同时崩溃的实例正在重新启动或恢复。因为没有单点故障，所以没有服务中断。
- en: For all their benefits, distributed systems come with their drawbacks and challenges.
    The communication between components is subject to failure and network disruptions.
    The application needs to implement a retry mechanism and error handling, and then
    deal with lost messages. Another challenge is managing shared state. For example,
    if all the nodes use a single database server to save and retrieve information,
    the database has to implement some form of a locking mechanism to ensure that
    concurrent modifications do not collide. It is also possible that once the cluster
    node count grows sufficiently large, the database will not be able to serve them
    all efficiently and will become a bottleneck.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分布式系统带来了诸多好处，但也伴随着其自身的缺点和挑战。组件之间的通信可能会出现故障和网络中断。应用程序需要实现重试机制和错误处理，并处理丢失的消息。另一个挑战是管理共享状态。例如，如果所有节点都使用单个数据库服务器来保存和检索信息，数据库必须实现某种形式的锁定机制来确保并发修改不会冲突。一旦集群节点数量足够大，数据库可能无法高效地为他们提供服务，并可能成为瓶颈。
- en: Now that you have been briefly introduced to distributed systems, we will go
    back to MVT. The team has decided to turn the automated trader into a distributed
    application to be able to scale the platform. You have been tasked with the design
    of the system. Time to go to the whiteboard.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经简要了解了分布式系统，我们将回到MVT。团队决定将自动化交易转变为分布式应用程序，以便能够扩展平台。您被分配了设计系统的任务。是时候去白板前了。
- en: The first attempt at a distributed automated trader
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首次尝试分布式自动化交易
- en: 'Your first strategy is simple. You plan to deploy several instances of the
    automated trader to form a cluster of nodes. These nodes can share the work and
    handle each part of the incoming orders. A load balancer in front of the cluster
    can distribute the load evenly among the nodes. This new architecture helps scale
    out the automated trader. However, you are facing a common problem with distributed
    systems: the nodes have to share a common state to operate. To understand this
    requirement, we explore one of the features of the automated trader. To be able
    to use MVT''s automated trading system, customers have to open an account with
    MVT and provision it with enough money to cover their trades. This is used as
    a safety net by MVT to execute orders on behalf of its clients without running
    the risk of a customer being unable to honor their transactions. To ensure that
    the automated strategies do not overspend, the automated trader keeps track of
    the current balance of each customer and checks the balance of a customer before
    placing an automated order on their behalf.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个策略很简单。您计划部署几个自动化交易员的实例以形成一个节点集群。这些节点可以共享工作并处理进入订单的每个部分。集群前面的负载均衡器可以在节点之间均匀分配负载。这种新的架构有助于扩展自动化交易。然而，您面临着分布式系统的一个常见问题：节点必须共享一个公共状态才能运行。为了理解这个要求，我们探索自动化交易的一个功能。为了能够使用MVT的自动化交易系统，客户必须向MVT开设账户并为其提供足够的资金以覆盖他们的交易。这是MVT作为安全网执行其客户订单的一种方式，以免客户无法履行其交易的风险。为了确保自动化策略不会过度消费，自动化交易员跟踪每个客户的当前余额并在代表他们下单之前检查客户的余额。
- en: Your plan consists of deploying several instances of the automated trading system.
    Each instance receives a portion of the orders processed by the order book, runs
    a strategy and places matching order on behalf of a customer. Now that the system
    consists of several identical instances running in parallel, each instance can
    place orders on behalf of the same customer. To be able to perform the balance
    validation, they all need to be aware of the current balance of all customers.
    Customer balances become a shared state that has to be synchronized in the cluster.
    To solve this problem, you envision a balance monitor server deployed as an independent
    component and holding the state of each customer's balance. When a trade order
    is received by a node of the automated trading cluster, the node interrogates
    the balance monitor server to verify that a customer's account has enough funds
    to place an automated trade. Similarly, when a trade is executed, a node instructs the
    balance monitor server to update the balance of the customer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您的计划包括部署几个自动化交易系统的实例。每个实例接收订单簿处理的部分订单，运行策略并代表客户下单。现在系统由几个并行运行的相同实例组成，每个实例都可以代表同一客户下单。为了能够执行余额验证，它们都需要知道所有客户的当前余额。客户余额成为一个需要在集群中同步的共享状态。为了解决这个问题，您设想部署一个作为独立组件的余额监控服务器，并持有每个客户余额的状态。当一个交易订单被自动化交易集群的节点接收时，该节点会查询余额监控服务器以验证客户的账户是否有足够的资金进行自动化交易。同样，当交易执行时，一个节点会指示余额监控服务器更新客户的余额。
- en: '![The first attempt at a distributed automated trader](img/image_07_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![分布式自动化交易员的第一尝试](img/image_07_001.jpg)'
- en: The preceding diagram describes various interactions between the components
    of your architecture. **Automated Trader 1** receives an incoming trade and queries
    the balance monitor server to check whether the client has enough funds to perform
    a trade. The balance monitor server either authorizes or rejects the order. At
    the same time, **Automated Trader 3** sends an order that was previously approved
    by the balance monitor server and updates the client's balance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图描述了您架构组件之间的各种交互。**自动化交易员1**接收一个进入的交易并查询余额监控服务器以检查客户端是否有足够的资金进行交易。余额监控服务器要么授权要么拒绝订单。同时，**自动化交易员3**发送一个之前由余额监控服务器批准的订单并更新客户端的余额。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You probably spotted a flaw in this design. It is possible to run into a race
    condition where two different instances of the automated trader may validate the
    balance of the same customer, receive an authorization from the **Balance Monitor
    Server**, place both trades in parallel and go over the limit of the client's
    account. This is comparable to a race condition that you can encounter with a
    concurrent system running on a single machine. In practice, the risk is low and
    is accepted by companies that are similar to MVT. The limit used to cut-off a
    client is usually set lower than the actual balance to account for this risk.
    Designing a platform to handle this case would increase the latency of the system
    because we would have to introduce more drastic synchronization across the nodes.
    This is a good example of business and technical domains working together to optimize
    the solution.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经发现了这个设计的缺陷。可能会遇到一个竞态条件，其中两个不同的自动化交易实例可能会验证同一客户的余额，从**余额监控服务器**接收授权，并行执行两笔交易并超过客户的账户限额。这类似于你在单台机器上运行的并发系统可能遇到的竞态条件。在实践中，这种风险很低，类似于MVT的公司可以接受这种风险。用于切断客户的限额通常设置得低于实际余额，以考虑到这种风险。设计一个处理这种情况的平台会增加系统的延迟，因为我们不得不在节点之间引入更多的剧烈同步。这是一个很好的例子，说明了商业和技术领域如何合作以优化解决方案。
- en: 'At the end of this design session, you take a short walk to clear your mind
    while drinking a bottle of carbonated water. As you return to the whiteboard,
    the crude reality hits you. Like a flat bottle of carbonated water, your idea
    has fizzled out. You realize that all these arrows linking rectangles are in reality
    messages that are traveling over the network. Currently, while a single automated
    trader relies on its internal state to execute strategies and place orders, this
    new design requires the automated trader to query an external system over the
    network and wait for the answer. This query happens on the critical path. This
    is another common issue with distributed systems: components with focused roles
    need to communicate with each other to accomplish their tasks. This communication
    comes at a cost. It involves serialization, I/O operations, and transfer over
    a network. You share your reflections with Alice, who confirms that this is a
    problem. The automated trader has to keep the internal latency as low as possible
    for its decisions to be relevant. After a short discussion, you agree that it
    would endanger performance for the automated trader to perform a remote call on
    the critical path. You are now left with the task of implementing a distributed
    system with components sharing a common state without communicating with each
    other on the critical path. This is where we can start talking about CRDTs.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计会议结束时，你走一小段路来清醒一下头脑，同时喝一瓶碳酸水。当你回到白板前，残酷的现实让你感到震惊。就像一个扁平的碳酸水瓶一样，你的想法已经消散了。你意识到，所有这些连接矩形的箭头实际上是在网络上传输的消息。目前，虽然单个自动化交易者依赖于其内部状态来执行策略和下订单，但这个新设计要求自动化交易者通过网络查询外部系统并等待答案。这个查询发生在关键路径上。这是分布式系统中的另一个常见问题：具有特定角色的组件需要相互通信以完成任务。这种通信是有成本的。它涉及到序列化、I/O操作和网络传输。你与爱丽丝分享你的反思，她确认这是一个问题。自动化交易者必须尽可能保持内部延迟低，以便其决策具有相关性。经过简短讨论后，你们同意在关键路径上对自动化交易者执行远程调用将危害性能。你现在面临着一个任务，即实现一个由共享公共状态的组件组成但不通过关键路径相互通信的分布式系统。这就是我们可以开始讨论CRDTs的地方。
- en: Introducing CRDTs
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入CRDTs
- en: '**CRDT** stands for **Conflict-free Replicated Data Types**. CRDTs were formally
    defined by Marc Shapiro and Nuno Preguiça in their paper, *Designing a commutative
    replicated data type* (refer to [https://hal.inria.fr/inria-00177693/document](https://hal.inria.fr/inria-00177693/document)).
    A CRDT is a data structure that is specifically designed to ensure eventual consistency
    across multiple components without the need for synchronization. Eventual consistency
    is a well-known concept in distributed system, which is not exclusive to CRDTs.
    This model guarantees that eventually, if a piece of data is no longer modified,
    all nodes in a cluster will end up with the same value for this piece of data.
    Nodes send each other update notifications to keep their state synchronized. The
    difference with strong consistency is that at a given time, some nodes may see
    a slightly outdated state until they receive the update notice:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing CRDTs](img/image_07_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows an example of eventual consistency. All the nodes
    of the cluster hold the same piece of data (A = 0). Node 1 receives an update
    to set the value of A to 1\. After updating its internal state, it broadcasts
    the update to the rest of the cluster. The messages reach their targets at different
    instants, which means that until we reach step 4, A has a different value depending
    on the node. If a client queries node 4 for the value of A at step 3, they receive
    an older value as the change has not yet been reflected in node 4.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'A problem that may arise with eventual consistency is the resolution of conflicts.
    Imagine a simple example where nodes in a cluster share the state of an array
    of integers. The following table describes a sequence of events involving updating
    the state of this array:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instant** | **Event** | **State change** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| **T0** | Initialization of the cluster | Nodes 1 and 2 hold the same value
    for the array of integers: `[1,2,3]` |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| **T1** | Node 1 receives a request to update the value at index 1 from 2
    to 4 | Node 1 updates its internal state to `[1,4,3]` and sends an update message
    to node 2 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| **T2** | Node 2 receives a request to update the value at index 1 from 2
    to 5 | Node 2 updates its internal state to `[1,5,3]` and sends an update message
    to node 1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| **T3** | Node 1 receives the update from node 2 | Node 1 needs to decide
    whether it should ignore or take into account the update message |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: Our cluster now needs to resolve the conflict. Should node 1 update its state
    when receiving the update from node 2? If node 2 does the same, we end up with
    two nodes holding a different state. What about the other nodes? Some may receive
    the broadcast from node 2 before the one from node 1 and vice versa.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Various strategies exist to deal with this problem. Some protocols use timestamps
    or vector clocks to determine which update was performed later in time and should
    take precedence. Others simply assume that the last writer wins. This is not a
    simple problem and CRDTs are designed to completely avoid conflicts altogether.
    Actually, CRDTs are defined to make conflicts mathematically impossible. To be
    defined as a CRDT, a data structure has to support only commutative updates. That
    is, regardless of the ordering in which the update operations are applied, the
    end state must always be the same. This is the secret of eventual consistency
    without merge conflict. When a system uses CRDTs, all the nodes can send each
    other update messages without a need for strict synchronization. The messages
    can be received in any order, and all the local states will converge to the same
    value eventually.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种策略来处理这个问题。一些协议使用时间戳或向量时钟来确定哪个更新是在时间上更晚执行的，应该具有优先权。其他协议简单地假设最后写入者获胜。这不是一个简单的问题，CRDTs旨在完全避免冲突。实际上，CRDTs被定义为使冲突在数学上成为不可能。要被定义为CRDT，数据结构必须仅支持交换性更新。也就是说，无论更新操作应用的顺序如何，最终状态必须始终相同。这是无合并冲突的最终一致性的秘密。当系统使用CRDTs时，所有节点都可以相互发送更新消息，而不需要严格的同步。消息可以以任何顺序接收，所有局部状态最终都会收敛到相同的值。
- en: '![Introducing CRDTs](img/image_07_003.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![介绍CRDTs](img/image_07_003.jpg)'
- en: In the preceding diagram, we see that node 3 and node 1 receive two different
    changes. They send this update information to all the other nodes. Note that we
    are not concerned with the order in which the updates are received by the other
    nodes. As the updates are commutative, their order has no impact on the final
    state that will be computed by each node. They are guaranteed to hold the same
    piece of data once all of them have received all the update broadcasts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们看到节点3和节点1接收到了两个不同的更改。它们将这个更新信息发送给所有其他节点。请注意，我们并不关心其他节点接收更新的顺序。由于更新是交换的，它们的顺序对每个节点最终计算出的最终状态没有影响。一旦所有节点都接收到了所有的更新广播，它们将保证持有相同的数据。
- en: 'There exist two types of CRDT:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种类型的CRDT：
- en: Operation-based
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于操作的
- en: State-based
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于状态的
- en: 'They are equivalent in that it is always possible to define a state-based CRDT
    for each operation-based CRDT and vice-versa. However, their implementations differ
    and provide different guarantees in terms of error-recovery and performance. We
    define each type and consider its characteristics. As an example, we implement
    each version of the simplest CRDT: an increase-only counter.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在这一点上是等效的，因为对于每个基于操作的CRDT，总是可以定义一个基于状态的CRDT，反之亦然。然而，它们的实现不同，在错误恢复和性能方面提供了不同的保证。我们定义每种类型并考虑其特性。作为一个例子，我们实现了最简单的CRDT版本：仅增加计数器。
- en: The state-based increase-only counter
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于状态的仅增加计数器
- en: 'With this model, when a CRDT receives an operation to perform from a client,
    it updates its state accordingly and sends an update message to all the other
    CRDTs in the cluster. This update message contains the full state of the CRDT.
    When the other CRDTs receive this message, they perform a merge of their state
    with the received new state. This merge operation has to guarantee that the end
    state will always be the same. It has to be commutative, associative, and idempotent.
    Let''s look at a possible implementation of this data type:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个模型，当CRDT从客户端接收到要执行的操作时，它会相应地更新其状态，并向集群中的所有其他CRDT发送更新消息。这个更新消息包含CRDT的完整状态。当其他CRDT接收到这个消息时，它们将它们的状态与接收到的新的状态合并。这个合并操作必须保证最终状态始终相同。它必须是交换的、结合的和幂等的。让我们看看这个数据类型的一个可能的实现：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `update` method can be used by clients to increase the value of the counter.
    This returns a new state-based counter containing an updated count, and it generates
    a `CounterState` object that can be sent to all the other CRDTs in the cluster.
    The `merge` is used to handle these `CounterState` messages and merge the new
    state of the other counters with the local state. A counter has a unique ID in
    the cluster. The internal state is composed of the local state (that is, `count`)
    and the states of all the other counters in the cluster. We keep these counters
    in a map that we update in the `merge` method when receiving state information
    from a different counter. Merging is a simple operation. We compare the incoming
    value with the one that we have in the map and keep the greatest one. This is
    to ensure that if we receive two update messages in the wrong order, we do not
    override the latest state (that is, the greatest number) with an older update
    message that was delayed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以使用 `update` 方法来增加计数器的值。这会返回一个新的基于状态的计数器，包含更新的计数，并生成一个可以发送到集群中所有其他 CRDT
    的 `CounterState` 对象。`merge` 用于处理这些 `CounterState` 消息，并将其他计数器的新状态与本地状态合并。计数器在集群中有一个唯一的
    ID。内部状态由本地状态（即 `count`）和集群中所有其他计数器的状态组成。我们保持这些计数器在一个映射中，并在从不同的计数器接收状态信息时在 `merge`
    方法中更新这个映射。合并是一个简单的操作。我们比较传入的值与我们映射中的值，并保留最大的一个。这是为了确保如果我们收到两个更新消息的顺序错误，我们不会用延迟的较旧更新消息覆盖最新的状态（即最大的数字）。
- en: The operation-based increase-only counter
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于操作的仅增加计数器
- en: 'Operation-based CRDTs are similar to state-based CRDTs with the difference
    that update messages only contain a description of the operation that was just
    performed. These CRDTs do not send their full-state in an update message, but
    they are merely a copy of the operation that they just performed to update their
    own state. This ensures that all the other CRDTs in the cluster perform the same
    operation and maintain their state in sync. The updates can be received in a different
    order by each node of the cluster. To guarantee that the end state is the same
    for all the nodes, the updates have to be commutative. You can see an example
    of this data structure, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于操作的 CRDT 与基于状态的 CRDT 类似，不同之处在于更新消息只包含刚刚执行的操作的描述。这些 CRDT 不在更新消息中发送它们的完整状态，但它们仅仅是它们刚刚执行以更新自己状态的操作的副本。这确保了集群中的所有其他
    CRDT 都执行相同的操作并保持它们的状态同步。更新消息可以按不同的顺序被集群的每个节点接收。为了保证所有节点的最终状态相同，更新必须是交换的。您可以在以下示例中看到这种数据结构：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This implementation is shorter than the state-based example. The `update` method
    still returns an updated instance of the counter, and the `CounterUpdate` object
    that was applied. For an operation-based counter, it is enough to broadcast the
    operation that was applied. This update is received by the `merge` method of the
    other instances to apply the same operation to their own internal state. Note
    that `update` and `merge` are equivalent, `merge` is even implemented in terms
    of `update`. In this model, there is no need for a unique ID per counter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现比基于状态的示例更短。`update` 方法仍然返回计数器的更新实例以及应用过的 `CounterUpdate` 对象。对于基于操作的计数器，只需广播应用过的操作即可。这个更新通过其他实例的
    `merge` 方法接收，以将相同的操作应用于它们自己的内部状态。请注意，`update` 和 `merge` 是等效的，`merge` 实际上是基于 `update`
    实现的。在这个模型中，每个计数器不需要唯一的 ID。
- en: Operation-based CRDTs use potentially smaller messages because they only send
    each discrete operation as opposed to their full internal state. In our example,
    the state-based update contains two integers, as opposed to only one for the operation-based
    update. Smaller messages can help reduce bandwidth usage and improve the throughput
    of your system. However, they are sensitive to communication failures. If an update
    message is lost during the transmission and does not reach a node, this node will
    be out of sync with the rest of the cluster with no way of recovering. If you
    decide to use operation-based CRDTs, you have to be able to trust your communication
    protocol and be confident that all update messages reach their destination and
    are properly processed. State-based CRDTs do not suffer from this issue because
    they always send their entire state in an update message. If a message is lost
    and does not reach a node, this node will only be out of sync until it receives
    the next update message. It is possible to make this model even more robust by
    implementing a periodic broadcast of the node's state, even when no updates are
    performed. This would force all nodes to regularly send their current state and
    ensure that the cluster is always eventually consistent.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于操作的CRDTs使用可能更小的消息，因为它们只发送每个离散操作，而不是它们的完整内部状态。在我们的例子中，基于状态的更新包含两个整数，而基于操作的更新只有一个。较小的消息可以帮助减少带宽使用并提高系统的吞吐量。然而，它们对通信故障敏感。如果在传输过程中更新消息丢失并且没有到达节点，这个节点将无法与集群的其他部分保持同步，并且无法恢复。如果您决定使用基于操作的CRDTs，您必须能够信任您的通信协议，并确信所有更新消息都到达了目的地并且得到了适当的处理。基于状态的CRDTs不受此问题的影响，因为它们总是在更新消息中发送整个状态。如果消息丢失并且没有到达节点，该节点将仅在收到下一个更新消息之前与集群不同步。通过实现节点的状态周期性广播，即使没有执行更新，也可以使此模型更加健壮。这将迫使所有节点定期发送其当前状态，并确保集群始终最终一致。
- en: CRDTs and automated traders
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CRDTs和自动化交易员
- en: Based on the requirements of our system, it seems that CRDTs are a good fit
    for our implementation. Each node can keep the current state of each customer's
    balance in memory as a counter, update it when placing orders, and broadcast update
    messages to the rest of the system. This broadcast can be done outside the critical
    path, and we do not have to worry about handling conflicts, as this is what CRDTs
    are designed for. Eventually, all nodes will have in memory the same value for
    each balance, and they will be able to locally check for trade authorization.
    The balance monitor server can be removed entirely.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们系统的需求，CRDTs似乎非常适合我们的实现。每个节点可以将每个客户的当前余额状态保存在内存中作为一个计数器，在下单时更新它，并将更新消息广播到系统的其余部分。这种广播可以在关键路径之外进行，我们不必担心处理冲突，因为这是CRDTs的设计目的。最终，所有节点都将内存中每个余额具有相同的值，并且它们将能够本地检查交易授权。余额监控服务器可以完全移除。
- en: 'To implement the state of the balance as a CRDT, we need a more sophisticated
    counter than the one we previously explored. The balance cannot be represented
    as an increase-only counter because, occasionally, orders are canceled and the
    system must credit the customer''s account. The counter has to be able to handle
    both increment and decrement operations. Luckily, such a counter exists. Let''s
    look at a simple implementation of a state-based counter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要将余额状态实现为CRDT，我们需要一个比我们之前探索的更复杂的计数器。余额不能表示为只增加的计数器，因为偶尔订单会被取消，系统必须向客户的账户进行信用。计数器必须能够处理增加和减少操作。幸运的是，这样的计数器存在。让我们看看基于状态的计数器的一个简单实现：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The PN counter leverages our previous implementation of an increase-only counter
    to provide the decrement capability. To be able to represent a counter as a state-based
    CRDT, we need to keep track of the state of both increment and decrement operations.
    This is necessary to guarantee that we do not lose information if our update messages
    are received in the wrong order by other nodes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PN计数器利用我们之前实现的只增加的计数器来提供减少功能。为了能够将计数器表示为基于状态的CRDT，我们需要跟踪增加和减少操作的状态。这是必要的，以确保如果我们的更新消息以错误的顺序被其他节点接收，我们不会丢失信息。
- en: Tip
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Remember that the increase-only counter guarantees conflict resolution by assuming
    that the highest value of the counter is necessarily the most up-to-date. This
    invariant does not hold true for the PN counter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，只增加的计数器通过假设计数器的最高值必然是最新的来保证冲突解决。这个不变量对于PN计数器不成立。
- en: 'This implementation shows you another interesting property of CRDTs: simple
    and basic structures can be composed to create more complex and feature-rich CRDTs.
    Should we proceed to demonstrate the implementation of an operation-based counter?
    As it turns out and we are sure you spotted this earlier, our previous increase-only
    counter already supports decrement operations. Applying a positive or a negative
    delta is handled by the operation-based counter.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现展示了CRDTs的另一个有趣特性：简单和基本的结构可以组合起来创建更复杂和功能丰富的CRDTs。我们是否应该继续演示基于操作的计数器的实现？结果证明，并且我们确信你之前已经注意到了，我们之前的只增计数器已经支持减操作。应用正或负的delta由基于操作的计数器处理。
- en: When the balance is not enough
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当余额不足时
- en: You have finished the implementation of the proof-of-concept and call Alice
    to get some feedback. She spends a few minutes studying your new design and your
    code. "Looks good to me. Do not forget to synchronize the account blacklist as
    well." What is she talking about? "Checking the account balance is only one of
    the criteria to allow or block an automated trade. Other attributes of the client
    need to be taken into consideration. Today, the automated trader runs a trust
    algorithm in the background, and it calculates a score for each customer. If the
    score falls below a certain threshold, the account is blacklisted until the end
    of the trading day, and all automated orders are denied. I like your design, but
    you need to incorporate this blacklist into the new system." Faced with this new
    challenge, you think that the best solution would be to implement the blacklist
    as a CRDT as well, provided that it fits your current design.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经完成了概念验证的实现，并打电话给Alice获取一些反馈。她花了几分钟研究你的新设计和你的代码。“看起来不错。别忘了同步账户黑名单。”她在说什么？“检查账户余额只是允许或阻止自动化交易的一个标准之一。客户的其他属性也需要考虑。今天，自动化交易员在后台运行信任算法，并为每位客户计算一个分数。如果分数低于某个阈值，账户将被列入黑名单直到交易日结束，并且所有自动化订单都将被拒绝。我喜欢你的设计，但你需要将这个黑名单纳入到新系统中。”面对这个新的挑战，你认为最好的解决方案是将黑名单也实现为CRDT，前提是它适合你的当前设计。
- en: A new CRDT - the grow-only set
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个新的CRDT - 只增集合
- en: 'One CRDT is designed to handle our new use case. The grow-only set data type
    implements a set that only supports the addition of new elements without duplicates.
    We can implement the blacklist as a grow-only set. Each node can run its own trust
    algorithm and can decide whether a client should be blacklisted and denied automated
    trading for the rest of the day. At the end of the day, the system can clear the
    set. We display a possible implementation of a state-based grow-only set, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CRDT被设计来处理我们的新用例。只增集合数据类型实现了一个只支持添加新元素而不重复的集合。我们可以将黑名单实现为只增集合。每个节点都可以运行自己的信任算法，并决定是否应该将客户端列入黑名单并拒绝当天剩余的自动化交易。在一天结束时，系统可以清除集合。以下是我们展示的一个基于状态的只增集合的可能实现：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Our implementation supports adding an element by calling the `update` method.
    It returns a new instance of `StateBasedGSet` with an updated set, as well as
    a `GSetState` instance to be broadcast to the other nodes. This update contains
    the entire state of the counter, that is, the internal set. An operation-based
    implementation is trivial and left as an exercise for the reader (a possible solution
    is provided in the code repository). Similar to the increment-decrement counter
    explored earlier, it is possible to create a set that supports both adding and
    removing an element. There is one caveat though: as adding and removing an element
    are not commutative operations, one must take precedence on the other. In practice,
    a 2P-set can be created to support adding and removing items, but once removed,
    an element cannot be added again. The remove operation takes precedence and guarantees
    that the operations are commutative and can be handled without conflicts. A possible
    implementation is to combine two grow-only sets, one for adding elements, and
    the other to remove them. Again, we see the power of simple CRDTs that can be
    combined to create more powerful data types.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Free trading strategy performance improvements
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You stare at your agile burn down chart and discover that you completed all
    your story points before the sprint ends tomorrow. You are excited to have delivered
    this week's features early, but you are left wondering whether or not you will
    have yet another discussion with the scrum master about estimation. Instead of
    spending mental energy on estimating, you instead return your attention to an
    issue that Dave raised. At a recent lunch together, Dave talked about how the
    company's trading strategies lose money when trading decisions are made based
    on stale information. Even several milliseconds can make the difference between
    extremely profitable trades and losses. His words piqued your interest to see
    if you can improve the performance of MVT's trading strategies.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'MVT''s trading strategies are downstream consumers of the order book. The trading
    strategies listen for changes in the best bid and offer (BBO) in order to determine
    when to submit buy or sell orders. At lunch, Dave explained that tracking the
    BBO has historically proven to give the most signals for MVT''s trading strategies.
    The best bid refers to the bid with the highest price, and the best offer refers
    to the offer with the lowest price. When either side of the BBO changes due to
    a cancellation, execution, or new limit order, then a BBO update event is transmitted
    to downstream trading strategies. The model representing this event is `BboUpdated`,
    and it looks like the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: MVT deploys each trading strategy within its own JVM to ensure that failures
    do not affect other running strategies. When deployed, each trading strategy maintains
    BBO subscriptions for the set of tickers it trades.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Having spent a significant amount of time working on the order book, you hope
    to find opportunities to apply your functional programming knowledge to yield
    better performance. During your lunch with Dave, you discovered that "better performance"
    has a slightly different meaning for trading strategy development than it does
    for other systems. You asked Dave, "If you could choose between an improvement
    in latency or throughput, which would you choose?" Dave sarcastically replied,
    "Why do I have to choose? I want both!" Afterwards, he went on to say, "Latency!
    Almost every time a trading strategy makes a decision using old BBO updates, we
    lose money. In fact, if we could, I would rather throw away old BBO updates. We
    only trade high-volume tickers, so we are pretty much guaranteed to see another
    BBO update immediately." As you start looking into the code base, you wonder whether
    you can utilize Dave's thinking to improve trading strategy performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking the trading strategy
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recalling the lessons that you learned when working on the order book, your
    first step is to benchmark. You select one of MVT's production trading strategies
    and adapt the benchmark that you wrote to exercise the order book, `FinalLatencyBenchmark`,
    to send the `BboUpdated` events to the trading strategy. Originally, the benchmark
    focused on displaying the 99^(th) percentile latency and higher. As you know that
    latency is the most important factor in your performance investigation, you modify
    the benchmark to also emit the median and 75^(th) percentile latencies. This will
    give you a more holistic view into the latency of trading strategy performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the production metrics system, you see a time series trading volume
    chart for the system that you want to benchmark. It shows that it is a low-volume
    day, only about 4,000 BBO updated events per second. You dig through historical
    metrics to find the highest volume day in the last few weeks. The market has been
    volatile again, so a recent high-volume day is likely a good proxy for a high
    throughput rate to benchmark. About two weeks ago, there was a trading day with
    a sustained peak of 12,000 BBO updated events per second. You plan to begin benchmarking
    at the lower end of the spectrum with 4,000 events per second, ramping up to 12,000
    events per second to see how performance changes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'The testing methodology is to measure latency for an equivalent number of events
    across throughput rates while ensuring a thorough test at each throughput level.
    To accomplish this goal, you multiply the higher throughput, 12,000 events per
    second, by 30 trials for a sum total of 360,000 events. At 4,000 events per second,
    running the benchmark for 90 trials produces the equivalent of 360,000 events.
    Running the benchmarks in a test environment replicating production gives the
    results displayed in the following table. The table abbreviates events per second
    as EPS:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '| **Percentile** | **4,000 EPS** | **12,000 EPS** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) | 0.0 ms | 1,063.0 ms |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) | 0.0 ms | 1,527.0 ms |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) | 10.0 ms | 2,063.0 ms |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) | 22.0 ms | 2,079.0 ms |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) | 36.0 ms | 2,079.0 ms |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: 'These results illustrate a startling contrast in performance. At 4,000 events
    per second, the trading strategy appears to perform well. 99% of events are responded
    to within 10 ms, and we observe that up to the 75^(th) percentile, the strategy
    is responding with miniscule delay. This suggests that on low-volume days, this
    trading strategy is able to decide on information quickly, which should bode well
    for profitability. Unfortunately, at 12,000 events per second, the performance
    is unacceptable. Having not yet looked at the code, you wonder whether you can
    spot any sudden changes in performance by sweeping several more throughputs. You
    try a binary search between 4,000 and 12,000 events per second and get the following
    results:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '| **Percentile** | **9,000 EPS** | **10,000 EPS** | **11,000 EPS** | **11,500
    EPS** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) | 0.0 ms | 4.0 ms | 41.0 ms | 487.0ms |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) | 5.0 ms | 9.0 ms | 66.0 ms | 715.0 ms |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) | 32.0 ms | 47.0 ms | 126.0 ms | 871.0 ms |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) | 58.0 ms | 58.0 ms | 135.0 ms | 895.0 ms |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) | 67.0ms | 62.0 ms | 138.0 ms | 895.0 ms |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: You chose 9,000 events per second as a starting point because it divided evenly
    into the total event count, 360,000\. At this level of throughput, the strategy's
    profile is qualitatively closer to the 4,000 events per second profile. As results
    looked reasonable at this level, you increased the throughput approximately halfway
    between 9,000 and 12,000 events per second to the next level that divides evenly
    into 360,000\. At 10,000 events per second, we once again observe a profile that
    remains similar to the 4,000 events per second profile. There is a discernible
    increase in the median and 75^(th) percentile latencies, suggesting the strategy's
    performance is beginning to degrade. Next, you increase the throughput to the
    midpoint, 11,000 events per second. As you cannot run 32.72 trials, you instead
    round up to 33 trials for a total of 363,000 events. These results are qualitatively
    worse than the 4,000 events per second results by approximately an order of magnitude
    at each measured percentile. Admittedly, these are weak performance results, but
    does this profile closely resemble the profile at 12,000 events per second?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'You are now a bit alarmed because 11,000 events per second is approximately
    90% of the throughput at 12,000 events per second. Yet, the results do not display
    close to 90% similarity. If the trading strategy decreased linearly you would
    expect to see latencies approximating 90% of the latencies that were observed
    at 12,000 events per second. Unsatisfied with this performance profile, you try
    one more throughput, 11,500 events per second. At this throughput level, you run
    the benchmark for 31 trials, totaling 356,500 events. Increasing the throughput
    by approximately 5% resulted in an observed median latency that is roughly 11
    times greater and an observed 99^(th) percentile latency that is nearly six times
    greater. These results make it clear that the strategy''s runtime performance
    degrades exponentially. To better reason about the results, you quickly throw
    together the following bar graph:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Benchmarking the trading strategy](img/image_07_004.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: This bar graph visualizes the exponential decay in performance. Interestingly,
    we observe that all measured latency percentiles follow consistent patterns of
    decay, further substantiating the hypothesis that the strategy has exhausted its
    capacity to process requests. Before jumping into improving the trading strategy
    performance, you ponder, "How can I bound the exponential increases in latency?"
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of seeing consistent decay across all measured latency percentiles,
    imagine that the median and 75^(th) percentiles remained qualitatively constant
    across all configured throughput levels. Does this profile suggest the same types
    of performance impediment as the scenario that we are working through? Take a
    moment to consider what could cause such a distribution to arise.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The danger of unbounded queues
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Benchmarking revealed a universal truth about performance tuning: unbounded
    queues kill performance. Here, we use the term queue to broadly mean a waiting
    line, instead of specifically focusing on the queue data structure. For example,
    this benchmark queues events to be transmitted at specific points in time in a
    `List`. In a production environment, this queue exists at multiple levels. The
    sender of the `BboUpdated` events likely queues events at the application-level,
    and subsequently, the network protocol (for example, TCP) may employ its own sets
    of queues to manage transmission to the consumer. When events are processed at
    a rate slower than they are produced, the system becomes unstable because the
    backlog of work always increases. Given infinite memory and zero response time
    guarantees, it is possible for an application to continue processing an ever-growing
    queue of items. However, in practice, when a system cannot stabilize itself by
    increasing its consumption rate to match or exceed the production rate, the system
    eventually spirals out of control. A system''s hardware resources are finite,
    and as a consumer falls behind, it will require increasing amounts of memory to
    cope with the growing backlog. Taken to an extreme, increasing memory requirements
    causes more frequent garbage collections, which in turn, further slow down consumption.
    This is a cyclical problem that will eventually exhaust memory resources, causing
    a system to crash.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: By inspecting the trading system code, you will discover that there is a queue
    for message processing within the trading system. This application-level queue
    is a `LinkedBlockingQueue` that separates the network I/O thread from the application
    thread. In the benchmark, the thread driving the benchmark adds events directly
    to the queue, simulating the behavior of a production network thread receiving
    events from the outside world. It is a common practice to group together logical
    parts of an application into separate thread pools in order to gain efficiencies
    by parallelizing processing work.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we previously explored concurrency with `Future` and `Task`, we indirectly
    worked with queues. The `ExecutorService` that receives submissions from `Future`
    and `Task` manages its workload by enqueuing tasks into a `BlockingQueue`. The
    factory methods that are provided in `Executors` do not allow the caller to provide
    a queue. If you explore the implementation of these factory methods you discover
    the kind and the size of `BlockingQueue` created.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Adding a buffer between the network layer and the application layer typically
    bodes well for performance. A queue can enable an application to tolerate momentary
    consumption slowdowns and bursts of messages from a producer. However, as we have
    seen in our benchmarking, buffers are a double-edged sword. The default constructor
    for `LinkedBlockingQueue` is effectively unbounded, setting a limit that is equal
    to the maximum supported integer value. By buffering messages indefinitely when
    the rate of production is consistently higher than the consumption rate, the trading
    system's performance degrades to an unusable state.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Applying back pressure
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What would happen if we instead chose to bound the queue that is receiving events
    to a smaller limit? When the production rate exceeds the consumption rate and
    the queue reaches capacity, one option is for the system to block until a spot
    is available in the queue. Blocking forces event production to halt, which describes
    a strategy for applying back pressure. In this context, pressure refers to the
    queue of events to be processed. The pressure manifests itself with increasing
    resource usage (for example, memory). By adopting a policy of blocking further
    production, the system is applying pressure back to the producer. Any queues that
    exist between the application-level consumer and the producer will also eventually
    reach capacity, forcing the producer to change its production rate in order to
    continue transmitting events.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this back pressure policy, all queues must be bounded to a size
    that avoids excessive resource usage, and production into queues must block when
    full. This is straightforward to implement with implementations of the JDK-provided
    `BlockingQueue` interface. For example, the following snippet displays this strategy
    with `LinkedBlockingQueue`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this snippet, we see construction of a `LinkedBlockingQueue` with a capacity
    limit of 1,000 messages. Based on knowledge of the production environment, you
    feel comfortable retaining up to 1,000 messages in-memory without exhausting memory
    resources. The second line in the snippet demonstrates a blocking operation to
    enqueue an element via `put`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: When applying back pressure, the choice in queue size is critical. To illustrate
    why, let's assume that we measured the maximum trading system processing latency
    to be 0.5 ms once a message is consumed from the event queue. At maximum, the
    total processing latency for an event is equal to 0.5 ms plus the time spent waiting
    to be processed. Consider the scenario where the queue has a size of 1,000 and
    999 events are queued when a new event arrives. In the worst case scenario, the
    new event waits 499.5 ms for the 999 other events that are already enqueued to
    be processed, plus 0.5 ms to be processed. Configuring a queue size of 1,000 yielded
    a maximum latency of 500 ms, showing that maximum latency is directly proportional
    to queue size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'A more disciplined approach to sizing queues involves considering environment
    resources and understanding the maximum latency that is tolerated by the business.
    From informal discussions with Dave, we learned that even several milliseconds
    can make or break a trading strategy''s profitability. Until we have a moment
    to check in with him, let''s assume that 10 ms is the maximum delay the strategy
    can tolerate without risking significant trading losses. Using this information,
    we can calculate a queue size that ensures that the 10 ms latency limit is respected.
    In the previous example, we performed the following worst-case scenario arithmetic:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can rearrange this formula to solve for queue size, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From this arithmetic, we substitute in known values to compute queue size,
    as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The arithmetic suggests that we bound the queue size for twenty elements to
    ensure that in the worst case scenario an event can be enqueued and processed
    within 10 ms. To explore back pressure deeper, we encourage you to read the following
    blog post by Martin Thompson at [http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html](http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html).
    Martin is an authority on high-performance software development, and this particular
    blog post was an invaluable learning source for back pressure.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Applying load-control policies
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back pressure is a strategy that works well when the message producer respects
    consumers that operate at different rates and does not penalize slow consumers.
    Particularly when dealing with third-party systems, there are situations where
    applying back pressure to force the producer to slow down will not be well received.
    In these scenarios, we need to consider additional strategies that improve the
    capacity of our systems without requiring algorithmic improvements to our business
    logic.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors have worked in the **real-time bidding** (**RTB**) space where a
    bidding system participates in auctions to bid on opportunities to display advertisements.
    In this industry, there is low tolerance for bidding systems that are unable to
    cope with the configured auction rate. Failure to respond to a high percentage
    of auctions with a bidding decision (either bid or no-bid) in a timely manner
    results in the bidding system being penalty-boxed. While in the penalty box, the
    bidding systems received a reduced auction rate. Bidding systems that remain in
    the penalty box for extended periods of time may be disallowed from participating
    in any auctions until their performance improves.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Let's revisit a scenario that we considered when describing back pressure to
    motivate our discussion. The precondition to apply back pressure is reaching the
    capacity of a queue. When a queue is filled, our first strategy blocks further
    additions until there is room available. Another option that we can investigate
    is to discard the event because the system is saturated. Discarding the event
    requires extra domain knowledge to understand the semantics of what it means to
    abruptly terminate processing. In the trading system domain, the trading strategy
    is only required to send back a response when a bid or an offer is made. The trading
    strategy is not required to send back a response when it does not decide to make
    either a bid or an offer. For the trading system domain, discarding an event simply
    means halting processing. In other domains, such as RTB, discarding an event implies
    halting processing and responding with a message indicating that there will not
    be a bid placed in this auction.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it is relevant that that each event is a snapshot of the best
    bid and offer. In contrast to the snapshot, imagine if instead of `BboUpdated`,
    the trading strategy received discrete events for changes in the best bid and
    offer. This is analogous to the state-based versus operation-based CRDT operations
    that we explored. Discarding an event would mean having partial information until
    a subsequent event is received. In this scenario, it is important to work with
    domain experts and product owners to determine if and for how long operating with
    partial information is acceptable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing load-control policies is another shift in thinking when working
    on high performance systems. Like the introduction of back pressure, this is another
    opportunity to reconsider assumptions that are made along the way to improve performance.
    Our lunchtime discussion with Dave provided great insight into a load-control
    policy that we can apply. Dave stated that he believes latent `BboUpdated` events
    cause more harm than good for trading strategy profitability. There are two assumptions
    we can challenge:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: All events must be processed
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An event being processed must complete processing
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can challenge these assumptions because Dave also indicated that MVT trades
    only high-volume tickers. If a BBO update is discarded, Dave is confident that
    a new BBO update is sure to follow quickly. Let's take a deeper look at how these
    policies can be defined.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Rejecting work
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rejecting work is not about rejecting sprint tasks, sorry! When we discuss work
    in this context, the term refers to processing effort. In the case of the benchmarked
    trading system, the work in hand is processing a new `BboUpdated` event. Although
    we have not dived into the code yet, we do know from previous benchmarking work
    that there is a queue used to accept the `BboUpdated` events from the network
    for application-level processing. This queue is the entry point into the application,
    and it represents the first application-level opportunity to reject the event
    due to capacity constraints.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: From our earlier domain investigation, we learned that to reject a request,
    it can simply be dropped on the floor without response. A trading strategy is
    only required to respond when it wishes to trade. This means that the policy of
    rejecting work can be implemented by dropping the request on the floor when the
    queue is at capacity.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'By inspecting the trading system source code, we see that the architecture
    is quite barebones. At start-up, a `LinkedBlockingQueue` is created to buffer
    the `BboUpdated` events, and a consumer thread is started to consume from the
    queue. The following snippet shows this logic:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As per our earlier work, we see that the work queue is sized with twenty elements
    to ensure a maximum processing latency of 10 ms. After the queue is instantiated,
    the consumer thread is created and started. The processing logic is omitted from
    this snippet, but we observe that the sole purpose of this thread is to consume
    events as they become available. The logic to add work to the queue is trivial.
    This snippet assumes a `MessageSentTimestamp` and a `BboUpdated` event are in
    lexical scope with the names, `ts` and `e`, respectively:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Our exploration of back pressure application indicated that `put` is a blocking
    call. As our intent is now to discard work,  `put` is no longer a viable strategy.
    Instead, we can make use of `offer`. As per the API documentation, `offer` returns
    a `boolean` value, indicating whether or not the element was added to the queue.
    When the queue is full, it returns false. These are exactly the semantics that
    we wish to enforce. We can modify this snippet accordingly:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pattern matching in the preceding snippet provides a good entry point to
    introduce application metrics for introspection and transparency. For example,
    it is likely an interesting business metric to track how many events a trading
    system discards over time. This information may also be useful to the data science
    team for offline analysis in order to determine interesting patterns between discarded
    events and profitability. Whenever you encounter state changes, it is worth considering
    whether a metric should be recorded or whether an event should be emitted. Take
    a moment to consider state changes in your application. Are you making state changes
    available for introspection to nontechnical team members?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Performing a benchmark with 12,000 events per second and 30 trials, totaling
    360,000 events processed, yields the following result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **12,000 EPS with queue size = 20** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) latency | 0.0 ms |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) latency | 0.0 ms |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) latency | 3.0 ms |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) latency | 11.0 ms |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) latency | 45.0 ms |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| Mean latency | 0.1 ms |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| Events processed as percentage of total events | 31.49% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: This table introduces two rows to record the observed mean latency and the percentage
    of events processed out of the 360,000 that are provided. This row is important
    because the system now rejects events, which is an example of trading throughput
    for latency improvements. The latency profile looks great in comparison to the
    first benchmarking attempt at 12,000 events per second. The maximum latency is
    four times larger than our desired maximum latency. This suggests that our performance
    model is optimistic. The higher maximum latency can be attributed to an unlucky
    garbage collection pause in combination with wrongly estimating the actual processing
    latency. Even so, the maximum latency is two orders of magnitude lower than the
    maximum latency that was observed during the first benchmarking trial. We also
    observe that 99.9% of requests have a latency less than or equal to 11 ms, which
    is within 10% of our stated maximum latency goal.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'While the latency profile looks excellent, the same cannot be said about the
    throughput. Due to our new load-control policy, only approximately 30% of the
    provided events were processed. When an event is processed, it is processed quickly,
    but unfortunately events are discarded two-thirds of the time. Another takeaway
    from performance tuning with load-control policies is that you will likely require
    multiple iterations to properly tune a policy for the right balance between trading
    throughput for latency and vice-versa. Reviewing the results of the benchmark,
    you note the mean observed latency is 0.1 ms. As a next step, you choose to calibrate
    the queue size according to the mean latency. By tuning according to the mean
    latency, you are implying that you are willing to introduce latency in exchange
    for improved throughput. Performing the arithmetic reveals the new queue size:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After re-running the benchmark with the new queue size, you observe the following
    results:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **12,000 EPS with queue size = 100** |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) latency | 3.0 ms |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) latency | 5.0 ms |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) latency | 19.0 ms |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) latency | 43.0 ms |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) latency | 163.0 ms |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| Mean latency | 3.9 ms |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| Events processed as percentage of total events | 92.69% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: As expected, the latency profile lost ground when compared to the trial with
    a queue size of 20\. Except for the maximum latency, each percentile experienced
    at least a doubling in latency. The good news from this experiment is that the
    tail latencies did not experience exponential growth. The throughput picture is
    dramatically changed as well. We observe more than a doubling in throughput, yielding
    nearly 93% of all events processed. The mean latency is 39 times larger than the
    previously recorded 0.1 ms mean latency. For comparative purposes, the mean reflects
    the significant increase in median and 75^(th) percentile latencies.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final test, out of curiosity, you try doubling the throughput rate while
    retaining a queue size of 100 elements. Will the trading system crash and burn,
    will it process all the requests, or will it do something different? Running the
    benchmark produces the following results:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **24,000 EPS with queue size = 100** |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) latency | 7.0 ms |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) latency | 8.0 ms |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) latency | 23.0 ms |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) latency | 55.0 ms |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) latency | 72.0 ms |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Mean latency | 8.4 ms |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Events processed as percentage of total events | 44.58% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: The good news is that the trading system did not crash and burn. It withstood
    receiving double the throughput that previously caused second delays with a latency
    profile qualitatively similar to the same trial at 12,000 events per second. This
    suggests that the work rejection policy has made the trading system significantly
    more robust to high volumes of incoming events.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The tradeoff for improved durability and acceptable processing latencies at
    higher volumes is lower throughput. These experiments revealed the value of bounding
    queue sizes, which we learned about when studying how to apply back pressure along
    with the value of rejecting work. After implementing the load-control policy and
    only tuning queue size, we are able to produce dramatically different results.
    There is definitely room for further analysis and tuning. Further analysis should
    involve product owners to weigh the throughput versus latency tradeoffs. It is
    important to remember that although the load control policy's implementation relies
    on knowledge of highly technical topics, the benefit should be measured in terms
    of business value.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Interrupting expensive processing
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A second idea that we can explore is to halt processing before it completes.
    This is a powerful technique to ensure processing cycles are not spent on work
    that is already stale. Consider a request that is taken from the queue and undergoes
    partial processing before being interrupted by a garbage collection cycle. If
    the garbage collection cycle takes more than a couple of milliseconds, the event
    is now stale and will likely harm trading strategy profitability. Worse, all subsequent
    events in the queue are also now more likely to be stale as well.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this shortcoming, we can apply a technique that is analogous to
    rejecting work by imposing latency limits throughout processing. By carrying a
    timestamp that indicates when processing was started, it is possible to evaluate
    a computation''s latency at discrete points in time. Let''s consider a manufactured
    example to illustrate the idea. Consider the following processing pipeline, which
    runs arbitrary business logic for an event after journaling the event and updating
    metrics:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To avoid processing latent events, we may write logic similar to the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this snippet, a `hasEventProcessingExpiryExpired` method is introduced to
    branch processing, which is based on time. The implementation of this method is
    omitted, but you can imagine that system time is queried and compared to a known
    and allowed processing duration (for example, 5 ms). While this approach accomplishes
    our goal of interrupting latent event processing, the code is now cluttered with
    multiple concerns. Even in this trivial example, it becomes more challenging to
    follow the sequence of processing steps.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The pain point with this code is that the business logic is intertwined with
    the cross-cutting concern of interrupting latent processing. One way to improve
    the readability of this code is to separate the description of what is being accomplished
    from how this description is executed. There is a construct in functional programming,
    known as the free monad that can help us do exactly this. Let's take a deeper
    look at the free monad to see how we can use it to improve the trading strategy's
    performance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Free monads
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monads and their mathematical underpinnings in the subject of category theory
    are dense subjects deserving a dedicated exploration. As your sprint ends tomorrow
    and you want to deliver improved trading strategy performance, we instead provide
    a practitioner''s perspective on free monads to show how you can use them to address
    a real-world problem. To demonstrate the power of applying free monads to our
    problem, we start by showing the end result and work backwards to develop an intuition
    about how free monads work. To begin, let''s consider the sequence of processing
    steps that are required for a trading strategy to process a `BboUpdated` event
    once picked up from the work queue:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'There are three steps that happen before the trading strategy makes a trading
    decision. If the trading decision is to submit a bid or an offer, the decision
    is sent to the exchange. `strategy` is an implementation of the `TradingStrategy`
    trait, which looks like the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, let's look at how we can translate this processing sequence into the free
    monad and also add in early termination logic.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Describing a program
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To build our new version of the trading strategy pipeline, we use the Scalaz-provided
    free monad implementation, `scalaz.Free`. The end result of our efforts to use
    the free monad in conjunction with a domain-specific language (DSL) for simpler
    construction looks like the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Recall that our first attempt at implementing short-circuiting logic involved
    a series of if-statements. Instead of if-statements, the free monad-based snippet
    shows that the processing pipeline can now be defined as a for-comprehension.
    This approach removes the branching statements, making it simpler to understand
    what is happening. Without seeing how the DSL is made, you likely can already
    infer what this pipeline will do. For example, you likely inferred that if `journalEvent`
    takes more than 10 ms to execute, then the processing is halted and neither `performPreTradeBalanceChecks`
    nor `MakeTradingDecision` will be invoked.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The construction of the pipeline is only one half of the story. Underlying
    the implementation of this for-comprehension is the free monad. Creating a free
    monad involves two parts:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Building a description of a program
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing an interpreter to execute the description
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The for-comprehension represents our description of a program. It is a description
    of how to process the `BboUpdated` events that also defines execution delay constraints.
    To execute this description, we must build an interpreter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Building an interpreter
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our interpreter looks like the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `foldRun` method is a method that is provided by `Free` to execute the
    description of the program that we wrote. Analogous to the signature of `foldLeft`, `foldRun`
    accepts a value representing an initial state, a curried function that accepts
    the current state, and the next processing step from our processing pipeline.
    The next processing step is represented as an ADT named `Thunk` with the following
    members:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `Thunk` algebra defines the possible operations that can be transcribed
    into the free monad. The pipeline that we previously showed is constructed by
    composing together combinations of the `Thunk` members. This pipeline hides the
    construction behind the DSL to eliminate verbosity and to improve readability.
    The following table maps each processing step to its associated `Thunk`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '| **Step DSL** | **Thunk** |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| `StartWith` | `StartProcessing` |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| `Step` | `Timed` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| `MakeTradingDecision` | `TradingDecision` |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: Returning to the curried `foldRun` function, we see that the interpreter pattern
    matches to determine which `Thunk` is the next processing step. These pattern
    match statements are how the interpreter applies the behavior that is described
    by our program's description. `StartProcessing` and `Timed` use system time to
    determine which method to execute, based on the provided millisecond expiry (`LimitMs`). `StartProcessing`
    and `TradingDecision` require states from the outside world to support execution.
    For `StartProcessing`, the `BboUpdated` event from the work queue must be supplied,
    and for `TradingDecision`, a `Strategy` must be provided to yield a trading decision.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The return value of `foldRun` is a tuple of the accumulated state, which is
    discarded in the snippet, and the return value of interpreting the free monad.
    The return value of executing the sequence of `Thunk`s that is defined by `pipeline`
    is `\/[BboProcessingFailure, Option[Either[Bid,Offer]]]`. The return value is
    a disjunction to account for failure scenarios, which can occur as part of the
    business logic or because the processing expiry expired. These failures are represented
    with an ADT of type `BboProcessingFailure`. The right side of the disjunction
    matches the return type of `TradingStrategy`, indicating that completing all steps
    in `pipeline` yields a trading decision. The final step is to fold over the trading
    decision to record processing latency when the pipeline was completed (that is,
    a `\/-` was returned) and to conditionally send the order to the exchange.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: At this juncture, the intuition that you should have developed is that we have
    separated the description of what we would like to have happen from how it happens.
    The free monad allows us to do this by first creating a description of our program,
    and then secondly, building an interpreter to execute the instructions that are
    provided by the description. As a concrete example, our program description in
    `pipeline` is not bogged down with providing a strategy for how to implement early
    termination. Instead, it only describes that certain steps in the processing sequence
    are subject to time constraints. The interpreter provided to `foldRun` enforces
    this constraint using system time. Having built a functioning version of the trading
    strategy pipeline, let's benchmark again to see what effect our changes had.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking the new trading strategy pipeline
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Running the benchmark at 12,000 and 24,000 events per second using the new
    trading strategy pipeline yields the following results. The results columns show
    two values per row. The value before the slash is the result from running with
    the new implementation that provides early termination. The value after the slash
    is the copied over result from running without the early termination for comparative
    purposes:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **12,000 EPS with queue size = 100** | **24,000 EPS with queue
    size = 100** |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) latency | 1.0 ms / 3.0 ms | 6.0 ms / 7.0 ms |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) latency | 3.0 ms / 5.0 ms | 7.0 ms / 8.0 ms |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) latency | 7.0 ms / 19.0 ms | 8.0 ms / 23.0 ms |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) latency | 10.0 ms / 44.0 ms | 16.0 ms / 55.0 ms |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) latency | 197.0 ms / 163.0 ms | 26.0 ms / 72.0 ms |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Mean latency | 2.0 ms / 3.9 ms | 6.0 ms / 8.4 ms |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Events processed as percentage of total events | 90.43% / 92.69% | 36.62%
    / 44.58% |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: 'From a latency perspective, early termination appears to be a clear win. Excluding
    maximum latency, early termination yielded lower latencies at each percentile.
    For example, at 12,000 events per second, half of all requests are processed in
    one-third of the time, a mere millisecond, as compared to the median when processing
    is not interrupted. At 12,000 events per second, the observed maximum latency
    increases, which is likely indicative of garbage collection pauses after the early
    termination checks. There are two possible improvements to make to our implementation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Check the processing duration after invoking `performPreTradeBalanceChecks`
    before the `TradingStrategy` is executed
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the processing duration after the trading decision is created
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both scenarios, processing could be interrupted if the latency exceeds a
    threshold. It is straightforward to see that these two steps of the processing
    require attention to reduce the maximum latency because of the clear separation
    of concerns provided by our free monad implementation. Consider how much more
    challenging it would be to reason about execution with the pipeline and early
    termination logic intertwined.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: From a throughput perspective, we see a reduction in throughput in both trials.
    The throughput drop arises from the latent events that are discarded. Here, we
    again see the tradeoff between throughput and latency. We sacrificed throughput
    for a better latency profile. Arguably, it is a worthy tradeoff because the higher
    throughput included stale events, which are more likely to yield trading losses.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: A Task interpreter
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our efforts so far have yielded a significantly improved latency profile while
    sacrificing throughput. What if we could have the best of both worlds? An improved
    latency profile with higher throughput would be ideal but seems to be out of reach.
    One strategy for improved throughput is to introduce concurrency. Perhaps, we
    can make the trading strategy execution concurrent to take advantage of hardware
    with multiple cores. Before diving in, you ping Gary, your colleague who helped
    you discover the lineage of the order book implementations. You double-check with
    Gary to confirm that MVT strategies are thread-safe. He responds with a thumbs
    up emoji, which gives us the green light to parallelize execution of trading strategies.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'In our exploration of the free monad thus far, we have seen the relationship
    between the program description and the interpreter. The program description,
    which is represented with the `Thunk` ADT, is agnostic to the interpreter. This
    statement represents the essence of the free monad and is best stated by Adam
    Warski in his excellent free monad blog post at [https://softwaremill.com/free-monads/](https://softwaremill.com/free-monads/).
    The semantics of the term "free" in free monad is that the monad is free to be
    interpreted in any way. We will see this idea in practice by demonstrating that
    we can transform our existing interpreter to a `Task` interpreter. To do this,
    we must map `Thunk` to `Task`. Scalaz provides a trait to express this mapping,
    called `NaturalTransformation`, with a type alias of `~>`. The following snippet
    shows how to map from `Thunk` to `Task` via a `NaturalTransformation`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The trait defines one method to be implemented that is provided a `Thunk` and
    returns a `Task`. As with our previous interpreter within `foldRun`, the interpreter
    requires the same state to provide the `BboUpdated` event, `MessageSentTimestamp`,
    and `TradingStrategy`. We use pattern matching to handle the mapping of each ADT
    member. Note the usage of `Task.suspend`, which has the following signature:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In contrast to `Task.now`, `suspend` defers evaluation of the argument. This
    is necessary because the interpreter has the side-effect of checking the system
    clock when invoking `hasProcessingTimeExpired`. Using `suspend` defers the call
    to the system clock until the `Task` is run instead of executing at `Task` construction
    time.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'A second interesting implementation note is the usage of `Task.fork` when translating `TradingDecision`.
    Here is the introduction of concurrency to the trading strategy pipeline. With
    our transformation complete, the remaining step is to run the interpreter. Fortunately, `Free`
    provides a method analogous to `foldRun` that accepts a `NaturalTransformation`
    named `foldMap`. The following snippet shows how the existing `Thunk` pipeline
    can be executed using `Task`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Invoking `foldMap` applies the transformation, yielding a `Task`. The `Task`
    is executed asynchronously via `unsafePerformAsync`. Let''s run a benchmark at
    24,000 events per second with our new implementation and compare the results against
    the `foldRun` interpreter:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **24,000 EPS with queue size = 100** |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| 50^(th) (median) latency | 0.0 ms / 6.0 ms |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| 75^(th) latency | 0.0 ms / 7.0 ms |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| 99^(th) latency | 4.0 ms / 8.0 ms |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| 99.9^(th) latency | 13.0 ms / 16.0 ms |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| 100^(th) (maximum) latency | 178.0 ms / 26.0 ms |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| Mean latency | 0.13 ms / 6.0 ms |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| Events processed as percentage of total events | 96.60 % / 36.62% |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: Running the `Task` interpreter on a computer with four cores yields a substantive
    difference in latency and performance. From a throughput perspective, nearly all
    events can be processed, in contrast to the 36% processing rate previously. The
    throughput improvement is indicative of the extra capacity gained by use of `Task.fork`,
    which is providing runtime parallelism. We also observe a significant reduction
    in lower percentile latencies, which can also be attributed to the use of `Task.fork`
    on a multicore machine. Interestingly, the higher percentile latencies remain
    quite similar. As we previously noted, this is because we are still not defending
    against latent events at the end of the processing pipeline. The takeaway from
    this benchmark is that judicious usage of `Task` yields double the throughput
    with an improved latency profile. This is an exciting result to have achieved
    by treating the trading strategy as a black box and only changing how the system
    interacts with the trading strategy.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Exploring free monads further
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our exploration into free monads has deliberately avoided a deep dive into monads
    and instead focused on showing you the practical results from using this approach.
    With free monads, we have shown you that we can separate the description of a
    program from its execution. This allowed us to cleanly introduce logic to interrupt
    the processing of latent events. We also added concurrency to the processing pipeline
    without affecting its construction by writing a `Task` interpreter. The core business
    logic remains pure while retaining excellent runtime characteristics. Here, we
    see the salient point about the free monad. The description of our program is
    a value and the interpreter is responsible for handling side-effects.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you can see the benefits of applying this technique, but you
    are still in the dark about the underlying mechanisms. A full treatment of monads
    is beyond the scope of our exploration. By studying the source code that is associated
    with these examples and exploring other learning sources, you will gain a deeper
    understanding of how to apply this technique in your own systems. We recommend
    reading Adam Warski's aforementioned blog post in-depth and reviewing the presentation
    linked from another free monad example built by Ken Scrambler that is available
    at [https://github.com/kenbot/free](https://github.com/kenbot/free). To get a
    deeper understanding of monads, we encourage you to read, *Functional Programming
    in Scala* by Paul Chiusano and Rúnar Bjarnason.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on high-performance system design in a more language-agnostic
    context. We introduced distributed architectures and explained how they can help
    scale a platform. We presented some of the challenges that such a paradigm involves,
    and we focused on solving the problem of shared state inside a cluster. We used
    CRDTs to implement efficient and performant synchronization among the nodes of
    a cluster. Using these data types, we were able to simplify our architecture and
    avoid creating a bottleneck by eliminating the need for a standalone service that
    is dedicated to storing the shared state. We also kept the latency low by avoiding
    remote calls on the critical path.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this chapter, we analyzed how queues impact latency, and
    how we can apply load control policies to control latency. By benchmarking the
    trading strategy pipeline, we discovered the importance of applying back pressure
    and bounding queue sizes in order to reason about maximum latency. Unbounded queues
    will eventually lead to disastrous production performance. The formal name for
    the study of queues is a branch of mathematics known as queueing theory. Queueing
    theory, like monads, is a topic that deserves a more formal treatment. We focused
    on using empirical observations to drive improvements. Studying queueing theory
    will provide you with a stronger theoretical background and the ability to build
    models for system performance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: We extended the policy of rejecting work to interrupting work that is taking
    too long. In doing so, we explored a new functional programming technique in the
    form of the free monad. The free monad allowed us to maintain clean business logic
    describing what the pipeline does without focusing on how the pipeline accomplishes
    its goals. This separation of concerns enabled us to also add concurrency to the
    pipeline without complicating the pipeline description. The principles that we
    discussed enable you to write high-throughput and low-latency systems that remain
    robust when the system is at capacity, while retaining an emphasis on functional
    design.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
