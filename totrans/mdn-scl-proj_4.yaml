- en: Building a Spam Classification Pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建垃圾邮件分类管道
- en: Two pillars of Google's Gmail service stand out. These are an Inbox folder,
    receiving benign or wanted email messages, and a Spam folder, receiving unsolicited,
    junk emails, or simply spam.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌Gmail服务的两大支柱尤为突出。这些是收件箱文件夹，接收良性或期望收到的电子邮件消息，以及垃圾邮件文件夹，接收未经请求的垃圾邮件，或者简单地说是垃圾邮件。
- en: 'The emphasis of this chapter is on identifying spam and classifying it as such.
    It explores the following topics concerning spam detection:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是识别垃圾邮件并将其分类。它探讨了以下关于垃圾检测的主题：
- en: What are the techniques of separating spam from ham?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何区分垃圾邮件和正常邮件的技术是什么？
- en: If spam filtering is one suitable technique, how can it be formalized as a supervised
    learning classification task?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果垃圾邮件过滤是一种合适的技巧，它如何被形式化为一个监督学习分类任务？
- en: Why is a certain algorithm better than another for spam filtering, and in what
    respect?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么某种算法在垃圾邮件过滤方面比另一种算法更好，以及它在哪些方面更好？
- en: Where are the tangible benefits of effective spam filtering most felt?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的垃圾邮件过滤带来的实际好处在哪里最明显？
- en: This chapter implements a spam filtering data analysis pipeline.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章实现了垃圾邮件过滤数据分析管道。
- en: Implementing a spam classifier with Scala and **machine learning** (**ML**)
    is the overall learning objective of this chapter. Starting from the datasets
    we created for you, we will rely on the Spark ML library's machine learning APIs
    and its supporting libraries to build a spam classification pipeline.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scala和**机器学习**（**ML**）实现垃圾邮件分类是本章的整体学习目标。从为您创建的数据集开始，我们将依赖Spark ML库的机器学习API及其支持库来构建垃圾邮件分类管道。
- en: 'The following list is a section-wise breakdown of the individual learning outcomes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表是按章节划分的个别学习成果的分解：
- en: Introduction to the spam classification problem
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件分类问题简介
- en: The project's problem formulation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目的定义问题
- en: Implementing the spam binary classification pipeline using the various algorithms
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种算法实现垃圾邮件的二分类管道
- en: The goal is to start with `DataFrame` and proceed towards analysis
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是从`DataFrame`开始，然后进行数据分析
- en: Spam classification problem
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件分类问题
- en: Any email service should process incoming mail intelligently. This could be
    classifications that produces two distinct, sorted streams of email, ham and spam.
    Email processing at the sentry level entails a smart vetting process—a classification
    task that produces two distinct, sorted streams of email—ham and spam. Gmail's
    sophisticated spam filtering engine filters out spam by a classification process,
    fulfilling, in a proverbial sense, the separation of the wheat from the chaff.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 任何电子邮件服务都应该智能地处理收到的邮件。这可能包括产生两个不同、排序的邮件流，即正常邮件和垃圾邮件。在哨兵级别的邮件处理涉及一个智能的审查过程——一个产生两个不同、排序的邮件流的分类任务——正常邮件和垃圾邮件。Gmail的复杂垃圾邮件过滤引擎通过分类过程过滤掉垃圾邮件，在比喻意义上实现了“去粗取精”。
- en: 'Spam can be a pernicious phenomenon in our daily lives, which is intimately
    tied to an increasingly connected world. For example, a binary classification
    is an ongoing deceptive link to apparently innocent looking websites hosting malware. Readers
    can learn why a spam filter can minimize problems spam can cause. These are summarized
    as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件在我们的日常生活中可能是一种有害现象，它与日益紧密相连的世界密切相关。例如，二元分类是到看似无辜的网站（这些网站托管恶意软件）的持续欺骗性链接。读者可以了解为什么垃圾邮件过滤器可以最大限度地减少垃圾邮件可能引起的问题。以下是一些总结：
- en: Unethical companies harvest email addresses from the web and send out a flood
    of bulk emails to people. A Gmail user, say, `gmailUser@gmail.com`, is enticed
    to click on an innocent-looking website masquerading as a popular website people
    commonly recognize as reputable. One kind of vile intention is to trap the user
    into giving up personal information on entering a supposedly popular reputable
    website.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不道德的公司从网络上收集电子邮件地址，并向人们发送大量的垃圾邮件。例如，一个Gmail用户，比如`gmailUser@gmail.com`，被诱使点击一个看似无辜的网站，该网站伪装成人们普遍认为是值得信赖的知名网站。一种卑鄙的意图是将用户诱骗在进入一个所谓的流行值得信赖的网站时放弃个人信息。
- en: Another spam email like the one sent out by operators of a shady website, `frz7yblahblah.viral.sparedays.net`,
    is preying on people's predilection for making easy money. For example, the innocent
    looking link contains a deceptive link to some shady website hosting malware,
    a rootkit virus for example. A rootkit is very hard to remove. It is a virus that
    embeds itself into your OS kernel. It can be so elusive and potentially destructive
    that a remote hacker can gain control of your system, and before you know it,
    your network may come to a grinding halt. Several man hours will be lost and,
    if you are a company, revenue will be lost as well.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一封类似于一封由可疑网站运营商发送的垃圾邮件，`frz7yblahblah.viral.sparedays.net`，正在利用人们渴望轻松赚钱的倾向。例如，看起来无害的链接包含了一个欺骗性的链接，指向一些可疑网站，该网站托管恶意软件，例如rootkit病毒。rootkit非常难以删除。它是一种将自己嵌入到操作系统内核的病毒。它可以非常难以追踪和具有潜在的破坏性，以至于远程黑客可以控制您的系统，而您可能还没有意识到，您的网络可能突然停止工作。您将损失数小时的工作时间，如果您是一家公司，您还会损失收入。
- en: 'The following is a screenshot of a spam email sitting in Gmail''s Spam folder,
    indicative of phishing. **Phishing** refers to deliberate attempts at maliciously
    gaining fraudulent access to personal information by deception, as shown in the
    following screenshot:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一张位于Gmail垃圾邮件文件夹中的垃圾邮件截图，表明了钓鱼行为。**钓鱼**是指通过欺骗手段故意恶意获取欺诈性访问个人信息的行为，如下面的截图所示：
- en: '![](img/a636ff4b-2a50-472b-91e3-d8e32d41d807.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a636ff4b-2a50-472b-91e3-d8e32d41d807.jpg)'
- en: An example of a spam email
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一封垃圾邮件的例子
- en: In the next section, we will explore a handful of topics that are relevant to
    the development of the spam classification pipeline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一些与垃圾邮件分类管道开发相关的话题。
- en: Relevant background topics
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关背景话题
- en: 'The following topics are reviewed in this section prior to developing the spam
    classifier:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发垃圾邮件分类器之前，本节回顾了以下主题：
- en: Multidimensional data
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维数据
- en: Importance of features
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征的重要性
- en: Classification task
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类任务
- en: Individual feature importance in relation to another feature
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与另一个特征相关的单个特征重要性
- en: Term frequency-inverse document frequency (TF-IDF)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词频-逆文档频率（TF-IDF）
- en: Hashing trick
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希技巧
- en: Stop word removal
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词去除
- en: Normalization
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化
- en: In the next section, we will talk more about each topic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地讨论每个主题。
- en: Multidimensional data
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多维数据
- en: Multidimensional data is data bearing more than one feature. We have dealt with
    many features in the earlier chapters. That said, let's restate this with an example,
    explaining what a feature means and why features are such a big deal.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 多维数据是包含多个特征的数据。在前面的章节中，我们已经处理了许多特征。话虽如此，让我们用一个例子来重申这一点，解释特征的含义以及为什么特征如此重要。
- en: Features and their importance
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征及其重要性
- en: 'Each feature in a multidimensional dataset is a contributing factor in arriving
    at a prediction:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在多维数据集中，每个特征都是影响预测的一个因素：
- en: A prediction is to be made on a new sample; for example, a new breast cancer
    mass sample belonging to a certain individual
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要对一个新样本进行预测；例如，属于某个个体的新的乳腺癌肿块样本
- en: Each contributing factor has a certain feature importance number or feature
    weight
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个影响因素都有一个特定的特征重要性数字或特征权重
- en: 'Some features are more important than others in contributing to the final prediction.
    In other words, a (final) prediction is made on what category a new sample belongs
    to. For example, in the breast cancer dataset in [Chapter 2](a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml),
    *Build a Breast Cancer Prognosis Pipeline with Spark and Scala*, the Random Forest
    algorithm can be used to estimate feature importance. In the following list the
    top-most features have the highest weight; the feature at the bottom of the list
    has the lowest weight (in order of decreasing importance):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在影响最终预测方面，一些特征比其他特征更重要。换句话说，预测是根据新样本属于哪个类别进行的。例如，在[第2章](a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml)“使用Spark和Scala构建乳腺癌预后管道”中的乳腺癌数据集中，可以使用随机森林算法来估计特征的重要性。在以下列表中，最上面的特征权重最高；列表底部的特征权重最低（按重要性递减顺序）：
- en: '**Uniformity_of_Cell_Size**'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Uniformity_of_Cell_Size**'
- en: '**Uniformity_of_Cell_Shape**'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Uniformity_of_Cell_Shape**'
- en: '**Bare_Nuclei**'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Bare_Nuclei**'
- en: '**Bland_Chromatin**'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Bland_Chromatin**'
- en: '**Single_Epithelial_Cell_Size**'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Single_Epithelial_Cell_Size**'
- en: '**Normal_Nucleoli**'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Normal_Nucleoli**'
- en: '**Clump_Thickness**'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Clump_Thickness**'
- en: '**Marginal_Adhesion**'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Marginal_Adhesion**'
- en: '**Mitosis**'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Mitosis**'
- en: This means that the first feature has the most impact on the final predicted
    outcome, second feature has the second biggest impact, and so on.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着第一个特征对最终预测结果的影响最大，第二个特征有第二大影响，依此类推。
- en: We have just covered features, feature importance, weight, and so on. This exercise
    has laid the groundwork for this chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚讨论了特征、特征重要性、权重等。这个练习为本章奠定了基础。
- en: In the following section, we will look at classification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨分类。
- en: Classification task
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类任务
- en: '**Classification** implies a categorization action, a task involving categories.
    A classification task then typically denotes a supervised learning technique that
    lets us categorize a new sample previously unseen (such as an Iris flower, whose
    species we do not know yet). By categorizing, we allude to the fact that the classification task
    is tagging an unseen sample with one of the predicted labels in the training dataset.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**意味着一个分类动作，一个涉及类别的任务。因此，分类任务通常表示一种监督学习技术，它使我们能够对之前未见过的样本（例如，我们尚不知道其物种的鸢尾花）进行分类。通过分类，我们暗示分类任务是在用训练数据集中的预测标签之一标记未见样本。'
- en: '![](img/d01dd528-4984-4ba1-ad03-116f94e54ccd.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d01dd528-4984-4ba1-ad03-116f94e54ccd.png)'
- en: What is classification?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是分类？
- en: Before we move on the to next question, we will draw your attention to the term
    **training dataset**. In the next topic on classification outcomes, we discuss
    classification outcomes as binary or categorical and explain supporting concepts
    like **target** variable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续下一个问题之前，我们将把您的注意力引向术语**训练数据集**。在分类结果的主题中，我们将讨论分类结果作为二元或分类的，并解释支持概念如**目标**变量。
- en: Classification outcomes
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类结果
- en: Up until [Chapter 3](dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml), *Stock Price
    Predictions**,* we worked with ML problems related to classification as a supervised
    learning technique. The classification tasks were data-centered (data as in samples, observations,
    or measurements) for which you already know the target answer. That brings us
    to the term **t****arget variable**. This is another commonly-used alternative
    name for the **response variable**, a term from statistics. A target variable,
    in the ML context, is the variable that is typically the output or outcome. For
    example, it could be a binary outcome variable with only two classification outcomes—`0`
    or `1`. With that point made, we know that data for which the target answer is
    readily known or predetermined is called **labeled data**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 直到[第3章](dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml)“股票价格预测”，我们一直在处理与分类作为监督学习技术相关的机器学习问题。分类任务是以数据为中心的（数据在这里指样本、观察或测量），对于这些数据，你已经知道目标答案。这引出了术语**目标变量**。这是**响应变量**（统计学中的一个术语）的另一个常用名称。在机器学习背景下，目标变量通常是输出或结果。例如，它可能是一个只有两个分类结果——`0`或`1`的二进制结果变量。有了这一点，我们知道目标答案已知或预先确定的数据被称为**标记数据**。
- en: The classification task developed in this chapter is all about supervised learning,
    where an algorithm is teaching itself to learn from the labeled samples that we
    provide. A notable example from a previous chapter is the breast cancer dataset,
    which is also a supervised learning classification task. In that chapter, we classified
    breast cancer samples in two categories—benign and malignant. These are the two
    classification outcomes, outcome values that we can use tolabel or tag so-called
    training data that an algorithm will be trained on. On the other hand, unlabeled
    data is a new breast cancer sample data waiting to be diagnosed. More pertinently,
    a new incoming corpus possibly containing both ham and spam contains unlabeled
    data. Based on labeled samples from the training set, you could try to classify
    the unlabeled samples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开发的分类任务完全是关于监督学习，其中算法通过我们从标记样本中学习来自我教学。前一章的一个显著例子是乳腺癌数据集，它也是一个监督学习分类任务。在前一章中，我们将乳腺癌样本分为两类——良性肿瘤和恶性肿瘤。这些都是两个分类结果，是我们可以用作标记或标记所谓的训练数据的值。另一方面，未标记数据是等待诊断的新乳腺癌样本数据。更有意义的是，一个可能包含垃圾邮件和正常邮件的新到达语料库包含未标记数据。基于训练集中的标记样本，你可以尝试对未标记样本进行分类。
- en: Two possible classification outcomes
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种可能的分类结果
- en: Spam filtering is a binary classification task, an ML task that generates predicted
    values that only contain one of two possible classification outcomes. In this
    chapter, we will set out to build a spam classifier. Labels in the spam classification
    set belong to a finite set consisting of text from two types of emails, spam and
    ham. The binary classification task then becomes one of predicting the (output)
    label from previously unseen data. Deciding whether an email is spam or not, therefore,
    become a binary classification problem. By convention, we assign the ham mutually
    exclusive state a value of `1`, and `0` for the other state, spam. In the next
    section, we will formulate the spam classification problem at hand. This will
    give us an overview of the project as well.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤是一个二元分类任务，一个生成只包含两种可能分类结果的预测值的机器学习任务。在本章中，我们将着手构建一个垃圾邮件分类器。垃圾邮件分类集中的标签属于一个有限集合，包括两种类型电子邮件的文本，即垃圾邮件和正常邮件。因此，二元分类任务变成了从先前未见数据中预测（输出）标签的问题。因此，判断一封邮件是否为垃圾邮件成为一个二元分类问题。按照惯例，我们将正常邮件的互斥状态赋值为`1`，而将另一种状态垃圾邮件赋值为`0`。在下一节中，我们将对当前的垃圾邮件分类问题进行公式化。这将给我们一个项目的概述。
- en: Project overview – problem formulation
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目概述——问题公式化
- en: In this chapter, the stated goal is to build a spam classifier, one that is
    capable of distinguishing spam terms in email messages that are mixed in with
    regular or expected email content as well. It is important to know that spam messages
    are email messages that are sent out to multiple recipients with the same content,
    as opposed to regular messages. We start with two email datasets, one that represents
    ham and one that represents spam. After stages of preprocessing, we fit the model
    on a training set, say 70% of the entire dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们设定的目标是构建一个垃圾邮件分类器，它能够区分电子邮件中的垃圾邮件术语，这些术语与常规或预期的电子邮件内容混合在一起。重要的是要知道，垃圾邮件是指发送给多个收件人的具有相同内容的电子邮件，与常规邮件相反。我们开始使用两个电子邮件数据集，一个代表正常邮件，另一个代表垃圾邮件。经过预处理阶段后，我们在训练集上拟合模型，比如说整个数据集的70%。
- en: This application is a typical spam filtering application in the sense that it
    works on text. We then put algorithms to work that help the ML process detect
    words, phrases, and terms most likely found in spam emails. Next, will go over
    the ML workflow at a high level in relation to spam filtering.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个意义上说，这个应用程序是一个典型的基于文本的垃圾邮件过滤应用程序。然后我们使用算法来帮助机器学习过程检测在垃圾邮件中最可能出现的单词、短语和术语。接下来，我们将从高层次上概述与垃圾邮件过滤相关的机器学习工作流程。
- en: 'The ML workflow is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程如下：
- en: We will be developing a pipeline that will use dataframes
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将开发一个使用数据框的管道。
- en: A dataframe contains a `predictions` column and another column containing preprocessed
    text
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据框包含一个`predictions`列和另一个包含预处理文本的列
- en: The classification process involves transformation operations—one `DataFrame`
    is transformed into another
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类过程涉及转换操作——一个`DataFrame`被转换成另一个
- en: Our pipeline runs a series of stages, involving TF-IDF, a hashing trick, stop
    word removal, and Naive Bayes algorithms
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的管道运行一系列阶段，包括TF-IDF、哈希技巧、停用词去除和朴素贝叶斯算法
- en: In essence, the spam filtering or classification problem is a supervised learning
    task, where we supply labeled data to the pipeline. A natural language processing
    step in this task entails labeled feature data being converted into a bag of feature
    vectors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，垃圾邮件过滤或分类问题是一个监督学习任务，我们向管道提供标记数据。在这个任务中，自然语言处理步骤包括将标记特征数据转换为特征向量集合。
- en: 'At this point, we can lay out the steps needed to build the spam classifier:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以列出构建垃圾邮件分类器所需的步骤：
- en: '![](img/48f4e0eb-d68e-4f20-a159-d6d7fbd1ffcf.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/48f4e0eb-d68e-4f20-a159-d6d7fbd1ffcf.jpg)'
- en: Steps needed to build the spam classifier
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建垃圾邮件分类器所需的步骤
- en: The steps described in the preceding list are useful and help us come up with
    an outline of the spam classifier.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上列步骤描述的步骤是有用的，并帮助我们制定垃圾邮件分类器的概要。
- en: 'The following diagram represents a formulation of the spam classification problem:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表示了垃圾邮件分类问题的公式化：
- en: '![](img/de19765f-3762-4f3b-a671-a0c335125a7b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/de19765f-3762-4f3b-a671-a0c335125a7b.png)'
- en: Spam classifier outline
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件分类器概要
- en: 'Here is a refresher on the following topics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是以下主题的复习：
- en: Stop words
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词
- en: Punctuation marks
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号
- en: Regular expressions
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则表达式
- en: 'We want to eliminate two categories of text in our spam and ham datasets. These
    are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在垃圾邮件和正常邮件数据集中消除两类文本。具体如下：
- en: 'Punctuation marks may be characterized in three categories:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号可以分为三类：
- en: Terminal points or marks
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终止点或标记
- en: Dashes and hyphens
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 破折号和连字符
- en: Pausing points, or marks
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暂停点或标记
- en: Stop words.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词。
- en: 'The following is a representative list of punctuation marks:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代表性标点符号列表：
- en: '![](img/f1993867-b611-4904-ac75-f5d2031318da.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f1993867-b611-4904-ac75-f5d2031318da.jpg)'
- en: Punctuation marks
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标点符号
- en: We covered a list of commonly encountered punctuation marks in a text corpus
    that we want to be removed from our spam and ham datasets.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在文本语料库中覆盖了想要从我们的垃圾邮件和火腿数据集中移除的常见标点符号列表。
- en: We also need to remove stop words—words that are common. Our spam classifier
    will remove these in the preliminary preprocessing steps.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要移除停用词——即常见的词。我们的垃圾邮件分类器将在初步预处理步骤中移除这些词。
- en: 'Here is a representative list of stop words:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个停用词的代表性列表：
- en: '![](img/cb476fb0-89b2-4270-a6be-6f61b5631b7e.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cb476fb0-89b2-4270-a6be-6f61b5631b7e.jpg)'
- en: A representative list of stop words
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词的代表性列表
- en: 'The following is a representative list of regular expressions to help with
    punctuation mark removal. A spam corpus can be daunting. Regular expressions can
    get as complex as can be in order to cope with spam:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个帮助删除标点符号的代表性正则表达式列表。垃圾邮件语料库可能令人望而生畏。正则表达式可以变得非常复杂，以便应对垃圾邮件：
- en: '![](img/d8d2580b-95fb-479a-9d78-e0c68ab15dc0.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d8d2580b-95fb-479a-9d78-e0c68ab15dc0.jpg)'
- en: Some relevant regular expressions
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一些相关的正则表达式
- en: In the *Getting started* section, we will get started implementing the project.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在*入门*部分，我们将开始实施项目。
- en: Getting started
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: In order to get started, download the dataset from the `ModernScalaProjects_Code`
    folder and drop it into the root folder of your project.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，请从`ModernScalaProjects_Code`文件夹下载数据集，并将其放入您项目的根目录。
- en: Setting up prerequisite software
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置先决软件
- en: You may use your existing software setup from previous chapters. Apache Log4j
    2 Scala API is the notable exception. This is a Scala wrapper over Log4j 2, which
    is an upgraded `Logger` implementation of Log4j version 1.x (the version provided
    by Spark).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用之前章节中现有的软件设置。Apache Log4j 2 Scala API是一个值得注意的例外。这是一个Log4j 2的Scala包装器，它是Log4j
    1.x版本（Spark提供的版本）的`Logger`实现的一个升级版。
- en: Simply override the existing Log4j from Spark (version 1.6) with Log4j 2 Scala
    by adding in appropriate entries in the `build.sbt` file.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地通过在`build.sbt`文件中添加适当的条目，用Log4j 2 Scala覆盖Spark中现有的Log4j（版本1.6）。
- en: 'The following table lists two choices of prerequisite software:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了两种先决软件的选择：
- en: '![](img/ada1086f-6195-4c7f-9d64-c10b4004edc7.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ada1086f-6195-4c7f-9d64-c10b4004edc7.jpg)'
- en: Implementation infrastructure
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实施基础设施
- en: Download the dataset from the `ModernScalaProjects_Code` folder and drop it
    into the root folder of your project.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ModernScalaProjects_Code`文件夹下载数据集，并将其放入您项目的根目录。
- en: Spam classification pipeline
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件分类管道
- en: 'The most important development objective of this chapter is to perform spam
    classification tasks with the following algorithms:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最重要的开发目标是使用以下算法执行垃圾邮件分类任务：
- en: Stop word remover
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词移除器
- en: Naive Bayes
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Inverse document frequency
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆文档频率
- en: Hashing trick transformer
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希技巧转换器
- en: Normalizer
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化器
- en: 'The practical goal of our spam classification task is this: Given a new incoming
    document, say, a collection of random emails from either Inbox or Spam, the classifier
    must be able to identify spam in the corpus. After all, this is the basis of an
    effective classifier. The real-world benefit behind developing this classifier
    to give our readers experience of developing their own spam filters. After learning
    how to put together the classifier, we will develop it.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们垃圾邮件分类任务的实用目标是这样的：给定一个新的传入文档，比如，来自收件箱或垃圾邮件的随机电子邮件集合，分类器必须能够识别语料库中的垃圾邮件。毕竟，这是有效分类的基础。开发这个分类器的现实世界好处在于给读者提供开发他们自己的垃圾邮件过滤器体验。在学习如何组装分类器之后，我们将开发它。
- en: The implementation steps are in the next section. This takes us straight into
    the development of Scala code in a Spark environment. Given that Spark allows
    us to write powerful distributed ML programs such as pipelines, that is exactly
    what we will set out to do. We will start by understanding the individual implementation
    steps required to reach our goal.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 实施步骤将在下一节中介绍。这将直接带我们进入在Spark环境中开发Scala代码。鉴于Spark允许我们编写强大的分布式ML程序，如管道，这正是我们将着手去做的事情。我们将从理解达到目标所需的单个实施步骤开始。
- en: Implementation steps
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施步骤
- en: 'The spam detection (or classification) pipeline involves five stages of implementation,
    grouped by typical ML steps that are needed. These are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocessing data
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracting features
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the spam classifier
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating predictions
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we will set up a Scala project in IntelliJ.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – setting up your project folder
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is what the project looks like in IntelliJ:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5993f097-c76c-4a0d-8e3c-b66c52adf47b.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Project outline in IntelliJ
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will upgrade the `build.sbt` file.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – upgrading your build.sbt file
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the upgraded `build.sbt` file. What is new here? Remember, earlier,
    we talked about a new `Logging` library. Those new entries you see in the following
    screenshot are the new dependencies you need to move from Log4j 1.6 to the new
    Scala wrapper for Log4j 2:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36d57b4b-0780-4597-94ac-e28ad944b4ef.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: New entries in the build.sbt file
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will start with the Scala code, starting with a trait.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – creating a trait called SpamWrapper
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In IntelliJ, using File | New | Scala class, create an empty Scala trait called
    `SpamWrapper` in a file called `SpamWrapper.scala`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first. At the top of the file, we will set up the following imports
    for implementing classes to take advantage of this trait:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession`—the entry point to programming with Spark'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The appropriate Log4J library imports, so that we can dial down logging messages
    from Spark:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the minimum imports. Next, create an empty `trait`. The following
    is an updated `trait`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Inside the `SpamWrapper` trait, create a `SparkSession` instance called `session`.
    At this point, here is a refresher on Spark:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: We need a `SparkSession` object instance to be the entry point to programming
    with Spark.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not need a separate `SparkContext`. This is provided by `SparkSession`.
    The underlying context is available to us easily as `session.sparkContext`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create a `SparkSession` object instance or get an existing `SparkSession`,
    a builder pattern is used.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SparkSession` instance is available for the entire time frame of a Spark
    job.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the updated `SpamWrapper` trait:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/162c02a7-0964-4834-9f57-13d71a19cb94.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: SpamWrapper trait with SparkSession value
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we create a `val` called `session` that our pipeline class will use.
    Of course, this will be the entry point to programming with Spark to build this
    spam classifier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – describing the dataset
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the dataset from the `ModernScalaProjects_Code` folder. It consists
    of two text files:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '`inbox.txt`: Ham emails (I created this file from my Gmail Inbox folder)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`junk.txt`: Spam emails (I created this out of spam from my Gmail Spam folder)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop these files into the root of your project folder. In the next section,
    we will describe the dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Description of the SpamHam dataset
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we present the actual dataset, here are a few real-world spam samples:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fbb4607-4350-4b47-b611-78f1cbf2b1ed.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Spam email with phishing example
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 带有钓鱼示例的垃圾邮件
- en: 'Here is an example of regular or wanted mail, also known as ham:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是常规或所需邮件的示例，也称为ham：
- en: '![](img/2601a3a7-7c2f-4190-95bd-14a4981fa7f3.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2601a3a7-7c2f-4190-95bd-14a4981fa7f3.jpg)'
- en: A perfectly normal email from Lightbend
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Lightbend的一个完全正常的电子邮件
- en: 'The following is a glimpse into the actual dataset used in our spam-ham classification
    task. There are two datasets:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们用于垃圾邮件-ham分类任务的实际数据集的预览。有两个数据集：
- en: '`inbox.txt`: A ham dataset compiled from a small collection of regular emails
    from my Inbox folder'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inbox.txt`：从我的收件箱文件夹中收集的一小批常规电子邮件组成的ham数据集'
- en: '`junk.txt`: A spam dataset compiled from a small collection of junk email from
    my Spam/Junk folder'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`junk.txt`：从我的垃圾邮件/垃圾文件夹中收集的一小批垃圾邮件组成的垃圾邮件数据集'
- en: 'Here is the regular dataset:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是常规数据集：
- en: '![](img/d01fdfb1-2a11-47ea-a21f-178684369570.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d01fdfb1-2a11-47ea-a21f-178684369570.jpg)'
- en: A portion of the regular email dataset
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 常规电子邮件数据集的一部分
- en: 'Here is the spam dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是垃圾邮件数据集：
- en: '![](img/7942ce19-5c5f-43d5-aa06-f9ee35a68a72.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7942ce19-5c5f-43d5-aa06-f9ee35a68a72.jpg)'
- en: A portion of the spam dataset
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件数据集的一部分
- en: The preceding email wants you to confirm here. This is an attempt at phishing.
    This completes our description of our datasets. In the next step, we will proceed
    with data preprocessing. We need a new Scala object called `SpamClassifierPipeline`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的邮件要求你在这里确认。这是一个钓鱼尝试。这完成了我们对数据集的描述。在下一步，我们将进行数据预处理。我们需要一个新的Scala对象，名为`SpamClassifierPipeline`。
- en: Step 5 – creating a new spam classifier class
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5步 – 创建一个新的垃圾邮件分类器类
- en: 'We will create a new Scala file called `SpamClassifierPipeline.scala`. First
    of all, we need the following imports:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个名为`SpamClassifierPipeline.scala`的新Scala文件。首先，我们需要以下导入：
- en: '![](img/7a622b9e-8f5f-4813-b51f-5c4dd991df3d.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a622b9e-8f5f-4813-b51f-5c4dd991df3d.jpg)'
- en: Required imports
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 必需的导入
- en: 'Now that the imports have been created, let''s create an empty `SpamClassifierPipeline`
    object in the same package as the `SpamWrapper` trait, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经创建了导入，让我们在`SpamWrapper`特质相同的包中创建一个空的`SpamClassifierPipeline`对象，如下所示：
- en: '[PRE1]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A prototype spam classifier is ready. We need to create code in it to do such
    things as preprocessing the data and, of course, much more. In the next step,
    we will list the necessary preprocessing steps to take up.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个原型垃圾邮件分类器已经准备好了。我们需要在其中编写代码来做诸如数据预处理等事情，当然还有更多。在下一步，我们将列出必要的预处理步骤。
- en: Step 6 – listing the data preprocessing steps
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步 – 列出数据预处理步骤
- en: There is a purpose to preprocessing. In most data analytics tasks, the question
    that begs to be asked is—is our data necessarily usable? The answer lies in the
    fact that most real-world datasets need preprocessing, a massaging step meant
    to give data a new usable form.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理有一个目的。在大多数数据分析任务中，迫切需要问的问题是——我们的数据是否必然可用？答案在于，大多数现实世界的数据集都需要预处理，这是一个按摩步骤，旨在给数据一个新的可用形式。
- en: 'With the spam and ham datasets, we identify two important preprocessing steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用垃圾邮件和ham数据集，我们确定了两个重要的预处理步骤：
- en: 'Removing punctuation marks:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除标点符号：
- en: Processing punctuation using regular expressions
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式处理标点符号
- en: Removing stop words
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: In the next step, we will write Scala code for two regular expressions, expressions
    that are fairly simple and only address a small subset of spam. However, it's
    a start.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步，我们将编写Scala代码来编写两个正则表达式，这些表达式相当简单，仅针对一小部分垃圾邮件。但这是一个开始。
- en: In the next step, we will load our datasets into Spark. Naturally, we want a
    ham dataframe and a spam dataframe. We will take up the task of creating a ham
    dataframe first. Both our ham and spam datasets are ready for preprocessing. That
    brings us to the next step.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步，我们将把我们的数据集加载到Spark中。自然地，我们想要一个ham数据框和一个spam数据框。我们首先承担创建ham数据框的任务。我们的ham和spam数据集都准备好了，可以进行预处理。这带我们到了下一步。
- en: Step 7 – regex to remove punctuation marks and whitespaces
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7步 – 移除标点符号和空白的正则表达式
- en: 'Here is a regular expression for our immediate needs:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们当前需要的正则表达式：
- en: '[PRE2]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`raw` is a method in the string interpolation `StringContext` class of the
    standard Scala library.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`raw`是标准Scala库中`StringContext`类的内联代码方法。'
- en: 'Our corpus is likely to contain trailing and leading whitespaces which are
    sometimes ill-formatted. For example, we will run into whitespaces where lines
    are indented too much. To get rid of whitespaces, we will use regular expressions.
    This will work with the anchors, the hat `^`, and the `$` sign to extract the
    text without whitespaces. The updated regular expression now looks like this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语料库可能包含尾随和前导空格，有时格式不正确。例如，我们会在行缩进过多的地方遇到空格。为了去除空格，我们将使用正则表达式。这将使用锚点，帽子`^`和美元符号`$`来提取不带空格的文本。更新后的正则表达式现在看起来是这样的：
- en: '[PRE3]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We just created a regular expression, `regex2`, a whitespace, and punctuation
    remover. Very soon, we will need this `regex2`. In the next section, we will create
    a new ham dataframe after applying the first of a few essential preprocessing
    steps—removing punctuation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建了一个正则表达式`regex2`，一个空格和一个标点符号去除器。很快，我们就需要这个`regex2`。在下一节中，我们将创建一个新的ham数据框，在应用几个基本预处理步骤之一——移除标点符号后。
- en: Step 8 – creating a ham dataframe with punctuation removed
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8步 – 创建一个移除标点的ham数据框
- en: 'We will have the regex work on each row of our ham **resilient distributed
    dataset** (**RDD**) and then use the `replaceAll` method. The underlying regex
    engine will use the regex to conduct a search and match on our ham corpus to come
    up with matched occurrences, as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对ham的**弹性分布式数据集**（**RDD**）的每一行应用正则表达式，然后使用`replaceAll`方法。底层的正则表达式引擎将使用正则表达式在我们的ham语料库中进行搜索和匹配，以找到匹配的实例，如下所示：
- en: '[PRE4]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `replaceAll` method kicked in and replaced all occurrences of whitespaces
    and punctuation. In the next step, we will convert this RDD into a dataframe.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`replaceAll`方法启动并替换了所有空格和标点的出现。在下一步中，我们将把这个RDD转换成数据框。'
- en: 'We created a new ham RDD with leading and trailing whitespaces, and with punctuation
    removed. Let''s display this new ham dataframe:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个新的ham RDD，其中包含首尾空格，并且移除了标点符号。让我们显示这个新的ham数据框：
- en: '[PRE5]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We created a new ham dataframe. We need to transform this dataframe by assigning
    a `label` of `0.0` to each ham sentence. We shall do that in the next step:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个新的ham数据框。我们需要通过将每个ham句子分配`0.0`标签来转换这个数据框。我们将在下一步中这样做：
- en: '[PRE6]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Therefore, we will create a case called `LabeledHamSpam` to model a sentence
    as a feature with a `Double` `label`. Next, create a new ham RDD that has exactly
    four partitions.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将创建一个名为`LabeledHamSpam`的案例来模拟一个句子作为一个带有`Double`标签的特征。接下来，创建一个新的ham RDD，它恰好有四个分区。
- en: Creating a labeled ham dataframe
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建标记的ham数据框
- en: 'We will repartition our ham datafame and apply a `transform` operation to each
    `"Ham sentence"` in the ham dataframe, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新分区我们的ham数据框，并对ham数据框中的每个`"Ham sentence"`应用`transform`操作，如下所示：
- en: '[PRE7]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We repartitioned and created a new RDD that is structured as a row of ham sentences
    labeled with `0.0`. Now, display the first `10` rows of the new ham RDD:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新分区并创建了一个新的RDD，它以带有`0.0`标签的ham句子行结构化。现在，显示新的ham RDD的前`10`行：
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So, we assigned `0.0` for all ham sentences. It is time to create the spam
    RDD:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们为所有ham句子分配了`0.0`。现在是创建垃圾邮件RDD的时候了：
- en: '[PRE9]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Repeat the same set of preprocessing steps for the spam dataset as well.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 同样为垃圾邮件数据集重复相同的预处理步骤。
- en: Step 9 – creating a spam dataframe devoid of punctuation
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9步 – 创建一个不带标点的垃圾邮件数据框
- en: 'We will use the same `LabeledHamSpam` case class to assign a `Double` `value`
    of `1.0` for spam sentences, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的`LabeledHamSpam`案例类来为垃圾邮件句子分配`Double`类型的`value`值为`1.0`，如下所示：
- en: '[PRE10]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the next step, we want a combined dataframe containing both spam and ham
    dataframes.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们想要一个包含垃圾邮件和ham数据框的合并数据框。
- en: Step 10 – joining the spam and ham datasets
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10步 – 合并垃圾邮件和ham数据集
- en: 'In this step, we will use the `++` method to join both dataframes in a `Union` operation:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将使用`++`方法在`Union`操作中连接两个数据框：
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the next section, let''s create a dataframe, with two columns:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们创建一个包含两列的数据框：
- en: A row containing `feature sentences` with punctuation removed
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含已移除标点的`feature sentences`的行
- en: A predetermined `label` column
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预设的`label`列
- en: 'Check for the following code snippet for better understanding:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 检查以下代码片段以获得更好的理解：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We created the new dataframe. Let''s display this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了新的数据框。让我们显示这个：
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, let''s run the following optional checks:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们运行以下可选的检查：
- en: The schema of the dataframe
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框的模式
- en: Columns present in the dataframe
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框中存在的列
- en: 'Here is how we will print out the schema:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这是打印模式的方法：
- en: '[PRE14]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'They look good! Let''s read off the `columns` now:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 它们看起来不错！现在让我们读取`columns`：
- en: '[PRE15]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Up until now, we have created a dataframe that was punctuation-free, but not
    necessarily free of rows that contain null values. Therefore, in order to drop
    any rows containing null values, we will need to import the `DataFrameNaFunctions` class,
    that is, if you have not already imported it:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了一个没有标点符号的dataframe，但并不一定没有包含null值的行。因此，为了删除包含null值的任何行，我们需要导入`DataFrameNaFunctions`类，如果您还没有导入它的话：
- en: '[PRE16]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In order to drop nulls from the punctuation-free dataframe, there is a column
    called `punctFreeSentences`. We will invoke the `drop()` method as shown in the
    following code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从无标点符号的dataframe中删除null值，有一个名为`punctFreeSentences`的列。我们将按照以下代码调用`drop()`方法：
- en: '[PRE17]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The call to the `drop` method in the preceding code causes any rows of sentences
    that contain null values to be dropped. If you so wish, display the first 20 rows
    of the dataframe:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中调用`drop`方法会导致包含null值的句子行被删除。如果您愿意，可以显示dataframe的前20行：
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Display the dataframe. The following code will help you do just that:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 显示dataframe。以下代码将帮助您做到这一点：
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: At this point, a good next step relates to tokenizing punctuation-free rows,
    which also feature what we want to be tokenized. Tokenizing the current dataframe
    is the focus of the next section. Tokenizing brings us one step closer to the
    next preprocessing step—removing stop words.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一个很好的下一步是与分词无标点符号的行相关，这些行也包含我们想要分词的内容。分词是下一节的重点。分词使我们更接近下一个预处理步骤——移除停用词。
- en: Step 11 – tokenizing our features
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11步 – 对我们的特征进行分词
- en: 'Tokenizing is simply an operation executed by an algorithm. It results in the
    tokenization of each row. All the following terms define tokenization:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 分词只是由算法执行的操作。它导致每行的分词。所有以下术语都定义了分词：
- en: Breaking up
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割
- en: Splitting
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割
- en: 'The appropriate term appears to be the second term in the preceding list. For
    each row in the current dataframe, a tokenizer splits a `feature` row into its
    constituent tokens by splitting it along separating whitespaces. Each resulting
    Spark supplies two tokenizers:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎适当的术语是前面列表中的第二个术语。对于当前dataframe中的每一行，一个分词器通过在分隔空格处分割将其`feature`行分割成其构成标记。每个结果Spark提供了两个分词器：
- en: '`Tokenizer` from the package `org.apache.spark.ml`'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自`org.apache.spark.ml`包的`Tokenizer`
- en: '`RegexTokenizer` from the same package'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自同一包的`RegexTokenizer`
- en: 'Both tokenizers are transformers. A transformer in Spark is an algorithm that
    accepts an input column as a (hyper) parameter and spits out a new `DataFrame`
    with a transformed output column, as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个分词器都是转换器。在Spark中，转换器是一个接受输入列作为（超）参数的算法，并输出一个具有转换输出列的新`DataFrame`，如下所示：
- en: '[PRE20]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Calling the `transform` method on `mailTokenizer` will give us a newly transformed
    dataframe:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mailTokenizer`上调用`transform`方法将给我们一个新的转换后的dataframe：
- en: '[PRE21]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The resulting dataframe, `tokenizedBagOfWordsDataFrame`, is a tokenized non-null
    bag of lowercased words. It looks like this:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 结果dataframe，`tokenizedBagOfWordsDataFrame`，是一个分词的非空单词包，全部为小写。它看起来像这样：
- en: '[PRE22]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The important thing to notice here is that a row in the transformed column, `mailFeatureWords`,
    resembles an array of words. Readers will not fail to notice that there are words
    in `mailFeatureWords` known as stop words. These are words that do not significantly
    contribute to our spam classification task. These words can be safely eliminated
    by Spark's `StopWordRemover` algorithm. In the next step, we will see how to put
    `StopWordRemover` to work.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要一点是，转换列中的行`mailFeatureWords`类似于一个单词数组。读者不会错过注意到`mailFeatureWords`中有一些被称为停用词的单词。这些单词对我们的垃圾邮件分类任务没有显著贡献。这些单词可以通过Spark的`StopWordRemover`算法安全地删除。在下一步中，我们将看到如何使用`StopWordRemover`。
- en: Step 12 – removing stop words
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12步 – 移除停用词
- en: 'First, make sure you have the `StopWordRemover` import in the `SpamClassifierPipeline`
    class. Next, we will create an instance of `StopWordRemover` and pass into it
    a (hyper) parameter column, `mailFeatureWords`. We want an output column that
    is devoid of stop words:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保您在`SpamClassifierPipeline`类中导入了`StopWordRemover`。接下来，我们将创建一个`StopWordRemover`的实例，并将其传递给一个（超）参数列，`mailFeatureWords`。我们希望输出列中没有停用词：
- en: '[PRE23]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Just like with `mailTokenizer`, we call the `transform` method to get a new
    `noStopWordsDataFrame`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用`mailTokenizer`一样，我们调用`transform`方法来获取一个新的`noStopWordsDataFrame`：
- en: '[PRE24]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The resulting dataframe, a tokenized, non-null bag of lowercase words with
    no stop words looks like this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 结果dataframe，一个分词的、非空的、无停用词的小写单词包，看起来像这样：
- en: '[PRE25]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the next step, we will do a second transformation on our current dataframe:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将对我们当前的数据帧进行第二次转换：
- en: '[PRE26]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The exploded, tokenized non-null bag of lowercase words with no stop words
    looks like this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 分解、标记、非空、小写且无停用词的词袋看起来如下：
- en: '[PRE27]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This completes data preprocessing. This sets the stage for feature extraction,
    an extremely important ML step.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了数据预处理。这为特征提取，一个极其重要的机器学习步骤做好了准备。
- en: Step 13 – feature extraction
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13步 – 特征提取
- en: 'In this step, we will extract the features of this dataset. We will do the
    following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将提取这个数据集的特征。我们将执行以下操作：
- en: Creating feature vectors
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建特征向量
- en: Creating features entails converting the text into bigrams of characters using
    an n-gram model
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建特征涉及使用n-gram模型将文本转换为字符的二元组
- en: These bigrams of characters will be hashed to a length `10000` feature vector
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些字符的二元组将被哈希到一个长度为`10000`的特征向量
- en: The final feature vector will be passed into Spark ML
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的特征向量将被传递到Spark ML
- en: 'Check out the following code snippet:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码片段：
- en: '[PRE28]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will `transform` the featured version of the `noStopWordsDataFrame`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对`noStopWordsDataFrame`的特征版本进行`transform`操作：
- en: '[PRE29]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'With a hash-featured and tokenized non-null bag of lowercased words with no
    stop words, this `DataFrame` looks like this:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哈希特征和标记、非空、小写且无停用词的词袋，这个`DataFrame`看起来如下：
- en: '![](img/85365a2b-f55c-4efd-a102-c14d0f542532.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/85365a2b-f55c-4efd-a102-c14d0f542532.jpg)'
- en: Dataframe with hash-featured and tokenized non-null bag of lowercase words
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希特征和标记、非空、小写且无停用词的数据帧
- en: At this point, we are ready to create training and test sets.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好创建训练集和测试集。
- en: Step 14 – creating training and test datasets
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14步 – 创建训练集和测试集
- en: 'This step is important because we are going to create a model that we want
    to train with a training set. One way to create a training set is to partition
    the current dataframe and assign 80% of it to a new training dataset:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步很重要，因为我们将要创建一个我们想要用训练集训练的模型。创建训练集的一种方法是将当前的数据帧分区，并将其中80%分配给新的训练集：
- en: '[PRE30]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s retrieve the training set:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检索训练集：
- en: '[PRE31]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The testing dataset follows. Here is how we will create it:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集如下。这是我们创建它的方法：
- en: '[PRE32]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We need to go one step further. A modified version of the training set that
    has the following columns removed is needed:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更进一步。需要一个修改过的训练集版本，以下列被移除：
- en: '`mailFeatureWords`'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mailFeatureWords`'
- en: '`noStopWordsMailFeatures`'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noStopWordsMailFeatures`'
- en: '`mailFeatureHashes`'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mailFeatureHashes`'
- en: 'Here is the new training set, after doing `drop` on the preceding columns:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是经过`drop`操作前几列后的新训练集：
- en: '[PRE33]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The invocation of the `show()` method results in the following display of the
    new training dataset:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`show()`方法会导致以下新训练集的显示：
- en: '![](img/711f0e12-e97a-4639-ad3f-2ba687a881a5.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/711f0e12-e97a-4639-ad3f-2ba687a881a5.jpg)'
- en: Training dataframe
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据帧
- en: 'The next important step right before training (fitting) the model is what is
    known as **inverse document frequency** (**IDF**). Spark provides an estimator
    called IDF that will compute the IDF for us. The IDF is an algorithm that will
    train (fit) models on our current dataframe:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练（拟合）模型之前的重要一步是所谓的**逆文档频率**（**IDF**）。Spark提供了一个名为IDF的估计器，将为我们计算IDF。IDF是一种算法，将在我们的当前数据帧上训练（拟合）模型：
- en: '[PRE34]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we will pass in our `featurizedDF` dataframe into the `fit` method on the
    IDF algorithm. This will produce our model:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将`featurizedDF`数据帧传递到IDF算法的`fit`方法上。这将产生我们的模型：
- en: '[PRE35]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The next step is a normalizing step. A `normalizer` normalizes the scales for
    the different `features` so that different-sized articles are not weighted differently:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个步骤是一个归一化步骤。`normalizer`将不同`特征`的尺度进行归一化，这样不同大小的文章不会被不同地加权：
- en: '[PRE36]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s use the Naive Bayes algorithm now. Initialize it and pass into it the
    hyperparameters it needs:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用朴素贝叶斯算法。初始化它，并传递给它所需的超参数：
- en: '[PRE37]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now it''s time to create the pipeline and set up all the stages in it. These
    are as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候创建管道并设置其中的所有阶段了。这些如下：
- en: '`StopWordRemover`'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StopWordRemover`'
- en: '`HashingTF`'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HashingTF`'
- en: '`mailIDF`'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mailIDF`'
- en: '`normalizer`'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalizer`'
- en: '`naiveBayes`'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`naiveBayes`'
- en: 'The code snippet sets up the stages as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段设置了以下阶段：
- en: '[PRE38]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Fit the pipeline to the training documents:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将管道拟合到训练文档中：
- en: '[PRE39]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Make predictions on the test dataset:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上做出预测：
- en: '[PRE40]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we will display generated raw predictions:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将显示生成的原始预测：
- en: '[PRE41]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Note that they are not two tables. It is one table broken in two for visual
    clarity:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它们不是两张表。为了视觉清晰，这是一张被分成两部分的表：
- en: '![](img/582c3cee-80bb-4450-9080-93be48bd420e.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/582c3cee-80bb-4450-9080-93be48bd420e.jpg)'
- en: Raw predictions table
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 原始预测表
- en: 'This is the final step, where we only want to show the relevant columns in
    the predictions table. The following line of code will drop those columns that
    are not required:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最后一步，我们只想在预测表中显示相关列。以下代码行将删除那些不需要的列：
- en: '[PRE42]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Display the final `predictions` table. We only need the first and the last
    column. The `label` column is the predictions the model generated:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 显示最终的`预测`表。我们只需要第一列和最后一列。`标签`列是模型生成的预测：
- en: '[PRE43]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It displays predictions as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 它如下显示预测：
- en: '![](img/cfb85d22-fda0-4bcd-8228-3ecaa8009df4.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cfb85d22-fda0-4bcd-8228-3ecaa8009df4.jpg)'
- en: Predictions
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: 'We are done, so `stop` the `session`:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了，所以`停止``会话`：
- en: '[PRE44]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we created a spam classifier. We started with two datasets,
    one representing ham and the other, spam. We combined both datasets into one combined
    corpus that we put through a set of preprocessing steps, as mentioned in the *Implementation
    steps* section.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们创建了一个垃圾邮件分类器。我们开始时使用了两个数据集，一个代表正常邮件，另一个代表垃圾邮件。我们将这两个数据集合并成一个综合语料库，然后按照*实现步骤*部分中提到的预处理步骤进行处理。
- en: In the next chapter, we will build on some of the techniques learned so far
    to create a fraud detection ML application.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将基于到目前为止学到的某些技术来创建一个欺诈检测机器学习应用。
- en: Questions
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Here are 10 questions that will help to reinforce all of the learning material
    presented in this chapter:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些问题，将有助于加强本章中展示的所有学习材料：
- en: Is the spam classification task a binary classification task?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 垃圾邮件分类任务是一个二元分类任务吗？
- en: What was the significance of the hashing trick in the spam classification task?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在垃圾邮件分类任务中，哈希技巧的意义是什么？
- en: What is hashing collision and how is it minimized?
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哈希冲突是什么，以及它是如何被最小化的？
- en: What do we mean by inverse document frequency?
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们所说的逆文档频率是什么意思？
- en: What are stop words and why do they matter?
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停用词是什么，为什么它们很重要？
- en: What is the role played by the Naive Bayes algorithm in spam classification?
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在垃圾邮件分类中，朴素贝叶斯算法扮演了什么角色？
- en: How do you use the `HashingTF` class in Spark to implement the hashing trick
    in your spam classification process?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在Spark中使用`HashingTF`类来实现垃圾邮件分类过程中的哈希技巧？
- en: What is meant by the vectorization of features?
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们所说的特征向量化是什么意思？
- en: Is there a better algorithm that you can think of to implement the spam classification
    process?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到一个更好的算法来实现垃圾邮件分类过程吗？
- en: What are the benefits of spam filtering, and why do they matter in business
    terms?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是垃圾邮件过滤的好处，为什么从商业角度来说它们很重要？
- en: Further reading
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The following paper is a comprehensive work that deserves reading:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 以下论文是一项全面的工作，值得阅读：
- en: '[https://www.sciencedirect.com/science/article/pii/S2405882316300412](https://www.sciencedirect.com/science/article/pii/S2405882316300412)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.sciencedirect.com/science/article/pii/S2405882316300412](https://www.sciencedirect.com/science/article/pii/S2405882316300412)'
