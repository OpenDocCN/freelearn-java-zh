- en: Building a Spam Classification Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two pillars of Google's Gmail service stand out. These are an Inbox folder,
    receiving benign or wanted email messages, and a Spam folder, receiving unsolicited,
    junk emails, or simply spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'The emphasis of this chapter is on identifying spam and classifying it as such.
    It explores the following topics concerning spam detection:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the techniques of separating spam from ham?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If spam filtering is one suitable technique, how can it be formalized as a supervised
    learning classification task?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is a certain algorithm better than another for spam filtering, and in what
    respect?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where are the tangible benefits of effective spam filtering most felt?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter implements a spam filtering data analysis pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a spam classifier with Scala and **machine learning** (**ML**)
    is the overall learning objective of this chapter. Starting from the datasets
    we created for you, we will rely on the Spark ML library's machine learning APIs
    and its supporting libraries to build a spam classification pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list is a section-wise breakdown of the individual learning outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the spam classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The project's problem formulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the spam binary classification pipeline using the various algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to start with `DataFrame` and proceed towards analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam classification problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any email service should process incoming mail intelligently. This could be
    classifications that produces two distinct, sorted streams of email, ham and spam.
    Email processing at the sentry level entails a smart vetting process—a classification
    task that produces two distinct, sorted streams of email—ham and spam. Gmail's
    sophisticated spam filtering engine filters out spam by a classification process,
    fulfilling, in a proverbial sense, the separation of the wheat from the chaff.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spam can be a pernicious phenomenon in our daily lives, which is intimately
    tied to an increasingly connected world. For example, a binary classification
    is an ongoing deceptive link to apparently innocent looking websites hosting malware. Readers
    can learn why a spam filter can minimize problems spam can cause. These are summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Unethical companies harvest email addresses from the web and send out a flood
    of bulk emails to people. A Gmail user, say, `gmailUser@gmail.com`, is enticed
    to click on an innocent-looking website masquerading as a popular website people
    commonly recognize as reputable. One kind of vile intention is to trap the user
    into giving up personal information on entering a supposedly popular reputable
    website.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another spam email like the one sent out by operators of a shady website, `frz7yblahblah.viral.sparedays.net`,
    is preying on people's predilection for making easy money. For example, the innocent
    looking link contains a deceptive link to some shady website hosting malware,
    a rootkit virus for example. A rootkit is very hard to remove. It is a virus that
    embeds itself into your OS kernel. It can be so elusive and potentially destructive
    that a remote hacker can gain control of your system, and before you know it,
    your network may come to a grinding halt. Several man hours will be lost and,
    if you are a company, revenue will be lost as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a screenshot of a spam email sitting in Gmail''s Spam folder,
    indicative of phishing. **Phishing** refers to deliberate attempts at maliciously
    gaining fraudulent access to personal information by deception, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a636ff4b-2a50-472b-91e3-d8e32d41d807.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a spam email
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore a handful of topics that are relevant to
    the development of the spam classification pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Relevant background topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following topics are reviewed in this section prior to developing the spam
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Individual feature importance in relation to another feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency-inverse document frequency (TF-IDF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashing trick
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop word removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will talk more about each topic.
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multidimensional data is data bearing more than one feature. We have dealt with
    many features in the earlier chapters. That said, let's restate this with an example,
    explaining what a feature means and why features are such a big deal.
  prefs: []
  type: TYPE_NORMAL
- en: Features and their importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each feature in a multidimensional dataset is a contributing factor in arriving
    at a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: A prediction is to be made on a new sample; for example, a new breast cancer
    mass sample belonging to a certain individual
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each contributing factor has a certain feature importance number or feature
    weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some features are more important than others in contributing to the final prediction.
    In other words, a (final) prediction is made on what category a new sample belongs
    to. For example, in the breast cancer dataset in [Chapter 2](a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml),
    *Build a Breast Cancer Prognosis Pipeline with Spark and Scala*, the Random Forest
    algorithm can be used to estimate feature importance. In the following list the
    top-most features have the highest weight; the feature at the bottom of the list
    has the lowest weight (in order of decreasing importance):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniformity_of_Cell_Size**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Uniformity_of_Cell_Shape**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bare_Nuclei**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bland_Chromatin**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Single_Epithelial_Cell_Size**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Normal_Nucleoli**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Clump_Thickness**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Marginal_Adhesion**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mitosis**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means that the first feature has the most impact on the final predicted
    outcome, second feature has the second biggest impact, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We have just covered features, feature importance, weight, and so on. This exercise
    has laid the groundwork for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will look at classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classification task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Classification** implies a categorization action, a task involving categories.
    A classification task then typically denotes a supervised learning technique that
    lets us categorize a new sample previously unseen (such as an Iris flower, whose
    species we do not know yet). By categorizing, we allude to the fact that the classification task
    is tagging an unseen sample with one of the predicted labels in the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d01dd528-4984-4ba1-ad03-116f94e54ccd.png)'
  prefs: []
  type: TYPE_IMG
- en: What is classification?
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on the to next question, we will draw your attention to the term
    **training dataset**. In the next topic on classification outcomes, we discuss
    classification outcomes as binary or categorical and explain supporting concepts
    like **target** variable.
  prefs: []
  type: TYPE_NORMAL
- en: Classification outcomes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until [Chapter 3](dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml), *Stock Price
    Predictions**,* we worked with ML problems related to classification as a supervised
    learning technique. The classification tasks were data-centered (data as in samples, observations,
    or measurements) for which you already know the target answer. That brings us
    to the term **t****arget variable**. This is another commonly-used alternative
    name for the **response variable**, a term from statistics. A target variable,
    in the ML context, is the variable that is typically the output or outcome. For
    example, it could be a binary outcome variable with only two classification outcomes—`0`
    or `1`. With that point made, we know that data for which the target answer is
    readily known or predetermined is called **labeled data**.
  prefs: []
  type: TYPE_NORMAL
- en: The classification task developed in this chapter is all about supervised learning,
    where an algorithm is teaching itself to learn from the labeled samples that we
    provide. A notable example from a previous chapter is the breast cancer dataset,
    which is also a supervised learning classification task. In that chapter, we classified
    breast cancer samples in two categories—benign and malignant. These are the two
    classification outcomes, outcome values that we can use tolabel or tag so-called
    training data that an algorithm will be trained on. On the other hand, unlabeled
    data is a new breast cancer sample data waiting to be diagnosed. More pertinently,
    a new incoming corpus possibly containing both ham and spam contains unlabeled
    data. Based on labeled samples from the training set, you could try to classify
    the unlabeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: Two possible classification outcomes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spam filtering is a binary classification task, an ML task that generates predicted
    values that only contain one of two possible classification outcomes. In this
    chapter, we will set out to build a spam classifier. Labels in the spam classification
    set belong to a finite set consisting of text from two types of emails, spam and
    ham. The binary classification task then becomes one of predicting the (output)
    label from previously unseen data. Deciding whether an email is spam or not, therefore,
    become a binary classification problem. By convention, we assign the ham mutually
    exclusive state a value of `1`, and `0` for the other state, spam. In the next
    section, we will formulate the spam classification problem at hand. This will
    give us an overview of the project as well.
  prefs: []
  type: TYPE_NORMAL
- en: Project overview – problem formulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the stated goal is to build a spam classifier, one that is
    capable of distinguishing spam terms in email messages that are mixed in with
    regular or expected email content as well. It is important to know that spam messages
    are email messages that are sent out to multiple recipients with the same content,
    as opposed to regular messages. We start with two email datasets, one that represents
    ham and one that represents spam. After stages of preprocessing, we fit the model
    on a training set, say 70% of the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This application is a typical spam filtering application in the sense that it
    works on text. We then put algorithms to work that help the ML process detect
    words, phrases, and terms most likely found in spam emails. Next, will go over
    the ML workflow at a high level in relation to spam filtering.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ML workflow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will be developing a pipeline that will use dataframes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataframe contains a `predictions` column and another column containing preprocessed
    text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classification process involves transformation operations—one `DataFrame`
    is transformed into another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our pipeline runs a series of stages, involving TF-IDF, a hashing trick, stop
    word removal, and Naive Bayes algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, the spam filtering or classification problem is a supervised learning
    task, where we supply labeled data to the pipeline. A natural language processing
    step in this task entails labeled feature data being converted into a bag of feature
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can lay out the steps needed to build the spam classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48f4e0eb-d68e-4f20-a159-d6d7fbd1ffcf.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Steps needed to build the spam classifier
  prefs: []
  type: TYPE_NORMAL
- en: The steps described in the preceding list are useful and help us come up with
    an outline of the spam classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents a formulation of the spam classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de19765f-3762-4f3b-a671-a0c335125a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Spam classifier outline
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a refresher on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Punctuation marks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We want to eliminate two categories of text in our spam and ham datasets. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Punctuation marks may be characterized in three categories:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminal points or marks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dashes and hyphens
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pausing points, or marks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a representative list of punctuation marks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1993867-b611-4904-ac75-f5d2031318da.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Punctuation marks
  prefs: []
  type: TYPE_NORMAL
- en: We covered a list of commonly encountered punctuation marks in a text corpus
    that we want to be removed from our spam and ham datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to remove stop words—words that are common. Our spam classifier
    will remove these in the preliminary preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representative list of stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb476fb0-89b2-4270-a6be-6f61b5631b7e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A representative list of stop words
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a representative list of regular expressions to help with
    punctuation mark removal. A spam corpus can be daunting. Regular expressions can
    get as complex as can be in order to cope with spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8d2580b-95fb-479a-9d78-e0c68ab15dc0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some relevant regular expressions
  prefs: []
  type: TYPE_NORMAL
- en: In the *Getting started* section, we will get started implementing the project.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to get started, download the dataset from the `ModernScalaProjects_Code`
    folder and drop it into the root folder of your project.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up prerequisite software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may use your existing software setup from previous chapters. Apache Log4j
    2 Scala API is the notable exception. This is a Scala wrapper over Log4j 2, which
    is an upgraded `Logger` implementation of Log4j version 1.x (the version provided
    by Spark).
  prefs: []
  type: TYPE_NORMAL
- en: Simply override the existing Log4j from Spark (version 1.6) with Log4j 2 Scala
    by adding in appropriate entries in the `build.sbt` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists two choices of prerequisite software:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ada1086f-6195-4c7f-9d64-c10b4004edc7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Implementation infrastructure
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset from the `ModernScalaProjects_Code` folder and drop it
    into the root folder of your project.
  prefs: []
  type: TYPE_NORMAL
- en: Spam classification pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important development objective of this chapter is to perform spam
    classification tasks with the following algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop word remover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inverse document frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashing trick transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The practical goal of our spam classification task is this: Given a new incoming
    document, say, a collection of random emails from either Inbox or Spam, the classifier
    must be able to identify spam in the corpus. After all, this is the basis of an
    effective classifier. The real-world benefit behind developing this classifier
    to give our readers experience of developing their own spam filters. After learning
    how to put together the classifier, we will develop it.'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation steps are in the next section. This takes us straight into
    the development of Scala code in a Spark environment. Given that Spark allows
    us to write powerful distributed ML programs such as pipelines, that is exactly
    what we will set out to do. We will start by understanding the individual implementation
    steps required to reach our goal.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The spam detection (or classification) pipeline involves five stages of implementation,
    grouped by typical ML steps that are needed. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracting features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the spam classifier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we will set up a Scala project in IntelliJ.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – setting up your project folder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is what the project looks like in IntelliJ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5993f097-c76c-4a0d-8e3c-b66c52adf47b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Project outline in IntelliJ
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will upgrade the `build.sbt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – upgrading your build.sbt file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the upgraded `build.sbt` file. What is new here? Remember, earlier,
    we talked about a new `Logging` library. Those new entries you see in the following
    screenshot are the new dependencies you need to move from Log4j 1.6 to the new
    Scala wrapper for Log4j 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36d57b4b-0780-4597-94ac-e28ad944b4ef.jpg)'
  prefs: []
  type: TYPE_IMG
- en: New entries in the build.sbt file
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will start with the Scala code, starting with a trait.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – creating a trait called SpamWrapper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In IntelliJ, using File | New | Scala class, create an empty Scala trait called
    `SpamWrapper` in a file called `SpamWrapper.scala`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first. At the top of the file, we will set up the following imports
    for implementing classes to take advantage of this trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession`—the entry point to programming with Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The appropriate Log4J library imports, so that we can dial down logging messages
    from Spark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the minimum imports. Next, create an empty `trait`. The following
    is an updated `trait`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the `SpamWrapper` trait, create a `SparkSession` instance called `session`.
    At this point, here is a refresher on Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: We need a `SparkSession` object instance to be the entry point to programming
    with Spark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not need a separate `SparkContext`. This is provided by `SparkSession`.
    The underlying context is available to us easily as `session.sparkContext`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create a `SparkSession` object instance or get an existing `SparkSession`,
    a builder pattern is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SparkSession` instance is available for the entire time frame of a Spark
    job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the updated `SpamWrapper` trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/162c02a7-0964-4834-9f57-13d71a19cb94.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SpamWrapper trait with SparkSession value
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we create a `val` called `session` that our pipeline class will use.
    Of course, this will be the entry point to programming with Spark to build this
    spam classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – describing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the dataset from the `ModernScalaProjects_Code` folder. It consists
    of two text files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inbox.txt`: Ham emails (I created this file from my Gmail Inbox folder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`junk.txt`: Spam emails (I created this out of spam from my Gmail Spam folder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop these files into the root of your project folder. In the next section,
    we will describe the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the SpamHam dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we present the actual dataset, here are a few real-world spam samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fbb4607-4350-4b47-b611-78f1cbf2b1ed.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spam email with phishing example
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of regular or wanted mail, also known as ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2601a3a7-7c2f-4190-95bd-14a4981fa7f3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A perfectly normal email from Lightbend
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a glimpse into the actual dataset used in our spam-ham classification
    task. There are two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inbox.txt`: A ham dataset compiled from a small collection of regular emails
    from my Inbox folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`junk.txt`: A spam dataset compiled from a small collection of junk email from
    my Spam/Junk folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the regular dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d01fdfb1-2a11-47ea-a21f-178684369570.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A portion of the regular email dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the spam dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7942ce19-5c5f-43d5-aa06-f9ee35a68a72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A portion of the spam dataset
  prefs: []
  type: TYPE_NORMAL
- en: The preceding email wants you to confirm here. This is an attempt at phishing.
    This completes our description of our datasets. In the next step, we will proceed
    with data preprocessing. We need a new Scala object called `SpamClassifierPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – creating a new spam classifier class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create a new Scala file called `SpamClassifierPipeline.scala`. First
    of all, we need the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a622b9e-8f5f-4813-b51f-5c4dd991df3d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Required imports
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the imports have been created, let''s create an empty `SpamClassifierPipeline`
    object in the same package as the `SpamWrapper` trait, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A prototype spam classifier is ready. We need to create code in it to do such
    things as preprocessing the data and, of course, much more. In the next step,
    we will list the necessary preprocessing steps to take up.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – listing the data preprocessing steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a purpose to preprocessing. In most data analytics tasks, the question
    that begs to be asked is—is our data necessarily usable? The answer lies in the
    fact that most real-world datasets need preprocessing, a massaging step meant
    to give data a new usable form.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the spam and ham datasets, we identify two important preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing punctuation marks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Processing punctuation using regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we will write Scala code for two regular expressions, expressions
    that are fairly simple and only address a small subset of spam. However, it's
    a start.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will load our datasets into Spark. Naturally, we want a
    ham dataframe and a spam dataframe. We will take up the task of creating a ham
    dataframe first. Both our ham and spam datasets are ready for preprocessing. That
    brings us to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – regex to remove punctuation marks and whitespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a regular expression for our immediate needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`raw` is a method in the string interpolation `StringContext` class of the
    standard Scala library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our corpus is likely to contain trailing and leading whitespaces which are
    sometimes ill-formatted. For example, we will run into whitespaces where lines
    are indented too much. To get rid of whitespaces, we will use regular expressions.
    This will work with the anchors, the hat `^`, and the `$` sign to extract the
    text without whitespaces. The updated regular expression now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We just created a regular expression, `regex2`, a whitespace, and punctuation
    remover. Very soon, we will need this `regex2`. In the next section, we will create
    a new ham dataframe after applying the first of a few essential preprocessing
    steps—removing punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: Step 8 – creating a ham dataframe with punctuation removed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will have the regex work on each row of our ham **resilient distributed
    dataset** (**RDD**) and then use the `replaceAll` method. The underlying regex
    engine will use the regex to conduct a search and match on our ham corpus to come
    up with matched occurrences, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `replaceAll` method kicked in and replaced all occurrences of whitespaces
    and punctuation. In the next step, we will convert this RDD into a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'We created a new ham RDD with leading and trailing whitespaces, and with punctuation
    removed. Let''s display this new ham dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We created a new ham dataframe. We need to transform this dataframe by assigning
    a `label` of `0.0` to each ham sentence. We shall do that in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, we will create a case called `LabeledHamSpam` to model a sentence
    as a feature with a `Double` `label`. Next, create a new ham RDD that has exactly
    four partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a labeled ham dataframe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will repartition our ham datafame and apply a `transform` operation to each
    `"Ham sentence"` in the ham dataframe, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We repartitioned and created a new RDD that is structured as a row of ham sentences
    labeled with `0.0`. Now, display the first `10` rows of the new ham RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we assigned `0.0` for all ham sentences. It is time to create the spam
    RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Repeat the same set of preprocessing steps for the spam dataset as well.
  prefs: []
  type: TYPE_NORMAL
- en: Step 9 – creating a spam dataframe devoid of punctuation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same `LabeledHamSpam` case class to assign a `Double` `value`
    of `1.0` for spam sentences, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, we want a combined dataframe containing both spam and ham
    dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: Step 10 – joining the spam and ham datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we will use the `++` method to join both dataframes in a `Union` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next section, let''s create a dataframe, with two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: A row containing `feature sentences` with punctuation removed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A predetermined `label` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check for the following code snippet for better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We created the new dataframe. Let''s display this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s run the following optional checks:'
  prefs: []
  type: TYPE_NORMAL
- en: The schema of the dataframe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns present in the dataframe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is how we will print out the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'They look good! Let''s read off the `columns` now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Up until now, we have created a dataframe that was punctuation-free, but not
    necessarily free of rows that contain null values. Therefore, in order to drop
    any rows containing null values, we will need to import the `DataFrameNaFunctions` class,
    that is, if you have not already imported it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to drop nulls from the punctuation-free dataframe, there is a column
    called `punctFreeSentences`. We will invoke the `drop()` method as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The call to the `drop` method in the preceding code causes any rows of sentences
    that contain null values to be dropped. If you so wish, display the first 20 rows
    of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the dataframe. The following code will help you do just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At this point, a good next step relates to tokenizing punctuation-free rows,
    which also feature what we want to be tokenized. Tokenizing the current dataframe
    is the focus of the next section. Tokenizing brings us one step closer to the
    next preprocessing step—removing stop words.
  prefs: []
  type: TYPE_NORMAL
- en: Step 11 – tokenizing our features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tokenizing is simply an operation executed by an algorithm. It results in the
    tokenization of each row. All the following terms define tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The appropriate term appears to be the second term in the preceding list. For
    each row in the current dataframe, a tokenizer splits a `feature` row into its
    constituent tokens by splitting it along separating whitespaces. Each resulting
    Spark supplies two tokenizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tokenizer` from the package `org.apache.spark.ml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RegexTokenizer` from the same package'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both tokenizers are transformers. A transformer in Spark is an algorithm that
    accepts an input column as a (hyper) parameter and spits out a new `DataFrame`
    with a transformed output column, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the `transform` method on `mailTokenizer` will give us a newly transformed
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataframe, `tokenizedBagOfWordsDataFrame`, is a tokenized non-null
    bag of lowercased words. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The important thing to notice here is that a row in the transformed column, `mailFeatureWords`,
    resembles an array of words. Readers will not fail to notice that there are words
    in `mailFeatureWords` known as stop words. These are words that do not significantly
    contribute to our spam classification task. These words can be safely eliminated
    by Spark's `StopWordRemover` algorithm. In the next step, we will see how to put
    `StopWordRemover` to work.
  prefs: []
  type: TYPE_NORMAL
- en: Step 12 – removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, make sure you have the `StopWordRemover` import in the `SpamClassifierPipeline`
    class. Next, we will create an instance of `StopWordRemover` and pass into it
    a (hyper) parameter column, `mailFeatureWords`. We want an output column that
    is devoid of stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like with `mailTokenizer`, we call the `transform` method to get a new
    `noStopWordsDataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataframe, a tokenized, non-null bag of lowercase words with
    no stop words looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we will do a second transformation on our current dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The exploded, tokenized non-null bag of lowercase words with no stop words
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This completes data preprocessing. This sets the stage for feature extraction,
    an extremely important ML step.
  prefs: []
  type: TYPE_NORMAL
- en: Step 13 – feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we will extract the features of this dataset. We will do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating feature vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features entails converting the text into bigrams of characters using
    an n-gram model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These bigrams of characters will be hashed to a length `10000` feature vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final feature vector will be passed into Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will `transform` the featured version of the `noStopWordsDataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'With a hash-featured and tokenized non-null bag of lowercased words with no
    stop words, this `DataFrame` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85365a2b-f55c-4efd-a102-c14d0f542532.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dataframe with hash-featured and tokenized non-null bag of lowercase words
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we are ready to create training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Step 14 – creating training and test datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This step is important because we are going to create a model that we want
    to train with a training set. One way to create a training set is to partition
    the current dataframe and assign 80% of it to a new training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s retrieve the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The testing dataset follows. Here is how we will create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to go one step further. A modified version of the training set that
    has the following columns removed is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mailFeatureWords`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`noStopWordsMailFeatures`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mailFeatureHashes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the new training set, after doing `drop` on the preceding columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The invocation of the `show()` method results in the following display of the
    new training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/711f0e12-e97a-4639-ad3f-2ba687a881a5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Training dataframe
  prefs: []
  type: TYPE_NORMAL
- en: 'The next important step right before training (fitting) the model is what is
    known as **inverse document frequency** (**IDF**). Spark provides an estimator
    called IDF that will compute the IDF for us. The IDF is an algorithm that will
    train (fit) models on our current dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will pass in our `featurizedDF` dataframe into the `fit` method on the
    IDF algorithm. This will produce our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is a normalizing step. A `normalizer` normalizes the scales for
    the different `features` so that different-sized articles are not weighted differently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the Naive Bayes algorithm now. Initialize it and pass into it the
    hyperparameters it needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to create the pipeline and set up all the stages in it. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StopWordRemover`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HashingTF`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mailIDF`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`naiveBayes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code snippet sets up the stages as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the pipeline to the training documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Make predictions on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will display generated raw predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that they are not two tables. It is one table broken in two for visual
    clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/582c3cee-80bb-4450-9080-93be48bd420e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Raw predictions table
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the final step, where we only want to show the relevant columns in
    the predictions table. The following line of code will drop those columns that
    are not required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the final `predictions` table. We only need the first and the last
    column. The `label` column is the predictions the model generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'It displays predictions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb85d22-fda0-4bcd-8228-3ecaa8009df4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'We are done, so `stop` the `session`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we created a spam classifier. We started with two datasets,
    one representing ham and the other, spam. We combined both datasets into one combined
    corpus that we put through a set of preprocessing steps, as mentioned in the *Implementation
    steps* section.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build on some of the techniques learned so far
    to create a fraud detection ML application.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are 10 questions that will help to reinforce all of the learning material
    presented in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the spam classification task a binary classification task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What was the significance of the hashing trick in the spam classification task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is hashing collision and how is it minimized?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do we mean by inverse document frequency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are stop words and why do they matter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role played by the Naive Bayes algorithm in spam classification?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you use the `HashingTF` class in Spark to implement the hashing trick
    in your spam classification process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by the vectorization of features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a better algorithm that you can think of to implement the spam classification
    process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the benefits of spam filtering, and why do they matter in business
    terms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following paper is a comprehensive work that deserves reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.sciencedirect.com/science/article/pii/S2405882316300412](https://www.sciencedirect.com/science/article/pii/S2405882316300412)'
  prefs: []
  type: TYPE_NORMAL
