<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Troubleshooting Performance and Reliability Issues"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Troubleshooting Performance and Reliability Issues</h1></div></div></div><p>This chapter focuses on troubleshooting common performance and reliability issues for Elasticsearch using case studies with real-world examples.</p><p>This chapter will help answer the following questions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How do I configure my Elasticsearch cluster to optimize performance?</li><li class="listitem" style="list-style-type: disc">How do I prevent <code class="literal">OutOfMemoryError</code> exceptions?</li><li class="listitem" style="list-style-type: disc">How does my data-indexing strategy affect cluster resources?</li><li class="listitem" style="list-style-type: disc">Why are my queries running slow?</li><li class="listitem" style="list-style-type: disc">How can I keep query performance strong when indexing a large amount of data?</li><li class="listitem" style="list-style-type: disc">How can I configure indices to use less disk space?</li></ul></div><div class="section" title="System configuration"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/>System configuration</h1></div></div></div><p>Elasticsearch<a id="id244" class="indexterm"/> configuration may lead to a number of performance and reliability issues, as mentioned in <a class="link" href="ch02.html" title="Chapter 2. Installation and the Requirements for Elasticsearch">Chapter 2</a>, <span class="emphasis"><em>Installation and the Requirements for Elasticsearch</em></span>. A quick reminder that the most important configuration changes to make to your cluster are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ensuring that the Elasticsearch heap size (<code class="literal">ES_HEAP</code>) is set to 1/2 of available system memory, but does not exceed 31 GB. Set this value in <code class="literal">/etc/defaults/elasticsearch</code></li><li class="listitem" style="list-style-type: disc">Disabling memory swapping</li><li class="listitem" style="list-style-type: disc">Locking the Elasticsearch address space into memory by setting <code class="literal">bootstrap.mlockall: true</code> in <code class="literal">elasticsearch.yml</code></li></ul></div><p>Refer to <a class="link" href="ch02.html" title="Chapter 2. Installation and the Requirements for Elasticsearch">Chapter 2</a>, <span class="emphasis"><em>Installation and the Requirements for Elasticsearch</em></span>, for more detailed<a id="id245" class="indexterm"/> instructions on how to set these values.</p></div></div>
<div class="section" title="The fielddata cache"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec35"/>The fielddata cache</h1></div></div></div><p>A poorly configured Elasticsearch fielddata cache is often the reason for <code class="literal">OutOfMemoryError</code> exceptions.</p><p>When <a id="id246" class="indexterm"/>running a <code class="literal">sort</code> or <code class="literal">aggregation</code> (or <code class="literal">facet</code>) query, Elasticsearch fills the cache with all distinct field values from the query. This allows similar, subsequent queries to execute more quickly. However, Elasticsearch doesn't put an upper bound on the cache size by default; therefore, the data is not automatically evicted. If the cache causes the total JVM memory to fill up beyond the <code class="literal">ES_HEAP</code> size, the node will throw an <code class="literal">OutOfMemoryError</code> exception and will require an Elasticsearch restart.</p><p>To limit the<a id="id247" class="indexterm"/> fielddata cache size, set the <code class="literal">indices.fielddata.cache.size</code> value:</p><div class="informalexample"><pre class="programlisting">indices.fielddata.cache.size: 30%</pre></div><p>This will limit the fielddata cache size to <code class="literal">30%</code> of the available JVM heap space.</p><p>You can set this value to a fixed value as well. For example, setting it to <code class="literal">10gb</code> will limit the cache size to no more than 10 gigabytes. The value that you choose will depend on the cluster and use case, but if you see an <code class="literal">OutOfMemoryError</code> caused by the fielddata cache overflowing, it's a good idea to set this field. The downside to limiting the fielddata cache is that it may affect query performance if a query needs to repopulate evicted fielddata cache items when the cache fills up.</p><p>If you see <code class="literal">OutOfMemoryError</code> logged to <code class="literal">/var/log/elasticsearch/</code>, you can check whether<a id="id248" class="indexterm"/> the fielddata cache is the problem by checking in Bigdesk or Marvel:</p><div class="mediaobject"><img src="graphics/B03798_06_01.jpg" alt="The fielddata cache"/><div class="caption"><p>The fielddata cache in Bigdesk</p></div></div><p>The <a id="id249" class="indexterm"/>fielddata cache from the Marvel Kibana dashboard looks like this:</p><div class="mediaobject"><img src="graphics/B03798_06_02.jpg" alt="The fielddata cache"/><div class="caption"><p>The fielddata cache in Marvel</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>Do not change the <code class="literal">indices.fielddata.cache.expire</code> setting. This is a legacy setting to expire old cache values, and it does not provide any increase in performance. Elasticsearch developers stated that it will be deprecated in a future release.</p></div></div><p>You can also reduce the fielddata cache footprint by optimizing queries that use the cache.</p><p>For example, in<a id="id250" class="indexterm"/> our Twitter data, we have a <code class="literal">timestamp_ms</code> field, which stores the tweet's timestamp at millisecond precision. Because there are <code class="literal">86,400,000</code> milliseconds in a day, if we collected <code class="literal">5,000,000</code> Tweets in 24 hours, it's likely that the majority of these tweets will have a unique timestamp. If we run a query that sorts on this field, it will fill up the fielddata cache with as many as <code class="literal">5,000,000</code> distinct timestamps. This will quickly fill up the cache.</p><p>A more functional approach would be to store the timestamp field at either second or minute precision. Using second precision, the fielddata cache will be reduced from holding <code class="literal">5,000,000</code> unique timestamps to approximately <code class="literal">86,400</code> timestamps. Using minute precision will reduce it to only <code class="literal">1,440</code> unique timestamps.</p><p>Even after limiting the fielddata cache size to a fixed amount, you may still face <code class="literal">OutOfMemoryError</code> exceptions related to the field cache. This may be a result of a single query loading the fielddata cache with more data than it has been allocated.</p><p>This may happen if, for example, the fielddata cache is set to 2 GB, but we run a single query that tries to load 2.5 GB of data into the cache. This issue can be fixed by editing the fielddata circuit breaker in <code class="literal">elasticsearch.yml</code>.</p><p>The fielddata circuit breaker is set by default to <code class="literal">60%</code> of the total JVM heap size:</p><div class="informalexample"><pre class="programlisting">indices.breaker.fielddata.limit: 60%</pre></div><p>This way, if a single query's fielddata is more than <code class="literal">60%</code> of the heap, the circuit breaker will trip and cause the query to throw an exception rather than causing an <code class="literal">OutOfMemoryError</code>. Using a lower percentage than the default <code class="literal">60%</code> may help in solving <code class="literal">OutOfMemoryError</code> exceptions even when the fielddata cache is limited in size.</p></div>
<div class="section" title="Analyzing queries"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Analyzing queries</h1></div></div></div><p>Analyzing slow<a id="id251" class="indexterm"/> queries and improving their performance can be very challenging. This section examines how to look for the root cause of poor query performance, and it offers some different approaches to finding a solution.</p><div class="section" title="Slow log"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec58"/>Slow log</h2></div></div></div><p>If you notice<a id="id252" class="indexterm"/> poor query performance, start with the slow log. To enable the slow log, edit <code class="literal">elasticsearch.yml</code> and add these configuration options to all nodes on the cluster:</p><div class="informalexample"><pre class="programlisting">index.search.slowlog.threshold.query.warn: 8s
index.search.slowlog.threshold.query.info: 4s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.trace: 500ms

index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 750ms
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.trace: 250ms

index.indexing.slowlog.threshold.index.warn: 8s 
index.indexing.slowlog.threshold.index.info: 4s 
index.indexing.slowlog.threshold.index.debug: 2s 
index.indexing.slowlog.threshold.index.trace: 500ms
index.indexing.slowlog.level: info
index.indexing.slowlog.source: 5000</pre></div><p>After updating <code class="literal">elasticsearch.yml</code> on all nodes, restart the cluster.</p><p>This configuration enables the slow log for three operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Query operations</strong></span>: This is when Elasticsearch is performing the actual search for documents matching the query</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Fetch operations</strong></span>: This is when Elasticsearch fetches relevant documents from the index after finding documents of interest</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Index operations</strong></span>: This is when indexing new documents in Elasticsearch</li></ul></div><p>We've also set a threshold level for each point: <code class="literal">warn</code>, <code class="literal">info</code>, <code class="literal">debug</code>, and <code class="literal">trace</code>. These levels identify the point at which Elasticsearch will write to the slow log. For example, if a query takes six seconds, based on our preceding configuration, the query will be logged at an <code class="literal">info</code> level. These levels make it possible to search for queries of a specific threshold.</p><p>Here's an example of searching the slow log for all queries that took longer than eight seconds, which were logged at the <code class="literal">warn</code> level:</p><div class="informalexample"><pre class="programlisting">grep "\[WARN \]"  /var/log/elasticsearch/my_elasticsearch_cluster_*_slowlog.log*</pre></div><div class="mediaobject"><img src="graphics/B03798_06_03.jpg" alt="Slow log"/><div class="caption"><p>Elasticsearch slow log for actions that took longer than eight seconds</p></div></div><p>The next <a id="id253" class="indexterm"/>section covers some additional approaches to improve query performance.</p></div></div>
<div class="section" title="Improving query performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec37"/>Improving query performance</h1></div></div></div><p>This section highlights<a id="id254" class="indexterm"/> common reasons behind certain slow queries on Elasticsearch, and offers instruction to improve performance.</p><div class="section" title="High-cardinality fields"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec59"/>High-cardinality fields</h2></div></div></div><p>As previously <a id="id255" class="indexterm"/>mentioned, running aggregation or sorts against high-cardinality fields (for example, dates precise to the millisecond) can fill up the fielddata cache which leads to <code class="literal">OutOfMemoryError</code> exceptions. However, even without these errors, running aggregations and sorts can be detrimental to <a id="id256" class="indexterm"/>performance. When it comes to dates, it's generally a good idea to store and use less precise dates in order to speed up query execution time.</p></div><div class="section" title="Querying smaller indices"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec60"/>Querying smaller indices</h2></div></div></div><p>As Elasticsearch indices grow larger, query performance will suffer. Another way to improve<a id="id257" class="indexterm"/> performance is to run queries against small indices. You can do this by storing our data in several smaller indices instead of one large one.</p><p>For example, with Twitter data, you can change the ingestion process to create a new index every day to store tweets. This way, we only query a subset of the total indices when running time-bounded queries.</p><p>Index templates are helpful in this case because they automatically apply a data mapping to new indices that follow a certain naming convention.</p><p>Let's create a new index template for our daily Twitter indices using the <code class="literal">twitter-YYmmdd</code> naming convention. Using this template, the <code class="literal">twitter-20160101</code> index will hold all tweets from January 1, 2016. Create this template with the following <code class="literal">cur</code>
<code class="literal">l</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/_template/templ</strong></span>
<span class="strong"><strong>ate_1 -d '</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    "template" : "twitter-*",</strong></span>
<span class="strong"><strong>    "settings" : {</strong></span>
<span class="strong"><strong>        "number_of_shards" : 5</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "mappings" : {</strong></span>
<span class="strong"><strong>        "twitter" : {</strong></span>
<span class="strong"><strong>            "status" : { </strong></span>
<span class="strong"><strong>                ...</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>            ...</strong></span>
<span class="strong"><strong>        }</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>Note the use of the <code class="literal">*</code> asterisk wildcard in the <code class="literal">twitter-*</code> template name. This wildcard that matches 0 or more characters, so it will match index names, such as <code class="literal">twitter-20160101</code>.</p></div></div><p>We can also create an index alias that allows us to query many or all of the indices at once.</p><p>The following <a id="id258" class="indexterm"/>example creates an alias using the <code class="literal">*</code> wildcard to query all available Twitter data:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST elasticsearch-node-01:9200/_aliases -d '{</strong></span>
<span class="strong"><strong>    "actions" : [</strong></span>
<span class="strong"><strong>        { "add" : { "index" : "twitter-*", "alias" : "all_twitter" } }</strong></span>
<span class="strong"><strong>    ]</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Play around with different index sizes to find the best fit, depending on your data and setup. It's important to test how they affect your performance before committing to a particular index size because changing the indexing strategy later will involve re-indexing all of your data.</p><p>If you have a five-node cluster and collect 10,000 records per day, it makes sense to create new indices monthly versus daily to keep the number of indices down and to ensure that each individual index isn't too small. However, it's important to test all assumptions before committing to an indexing strategy. Use a tool such as Logstash and Kibana to monitor average query performance using different index sizes before making this decision.</p></div><div class="section" title="Cold indices"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec61"/>Cold indices</h2></div></div></div><p>Sometimes, an Elasticsearch query is slow the first few times it runs, but it speeds up considerably afterwards. The lag occurs because the index is "cold" and the Elasticsearch caches are <a id="id259" class="indexterm"/>not populated with relevant data. After running the <a id="id260" class="indexterm"/>query a few times, Elasticsearch fills up the fielddata cache and other caches based on the query criteria. Subsequent queries with similar criteria will take advantage of these cached values and run faster as a result.</p><p>Elasticsearch "warmers" and "eager fielddata loading" solve the problem of cold indices by ensuring that the first time a user runs a query, required data for this query is already loaded in memory.</p><p>Indices can be cold for a variety of reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">New data is indexed</li><li class="listitem" style="list-style-type: disc">Automatic shard balancing and movement</li><li class="listitem" style="list-style-type: disc">An Elasticsearch node restarted</li><li class="listitem" style="list-style-type: disc">The cache was manually cleared</li></ul></div><p>To demonstrate the performance gains of a slow aggregation query, use the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST 'http://elasticsearch-node-01:9200/twitter/_cache/clear'</strong></span>

<span class="strong"><strong>curl -XGET 'http://elasticsearch-node-01:9200/twitter/_search' -d '{</strong></span>
<span class="strong"><strong>    "size" : 0,</strong></span>
<span class="strong"><strong>    "query" : {</strong></span>
<span class="strong"><strong>        "match_all" : {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "aggs" : {</strong></span>
<span class="strong"><strong>        "text" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "text"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }  </strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}' | python -m json.tool</strong></span>
</pre></div><p>The results of this are as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "took": 5864</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>If we <a id="id261" class="indexterm"/>run the query a few more times, we'll start to see results like<a id="id262" class="indexterm"/> the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "took": 529</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>This query took <code class="literal">5.8</code> seconds to finish at first, but after a few runs, it only took <code class="literal">0.529</code> seconds to complete. The initial slow query can be avoided and the performance can become more predictable after adding common queries to the Elasticsearch warmer. We'll demonstrate this by clearing the index cache again, then adding our query to the <code class="literal">twitter</code> index with the Elasticsearch Warmers API:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST 'http://elasticsearch-node-01:9200/twitter/_cache/clear'</strong></span>

<span class="strong"><strong>curl -XPUT http://elasticsearch-node-01:9200/twitter/twitter/_warmer/text_agg_warmer?pretty -d '{</strong></span>
<span class="strong"><strong>    "size" : 0,</strong></span>
<span class="strong"><strong>    "query" : {</strong></span>
<span class="strong"><strong>        "match_all" : {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "aggs" : {</strong></span>
<span class="strong"><strong>        "text" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "text"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }  </strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>We can<a id="id263" class="indexterm"/> verify that the warmer query made it into our index by <a id="id264" class="indexterm"/>checking the Kopf <span class="strong"><strong>REGISTERED WARMERS</strong></span> page at <code class="literal">http://elasticsearch-node-01:9200/_plugin/kopf</code> and navigating to <span class="strong"><strong>more</strong></span> | <span class="strong"><strong>warmers</strong></span>.</p><p>This screenshot shows the warmer query on the Kopf warmers page:</p><div class="mediaobject"><img src="graphics/B03798_06_04.jpg" alt="Cold indices"/><div class="caption"><p>Viewing query warmers in Kopf</p></div></div><p>The warmer will take effect after restarting Elasticsearch. Run the query again to see a performance increase:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "took": 418</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>This led to <a id="id265" class="indexterm"/>more than a 10x speedup, from <code class="literal">5.8</code> seconds to <code class="literal">0.41</code> seconds. We saw a similar increase after manually running the query a few times to <a id="id266" class="indexterm"/>populate the fielddata cache with data from the <code class="literal">text</code> field.</p><p>We can also enable eager fielddata loading for particular Elasticsearch fields:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT http://elasticsearch-node-01:9200/twitter/_mapping/twitter -d '{</strong></span>
<span class="strong"><strong>    "text": {</strong></span>
<span class="strong"><strong>        "type" : "string",</strong></span>
<span class="strong"><strong>        "doc_values" : true,</strong></span>
<span class="strong"><strong>        "fielddata" : {</strong></span>
<span class="strong"><strong>            "loading" : "eager" </strong></span>
<span class="strong"><strong>        }</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>If there are only a few distinct values for our fielddata cache, set the <code class="literal">loading</code> value to <code class="literal">eager_global_ordinals</code> for more memory optimization. After enabling either warming queries or eager fielddata loading, verify that the fielddata (and filter cache, in the case of warming queries) get populated by checking Marvel's node or Index statistics page or Bigdesk's fielddata chart.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>You can <a id="id267" class="indexterm"/>read more about warmers and eager field data loading <a id="id268" class="indexterm"/>at <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-warmers.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-warmers.htmlload-fielddata.html</a> and <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/guide/current/preload-fielddata.html">https://www.elastic.co/guide/en/elasticsearch/guide/current/preload-fielddata.html</a>.</p></div></div></div><div class="section" title="The shard query cache"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec62"/>The shard query cache</h2></div></div></div><p>The <a id="id269" class="indexterm"/>shard query cache saves results for specific<a id="id270" class="indexterm"/> queries. Unlike the fielddata cache where any query that needs fielddata will speed up, with cached queries, we have to run the exact same query more than once to have a cache hit. Additionally, the entire query result is stored with the query cache. This is different from the fielddata cache, in which only part of the query result is stored. This means that the query cache will return results extremely quickly.</p><p>The shard query cache currently only stores hit counts, aggregations, and search suggestions. It does not store actual search results or hits. Moreover, the <code class="literal">search_type=count</code> query parameter is required when running cached queries. This may be updated in a future Elasticsearch release.</p><p>The query <a id="id271" class="indexterm"/>cache defaults to <code class="literal">1%</code> of the JVM heap, but it<a id="id272" class="indexterm"/> can be changed in <code class="literal">elasticsearch.yml</code>:</p><div class="informalexample"><pre class="programlisting">indices.cache.query.size: 2%</pre></div><p>The cache key is the JSON body of a search request. Even if a query is logically identical to a query already in the cache, if there is a difference in whitespace or key order, the cache will store these as two separate entries.</p><p>The shard query cache is disabled by default. To enable it on an existing index, run the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter/_settings?pretty -d'{</strong></span>
<span class="strong"><strong>    "index.cache.query.enable": true </strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Or when creating a new index, add the same parameter to the <code class="literal">settings</code> section:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter -d'</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "settings": {</strong></span>
<span class="strong"><strong>        "index.cache.query.enable": true</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>When using the query cache, you will always receive the same up-to-date query results that you would get when running noncached queries. This is because cache entries are invalidated automatically when new data is loaded into a shard once the shard refreshes.</p><p>Run the text aggregation query again a few times, this time using the query cache:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XGET 'elasticsearch-node-01:9200/twitter/_search?search_type=count&amp;query_cache=true' -d '{</strong></span>
<span class="strong"><strong>    "size" : 2,</strong></span>
<span class="strong"><strong>    "query" : {</strong></span>
<span class="strong"><strong>        "match_all" : {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "aggs" : {</strong></span>
<span class="strong"><strong>        "text" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "text"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }  </strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}' | python -m json.tool</strong></span>
</pre></div><p>After a few<a id="id273" class="indexterm"/> runs of this query, performance<a id="id274" class="indexterm"/> results like this should appear:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "took": 4</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The <code class="literal">4ms</code> response is an improvement from the <code class="literal">418ms</code> response using just the fielddata cache, and a huge improvement from the original <code class="literal">5.8</code> seconds against a cold Elasticsearch index.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>Read<a id="id275" class="indexterm"/> more about the shard query cache at <a class="ulink" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-request-cache.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-request-cache.html</a>.</p></div></div></div><div class="section" title="Script queries"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec63"/>Script queries</h2></div></div></div><p>Script queries <a id="id276" class="indexterm"/>are a powerful way to query an index by<a id="id277" class="indexterm"/> running arbitrary code to manipulate or filter each hit the query comes across. However, they are also very costly, and they can hurt performance in large indices.</p><p>Whenever possible, it is best to avoid using scripts in Elasticsearch queries that need to return in a timely fashion. If you find yourself using them, try to think of ways to restructure your data to make them no longer necessary.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>If you do use scripts in your application, make sure that you access source document fields with <code class="literal">doc["text"]</code> instead of <code class="literal">_source.text</code>; the latter will access the record on disk, while the former accesses it from memory.</p></div></div></div><div class="section" title="Testing meticulously"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec64"/>Testing meticulously</h2></div></div></div><p>It's important to <a id="id278" class="indexterm"/>meticulously test each optimization strategy individually to see which is most effective. If you experience a slow query, try to recreate the problem on a smaller scale and test different optimizations until you find one that works. Make sure that you only test one change at a time in configuration or in query parameters. Also, run testing scripts for a long enough time period to account for normal deviation in performance due to garbage collection, cache evictions, and so on.</p><p>This approach to <a id="id279" class="indexterm"/>testing may feel tedious, but it will ultimately provide greater insight into the cluster, and it will help avoid making unnecessary changes to the system in the long run.</p></div></div>
<div class="section" title="System and data architecting"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec38"/>System and data architecting</h1></div></div></div><p>This section covers<a id="id280" class="indexterm"/> strategies to improve overall system performance, data indexing performance, and to maximize storage space.</p><div class="section" title="Hot-Warm architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec65"/>Hot-Warm architecture</h2></div></div></div><p>For<a id="id281" class="indexterm"/> time-series data, including Twitter and other social media data as well as data from Logstash, Elastic.co recommends setting up what they have dubbed a <span class="strong"><strong>Hot-Warm</strong></span> architecture. This setup puts nodes into three groups.</p><div class="section" title="Master nodes"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec11"/>Master nodes</h3></div></div></div><p>Ideally, dedicate <a id="id282" class="indexterm"/>three nodes as master nodes that do<a id="id283" class="indexterm"/> not store data or fulfill queries. These machines don't need to be very powerful; they just perform cluster management operations.</p></div><div class="section" title="Hot nodes"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec12"/>Hot nodes</h3></div></div></div><p>Hot nodes<a id="id284" class="indexterm"/> hold the most recent data indices. All data writes<a id="id285" class="indexterm"/> are directed at these machines, and they are likely the most-frequently queried nodes. Elastic.co recommends equipping hot nodes with solid state drives (SSDs) for better I/O performance.</p></div><div class="section" title="Warm nodes"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec13"/>Warm nodes</h3></div></div></div><p>In this<a id="id286" class="indexterm"/> architecture, data is not being written to warm <a id="id287" class="indexterm"/>nodes; instead, they contain historical time-based data. For example, if we create a new Twitter index every day, we can move an index from "Hot" to "Warm" after seven days.</p><p>To configure a Hot node, add the following to <code class="literal">elasticsearch.yml</code>:</p><div class="informalexample"><pre class="programlisting">node.box_type: hot</pre></div><p>Likewise, for a Warm node, add the following:</p><div class="informalexample"><pre class="programlisting">node.box_type: warm</pre></div><p>To ensure <a id="id288" class="indexterm"/>that newly-created indices are allocated to the<a id="id289" class="indexterm"/> Hot nodes, configure the index on creation with the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter-2016-03-06</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "settings": {</strong></span>
<span class="strong"><strong>        "index.routing.allocation.require.box_type" : "hot"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>After seven days to move it to the Warm nodes, use the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST elasticsearch-node-01:9200/twitter-2016-03-06/_settings -d '{</strong></span>
<span class="strong"><strong>    "settings": {</strong></span>
<span class="strong"><strong>        "index.routing.allocation.require.box_type" : "warm"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Read more<a id="id290" class="indexterm"/> about the "Hot-Warm" architecture at <a class="ulink" href="https://www.elastic.co/blog/hot-warm-architecture">https://www.elastic.co/blog/hot-warm-architecture</a>.</p></div></div><div class="section" title="Reducing disk size"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec66"/>Reducing disk size</h2></div></div></div><p>This <a id="id291" class="indexterm"/>section covers how to save disk space on your cluster.</p><div class="section" title="Compression"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec14"/>Compression</h3></div></div></div><p>In Elasticsearch 2.0 and higher, you can increase the compression level for an index to reduce its footprint on disk. Unfortunately, this also makes indexing new data slower.</p><p>For use<a id="id292" class="indexterm"/> cases such as the preceding Hot-Warm architecture, it makes sense to increase the compression level on Warm nodes because they are less taxed than the Hot nodes.</p><p>To increase the compression level on an Elasticsearch 2.0+ node, perform the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Close the index.</li><li class="listitem">Configure the <code class="literal">index.codec</code> setting to <code class="literal">best_compression</code>.</li><li class="listitem">Re-open the index.</li></ol></div><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST elasticsearch-node-01:9200/twitter/_close</strong></span>
<span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter/_settings -d '{</strong></span>
<span class="strong"><strong>    "settings": {</strong></span>
<span class="strong"><strong>        "index.codec": "best_compression"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
<span class="strong"><strong>curl -XP</strong></span>
<span class="strong"><strong>OST elasticsearch-node-01:9200/twitter/_open</strong></span>
</pre></div></div><div class="section" title="Storing the _source and analyzed fields"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec15"/>Storing the _source and analyzed fields</h3></div></div></div><p>By default, Elasticesarch stores all documents passed to it in the <code class="literal">_source</code> field and sets all fields to <code class="literal">analyzed</code>. This means that some basic tokenizers are run on the field. Disabling <a id="id293" class="indexterm"/>these options can save some disk space. We may<a id="id294" class="indexterm"/> decide to disable the <code class="literal">_source</code> field if we have documents stored elsewhere in our system. Or, we can disable the <code class="literal">_source</code> field and set the individual fields that we want to retrieve to <code class="literal">store: true</code>.</p><p>For the <code class="literal">analyzed</code> fields, think carefully about how you will use your data and set a field to <code class="literal">index: not_analyzed</code> if you don't need it tokenized. E-mail addresses, IP addresses, social media usernames, or other fields that we don't want to split up should be set to <code class="literal">not_analyzed</code>.</p><p>Create a new index with <code class="literal">_source</code> disabled, and set some fields to <code class="literal">not_analyzed</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST elasticsearch-node-01:9200/newindex -d '{</strong></span>
<span class="strong"><strong>    "mappings" : {</strong></span>
<span class="strong"><strong>        "newtype" : {</strong></span>
<span class="strong"><strong>            "_source" : {</strong></span>
<span class="strong"><strong>                "enabled" : false</strong></span>
<span class="strong"><strong>            },</strong></span>
<span class="strong"><strong>            "properties" : {</strong></span>
<span class="strong"><strong>                "username" : { </strong></span>
<span class="strong"><strong>                    "type" : "string", </strong></span>
<span class="strong"><strong>                    "index" : "not_analyzed",</strong></span>
<span class="strong"><strong>                    "store" : true</strong></span>
<span class="strong"><strong>                },</strong></span>
<span class="strong"><strong>                "text" : { </strong></span>
<span class="strong"><strong>                    "type" : "string"</strong></span>
<span class="strong"><strong>                }</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Unfortunately, there are some pretty big downsides to disabling the <code class="literal">_source</code> field. In addition to not being able to retrieve the full source during a query, the following are only supported if <code class="literal">_source</code> is enabled:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Elasticsearch update API</li><li class="listitem" style="list-style-type: disc">Elasticsearch highlighting</li><li class="listitem" style="list-style-type: disc">Many tools and strategies to re-index data</li></ul></div><p>If disk <a id="id295" class="indexterm"/>space is a major concern, first check whether <a id="id296" class="indexterm"/>enabling data compression will meet your storage needs before disabling the <code class="literal">_source</code> field.</p></div></div><div class="section" title="Optimizing data ingestion"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec67"/>Optimizing data ingestion</h2></div></div></div><p>This section <a id="id297" class="indexterm"/>goes over some additional methods to improve data ingestion. In all of these methods, it's important to monitor the data ingestion rate to ensure that changes have the desired impact on performance.</p><p>As mentioned earlier, test one change at a time and run each test for a long enough period to return meaningful results. The best place to monitor ingestion performance is to select the index of interest in the Marvel <span class="emphasis"><em>Indices</em></span> dashboard. </p><p>The following screenshot shows the Marvel Indices dashboard for our <code class="literal">twitter</code> data index:</p><div class="mediaobject"><img src="graphics/B03798_06_05.jpg" alt="Optimizing data ingestion"/><div class="caption"><p>Marvel indexing requests</p></div></div><p>Monitor this <a id="id298" class="indexterm"/>page in Marvel as you make changes to data ingest operations. This will allow you to see how changes affect the indexing rate in real time, and you'll be able to refer back to past indexing rate metrics for reference.</p><div class="section" title="Bulk indexing operations"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec16"/>Bulk indexing operations</h3></div></div></div><p>For bulk indexing operations, test various ingestion sizes and monitor them in Marvel until you find the<a id="id299" class="indexterm"/> optimal size. For example, run tests at <code class="literal">1MB</code>, <code class="literal">5MB</code>, <code class="literal">10MB</code>, <code class="literal">15MB</code>, and <code class="literal">20MB</code> until you find the value that works best. If you run daily<a id="id300" class="indexterm"/> ingestion jobs, consider running them during off-peak hours so that resulting slowdowns affect fewer users.</p><p>After inserting data into Elasticsearch, the index must be refreshed before a user can see the data. By default, the refresh interval is set to once a second. This means that after indexing a document, it will appear in search results within one second.</p><p>Refreshing as often as once a second can hurt performance during large indexing operations. Lowering the refresh rate to a value such as <code class="literal">10s</code> or <code class="literal">30s</code> is worthwhile if your system doesn't need to display new results immediately after they are indexed.</p><p>Setting the refresh rate to <code class="literal">-1</code> will disable refreshing altogether. This can be useful for very large, one-time, or less-frequent periodic indexing operations. Remember to enable index refreshing afterwards.</p><p>To disable index refreshing, use the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter/_settings -d '{</strong></span>
<span class="strong"><strong>    "index" : {</strong></span>
<span class="strong"><strong>        "refresh_interval" : "-1"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Turn enable index refreshing on afterwards:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter/_settings -d '{</strong></span>
<span class="strong"><strong>    "index" : {</strong></span>
<span class="strong"><strong>        "refresh_interval" : "5s"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Warming queries are run every time an index is refreshed. Another option is to keep index refreshing on, disable warming queries during large index operations, and then re-enable warming when the indexing job is complete.</p><p>Disable index warmers:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter/_settings -d '{</strong></span>
<span class="strong"><strong>    "settings" : {</strong></span>
<span class="strong"><strong>        "index.warmer.enabled" : "false"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div><p>Re-enable<a id="id301" class="indexterm"/> index <a id="id302" class="indexterm"/>warmers:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPUT elasticsearch-node-01:9200/twitter/_settings -d '{</strong></span>
<span class="strong"><strong>    "settings"</strong></span>
<span class="strong"><strong> : {</strong></span>
<span class="strong"><strong>        "index.warmer.enabled" : "true"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}'</strong></span>
</pre></div></div><div class="section" title="Drive configuration"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec17"/>Drive configuration</h3></div></div></div><p>We mentioned in the "Hot-Warm" architecture section that SSDs are great for data indexing<a id="id303" class="indexterm"/> performance. Even if you don't use the "Hot-Warm" architecture, consider<a id="id304" class="indexterm"/> using SSDs for data nodes on your Elasticsearch cluster.</p><p>If SSDs are not an option, consider using fast hard drives (10,000+ RPM) configured in <span class="strong"><strong>RAID 0</strong></span>. Remember <a id="id305" class="indexterm"/>that RAID 0 mirrors for performance, not reliability, but Elasticsearch's data replicas are sufficient for data reliability.</p><p>It's best to avoid storing data on network storage. If you run an Elasticsearch cluster on virtual machines, make sure that they use local disks for storage.</p></div></div></div>
<div class="section" title="Case studies"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Case studies</h1></div></div></div><p>This section offers some real-world problem scenarios and solutions to use Elasticsearch.</p><div class="section" title="Node configuration"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec68"/>Node configuration</h2></div></div></div><p>You have a<a id="id306" class="indexterm"/> five-node production cluster, where each node has <code class="literal">32GB</code> of total memory and <code class="literal">16GB</code> is allocated to Elasticsearch. Lately, you've noticed a problem: every couple of days, <code class="literal">node-05</code> leaves the cluster without warning. Restarting Elasticsearch on this node solves the problem temporarily, but the node will drop out of the cluster again in a few days. How do we go about looking into this issue?</p><p>The next time this error happens, check the Elasticsearch logs before restarting the node:</p><div class="informalexample"><pre class="programlisting">tail -n 500  /var/log/elasticsearch/*.log</pre></div><p>You notice in the log file that Elasticsearch is throwing an <code class="literal">OutOfMemoryError</code> exception, like the following:</p><div class="informalexample"><pre class="programlisting">Caused by: java.lang.OutOfMemoryError: Java heap space
        at org.apache.lucene.store.DataOutput.copyBytes(DataOutput.java:273)
        at org.apache.lucene.util.fst.FST.&lt;init&gt;(FST.java:342)
        at org.apache.lucene.util.fst.FST.&lt;init&gt;(FST.java:321)</pre></div><p>You know that <a id="id307" class="indexterm"/>running out of fielddata can cause <code class="literal">OutOfMemoryError</code> exceptions, so after checking the <code class="literal">elasticsearch.yml</code> file, you find the following:</p><div class="informalexample"><pre class="programlisting"># indices.fielddata.cache.size: 30%</pre></div><p>The cache setting was commented out. Uncomment this line and restart the Elasticsearch node. This seems to solve the problem at first. However, after two weeks, another <code class="literal">OutOfMemoryError</code> from <code class="literal">node-05</code> appears. After restarting the node, log into Bigdesk for insight. Clicking on <code class="literal">node-05</code>, you see the following:</p><div class="mediaobject"><img src="graphics/B03798_06_06.jpg" alt="Node configuration"/><div class="caption"><p>JVM memory in Bigdesk</p></div></div><p>It doesn't look like Elasticsearch is using much of the available memory, but this is probably because the node was just restarted.</p><p>Note that the maximum heap memory available for <code class="literal">node-05</code> is only about <code class="literal">250MB</code>. This is odd, considering the host has <code class="literal">32GB</code> of system memory. At this point, you want to ensure that the <code class="literal">ES_HEAP</code> variable was set properly. Open the following file:</p><div class="informalexample"><pre class="programlisting">/etc/default/elasticsearch</pre></div><p>You will see the following:</p><div class="informalexample"><pre class="programlisting"># ES_HEAP_SIZE=16g</pre></div><p>It looks like this<a id="id308" class="indexterm"/> configuration, like the <code class="literal">indices.fielddata.cache.size</code>, was also commented out. Uncommenting this line and restarting Elasticsearch brings the node's total available memory to <code class="literal">16GB</code>, and eliminates the <code class="literal">OutOfMemoryError</code> exceptions.</p><p>As mentioned earlier, node configuration errors are one of the most common reasons for poor Elasticsearch performance or crashes. It's important to validate each configuration change after it is made.</p></div><div class="section" title="Query optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec69"/>Query optimization</h2></div></div></div><p>You found a <a id="id309" class="indexterm"/>problem in one of your company's internal enterprise Elasticsearch web applications. First thing in the morning, the web application takes a long time to load query results. Performance starts to improve only after running a few queries.</p><p>To tackle this problem, take a look at the slow log. In one of the nodes, you see a query that takes <code class="literal">4.7</code> seconds to run as an <code class="literal">INFO</code> event in the log:</p><div class="informalexample"><pre class="programlisting">[2016-02-29 16:52:43,569][INFO ][index.search.slowlog.query] [elasticsearch-node-03] [twitter][1] took[4.7s], took_millis[4709], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[3], source[{"size":0,"query":{"match_all":{}},"aggs":{"screen_name":{"terms":{"field":"user.screen_name"}},"text":{"terms":{"field":"text"}}}}], extra_source[],</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/>Note</h3><p>The slow log won't necessarily write entries to all nodes, so check the log on each host.</p></div></div><p>Use <code class="literal">python -m json.tool</code> to pretty-print the query:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>echo '{"size":0,"query":{"match_all":{}},"aggs":{"screen_name":{"terms":{"field":"user.screen_name"}},"text":{"terms":{"field":"text"}}}}' | python -m json.tool</strong></span>
</pre></div><p>You will see the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    "aggs": {</strong></span>
<span class="strong"><strong>        "screen_name": {</strong></span>
<span class="strong"><strong>            "terms": {</strong></span>
<span class="strong"><strong>                "field": "user.screen_name"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        },</strong></span>
<span class="strong"><strong>        "text": {</strong></span>
<span class="strong"><strong>            "terms": {</strong></span>
<span class="strong"><strong>                "field": "text"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "query": {</strong></span>
<span class="strong"><strong>        "match_all": {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "size": 0</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>This <code class="literal">aggs</code> parameters <a id="id310" class="indexterm"/>may mean that this query makes heavy use of the field data cache. Diagnose this query and figure out what is causing the performance issue.</p><p>First, clear the fielddata cache to ensure consistent results:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XPOST 'http://elasticsearch-node-01:9200/twitter/_cache/clear'</strong></span>
</pre></div><p>Now, run the query, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XGET 'http://elasticsearch-node-01:9200/twitter/_search' -d '{</strong></span>
<span class="strong"><strong>    "size" : 0,</strong></span>
<span class="strong"><strong>    "query" : {</strong></span>
<span class="strong"><strong>        "match_all" : {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "aggs" : {</strong></span>
<span class="strong"><strong>        "screen_name" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "user.screen_name"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        },</strong></span>
<span class="strong"><strong>        "text" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "text"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }  </strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}' | python -m json.tool</strong></span>
</pre></div><p>Results will be as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>    "took": 4183</strong></span>
<span class="strong"><strong>    ...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>After running the<a id="id311" class="indexterm"/> query a few more times to ensure that the values it needs are in the fielddata cache, the query runs in around <code class="literal">.6</code> seconds. This is a pretty good improvement. Verify that the fielddata cache is now populated using Bigdesk or Marvel (refer to images for fielddata cache for Bigdesk and Marvel).</p><p>The fielddata cache was probably getting cleared due to new data ingestion or shard relocation overnight. To solve this problem, enable eager fielddata loading on both the <code class="literal">user.screen_name</code> and <code class="literal">text</code> fields in the Elasticsearch mapping.</p><p>However, this query's performance still isn't great. Checking the slow log again, we note it still triggers a <code class="literal">TRACE</code> event:</p><div class="informalexample"><pre class="programlisting">[2016-02-29 16:54:20,069][TRACE][index.search.slowlog.query] [elasticsearch-node-03] [twitter][1] took[680ms], took_millis[680], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[3], source[{"size":0,"query":{"match_all":{}},"aggs":{"screen_name":{"terms":{"field":"user.screen_name"}},"text":{"terms":{"field":"text"}}}}], extra_source[],</pre></div><p>To figure out why this query takes over 0.5 seconds to run even after the fielddata cache is populated, break the query down into individual queries—one that runs the <code class="literal">text</code> aggregation, and another that runs the <code class="literal">screen_name</code> aggregation:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XGET 'http://elasticsearch-node-01:9200/twitter/_search' -d '{</strong></span>
<span class="strong"><strong>    "size" : 0,</strong></span>
<span class="strong"><strong>    "query" : {</strong></span>
<span class="strong"><strong>        "match_all" : {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "aggs" : {</strong></span>
<span class="strong"><strong>        "text" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "text"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }  </strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}' | python -m json.tool</strong></span>
</pre></div><p>This query takes approximately <code class="literal">.4</code> seconds to run:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>curl -XGET 'http://elasticsearch-node-01:9200/twitter/_search' -d '{</strong></span>
<span class="strong"><strong>    "size" : 0,</strong></span>
<span class="strong"><strong>    "query" : {</strong></span>
<span class="strong"><strong>        "match_all" : {}</strong></span>
<span class="strong"><strong>    },</strong></span>
<span class="strong"><strong>    "aggs" : {</strong></span>
<span class="strong"><strong>        "screen_name" : {</strong></span>
<span class="strong"><strong>            "terms" : {</strong></span>
<span class="strong"><strong>                "field" : "user.screen_name"</strong></span>
<span class="strong"><strong>            }</strong></span>
<span class="strong"><strong>        }  </strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}' | python -m json.tool</strong></span>
</pre></div><p>This query<a id="id312" class="indexterm"/> runs in <code class="literal">.08</code> seconds; this is a vast improvement over the <code class="literal">text</code> aggregation query.</p><p>As we've identified the <code class="literal">text</code> aggregation as the slow part of the query, consider removing that operation and finding another solution which will yield similar results. Although it depends what the aggregation is used for, aggregating on a lower-cardinality field may be a suitable solution. For example, if the <code class="literal">text</code> aggregation is used to build a word cloud, consider instead using the <code class="literal">entities.hashtags.text</code> hashtag field to get a similar result.</p><p>Another option is keeping the <code class="literal">text</code> aggregation, but running it periodically in the background and caching the results.</p><p>Finally, consider using the shard query cache on this query. As no queries are returned (<code class="literal">size=0</code>), we can enable the <code class="literal">search_type=count</code> and <code class="literal">query</code>
<code class="literal">_cache=true</code> parameter to cache the results of the aggregation.</p></div><div class="section" title="Web application performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec70"/>Web application performance</h2></div></div></div><p>You are working on a web application that searches Twitter data in an Elasticsearch index. In addition to displaying tweets in the search results page, you want to display:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Tweet activity over time</li><li class="listitem" style="list-style-type: disc">Top users in the results</li><li class="listitem" style="list-style-type: disc">Top hashtags in the results</li><li class="listitem" style="list-style-type: disc">Top user mentions</li></ul></div><p>We can implement all of these items using Elasticsearch aggregations, but these operations are much more costly than simply running a search for hits.</p><p>To speed up page load times, we split this into two AJAX requests: one query for results, and one query for all aggregations. The queries are both AJAX requests, meaning that the page will load immediately. The query results will follow shortly after, and the aggregation <a id="id313" class="indexterm"/>results will load last. Because the aggregations query doesn't return any hits, we can set the parameters <code class="literal">search_type=count</code> and <code class="literal">query_cache=true</code> to cache the aggregations for future queries.</p><p>When paging through results, make sure to only query for the hits and not for the aggregation results. Aggregation results will stay the same no matter what page of data is being looked at.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec40"/>Summary</h1></div></div></div><p>This chapter addressed some common performance and reliability issues that come up when using Elasticsearch. To reiterate some of the major points in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Always double-check your Elasticsearch cluster's configuration for errors</li><li class="listitem" style="list-style-type: disc">Set the fielddata cache size, especially if you see <code class="literal">OutOfMemoryError</code> exceptions</li><li class="listitem" style="list-style-type: disc">Use the slow log to find what queries run slow on your cluster</li><li class="listitem" style="list-style-type: disc">Avoid aggregations on high-cardinality fields (such as millisecond timestamps)</li><li class="listitem" style="list-style-type: disc">Be cognizant of your data indexing strategy so that no one index grows too large</li><li class="listitem" style="list-style-type: disc">Use index warmers or enable <code class="literal">eager_global_ordinals</code> to ensure queries that use the fielddata cache are fast the first time we run them</li><li class="listitem" style="list-style-type: disc">If possible, use SSDs on nodes that index data, and avoid storing Elasticsearch indices on network storage</li></ul></div><p>Most importantly, when diagnosing Elasticsearch issues, be meticulous about testing at each stage. For example, don't try to optimize a query by making changes to <code class="literal">elasticsearch.yml</code>, modifying the query criteria, and enabling index <code class="literal">warmers</code> all at once before running the query again. Test one variable at a time to extract precisely where the problem is before deciding how to fix it.</p><p>The next chapter discusses how to understand and fix node failures after they've already happened.</p></div></body></html>