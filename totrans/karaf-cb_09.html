<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Providing a Big Data Integration Layer with Apache Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Providing a Big Data Integration Layer with Apache Hadoop</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing Hadoop client bundles in Apache Karaf</li><li class="listitem" style="list-style-type: disc">Accessing Apache Hadoop from Karaf</li><li class="listitem" style="list-style-type: disc">Adding commands that talk to HDFS for deployment in Karaf</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec72"/>Introduction</h1></div></div></div><p>To continue building on data storage models that aren't as inflexible as traditional RDBMS structures, we will look at Apache Hadoop. Hadoop was created by Doug Cutting and Mike Cafarella in 2005. Cutting, who was working at Yahoo! at the time, named it after his son's toy elephant. It was originally developed to support distribution for the Nutch search engine project.</p><p>Hadoop followed the ideas published by Google in the papers pertaining to Google File System and Google MapReduce. With over a decade of use, Hadoop has grown to a very large and complex ecosystem with a projected revenue of around $23 billion in 2016. Hadoop drives everything from repackaged distributions to full database implementations, analytics packages, and management solutions.</p><p>Hadoop has<a id="id630" class="indexterm"/> also started changing the way startups look at their data models, allowing new companies to make Big Data part of their overall strategy.</p><p>At the <a id="id631" class="indexterm"/>core of Hadoop, you have <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>). This mechanism is what allows the distribution of data. It evolved from a potential single point of failure scenario to having competing implementations from companies like DataStax and RedHat with Cassandra FS and RedHat Cluster File System, respectively.</p><p>In the arena of complete product offerings, you'll find MapR Cloudera, Hortonworks, Intel, and IBM, to name a few as alternatives to the Apache Hadoop distribution.</p><p>If you ponder the future, it does seem that the marriage of SQL-like techniques with a distributed store is where the majority of use cases are headed. This allows users to, at the very least, leverage the RDBMS ideas that are already tried in combination with practically unlimited storage for data mining, social networking aspects, monitoring, and decision making.</p><p>With YARN (Cluster Resource Management) becoming part of the Hadoop infrastructure, HDFS is taken from just storage and MapReduce to an environment that can handle batch, interactive, and streaming as well as application deployment.</p></div></div>
<div class="section" title="Starting a standalone Hadoop cluster"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec73"/>Starting a standalone Hadoop cluster</h1></div></div></div><p>To <a id="id632" class="indexterm"/>start this all off, we'll begin by setting up a simple standalone cluster. We'll need to download and configure an Apache Hadoop release and make sure that we can use it. Then, we will move on to configuring the same configurations and access methods in an Apache Karaf container. We will utilize an external cluster to show how you can utilize Apache Karaf to spin up a new job engine against a large existing cluster. With the features we have and will deploy, you can also embed an HDFS filesystem from a Karaf container.</p><p>Download <a id="id633" class="indexterm"/>a Hadoop release from one of the Apache mirrors at <a class="ulink" href="http://hadoop.apache.org/releases.html#Download">http://hadoop.apache.org/releases.html#Download</a>. At the time of writing this book, the latest release is 2.4.0. A full walkthrough of setting up a cluster can be found at <a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html</a>.</p><p>The following are the changes you need to make to get HDFS up and running and talking to a locally installed node, replication handler, and job tracker.</p><p>Expand the downloaded archive and modify the files in the <code class="literal">etc/hadoop/*</code> folder. The <code class="literal">core-site.xml</code> file needs to be modified as follows:</p><div class="informalexample"><pre class="programlisting">&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre></div><p>The <code class="literal">hdfs-site.xml</code> file needs to be modified as follows:</p><div class="informalexample"><pre class="programlisting">&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre></div><p>The <code class="literal">mapred-site.xml</code> file needs to be modified as follows:</p><div class="informalexample"><pre class="programlisting">&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;localhost:9001&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre></div><p>Once this is accomplished and you can run SSH to your localhost without a password, you can start your daemons. If you cannot run SSH, run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa    </strong></span>
<span class="strong"><strong>cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</strong></span>
</pre></div><p>The preceding commands will create an empty SSH key for remote login. If you already have an existing SSH key associated with your account, you only need to run the second command to make sure that you can remotely log in to your localhost.</p><p>Now, let's verify<a id="id634" class="indexterm"/> that the existing installation is accessible and we can operate against it. Once this is done, we can start all the daemons in one fell swoop using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./sbin/start-all.sh</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>Keep in mind that this command will probably go away as you configure more YARN options in the future.</p></div></div><p>You should see several daemons starting and hopefully no error messages other than possible warnings pertaining to your missing native libraries. (This isn't covered in this little tutorial but pertains to IO libraries and optimized access for your particular platform.)</p><p>When we have a running HDFS system, we'll first create a directory using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> ./bin/hadoop fs -mkdir -p /karaf/cookbook</strong></span>
</pre></div><p>Then, we verify that we can read using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./bin/hadoop fs -ls /karaf</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - joed supergroup          0 2014-05-15 21:47 /karaf/cookbook</strong></span>
</pre></div><p>We've created a directory in the simple single node cluster we started (we are calling this a cluster as all the components necessary to cluster are running and we are simply running them all on one node). We've also ensured that we can list the contents of the said directory. This tells us that HDFS is accessible and active.</p><p>The following is what we have accomplished so far and what we will be covering next.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We <a id="id635" class="indexterm"/>now have a Karaf external HDFS system running. This could have been an existing deployment, an Amazon job, or a set of virtual servers. Basically, we have the fundamentals of a cluster and we know we can access it and create content.</li><li class="listitem" style="list-style-type: disc">The next step is going to be a deep dive into transforming an existing model of deployment into an OSGi-friendly deployment.</li></ul></div></div>
<div class="section" title="Installing Hadoop client bundles in Apache Karaf"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec74"/>Installing Hadoop client bundles in Apache Karaf</h1></div></div></div><p>With Hadoop running, we are ready to start utilizing the resources from Apache Karaf.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec217"/>Getting ready</h2></div></div></div><p>The <a id="id636" class="indexterm"/>ingredients of this recipe include<a id="id637" class="indexterm"/> the Apache Karaf distribution kit, access to JDK, and Internet connectivity. We also assume that an Apache Hadoop distribution is <a id="id638" class="indexterm"/>downloaded and installed. It can be downloaded from <a class="ulink" href="http://hadoop.apache.org/#Download">http://hadoop.apache.org/#Download</a>.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec218"/>How to do it…</h2></div></div></div><p>Hadoop's HDFS libraries aren't part of the standard Karaf feature library; so, we either have to write our own feature or manually install the necessary bundles for the client to run. Apache Camel does have this feature available via Camels HDFS2 component. We can either use Camel's existing feature or build the feature ourselves.</p><p>With the current version of Snappy Java used in the Camel feature, you will run into problems using native libraries with Java 7. This is a well-known issue and is being addressed in<a id="id639" class="indexterm"/> the 1.0.5 release of Snappy Java (<a class="ulink" href="https://github.com/xerial/snappy-java">https://github.com/xerial/snappy-java</a>). To resolve this issue for all platforms, we will build a Karaf feature of our own where we can utilize all of the bundles that are in the Camel feature as well as some additional JAR files that will allow us to run the latest versions. This can be done as follows:</p><div class="informalexample"><pre class="programlisting">&lt;features name="com.packt.chapter.nine-${project.version}"&gt;

  &lt;repository&gt;mvn:org.apache.cxf.karaf/apache-cxf/3.0.0/xml/features&lt;/repository&gt;

  &lt;feature name='xml-specs-api' version='${project.version}' resolver='(obr)' start-level='10'&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.specs/org.apache.servicemix.specs.activation-api-1.1/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.specs/org.apache.servicemix.specs.stax-api-1.0/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.specs/org.apache.servicemix.specs.jaxb-api-2.2/&lt;/bundle&gt;
    &lt;bundle&gt;mvn:org.codehaus.woodstox/stax2-api/&lt;/bundle&gt;
    &lt;bundle&gt;mvn:org.codehaus.woodstox/woodstox-core-asl/&lt;/bundle&gt;
    &lt;bundle&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.jaxb-impl/&lt;/bundle&gt;
  &lt;/feature&gt;

  &lt;feature name='hdfs2' version='${project.version}' resolver='(obr)' start-level='50'&gt;
    &lt;feature&gt;xml-specs-api&lt;/feature&gt;
    &lt;feature&gt;cxf-jaxrs&lt;/feature&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-lang/commons-lang/2.6&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:com.google.guava/guava/16.0.1&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:com.google.protobuf/protobuf-java/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.guice/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.jsch/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.paranamer/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.avro/1.7.3_1&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.commons/commons-compress/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.commons/commons-math3/3.1.1&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-cli/commons-cli/1.2&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-configuration/commons-configuration/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.commons-httpclient/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:io.netty/netty/3.9.2.Final&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.codehaus.jackson/jackson-core-asl/1.9.12&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.codehaus.jackson/jackson-mapper-asl/1.9.12&lt;/bundle&gt;
    &lt;bundle dependency="true"&gt;mvn:org.codehaus.jackson/jackson-jaxrs/1.9.12&lt;/bundle&gt;
    &lt;bundle dependency="true"&gt;mvn:org.codehaus.jackson/jackson-xc/1.9.12&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.snappy-java&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-codec/commons-codec/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-collections/commons-collections/3.2.1&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-io/commons-io/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:commons-net/commons-net/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.zookeeper/zookeeper/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.xmlenc/0.52_1&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.xerces/&lt;/bundle&gt;
    &lt;bundle dependency='true'&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.xmlresolver/&lt;/bundle&gt;
    &lt;bundle&gt;mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.hadoop-client/&lt;/bundle&gt;
  &lt;/feature&gt;

&lt;/features&gt;</pre></div><p>We <a id="id640" class="indexterm"/>can now utilize this feature's file to deploy a Hadoop client that is OSGi-friendly and will allow us to utilize all the<a id="id641" class="indexterm"/> functionalities in Hadoop. We do this by utilizing another project, Apache ServiceMix. Apache ServiceMix maintains a bundle repository where commonly used or sought after resources are repackaged and turned into working OSGi bundles if necessary.</p><p>The <code class="literal">Apache ServiceMix :: Bundles :: hadoop-client</code> bundle that we are using is an uber bundle containing core, YARN, HDFS, MapReduce, common client JAR files, and the Hadoop annotations in one fell swoop.</p><p>We can verify the installation by executing the <code class="literal">list | grep –i hadoop</code> command as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt; list | grep -i hadoop</strong></span>
<span class="strong"><strong>151 | Active |  50 | 2.4.0.1     | Apache ServiceMix :: Bundles :: hadoop-client</strong></span>
</pre></div></div></div>
<div class="section" title="Accessing Apache Hadoop from Karaf"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec75"/>Accessing Apache Hadoop from Karaf</h1></div></div></div><p>In <a id="id642" class="indexterm"/>Hadoop, the core of a cluster is the distributed <a id="id643" class="indexterm"/>and replicated filesystem. We have HDFS running and can access it from our command line window as a regular user. Actually, getting to it from an OSGi container will prove to be slightly more complicated than just writing the Java components.</p><p>Hadoop requires us to provide configuration metadata for our cluster that can be looked up as file or classpath resources. In this recipe, we will simply copy the HDFS site-specific files we created earlier in the chapter to our <code class="literal">src/main/resources</code> folder.</p><p>We will also include the default metadata definitions into our resources by copying them from a dependency, and finally, we'll allow our bundle classloader to perform fully dynamic class loading. To sum it up, we have to copy the <code class="literal">core-site.xml</code>, <code class="literal">hdfs-site.xml</code>, and <code class="literal">mapred-site.xml</code> files into our classpath. These files together describe to our client how to access HDFS.</p><p>As we get to the code, there is also a step we'll perform to trick the Hadoop classloaders into utilizing our bundle classloader as well as respecting the configuration data by providing specific implementations.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec219"/>How to do it…</h2></div></div></div><p>The first thing we'll do is make sure that the necessary defaults are copied into our tutorial bundle.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we will modify the Felix bundle plugin and add the following segment:<div class="informalexample"><pre class="programlisting">&lt;Include-Resource&gt;
  {maven-resources},
  @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/core-default.xml,
  @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/hdfs-default.xml,
  @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/mapred-default.xml,
  @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/hadoop-metrics.properties
&lt;/Include-Resource&gt;</pre></div></li><li class="listitem">Next, we will add another section for dynamic loading of classes as they are needed. This isn't necessarily best practice in OSGi, but sometimes, it is one of the few possible ways of getting bundles and JAR files not intended for OSGi to work. We do this by adding another little snippet to the Felix bundle plugin as follows:<div class="informalexample"><pre class="programlisting">&lt;DynamicImport-Package&gt;*&lt;/DynamicImport-Package&gt;</pre></div><p>With this addition, we tell our bundle that if you need a class at a later time, just go look it up when we need it. It is a tad more costly and certainly not recommended as a general practice as it forces the bundle to scan classpaths.</p></li><li class="listitem">Finally, we pull two more tricks out of our hat inside our implementation code. We <a id="id644" class="indexterm"/>do this as the Hadoop code is multithreaded, and by default, the classloader for a new thread is the system classloader. In an OSGi context, the system classloader is going to have very limited visibility.<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we replace the <code class="literal">ThreadContextClassLoader</code> class as follows:<div class="informalexample"><pre class="programlisting">ClassLoader tccl = Thread.currentThread().getContextClassLoader();
  try {
    Thread.currentThread().setContextClassLoader(getClass().getClassLoader());
    .
  } catch (IOException e) {
    e.printStackTrace();
  } finally {
    Thread.currentThread().setContextClassLoader(tccl);
  }</pre></div></li><li class="listitem">Secondly, as we are using Maven and as Hadoop unfortunately reuses the same file for SPI depending on your implementation, we cannot simply copy resources. Our aggregate JAR file and any dependencies we import will be overwritten with the last imported version.<p>To get around this, we explicitly tell our <code class="literal">Configuration</code> object which implementations to use when accessing our cluster. This is shown in the following code:</p><div class="informalexample"><pre class="programlisting">Configuration conf = new Configuration();
  conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
  conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());</pre></div></li></ol></div></li></ol></div><p>With the <a id="id645" class="indexterm"/>preceding actions, we have satisfied all the classloading and instantiation issues. Now, we are actually ready to access HDFS remotely. However, we still haven't forced our bundle to import and export all of Hadoop, but we can fairly easily change versions and we can externalize the static XML files that define the cluster if needed.</p></div><div class="section" title="How it works"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec220"/>How it works</h2></div></div></div><p>We have basically tricked our bundle into being able to imitate a single monolithic classloader in a normal classpath-provided JVM. We did this by creating a bundle that dynamically imports resources it needs from the Apache Hadoop bundle, and we have ensured<a id="id646" class="indexterm"/> that our bundle can access all the necessary <a id="id647" class="indexterm"/>configuration resources. We also adjusted classloading so that the Hadoop code base uses our bundle classloader to instantiate new instances.</p></div></div>
<div class="section" title="Adding commands that talk to HDFS for deployment in Karaf"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec76"/>Adding commands that talk to HDFS for deployment in Karaf</h1></div></div></div><p>As HDFS at its core is a filesystem, let's see how we can access that with the standard tools and the bundle we've been building up so far.</p><p>What <a id="id648" class="indexterm"/>we'll do is store one level of configuration<a id="id649" class="indexterm"/> files from our running Karaf container into HDFS. Then, we'll provide a second command to read the files back.</p><p>We've learned how to build a feature for Hadoop that takes care of all the various dependencies needed to talk to HDFS, and we have also jumped a little bit ahead and discussed classloading and a few tricks to get the Hadoop libraries we deployed to cooperate. We are now at a point where we can start writing code against Hadoop using the libraries provided.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec221"/>Getting ready</h2></div></div></div><p>The ingredients of this recipe include the Apache Karaf distribution kit, access to JDK, and Internet connectivity. The sample code for this recipe is available at <a class="ulink" href="https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1">https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1</a>. Remember, you need both the drivers installed and Apache Hadoop's HDFS running for these recipes to work!</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec222"/>How to do it...</h2></div></div></div><p>Building a project that can access Hadoop will require the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is generating a Maven-based bundle project. Create an empty Maven-based project. A <code class="literal">pom.xml</code> file containing the essential Maven coordinate information and bundle packaging directives will suffice.</li><li class="listitem">The <a id="id650" class="indexterm"/>next <a id="id651" class="indexterm"/>step is adding dependencies to the POM file. This can be done as follows:<div class="informalexample"><pre class="programlisting">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.osgi&lt;/groupId&gt;
    &lt;artifactId&gt;org.osgi.core&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.osgi&lt;/groupId&gt;
    &lt;artifactId&gt;org.osgi.compendium&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.osgi&lt;/groupId&gt;
    &lt;artifactId&gt;org.osgi.enterprise&lt;/artifactId&gt;
    &lt;version&gt;5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.servicemix.bundles&lt;/groupId&gt;
    &lt;artifactId&gt;org.apache.servicemix.bundles.hadoop-client&lt;/artifactId&gt;
    &lt;version&gt;2.4.0_1&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;!-- custom felix gogo command --&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.karaf.shell&lt;/groupId&gt;
    &lt;artifactId&gt;org.apache.karaf.shell.console&lt;/artifactId&gt;
    &lt;version&gt;3.0.0&lt;/version&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;</pre></div><p>For Karaf 3.0.0, we use OSGi Version 5.0.0. The Hadoop libraries require quite a few supporting bundles. The existing Camel feature was used as a starting point but doesn't actually work on all platforms, so we have to rewrite it to suit our needs.</p></li><li class="listitem">The next step is adding build plugins. Our recipe requires only one build plugin to be configured, which is the bundle. We configure the <code class="literal">maven-bundle-plugin</code> to assemble our project code into an OSGi bundle. We add the <a id="id652" class="indexterm"/>following plugin configuration<a id="id653" class="indexterm"/> to our POM file:<div class="informalexample"><pre class="programlisting">&lt;plugin&gt;
  &lt;groupId&gt;org.apache.felix&lt;/groupId&gt;
  &lt;artifactId&gt;maven-bundle-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.4.0&lt;/version&gt;
  &lt;extensions&gt;true&lt;/extensions&gt;
  &lt;configuration&gt;
    &lt;instructions&gt;
      &lt;Bundle-SymbolicName&gt;${project.artifactId}&lt;/Bundle-SymbolicName&gt;

      &lt;Export-Package&gt;
        com.packt.hadoop.demo*
      &lt;/Export-Package&gt;
      &lt;Import-Package&gt;
        org.osgi.service.blueprint;resolution:=optional,
        org.apache.felix.service.command,
        org.apache.felix.gogo.commands,
        org.apache.karaf.shell.console,
        org.apache.hadoop*,
        *
      &lt;/Import-Package&gt;

      &lt;Include-Resource&gt;
        {maven-resources},
        @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/core-default.xml,
        @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/hdfs-default.xml,
        @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/mapred-default.xml,
        @org.apache.servicemix.bundles.hadoop-client-2.4.0_1.jar!/hadoop-metrics.properties
      &lt;/Include-Resource&gt;

      &lt;DynamicImport-Package&gt;*&lt;/DynamicImport-Package&gt;
    &lt;/instructions&gt;
  &lt;/configuration&gt;
&lt;/plugin&gt;</pre></div><p>The Felix and Karaf imports are required by the optional Karaf commands. We are starting to get a bit more of a complicated bundle plugin as we are enabling dynamic classloading and copying resources around so that they are available to our classloader.</p></li><li class="listitem">The <a id="id654" class="indexterm"/>next step is creating a Blueprint <a id="id655" class="indexterm"/>descriptor file. Create the <code class="literal">src/main/resources/OSGI-INF/blueprint</code> directory tree in your project. We'll then create a file named <code class="literal">blueprint.xml</code> in this folder. Consider the following code:<div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;
&lt;blueprint default-activation="eager"&gt;
  &lt;!-- Define RecipeBookService Services, and expose them. --&gt;
  &lt;bean id="hdfsConfigService" class="com.packt.hadoop.demo.hdfs.HdfsConfigServiceImpl" init-method="init" destroy-method="destroy"/&gt;

  &lt;service ref="hdfsConfigService" interface="com.packt.hadoop.demo.api.HdfsConfigService"/&gt;

  &lt;!-- Apache Karaf Commands --&gt;
  &lt;command-bundle &gt;
    &lt;command&gt;
      &lt;action class="com.packt.hadoop.demo.commands.ReadConfigs"&gt;
        &lt;property name="hdfsConfigService" ref="hdfsConfigService"/&gt;
      &lt;/action&gt;
    &lt;/command&gt;
    &lt;command&gt;
      &lt;action class="com.packt.hadoop.demo.commands.StoreConfigs"&gt;
        &lt;property name="hdfsConfigService" ref="hdfsConfigService"/&gt;
      &lt;/action&gt;
    &lt;/command&gt;

  &lt;/command-bundle&gt;
&lt;/blueprint&gt;</pre></div></li><li class="listitem">The next step is developing an OSGi service with a new Hadoop backend. We've created the basic project structure and plumbed in configurations for Blueprint descriptors. Now, we'll focus on the underlying Java code of our Hadoop-backed application. We break this process down into two steps: defining a service interface and providing a concrete implementation.<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we define a service interface. The service interface will define the user API to our project. In our sample code, we implement the <code class="literal">HdfsConfigService</code> interface, which provides the methods required to store and retrieve the configuration files we have in our Karaf instance. This can be done as follows:<div class="informalexample"><pre class="programlisting">package com.packt.hadoop.demo.api;

public interface HdfsConfigService {

  static final String HDFS_LOCATION = "/karaf/cookbook/etc";

  void storeConfigs();

  void readConfigs();
}</pre></div><p>The interface's implementation follows standard Java conventions, requiring no special OSGi packages.</p></li><li class="listitem">Next, we implement our<a id="id656" class="indexterm"/> communication <a id="id657" class="indexterm"/>with HDFS. Now that we have defined our service interface, we'll provide an implementation as two calls to store and retrieve the file data from HDFS. This can be done as follows:<div class="informalexample"><pre class="programlisting">public class HdfsConfigServiceImpl implements HdfsConfigService {

  private final static String BASE_HDFS = "hdfs://localhost:9000";

  @Override
  public void storeConfigs() {
    String KARAF_etc = System.getProperty("karaf.home") + "/etc";
    Collection&lt;File&gt; files = FileUtils.listFiles(new File(KARAF_etc), new String[]{"cfg"}, false);

    for (File f : files) {
      System.out.println(f.getPath());

      ClassLoader tccl = Thread.currentThread().getContextClassLoader();
      try {
        Thread.currentThread().setContextClassLoader(getClass().getClassLoader());
        String cfg = FileUtils.readFileToString(f);

        Path pt = new Path(BASE_HDFS + HDFS_LOCATION + "/" + f.getName());

        Configuration conf = new Configuration();
        conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
        conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());


        FileSystem fs = FileSystem.get(conf);

        BufferedWriter br = new BufferedWriter(new OutputStreamWriter(fs.create(pt, true)));
        // TO append data to a file, use fs.append(Path f)

        br.write(cfg);
        br.close();
      } catch (IOException e) {
        e.printStackTrace();
      } finally {
        Thread.currentThread().setContextClassLoader(tccl);
      }
    }
  }

  @Override
  public void readConfigs() {
    try {

      FileSystem fs = FileSystem.get(new Configuration());
      FileStatus[] status = fs.listStatus(new Path(BASE_HDFS + HDFS_LOCATION));
      for (int i = 0;i &lt; status.length;i++) {
        BufferedReader br = new BufferedReader(new InputStreamReader(fs.open(status[i].getPath())));
        String line;
        line = br.readLine();
        while (line != null) {
          System.out.println(line);
          line = br.readLine();
        }
      }
    } catch (Exception e) {
      e.printStackTrace();
    }
  }

  public void init() {

  }

  public void destroy() {

  }
}</pre></div></li></ol></div></li><li class="listitem">The next step is the<a id="id658" class="indexterm"/> optional creation <a id="id659" class="indexterm"/>of Karaf commands to directly test the persistence service. To simplify manual testing of our <code class="literal">HdfsConfigService</code> interface, we can create a set of custom Karaf commands that will exercise our HDFS storage and retrieval operations. The sample implementations of these commands are available from the book's website. Of particular interest is how they obtain a reference to the <code class="literal">HdfsConfigService</code> interface and make calls to the service. We must wire the command implementation into Karaf via Blueprint. This can be done as follows:<div class="informalexample"><pre class="programlisting">&lt;!-- Apache Karaf Commands --&gt;
&lt;command-bundle &gt;
  &lt;command&gt;
    &lt;action class="com.packt.hadoop.demo.commands.ReadConfigs"&gt;
      &lt;property name="hdfsConfigService" ref="hdfsConfigService"/&gt;
    &lt;/action&gt;
  &lt;/command&gt;
  &lt;command&gt;
    &lt;action class="com.packt.hadoop.demo.commands.StoreConfigs"&gt;
      &lt;property name="hdfsConfigService" ref="hdfsConfigService"/&gt;
    &lt;/action&gt;
  &lt;/command&gt;

&lt;/command-bundle&gt;</pre></div><p>Each of our custom command's implementation classes are wired to our <code class="literal">hdfsConfigService</code> instance.</p></li><li class="listitem">The next step is deploying the project into Karaf.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>This demo will require a running Hadoop cluster!</p></div></div><p>We make sure that all the Hadoop bundles are installed correctly. This <a id="id660" class="indexterm"/>can be done using the<a id="id661" class="indexterm"/> following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt; feature:repo-add mvn:com.packt/chapter-9-recipe1/1.0.0-SNAPSHOT/xml/features</strong></span>
<span class="strong"><strong>Adding feature url mvn:com.packt/chapter-9-recipe1/1.0.0-SNAPSHOT/xml/features</strong></span>

<span class="strong"><strong>karaf@root()&gt; feature:install hdfs2</strong></span>
<span class="strong"><strong>karaf@root()&gt;</strong></span>

<span class="strong"><strong>karaf@root()&gt; list</strong></span>
<span class="strong"><strong>START LEVEL 100 , List Threshold: 50</strong></span>
<span class="strong"><strong> ID | State  | Lvl | Version        | Name</strong></span>
<span class="strong"><strong>--------------------------------------------------------------</strong></span>
<span class="strong"><strong>126 | Active |  50 | 2.6            | Commons Lang</strong></span>
<span class="strong"><strong>127 | Active |  50 | 16.0.1         | Guava: Google Core Libraries for Java</strong></span>
<span class="strong"><strong>128 | Active |  50 | 2.5.0          | Protocol Buffer Java API</strong></span>
<span class="strong"><strong>129 | Active |  50 | 3.0.0.1        | Apache ServiceMix :: Bundles :: guice                                 </strong></span>
<span class="strong"><strong>130 | Active |  50 | 0.1.51.1       | Apache ServiceMix :: Bundles :: jsch                                  </strong></span>
<span class="strong"><strong>131 | Active |  50 | 2.6.0.1        | Apache ServiceMix :: Bundles :: paranamer                             </strong></span>
<span class="strong"><strong>132 | Active |  50 | 1.7.3.1        | Apache ServiceMix :: Bundles :: avro                                  </strong></span>
<span class="strong"><strong>133 | Active |  50 | 1.8.1          | Apache Commons Compress</strong></span>
<span class="strong"><strong>134 | Active |  50 | 3.1.1          | Commons Math</strong></span>
<span class="strong"><strong>135 | Active |  50 | 1.2            | Commons CLI</strong></span>
<span class="strong"><strong>136 | Active |  50 | 1.10.0         | Apache Commons Configuration</strong></span>
<span class="strong"><strong>137 | Active |  50 | 3.1.0.7        | Apache ServiceMix :: Bundles :: commons-httpclient</strong></span>
<span class="strong"><strong>138 | Active |  50 | 3.9.2.Final    | The Netty Project</strong></span>
<span class="strong"><strong>139 | Active |  50 | 1.9.12         | Jackson JSON processor</strong></span>
<span class="strong"><strong>140 | Active |  50 | 1.9.12         | Data mapper for Jackson JSON processor</strong></span>
<span class="strong"><strong>141 | Active |  50 | 1.9.12         | JAX-RS provider for JSON content type, using Jackson data binding</strong></span>
<span class="strong"><strong>142 | Active |  50 | 1.9.12         | XML Compatibility extensions for Jackson data binding</strong></span>
<span class="strong"><strong>143 | Active |  50 | 1.0.4.1_1      | Apache ServiceMix :: Bundles :: snappy-java</strong></span>
<span class="strong"><strong>144 | Active |  50 | 1.9.0          | Apache Commons Codec</strong></span>
<span class="strong"><strong>145 | Active |  50 | 3.2.1          | Commons Collections</strong></span>
<span class="strong"><strong>146 | Active |  50 | 2.4.0          | Commons IO</strong></span>
<span class="strong"><strong>147 | Active |  50 | 3.3.0          | Commons Net</strong></span>
<span class="strong"><strong>148 | Active |  50 | 3.4.6          | ZooKeeper Bundle</strong></span>
<span class="strong"><strong>149 | Active |  50 | 0.52.0.1       | Apache ServiceMix :: Bundles :: xmlenc</strong></span>
<span class="strong"><strong>150 | Active |  50 | 2.11.0.1       | Apache ServiceMix :: Bundles :: xercesImpl</strong></span>
<span class="strong"><strong>151 | Active |  50 | 2.4.0.1        | Apache ServiceMix :: Bundles :: hadoop-client</strong></span>
<span class="strong"><strong>152 | Active |  80 | 1.0.0.SNAPSHOT | Chapter 9 :: Manage Big Data with Apache Hadoop - HDFS Client Example.</strong></span>
<span class="strong"><strong>karaf@root()&gt;</strong></span>
</pre></div><p>We install our project bundle by executing the <code class="literal">install</code> command on its Maven coordinates:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt;  install –s mvn:com.packt/chapter-9-recipe1/1.0.0-SNAPSHOT</strong></span>
</pre></div></li><li class="listitem">The last step is testing the project. We can use the following commands for this:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>karaf@root()&gt; test:storeconfigs </strong></span>
<span class="strong"><strong>karaf@root()&gt; test:readconfigs </strong></span>
<span class="strong"><strong>karaf@root()&gt;</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec223"/>How it works…</h2></div></div></div><p>We<a id="id662" class="indexterm"/> have a Karaf container now communicating<a id="id663" class="indexterm"/> with an external HDFS filesystem. We can back up configuration files from Karaf to HDFS and we can read them back.</p><p>A new Karaf instance can be started to consume and copy these configurations or we can use this recipe as the basis for starting up MapReduce jobs, tasks, and batch jobs.</p></div></div></body></html>