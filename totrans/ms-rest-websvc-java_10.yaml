- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing Strategies for Robust APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While researching and drafting this chapter, we had a completely different vision
    in mind. The original plan was to write a traditional piece on testing APIs, a
    structured, methodical exploration of established practices. The focus was going
    to be on the tools and techniques that have been staples of **API testing** for
    years, such as Postman, Bruno, and various mocking frameworks. These tools have
    been essential in helping engineers ensure API reliability and are still widely
    used, particularly in projects with legacy systems.
  prefs: []
  type: TYPE_NORMAL
- en: However, the more we thought about the topic, it became clear that the landscape
    of software testing has undergone a significant shift. The rise of **generative
    AI** and **large language models** ( **LLMs** )—such as ChatGPT, Gemini, and similar
    tools—has fundamentally transformed how we approach software development, including
    testing. What was once a slow and often repetitive process has now evolved into
    a dynamic, AI-enhanced workflow. Generative AI doesn’t just make writing tests
    faster; it makes the entire process more engaging, efficient, and effective.
  prefs: []
  type: TYPE_NORMAL
- en: This transformation isn’t just about convenience. The integration of generative
    AI into API testing has far-reaching benefits for everyone involved in the software
    development lifecycle. Engineers can now focus on higher-level problem solving
    rather than being bogged down by tedious test creation. Product managers and owners
    gain greater confidence in the quality of their APIs, knowing that testing has
    become more thorough and adaptive. The results are more meaningful, ensuring that
    APIs meet real-world needs while maintaining the highest quality standards.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter, therefore, is not the one originally set out to be written. We
    have shifted focus entirely to reflect this new reality. Here, we’ll explore how
    generative AI can revolutionize the way we write tests for APIs, providing insights
    and techniques to help you leverage this game-changing technology. By embracing
    these advancements, we can create better APIs, foster collaboration, and deliver
    software that exceeds expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test format and tooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a development environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running and evolving the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the advantages of focusing on these new ideas for integration and regression
    tests is in tools and installations. If you can run a JavaJUnit test on your local
    machine, you already have everything needed to run a test.
  prefs: []
  type: TYPE_NORMAL
- en: We will also utilize Docker to deploy our applications and dependencies locally.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need access to an LLM. Any of the freely available LLMs should
    work well (ChatGPT, Gemini, Claude, …).
  prefs: []
  type: TYPE_NORMAL
- en: Types of tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With a few minutes of online search, one can identify 19 different types of
    tests commonly referenced in the industry, and I’m sure there are a few more:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Functional testing** : Validates that the API functions as expected by checking
    its behavior against specified requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration testing** : Ensures the API interacts correctly with other software
    components, systems, or APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit testing** : Tests individual units or components of the API in isolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance testing** : Evaluates the API’s responsiveness, scalability,
    and stability under different conditions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load testing** : Checks how the API handles expected user traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stress testing** : Pushes the API beyond its limits to find breaking points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spike testing** : Assesses the API’s reaction to sudden traffic spikes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Soak testing** : Monitors the API’s performance over an extended period under
    sustained load'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security testing** : Ensures the API is secure from vulnerabilities and unauthorized
    access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation testing** : Verifies the API meets the expected system and business
    requirements as a whole'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability testing** : Ensures the API is easy to understand and use by developers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression testing** : Ensures new changes or updates to the API do not break
    existing functionality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance testing** : Verifies the API meets specific legal, regulatory,
    or industry standards'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploratory testing** : Performs unscripted interactions with the API to
    uncover unexpected issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interoperability testing** : Verifies the API works correctly with different
    platforms, languages, and environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fuzz testing** : Identifies vulnerabilities by sending random, malformed,
    or unexpected data to the API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**End-to-end testing** : Validates entire workflows or user scenarios involving
    the API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data testing** : Ensures the integrity and correctness of the data processed
    or stored by the API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mock and sandbox testing** : Tests API functionality in isolation or simulates
    environments without impacting live systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clearly, a comprehensive discussion on all these types of testing would require
    an entire book or more. Therefore, in this chapter, we will focus on a few specific
    types of testing: functional, regression, and validation tests.'
  prefs: []
  type: TYPE_NORMAL
- en: Why these specific types of tests? Well, as stated earlier, the main goal is
    to help software engineers create better APIs. Among the various types of tests,
    functional, regression, and validation provide the most value in achieving this
    goal. Their primary benefit is the confidence they instill when making changes.
    One of the biggest challenges software engineers face is the unpredictable consequences
    of modifying code. Changes needed to address one problem might inadvertently break
    something else. This problem is both common and paralyzing. How can we ensure
    that our changes won’t disrupt existing functionality? By prioritizing functional
    and regression testing, we can tackle this concern head-on, ensuring the API continues
    to meet its requirements while safeguarding against unintended consequences introduced
    by new changes. This is the essence of the **test-driven development** ( **TDD**
    ) methodology that started in the early 2000s.
  prefs: []
  type: TYPE_NORMAL
- en: However, creating and maintaining comprehensive test sets is a resource-intensive
    process, both in terms of time and cost. Writing effective tests requires a deep
    understanding of the system, careful planning, and meticulous implementation.
    Once written, these tests must be regularly updated and adapted to reflect changes
    in the system, which adds to the ongoing maintenance effort. This level of investment
    can be difficult for organizations to justify, especially in fast-paced environments
    where tight deadlines and budget constraints are common. As a result, many projects
    have historically deprioritized testing, often preventing development teams from
    dedicating the time and resources needed to build robust test suites. This short-sighted
    approach may save effort in the short term but often leads to higher costs later
    when defects surface, affecting system reliability and customer satisfaction.
    Also, let’s be honest—writing tests can be boring, and it’s a sentiment shared
    by many in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in November of 2022, the introduction of generative AI completely transformed
    the creation of software tests. Generative AI has become an incredible asset in
    writing tests. While there’s an ongoing debate about its impact on developing
    actual business logic—a discussion for another time—for testing purposes, it’s
    a phenomenal tool. We will leverage generative AI extensively to write our tests
    and assist in their development. This innovation not only enhances the value of
    functional and regression testing for software engineers but also makes these
    tests much easier and cheaper to write.
  prefs: []
  type: TYPE_NORMAL
- en: Test format and tooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to testing APIs, we have access to a variety of tools and formats,
    each serving different needs and levels of expertise. Tools such as Postman and
    Bruno provide user-friendly interfaces for building, managing, and running API
    tests. These tools are especially useful for quick exploratory testing or for
    creating manual test workflows. They excel in environments where visual representation
    and non-technical collaboration are essential. However, when we shift our focus
    to automated, scalable, and pipeline-ready testing, tools such as JUnit, which
    are integrated directly into the development ecosystem, offer significant advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as Postman allow users to design API tests with minimal coding, making
    them accessible to a broader audience. These tools often include features such
    as sharing test collections, generating test documentation, and providing real-time
    visual results. Despite these strengths, they fall short when integrated into
    sophisticated CI/CD pipelines. Exporting and running tests from Postman or Bruno
    often require additional tooling or manual steps, and they lack the seamless compatibility
    with source control systems such as Git that code-based testing frameworks offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, JUnit, a widely used testing framework for Java, provides a code-first
    approach to testing APIs. By writing API tests directly in Java using JUnit, developers
    can ensure their tests are treated as part of the source code. This approach brings
    several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration into deployment pipelines** : JUnit tests are inherently compatible
    with CI/CD tools such as Jenkins, GitHub Actions, and GitLab CI. This allows API
    tests to run automatically during builds or deployments, ensuring that no changes
    go live without thorough testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leverage of source code infrastructure** : Because JUnit tests are written
    in pure code, they benefit from the same tools and workflows used for the application
    code itself—version control, peer reviews, static analysis, and more. This consistency
    reduces complexity and ensures tests are always in sync with the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and maintainability** : Code-based tests are easier to maintain
    as they evolve alongside the application. Refactoring tools, IDE features, and
    linters help developers quickly adapt tests when APIs change, minimizing the risk
    of outdated or broken tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reusability and customization** : JUnit enables developers to write reusable
    test helpers and utilities, providing greater flexibility for complex testing
    scenarios. It also supports advanced features such as parameterized tests, mock
    environments, and dependency injections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, JUnit’s ability to directly call real APIs—whether deployed locally
    or in a test environment—ensures that tests mimic real-world usage. This helps
    catch issues such as incorrect request formatting, unexpected responses, or performance
    bottlenecks, which can be missed in abstracted environments such as Postman or
    Bruno.
  prefs: []
  type: TYPE_NORMAL
- en: While Postman and Bruno are excellent for quick, one-off testing or for collaborating
    with non-developers, JUnit (and similar code-first tools) is the superior option
    for teams aiming to achieve robust, automated, and pipeline-integrated testing.
    By treating API tests as first-class citizens within the code base, developers
    can maximize efficiency, scalability, and confidence in their APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the strengths of automated testing is essential, yet AI is beginning
    to redefine testing entirely and a new approach to testing has appeared.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of software development practices has always revolved around improving
    efficiency, quality, and collaboration. One such recent advancement is the integration
    of generative AI into Agile software development, particularly within the SCRUM
    framework (see [https://www.scrum.org/resources/what-scrum-module](https://www.scrum.org/resources/what-scrum-module)
    ). Some teams are already experimenting with a new practice where prompts for
    AI-driven testing become an integral part of the development process, transforming
    how teams define, implement, and validate new features.
  prefs: []
  type: TYPE_NORMAL
- en: In this updated workflow, the Technical Lead, in collaboration with the Product
    Owner, plays a pivotal role in shaping the development and testing lifecycle.
    When a new story is defined, a feature or enhancement to an API is introduced,
    and the Technical Lead crafts detailed prompts designed for generative AI tools.
    These prompts serve as the blueprint for testing the story’s acceptance criteria.
    By leveraging the domain expertise of both the Technical Lead and the Product
    Owner, the prompts encapsulate not only the technical requirements but also the
    business context and expected outcomes of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Incorporating AI-driven prompts into Agile SCRUM workflows](img/B21843_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Incorporating AI-driven prompts into Agile SCRUM workflows
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a new API endpoint is being added to return a customer’s transaction
    history, the prompt might specify the exact input parameters, the expected structure
    of the JSON response, and edge cases to test, such as handling invalid input or
    empty datasets. These prompts are more than just instructions—they’re a shared
    artifact that bridges technical and business perspectives, ensuring alignment
    and clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'This integration of generative AI into Agile workflows offers several significant
    advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency** : Standardized prompts ensure a consistent approach to testing
    across stories, reducing variability and improving reliability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency** : By automating the generation of test cases, developers can
    focus on coding the feature itself, reducing repetitive tasks and freeing up time
    for innovation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration** : The joint creation of prompts by Technical Leads and Product
    Owners fosters better communication and alignment between technical and business
    teams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality assurance** : Prompts encourage the team to think critically about
    edge cases, error handling, and expected outcomes from the start, reducing bugs
    and rework down the line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing prompts in the code base
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To solidify the importance of these prompts, they are stored in the project’s
    Git repository alongside the source code. This integration elevates prompts to
    a first-class citizen within the development lifecycle, treating them with the
    same rigor as the code itself. Storing prompts in Git provides several key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version control** : Prompts evolve as features mature or requirements change.
    Storing them in Git ensures a complete history of modifications, enabling teams
    to trace changes over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration** : Developers can review and refine prompts during code reviews,
    ensuring their accuracy and relevance before implementation begins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traceability** : Linking prompts to specific stories, commits, and branches
    creates a transparent and auditable process that ties testing directly to development
    efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reusability** : Over time, prompts for similar features or scenarios can
    be repurposed, reducing redundancy and accelerating future development cycles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI-generated code as acceptance criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the prompts are finalized and stored, they become an integral part of the
    story’s acceptance criteria. The assigned software engineer uses the prompt to
    generate test cases or other relevant code through generative AI tools. This generated
    code acts as a baseline for validating the new feature. The key requirement is
    that the generated code must pass successfully—be *green* —before the story is
    marked as complete.
  prefs: []
  type: TYPE_NORMAL
- en: This approach introduces a new layer of rigor to the definition of *done* in
    SCRUM. It ensures that testing is not an afterthought but a parallel process that
    evolves alongside the feature’s development. Additionally, by automating large
    portions of the testing process through AI, teams can deliver high-quality software
    faster, with fewer manual testing bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating AI-driven testing into Agile practices demands more than tools—it
    requires a cultural shift.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting a new Agile mindset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This practice represents a shift in mindset for Agile teams. The act of defining
    prompts moves testing considerations to earlier in the development process, aligning
    with the principles of TDD. It emphasizes the importance of testing not just as
    a verification step but as a collaborative, iterative process that enhances the
    overall quality of the software.
  prefs: []
  type: TYPE_NORMAL
- en: 'By embedding AI-driven testing prompts into the Agile workflow, teams not only
    embrace the power of generative AI but also elevate their development practices
    to a new level of precision and efficiency. This approach aligns perfectly with
    the goals of Agile: to deliver value faster, adapt to change effectively, and
    maintain a high standard of quality throughout the development lifecycle.'
  prefs: []
  type: TYPE_NORMAL
- en: The structure of an API test environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of an API testing environment is to replicate all critical components
    of your application locally, facilitating comprehensive and realistic tests of
    your code. The following figure illustrates a typical setup for such an environment,
    emphasizing a self-contained system suitable for development and testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Typical structure of a local API testing environment using Docker
    and cloud networks](img/B21843_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Typical structure of a local API testing environment using Docker
    and cloud networks
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, **API App** represents the primary application whose APIs you
    are testing. This app connects directly to essential backend components such as
    databases, caching services, or messaging queues, all deployed within a containerized
    (Docker) or cloud-based, non-production network environment. The **API Tests**
    component directly interacts with **API App** , simulating real-world scenarios
    and ensuring thorough coverage and validation of API functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term “local” here is flexible: it can refer to an individual software engineer’s
    machine, typically using Docker containers, or any non-production cloud-based
    environment provisioned for development purposes. Ensuring every team member has
    access to such standardized, isolated environments is crucial, allowing consistent
    and repeatable tests across different workstations and reducing variability in
    test outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: There is an upfront effort involved in creating this local or development environment.
    However, making this investment early in the development lifecycle significantly
    enhances efficiency, reducing the risk of integration issues and accelerating
    overall development.
  prefs: []
  type: TYPE_NORMAL
- en: “Prompting” the LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Prompt engineering** is the practice of designing and refining prompts given
    to generative AI models to produce precise, accurate, and relevant outputs. It
    involves careful phrasing and iterative tuning to achieve desired responses efficiently.
    And now let’s look at some mechanisms needed to create effective prompts for test
    generation.'
  prefs: []
  type: TYPE_NORMAL
- en: As was said before, the use of generative AI for software testing is a perfect
    application of this technology. Specifically, tests are generally less sensitive
    in terms of intellectual property and ownership concerns. Tests are primarily
    tools for validation, ensuring that your main code behaves as expected. Since
    the focus of tests is on functionality, edge cases, and coverage, generative AI
    can quickly generate a wide variety of test cases, saving developers time and
    effort. Moreover, tests are often repetitive and detail-oriented—tasks that AI
    excels at. The use of generative AI here can allow developers to focus on more
    creative or complex aspects of their work, while the AI takes care of the grunt
    work. This means better productivity, faster iteration, and higher test coverage
    with minimal manual effort.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, when it comes to using generative AI to write the main production
    code, the situation becomes a little more complex. A major concern is ownership
    of the generated code. Depending on the terms of the AI service, the code you
    use might still be partially owned or influenced by the AI provider. This can
    create legal or compliance issues, especially in industries where intellectual
    property is a critical asset. Additionally, there’s the concern that your code—or
    patterns from it—could be used to train future AI models. This could inadvertently
    expose proprietary algorithms, business logic, or sensitive methodologies to external
    entities. While this may not be a major issue for tests (which are often more
    generic and less confidential), it can be a significant risk for production code,
    where the intellectual property and security of the organization are at stake.
    At the time of writing, there are several legal decisions establishing that AI-generated
    content cannot be copyrighted *[3][4]* . Now, we will create the actual prompts
    for our test.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Products API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Products API that we have been using in this book is a very good use case
    for this new testing strategy. Let’s look at a test-generating prompt for that
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating a test to validate the creation of new products with
    the `PUT` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at each part of this prompt in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the LLM perspective. This is relevant to establish the role
    the LLM should play, and it does influence the answers. Sometimes it is clear
    in the resulting code when you tell the LLM to act as a senior or junior professional:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, give a clear request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The LLM’s output is heavily influenced by what you want as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, explain the API behavior as clearly as possible. This is important because
    if the underlying behavior is not clear then the LLM will try to fill in the blanks,
    sometimes with invalid assumptions. The more details you add, the better the test
    cases created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Part of these details is the code that defines the data being tested. Either
    present the data structures used or the expected HTTP responses, but some form
    of unambiguous definition must be present.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, add any specific assertions we want the test to validate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this prompt is the following code and `pom.xml` dependency elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code is not in itself complex or hard to write, but it is time-consuming,
    and most software engineers don’t particularly enjoy writing it. Also, using an
    LLM to create the tests adds a lot of flexibility to the process. For example,
    if you want this test to be created in a different language—for example, in Go—we
    only need to change the expected results to have a new version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Go ahead and play around with the code generation options.
  prefs: []
  type: TYPE_NORMAL
- en: In some projects, there is even the requirement that automated tests be written
    in a different language than the original code. There may not be a real technical
    reason for such a requirement, as an HTTP client is exactly like another, but
    it offers some extra psychological comfort to non-technical people in the project
    to have this extra layer of separation.
  prefs: []
  type: TYPE_NORMAL
- en: A more complex API to test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous exercise, we looked at testing an API in which we had complete
    access to the source code, and we were developing that API from scratch. Also,
    the example API was intentionally simple. But, there is another common situation
    that many software engineers face: An API that is already in production, with
    not many automated tests available, that is more complex and returns sophisticated
    responses. Let’s look at one such scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: For this section, we are going to use the HAPI FHIR APIs and write some tests
    for them.
  prefs: []
  type: TYPE_NORMAL
- en: HAPI FHIR is a complete Java implementation of the HL7 FHIR standard for healthcare
    interoperability. We chose this example because the API is complex, there is a
    public test implementation available, and it is used extensively in the healthcare
    market. This implementation is used by many organizations worldwide, including
    large hospital complexes, healthcare organizations, and governmental health agencies.
    But what is interesting to us from a testing perspective is how complex these
    APIs are. It is not uncommon to receive a few thousand lines of JSON in the response
    to a search, which allows us to create more nuanced prompts and corresponding
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/Information_Box_Icon.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To know more about HAPI FHIR and HL7 FHIR, please refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hapifhir.io/](https://hapifhir.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hl7.org/fhir/](https://hl7.org/fhir/)'
  prefs: []
  type: TYPE_NORMAL
- en: Because healthcare requirements are usually unique to each organization and
    dependent on many local regulations, there is a high level of customization required
    for any HAPI FHIR implementation. It is not uncommon to find several thousand
    lines of custom code in a specific implementation. A strong set of tests is crucial
    when customizing such a complex API. You don’t want to break any of the HAPI FHIR
    contracts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, one great advantage of HAPI FHIR as a test example for us is the availability
    of a public test server: [https://hapi.fhir.org/baseR4](https://hapi.fhir.org/baseR4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having prior knowledge of the HAPI FHIR APIs is not necessary to follow this
    section. We are interested only in looking at the API’s behavior and preparing
    tests to validate that such behavior does not change. Here are a couple of HAPI
    FHIR concepts we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PATIENT** : HAPI FHIR is a healthcare system; therefore, PATIENT is a first-class
    element in it. We will write some tests to validate that searching for a patient
    returns the elements we want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RESOURCE TYPE – BUNDLE** : In HAPI FHIR, all results of a search are bundles.
    This is just the name used to define a set of data, but we will validate this
    behavior as part of our tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that in mind, let’s assume you are a software engineer in the HAPI FHIR
    support team in a large hospital, and you just received the task of customizing
    the HAPI FHIR API to add a new element to a `Patient` record. This new element
    is used only in this hospital’s system. Assume that there are no test cases for
    the Patient API. What can you do?
  prefs: []
  type: TYPE_NORMAL
- en: Well, you could start by looking at the source code and try to find all the
    relevant classes and all, but this is a time-consuming task and will delay the
    generation of the tests. Maybe a more pragmatic approach is needed.
  prefs: []
  type: TYPE_NORMAL
- en: So, start by looking at the API responses and prepare tests so your changes
    will not break the current contract.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a `GET` request that returns all the patients in the system, one at
    a time. All the data is synthetic and not related to any real individual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine in detail the response we received (you will not get the same
    data when running this example, but the format and structure will be the same):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This result is already 70+ lines long and is a good start for our API analysis
    and to design our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the results. We can see that the result of a `GET` function has
    a `resourceType` , which in this case is `Bundle` . `Type` is a `searchset` .
  prefs: []
  type: TYPE_NORMAL
- en: There is an array of links (“link”) and an array of entries (“entry”) with the
    actual patient’s data.
  prefs: []
  type: TYPE_NORMAL
- en: Good, this is a start.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing your test prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that your goal is to make changes to the code, without breaking the
    current state, so start by defining clearly what you want to validate with your
    test—for example, “Test that all `GET` searches will return a valid pagination
    JSON.” Now, let’s write a prompt to generate our first test. The results are from
    ChatGPT 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Paste the whole JSON sample with the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are the following tests and `pom.xml` changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This code runs immediately without any modifications. This was not the case
    with the previous generation of LLMs. If you are using an older model, such as
    ChatGPT 3.5 for example, you may have to manually adjust the code.
  prefs: []
  type: TYPE_NORMAL
- en: This whole test was completed in less than a minute by the LLM and it is actually
    fun to write and see the results so quickly—a very different experience from having
    to manually write tests by hand. The test can be improved, of course. For example,
    it could be changed to get the URL to be tested from a list or even a property
    file for added flexibility, but the core of the code is there and ready to be
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to a more complex type of test and see how far we can go
    with our validations.
  prefs: []
  type: TYPE_NORMAL
- en: Validating more complex behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the API’s response, we can see that the “link” array has two entries
    on this first page: “ `self"` and “ `next"` .'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: A good test would be to validate that a response always has the `self` link.
    Let’s expand the prompt to include this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the new prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This will add the following code to our test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, the code is not very complex, but it is already showing signs of increasing
    difficulty. Writing this by hand would not be hard, but would take some time and
    attention. LLMs can create it in 30 seconds or less.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s add even more complexity. This is a pagination exercise, so let’s
    ask the code to follow the `next` link for a few pages and validate that page.
    Then, validate if every entry in the result is unique, using `fullUrl` inside
    the `entry` array as a candidate key.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result is starting to look more sophisticated and interesting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The test is now a lot more complex. Look at how some helping functions are created
    ( `isValidUUID()` , `createConnection()` ). This test would take longer to write
    and be costlier to maintain. But with the use of LLMs, the whole test is ready
    to run in just a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you are seeing the pattern at this point. As the API gets more complex,
    the tests also evolve. The LLM prompt itself is now stored in the Git repository
    and is an integral part of the code base, to the point that it can be seen as
    part of the software documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The introduction of generative AI holds immense potential to reshape the landscape
    of software development, especially in the realm of testing. By automating test
    creation, streamlining complex processes, and enhancing adaptability, generative
    AI offers the promise of transforming testing into a more efficient, accessible,
    and cost-effective part of the development lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Through the strategies and examples discussed in this chapter, we hope you have
    acquired the skills and inspiration to explore and extend the testing of your
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Testing is a tool for validating behavior during the development and maintenance
    of our APIs, but for a truly robust service, we need to monitor these behaviors
    in production. That is what we will be looking at in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Test driven development* : [https://en.wikipedia.org/wiki/Test-driven_development](https://en.wikipedia.org/wiki/Test-driven_development
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prompt Engineering* : [https://en.wikipedia.org/wiki/Prompt_engineering](https://en.wikipedia.org/wiki/Prompt_engineering
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Legal Ruling: AI-Generated Art and Copyright* : [https://www.reuters.com/legal/ai-generated-art-cannot-receive-copyrights-us-court-says-2023-08-21/](https://www.reuters.com/legal/ai-generated-art-cannot-receive-copyrights-us-court-says-2023-08-21/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U.S. Copyright Office’s View on AI Prompts* : [https://www.theverge.com/news/602096/copyright-office-says-ai-prompting-doesnt-deserve-copyright-protection](https://www.theverge.com/news/602096/copyright-office-says-ai-prompting-doesnt-deserve-copyright-protection
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HAPI FHIR: Open-Source Implementation of HL7 FHIR* : [https://hapifhir.io/](https://hapifhir.io/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HL7 FHIR Official Specification* [https://hl7.org/fhir/](https://hl7.org/fhir/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
