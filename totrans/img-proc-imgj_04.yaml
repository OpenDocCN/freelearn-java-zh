- en: Chapter 4. Image Segmentation and Feature Extraction with ImageJ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter looked at processing images to view and correct imperfections
    in acquisition. This chapter will introduce techniques for segmenting images and
    extracting features that are relevant for processing and analysis. The following
    topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morphological processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image filtering and convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many steps in image analysis, it is important to split the image into two
    separate (non-overlapping) components. These components are usually labeled as
    background and foreground. Generally speaking, the background is the part of the
    image we are not directly interested in when we analyze the image. We normally
    restrict our analysis to parts of the image that are deemed as the foreground.
    This splitting into two components is called segmentation and is primarily based
    on pixel intensity. This is important if you wish to count and measure a number
    of unique objects of a specific type or measure the intensity of a single complex
    object while excluding the background from the measurement.
  prefs: []
  type: TYPE_NORMAL
- en: Image thresholding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To achieve the split of an image into background and foreground, we will set
    a threshold value. Values below this threshold will be classified as one group,
    while pixels with higher or equal values will be classified as another group.
    In general, the background in fluorescent images contains values close to black
    (that is, a dark background), while brightfield images have background values
    closer to white (a light background). The output of thresholding is an image called
    a mask in ImageJ, which is a binary image. Its pixels have only two values (0
    and 255).
  prefs: []
  type: TYPE_NORMAL
- en: We will look at how to perform basic thresholding on a grayscale image first.
    After that, we will look at the possibilities for thresholding a color image.
    The difference between these two image types stems from the fact that a color
    image does not have an easy way of setting a threshold. Each pixel contains three
    values (red, green, and blue), and a single threshold value does not segment the
    image in useful ways generally.
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding grayscale images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will start by taking a grayscale image from the sample images and segmenting
    it. For this example, we will use the **Blobs** image. Thresholding would be the
    first step if you wanted to measure the size of each individual blob as well as
    get a count of the number of blobs in the image. Note that for small images such
    as this example, counting could be done by hand. However, if you need to do this
    for a large number of images, this method of counting by hand would be very tedious.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set a threshold, go to **Image** | **Adjust** | **Threshold…** or press
    *Ctrl* + *Shift* + *T*. The threshold dialog will open with a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thresholding grayscale images](img/Insert_image_4909_03b_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For fluorescent images, the **Dark background** checkbox needs to be selected,
    while for brightfield images, it needs to be deselected (unless you use darkfield
    illumination methods). The methods available can be set in the drop-down list
    on the left-hand side. The default method is based on the **IsoData** method.
    The **IsoData** method determines the value of the threshold based on the following
    procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Take an initial value for the threshold *T*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the average intensity of the background (*BG*) and the foreground
    (*FG*) pixels based on the value of *T*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the average of *BG* and *FG* in step *ii* is not equal to *T*, increment
    the threshold value *T* and repeat step *ii*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information and references on thresholding methods, refer to the Fiji
    website at [http://fiji.sc/Auto_Threshold#Available_methods](http://fiji.sc/Auto_Threshold#Available_methods)
    for an overview. The drop-down list on the right-hand side gives the option to
    show the effect of the thresholding. When **Red** is selected, the foreground
    selection is displayed as red while the background stays in grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the threshold set, you can create a binary image by pressing
    **Apply** in the threshold window or by going to **Edit** | **Selection** | **Create
    Mask**. The former method will modify your original image, while the latter method
    will open a new window with the thresholded image. The red parts (that is, the
    above-threshold values) in the original image are now white, while the non-red
    parts (that is, the below-threshold values) in the original image are now black.
    Sometimes, the threshold is not perfect and has gaps or holes in places where
    the signal was not even. You will learn how to deal with these issues in the *Morphological
    processing* section. The three stages of this process are shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thresholding grayscale images](img/Insert_image_4909_03b_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The image in the left panel is the original image. The middle panel shows the
    auto threshold with the foreground areas in red. The right panel shows the resulting
    mask that was created based on the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding color images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, color images are more complicated to segment. When talking
    about color images, it is important to distinguish between RGB images and multichannel
    stacks. The latter can be thresholded just fine using the techniques described
    in the previous section. Multichannel stacks can be seen as individual grayscale
    images that were given a specific LUT to appear colored. RGB images, on the other
    hand, are a little more complex. If the image only contains pixels that are red,
    green, or blue, you could convert the image into a multichannel image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To segment an RGB image with more colors, you need to transform the image to
    a different color space. To select the foreground based on color, the HSB color
    space is more convenient. As we saw in the [Chapter 2](ch02.html "Chapter 2. Basic
    Image Processing with ImageJ"), *Basic Image Processing with ImageJ*, the color
    information in HSB images is a separate channel encoded in grayscale values. When
    you want to set a threshold on an RGB color image in ImageJ and Fiji, the **Threshold
    Color** dialog opens automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thresholding color images](img/Insert_image_4909_03b_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By default, it opens in the HSB color space, where the top chart shows the
    distribution of the hue channel. The two sliders underneath indicate which colors
    you wish to select. In this case, orange is selected. The second panel shows the
    controls for saturation. As the sliders are far to the right, we only select bright
    orange colors. Finally, the third panel at the bottom shows the controls for brightness,
    which is set for a wide range of brightness values starting at the mid-level.
    This example shows you how to select a specific range of colors. In this case,
    the threshold was set to select the hair of the clown in the **Clown** sample
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Thresholding color images](img/Insert_image_4909_03b_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the threshold is not perfect. There are small areas on the cheek
    and near the nose that are also within the threshold. Furthermore, there are also
    gaps in the area that are part of the hair, especially around the right eye and
    in the top-right corner of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The threshold method list has the same methods as the standard ImageJ threshold
    dialog, and it works only on the brightness channel. The **Original** button is
    similar to the **Reset** option in the grayscale threshold dialog. The **Select**
    button will convert the thresholded region into a selection. The **Sample** button
    will use a selected portion of the image to generate a threshold based on the
    hue, saturation, and brightness channels in that area.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After segmenting the image into the two components, you are left with a mask
    or binary image. As was clear from the examples, these masks are not always suitable
    for direct measurement. Imperfections in the image may result in gaps in objects
    or small discontinuities in structures. Also, some areas might be detected as
    foreground when they are actually not really objects of interest. You could manually
    correct this by converting the missing pixels to white or black in order to include
    or exclude them, respectively. In some cases, this might be the only possible
    recourse. However, in many cases, there are a few processing steps available that
    can fix these problems in a systematic way. These steps are called morphological
    processing, which we will examine in greater detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Morphological operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ImageJ supports the two main principal operators for morphological processing:
    **erode** and **dilate**. It also has functions for filling holes, skeletonizing,
    and watershedding binary images, which will be discussed in a later section. These
    functions will be explained in the upcoming sections using a few basic examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Erode and dilate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start with, we will look at the basic operators **erode** and **dilate**.
    The erode operator takes a foreground pixel (**FG**) and looks at the surrounding
    pixels in a 3 x 3 neighborhood. Based on the number of FG pixels, the pixel will
    be changed to a background pixel (**BG**), or it stays as an FG pixel. The dilate
    operator functions in the opposite way. ImageJ determines whether a pixel will
    be changed or not based on the binary options, which can be set by going to **Process**
    | **Binary** | **Options…**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Erode and dilate](img/Insert_image_4909_03b_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Iterations** determines how many times the operator is repeated, and **Count**
    determines the number of pixels used for the threshold that determines whether
    a pixel is switched or not. **EDM output** determines where the results from distance
    mapping functions are written. When **Overwrite** is selected, the distance mapping
    overwrites the pixels in your mask image. **Pad edges when eroding** determines
    whether pixels will be eroded when they are located on the edge of the image.
    When selected, there will be no erosion at the edges of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: For the following example, I will assume that the number of iterations is set
    to `1`, the count to `1`, and the black background is unchecked.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `4909_03b_binary.tif` image in ImageJ. It is available on the Packt
    website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a threshold for the image using `0` for the top slider and `75` for the
    bottom slider, using the default method. Leave the dark background unchecked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Edit** | **Selection** | **Create Mask** to generate a new image or
    press **Apply** in the threshold dialog to overwrite the original image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, select the masked image and press *Ctrl* + *Shift* + *I* to invert
    the image so that it has a white background. You should now have the following
    result (the original is on the left-hand side and the mask is on the right-hand
    side):![Erode and dilate](img/Insert_image_4909_03b_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you look closely at the masked image, you will appreciate that there are
    a few small problems. Most notably, the letter **a** in **Binary** and **ImageJ**
    is broken in three disconnected parts. Also, the letters **p** and **g** are not
    entirely complete and have a break but are not completely disconnected. For humans,
    this is not a large problem. We can easily fill in the gaps in our minds and read
    the text. Computers, on the other hand, may have a more difficult time trying
    to decipher the text. We will now look at the effect of binary operators on this
    mask. You will also see how this may solve our problem of fragmented letters.
  prefs: []
  type: TYPE_NORMAL
- en: Select the masked image and go to **Process** | **Binary** | **Options…** to
    open the options dialog. This will show a few more options now that we have a
    masked image, most notably the **Do** drop-down menu and the **Preview** checkbox.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zoom in on one of the letters a using the magnification tool or the *+* key
    on the keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Erode** from the **Do** drop-down menu and check the **Preview** checkbox,
    but don't press OK!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In preview mode, you will notice that the entire mask went white, and the text
    completely disappeared when you selected the erode operator. When you increase
    the value in the **Count** field, you will start to notice that parts of the text
    will start to come back. With a value of `3`, some pixels are visible, while a
    value of `7` or `8` gives you most of the text unscathed. When the value is set
    to `8`, the only victim of the erode operation is the isolated pixel of the letter
    **a**. All the other pixels remain intact, but this isolated pixel is removed.
    This is one of the most used applications of the erode operator—removing isolated
    single pixels caused by noise in your image.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using **Erode**, isolated pixels can be removed, but the entire mask becomes
    smaller, reducing the area that we would like to measure. Using **Dilate** directly
    after an **Erode** operation (or using **Open**), we can remove isolated pixels
    while still preserving the area we would like to measure. Once an FG pixel is
    lost because of **Erode**, it can never return, no matter how many times you use
    **Dilate**!
  prefs: []
  type: TYPE_NORMAL
- en: Now, select **Dilate** from the **Do** drop-down menu, set **Count** to `1`
    again, and see what the effect is. When you use the dilate operator, the text
    will become thicker, but it also fills in the gaps in the letters. This outcome
    is much more useful. However, there are a few problems here. The bottom tail of
    the letter **g** as well as the opening of the letter **e** are now filled in.
    By increasing **Count** to `2`, this problem is ameliorated, and the letter **e**
    as well as the tail of the letter **g** are open again. When **Count** is `2`,
    **Dilate** fixes the problem of the fragments. However, our letters are now much
    thicker, and some letters have merged. Take a look at **eJ** in the word **ImageJ**.
    The tail of the letter **e** is directly connected to the tail of the letter **J**.
    We would now like to take two steps. First, we want to dilate the mask to fill
    the gaps, and then, we want to erode the mask to get rid of the connected letters.
    Executing the operators in succession on the mask can perform this combination.
    First, we will dilate the mask, and then, we will erode the result. However, there
    is also a special function that performs both steps in this order called **Close**.
    If you want to perform the steps in the opposite order (first erode and then dilate),
    you can use the **Open** function.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you select the **Close** option in the drop-down menu, you can see the
    result of this action. The result is OK, but not that great. The fact that the
    result in this case is not that great is caused by the fact that we used different
    values of **Count** for each step. The **Dilate** operator worked best when we
    used `2`, while the **Erode** operator worked best when we used a value of `7`
    or `8`. For this example, it is better to perform the **Dilate** and **Erode**
    operators in succession with specific values for count in each step. In the following
    images, the **Close** operation was used with a value of `2` for count, while
    the succession of **Dilate** and **Erode** were performed using `2` and `5`, respectively
    (the left-hand-side image is the original mask):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Erode and dilate](img/Insert_image_4909_03b_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen in the middle and right panels, both methods have their advantages
    and drawbacks. The **Close** operation (middle panel) filled in the letter **e**,
    and there is still an isolated pixel that is part of the letter **a**. However,
    the letters themselves still have good details. The manual successive dilate/erode
    steps (right panel) preserved the hole in the letter e as well as the details
    of the letter **g**. However, the details of the letter **a** are less pronounced.
    Specifically, the *serif* (the small hook at the bottom-right corner of the letter
    **a**) is completely lost.
  prefs: []
  type: TYPE_NORMAL
- en: Skeletonize and watershed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After processing the mask using **Dilate**, **Erode**, **Open**, and **Close**,
    we may want to reduce the mask to the most basic features. The core of the letters
    that we segmented earlier is formed by the strokes. Each character consists of
    a set of strokes in different directions, which together define the character.
    In ImageJ, we can recreate these strokes using the skeletonize function, which
    can be found by going to **Process** | **Binary** | **Skeletonize** in the menu
    or by selecting it from the **Do** drop-down menu in the binary options dialog.
    **Skeletonize** looks at each pixel's neighbors and removes a pixel if it is flanked
    by other FG pixels. This leads to reducing the mask to a single pixel width mask.
  prefs: []
  type: TYPE_NORMAL
- en: 'When applied to the result of the masks after our close (left panel) and successive
    dilate/erode (right panel) operations, the results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Skeletonize and watershed](img/Insert_image_4909_03b_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result from the **Close** operation (left panel) is not very satisfactory.
    The letter **e** is unrecognizable and looks more like a letter **c**. The successive
    dilate/erode operation (right panel) has a slightly better result due to the skeletonize
    operations. Although the letters look a bit funny and wobbly, all the important
    strokes are present.
  prefs: []
  type: TYPE_NORMAL
- en: The watershed function separates objects that are touching. We will look at
    the effect of this operation using the blobs sample image. You could apply it
    to the text example. However, the problem in the text example was the fact that
    objects needed to be joined not separated.
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Blobs** image from the sample images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a threshold using the **Default** method, leave the **Dark background**
    box unchecked, and click on **Apply** to create the mask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, go to **Process** | **Binary** | **Watershed** from the menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The result will look as follows, with the original mask to the left and the
    result of the watershed operation to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Skeletonize and watershed](img/Insert_image_4909_03b_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, four blobs were split into two separate objects each. This operation
    looks for areas that are pinched. When an object has a narrow part between two
    thicker parts (similar to the middle of an 8), it will be separated along the
    narrow part. Notice, however, that this does not work for some of the blobs (indicated
    by blue rectangles). When there is no pinch in the outline, the watershed algorithm
    will not split the object. This would be very useful if you wish to quantify the
    number of objects when you know that objects can overlap. However, you may run
    into problems if you wish to measure object size or area. As the overlapping area
    cannot be measured accurately, the measurements for overlapping objects will underestimate
    the actual size. This problem can be solved by assuming that the objects have
    a regular shape, such as an oval, but this might not hold in many cases. In ImageJ,
    this latter assumption can be used using the particle analyzer, which will be
    discussed in [Chapter 5](ch05.html "Chapter 5. Basic Measurements with ImageJ"),
    *Basic Measurements with ImageJ*. The best way to solve this problem is by making
    sure that the amount of overlap is reduced, which might require changes in your
    sample preparation or acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: Image filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section looked at ways to segment the image in the foreground and
    background using a threshold. It also looked at ways to derive a result suitable
    for analysis with the use of morphological operators. The morphological operators
    were used to clean the results of the threshold by removing isolated pixels. In
    most real-life applications, these isolated pixels are due to the effect of noise
    in your image-acquisition system. Some of the noise can be removed using the techniques
    described in the previous chapter, but this may not remove all the noise. In this
    section, we will look at ways to use filters to remove noise and prepare images
    to create masks. Filtering can be a step that is inserted before thresholding
    and morphological processing. If your images are high contrast and have extremely
    low levels of noise, this might not be required. However, this is relatively rare.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories of filtering, depending on the type of domain that
    is used for filtering. Images can be seen in two different domains: the **spatial**
    and the **frequency** domain. The most recognizable to humans is the spatial domain.
    This is an image as we recognize it from our cameras. Each location in space has
    a value, and the combination of an area filled with closely spaced locations with
    differing values forms an image. If all the values were identical, the image would
    appear uniform as a single color or gray. In the case of digital images, locations
    are specified by the pixels that form the image, and the value is represented
    as a gray value or an RGB value.'
  prefs: []
  type: TYPE_NORMAL
- en: The frequency domain is less recognizable to humans. An image in the frequency
    domain is represented by the rate of change of values or frequency. Humans recognize
    frequency in terms of wavelengths of light. Light with a higher frequency will
    appear blue/violet, while light with a lower frequency will appear orange/red.
    However, in image processing, the frequency of an image is determined by the way
    pixel intensities change within an image, and not necessarily the color of the
    pixels. I will start with filtering in the frequency domain, as this is more complex.
    Note that most of the filtering for image processing is done in the spatial domain
    with excellent results.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering in the frequency domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Filtering of images is based on a transformation technique described by Joseph
    Fourier in 1822\. This transformation takes data in one domain and transforms
    it to a different domain and back again. For image processing, the transformation
    goes from the *spatial domain* to the *frequency domain*. The spatial domain considers
    points to be in a space, either a plane (2D) or a volume (3D). Each location of
    a point has an intensity value, which changes over different locations for most
    images. The rate at which the intensities change along a dimension determines
    the frequency. Take a look at this artificial image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the profile of intensity along the width of the image as well
    as along the height of the image in the middle, we would get the following results
    (horizontal profile to the left and vertical profile to the right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As is obvious from these plots, there is a clear difference in the rate of intensity
    changes. The horizontal profile (left) shows fast changes in intensity over distance,
    while the vertical profile (right) shows no change whatsoever. Another way of
    describing this is that the frequency along the horizontal profile is large, while
    it is low on the vertical axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fourier transform will calculate the frequencies in the spatial domain
    and plot them as frequencies in the *X* and *Y* direction. The idea of the transform
    is based on the fact that any signal can be described as the (infinite) sum of
    harmonic functions (that is, sines and cosines) with different frequencies. These
    frequencies are represented by the coefficients for the sines and cosines, which
    are displayed as gray values by ImageJ in an image. We will obtain the Fourier
    transform that is, **Fast Fourier Transform** (**FFT**) of the artificial image
    by going to **Process** | **FFT** | **FFT** from the menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The center (that is, origin) of the image has a frequency of 0, while the horizontal
    line through the origin represents the frequencies along the *x* axis of the image.
    The values in the quadrants determine the frequencies along the diagonals of the
    image. Values close to the center of the image represent low frequencies, while
    values close to the edge represent higher frequencies. As there is only a change
    in frequency along the *X* coordinates of the image, the transformed image shows
    only vertical lines. If the pattern had been diagonal, the lines in the transformed
    image would also be diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dashed appearance of the lines in the transformed image is caused by the
    fact that the input image was not square. The width is 512, but the height only
    128 pixels. If the image were a 512 x 512 square, the transformed image would
    only show a row of dots along the *x* axis through the origin. If you halved the
    height of the sample image, the dashes become roughly twice as long.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use the FFT image as input, we can create the original image when we
    select **Process** | **FFT** | **Inverse FFT** from the menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that since we used the FFT and immediately the inversed FFT, we actually
    did not apply any filtering. The image before and after the transform is identical.
    This is a very desirable feature of the transform, because this means that the
    transform is lossless. No information was lost during the process. To actually
    filter the image, we need to modify the transformed image by modifying the pixel
    values in the transformed image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply some (crude) filtering, we will take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the transformed image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to **Edit** | **Selection** | **Specify…** from the menu and enter the following
    values: **Width** as `255`, **Height** as `255`, **X coordinate** as `0`, and
    **Y coordinate** as `0`. Then, press **OK**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the color picker by going to **Image** | **Color** | **Color Picker** from
    the menu or pressing *Ctrl* + *Shift* + *K* on the keyboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that the foreground is set to black by clicking on the little icon
    of a black-and-white square in the bottom-right corner of the color picker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, fill the selection that we specified with black by going to **Edit** |
    **Fill** or by pressing *Ctrl* + *F*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 and 5, but now, specify the selection to have the *X* and *Y*
    coordinates of `257`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, go to **Process** | **FFT** | **Inverse FFT** from the menu to generate
    the filtered image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you followed the instructions, your FFT image would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The inverse FFT image will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen in the inverse FFT image, there are now significant differences
    before and after the manipulations. For instance, the frequency in the vertical
    direction is different. Each bar now changes intensity as you go from top to bottom.
    Try the same routine, but this time, specify the selection using the following
    parameters in step 2, and skip step 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Width**: `64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Height**: `512`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**X coordinate**: `272`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Y coordinate**: `0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After filling in the selection with black and calculating the inverse FFT,
    the image will appear as shown here in the right panel. You have specifically
    removed a small subset of the frequencies from the frequency domain. After calculating
    the inverse FFT, you will get the following result (zoomed area in the top-left
    corner):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering in the frequency domain](img/Insert_image_4909_03b_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On the left-hand side, you see the original image, and on the right-hand side
    is the filtered image. As the region selected and removed entailed the lower frequencies,
    the higher frequencies remain resulting in a greater change in the intensity values
    along the horizontal axis.
  prefs: []
  type: TYPE_NORMAL
- en: As this example is very artificial, the results here are not necessarily practical
    for analysis. However, if you have an image that is corrupted by high-frequency
    intensity changes (for example, imaging noise), you know you have to remove the
    frequencies at the edge of the FFT transform. On the other hand, if you have a
    slow gradient of intensity changes (for example, uneven illumination), you need
    to remove the low frequencies in the FFT transform. Using black to remove the
    frequencies, you're creating a filter that excludes the frequencies covered by
    your selection. If you filled the selection with white, you would include all
    the selected frequencies covered by your selection. In the next section, we will
    look at filtering in the spatial domain, which is slightly more intuitive to apply.
  prefs: []
  type: TYPE_NORMAL
- en: Image filtering in the spatial domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Filtering in the spatial domain involves using a filter, usually referred to
    as a **kernel**. This filter transforms every pixel using a method called **convolution**.
    Convolution involves taking a center pixel with a small array of neighboring pixels
    (usually 3 x 3) and multiplying the intensities with a set of weights as defined
    in the kernel. The sum of the multiplications will become the new pixel intensity
    for the center pixel. In the following example, there is a part of an image (left),
    the kernel (middle), and the outcome of the convolution (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image filtering in the spatial domain](img/Insert_image_4909_03b_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The center pixel (highlighted in orange) and the surrounding pixels in a 3
    x 3 neighborhood are multiplied with the kernel (middle). The result of the convolution
    is shown on the right-hand side. The value of the center pixel used to be 128,
    but is now 78 after convolution. The kernel shown in this example is a simple
    smoothing filter (also called a **box** filter). The main effect of this filter
    is that it averages pixels, resulting in a blurring of the image. The following
    image is a detail from the **Boats** sample image, before (left) and after (right)
    convolving with the box filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image filtering in the spatial domain](img/Insert_image_4909_03b_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When you change the filter size to 7 x 7, the effect of the smoothing will
    be much stronger, as more pixels in the neighborhood will influence the value
    of the center pixel. When using the box filter with a size of 7 x 7, each weight
    will be equal to 1/49\. The result for the same image will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image filtering in the spatial domain](img/Insert_image_4909_03b_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the filtering has almost completely smoothed the letters, making
    them unrecognizable. The box filter functions as a low-pass filter—only low frequencies
    in the image will remain. This is caused by the fact that fast changes in intensity
    will be smoothed out more aggressively by the box filter than the low frequencies.
    Even though this filtering happens in the spatial domain, the effects are also
    reflected in the frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recreate the preceding images, follow these steps on the **Boats** image
    from the sample images:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Process** | **Filters** | **Convolve…** from the menu and remove everything
    in the text field in the dialog that opens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type three 1s separated by a space and press enter. Repeat this twice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that the **Normalize kernel** checkbox is selected and press **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The image now looks a little less sharp as it has been convolved with a 3 x
    3 box filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to convolve with a 7 x 7 box filter, just type seven rows of seven
    1s separated by spaces, and repeat the steps on a newly opened Boats image to
    see the effect of kernel size.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When applying the kernel on an image that was already convolved, the effect
    will be larger than when the image wasn't convolved yet. When using the 3 x 3
    box filter twice in succession, the effect will be the same as running a 3 x 3
    box filter with weights of 1/81 per pixel (*1/9 * 1/9*).
  prefs: []
  type: TYPE_NORMAL
- en: The result of filtering using a kernel depends on the values of the weights
    that you specify and the kernel's size. Typically, there are two types of kernel
    that can be separated based on the sum of their weights. When the sum of the weights
    in a kernel adds up to 1, the kernel is called normalized. The advantage of a
    normalized kernel is that the result of the convolution will not exceed the maximum
    pixel value allowed by the bit depth of the image. When the **Normalized kernel**
    checkbox is checked in the **Convolve** dialog, ImageJ will automatically take
    care of the normalization. Non-normalized kernels can exhibit clamping artifacts.
    When the sum of the kernel exceeds 1, the result of convolution may exceed the
    maximum allowable value (that is, 255 for 8-bit images). When this happens, the
    value after the transformation will be clamped at the maximum value. This clamping
    may result in artifacts such as blocks of white pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The box filter is a very simple filter, but it does not discriminate any features
    in the image. It averages evenly in all directions. Other filters exist that actually
    enhance certain features in your image. An example of such a filter is the **Mexican
    hat** filter. This filter emphasizes the center pixel over the surrounding pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image filtering in the spatial domain](img/Insert_image_4909_03b_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Mexican hat filter is shaped a little like a sombrero, hence the name.
    It makes areas with high contrast become bright, while areas of uniform intensity
    become darker. Applied on the **Boats** image, the result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image filtering in the spatial domain](img/Insert_image_4909_03b_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What stands out immediately is the fact that the edges of the letters are greatly
    emphasized. This makes sense because the contrast is relatively strong. These
    are black letters on a mostly even light-gray background. The only edges that
    are not clearly recognizable are the points where the letters touch each other
    and the places where the ropes hide parts of the letters. You can imagine that
    this filter might also work well for the text example and the segmentation of
    the blobs mentioned earlier. It basically functions as a high-pass filter. Only
    regions with fast changes in intensity are emphasized, while regions with slow
    changes in intensity (that is, low frequencies) are reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Besides entering kernel weights manually, ImageJ and Fiji also have some common
    filter kernels that can be accessed by going to **Process** | **Filters**. Two
    of the most often used filter kernels include the **Gaussian Blur…** and the **Mean…**
    filter. The latter is identical to the box filter. The former is similar to the
    Mexican hat filter. However, it does not use negative values in the kernel. The
    **Gaussian Blur** filter smoothens the image just like a box filter does, but
    it does it in a more gradual way. The advantage of **Gaussian Blur** is that it
    can have fewer artifacts when you apply it. The response of the filter in the
    frequency domain is also better, making it possible to combine spatial and frequency
    domain filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at some operators that can be used to detect specific features
    in an image that may be relevant for processing. These operators also use convolution,
    but they have different properties compared to the filters described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the earlier sections, filters can be used to isolate different
    frequencies using filters. By convolving an image with a Mexican hat filter, high
    frequencies are preserved, while using the box filter has the opposite effect.
    The difference between the filters in this section and the filters in the previous
    section is in specificity. The Mexican hat filter had no preference for direction.
    When there was an edge with sharp contrast (quick change in intensity), the filter
    had a strong effect. However, sometimes, you are only interested in a specific
    type of edge. Let's assume that we only want to detect vertical edges. The Mexican
    hat filter will give us all the edges in all directions, not just the vertical
    ones. This will be the topic of the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Edge detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To detect only vertical edges, we need to create a kernel that emphasizes pixels
    that are in a vertical orientation. The following kernels can detect different
    orientations of edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Edge detection](img/Insert_image_4909_03b_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To perform the **Sobel edge** detection, you can use the **Find Edges** command
    from the **Process** menu. This command will run both the horizontal and the vertical
    Sobel kernel over the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is also the **Canny procedure** for edge detection that involves
    five steps. This procedure was developed by John F. Canny and consists of the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply Gaussian smoothing to remove noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect gradients in the image using edge detection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thin edges using convolution with a kernel such as the Mexican hat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply two different thresholds to determine weak and strong edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove weak edges that are not connected to strong edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first three steps involve using different kernels for smoothing, edge detection,
    and edge thinning in succession. Note that the first step is only required if
    the image is degraded by noise. If the contrast is high and noise is absent, this
    step can be skipped. This step is also the weakest point of the procedure. Both
    noise and edges are forms of high-frequency signals, and the Gaussian filter smoothens
    both equally. If noise is present, techniques that reduce the noise specifically
    while leaving the edges intact should show great improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at ways to separate an image into foreground and
    background. We saw different methods to set the threshold in grayscale and color
    images. We applied filtering in the spatial and frequency domains to aid in cleaning
    the image and extracting edges for further processing. All these steps will help
    us when we wish to measure objects in the image, which is the topic of the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
