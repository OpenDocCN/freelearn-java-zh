<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer140">
<h1 class="chapter-number" id="_idParaDest-109"><a id="_idTextAnchor110"/>7</h1>
<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/>Core Architectural Design Patterns</h1>
<p>In the previous chapters, we learned how to architect data engineering solutions for both batch-based and real-time processing using specific use cases. However, we haven’t discussed the various options available concerning architectural design patterns for batch and real-time stream processing engines.</p>
<p>In this chapter, we will learn about a few commonly used architectural patterns for data engineering problems. We will start by learning about a few common patterns in batch-based data processing and common scenarios where they are used. Then, we will learn about various streaming-based processing patterns in modern data architectures and how they can help solve business problems. We will also discuss the two famous hybrid data architectural patterns. Finally, we will learn about various serverless data ingestion patterns commonly used in the cloud.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Core batch processing patterns</li>
<li>Core stream processing patterns</li>
<li>Hybrid data processing patterns</li>
<li>Serverless patterns for data ingestion</li>
</ul>
<h1 id="_idParaDest-111"><a id="_idTextAnchor112"/>Core batch processing patterns</h1>
<p>In this section, we will look at a <a id="_idIndexMarker757"/>few commonly used data engineering patterns to solve batch processing problems. Although there can be many variations of the implementation, these patterns are generic, irrespective of the technologies used to implement the patterns. In the following sections, we’ll discuss the commonly used batch processing patterns.</p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor113"/>The staged Collect-Process-Store pattern</h2>
<p>The <strong class="bold">staged Collect-Process-Store pattern</strong> is the most common batch processing pattern. It is also <a id="_idIndexMarker758"/>commonly known as the <strong class="bold">Extract-Transform-Load</strong> (<strong class="bold">ETL</strong>) pattern in<a id="_idIndexMarker759"/> data engineering. This <a id="_idIndexMarker760"/>architectural pattern is used to ingest data and store it as information. The following diagram depicts this architectural pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="Figure 7.1 – The staged Collect-Process-Store pattern " height="250" src="image/B17084_07_001.jpg" width="910"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The staged Collect-Process-Store pattern</p>
<p>We can break this pattern into a series of stages, as follows:</p>
<ol>
<li>In this architectural <a id="_idIndexMarker761"/>pattern, one or more data sources are extracted and kept in a form of data storage called a raw zone or landing zone. The landing zone<a id="_idIndexMarker762"/> data is often raw data, which consists of noise such as extra spaces, junk characters, important fields missing, and so on. The extraction or collection job has the responsibility to extract and store the data in the raw zone. The data storage that’s used for the landing zone can vary from <a id="_idIndexMarker763"/>a filesystem, the <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>), an S3 bucket, or some relational database based on the use case and the platform chosen to solve the problem.</li>
<li>The processing jobs read the data from the raw zone and perform a series of transformations on the data, such as data cleansing, data standardization, and data validation. The job stores its output in the intermediate processing zone(s). There can be one or more transformation jobs, as well as multiple intermediate processing zones, based on the project and technology. Sometimes, processing jobs fetch related information from the intermediate data zone to enrich the processed data. In such a scenario, it reads the data from the intermediate processing zone or any external reference database. The final intermediate processing zone contains the data, which is cleansed, transformed, validated, and well organized. </li>
<li>The fetch and load <a id="_idIndexMarker764"/>process picks up the transformed data and loads it into the sorted dataset layer. The sorted dataset layer contains clean and usable data in a specific format that can easily be consumed by downstream applications for data analytics, reference, and so on. The sorted dataset layer is also popularly known as the <strong class="bold">Organized Data Layer</strong> (<strong class="bold">ODL</strong>). There is no <a id="_idIndexMarker765"/>hard and fast rule regarding the type of database or data store used for the sorted dataset layer. However, based on whether<a id="_idIndexMarker766"/> the sorted data will be used for <strong class="bold">Online Transaction Processing</strong> (<strong class="bold">OLTP</strong>) or <strong class="bold">Online Analytical Processing</strong> (<strong class="bold">OLAP</strong>), a database<a id="_idIndexMarker767"/> is chosen. Generally, this pattern is used to ingest and store data for OLAP purposes. </li>
</ol>
<p>The jobs for this<a id="_idIndexMarker768"/> architectural pattern typically run periodically based on a predetermined schedule, such as once daily or once weekly, or every Friday and Wednesday at 8 P.M. One of the advantages of this pattern is that it ingests the data and processes in a series of stages. The output of each stage is stored in an intermediate processing zone, and the next stage fetches data from the output of the previous stage. This staged architecture makes the design loosely coupled. Often, in production, the data processing job fails. In such a situation, we don’t need to rerun the full ingestion pipeline; instead, we can restart the pipeline from the job that failed.</p>
<p>Now, let’s look at a real-world use case where this pattern will be a good fit. A health insurance firm receives tons of insurance claims every day. To process these claims and determine the cost that will be paid by the insurance firm, the data needs to be cleansed, enriched, organized, and stored. In such a use case, this architectural pattern can be used to ingest different kinds of claims, such as medical, dental, and vision, from various sources; then, they can be extracted, transformed, and loaded into ODL. Another example implementation of this pattern was discussed in <a href="B17084_04.xhtml#_idTextAnchor062"><em class="italic">Chapter 4</em></a>, <em class="italic">ETL Data Load – A Batch-Based Solution to Ingesting Data in a Data Warehouse</em>.</p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/>Common file format processing pattern</h2>
<p>Suppose there is a scenario where there are multiple files (say, for example, 25 source files) for the data sources and <a id="_idIndexMarker769"/>the structure of these sources are quite different from each other. Now, the question is, <em class="italic">Can the staged collect-process-store pattern handle such a use case?</em> Yes, it can. But is it optimized to do so? No, it’s not. The problem is that for all 25 different kinds of source<a id="_idIndexMarker770"/> files, we need to have a separate set of transformation logic written to process and store them into a sorted dataset. We may require 25 separate data pipelines to ingest the data. This not only increases development effort but also increases analysis and testing effort. Also, we may need to fine-tune all the jobs in all 25 data pipelines. The <strong class="bold">common file format processing pattern</strong> is well suited to overcome such problems. The following diagram depicts how the common file format processing pattern works:</p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 7.2 – The common file format processing pattern " height="322" src="image/B17084_07_002.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – The common file format processing pattern</p>
<p>This pattern is divided into the following stages:</p>
<ol>
<li value="1">In this architectural pattern, multiple source files with distinctively different source file structures are stored or sent to the landing zone from the sources. The landing zone can be a filesystem, a NAS mount, an SFTP location, or an HDFS location.</li>
<li> A common file format conversion process runs, which takes the different incoming source files and converts them into a uniform structure called a <strong class="bold">common file format</strong>. The job or <a id="_idIndexMarker771"/>pipeline that runs to do this conversion is lightweight. It should not do cleansing or business transformation in this layer. The common file format conversion process writes its output in the common file format zone. </li>
<li>Now that all the files are in the same format or structure, a single set of process and load jobs can run on top of those files present in the common file format zone. The <a id="_idIndexMarker772"/>process and load process can be a single job or a series of jobs that writes the final organized and sorted data into ODL or the sorted dataset layer. The process and load job may write its intermediate result to temporary storage zones if required.</li>
</ol>
<p>Now, let’s look at a real-world scenario. A credit card company wants to generate and provide offers for its customers based on their buying and spending patterns, as well as a set of complex rules. However, transaction data can be received from various kinds of sources, which includes web-based payment gateways, physical transaction gateways, payment apps such as PayPal and Cash App, foreign payment gateways, and various other similar apps. However, the transaction files that are received from all these sources are in different formats. One option is to create a separate set of transformation mappings for each source and apply the rules differently. However, that will cause a lot of development time and costs, as well as a maintenance challenge. In such a scenario, the common file format processing pattern can be used to convert all transaction files coming from different source systems into a common file format. Then, a single set of rule engine jobs can process transactions received from different sources.</p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor115"/>The Extract-Load-Transform pattern</h2>
<p>Previously in this book, we<a id="_idIndexMarker773"/> learned about the classical ETL-based pattern, where we extract the data first, transform and process the data, and finally <a id="_idIndexMarker774"/>store it in the final data store. However, with modern processing capabilities and the scalability that the cloud offers, we <a id="_idIndexMarker775"/>have seen many <strong class="bold">Massive Parallel Processing</strong> (<strong class="bold">MPP</strong>) databases such as Snowflake, Redshift, and Google’s Big Query becoming popular. These MPP databases have enabled a new pattern of data ingestion where we extract and load the data into these MPP databases first and then process the data. This pattern is <a id="_idIndexMarker776"/>commonly known as the <strong class="bold">Extract-Load-Transform</strong> (<strong class="bold">ELT</strong>) pattern or the Collect-Store-Process pattern. This pattern is useful for building high-performing data warehouses that contain a huge amount of data. The following diagram provides an overview of the ELT pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 7.3 – The Extract-Load-Transform (ELT) pattern " height="390" src="image/B17084_07_003.jpg" width="680"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – The Extract-Load-Transform (ELT) pattern</p>
<p>The preceding diagram <a id="_idIndexMarker777"/>depicts the typical flow of an ELT pattern. This can <a id="_idIndexMarker778"/>be described as follows:</p>
<ol>
<li value="1">As shown in the preceding diagram, raw data is extracted and loaded into the MPP database. This data is stored in the staging zone of the MPP database. </li>
<li>Then, using MPP queries and a transformation pipeline, the data is transformed into the final set of tables. These final tables are exposed as a data warehouse. For security purposes, sometimes, views are created and exposed as a data warehouse on top of the tables.</li>
</ol>
<p>Again, let’s look at an example of<a id="_idIndexMarker779"/> how this pattern is used in the industry. As the customer experience continues to rise, businesses face a gap between the data needed to meet customer expectations and the ability to deliver using the current data management practice. Customer 360 involves building a complete and accurate repository of all the structured and unstructured data across the organization related to the customer. It is an aggregation of all customer data into a single unified location so that it can be queried and used for analytics to improve customer experience. To build Customer 360 solutions, we can leverage the power of MPP databases to create a single unified Customer 360 data<a id="_idIndexMarker780"/> warehouse. An example of a Customer 360 design using Snowflake on AWS is shown here:</p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 7.4 – Example of an ELT pattern on AWS " height="527" src="image/B17084_07_004.jpg" width="810"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Example of an ELT pattern on AWS</p>
<p>Here, data from cloud storage, event streams, and third-party sources all land in the staging area of Snowflake (an MPP database). Then, using Snowflake pipelines, data is cleansed, transformed, and enriched and is stored in the final tables to be consumed by the organization as the centralized enterprise data warehouse.</p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor116"/>The compaction pattern </h2>
<p>Data warehouses are not only built <a id="_idIndexMarker781"/>on MPP databases. For big <a id="_idIndexMarker782"/>data needs, a lot of the time, they are built on top of HDFS using Hive as the querying engine. However, in modern pipelines, a lot of data is dumped in the landing zone by real-time processing engines such as Kafka or Pulsar. Although the use case needs our processing jobs to run a few times a day or once daily, the files are landed when any records come in. This creates a different kind of problem. Due to the scenario described earlier, too many small files containing few records are created. HDFS is not designed to work with small files, especially if it is significantly smaller than the HDFS block size; for example, 128 MB. HDFS works better if a smaller number of huge files are stored instead of a huge number of small files. </p>
<p>Eventually, as the small files grow, the query performance reduces, and eventually, Hive is unable to query those records. To overcome this problem, a pattern is commonly used. This is called the<a id="_idIndexMarker783"/> compaction pattern. The following diagram provides an overview of the compaction pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 7.5 – The compaction pattern " height="413" src="image/B17084_07_005.jpg" width="723"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – The compaction pattern</p>
<p>In this architectural pattern, the small files are stored in the landing zone. A batch-based periodical job runs and compacts those small files to create a single large file. In between, it uses the status <a id="_idIndexMarker784"/>and state storage to store job audit information. It is also used to store state information that may be used by subsequent compaction jobs. </p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/>The staged report generation pattern</h2>
<p>We have discussed multiple patterns to<a id="_idIndexMarker785"/> show how data is ingested and stored as a sorted dataset or in a data warehouse. This pattern, on the other<a id="_idIndexMarker786"/> hand, focuses on running data analytics jobs and generating report(s) from the <strong class="bold">ODL</strong> or data warehouse. The following diagram shows the generic architecture of this pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 7.6 – The staged report generation pattern " height="699" src="image/B17084_07_006.jpg" width="878"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – The staged report generation pattern</p>
<p>The staged report <a id="_idIndexMarker787"/>generation pattern consists of primarily two stages and an auxiliary step, as follows:</p>
<ol>
<li value="1"><strong class="bold">Report generation stage</strong>: Various analytics jobs run on top of the sorted data or organized data layer. Such<a id="_idIndexMarker788"/> analytics jobs may even run on the data stored in the data warehouse. These jobs then save the report of the analysis in a reporting database. A reporting database can be a relational database, a NoSQL database, or a search engine such as Elasticsearch. </li>
<li><strong class="bold">Summary generation stage</strong>: The summary reporting jobs fetch data from the reporting database and report the<a id="_idIndexMarker789"/> summary data in the summary database. Summary databases are usually relational databases, data warehouses, or search engines. </li>
<li>Using exporters and connectors, the data present in either the reporting database or the summary database can be visualized using BI tools or used for data science and analytics purposes, or simply used to extract flat files containing reports.</li>
</ol>
<p>Now, let’s look at a real-world scenario where this pattern is suitable. Let’s say that a company has an on-premises data center. Every day, monitoring and resolution logs are generated for all the servers and storage, backup storage, and networking devices present in the data<a id="_idIndexMarker790"/> center. This data is ingested and stored in a data warehouse that contains daily, weekly, and monthly outage and resolution details. Using this data warehouse, the organization wants to generate various reports for the average SLA for various kinds of incidents, the performance or KPI ratio before and after the resolutions, and the team-wise velocity of closing incidents. </p>
<p>Finally, the company wants to generate a summary of all incidents on a weekly, monthly, and quarterly basis. This use case is well-suited for using this pattern. In this use case, we can generate all the reports and store them in a reporting database and generate the summary reports to the summary database. Both general reports and summary reports can be visualized using BI tools such as Tableau by pulling the data from the reporting databases using proper connectors.</p>
<p>In this section, we learned about a few popular batch processing architectural patterns and a few real-world scenarios that can be applied. In the next section, we will cover a few common patterns used for real-time stream processing. </p>
<h1 id="_idParaDest-117"><a id="_idTextAnchor118"/>Core stream processing patterns</h1>
<p>In the previous section, we<a id="_idIndexMarker791"/> learned about a few commonly used batch processing patterns. In this section, we will discuss various stream processing patterns. Let’s get started.</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor119"/>The outbox pattern </h2>
<p>With modern data engineering, monolithic<a id="_idIndexMarker792"/> applications have been replaced by a series of microservices application working in tandem. Also, it is worth noting<a id="_idIndexMarker793"/> that microservices usually don’t share their databases with other microservices. The database session commits and interservice communications should be atomic and in real time to avoid inconsistencies and bugs. Here, the outbox pattern comes in handy. The following diagram shows the generic architecture of the outbox pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 7.7 – The outbox pattern " height="331" src="image/B17084_07_007.jpg" width="971"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – The outbox pattern</p>
<p>As we can see, a microservice (here, <strong class="bold">Service 1</strong>) writes a transaction to not only the required table where online reads and writes happen (denoted in the diagram as <strong class="bold">Online Table</strong>) but also to an <strong class="bold">Outbox Table</strong>, whose structure is where messages to the message broker should be published. Just like the physical trays on office desks that once held outgoing letters and documents, the outbox pattern uses an <strong class="bold">Outbox Table</strong> to send messages to the <a id="_idIndexMarker794"/>message broker. A <strong class="bold">Change Data Capture</strong> (<strong class="bold">CDC</strong>) publisher picks the CDC events from the <strong class="bold">Outbox Table</strong> area and publishes <a id="_idIndexMarker795"/>them to our <strong class="bold">Message Broker</strong>. Downstream services that need data from Service 1 consume the data from the topic.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor120"/>The saga pattern </h2>
<p>The saga pattern is a design pattern that is<a id="_idIndexMarker796"/> used to manage and handle distributed transactions across multiple applications or services successfully. In <a id="_idIndexMarker797"/>a real-world scenario, a single business transaction can never be done with one application or backend service. Usually, multiple applications work in tandem to complete a successful business transaction. However, we need to have an asynchronous, reliable, and scalable way to communicate between these systems. Each business transaction that spans multiple services is called a saga. The pattern to implement such a transaction is called the saga pattern.</p>
<p>To understand the saga pattern, let’s take a look at an e-commerce application. The following diagram shows the workflow of a simplified ordering system in an e-commerce application:</p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 7.8 – A simplified e-commerce ordering system " height="369" src="image/B17084_07_008.jpg" width="914"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – A simplified e-commerce ordering system</p>
<p>As we can see, an ordering system consists of multiple services, each of which has its own set of functions to perform. Essentially, there are three services: the ordering service, the credit management service, and the payment service. For a successful ordering transaction, the ordering service receives the order. If the order is received successfully, it goes to the credit management service, which checks the credit card’s balance and validates the card. If the<a id="_idIndexMarker798"/> credit check is successful, the<a id="_idIndexMarker799"/> system uses the payment service to request payment. If the payment goes through successfully, the order is marked as accepted. If it fails at any stage, the transaction is aborted, and the order gets rejected.</p>
<p>Now, let’s see how the saga pattern is implemented in this situation. Here inter-service communication is decoupled and made asynchronous by introducing a message broker platform to exchange messages between them. The following diagram shows how the saga pattern is used to implement the ordering system for an e-commerce application:</p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 7.9 – The saga pattern applied to implement an ordering system " height="589" src="image/B17084_07_009.jpg" width="929"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – The saga pattern applied to implement an ordering system</p>
<p>Here, the saga pattern is applied to the saga transaction of placing an order. Here, the ordering service stores the orders in a local database. A CDC publisher is used to publish the messages containing the order to <strong class="bold">Topic 2</strong> in the streaming platform. Data sent to Topic 2 is consumed by the credit management service to do the credit check functionality (marked as flow <em class="italic">1</em> in the preceding diagram). The output of the credit check functionality is sent to a local database. The message containing the credit check result is sent from the local database to <strong class="bold">Topic 1</strong>. The ordering service consumes and stores the output for further processing. </p>
<p>If the credit check report is positive, a payment request event is published in <strong class="bold">Topic 3</strong> using the CDC processor (depicted as flow <em class="italic">3</em> in the preceding diagram). The event that’s published in flow <em class="italic">3</em> is <a id="_idIndexMarker800"/>picked up by the payment service and requests payment. The result of the payment request is <a id="_idIndexMarker801"/>saved in the local payment database. The CDC publisher from the payment database produces the payment output to <strong class="bold">Topic 4</strong>, which is denoted as flow <em class="italic">4</em> in the preceding diagram. Using the information that’s been shared over <strong class="bold">Topic 4</strong>, the ordering service determines whether the order was placed or whether it was rejected. One of the interesting things that you can see is that each step of the saga pattern follows the outbox pattern, as described earlier. We can say a series of outbox patterns are knitted in a certain manner to create the saga pattern.</p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor121"/>The choreography pattern</h2>
<p>This pattern is used specifically where <a id="_idIndexMarker802"/>each component independently takes part in the decision-making process to complete a business transaction. All these<a id="_idIndexMarker803"/> independent components talk to a centralized orchestrator application or system. Just like how in choreography, the choreography pattern enables all independent dancers to perform separately and create a wonderfully synchronized show, so an orchestrator orchestrates decentralized decision-making components to complete a business transaction. This is the reason that this pattern is called the choreography pattern. The following diagram provides an overview of the choreography pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 7.10 – The choreography pattern " height="371" src="image/B17084_07_010.jpg" width="801"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – The choreography pattern</p>
<p>As we can see, events from the client are streamed to the topic. Each event contains a specific message header or a message key. Based on the type of message header value or key value, each consuming <a id="_idIndexMarker804"/>application can filter and process the messages required by that application. Once it <a id="_idIndexMarker805"/>processes the event, it generates a result event to the same topic but with a different key or header value. The client consumes all the resulting events to create the final output or decision. </p>
<p>This pattern is useful when you have scenarios where applications may be frequently added, removed, or updated or there is a bottleneck in the centralized orchestration layer. </p>
<p>Let’s take a look at a real-world use case where this pattern may come in handy. A service provider receives different events whenever a client does a recharge. In a single recharge, the client can buy different bundles, such as a top-up bundle, data bundle, and so on. Each bundle adds a message header to the event. Different client applications provide customized offers for each kind of bundle. This use case is suitable for the choreography pattern. Suppose an event comes with both the top-up and data bundles; this will add two pieces of header information, so there will be two consuming applications based on the type of bundle that will be consumed; its own set of offers will be generated and sent back to the client using the topic. It makes sense to use the choreography pattern here as the type of bundles are dynamic, which can vary year-to-year and season-to-season. So, the consuming applications may be frequently added or removed from the ecosystem.</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor122"/>The Command Query Responsibility Segregation (CQRS) pattern  </h2>
<p>This is a very famous pattern, where the<a id="_idIndexMarker806"/> read responsibility and write responsibility is segregated out. This means the data is written to a different data store and read from another data store. While the write data store is optimized for fast writes, the read data store is optimized for fast data reads. The following diagram shows how the CQRS pattern works:</p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 7.11 – The CQRS pattern " height="322" src="image/B17084_07_011.jpg" width="461"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – The CQRS pattern</p>
<p>The preceding diagram depicts how the CQRS pattern works. The flow of this pattern is as follows:</p>
<ol>
<li value="1">First, the producer or publisher writes the event to a topic. </li>
<li>Using a streaming application, this record is streamed into the read database. While the topic is optimized for fast data writes, the read database is optimized for high-performance data reads. This kind of pattern is very useful for scenarios where we need to have a high write as well as high read speed.</li>
</ol>
<p>For example, for a big e-commerce website such as Amazon, the traffic increases heavily on Amazon sale days. In this scenario, there is the possibility of a high number of writes as well as a high number of searches. In this case, various sources such as mobile apps, web portals, and so on will accept orders and update the inventory. Also, offers and discounts are changed hourly <a id="_idIndexMarker807"/>using the Amazon Big Day sale event management portal by sellers and Amazon representatives. Although there will be a high number of reads and writes, customers expect subsecond response times regarding search results. This can be achieved by maintaining separate write and search databases. </p>
<p>Hence, this use case is ideal for<a id="_idIndexMarker808"/> the CQRS pattern. Here, when a customer searches, data is fetched from the search database, and when the customer orders or adds something to the cart, it is written to the write database. The information available in the write database will be streamed in real time to a search database such as Elasticsearch or AWS OpenSearch. So, users who are searching for products and discounts should get the search results in a fraction of a second.</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor123"/>The strangler fig pattern  </h2>
<p>The strangler fig pattern derives its <a id="_idIndexMarker809"/>name from a species of tropical fig plants that grow around their host trees, slowly strangling the host tree so that it dies. This pattern was first proposed by Martin Fowler. Although the basic pattern may be<a id="_idIndexMarker810"/> implemented in different ways, the streaming pipeline gives us an indigenous way to use this pattern. To understand this pattern, let’s look at an example.</p>
<p>Suppose there is a monolithic application that consists of three modules – A, B, and C. A, B, and C read and write data to the database. Initially, the architecture looked as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 7.12 – Initial state of the monolithic application " height="221" src="image/B17084_07_012.jpg" width="586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Initial state of the monolithic application</p>
<p>As we can see, all the modules <a id="_idIndexMarker811"/>have double-ended arrows, denoting both reads and writes are happening. Now, using the strangler fig pattern, we <a id="_idIndexMarker812"/>can convert this monolithic legacy application into a microservices-based application by slowly migrating the individual modules as separate microservices – one at a time. The following diagram shows that <strong class="bold">Module A</strong> is being moved to the microservices pattern from the monolithic app:</p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 7.13 – Module A replaced with Microservice A " height="459" src="image/B17084_07_013.jpg" width="586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Module A replaced with Microservice A</p>
<p>As we can see, <strong class="bold">Microservice A</strong> (which has successfully replaced <strong class="bold">Module A</strong>) reads and writes data to an event stream. This <a id="_idIndexMarker813"/>event stream is, in turn, connected<a id="_idIndexMarker814"/> to the database using an event source or sink connector. Slowly, the monolithic application will be strangled, and the final transformed architecture will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<img alt="Figure 7.14 – All modules migrated using the strangler fig pattern " height="980" src="image/B17084_07_014.jpg" width="586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – All modules migrated using the strangler fig pattern</p>
<p>As we can see, all the modules have been migrated from the monolithic application to the federated microservice <a id="_idIndexMarker815"/>pattern, allowing the<a id="_idIndexMarker816"/> monolithic application to retire.</p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor124"/>The log stream analytics pattern </h2>
<p>In this pattern, we will learn how logs <a id="_idIndexMarker817"/>collected across <a id="_idIndexMarker818"/>various apps, web portals, backend services, and IoT devices are used for analytics and monitoring. The following diagram shows a typical log streaming pattern used to facilitate log analytics and monitoring:</p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<img alt="Figure 7.15 – The log stream analytics pattern " height="354" src="image/B17084_07_015.jpg" width="774"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – The log stream analytics pattern</p>
<p>Let’s learn how this pattern works:</p>
<ol>
<li value="1">As evident from the preceding diagram, all log events from various IoT devices, apps, web portals, and services are streamed into an event stream. </li>
<li>Then, using the <a id="_idIndexMarker819"/>event sink connector, the events are sent to both a search database and a querying database. </li>
</ol>
<p>A search database can be a search engine such as Elasticsearch, AWS OpenSearch, Splunk, or Apache Solar. This database facilitates quick searches with complex query patterns. It also enables visualization and analytics using the capabilities of the search engine. The query database is either <a id="_idIndexMarker820"/>an MPP database such as Redshift or Snowflake or a query engine such as Athena. A query engine allows users to run SQL queries on top of ObjectStores such as S3 objects.</p>
<p>The following diagram shows a sample implementation of this kind of pattern in AWS:</p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="Figure 7.16 – Example of a log analytics pattern in AWS " height="346" src="image/B17084_07_016.jpg" width="799"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Example of a log analytics pattern in AWS</p>
<p>Here, log events from various <a id="_idIndexMarker821"/>AWS services such as EC2, ECR, EKS, and others are streamed to a Kinesis topic using Kinesis Firehose. Kinesis Analytics transformation is done and, using Kinesis Firehose, streamed to AWS OpenSearch for search and analytics purposes. On the other <a id="_idIndexMarker822"/>hand, data gets streamed into S3 from the first Kinesis Firehose. Athena tables are created on top of the S3 objects. Athena then provides an easy-to-use query interface for batch-based analytic queries to be performed on the log data.</p>
<p>In this section, we learned about various streaming patterns that are popular and can be used to solve common data engineering problems. We also looked at a few examples and learned when these patterns should be used. Next, we will investigate a few popular patterns that are a mix of both batch and stream processing. These are known as hybrid data processing patterns.</p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor125"/>Hybrid data processing patterns</h1>
<p>In this section, we will discuss <a id="_idIndexMarker823"/>two very famous patterns that support both batch and real-time processing. Since these patterns support both batch processing and stream processing, they are categorized as hybrid patterns. Let’s take a look at the <a id="_idIndexMarker824"/>most popular hybrid architectural patterns.</p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor126"/>The Lambda architecture  </h2>
<p>First, let’s understand the need for <a id="_idIndexMarker825"/>Lambda architecture. In distributed computing, the CAP theorem states that any distributed data can guarantee only two out of the three features of the data – that is, consistency, availability, and partition tolerance. However, Nathan Marz proposed a new pattern in 2011 that <a id="_idIndexMarker826"/>made it possible to have all three characteristics present in a distributed data store. This pattern is called the Lambda pattern. The <a id="_idIndexMarker827"/>Lambda architecture consists of three layers, as follows:</p>
<ul>
<li><strong class="bold">Batch layer</strong>: This layer is <a id="_idIndexMarker828"/>responsible for batch processing</li>
<li><strong class="bold">Speed layer</strong>: This layer is<a id="_idIndexMarker829"/> responsible for real-time processing</li>
<li><strong class="bold">Serving layer</strong>: This layer serves<a id="_idIndexMarker830"/> as the unified serving layer where querying can be done by downstream applications</li>
</ul>
<p>The following diagram shows an overview of the Lambda architecture:</p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 7.17 – The Lambda architecture " height="437" src="image/B17084_07_017.jpg" width="866"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – The Lambda architecture</p>
<p>In the Lambda architecture, the input <a id="_idIndexMarker831"/>data or the source data is written to the master data store present in the batch layer, as well as the event streams present in the speed layer. The master data store may be a relational or NoSQL database or a filesystem such as HDFS. Batch processing jobs run on top of this data store to do any data processing, as well as to load the data into batch views present in the serving layer. Events written in the event stream are picked up, processed, and loaded into the real-time view by stream processing<a id="_idIndexMarker832"/> jobs (as shown in <em class="italic">Figure 7.17</em>). Queries can be made separately to query batch views and real-time views, or they can be queried simultaneously on both views to view the results. Batch views are mainly for historical data, while real-time views are for Delta data.</p>
<p>Although it solves the problem of eventual consistency as queries can combine data from both real-time views and batch-based views, it comes with a few shortcomings. One of the major shortcomings is that we must maintain two different workflows – one for the batch layer and another for the speed layer. Since, in a lot of scenarios, the technology to implement a streaming application is quite different from a batch-based application, we must maintain two different source codes. Also, debugging and monitoring for both batch and stream processing systems becomes an overhead. We will discuss how to overcome these challenges in the next pattern.</p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/>The Kappa architecture </h2>
<p>One of the reasons the Lambda architecture is <a id="_idIndexMarker833"/>widely accepted is because it can overcome the limitation of the CAP theorem and enable more <a id="_idIndexMarker834"/>use of stream processing across the industry. Before the Lambda architecture, businesses were skeptical to use stream processing as they feared losing messages during real-time processing. However, this assumption is not true with modern distributed streaming platforms such as Kafka and Pulsar. Let’s take a look at how the Kappa architecture provides a simpler alternative to the Lambda architecture. The following diagram depicts the Kappa architecture:</p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 7.18 – The Kappa architecture " height="433" src="image/B17084_07_018.jpg" width="887"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – The Kappa architecture</p>
<p>In the Kappa architecture, the idea is not to use two different flows – one for batch and one for streaming. Instead, it proposes all processing should be done using a stream processing engine. This <a id="_idIndexMarker835"/>means that both batch-based workloads and stream-based workloads can be handled by a single pipeline. Here, input data or source data is written in a special event stream. This event stream is an immutable append-only transaction log. Since it is an append-only log, it has fast writing capabilities. To read data, we can read from the offset where the data read stopped earlier. In addition to this last read offset, it should support replayability, which means we can read from the first message as well. </p>
<p>Since we are talking about distributed computing, this transaction log will be partitioned, which will improve the read and write performance as well. Stream processing jobs read the events, process them, and write the output to unified views (containing both batch and real-time data). One question that comes to mind is, <em class="italic">How does this kind of flow support high-volume batch loads?</em> Huge volumes of data are also sent to the transaction log, which is then picked up by stream processing jobs. The output is stored in the view present in the serving layer. To process such a high-volume event stream, we need to do more parallelism by increasing the number of partitions in the event stream.</p>
<p>This event stream log should have retention capabilities. Also, consumers should be able to replay the stream using either the event time or event offset. Each event has an offset and an event <a id="_idIndexMarker836"/>timestamp. The replayability feature allows consumers to re-read already fetched data by setting the event offset or event timestamp to an older value. </p>
<p>So far, we have <a id="_idIndexMarker837"/>discussed commonly used batch-based, real-time, and hybrid architectural patterns. In the penultimate section, we will quickly look at a few common serverless patterns.</p>
<h1 id="_idParaDest-127"><a id="_idTextAnchor128"/>Serverless patterns for data ingestion</h1>
<p>We will start by answering the question, <em class="italic">What is serverless computing?</em> Serverless computing is a cloud execution model in which a cloud provider takes care of allocating resources such as storage and compute based on demand while taking care of the servers on behalf of <a id="_idIndexMarker838"/>customers. Serverless computing removes the burden of maintaining and managing servers and resources associated with it. Here, the customers of serverless computing don’t care how and where the jobs or applications are running. They just focus on the business logic and let the cloud provider take care of managing the resources for running and executing that code. A few examples of serverless computing are as follows: </p>
<ul>
<li><strong class="bold">AWS Lambda Function or Azure Function</strong>: This is <a id="_idIndexMarker839"/>used to run <a id="_idIndexMarker840"/>any application or service</li>
<li><strong class="bold">AWS Glue</strong>: This is <a id="_idIndexMarker841"/>used to<a id="_idIndexMarker842"/> run big data-based ETL jobs</li>
<li><strong class="bold">AWS Kinesis</strong>: This is a serverless <a id="_idIndexMarker843"/>event streaming and<a id="_idIndexMarker844"/> analytics platform</li>
</ul>
<p>Although there are many useful serverless patterns, in this section, we will discuss the two most relevant patterns that can help us architect data engineering solutions. The following are the serverless patterns that we will be discussing:</p>
<ul>
<li><strong class="bold">The event-driven trigger pattern</strong>: This is a<a id="_idIndexMarker845"/> very common pattern that’s used in cloud architectures. In this pattern, upon <a id="_idIndexMarker846"/>creating or updating any file in object storage such as an S3 bucket, a serverless function gets triggered. The following diagram provides an overview of this pattern:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 7.19 – The event-driven trigger pattern " height="289" src="image/B17084_07_019.jpg" width="603"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – The event-driven trigger pattern</p>
<p>In this pattern, any change to an object, such as it being created or deleted, in object storage can<a id="_idIndexMarker847"/> trigger a serverless function. This serverless function may either directly process the data or be used to trigger a <a id="_idIndexMarker848"/>big data job. Serverless functions such as AWS Lambda and Azure Function can set triggers that can trigger them. For example, a Lambda function can be configured to have an S3 trigger from Bucket1 for any new object being created or updated in Bucket1. The triggered Lambda <a id="_idIndexMarker849"/>function can, in turn, trigger an EMR job or a serverless Glue job, which transforms and processes the necessary data and writes the final output to a data store. Alternatively, the Lambda function can do some data processing and store the output result in the final data store. The final data store can be a SQL database, NoSQL database, MPP database, or object storage such as AWS S3.</p>
<p>A real-world scenario for using this pattern and its solution was explained in detail in <a href="B17084_05.xhtml#_idTextAnchor074"><em class="italic">Chapter 5</em></a>, <em class="italic">Architecting a Batch Processing Pipeline</em>.</p>
<ul>
<li><strong class="bold">The serverless real-time pattern</strong>: This is an<a id="_idIndexMarker850"/> oversimplistic serverless pattern<a id="_idIndexMarker851"/> that is quite popular for data ingestion in the cloud. An overview of this pattern can be seen in the following diagram:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 7.20 – The serverless real-time pattern " height="500" src="image/B17084_07_020.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – The serverless real-time pattern</p>
<p>In the serverless real-time pattern, event or data streaming, as well as data processing, happens using serverless services in the cloud. Events, logs, and messages from different source systems publish the events to a serverless data streaming <a id="_idIndexMarker852"/>platform such as AWS Kinesis. The data stream triggers one or a series of serverless functions chained one after <a id="_idIndexMarker853"/>another to do the data processing on the fly. Once the data has been processed, it is written back to a final data store. The final data store can be SQL, NoSQL, an MPP database, object storage, or a search engine.</p>
<p>A real-world example where this pattern may be used is in a real-time fraud detection system for credit card usage. The following diagram depicts a sample solution of fraud detection in AWS using this pattern:</p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 7.21 – A sample implementation of the serverless real-time pattern in AWS " height="410" src="image/B17084_07_021.jpg" width="718"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 – A sample implementation of the serverless real-time pattern in AWS</p>
<p>Here, API Gateway streams real-time credit card transactions directly into Kinesis Data Streams (a serverless data streaming platform). The transaction events written in Kinesis Data Streams trigger the Lambda function to perform fraud and anomaly detection on the event. The Lambda function makes use of AWS SageMaker, which, in turn, uses the already stored data science models stored in S3 to determine fraud and anomalies in the transaction. The output is then passed to Kinesis Data Firehose, which captures the result from the Lambda function and stores it in an S3 bucket. This S3 bucket contains the results in real time. We can use a service such as Amazon QuickSight to visualize the results and take any <a id="_idIndexMarker854"/>action if required.</p>
<p>With that, we have discussed what serverless computing is and discussed two highly used patterns for serverless computing for data ingestion. Now, let’s summarize what we learned in this chapter.</p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>Summary</h1>
<p>In this chapter, we started by discussing various popular batch processing patterns. We covered five commonly used patterns to solve batch processing problems. We also looked at examples of those patterns and real-world scenarios where such patterns are used. Then, we looked at five popular patterns available to architect stream processing pipelines and how they are used to solve real-world problems in data engineering. Next, we learned about the Lambda and Kappa architectures and how they are useful for both batch and stream processing. Finally, we learned what serverless architecture is and looked at two popular serverless architectures that are used to solve many data engineering problems in the cloud.</p>
<p>At this point, we know how to implement batch and streaming solutions, as well as have a fair idea of different data engineering patterns that are commonly used across the industry. Now, it is time to put some amount of security and data governance into our solutions. In the next chapter, we will discuss various data governance techniques and tools. We will also cover how and why data security needs to be applied to data engineering solutions.</p>
</div>
</div>
</body></html>