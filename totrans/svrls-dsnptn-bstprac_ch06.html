<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Asynchronous Processing with the Messaging Pattern</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Asynchronous Processing with the Messaging Pattern</h1>
                </header>
            
            <article>
                
<p>In the last chapter, we discussed the Fan-out Pattern, which we can implement using different strategies. At the end of that section, we reviewed an&#160;implementation of the Fan-out Pattern, which used&#160;<span>AWS's Simple Queuing Service (SQS) as a destination for an event trigger. Queuing systems such as SQS provide a level of safety and security because they're intended to be a mostly durable persistent store where data lives until some process has the chance to pull it out, perform some work, and delete the&#160;item. If a downstream worker processes a crash entirely and processing stops for some time, queues merely back up, drastically reducing the risk of data loss. If a worker process runs into some unrecoverable problem in the middle of processing, queue items will typically be left on the queue to be retried by another processor in the future.</span></p>
<p>In this&#160;chapter, we will cover using queues as messaging systems to glue together multiple serverless components. Readers can already be familiar with queuing systems such as RabbitMQ, ActiveMQ, or SQS. We will learn how to pass messages between serverless systems using queues to provide durable and fault-tolerant distributed systems for data-heavy&#160;applications.</p>
<p>At the end of this chapter, you can expect to understand the following topics:</p>
<ul>
<li>What queuing systems are available and make sense in a serverless architecture</li>
<li>Options for processing messages using serverless functions (polling&#160;and fan-out)</li>
<li>Differences between queues and streaming systems and when to use one over the other</li>
<li>Dead letter queues to ensure messages are never dropped</li>
<li>Using queues as a way of rate limiting</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basics of queuing systems</h1>
                </header>
            
            <article>
                
<p>Queuing systems are by no means new in the world of software. Generally speaking, queues are one of the fundamental data structures most introductory computer science&#160;courses cover. Before going any further, let's briefly review the queue as a fundamental data structure in computer science.</p>
<p>Simply put, a queue is a collection of items where new items are pushed onto the back and pulled off the front. Consider that we're all waiting in line for a movie. Provided people follow the rules and don't line up out of order, you've waited in a queue (which is, of course, the reason British English uses <em>queue,</em>&#160;which is more accurate than the U.S. term <em>line</em>). Formally, we can define&#160;<span>a queue as a collection of items that have the property of first-in-first-out (FIFO). The primary operators of a queue data type are</span> <kbd>enqueue</kbd>&#160;<span>and</span> <kbd>dequeue</kbd><span>. These operators add new items to the back of the queue and pop items off the front, respectively.</span></p>
<p>In software, queueing systems such as RabbitMQ and the like are commonly used to deal with asynchronous processing. A triggered event can mean that your system needs to perform some recalculations or data processing that doesn't need to occur in real time. Rather than having a user sit and wait until they click a button, an application will place metadata into a queue that contains enough information for a downstream worker process to do its job. These worker processes' sole responsibility is to sit and wait until a new item arrives and then carry out some computation. As messages show up in the queue, workers pluck off those messages, do their work, and return for more. This architecture has multiple benefits:</p>
<ul>
<li><strong>Durability</strong>: Provided clients write data to the queue successfully and the queuing system is healthy, messages will persist in the queue until there is enough computing power available to pull them off, process, and finally remove them.</li>
<li><strong>Scalability</strong>: Most queuing architectures are parallelizable, meaning multiple workers can pull messages from a queue and process individual items in parallel. To operate with more throughput, we can add more workers&#160;into the system, which results in faster processing through greater parallelism.</li>
<li><strong>Predictable load:</strong> Often, worker processes will need to read and/or write data from/to a database. When the load is exceptionally high, the queue can serve as a buffer between the processing tasks and database. To limit pressure on a database we can scale the number of worker processes such that the parallelism is as high as possible, but not so much we overwhelm the database with an inordinate amount of reads or writes.</li>
</ul>
<div class="packt_infobox">Readers should note that they can implement a Messaging Pattern with either a queue or streaming system. In this chapter, we focus on queues but later discuss the merits of stream systems and the differences between the two types of message broker. In subsequent chapters, we will work through the details of streaming systems.</div>
<p>One of the most dangerous spots to be in is when all the incredible benefits of a queue are in place, only to have the actual queue server (RabbitMQ and so on) running as a single node. I have worked at multiple companies that have relied quite heavily on RabbitMQ as the queueing backbone, running very high business-critical workloads from it. However, we ran&#160;these RabbitMQ deployments as a single EC2 instance with lots of computing capacity. Inevitably, when an individual instance runs into problems or for some reason dies, the entire system falls apart, resulting in lost messages, failing clients who attempt to write the queue and error out, and an all-around bad day.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing a queue service</h1>
                </header>
            
            <article>
                
<p>The good news today is that multiple cloud providers now offer queuing systems as a service. In the following examples, we'll use SQS from AWS. While I haven't worked with them directly, Google Compute Cloud has Task Queue, and Azure has Queue Storage. Undoubtedly, other cloud providers offer similar services. When evaluating hosted queuing services, there are several factors to consider:</p>
<ul>
<li>What type of data fetching model is supported, pull or push?</li>
<li>What is the maximum lifetime of messages or the maximum queue depth?</li>
<li>What happens with messages that consistently fail? Is there a dead-letter queue option?</li>
<li>Is there a guarantee of exactly-once delivery, or can messages be delivered multiple times?</li>
<li>Is ordering guaranteed, or can messages arrive out of order relative to the order from which they were sent?</li>
</ul>
<p>Answers to these questions will vary by provider and by service offering for a given cloud provider. SQS, for example, comes in two flavors: Standard Queues and FIFO Queues. Which one you'll pick when building on top of AWS will come down to your particular use case. If building with a different cloud provider, you'll need to dig into their documentation to fully understand the behavior and semantics of whatever queueing service you're using.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Queues versus streams</h1>
                </header>
            
            <article>
                
<p>You can think of a queue as a broker of messages. Some data producer will place messages onto the queue, and some data consumer will read those messages. The queue simply brokers that exchange of message passing. Stream processing systems provide similar functionality, but with much different behavior, features, and applications. I'll present a brief discussion of the differences between queues and streams for the sake of clarity.</p>
<p>Apache Kafka is a very popular stream processing system in widespread use, of which you can have heard. Today, cloud providers have come out with hosted stream processing systems:</p>
<ul>
<li><strong>Azure</strong>: Event Hubs</li>
<li><strong>AWS</strong>: Kinesis</li>
<li><strong>Google Compute Cloud</strong>: Cloud Dataflow</li>
</ul>
<p>So, what exactly is a stream processing system&#160;as opposed to a queueing system? To my mind, the most significant and most easily understood difference is the way in which items are processed or delivered. In a queuing system, messages that arrive in the queue are typically processed&#160;once, by a single process. There are of course exceptions to this rule, but in all systems I've worked on that use a queue, the <em>happy path</em>&#160;was designed such that a single message would be read and processed once.</p>
<p>Streaming systems, on the other hand, can be thought of as a collection of records where old records eventually expire off the back (the oldest expiring first), and new records are added to the front. Rather than being processed and removed, messages sit there, without knowledge of who is reading them and without being deleted by consumers. Data consumers are responsible for keeping track of their position within the stream using an offset value. The streaming service itself is responsible for retaining the messages, generally with some configurable expiration period:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="images/fc1b1e00-3624-4611-8c52-b6dac6bd25bb.png" style="width:27.17em;height:18.08em;"/></div>
<p>It is possible to have multiple consumers reading and processing the same message when using a streaming system. For example, one consumer can be reading items on the stream and calculating a running average of some metric, while another consumer can be reading the same messages and calculating the standard deviation. In a queuing system, this wouldn't be possible without duplicating messages across different queues or implementing some logic or heuristic to remove messages only after all consumers have done their job. Either way, a queue would not be a good fit for such a problem whereas streaming systems are purpose-built just for this.</p>
<p>Another exciting feature of streaming systems is that new consumers who come online can start at the back of the stream and work forward. For example, if a stream holds one week's worth of data, any new system that starts will be able to go back seven days and begin its processing from there. Because consumers keep track of their location or offset within the stream, they can pick up where they left off in the case of failure.</p>
<p>Technically speaking, you can implement the Messaging Pattern with a queue or a stream, and the choice depends on the problem at hand. We'll look at AWS Kinesis and discuss streaming systems in later chapters. For now, we'll focus on using queues and specifically SQS for the example application. In my mind, a Message Pattern at its core entails separating the communication between different system via some message broker, such as a queue or streaming system.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Asynchronous processing of Twitter streams</h1>
                </header>
            
            <article>
                
<p>Twitter is an excellent source of random data. Given the volume and variety of data, we can readily come up with example (and real) problems to solve. In our case, we're going to build a serverless processing system by sipping off the public twitter stream. Our example system will &#160;have the following workflow:</p>
<ol>
<li>Read a tweet with cat or dog images from the Twitter firehose</li>
<li>Place messages on an SQS queue.</li>
<li>Worker processes will read those image URLs off the queue and perform image recognition.</li>
</ol>
<p>While this example can be a bit contrived, the concepts demonstrated are true to life. We'll use the AWS Rekognition service to perform image recognition and labeling of any cat or dog images we find. Rekognition is quite fast at what it does, but it's easy to imagine processing images with a much slower service. In that case, adding items onto a queue and processing them at our leisure with one or more worker processes would allow us to scale out to achieve a higher processing rate.</p>
<div class="packt_infobox">You can find all code in this chapter at&#160;<a href="https://github.com/brianz/serverless-design-patterns/tree/master/ch6">https://github.com/brianz/serverless-design-patterns/tree/master/ch6</a>.<a href="https://github.com/brianz/serverless-design-patterns/tree/master/ch6"></a></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">System architecture</h1>
                </header>
            
            <article>
                
<p>The system architecture for our image analysis example is quite simple. Ingestion of tweets will begin using a stream listener for the Twitter API via the Python <kbd>tweepy</kbd>&#160;library,&#160;<a href="https://github.com/tweepy/tweepy">https://github.com/tweepy/tweepy</a>. This listener will filter out only specific tweets on our behalf. From there, the listener will place messages&#160;onto an SQS queue. Once it delivers messages to the queue, the job of our stream listener is complete. With this type of design, we realize a real separation of concerns. If we enumerated the things our stream listener cares about, the list would be quite short:</p>
<ul>
<li>Twitter access</li>
<li>Some business logic as to what types of data to extract from tweets</li>
<li>Which queue to place the extracted tweet information in</li>
</ul>
<p>That is it. Once the stream listener performs the last step of adding items onto the queue, it neither cares about nor is affected by, any downstream processing or lack thereof.</p>
<p>From there, worker processes will pull images off the queue and perform their bit of work, ultimately calling AWS Rekognition and storing the results in DynamoDB for future review. Our example will use a classifier processor, which will run with a single level of parallelism. That is, at any given time there will only be a single classifier process running. However, there would be very few changes if we wished to scale this out and operate multiple classifiers at the same time, increasing our parallelism and hence the overall throughput of the system.</p>
<p>Again, with this design, the classifier's job is much simpler than if we implemented all of this work as a single process. Classifiers also care about a small number of items to perform their work:</p>
<ul>
<li>Which queue to get data from</li>
<li>A small bit of business logic to perform the image classification</li>
<li>Where to put the results</li>
</ul>
<p>Our classifier neither knows nor cares how data arrived in the queue. All that matters from the classifier's perspective is that data comes with the correct format (which is quite simple) and that is has access to the resources it needs to perform its job.</p>
<p>With a queue acting as the broker between data producer (stream listener) and data consumer (classifier), we have a reasonably good level of durability. If our listener dies (which it will by design, as you'll see shortly) or if our classifiers die, SQS will hold onto our data, ensuring we can get to it and process it when our systems are back to full health. Also, we'd be able to scale this up as needed, adding more classifiers in the event that the stream listener produced more messages than a single classifier could keep up:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/e7a8be1d-12e9-4613-b019-4c371afb0eb2.png" style="width:36.58em;height:14.33em;"/></div>
<p class="mce-root">Messages are placed onto an SQS queue by a stream listener process, which is implemented as an AWS Lambda function and runs on a schedule. A single classifier Lambda function also runs on a schedule and is responsible for pulling messages from the queue, classifying them with AWS Rekognition and finally storing results in DynamoDB.</p>
<p>Furthermore, this design allows for a level of flexibility that would be difficult otherwise. Our example is processing tweets tagged with&#160;<kbd>#cat</kbd> or <kbd>#dog</kbd>&#160;and a few other related hashtags. We could also modify our stream processor to grab a more extensive set of tweets, perhaps directed at <kbd>@realDonaldTrump</kbd>. Those tweets could be directed to an entirely different queue, which would be processed separated and completely different. Since the volume of <kbd>@realDonalTrump</kbd> tweets is much higher than <kbd>#cat</kbd> and <kbd>#dog</kbd> tweets, separating them out and handling them differently would be an excellent idea from a systems architecture perspective.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data producer</h1>
                </header>
            
            <article>
                
<p>Most of the complexities in this code revolve around the Twitter API, which I won't go into in detail. I haven't worked with the Twitter API much myself, but the tweepy GitHub page and website have plenty of resources and example code to get you started, which is just what I followed to get this working. The following code is the entry point to the entire process, which begins reading the public Twitter stream for tweets related to cats or dogs and placing a subset of each tweet onto the SQS queue:&#160;</p>
<pre style="padding-left: 30px">import os<br/>import tweepy<br/><br/>from .queue import publish_tweet<br/><br/>consumer_key = os.environ['TWITTER_CONSUMER_KEY']<br/>consumer_secret = os.environ['TWITTER_CONSUMER_SECRET']<br/><br/>access_token = os.environ['TWITTER_ACCESS_TOKEN']<br/>access_token_secret = os.environ['TWITTER_ACCESS_SECRET']<br/><br/><br/>class PhotoStreamListener(tweepy.StreamListener):<br/><br/>    def _get_media_urls(self, media):<br/>        if not media:<br/>            return []<br/><br/>        return [m['media_url_https'] for m in media if m['type'] == <br/>        'photo']<br/><br/>    def _get_hashtags(self, container):<br/>        hashtags = [h['text'] for h in container.get('hashtags', ())]<br/>        return [h for h in hashtags if '#' + h in self.tags]<br/><br/>    def on_status(self, tweet):<br/>        container = tweet._json<br/><br/>        entities = container.get('entities', {}).get('media')<br/>        extended_entities = container.get('extended_entities', <br/>        {}).get('media')<br/>        extended_tweet = container.get('extended_tweet', <br/>        {}).get('entities', {}).get('media')<br/><br/>        all_urls = set()<br/>        for media in (entities, extended_entities, extended_tweet):<br/>            urls = self._get_media_urls(media)<br/>            all_urls.update(set(urls))<br/><br/>        hashtags = self._get_hashtags(container.get('entities', {}))<br/><br/>        if all_urls:<br/>            for url in all_urls:<br/>                publish_tweet({<br/>                    'text': tweet.text,<br/>                    'url': url,<br/>                    'hashtags': hashtags,<br/>                })<br/><br/>    @staticmethod<br/>    def start(tags=None):<br/>        tags = tags or ['#dog', '#dogs', '#puppy', '#cat', '#kitty', <br/>        '#lolcat', '#kitten']<br/><br/>        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)<br/>        auth.set_access_token(access_token, access_token_secret)<br/><br/>        api = tweepy.API(auth)<br/><br/>        listener = PhotoStreamListener()<br/>        listener.tags = tags<br/>        stream = tweepy.Stream(auth=api.auth, listener=listener)<br/>        try:<br/>            stream.filter(track=tags)<br/>        except Exception as e:<br/>            print 'Shutting down'<br/>            print e</pre>
<p>Let's begin by looking at the <kbd>start</kbd> function, which does what you'd expect. Once the listener class is instantiated and begins running, it will operate as a long-lived daemon process invoking the <kbd>on_status</kbd> function whenever it encounters a tweet. Since we are only interested in certain types of message, I'll pass a list of tags to the <kbd>filter</kbd> function.</p>
<p>All of our application logic is wrapped up in the <kbd>on_status</kbd> method. Tweets are a reasonably elaborate data structure, and the actual image URLs we're interested in can live in multiple locations. As a Twitter API novice, I'm not entirely sure of the exact logic to look for image URLs, but the little bit of logic in <kbd>on_status</kbd> seems to get enough images for our example. After grabbing as many image URLs as we can along with some extracted hashtags, we will publish that data structure to our SQS queue using our <kbd>publish_tweet</kbd> wrapper function. Details on <kbd>publish_tweet</kbd> can be found in the following queue-specific code block. It's not complex at all, and the only really important bit is to understand what exactly ends up on the queue. In this case, we're placing a Python dictionary onto SQS, which ultimately gets serialized as a JSON record. This record contains the original tweet text, a URL for the cat or dog image, and any hashtags embedded in the tweet:</p>
<pre style="padding-left: 30px">publish_tweet({<br/>    'text': tweet.text,<br/>    'url': url,<br/>    'hashtags': hashtags,<br/>})</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Mimicking daemon processes with serverless functions</h1>
                </header>
            
            <article>
                
<p>By definition, serverless functions are short-lived and have a maximum lifetime before your platform of choice kills them. At the time of writing, the current limitation for an AWS Lambda function is 300 seconds (5 minutes) and the default 6 seconds. Our example relies on a long-lived process that is continually reading from the Twitter stream and publishing those results to the queue. How then can we accomplish this long-lived behavior with an inherently&#160;short-lived system?</p>
<p>To mimic a constantly-running process, we can take advantage of the scheduled invocation of Lambda functions. Other cloud providers should provide similar functionality. In short, we can use the maximum lifetime of Lambda functions to our advantage. The trick here is to set the <kbd>timeout</kbd> value of our Lambda function to 58 seconds, which is just below the scheduled invocation rate of 60 seconds.</p>
<p>Since that code will run indefinitely, we can rely on AWS killing the Lambda function after 58 seconds. After a running <kbd>Firehose</kbd> Lambda function is killed, we know another one will start up within a second or two, which results in a continually running <kbd>Firehose</kbd> process.</p>
<p>There is a chance that there will be one or two seconds when two instances of firehose processing run concurrently. In this case, that's not a concern since the data consumer can handle duplicate data elegantly by merely ignoring duplicates. If you plan on using the same pattern, it's essential to ensure your data consumer can deal with duplicates and is idempotent with its computation and processing. This pattern may not be applicable for all problems, but it works well for this and similar systems:</p>
<pre style="padding-left: 30px">service: twitter-stream<br/><br/>provider:<br/>  name: aws<br/>  runtime: python2.7<br/>  memorySize: 128<br/>  region: ${env:AWS_REGION}<br/>  iamRoleStatements:<br/>    - Effect: Allow<br/>      Action:<br/>        - sqs:*<br/>      Resource: "arn:aws:sqs:${env:AWS_REGION}:*:${env:ENV}TwitterFirehoseQueue"<br/>    - Effect: Allow<br/>      Action:<br/>        - rekognition:DetectLabels<br/>      Resource: "*"<br/>    - Effect: "Allow"<br/>      Action:<br/>        - "dynamodb:*"<br/>      Resource: "arn:aws:dynamodb:${env:AWS_REGION}:*:table/${env:DYNAMODB_RESULTS_TABLE_NAME}"<br/>  environment:<br/>    TWITTER_ACCESS_SECRET: ${env:TWITTER_ACCESS_SECRET}<br/>    TWITTER_ACCESS_TOKEN: ${env:TWITTER_ACCESS_TOKEN}<br/>    TWITTER_CONSUMER_KEY: ${env:TWITTER_CONSUMER_KEY}<br/>    TWITTER_CONSUMER_SECRET: ${env:TWITTER_CONSUMER_SECRET}<br/>    TWITTER_STREAM_QUEUE_NAME: ${env:ENV}TwitterFirehoseQueue<br/>    DYNAMODB_RESULTS_TABLE_NAME: ${env:DYNAMODB_RESULTS_TABLE_NAME}<br/><br/>package:<br/>  exclude:<br/>    - .git/**<br/>    - __pycache__/**<br/>    - "**/__pycache__/**"<br/>    - "*.pyc"<br/>    - "*.swp"<br/><br/>resources:<br/>  Resources:<br/>    FirehoseSQS:<br/>      Type: AWS::SQS::Queue<br/>      Properties:<br/>        QueueName: ${env:ENV}TwitterFirehoseQueue<br/>        VisibilityTimeout: 30<br/>    DynamoResultsTable:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        TableName: ${env:DYNAMODB_RESULTS_TABLE_NAME}<br/>        AttributeDefinitions:<br/>          - AttributeName: url<br/>            AttributeType: S<br/>        KeySchema:<br/>          - AttributeName: url<br/>            KeyType: HASH<br/>        ProvisionedThroughput:<br/>          ReadCapacityUnits: ${env:DYNAMODB_TABLE_READ_IOPS}<br/>          WriteCapacityUnits: ${env:DYNAMODB_TABLE_WRITE_IOPS}<br/><br/>functions:<br/>  Firehose:<br/>    handler: handler.firehose<br/>    timeout: 58<br/>    events:<br/>      - schedule: rate(1 minute)<br/>  Classify:<br/>    handler: handler.classify<br/>    timeout: 58<br/>    events:<br/>      - schedule: rate(1 minute)</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data consumers</h1>
                </header>
            
            <article>
                
<p>If you've worked with RabbitMQ or other queuing systems, you can be used to registering workers on specific queues or topics where those listeners/workers get notified when messages of interest arrive. With SQS, the model is entirely different. SQS is a purely poll-based system; that is, any code that is interested in reading data from a queue needs to poll using the appropriate AWS APIs. Additionally, application code must explicitly delete messages from the queue&#160;once it has completed its processing.</p>
<p>Some APIs for other queuing systems will automatically <kbd>ack</kbd> a message provided no exception occurs, resulting in the removal of those message from the queue. It's imperative to remember to delete messages from an SQS queue even if no processing should occur.</p>
<p>Just as the <kbd>Firehose</kbd> function executes on a one-minute interval, so too will our <kbd>Classify</kbd> process. When this function runs, it starts by pulling batches of records from the SQS queue in quantities of 10. You can see in the following that there is an infinite loop with the <kbd>while True:</kbd> statement. Again, once this loop starts, it will run until the Lambda itself terminates it according to our 58-second timeout. If there aren't any messages available for processing, everything just shuts down. This technique is more straightforward to implement and less expensive than dealing with the process sleeping. By merely quitting we can rely on the next run to pick up the next batch of work and don't need to waste CPU cycles doing anything but wait for messages to arrive:</p>
<pre style="padding-left: 30px">import boto3<br/>import json<br/>import os<br/>import urllib2<br/><br/>from decimal import Decimal<br/><br/>TWITTER_STREAM_QUEUE_NAME = os.environ['TWITTER_STREAM_QUEUE_NAME']<br/><br/>from .storage import ClassiferResults<br/><br/>_sqs_client = None<br/>_s3_client = None<br/>_sqs_url = None<br/><br/><br/>def get_sqs_client():<br/>    global _sqs_client<br/>    if _sqs_client is None:<br/>        _sqs_client = boto3.client('sqs')<br/>    return _sqs_client<br/><br/><br/>def get_queue_url():<br/>    global _sqs_url<br/>    if _sqs_url is None:<br/>        client = get_sqs_client()<br/>        response =    <br/>        client.get_queue_url(QueueName=TWITTER_STREAM_QUEUE_NAME)<br/>        _sqs_url = response['QueueUrl']<br/>    return _sqs_url<br/><br/><br/>def publish_tweet(payload):<br/>    msg = json.dumps(payload)<br/>    client = get_sqs_client()<br/>    sqs_url = get_queue_url()<br/><br/>    return client.send_message(<br/>                QueueUrl=sqs_url,<br/>                MessageBody=msg)<br/><br/><br/>def classify_photos():<br/>    rekognition = boto3.client('rekognition')<br/>    sqs = get_sqs_client()<br/>    sqs_url = get_queue_url()<br/><br/>    while True:<br/>        response = sqs.receive_message(<br/>            QueueUrl=sqs_url,<br/>            MaxNumberOfMessages=10,<br/>        )<br/>        messages = response.get('Messages')<br/>        if not messages:<br/>            break<br/><br/>        for msg in messages:<br/>            receipt = msg['ReceiptHandle']<br/>            body = json.loads(msg['Body'])<br/><br/>            url = body['url']<br/><br/>            # first check if we already have this image<br/>            classifier_store = ClassiferResults(url=url)<br/>            if classifier_store.exists:<br/>                print 'Deleting queue item due to duplicate image'<br/>                sqs.delete_message(QueueUrl=sqs_url, <br/>                ReceiptHandle=receipt)<br/>                continue<br/><br/>            image_response = urllib2.urlopen(url)<br/>            results = rekognition.detect_labels(Image={'Bytes': <br/>            image_response.read()})<br/><br/>            scores = [{<br/>                'Confidence': Decimal(l['Confidence']),<br/>                'Name': l['Name'],<br/>            } for l in results['Labels']]<br/><br/>            classifier_store.upsert(<br/>                    text=body['text'],<br/>                    hashtags=body['hashtags'],<br/>                    scores=scores,<br/>                    labels=[l['Name'] for l in results['Labels']],<br/>            )<br/><br/>            sqs.delete_message(QueueUrl=sqs_url, ReceiptHandle=receipt)</pre>
<p>After the <kbd>classify_photos</kbd>&#160;function finds some messages, processing them isn't very complicated. With tweets, there is a good chance our classifier will encounter duplicate photos. This job will store results in DynamoDB, so the first step is to check whether that URL already exists. Our DynamoDB table will use the URL as the partition key, which is analogous to a simple primary key in a relational database. This DynamoDB schema means that the URL must be unique. If we've already stored a particular URL, we won't do any more processing. Still, we need to remember to delete the message from the queue. Without that step, worker processes would repeatedly process a queue item, resulting in a never empty queue.</p>
<p>For any new URLs, we'll download the image and throw it over to the AWS&#160;Rekognition service to get a listing of labels and associated scores. If you're unfamiliar with Rekognition, it's quite a fantastic service. Rekognition provides several impressive features such as facial recognition. We'll be using the image detection or <em>labeling</em>&#160;feature, which will detect objects in a given image with a corresponding score:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/f7568edd-f0b9-4119-bd20-50e2415432f0.jpg" style="width:25.58em;height:19.25em;"/></div>
<p><span>As an example, this image of a cat results in the following <kbd>Labels</kbd> from Rekognition:</span></p>
<pre style="padding-left: 30px">{<br/>    "Labels": [<br/>        {<br/>            "Name": "Animal",<br/>            "Confidence": 86.34986877441406<br/>        },<br/>        {<br/>            "Name": "Cat",<br/>            "Confidence": 86.34986877441406<br/>        },<br/>        {<br/>            "Name": "Kitten",<br/>            "Confidence": 86.34986877441406<br/>        },<br/>        {<br/>            "Name": "Mammal",<br/>            "Confidence": 86.34986877441406<br/>        },<br/>        {<br/>            "Name": "Pet",<br/>            "Confidence": 86.34986877441406<br/>        },<br/>        {<br/>            "Name": "Manx",<br/>            "Confidence": 82.7002182006836<br/>        },<br/>        {<br/>            "Name": "Asleep",<br/>            "Confidence": 54.48805618286133<br/>        },<br/>        {<br/>            "Name": "Siamese",<br/>            "Confidence": 52.179630279541016<br/>        }<br/>    ]<br/>}</pre>
<p>So, our worker process will fetch pictures embedded in tweets and hand them off to Rekognition for labeling. Once Rekognition finishes its work, the worker process will store the scores and other data about the image and tweet in DynamoDB.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Viewing results</h1>
                </header>
            
            <article>
                
<p>In this example, we don't have a custom user interface to view results, so the DynamoDB console will have to do. As we can see in the following screenshot, I'm storing image URLs along with any embedded hashtags from the original tweet as well as the detected labels and scores from the Rekognition query:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/b8aa2feb-fdf8-453a-a307-0b7ca37b3a4f.png"/></div>
<p>Using the DynamoDB API, let's take a look at one of the records in detail using Python's <kbd>boto3</kbd> library from Amazon:</p>
<pre class="mce-root"><strong>&gt;&gt; import boto3</strong><br/><strong>&gt;&gt;&gt; url = 'https://pbs.twimg.com/media/DSQYcgMWAAAwfXf.jpg'</strong><br/><strong>&gt;&gt;&gt; dynamo = boto3.resource('dynamodb')</strong><br/><strong>&gt;&gt;&gt; table = dynamo.Table('devTweetClassifierResults')</strong><br/><strong>&gt;&gt;&gt; table.get_item(Key={'url': url})</strong><br/><strong>{u'hashtags': [u'cat', u'kitten', u'kitty'],</strong><br/><strong> u'labels': [u'Animal', u'Pet', u'Cat', u'Mammal', u'Manx'],</strong><br/><strong> u'scores': [{u'Confidence': Decimal('86.49568939208984375'),</strong><br/><strong>              u'Name': u'Animal'},</strong><br/><strong>             {u'Confidence': Decimal('86.49568939208984375'),</strong><br/><strong>              u'Name': u'Pet'},</strong><br/><strong>             {u'Confidence': Decimal('79.18183135986328125'),</strong><br/><strong>              u'Name': u'Cat'},</strong><br/><strong>             {u'Confidence': Decimal('79.18183135986328125'),</strong><br/><strong>              u'Name': u'Mammal'},</strong><br/><strong>             {u'Confidence': Decimal('79.18183135986328125'),</strong><br/><strong>              u'Name': u'Manx'}],</strong><br/><strong> u'text': u'#cat #cats #kitten #kittens #nice #cute #cutie #cool #ilovecats #kitty #groomers #vets #photooftheday #mycat\u2026 https://t.co/YXrs0JFb1d',</strong><br/><strong> u'url': u'https://pbs.twimg.com/media/DSQYcgMWAAAwfXf.jpg'}</strong></pre>
<p>With that, we have a reasonably sophisticated system with very few lines of application code. Most importantly and keeping this in context, every single system we've leveraged is entirely managed. Amazon will do the hard work of maintaining and scaling Lambda, SQS, and DynamoDB on our behalf. There are some tricks and essential details about managing DynamoDB read and write capacity and I encourage you to read up on that on your own.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Alternate Implementations</h1>
                </header>
            
            <article>
                
<p>Our example application is quite robust and can handle quite a bit of load and traffic with few to no changes. As easy as this pattern is to understand, implement, and run, it's not a silver bullet. You will likely require different implementations of this Messaging Pattern in your scenarios. We'll review a few alternative applications of the same pattern, which uses a queue as a message broker between disparate systems.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using the Fan-out and Messaging Patterns together</h1>
                </header>
            
            <article>
                
<p>Earlier, during the explanation of our system architecture, I briefly discussed the possibility&#160;of fanning out messages from the stream listener to multiple queues. A design such as this would be useful when there are different types of workload to be performed from a single data producer. The following example architecture shows a system made up of an individual Twitter stream data producer that fans out messages to multiple queues based on the payload:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/ab38d077-cdc4-4d47-8a72-3807825b645e.png" style="width:32.58em;height:12.25em;"/></div>
<p>For this example, assume we're interested in processing a more extensive range of tweets. This system will still classify dog and cat images as before; however, this time we can split the processing apart more granularly by using separate queues for cat images and dog images. We can not be able to warrant this split in processing at the beginning, but it will allow us to treat and scale those systems separately.</p>
<p>A better example is the splitting of <kbd>@realDonaldTrump</kbd> tweets into an entirely different processing pipeline using a dedicated queue. The volume on this queue would be much higher than cat and dog images. Likely, we'd want to be running multiple concurrent workers to process this higher amount. Additionally, we could do something completely different from the labeling of images, such as running sentiment analysis on those tweets. Even in cases where the sentiment analysis was underprovisioned and got behind, we could feel confident knowing that any message on the queue could eventually be processed either by adding more worker processes or by an eventual slowdown of new messages from the data producer.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using a queue as a rate-limiter</h1>
                </header>
            
            <article>
                
<p>Many public APIs have rate limits. If you are attempting to pull down any substantial amount of data using a particular API that requires many API calls, you'll undoubtedly need to work around those rate limits and find a way to get to your data as fast as possible without exceeding your request quota. In cases such as this, a queue architecture can help out:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/26af2a8a-757f-4401-be7f-352c137c42c9.png" style="width:40.17em;height:16.17em;"/></div>
<p class="mce-root">I have personally implemented this exact pattern with great success. Here, a third-party API provides an endpoint to <kbd>/items</kbd>. The actual data being retrieved is of little importance to explain the details. Here, the challenge is that we can only fetch the required details of these items by making another API call to an <kbd>/items/${id}</kbd> endpoint.&#160;When there are hundreds or thousands of things to download, which each require a separate API call (or more), we need to be careful to stay below the rate limit threshold. Typically, we would prefer that the system also runs as quickly as possible, so the overall process of retrieving item details doesn't take days or weeks.</p>
<p>With such constraints, we can use a queue and inspection of our consumption rate limit to download items as fast as possible while also staying within the bounds of our allotted rate limit. The trick here is to break up the work of producing objects for download from the act of downloading those objects. For the sake of this example, assume the <kbd>/items</kbd> endpoint retrieved up to 500 items at a time, where each element has a structure that includes a unique numeric&#160;<kbd>id</kbd> along with some metadata. Our <span>goal of retrieving&#160;</span>the entire view of each item requires another API call to <kbd>/item/${id}</kbd>. The data producer would make a single call to the <kbd>/items</kbd> endpoint and place messages onto the queue for each item that needs to be downloaded and stored. Each message on the queue would be somewhat generic, comprising a simple data structure such as <kbd>{'url': 'https://some-domain.io/item/1234'}</kbd>. This process could go as rapidly as it needed, since fetching an entire list of objects could realistically be done quite quickly and probably under whatever rate limit is imposed.</p>
<p>I put any intelligence about downloading item details and dealing with rate limiting into the downloader process. Just as with our cat and dog classifier, the downloader job is scheduled to wake up every minute and download as many messages as possible. Upon fetching the first item from the queue, the downloader will check the consumed rate limit which is provided by our third-party API via HTTP headers. There is no standard way of providing usage statistics to clients, but I've seen this type of data returned in the following header format:&#160;<kbd>X-Ratelimit-Usage: 2142,3000</kbd>. In this example, the API enforces a limit of 3,000 requests per unit time while the client has currently consumed 2,142 requests. If you do the math, 2,142 consumed units compared with a threshold of 3,000 equate to 71.4% usage.</p>
<p>After each API call, the downloader job checks its consumed API usage by doing this simple calculation. Once it nears some upper limit, the downloader can merely shut itself down and cease making API requests (perhaps when it gets above 90% usage). Of course, there must be a single API call made to inspect this usage. If the worker/downloader processes start up every two minutes, the worst-case scenario is that the system makes a single API call every two minutes. Only after some time has elapsed and the rate limits are reset (perhaps every 15 minutes) can the downloader start pulling items in bulk again. By using the same trick as our classifier example, it's trivial to have one or more downloader processing continually running by playing with the timeout value along with the scheduled invocation time.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using a dead-letter queue</h1>
                </header>
            
            <article>
                
<p>Under certain circumstances, worker processes can never successfully process a message sitting in a queue. Take for example our image classifier problem. The worker processes aren't responsible for much other than downloading an image, sending it to Rekognition, and storing those results in DynamoDB. However, what happens if, in between a tweet arriving in the queue and our processing of that tweet, a Twitter user deletes the original image or tweet. In this case, our classifier process would fail hard. Look for yourself, and you'll see there are no guards against an HTTP 404 response code from the image fetch.</p>
<p>A hard failure like this will result in the application code skipping the&#160;<kbd>sqs.delete_message</kbd> function altogether. After a configurable amount of time, that same message will be available on the queue to another worker who will encounter the same problem and fail in the same way. Without some protections in place, this cycle will repeat itself indefinitely.</p>
<p>It would be quite trivial to work around this case in the application code, since dealing with any non-200 HTTP response codes is quite easy and missing a few images isn't a significant problem. In more complicated applications where the failure scenarios cannot be as easy to foresee, setting up some fallback mechanism can be very helpful for debugging and for making the entire system more reliable. Specific queuing systems, including SQS, offer what is called a dead-letter queue.</p>
<p>A dead-letter queue is a separate queue where messages that cannot be successfully processed wind up. We can set up a dead-letter queue and configure our primary queue to place messages there if workers cannot successfully process&#160;messages after ten attempts. In that case, we guarantee the messages will eventually be removed from the primary queue either due to successful processing or by forceful removal due to 10 failures. A useful benefit of this is that we'll catch any problematic messages and can eventually inspect&#160;them and make changes to application code as needed. Since the dead-letter queue is a queue itself, we're still responsible for maintaining it and ensuring its health and size are kept in check.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the details of the Messaging Pattern and walked through a complete example using AWS SQS as a message broker. The example application comprised a Lambda function as the data producer, SQS as the message broker, and a Lambda function as the data consumer, which ultimately stored results in DynamoDB. We also discussed the difference between queues and streaming systems and reviewed their merits and use cases when one may be preferable over another. I also explained alternative architectures and implementations of the Messaging Pattern with specific problems and examples&#160;given for context.</p>
<p>At this point, readers should have a good understanding of&#160;how to break apart data-heavy serverless applications using queuing systems to provide scalability, fault tolerance, and reliability. I presented alternative architectures, which should give readers some insight into how they can structure their applications for improved decoupling and performance.</p>
<p>In the following chapter, we'll review another data-processing type of pattern which is useful in big data systems, the Lambda Pattern.</p>


            </article>

            
        </section>
    </div>
</body>
</html>