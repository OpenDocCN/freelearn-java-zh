<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Reactive Messaging with Quarkus</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn about the nuts and bolts of <strong>SmallRye Reactive Messaging</strong>, which can be used in Quarkus to implement the Eclipse MicroProfile Reactive Messaging specification. By the end of this chapter, you will have a solid development model for your data streaming applications and know how to connect to streaming platforms such as <strong>Apache Kafka</strong> and <strong>ActiveMQ</strong>.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Getting started with Reactive Messaging</li>
<li>Streaming messages with Apache Kafka</li>
<li>Streaming messages with <span class="st"><strong>Advanced Message Queuing Protocol</strong> (<strong>AMQP</strong>)</span><span class="st"><br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can find the source code for the project in this chapter on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10">https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter10</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with Reactive Messaging</h1>
                </header>
            
            <article>
                
<p>Reactive Streams is an initiative that aims to provide a standard for exchanging data streams across an asynchronous boundary. At the same time, it guarantees that the receiving side is not forced to buffer arbitrary amounts of data.</p>
<p>There are several available implementations of Reactive Stream, and we have already learned how you can feature Reactive Programming in Vert.x. In this chapter, we will complement our knowledge using the SmallRye Reactive Messaging implementation to show you how we can integrate it with a streaming platform such as Apache Kafka or a messaging broker such as ActiveMQ with minimal configuration changes.</p>
<p>To familiarize ourselves with MicroProfile Reactive Messaging, we need proper knowledge of some key concepts. First of all, MicroProfile Reactive Messaging is a specification that uses CDI beans to drive the flow of messages toward some specific channels.</p>
<p>A message is a basic interface that contains a payload to be streamed. The <kbd>Message</kbd> interface is parameterized in order to describe the type of payload it contains. Additionally, a <kbd>Message</kbd> interface contains attributes and metadata that are specific to the broker that's used for message exchange (for example, Kafka or AMQ).</p>
<p>A channel, on the other hand, is a string indicating which source or destination of messages is used. There are two types of channels:</p>
<ul>
<li>Internal channels are local to the application and are used to implement a multi-step process for messages.</li>
<li>Remote channels are connected to remote brokers (such as Apache Kafka or AMQ) through connectors.</li>
</ul>
<p>Since MicroProfile Reactive Messaging is fully governed by the CDI model, two core annotations are used to indicate whether a method is a producer or a consumer of messages:</p>
<ul>
<li><kbd>@Incoming</kbd><strong>:</strong> This annotation is used on a method to indicate that it consumes messages from the specified channel. The name of the channel is added to the annotation as an attribute. Here is an example:</li>
</ul>
<pre style="padding-left: 60px">@Incoming("channel")<br/>public void consume(Message&lt;String&gt; s) {   <br/>  // Consume message here:<br/>}</pre>
<p style="padding-left: 60px">The effect of placing this annotation on a method is that the method will be called each time a message is sent to that channel. From the user's perspective, it is totally transparent about whether the incoming message arrives from a collocated CDI bean or a remote broker. However, you may decide to make it clear that the method consumes a specific kind of message, such as <kbd>KafkaMessage</kbd> (which inherits from <kbd>Message</kbd>). Here is an example:</p>
<pre style="padding-left: 60px">@Incoming("channel")<br/>public void consume(KafkaMessage&lt;String&gt; s) {     <br/>   // Consume message here:<br/>}</pre>
<ul>
<li><kbd>@Outgoing</kbd>: This annotation indicates that a method publishes messages to a channel. In much the same way, the name of the channel is stated in the annotation's attribute:</li>
</ul>
<pre style="padding-left: 60px">@Outgoing("channel")<br/> public Message&lt;String&gt; produce() {<br/>   // Produce and return a Message implementation<br/> }</pre>
<p style="padding-left: 60px">Within the method annotated with <kbd>@Outgoing</kbd>, we return a concrete implementation of the <kbd>Message</kbd> interface.</p>
<div class="packt_infobox">Be aware that you can only annotate a single method with <kbd>@Outgoing</kbd> for one channel. If you attempt to use the same channel in more than one <kbd>@Outgoing</kbd> annotated method, an error will be emitted at deployment time.</div>
<p>You can also annotate a method with both <kbd>@Incoming</kbd> and <kbd>@Outgoing</kbd> so that it behaves like a <strong>message processor</strong>, which transforms the content of the message data:</p>
<pre>@Incoming("from")<br/>@Outgoing("to")<br/>public String translate(String text) {<br/>   return MyTranslator.translate(text);<br/>}</pre>
<p>From the preceding examples, we can see that messages flow from an <kbd>@Outgoing</kbd> stream producer to an <kbd>@Incoming</kbd> stream consumer and Reactive Messaging transparently connects the two endpoints. In order to decouple <kbd>Producer</kbd> and <kbd>Consumer</kbd> messages, you can add a component such as Apache Kafka by using the connectors provided by MicroProfile API. In the next section, we will introduce our first example of Reactive Messaging using Apache Kafka.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming messages with Apache Kafka</h1>
                </header>
            
            <article>
                
<p class="mce-root">Apache Kafka (<a href="https://kafka.apache.org/">https://kafka.apache.org/</a>) is a distributed data streaming platform that can be used to publish, subscribe, store, and process streams of data from multiple sources in real time at amazing speeds.</p>
<p class="mce-root">Apache Kafka can be plugged into streaming data pipelines that distribute data between systems, and also into the systems and applications that consume that data. Since Apache Kafka reduces the need for point-to-point integrations for data sharing, it is a perfect fit for a range of use cases where high throughput and scalability are vital.</p>
<p class="mce-root">Additionally, once you combine Kafka with Kubernetes, you attain all the benefits of Kafka, as well as the advantages of Kubernetes, such as the following:</p>
<ul>
<li class="mce-root"><strong>Scalability and high availability</strong>: You can easily scale up and down resources with Kubernetes, which means you can automatically determine the pool of resources that Apache Kafka will share with other applications while guaranteeing the high availability of your Kafka cluster at the same time.</li>
<li class="mce-root"><strong>Portability</strong>: By running Kafka with Kubernetes, your cluster of Kafka nodes can span across on-site and public, private, or hybrid clouds, even using different operating systems.</li>
</ul>
<p class="mce-root">To manage the Kafka environment, you need a piece of software called <strong>ZooKeeper</strong>, which manages naming and configuring data in order to provide flexible and robust synchronization within distributed systems. ZooKeeper controls the status of the Kafka cluster nodes and also keeps track of Kafka topics, partitions, and all the Kafka services you need. We won't cover the details of ZooKeeper administration in this chapter, although it's worth mentioning its role as you will need to get to grips with it in order to land a Kafka Administrator job.</p>
<p>To demonstrate Apache Kafka and MicroProfile Streaming's powerful combination on Quarkus, we will design a simple application that simulates a stock trading ticker that's updated in real time by purchases and sales. Get ready and open for business!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Composing our stock trading application</h1>
                </header>
            
            <article>
                
<p>Let's start with the architecture of our stock trading application. To set up an application that has a minimal level of complexity, we will create the following channels:</p>
<ul>
<li>An <strong>outgoing</strong> producer that's bound to the <strong>"stock-quote"</strong> channel, where messages containing stock orders will be written into a topic named <strong><strong>"stocks"</strong></strong></li>
<li>An <strong>incoming</strong> consumer that's bound to the <strong>"stocks"</strong> channel, which read messages that are available in the <strong>"stocks"</strong> topic</li>
<li>An <strong>outgoing</strong> producer that's bound to the <strong>"in-memory-stream"</strong> channel, which broadcasts the new stock quote to all the available subscribers internally</li>
<li>An <strong>incoming</strong> consumer that's bound to the <strong>"in-memory-stream"</strong> channel, which reads the new stock quote and sends it as SSE to clients</li>
</ul>
<p>The following diagram depicts the basic stream of messages that will be used in our example:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e02a25f9-1583-4127-bde7-748e0f12d06e.png" style=""/></div>
<p>The example application can be found in the <kbd>Chapter10/kafka</kbd> folder of this book's GitHub repository. We recommend importing the project into your IDE before you move on.</p>
<p>As you can see from the <kbd>pom.xml</kbd> file of this project, we have included the following extension so that we can stream messages to the Apache Kafka server:</p>
<pre>&lt;dependency&gt;<br/>   &lt;groupId&gt;io.quarkus&lt;/groupId&gt;<br/>   &lt;artifactId&gt;quarkus-smallrye-reactive-messaging-kafka&lt;/artifactId&gt;<br/>&lt;/dependency&gt;</pre>
<p>Before we dive into the code, we need to fulfill some requirements to run Kafka in a container. As we mentioned previously, Kafka needs ZooKeeper to manage its cluster, so we need to start both services. A practical solution you can use in the development or test environment is to use <strong>Docker Compose</strong>, which is a tool that's used to manage and synchronize multiple container applications in a single configuration file written in YAML format.</p>
<p>The installation of Docker Compose is detailed on its documentation page (<a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a>), but for a Linux machine, you can install a stable release of it with the following shell command:</p>
<pre><strong>sudo curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose</strong></pre>
<p>When you're done, apply for the right permission on the <kbd>docker-compose</kbd> tool:</p>
<pre><strong>chmod a+x /usr/local/bin/docker-compose</strong></pre>
<p>Now, you can verify the installed version, as follows:</p>
<pre><strong>docker-compose --version</strong></pre>
<p>You should see the following output:</p>
<pre><strong>docker-compose version 1.24.1, build 1110ad01</strong></pre>
<p>Now that we're done with our preliminary requirements, it's time to add some lines of code!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding bean classes</h1>
                </header>
            
            <article>
                
<p>The first class we will add is <kbd>QuoteGenerated</kbd>, which is an <kbd>ApplicationScoped</kbd> CDI bean that produces random quotes for a company every two seconds. Here's the code for this:</p>
<pre><span>@ApplicationScoped<br/></span><span>public class </span>QuoteGenerator {<br/><br/>    <span>private </span>Random <span>random </span>= <span>new </span>Random()<span>;<br/></span><span><br/></span><strong><span>    </span><span>@Outgoing</span>(<span>"stock-quote"</span>)</strong><br/>    <span>public </span>Flowable&lt;String&gt; <span>generate</span>() {<br/>       <span>return </span>Flowable.<span>interval</span>(<span>2</span><span>, </span>TimeUnit.<span>SECONDS</span>)<br/>         .map(tick -&gt; generateOrder(<span>random</span>.nextInt(<span>2</span>)<span>, <br/>          </span><span>random</span>.nextInt(<span>5</span>)<span>,   </span><span>random</span>.nextInt(<span>100</span>)))<span>;<br/></span><span> </span>}<br/><br/>    <span>private </span>String <span>generateOrder</span>(<span>int </span>type<span>, int </span>company<span>, int </span>amount) {<br/>       Jsonb jsonb = JsonbBuilder.<span>create</span>()<span>;<br/></span><span>       </span>Operation operation = <span>new </span>Operation(type<span>, </span>Company.<span>values</span>()<br/>        [company]<span>, </span>amount)<span>;<br/></span><span>       return </span>jsonb.toJson(operation)<span>;<br/></span><span>    </span>}<br/>}</pre>
<p>This class produces messages that will be written to Kafka through the <kbd>"stock-quote"</kbd> channel. The message contains the stock order that is randomly generated through three parameters:</p>
<ul>
<li>The type of order (sale/purchase)</li>
<li>The company name</li>
<li>The number of shares purchased/sold</li>
</ul>
<p>At the end of the day, the <kbd>generate</kbd> method will produce a message that contains a JSON string, similar to the following:</p>
<pre>{"amount":32,"company":"Soylent","type":0}</pre>
<p>So that we have a better understanding of the accessory components, here is the <kbd>Company</kbd> enumeration, which contains the following set of companies:</p>
<pre><span>public enum </span>Company {<br/>        <span>Acme</span><span>, </span><span>Globex</span><span>, </span><span>Umbrella</span><span>, </span><span>Soylent</span><span>, </span><span>Initech<br/></span>}</pre>
<p>We also need the core part of the <kbd>Operation</kbd> class, which is a Java POJO that holds the data of each stock order:</p>
<pre><span>public class </span>Operation {<br/><br/>    <span>public static final int </span><span>SELL </span>= <span>0</span><span>;<br/></span><span>    public static final int </span><span>BUY </span>= <span>1</span><span>;<br/><br/></span><span>    private int </span><span>amount</span><span>;<br/></span><span>    private </span>Company <span>company</span><span>;<br/></span><span>    private int </span><span>type</span><span>;<br/></span><span><br/></span><span>    public </span><span>Operation</span>(<span>int </span>type<span>, </span>Company company<span>, int </span>amount) {<br/>        <span>this</span>.<span>amount </span>= amount<span>;<br/></span><span>        this</span>.<span>company </span>= company<span>;<br/></span><span>        this</span>.<span>type </span>= type<span>;<br/></span><span>    </span>}<br/>    // Getters/Setters method omitted for brevity<br/>}</pre>
<p>Now, a brief Wall Street 101: each stock order will determine a change in the quotation of a company. Simply put, by selling stocks, the price of a company will decrease, while a buy order will make the stock more demanded, which means that the price will rise. The number of shares sold/purchased will eventually determine how much the price goes up and down.</p>
<p>The following <kbd>QuoteConverter</kbd> class will do the job of converting a stock order into a new quotation for the <kbd>Company</kbd> involved in the transaction:</p>
<pre><span>@ApplicationScoped<br/></span><span>public class </span>QuoteConverter {<br/>    HashMap&lt;String<span>,</span>Double&gt; <span>quotes</span><span>;<br/></span><span><br/></span><span>    private </span>Random <span>random </span>= <span>new </span>Random()<span>;<br/></span><span>    </span><span>@PostConstruct<br/></span><span>    </span><span>public void </span><span>init</span>() {<br/>        <span>quotes </span>= <span>new </span>HashMap&lt;&gt;()<span>;<br/></span><span>        for </span>(Company company: Company.<span>values</span>())<br/>        <span>quotes</span>.put(company.name()<span>, new </span>Double(<span>random</span>.nextInt<br/>        (<span>100</span>) + <span>50</span>))<span>;<br/></span><span><br/></span><span>    </span>}<br/><br/><strong>    <span>@Incoming</span>(<span>"stocks"</span>)</strong><br/><strong>    <span>@Outgoing</span>(<span>"in-memory-stream"</span>)</strong><br/>    <span><strong>@Broadcast</strong><br/></span><span>    </span><span>public </span>String <span>newQuote</span>(String quoteJson) {<span><br/></span><span>        </span>Jsonb jsonb = JsonbBuilder.<span>create</span>()<span>;<br/></span><span><br/></span><span>        </span>Operation operation = jsonb.fromJson(quoteJson<span>, <br/>         </span>Operation.<span>class</span>)<span>;<br/></span><span>        double </span>currentQuote = <br/>         <span>quotes</span>.get(operation.getCompany().name())<span>;<br/></span><span>        double </span>newQuote<span>;<br/></span><span>        double </span>change = (operation.getAmount() / <span>25</span>)<span>;<br/></span><span><br/></span><span>        if </span>(operation.getType() == Operation.<span>BUY</span>) {<br/>              newQuote = currentQuote + change<span>;<br/></span><span>        </span>}<br/>        <span>else  </span>{<br/>            newQuote = currentQuote - change<span>;<br/></span><span>        </span>}<br/>        <span>if </span>(newQuote &lt; <span>0</span>) newQuote = <span>0</span><span>;<br/></span><span><br/></span><span>        </span><span>quotes</span>.replace(operation.getCompany().name()<span>, </span>newQuote)<span>;<br/></span><span>        </span>Quote quote = <span>new </span>Quote(operation.getCompany().name()<span>, <br/>         </span>newQuote)<span>;<br/></span><span><br/></span><span>        return </span>jsonb.toJson(quote)<span>;<br/></span><span><br/></span><span>    </span>}<br/><br/>}</pre>
<p>The <kbd>init</kbd> method of this class simply bootstraps the initial quotation of every <kbd>Company</kbd> with some random values.</p>
<p>The <kbd>newQuote</kbd> method is at the heart of our transaction system. By reading the operation data contained in the JSON file, a new quote is generated using a basic algorithm: for any 25 stocks that are transacted, there will be one point's impact on the value of the stock. The returned JSON string wraps the <kbd>Quote</kbd> class, which is broadcasted to all the matching subscribers of the <kbd>"in-memory-stream"</kbd> channel by means of the <kbd>@Broadcast</kbd> annotation being on top of the method.</p>
<p>For the sake of completeness, we'll also include the <kbd>Quote</kbd> Java class, which will be sent as JSON to the client:</p>
<pre><span>public class </span>Quote {<br/>    String <span>company</span><span>;<br/></span><span>    </span>Double <span>value</span><span>;<br/></span><br/>    <span>public </span><span>Quote</span>(String company<span>, </span>Double value) {<br/>        <span>this</span>.<span>company </span>= company<span>;<br/></span><span>        this</span>.<span>value </span>= value<span>;<br/></span><span>    </span>}<br/><br/>   // Getters Setters method omitted for brevity<br/>}</pre>
<p>Within our example, we have the following subscriber for the <kbd>"in-memory-stream"</kbd> channel, where <kbd>Quote</kbd> is published:</p>
<pre><span>@Path</span>(<span>"/quotes"</span>)<br/><span>public class </span>QuoteEndpoint {<br/><br/>    <span>@Inject<br/></span><strong><span>    </span></strong><strong><span>@Channel</span>(<span>"in-memory-stream"</span>)</strong> <br/>    Publisher&lt;String&gt; <span>quote</span><span>;<br/></span><span><br/></span><span>    </span><span>@GET<br/></span><span>    @Path</span>(<span>"/stream"</span>)<br/>    <span>@Produces</span>(MediaType.<span>SERVER_SENT_EVENTS</span>)<br/>    <span>@SseElementType</span>(<span>"text/plain</span><span>"</span>)<br/>    <span>public </span>Publisher&lt;String&gt; <span>stream</span>() {<br/><br/>        <span>return </span><span>quote</span><span>;<br/></span><span>    </span>}<br/>}</pre>
<p><kbd>QuoteEndpoint</kbd> is our REST endpoint. Within this, we are using the <kbd>@Channel</kbd> qualifier to inject the <kbd>"in-memory-stream"</kbd> channel into the bean. That's exactly the point where the reactive world (governed by streams) unifies with the imperative world (the CDI bean, which executes code in sequence). Simply put, this is where our bean is able to retrieve channels that are managed by Reactive Messaging.</p>
<p>All the preceding components need a broker, which is where we publish the stock quotes and read them. Here is the <kbd>application.properties</kbd> file, which keeps all of these pieces together:</p>
<pre><span>#Kafka destination<br/>mp.messaging.outgoing.stock-quote.connector</span><span>=</span><span>smallrye-kafka<br/></span><span>mp.messaging.outgoing.stock-quote.topic</span><span>=</span><span>stocks<br/></span><span>mp.messaging.outgoing.stock-quote.value.serializer</span><span>=</span><span>org.apache.kafka.common.serialization.</span><span>StringSerializer</span><span><br/></span><span><br/>#Kafka source<br/></span><span>mp.messaging.incoming.stocks.connector</span><span>=</span><span>smallrye-kafka<br/></span><span>mp.messaging.incoming.stocks.topic</span><span>=</span><span>stocks<br/></span><span>mp.messaging.incoming.stocks.value.deserializer</span><span>=</span><span>org.apache.kafka.common.serialization.StringDeserializer</span></pre>
<p>The first block is related to the Kafka destination, also known as a <strong>sink</strong> in streaming parlance, and is where we write the stock quote produced by <kbd>QuoteGenerator</kbd>. To replicate the data across the node of the classes, it is necessary to serialize its content. Byte streams are the standard language that the OS uses for I/O. In our case, since the data is in JSON format, we use <kbd>StringSerializer</kbd>.</p>
<p>In the second block, we configure the source topic and connector, where we read the stock quote as a JSON serialized stream.</p>
<p>Now, all we need to do is add a client application that is able to capture the SSE and display the text of it in a nicely formatted table of data. For the sake of brevity, we will just add the core JavaScript function that collects the SSE:</p>
<pre><span>&lt;script&gt;<br/></span><span>    </span>var source = new EventSource("/quotes/stream");<br/>    source.onmessage = function (event) {<br/>    var data = JSON.parse(event.data);<br/>    var company = data['company'];<br/>    var value = data['value'];<br/>        document.getElementById(company).innerHTML = value;<br/>    };<br/><span>&lt;/script&gt;</span></pre>
<p>The preceding code will be included in the <kbd>index.html</kbd> page, which can be found in the source code of this chapter. Let's see it in action! Before building the application, start the Kafka/ZooKeeper containers with the following command:</p>
<pre><strong>docker-compose up</strong></pre>
<p>The Docker Compose tool will search for the <kbd>docker-compose.yaml</kbd> file, which is in the root directory of this example. Here, we have configured the Kafka and ZooKeeper containers so that they start. A successful bootstrap will produce the following output at the bottom of your console:</p>
<pre><strong>kafka_1      | [2019-10-20 07:05:36,276] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)</strong><br/><strong> kafka_1      | [2019-10-20 07:05:36,277] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)</strong><br/><strong> kafka_1      | [2019-10-20 07:05:36,279] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)</strong></pre>
<p>You can verify that the Kafka and ZooKeeper containers are up and running by executing the <kbd>docker ps</kbd> command:</p>
<pre><strong>docker ps --format '{{.Names}}'</strong></pre>
<p>The preceding command will display the following active processes:</p>
<pre><strong>kafka_kafka_1</strong><br/><strong>kafka_zookeeper_1</strong></pre>
<p>Now, bootstrap the application as usual with the following command:</p>
<pre><strong>mvn install quarkus:dev</strong></pre>
<p>The welcome page of the application (available at <kbd>http://localhost:8080</kbd>) will show the stock quotes ticker in action, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4b47c63f-1243-4a6d-8287-ff3d5c75ba5e.png" style=""/></div>
<p>Each company in the list will start with an N/A quote until a random operation is executed on it. In the end, you will see that the preceding page is updated every two seconds, which is what we configured in the <kbd>QuoteGenerator</kbd> class. Pretty cool, isn't it?</p>
<p>When you're done with this example, stop all the running containers with the following command:</p>
<pre><strong>docker stop $(docker ps -a -q)</strong></pre>
<p>Once the <kbd>docker-compose</kbd> process terminates, the preceding command will display a list of all the container layers that have been stopped:</p>
<pre><strong>6a538738088f</strong><br/><strong>f2d97de3520a</strong></pre>
<p>Then, verify that the Kafka and ZooKeeper containers have been stopped by executing the <kbd>docker ps</kbd> command once more:</p>
<pre><strong>docker ps --format '{{.Names}}'</strong></pre>
<p>The preceding command shouldn't produce any output, which means that no pending Docker process is running.</p>
<p>We've just got started with the Docker Compose tool. Now, let's move on and deploy the full application stack on OpenShift.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming messages to Kafka in the cloud</h1>
                </header>
            
            <article>
                
<p>To complete our next challenge, we strongly recommend using the latest OpenShift 4.X version. As a matter of fact, in order to orchestrate multiple services such as Kafka and ZooKeeper, it is much simpler to use OpenShift release 4 as it is built on top of the concept of <strong>Operators</strong>. A Kubernetes Operator is a <span class="hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_rich_text">piece of software running in a Pod on the cluster, which introduces new object types through <strong>Custom Resource Definitions</strong> (<strong>CRDs</strong>). A CRD is nothing but an extension mechanism in Kubernetes that lets you define interfaces for a user; for example, you can define a CRD for a Kafka server, which provides a simpler way for us to configure and run it in our cluster.</span></p>
<p>In addition, Operators already have a common directory (<a href="https://operatorhub.io/">https://operatorhub.io/</a>), where you can find an existing Operator or add your own.</p>
<p>You can evaluate OpenShift 4 by going to <a href="https://www.openshift.com/trial/">https://www.openshift.com/trial/</a>. There, you can find several alternatives for evaluating OpenShift, either in the cloud or on your own machine. In this chapter, we are assuming that you have already completed the signup procedure and that you have OpenShift 4 up and running.</p>
<p>For the next project, please refer to the <kbd>Chapter10/kafka-openshift</kbd> directory, where you will find the stock trade application configured for OpenShift and the YAML files for setting up and configuring the Kafka cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Kafka on OpenShift</h1>
                </header>
            
            <article>
                
<p>The simplest way to install and manage an Apache Kafka cluster on the OpenShift cluster is via the <strong>Strimzi</strong> project (<a href="https://strimzi.io/">https://strimzi.io/</a>), which can be installed as an OpenShift Operator.</p>
<p>Start by creating a new OpenShift project named <kbd>kafka-demo</kbd>. You can either create it from the admin console or using the <kbd>oc</kbd> command-line utility, as follows:</p>
<pre><strong>oc new-project kafka-demo</strong></pre>
<p>The returned output will confirm that the project namespace has been created in your virtual address:</p>
<pre><strong>Now using project "kafka-demo" on server "https://api.fmarchioni-openshift.rh.com:6443".</strong></pre>
<p>The server name will be different in your case, depending on the account name you chose when you signed in.</p>
<p>We recommend continuing from the OpenShift web-console. From the left-hand <span class="packt_screen">Administrator</span> panel, select <span class="packt_screen">OperatorHub</span>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7eeb4369-78cc-41d0-a8e0-6c461bf12678.png" style=""/></div>
<p>The <span class="packt_screen">OperatorHub</span> Catalog will show up in the main OpenShift dashboard. Select the <strong>Strimzi</strong> Operator, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d98b877c-6503-4e14-83f3-f12931abd19a.png" style=""/></div>
<p>Then, in the following UI, choose to <span class="packt_screen">Install</span> the Operator:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a434ae1e-ced9-405b-b7b9-a1f8ad0676ca.png" style=""/></div>
<p>Next, you will be able to choose whether you want to install the Operator in all the available namespaces of your cluster or just in a specific project. Since we won't be using this Operator in other projects, just check the <span class="packt_screen">A specific namespace on the cluster</span> option and pick up your project. Your selection should look as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6928559a-0d2a-4f78-a26e-768bf838c3f0.png" style=""/></div>
<p>After a few seconds, in the main panel, you will be notified that the Operator has been installed, along with all its provided APIs:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4d01d9dd-0e17-4636-ba5d-255a89f0e6ff.png" style=""/></div>
<p>Now that you have the Strimzi Operator<span> available,</span> installing the Kafka cluster will be a piece of cake! Within the <kbd>Chapter10/kafka-openshift/strimzi</kbd> folder, you will find the following files:</p>
<ul>
<li><kbd>kafka-cluster-descriptor.yaml</kbd>: This file contains a Kafka cluster definition based on the Strimzi Operator.</li>
<li><kbd>kafka-topic-queue-descriptor.yaml</kbd>: This file defines a resource (a Kafka topic) that we need to configure in our Kafka cluster.</li>
</ul>
<p>You can install both of them with the <kbd>oc</kbd> command. Let's start from the cluster:</p>
<pre><strong>oc create -f strimzi/kafka-cluster-descriptor.yaml</strong></pre>
<p>The output of the preceding command is as follows:</p>
<pre><strong>kafka.kafka.strimzi.io/my-kafka created</strong></pre>
<p>Now, wait a few seconds until the Kafka cluster is up and running. You can check the status of your Pods in the current project with the following command:</p>
<pre><strong>oc get pods</strong> </pre>
<p>Then, wait until all the Pods are running, as follows:</p>
<pre><strong>NAME                                                READY   STATUS    RESTARTS   AGE</strong><br/><strong>my-kafka-entity-operator-58d546cf6c-dw85n           3/3     Running   0          5m50s</strong><br/><strong>my-kafka-kafka-0                                    2/2     Running   1          6m27s</strong><br/><strong>my-kafka-kafka-1                                    2/2     Running   1          6m27s</strong><br/><strong>my-kafka-kafka-2                                    2/2     Running   0          6m27s</strong><br/><strong>my-kafka-zookeeper-0                                2/2     Running   0          7m5s</strong><br/><strong>my-kafka-zookeeper-1                                2/2     Running   0          7m5s</strong><br/><strong>my-kafka-zookeeper-2                                2/2     Running   0          7m5s</strong><br/><strong>strimzi-cluster-operator-v0.14.0-59744f8569-d7j44   1/1     Running   0          7m47s</strong></pre>
<p>A successful cluster setup will be made up of the following components:</p>
<ul>
<li>Three Kafka cluster nodes in a running state</li>
<li>Three ZooKeeper cluster nodes also in a running state</li>
</ul>
<p>The name of the cluster (<kbd>my-kafka</kbd>) has been assigned within the <kbd>kafka-cluster-descriptor.yaml</kbd> file, as follows:</p>
<pre>apiVersion: kafka.strimzi.io/v1beta1<br/> kind: Kafka<br/> metadata:<br/>   name: my-kafka</pre>
<p>Now, let's move on by adding a queue named <kbd>stock</kbd>, as defined in the <kbd>kafka-topic-queue-descriptor.yaml</kbd> folder. You can create it with the following command:</p>
<pre><strong>oc create -f strimzi/kafka-topic-queue-descriptor.yaml</strong></pre>
<p>You will see the following output:</p>
<pre><strong>kafkatopic.kafka.strimzi.io/stocks created</strong></pre>
<p>If you want some insights into the Kafka cluster, you can check whether the topic is available. To do that, log in to any of the available Kafka nodes with <kbd>oc rsh</kbd>:</p>
<pre><strong>oc rsh my-kafka-kafka-0</strong></pre>
<p>By doing this, you will have access to the Terminal of that container. From there, execute the following command:</p>
<pre>sh-4.2$ <strong>./bin/kafka-topics.sh --list --zookeeper localhost:2181</strong></pre>
<p>The minimal output you will see in the console is <kbd>stocks</kbd>, which is our topic name:</p>
<pre>stocks</pre>
<p>To connect to the Kafka cluster, we won't use an IP address or a Pod name (which will vary upon restarts). Instead, we will use the service name, which will let you reach the cluster through an alias. You can check the available service names with the following command:</p>
<pre><strong>oc get services -o=name</strong></pre>
<p>The output of the preceding command will be restricted to the <kbd>name</kbd> column. In our case, it will look as follows:</p>
<pre><strong>service/my-kafka-kafka-bootstrap</strong><br/><strong>service/my-kafka-kafka-brokers</strong><br/><strong>service/my-kafka-zookeeper-client</strong><br/><strong>service/my-kafka-zookeeper-nodes</strong></pre>
<p>The service name we are interested in is <kbd>my-kafka-kafka-bootstrap</kbd>, which we will add to our Quarkus project soon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shaping up our project for native cloud execution</h1>
                </header>
            
            <article>
                
<p>To run our project on OpenShift, we will apply some minimal changes to the configuration file so that we can reach the Kafka service name we have just determined. In the following code, we have highlighted the changes that we have to apply to the <kbd>application.properties</kbd> file:</p>
<pre>mp.messaging.outgoing.stock-quote.connector=smallrye-kafka<br/>mp.messaging.outgoing.stock-quote.topic=stocks<br/>mp.messaging.outgoing.stock-quote.value.serializer=org.apache.kafka.common.serialization.StringSerializer<br/><strong>mp.messaging.outgoing.stock-quote.bootstrap.servers=my-kafka-kafka-bootstrap:9092</strong><br/> <br/>  <br/>mp.messaging.incoming.stocks.connector=smallrye-kafka<br/>mp.messaging.incoming.stocks.topic=stocks<br/>mp.messaging.incoming.stocks.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer<br/><strong>mp.messaging.incoming.stocks.bootstrap.servers=my-kafka-kafka-bootstrap:9092</strong></pre>
<p>As you can see, in the preceding configuration, we have used the <kbd>bootstrap.servers</kbd> property to specify the list of Kafka servers (<kbd>host:port</kbd>).</p>
<div class="packt_infobox">Multiple servers can be added in the configuration by using a comma to separate each entry.</div>
<p>Within the source code of this example, you will also find that all POJO classes that have been serialized in the messaging stream are annotated with <kbd>@io.quarkus.runtime.annotations.RegisterForReflection</kbd>, as follows:</p>
<pre><strong>@RegisterForReflection</strong><br/>public class Quote   { . . . }</pre>
<div class="paragraph">
<p>As a matter of fact, when building a native executable, GraalVM does some assumptions to remove all the classes, methods, and fields that are not used directly in your code. The elements that are used via reflection are not part of the call tree, so they are candidates when it comes to eliminating them from the native executable. Since JSON libraries heavily rely on reflection to perform their job, we must explicitly tell GraalVM not to exclude them by using the <kbd>@RegisterForReflection</kbd> annotation.</p>
<p>That's the small change we have applied in order to publish it to the cloud. Now, build and deploy the native application with the following commands:</p>
</div>
<pre>#Build the native application<br/><strong>mvn clean package -Pnative -Dnative-image.docker-build=true</strong><br/><br/>#Create a new build for it <br/><strong>oc new-build --binary --name=quarkus-kafka -l app=quarkus-kafka</strong><br/><br/>#Patch the Docker.native file <br/><strong>oc patch bc/quarkus-kafka -p "{\"spec\":{\"strategy\":{\"dockerStrategy\":{\"dockerfilePath\":\"src/main/docker/Dockerfile.native\"}}}}"</strong><br/><br/>#Deploy the application in the build<br/><strong>oc start-build quarkus-kafka --from-dir=. --follow</strong><br/> <br/># To instantiate the image as new app<br/><strong>oc new-app --image-stream=quarkus-kafka:latest</strong><br/> <br/># To create the route<br/><strong>oc expose service quarkus-kafka</strong></pre>
<div class="packt_infobox">Please note that you can find the preceding script, that is, <kbd>deploy-openshift.sh</kbd>, in the <kbd>Chapter10/kafka-openshift</kbd> folder.</div>
<p>Once the preceding script has been executed, verify that the <kbd>quarkus-kafka</kbd> Pod is up and running by using the following command:</p>
<pre><strong>oc get pods</strong></pre>
<p>The output will confirm this:</p>
<pre><strong>NAME                 READY   STATUS      RESTARTS    AGE</strong><br/><strong>kafka-demo-1-deploy   0/1     Completed   0          30s</strong><br/><strong>kafka-demo-1-p9qdr    1/1     Running     0          36s</strong></pre>
<p>You can check the route address as follows:</p>
<pre><strong>oc get routes</strong></pre>
<p>The route will be under the <kbd>HOST/PORT</kbd> column output:</p>
<pre><strong>NAME            HOST/PORT                                                 PATH   SERVICES        PORT       TERMINATION   WILDCARD</strong><br/><strong>quarkus-kafka   quarkus-kafka-kafka-demo.apps.fmarchio-qe.qe.rh-ocs.com          quarkus-kafka   8080-tcp                 None</strong></pre>
<p>If you want to reach your application with just a click, head to the <span class="packt_screen">Administration</span> console and select <span class="packt_screen">Networking</span> | <span class="packt_screen">Routes</span>. Then, click on the <span class="packt_screen">Route Location</span>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/802028dc-c9bd-4ca8-8832-550a3ca1ac8c.png" style=""/></div>
<p>Once the configured timeout for emitting quotes elapses, you will see the stock trading application in action on OpenShift:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9f3994e6-0532-489d-b0a2-29653bbdc1da.png" style=""/></div>
<p>We've come to the end of our glorious journey of Apache Kafka Streaming! In the next section, we will learn how to approach another candidate solution for streaming messaging, which is based on the AMQP protocol instead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming messages with AMQP</h1>
                </header>
            
            <article>
                
<p>If you have only just found Quarkus after years in the Java Enterprise community, you will already be familiar with message brokers, which are used to allow different Java applications to communicate using JMS as a standard protocol. Although JMS is a robust and mature solution for implementing a messaging system, one of the main limitations of it is that it's focused exclusively on Java. In the microservices world, it's fairly common to use different languages to compose the overall system architecture, so a platform-independent solution is required. In this context, AMQP offers a set of advantages that make it a perfect fit for implementing the Reactive Streams API when it comes to microservices in a distributed system.</p>
<p>In a nutshell, the following are some of the main features of the AMQP protocol:</p>
<ul>
<li>It provides a platform-independent wire-level messaging protocol that allows interoperability across multiple languages and platforms.</li>
<li>It is a wire-level protocol, by which data is sent across the network as a stream of bytes.</li>
<li>It can achieve high performance while working at low-level byte streams.</li>
<li>It supports long-lived messaging and classic message queues.</li>
<li>It supports distribution patterns such as round-robin (where the load is equally distributed across servers) and store and forward (where messages are stored at the sender side in a persistence store and are then forwarded to the receiver side).</li>
<li>It provides support for transactions (across message destination), as well as distributed transactions using common standards (XA, X/Open, MS DTC).</li>
<li>It provides support for data encryption using SASL and TLS protocols.</li>
<li>It allows us to control the message flow with metadata.</li>
<li>It provides flow control of messages to control backpressure.</li>
</ul>
<p>To let our applications interact with AMQP, we need a broker that supports this protocol. A commonly adopted solution in the Java Enterprise is <strong>Apache Artemis ActiveMQ</strong> (<a href="https://activemq.apache.org/components/artemis/">https://activemq.apache.org/components/artemis/</a>), which is compatible with Java Enterprise <strong>Message-Oriented Middleware</strong> (<strong>MOM</strong>) <span>as well</span>. In the next section, we will learn how to start it and configure it in our stock quotes application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the AMQP broker</h1>
                </header>
            
            <article>
                
<p>In order to kick-start <span>our application </span>as quickly as possible, we will use a Docker Compose script. This will download a suitable version of the message broker and set some environment variables that are needed so that we can reach the broker.</p>
<p>Simply start the <kbd>docker-compose.yaml</kbd> file contained in the <kbd>amqp</kbd> folder with the following command:</p>
<pre><strong>docker-compose up</strong></pre>
<p>If the startup is successful, you should see the following output:</p>
<pre><strong>artemis_1  | 2019-10-26 17:20:47,584 INFO  [org.apache.activemq.artemis] AMQ241001: HTTP Server started at http://0.0.0.0:8161</strong><br/><strong>artemis_1  | 2019-10-26 17:20:47,584 INFO  [org.apache.activemq.artemis] AMQ241002: Artemis Jolokia REST API available at http://0.0.0.0:8161/console/jolokia</strong><br/><strong>artemis_1  | 2019-10-26 17:20:47,584 INFO  [org.apache.activemq.artemis] AMQ241004: Artemis Console available at http://0.0.0.0:8161/console</strong></pre>
<p>You can verify that the Kafka and ZooKeeper containers are up and running by executing the <kbd>docker ps</kbd> command:</p>
<pre><strong>docker ps --format '{{.Names}}'</strong></pre>
<p>The preceding command will display the following active processes:</p>
<pre><strong>amqp_artemis_1</strong></pre>
<p>Now, let's configure our application so that it can use ActiveMQ. You will find the updated application in the <kbd>Chapter10/amqp</kbd> folder of this book's GitHub repository. First of all, we need to replace Kafka's Reactive Messaging dependency with the AMQP Reactive Messaging dependency:</p>
<pre>&lt;dependency&gt;<br/>      &lt;groupId&gt;io.quarkus&lt;/groupId&gt;<br/>      &lt;artifactId&gt;quarkus-smallrye-reactive-messaging-amqp&lt;/artifactId&gt;<br/>&lt;/dependency&gt;</pre>
<p>In terms of application configuration, some changes need to be made to our <kbd>application.properties</kbd> file. First of all, we need to include the username and password that we have set in <kbd>docker-compose.yaml</kbd> (<kbd>quarkus/quarkus</kbd>):</p>
<pre>amqp-username=quarkus<br/>amqp-password=quarkus</pre>
<p>Then, we need to configure the AMQP connector so that we can write to the <kbd>stock-quote</kbd> queue, by specifying that the queue is durable (for example, persists to disk and survives a broker restart):</p>
<pre> mp.messaging.outgoing.stock-quote.connector=smallrye-amqp<br/> mp.messaging.outgoing.stock-quote.address=stocks<br/> mp.messaging.outgoing.stock-quote.durable=true</pre>
<p> Conversely, we need to configure the AMQP connector so that it can read from the <kbd>stocks</kbd> queue:</p>
<pre> mp.messaging.incoming.stocks.connector=smallrye-amqp<br/> mp.messaging.incoming.stocks.durable=true  </pre>
<p>Now, we can bootstrap the application as usual with the following command:</p>
<pre><strong>mvn install quarkus:dev</strong></pre>
<p>The welcome page of the application (available at <kbd>http://localhost:8080</kbd>) will show the stock quotes ticker in action, which now uses ActiveMQ as its broker. As you can see, a minimal adjustment has been made to the UI (just the title), but it accurately masks the changes that were made under the hood:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ef0a228-fb93-4f34-94fe-41939a735c63.png" style=""/></div>
<p>You can find out more about this process by logging in to the AMQ management console, which is available at <kbd>http://localhost:8161/console</kbd>. Once you've logged in with your configured credentials (<kbd>quarkus/quarkus</kbd>), you can check that the destination queue has been created in the list of available addresses:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/30854435-e1e8-475b-a47c-0950985f5de4.png" style=""/></div>
<p>By selecting the <kbd>stocks</kbd> destination, you can check any further details in the main panel of the management console, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/02058696-2aea-4437-a369-14cf21332b8e.png" style=""/></div>
<p>When you're done, stop all the running containers with the following command:</p>
<pre><strong>docker stop $(docker ps -a -q)</strong></pre>
<p>The preceding command will display a list of all the container layers that have been stopped, like so:</p>
<pre><strong>6a538738088f</strong><br/><strong>f2d97de3520a</strong></pre>
<p>Then, verify that the ActiveMQ containers have been stopped by executing the <kbd>docker ps</kbd> command once more:</p>
<pre><strong>docker ps --format '{{.Names}}'</strong></pre>
<p>The preceding command shouldn't produce any output. Now, let's test the same application stack in the cloud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming messages to AMQ in the cloud</h1>
                </header>
            
            <article>
                
<p>The last thing we will do is deploy the Quarkus application to the cloud while using AMQ as a messaging broker. For this purpose, we will plug the ActiveMQ Docker image that we tested previously into OpenShift (more details about this image can be found on GitHub at <a href="https://github.com/vromero/activemq-artemis-docker">https://github.com/vromero/activemq-artemis-docker</a>).</p>
<p>First of all, create a new project named <kbd>amq-demo</kbd>:</p>
<pre><strong>oc new-project amq-demo</strong></pre>
<p>The output will confirm that the project namespace has been created in your virtual address:</p>
<pre><strong>Now using project "amq-demo" on server "https://api.fmarchioni-openshift.rh.com:6443"</strong></pre>
<p>Next, deploy the AMQ server to your project with the following command, which will set the username and password so that you can access the broker:</p>
<pre><strong>oc new-app --name=artemis vromero/activemq-artemis:2.9.0-alpine -e ARTEMIS_USERNAME=quarkus -e ARTEMIS_PASSWORD=quarkus -e RESTORE_CONFIGURATION=true</strong></pre>
<p>Take note of the <kbd>RESTORE_CONFIGURATION=true</kbd> environment variable. This is needed since OpenShift automounts empty volumes in all the declared volumes. Since this behavior impacts the <kbd>/etc</kbd> folder of this image, which is where the configuration is stored, we need to set the <kbd>RESTORE_CONFIGURATION</kbd> environment variable to <kbd>true</kbd>.</p>
<p>After executing the <kbd>new-app</kbd> command, the following output will be displayed:</p>
<pre><strong>--&gt; Found container image 2fe0af6 (10 days old) from Docker Hub for "vromero/activemq-artemis:2.9.0-alpine"</strong><br/> <br/><strong>* An image stream tag will be created as "artemis:2.9.0-alpine" that will track this image</strong><br/><strong>* This image will be deployed in deployment config "artemis"</strong><br/><strong>* Ports 1883/tcp, 5445/tcp, 5672/tcp, 61613/tcp, 61616/tcp, 8161/tcp, 9404/tcp will be load balanced by service "artemis"</strong><br/><strong>* Other containers can access this service through the hostname "artemis"</strong><br/><strong>* This image declares volumes and will default to use non-persistent, host-local storage.</strong><br/><strong>You can add persistent volumes later by running 'oc set volume dc/artemis --add ...'</strong><br/><strong>--&gt; Creating resources ...</strong><br/><strong>     imagestream.image.openshift.io "artemis" created</strong><br/><strong>     deploymentconfig.apps.openshift.io "artemis" created</strong><br/><strong>     service "artemis" created</strong><br/><strong>--&gt; Succes</strong></pre>
<p>You can check the status of the Pods with the <kbd>oc</kbd> command:</p>
<pre><strong>oc get pods</strong></pre>
<p>The following output confirms that the <kbd>artemis</kbd> Pod is in a running state:</p>
<pre><strong>NAME               READY   STATUS      RESTARTS   AGE</strong><br/><strong>artemis-1-deploy   0/1     Completed   0          80s</strong><br/><strong>artemis-1-p9qdr    1/1     Running     0          76s</strong></pre>
<p>Finally, let's check the service name, which will be <kbd>artemis</kbd>:</p>
<pre><strong>oc get services -o name</strong></pre>
<p>Check that the output that's returned matches the output shown here:</p>
<pre><strong>service/artemis</strong></pre>
<p>Now, let's go for the final kill: we will deploy the application located in the <kbd>Chapter10/amqp-openshift</kbd> directory. Within this folder, you will find the stock trade application, which has been configured to stream messages on AMQ.</p>
<p>Here is the updated <kbd>application.properties</kbd> file, which contains the AMQ username and password, along with the host and the port where the service runs:</p>
<pre><strong>amqp-username=quarkus</strong><br/><strong>amqp-password=quarkus</strong><br/> <br/># Configure the AMQP connector to write to the `stocks`  address<br/>mp.messaging.outgoing.stock-quote.connector=smallrye-amqp<br/>mp.messaging.outgoing.stock-quote.address=stocks<br/>mp.messaging.outgoing.stock-quote.durable=true<br/><strong>mp.messaging.outgoing.stock-quote.host=artemis</strong><br/><strong>mp.messaging.outgoing.stock-quote.port=5672</strong><br/> <br/># Configure the AMQP connector to read from the `stocks` queue<br/>mp.messaging.incoming.stocks.connector=smallrye-amqp<br/>mp.messaging.incoming.stocks.durable=true<br/><strong>mp.messaging.incoming.stocks.host=artemis</strong><br/><strong>mp.messaging.incoming.stocks.port=5672</strong></pre>
<p>Next, we will deploy the application contained in the same folder (<kbd>Chapter10/amqp-openshift</kbd>) into OpenShift. For convenience, you can simply run the <kbd>deploy-openshift.sh</kbd> script, which can be found in the same directory. Here is the content of the script, which should be pretty familiar to you:</p>
<pre>#Build native image of the project<br/><strong>mvn clean package -Pnative -Dnative-image.docker-build=true</strong><br/> <br/># Create a new binary build<br/><strong>oc new-build --binary --name=quarkus-amq -l app=quarkus-amq</strong><br/><br/># Patch the native file <br/><strong>oc patch bc/quarkus-amq -p "{\"spec\":{\"strategy\":{\"dockerStrategy\":{\"dockerfilePath\":\"src/main/docker/Dockerfile.native\"}}}}"</strong><br/><br/># Add project to the build<br/><strong>oc start-build quarkus-amq --from-dir=. --follow</strong><br/> <br/># To instantiate the image<br/><strong>oc new-app --image-stream=quarkus-amq:latest</strong><br/> <br/># To create the route<br/><strong>oc expose service quarkus-amq</strong></pre>
<p>Then, check that the <kbd>quarkus-amq</kbd> Pod is in a running state:</p>
<pre><strong>oc get pods</strong></pre>
<p>The output that you'll receive confirms this:</p>
<pre><strong> NAME                   READY   STATUS      RESTARTS   AGE</strong><br/><strong> artemis-1-deploy       0/1     Completed   0          9m9s</strong><br/><strong> artemis-1-p9qdr        1/1     Running     0          9m5s</strong><br/><strong> quarkus-amq-1-deploy   0/1     Completed   0          14s</strong><br/><strong> quarkus-amq-1-zbvrl    1/1     Running     0          10s</strong></pre>
<p>Now, you can verify that the application works by clicking on the route address. Just go to the <span class="packt_screen">Networking |</span> <span class="packt_screen">Routes</span> path in the console:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8e60ed7-837e-45d8-abc0-2c278f101eec.png" style=""/></div>
<p class="mce-root"/>
<p>The output will be pretty much the same, except for the route name, which celebrates your last achievement in this book:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/51eed0a9-3623-488e-ad76-d2c252a90e98.png" style=""/></div>
<p>That's all, folks!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to use <span>CDI beans to produce, consume, and process messages using the Reactive Messaging specification. </span>We also learned how to bootstrap and configure Apache Kafka and Active MQ's broker so that it acts as a distributed streaming platform for our CDI Beans. To put our new skills in place, we created an example stock trade application, which was initially run in development mode and then deployed as a native image on top of OpenShift.</p>
<p>Now, we have reached the end of this book, where we learned about the story of the gradual renewal of the Java Enterprise application, from monolith to native microservices running in the cloud. It was an exciting journey that certainly set a milestone, but this isn't the end of our hard work <span>–</span> it's just the end of this story. </p>


            </article>

            
        </section>
    </body></html>