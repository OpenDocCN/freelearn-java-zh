- en: Microservices and System Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapters covered how to develop a single enterprise application
    with Java EE. Modern applications contain infrastructure and configuration definitions
    as code, making it possible to create environments in automated ways, either on
    premises or in cloud platforms. Continuous Delivery pipelines together with sufficient,
    automated test cases make it possible to deliver enterprise applications with
    high quality and productivity. Modern zero-dependency Java EE approaches support
    these efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise systems rarely come with single responsibilities that could be reasonably
    mapped into single enterprise applications. Traditionally, enterprise applications
    combined multiple aspects of the business into monolithic applications. The question
    is, whether this approach to crafting distributed systems is advisable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: The motivations behind distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possibilities and challenges of distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to design interdependent applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application boundaries, APIs, and documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency, scalability, challenges, and solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event sourcing, event-driven architectures, and CQRS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservice architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Java EE fits the microservice world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to realize resilient communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivations behind distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first questions should ask for the need for distribution. There are
    several technical motivations behind designing distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Typical enterprise scenarios are in essence distributed. Users or other systems
    that are spread across locations need to communicate with a service. This needs
    to happen over the network.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason is scalability. If a single application reaches the point where
    it cannot reliably serve the overall load of clients, the business logic needs
    to be distributed to multiple hosts.
  prefs: []
  type: TYPE_NORMAL
- en: A similar reasoning aims toward a system's fault tolerance. Single applications
    represent single points of failure; if the single application is unavailable,
    the service won't be usable by the clients. Distributing services to multiple
    locations increases availability and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: There are also other less technology-driven motivations. An application represents
    certain business responsibilities. In Domain-Driven Design language they are contained
    in the application's **bounded context**. Bounded contexts include the business
    concerns, logic, and data of the application and differentiate it from external
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way as engineers cluster code responsibilities into packages and
    modules, it certainly makes sense to craft contexts on a system scale as well.
    Coherent business logic and functionality is grouped into separate services as
    part of separate applications. The data and schema is also part of a bounded context.
    It can therefore be encapsulated into several database instances, which are owned
    by the corresponding distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all these motivations, especially technical ones such as scalability, why
    shouldn't engineers distribute everything then? Distribution comes with certain
    overheads.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the overall overhead that comes on top of the system's distilled
    business logic will be multiplied by the number of applications involved. For
    example, a single, monolithic application requires a monitoring solution. Distributing
    this application will cause all resulting applications to be monitored as well.
  prefs: []
  type: TYPE_NORMAL
- en: Communication overhead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In distribution, first of all, there is an overhead cost in communicating between
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Technology is very effective in communicating within a single process. There
    is effectively no overhead in calling functionality that is part of the application.
    As soon as inter-process or remote communication is required, engineers have to
    define interface abstractions. Communication protocols such as HTTP have to be
    defined and used in order to exchange information.
  prefs: []
  type: TYPE_NORMAL
- en: This requires certain time and effort. Communication between applications has
    to be defined, implemented, and maintained. Within a single application, the communication
    is reflected in method invocations.
  prefs: []
  type: TYPE_NORMAL
- en: The required communication also becomes a concern of the business use case.
    It can no longer be assumed that certain functionality or data can be just used
    without any overhead. Communicating with the distributed system becomes a responsibility
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Performance overhead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributing applications at first decreases performance of the overall systems.
    Computer networks are slower than communication within a single host. Therefore
    networking will always come with a certain performance overhead.
  prefs: []
  type: TYPE_NORMAL
- en: The overhead in performance is not only caused by the communication itself,
    but also the need to synchronize. Synchronization within a single process already
    consumes certain processing time, and this impact is much bigger when distribution
    is involved.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite this overhead in performance, distribution eventually increases
    the overall performance of the system as its applications scale out. Scaling horizontally
    always comes with a certain performance overhead compared to a single instance.
  prefs: []
  type: TYPE_NORMAL
- en: Organizational overhead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed systems containing several applications certainly need more organizational
    effort than a single one.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple applications imply multiple deployments that need to be managed. Deploying
    new versions may have an impact on dependent applications. Teams need to ensure
    that versions of deployed applications work together well. Single, monolithic
    applications are not affected by this since they are consistent within themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, multiple applications are developed in several projects and repositories,
    usually by multiple development teams. In particular, having multiple teams requires
    communication, not necessarily technically, but human-related communication. In
    the same way as for deploying applications, responsibilities, system boundaries,
    and dependencies need to be agreed upon.
  prefs: []
  type: TYPE_NORMAL
- en: How to design systems landscapes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all of these challenges and overheads involved, a lot of scenarios still
    require distribution. It's important to mention, that there must be enough motivation
    behind distributing systems. Distribution comes with costs and risks. If it's
    not necessary to distribute, building monolithic applications is always to be
    preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look into how to design reasonable system landscapes, tailored for
    business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Context maps and bounded contexts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bounded contexts** define the application''s responsibilities in business
    logic, behavior, and data ownership. So-called **context maps**, as described
    in Domain-Driven Design, represent the entire system landscape. It shows the individual
    responsibilities, contexts, and belongings of its applications. Bounded contexts
    therefore fit within a context map to show how they exchange information among
    each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the context map of the *cars* domain, including two bounded
    contexts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cab69c57-671a-4028-b544-a1a602df0f2b.png)'
  prefs: []
  type: TYPE_IMG
- en: It's advisable to consider the different responsibilities of the system before
    designing and carving out applications. Lack of clarity on an application's responsibilities
    usually emerges quickly as soon as the system's context map is recorded.
  prefs: []
  type: TYPE_NORMAL
- en: Context maps are not only helpful during the initial project definition, but
    also during revisiting and refining responsibilities once business functionality
    changes. In order to prevent the boundaries and belongings of distribution applications
    from drifting apart, it's advisable to reflect on them from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: Separation of concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application's responsibilities should be clearly defined and differentiated
    from other applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way as with code level, the concerns of several applications should
    be separated. The single responsibility principle holds true here as well.
  prefs: []
  type: TYPE_NORMAL
- en: The application's concerns include all business concerns, application boundaries,
    and owned data. As the business logic evolves and changes over time, these concerns
    should be revisited from time to time. This may result in applications that split
    up or get merged into single ones. The responsibilities and concerns that emerge
    from the context map should be reflected in the system's applications.
  prefs: []
  type: TYPE_NORMAL
- en: Data and data ownership is an important aspect of distributed applications.
    The business processes, being part of the bounded context defines the data involved
    in the use cases. Owned data are a concern of the specific applications and are
    only shared via the defined boundaries. Use cases that require data that is under
    the responsibility of another, remote application need to retrieve the information
    by remotely invoking the corresponding use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Teams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Teams and organizational structure are other important aspects to consider when
    designing distributed systems, since, as of writing this book, software is developed
    by humans. Considering Conway's law, that the organization's communication structure
    will eventually leak into the constructed system, teams should be defined similarly
    to the applications in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Or, in other words, it makes sense for a single application to be developed
    by only a single team. Depending on the responsibilities and sizes, a single team
    can potentially craft multiple applications.
  prefs: []
  type: TYPE_NORMAL
- en: Again, comparing to the project code structure, this is a similar approach as
    horizontal versus vertical module layering. Similar to business motivated module
    structures, teams are therefore organized vertically, representing the structure
    of the context map. For example, rather than having several expert teams on software
    architecture, development, or operations, there will be teams for *car manufacture*,
    *assembly line*, and *order management*.
  prefs: []
  type: TYPE_NORMAL
- en: Project life cycles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With individual teams being involved in developing distributed systems, applications
    will have independent project life cycles. This includes the way teams operate,
    for example, how they organize their sprint cycles.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment cycles and schedules also emerge from the project life cycle.
    For the overall system to stay consistent and functional, potential dependencies
    on deployments of other applications need to be defined. This does not only target
    the application's availability.
  prefs: []
  type: TYPE_NORMAL
- en: Deployed application versions need to be compatible. In order to ensure this,
    applications that are dependent need to be clearly represented in the context
    map. Teams will have to communicate when dependent services introduce changes.
  prefs: []
  type: TYPE_NORMAL
- en: Again, painting a clear context map containing the bounded contexts helps define
    the interdependent applications and their responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: How to design system interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the responsibilities of the system landscape have been defined, the boundaries
    of dependent systems need to be specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous chapters, we have seen various communication protocols and how
    to implement them. Besides the actual implementation, the question is now: how
    to design the interfaces of applications? Which aspects need to be considered,
    especially in distributed systems?'
  prefs: []
  type: TYPE_NORMAL
- en: API considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The applications within a system are carved out based on their business responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the application's API should represent that business logic as well.
    The exposed API represents the business use cases a certain application comprises.
    This implies that a business domain expert can, without any further technical
    knowledge, identify the exposed business use cases from an API.
  prefs: []
  type: TYPE_NORMAL
- en: The business use cases are ideally offered in clear, lean interfaces. Invoking
    a use case should not require more technically-motivated communication steps or
    details than being part of the business logic. For example, if the *create a car*
    use case could be invoked as a single operation, the API of the *car manufacture*
    application should not require multiple invocations providing technical details.
  prefs: []
  type: TYPE_NORMAL
- en: An API should abstract the business logic in a clear, lean interface.
  prefs: []
  type: TYPE_NORMAL
- en: The API should therefore be decoupled from the application's implementation.
    The interface implementation should be independent from the chosen technology.
    This also implies that a communication format is chosen that doesn't set many
    constraints on the used technology.
  prefs: []
  type: TYPE_NORMAL
- en: It therefore makes sense to prefer technology that sets on standard protocols
    such as HTTP. It's more likely that engineer have knowledge in commonly used protocols,
    as that are supported by various technologies and tools. Creating application
    interfaces in HTTP web services allows clients to be developed in every technology
    that supports HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting the business logic in clear, lean interfaces that use standard protocols
    also enables change in used implementations, technologies, and platforms. Java
    Enterprise applications that only expose HTTP services could replace their technology
    with other implementations, without requiring dependent clients to change.
  prefs: []
  type: TYPE_NORMAL
- en: Interface management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application interfaces are often subject to change during the development process.
  prefs: []
  type: TYPE_NORMAL
- en: New business use cases are included and existing one refined. The question is,
    how are these changes reflected in the API?
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the nature and environment of the enterprise application how stable
    the API needs to be. If the project team is both in charge of the service, all
    clients and their life cycles, the API can introduce arbitrary changes that are
    reflected in the clients at the same time. The case is the same if for some reason
    the life cycles of involved applications are identical.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, life cycles of distributed systems aren't that tightly coupled. For
    any other client/server model, or applications that have different life cycles,
    the APIs must not break existing clients. This means that the APIs are fully backwards-compatible,
    not introducing breaking changes.
  prefs: []
  type: TYPE_NORMAL
- en: Change-resilient APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are certain principles in designing interfaces that prevent unnecessary
    breaks. For example, introducing new, optional payload data should not break the
    contract. Technology should be resilient as far as it can continue to work if
    all necessary data is provided. This matches the idea of *being conservative in
    what you do and liberal in what you accept*.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore adding new, optional functionality or data should be possible without
    breaking clients. But what if existing logic changes?
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the business logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The question to be asked here is what a breaking change in the API means for
    the business use case. Is the application's past behavior not valid anymore? Should
    the client have to stop working from now on?
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to, for example, a vendor of a widely-used smartphone app
    that decides to break existing versions and to force the users to update the installations
    to its latest version. There is arguably no need in doing so for existing functionality
    to continue.
  prefs: []
  type: TYPE_NORMAL
- en: If for some reason the existing use cases can't be used *as is* anymore, some
    additional, compensating business logic should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Hypermedia REST and versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hypermedia REST APIs can bring some relief with this issue. In particular, Hypermedia
    controls provide the ability to evolve the API by dynamically defining resource
    links and actions. The clients of the REST service will adapt to the changes in
    accessing the services and considerately ignore unknown functionality.
  prefs: []
  type: TYPE_NORMAL
- en: A quite often suggested possibility is to version the API. This means introducing
    different operations or resources, such as `/car-manufacture/v1/cars`, with the
    version as the identifying part of the API. Versioning APIs, however, contradicts
    the idea of clean interfaces. In particular, since REST APIs resources represent
    domain entities, introducing several *versions* of a car doesn't make sense in
    business terms. The car entity is identified by its URI. Changing the URI to reflect
    changes in the business functionality would imply a change to the car's identity.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes several, different representations, or versions, of the same domain
    entities are required, for example, JSON mappings containing different sets of
    properties. Via HTTP interface this is achievable via **content negotiation**,
    by defining content type parameters. For example, different JSON representations
    for the same car can be requested via content types such as `application/json;vnd.example.car+v2`,
    if supported by the corresponding service.
  prefs: []
  type: TYPE_NORMAL
- en: Managing interfaces is a relevant topic for distributed systems. It's advisable
    to carefully design APIs upfront, with backwards-compatibility in mind. Extra
    efforts, such as additional operations that prevent an API from breaking existing
    functionality, should be preferred over clean interfaces that disrupt clients.
  prefs: []
  type: TYPE_NORMAL
- en: Documenting boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application boundaries that define APIs to invoke the application's business
    logic need to be made public to its clients, for example, other applications within
    the system. The question is, what information needs to be documented?
  prefs: []
  type: TYPE_NORMAL
- en: The application's bounded context is part of the context map. Therefore, the
    domain responsibilities should be clear. The application fulfills certain business
    use cases within its context.
  prefs: []
  type: TYPE_NORMAL
- en: This domain information needs to be documented first. Clients should be aware
    of what the application offers. This includes the use cases as well as the exchanged
    information and data ownership.
  prefs: []
  type: TYPE_NORMAL
- en: The responsibility of the *car manufacture* application is to assemble cars
    due to provided, exact specifications. The status information of manufactured
    cars is owned by the application for the whole process of assembling, until the
    car reaches the end of the production line and is ready for delivery. The application
    can be polled to provide status updates about the creation process of a car.
  prefs: []
  type: TYPE_NORMAL
- en: The application's domain description should contain the information the clients
    require, be precise in responsibilities, but not too verbose, only exposing what
    clients *need to know*.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the business domain, there are technical aspects that need to be documented.
    Client applications need to be programmed against a system's API. They require
    information about the communication protocols, as well as data formats.
  prefs: []
  type: TYPE_NORMAL
- en: We covered several communication protocols and how to implement them in the
    second chapter of this book. At the time of writing, one of the most used protocols
    is HTTP, together with JSON or XML content types. With the example of HTTP, what
    needs to be documented?
  prefs: []
  type: TYPE_NORMAL
- en: HTTP endpoints, especially those following the REST constraints, represent the
    domain entities as resources, locatable by URLs. The available URLs need to be
    documented first. Clients will connect against these URLs in order to perform
    some business use cases. For example, the `/car-manufacture/cars/<car-id>` URL
    will refer to a particular car specified by its identifier.
  prefs: []
  type: TYPE_NORMAL
- en: The content type with detailed mapping information needs to be documented as
    well. Clients need to be aware of the structure and properties within the used
    content type.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a car specification that is provided in order to create a car
    contains an *identifier*, an *engine type,* and a *chassis color*. The JSON format
    will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The types and available values need to be documented as well. They will point
    to the business domain knowledge, the semantics behind an engine type. This is
    important, that both the content types as well as the semantics of the information
    are documented.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of HTTP there will be more aspects to be documented such as potentially
    required header information, status codes provided by the web service, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: All this documentation certainly depends on the used technology and communication
    protocol. The business domain, however, should also be part of the documentation,
    providing as much context as required.
  prefs: []
  type: TYPE_NORMAL
- en: The application's API documentation is part of the software project. It needs
    to be shipped together with the application in a particular version.
  prefs: []
  type: TYPE_NORMAL
- en: In order to ensure that the documentation matches the application's version,
    it should be part of the project repository, residing under version control as
    well. Therefore, it's highly advisable to use text-based documentation formats
    instead of binary formats such as Word documents. Lightweight markup languages
    such as **AsciiDoc** or **Markdown** have proven themselves well in the past.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of maintaining the documentation directly in the project, next to
    the application's sources, is to ensure the creation of documentation versions
    that are consistent with the developed service. Engineers are able to perform
    both changes in one step. Doing so prevents the documentation and service version
    from diverging.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of tool support in documenting application boundaries depending
    on the communication technology. For HTTP web services for example, the **OpenAPI
    Specification** together with **Swagger** as a documentation framework are widely
    used. Swagger outputs the API definition as browsable HTML, making it easy for
    developers to identify the offered resources together with their usages.
  prefs: []
  type: TYPE_NORMAL
- en: Using Hypermedia REST services, however, gets rids of the biggest necessity
    of service documentation. Providing the information of which resources are available
    in links removes the need for documenting URLs. In fact, the server gets back
    the control of how URLs are constructed. Clients only enter an entry point, for
    example `/car-manufacture/`, and follow the provided Hypermedia links based on
    their relations. The knowledge what a car URL consists of solely resides on the
    server side and is explicitly not documented.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is especially true for Hypermedia controls, not only directing the client
    to resources, but providing information on how to consume it. The *car manufacture*
    service that tells a client how to perform the `create-car` action: A POST request
    to `/car-manufacture/cars` is needed, including a request body in JSON content
    type with properties `identifier`, `engine-type`, and `color`.'
  prefs: []
  type: TYPE_NORMAL
- en: The client needs to know the semantics of all relations and action names as
    well as the properties and where they originate. This is certainly client logic.
    All information on how to consume the API becomes part of the API. Designing REST
    services then eliminates the need for a lot of documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency versus scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Certainly it's necessary for distributed system to communicate. Since computer
    networks cannot be considered as reliable, even not in company-internal networks,
    reliable communication is a necessity. Business use cases are required to communicate
    in a reliable way, in order to ensure correct behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this book, we introduced the so-called CAP theorem that claims that
    it's impossible for distributed data stores to guarantee at most two of the three
    specified constraints. Systems can effectively choose whether they want to guarantee
    consistency or horizontal scalability. This highly affects the communication in
    a distributed world.
  prefs: []
  type: TYPE_NORMAL
- en: In general, enterprise systems should be consistent in their use cases. Business
    logic should transform the overall system from one consistent state to another,
    different consistent state.
  prefs: []
  type: TYPE_NORMAL
- en: In distributed systems, an overall consistent state would imply that use cases
    that communicate to external concerns would have to ensure that the invoked external
    logic also adheres to consistency. This approach leads to distributed transactions.
    Use cases that are invoked on a system would execute in an *all-or-nothing* fashion,
    including all external systems. This implies a need for a lock on all involved,
    distributed functionality until every single distributed application successfully
    performed its responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this approach doesn't scale. The fact that the system is distributed
    requires this transaction orchestration to be performed over the potentially slow
    network. This introduces a bottleneck, which results in a locking situation, since
    involved applications have to block and wait a relatively large amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, synchronous, consistent communication is only advisable
    for applications that don't involve more than two applications at a time. Performance
    tests as well as production experience indicate whether a chosen communication
    scenario scales well enough for the given use case and environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using asynchronous communication is motivated by scalability. Distributed systems
    that communicate asynchronously won't, by definition, be consistent at all times.
    Asynchronous communication can happen on a logical level, where synchronous calls
    only initiate business logic without awaiting a consistent result.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look into the motivations and design behind asynchronous, eventually
    consistent communication in distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing, event-driven architectures, and CQRS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, enterprise applications are built using a model approach that
    is based on the atomic **Create Read Update Delete** (**CRUD**).
  prefs: []
  type: TYPE_NORMAL
- en: The current state of the system, including the state of the domain entities,
    is reflected in a relational database. If a domain entity is updated, the new
    state of the entity including all of its properties is put into the database and
    the old state is gone.
  prefs: []
  type: TYPE_NORMAL
- en: The CRUD approach requires applications to maintain consistency. In order to
    ensure the state of the domain entity is reflected correctly, all use case invocations
    have to be executed in a consistent manner, synchronizing modifications to the
    entities.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of CRUD-based systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This synchronization is also one of the shortcomings of CRUD-based systems,
    the way that we typically build applications.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The required synchronization prevents the system from scaling infinitely. All
    transactions are executed on the relational database instance, which eventually
    introduces a bottleneck if the system needs to scale out.
  prefs: []
  type: TYPE_NORMAL
- en: This ultimately becomes a challenge for situations with huge amounts of workloads
    or huge numbers of users. For the vast majority of enterprise applications, however,
    the scalability of relational databases is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Competing transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another challenge that comes with CRUD-based models is to handle competing transactions.
    Business use cases that include the same domain entities and operate simultaneously
    need to ensure that the resulting state of the entities is consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Editing a user's name and at the same time updating its account credit limit
    should not result in lost updates. The implementation has to ensure that the overall
    result of both transactions is still consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Competing transactions that rely on optimistic locking usually result in failing
    transactions. This is definitely not ideal from a user's perspective, but at least
    maintains consistency, rather than suppressing that a transaction has been lost
    in space.
  prefs: []
  type: TYPE_NORMAL
- en: Following this approach, however, potentially leads to unnecessary locking.
    From a business theory perspective it should be possible to simultaneously edit
    the user's name and account credit limit.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the application only stores its current state, all historical information
    about previous states is gone. The state is always overwritten by the new updates.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it hard to reproduce how an application got into its current state.
    If a current state was miscalculated from its originating use case invocations,
    there is no possibility of fixing the situation later on.
  prefs: []
  type: TYPE_NORMAL
- en: Some scenarios explicitly require reproducibility for legal terms. Some applications
    therefore include audit logs that permanently write certain information as soon
    as they happen to the system.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event sourcing is an approach that tackles reproducibility as a shortcoming
    of CRUD-based systems.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourced systems calculate the current state of the systems from atomic
    events that happened in the past. The events represent the individual business
    use case invocations, including the information provided in the invocations.
  prefs: []
  type: TYPE_NORMAL
- en: The current state is not permanently persisted, but emerges by applying all
    events one after another. The events themselves happened in the past and are immutable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give an example, a user with its characteristics is calculated from all
    events related to it. Applying `UserCreated`, `UserApproved`, and `UserNameChanged`
    one after another creates the current representation of the user up to its recent
    event:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/998e273c-5a6c-4907-9708-2acb5c58a123.png)'
  prefs: []
  type: TYPE_IMG
- en: The events contain self-sufficient information mostly concerning the corresponding
    use case. For example, a `UserNameChanged` event contains the time stamp and the
    name the user was changed to, not other, unrelated information about the user.
    The event's information is therefore atomic.
  prefs: []
  type: TYPE_NORMAL
- en: Events are never changed nor deleted. If a domain entity is removed from the
    application, there will be a corresponding deletion event such as `UserDeleted`.
    The current state of the system then won't contain this user anymore after applying
    all events.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An event-sourced application contains all of its information in atomic events.
    Therefore, the full history and context, how it got into its current state, is
    available. In order to reproduce the current state for debugging purposes, all
    events and their individual modifications to the system can be regarded.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that everything that happened to the system is stored atomically has
    a couple of benefits, not only for debugging purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Tests can make use of this information to replay everything that happened to
    a production system in system tests. Tests are then able to re-execute the exact
    business use case invocations that happened in productions. This is an advantage
    especially for system and performance tests.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true for statistics that use the atomic information to gather insights
    about the usage of the application. This enables use cases and insights that are
    designed after an application has been deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming a manager wants to know how many users were created on a Monday, after
    the application has been running for two years. With CRUD-based systems that information
    would have had to explicitly been persisted by the time the use case was invoked.
    Use cases that were not explicitly requested in the past can only be added as
    new features, and will add value in the future.
  prefs: []
  type: TYPE_NORMAL
- en: With event sourcing these functionalities are possible. Since information about
    whatever happened to the system is stored, use cases that are developed in the
    future are able to operate on data that happened in the past.
  prefs: []
  type: TYPE_NORMAL
- en: These benefits, however, are certainly possible without the need for distributed
    systems. A monolithic, independent application can base its model on event sourcing,
    gaining the same benefits from it.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually consistent real world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we go further into distributed systems in regard to consistency and scalability,
    let's look at an example of how consistent the real world is. Enterprise applications
    are typically built with the aspiration to provide full consistency. The real
    world, however, is highly distributed and not consistent at all.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you're hungry and you want to eat a burger. So you go to a restaurant,
    sit at a table, and tell the waiter that you would like to have a burger. The
    waiter will accept your order. Now, although your order has been accepted this
    doesn't necessarily mean that you will receive your meal. The process of ordering
    a meal is not fully consistent.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of things can go wrong at this point. For example, the chef may tell the
    waiter that unfortunately the last burger patty was just used and there won't
    be more burgers for the day. So although your order has transactionally been accepted,
    the waiter will come back and tell you that the order won't be possible.
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of asking you to leave, the waiter might suggest to you an alternative
    dish. And if you're hungry and fine with the substitute you might eventually receive
    a meal.
  prefs: []
  type: TYPE_NORMAL
- en: This is how the highly distributed real world handles business use case transactions.
  prefs: []
  type: TYPE_NORMAL
- en: If the restaurant would be modeled in a fully consistent way the scenario would
    look different. In order to guarantee that an order is only accepted if it will
    be possible to provide the prepared meal, the whole restaurant would need to be
    locked down. The customers would have to wait and hold the conversation while
    the waiter goes into the kitchen and orders the meal from the chef. Since many
    other things can go wrong after ordering, the whole order transaction would actually
    have to block until the meal is fully prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this approach would not work. Instead, the real world is all about
    collaboration, intentions, and eventually dealing with issues if the intentions
    can't be fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the real world operates in an eventually consistent way. Eventually,
    the restaurant system will be in a consistent state, but not necessarily at all
    times, which leads to initially accepting orders that are actually not possible.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world processes are represented as intentions or **commands**, such as
    ordering a burger, and atomic outcomes or **events**, such as that the order has
    been accepted. Events will then cause new commands that result in new outcomes
    or failures.
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now back to the topic of distributed systems. In the same way as for a restaurant,
    distributed systems that communicate in a consistent way, via distributed transactions,
    won't be able to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven architectures solve this issue. The communication in these architectures
    happens via asynchronous events that are published and consumed reliably.
  prefs: []
  type: TYPE_NORMAL
- en: By doing so, consistent use case transactions get split up into multiple, smaller-scaled
    transactions that are consistent in themselves. This leads the overall use case
    eventually being consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see an example of how the use case of ordering a burger is represented
    in an event-driven architecture. The restaurant system consists of at least two
    distributed applications, the *waiter* and the *chef*. The restaurant applications
    communicate by listening to each other''s events. The client application will
    communicate with the waiter in order to initiate the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0479e749-f675-490e-875d-cd1964440029.png)'
  prefs: []
  type: TYPE_IMG
- en: The client orders a meal at the waiter application, which results in the `OrderPlaced`
    event. Once the event has been published reliably, the `orderMeal()` method's
    invocation returns. The client therefore is able to perform other work in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The chef system receives the `OrderPlaced` event and validates whether the order
    is possible with the currently available ingredients. If the order wouldn't be
    possible, the chef would emit a different event, such as `OrderFailedInsufficientIngredients`.
    In that case, the waiter would update the order status to failed.
  prefs: []
  type: TYPE_NORMAL
- en: When initiating the meal preparation was successful, the waiter receives the
    `MealPreparationStarted` event and updates the status of the order, what results
    in `OrderStarted`. If the client would ask the waiter about the status of their
    order, it could respond appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: At some point the meal preparation would have been finished, resulting in a
    `MealPrepared` event, which notifies the waiter to deliver the order.
  prefs: []
  type: TYPE_NORMAL
- en: Eventual consistency in event-driven architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use case of ordering a meal is eventually consistent. Publishing the events
    reliably still ensures that all clients *eventually* know about the status of
    their order.
  prefs: []
  type: TYPE_NORMAL
- en: It is somewhat fine if processing the order doesn't happen immediately or if
    the order will fail for some reason. However, it must not happen that an order
    gets lost in the system due to unavailable applications. This needs to be ensured
    when publishing the events.
  prefs: []
  type: TYPE_NORMAL
- en: There are still transactions involved here, but on a much smaller scale and
    not involving external systems. Doing so enables distributed systems to cover
    transactional use cases while still enabling horizontal scalability.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that some reliability is required for approaches like event-driven
    architectures is an important aspect in distributed systems, and should be considered
    when designing solutions and choosing technology.
  prefs: []
  type: TYPE_NORMAL
- en: Enter CQRS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's combine the motivations behind event-driven architectures and event
    sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Event-driven architectures communicate by atomic events. It makes sense to piggyback
    on this approach and build the system using event sourcing, by using the events
    as the system's source of truth. Doing so combines the benefits of both approaches,
    enabling horizontally scalable, event-sourced systems.
  prefs: []
  type: TYPE_NORMAL
- en: The question is how to model event-driven applications that base their domain
    model on events? And how to efficiently calculate and return the current state
    of domain entities?
  prefs: []
  type: TYPE_NORMAL
- en: The **Command Query Responsibility Segregation** (**CQRS**) principle describes
    how to model these applications. It is a consequence of event-driven architectures
    and is based on event sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, CQRS separates the responsibilities for commands and queries,
    namely writes and reads.
  prefs: []
  type: TYPE_NORMAL
- en: A command changes the state of the system by ultimately producing events. It
    is not allowed to return any data. Either the command succeeds, which results
    in zero or more events, or it fails with an error. The events are produced reliably.
  prefs: []
  type: TYPE_NORMAL
- en: A query retrieves and returns data, without side effects on the system. It is
    not allowed to modify state.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example in Java code, a command acts like a `void doSomething()`
    method, which changes state. A query acts like a getter `String getSomething()`,
    which has no impact on the system's state. These principles sound simple, but
    have some implications on the system's architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The responsibilities of the commands and queries are separated into several
    concerns, allowing CQRS applications to emerge in fully independent applications
    that either write or read. Now, how to design and implement this approach?
  prefs: []
  type: TYPE_NORMAL
- en: Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following event-driven architectures, the write and read systems communicate
    solely via events. The events are distributed via an event store or event hub.
    There is no other coupling than the write systems that produce events and both
    write and read systems that consume for events to update their internal state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows the architecture of a CQRS system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32876ade-c2d7-4a57-b69f-40b88a6f40be.png)'
  prefs: []
  type: TYPE_IMG
- en: The command and query services consume events from the event store. This is
    the only way for communication between them.
  prefs: []
  type: TYPE_NORMAL
- en: All services maintain a current-state representation that reflects the state
    of the domain entities. Entities are, for example, *meal orders* or *cars*, including
    the latest state of their properties. This state is kept in memory or persisted
    in databases.
  prefs: []
  type: TYPE_NORMAL
- en: These representations just enable the systems to contain a current state. The
    golden source of truth is the atomic events contained in the event store.
  prefs: []
  type: TYPE_NORMAL
- en: All application instances individually update their state representations by
    consuming and applying the events from the event store.
  prefs: []
  type: TYPE_NORMAL
- en: The command services contain the business logic that initiates changes to the
    systems. They produce events via the event store after potential command verification
    using their state representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make the flow of information clear, let''s go through an example
    meal order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb0fbe4e-8d2c-4763-972a-bba3f416d84d.png)'
  prefs: []
  type: TYPE_IMG
- en: The client orders the meal at a command service instance. After a potential
    verification against its representation, the command service produces the `OrderPlaced`
    event to the event store. If publishing the event was successful, the `orderMeal()`
    method returns. The client can proceed with its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command service can create a meal identifier for later retrieval, for example,
    as a universally unique identifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93abf0cf-c901-4837-a8da-fd5b74122dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: The event store publishes the event to all consumers, which updates their internal
    representation accordingly. The client can access the status of the meal at the
    query service using its identifier. The query service will respond with its latest
    representation of the order.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to proceed with the order processing, an authority that invokes potential
    subsequent commands will handle the event as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba948444-0114-4543-8feb-8236d7179c48.png)'
  prefs: []
  type: TYPE_IMG
- en: An event handler will listen to the `OrderPlaced` event and invoke the `prepareMeal()`
    use case of the chef system. This subsequent command will then potentially result
    in new events.
  prefs: []
  type: TYPE_NORMAL
- en: The section *Implementing microservices with Java EE*, covers how to implement
    CQRS among other things.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CQRS enables distributed applications to not only scale horizontally, but independently
    in their write and read concerns. The replicas of query service, for example,
    can be different from the number of command services.
  prefs: []
  type: TYPE_NORMAL
- en: The read and write load in enterprise applications is usually not evenly distributed.
    Typically the read operations highly outperform the number of writes. For these
    cases the number of read instances can be scaled out independently. This would
    not be possible in a CRUD-based system.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit is that each service can optimize their state representations
    accordingly. For example, persistently storing the domain entities in a relational
    database might not be the best approach for every situation. It's also possible
    to just store the representation in memory and to recalculate all events at application
    startup. The point is that both the write and read instances are free to choose
    and optimize their representations according to the circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: A side effect of this approach is also that CQRS provides read-side failover
    availability. In case of the event store being unavailable no new events can be
    published, therefore no use cases that modify state can be invoked on the system.
    In CRUD-based systems this would correspond to the database being down. In CQRS
    systems, however, at least the query services can still provide the latest state
    from their representations.
  prefs: []
  type: TYPE_NORMAL
- en: The state representations of CQRS systems also solve the scalability issue of
    event-sourced systems. Event-sourced systems calculate the current application
    state from the atomic events. Executing this each and every time an operation
    is invoked will over time become slower and slower as more events arrive. The
    representations of the command and query services eliminate this need by continuously
    applying the recent events.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building CQRS systems not only has benefits, it also has shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Probably one of the biggest shortcoming of constructing these systems is that
    the majority of developers are not familiar with the concept, design and implementations.
    This will introduce difficulties when this approach is chosen in enterprise projects.
    Unlike CRUD-based systems, CQRS would require additional training and know-how.
  prefs: []
  type: TYPE_NORMAL
- en: Like any distributed system, there are naturally more applications involved
    in a CQRS system compared to the CRUD approach. As previously described for distributed
    systems in general, this requires some extra effort. Additionally, an event store
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the figures demonstrated, it is not mandatory to have the command and
    query sides in two or more independent applications. As long as the functionalities
    only communicate via events published by the event store, both can reside within
    the same application. This would, for example, result in a single waiter and chef
    application that still scales out horizontally. This is a reasonable trade-off,
    if individually scaling the write and read sides is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building CQRS systems is one approach to realizing asynchronous, eventually
    consistent communication. As we have seen previously in this book, there are many
    forms of communication, synchronous as well as asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: In order to enable scalable applications, distributed systems should not rely
    on synchronous communication that involves several systems. This leads to distributed
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to realize scalability with technology-agnostic, synchronous communication
    protocols is to model logically asynchronous processes. For example, communication
    protocols such as HTTP can be used to trigger processing that happens asynchronously
    while the caller immediately returns. This introduces eventual consistency, but
    enables the system to scale.
  prefs: []
  type: TYPE_NORMAL
- en: This also involves the consideration of whether the applications that made the
    distributed system make a difference in system-internal, and external communication.
    CQRS uses this approach by offering external interfaces, for example, using HTTP,
    to the clients, whereas the services themselves communicate via the event store.
    Modeling asynchronous processes that are accessed via uniform protocol doesn't
    distinguish here.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it's advisable to prefer availability, that is, scalability, over
    consistency when designing distributed systems. There are many approaches possible,
    CQRS is one of them, combining asynchronous communication with event sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: The following section covers the necessity of self-sufficient applications.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw the motivations, challenges, and benefits of distributed systems, as
    well as some approaches to handle communication and consistency. Now we will focus
    on the architecture of distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data and technology in enterprises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common idea in enterprises is to share and reuse technology as well as commonly
    used data. Earlier we looked at sharing Java modules and the shortcomings with
    that. What about sharing common technology or data models in distributed systems?
  prefs: []
  type: TYPE_NORMAL
- en: Multiple applications that form an enterprise system are often implemented using
    similar technology. This comes naturally with applications that are built by a
    single team or teams that work closely together. Doing so very often raises the
    idea of sharing technology that is being reused in the applications.
  prefs: []
  type: TYPE_NORMAL
- en: Projects could use commonly used modules that remove duplication in the overall
    system. A typical approach for this is shared models. There could be only one
    module within the organization that is being reused in all projects.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing models leads to the question whether potentially persisted domain entities
    or transfer objects are being reused. Domain entities that are persisted in a
    database could then even be directly retrieved from the database system, right?
  prefs: []
  type: TYPE_NORMAL
- en: Commonly used databases stand in total contradiction to distributed systems.
    They tightly couple the involved applications. Changes in schemas or technology
    welds the application and project life cycles together. Commonly used database
    instances prevent applications from being able to scale. This eliminates the motivations
    behind distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true for sharing technology in general. As shown in previous chapters,
    commonly used modules and dependencies introduce technical constraints in the
    implementations. They couple the applications and limit their independence in
    changes and life cycles. Teams will have to communicate and discuss modifications,
    even if they would not affect the application's boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the domain knowledge and the responsibilities in the context map
    of the system, sharing data and technology makes little sense. There are indeed
    points of contact between the systems that are subject to be shared in technology.
  prefs: []
  type: TYPE_NORMAL
- en: However, the point is to implement applications, which only depend on their
    business responsibilities on the one side and documented communication protocols
    on the other side. It's therefore advisable to choose potential duplication and
    independence rather than coupling in technology.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing other concerns rather than points of contact in the system's context
    map should alert engineers. The application's different responsibilities should
    make it clear that commonly used models or data reside in different contexts.
    The individual applications are exclusively responsible for their concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Shared-nothing architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With these thoughts in mind it's advisable to craft applications that share
    no common technology or data. They fulfill the application boundary contract in
    communication and business responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared-nothing architectures** are independent in technology, potentially
    used libraries, their data and schemas thereof. They are free to choose implementations
    and potential persistence technology.'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the implementation of an application within a distributed system from
    Python to Java should have no impact on the other applications, if the contract
    of its HTTP interface is still met.
  prefs: []
  type: TYPE_NORMAL
- en: If data is required within other applications, this needs to be defined explicitly
    in the context map, requiring the application to expose data via its business
    logic interfaces. Databases are not shared.
  prefs: []
  type: TYPE_NORMAL
- en: Shared-nothing architectures enable applications with independent life cycles
    that depend on nothing more than the explicitly defined contracts. Teams are free
    to choose technology and the project life cycles. The technology, as well as the
    data including databases, is owned by the application.
  prefs: []
  type: TYPE_NORMAL
- en: Interdependent systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shared-nothing architectures eventually have to collaborate with other applications.
    The defined contracts have to be met, documented, and communicated.
  prefs: []
  type: TYPE_NORMAL
- en: This is the point, that shared-nothing architectures are only dependent on the
    defined contracts and responsibilities. In case of changes in the business logic
    the contracts are redefined and communicated. Solely the application's team is
    responsible for how to implement the contracts.
  prefs: []
  type: TYPE_NORMAL
- en: Interdependent systems are made up of several shared-nothing applications with
    well-defined interfaces. The used interfaces should be technology-agnostic to
    not set constraints on the used implementation.
  prefs: []
  type: TYPE_NORMAL
- en: This is the idea behind microservice architectures. Microservices consist of
    several interdependent applications that realize their individual business responsibilities
    and, combined together, solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: The name microservice doesn't necessarily say anything about the size of the
    application. An application should be built by a single team of developers. For
    organizational reasons team sizes should not grow too big. There is an often-cited
    notion by Amazon that the whole team should be able to survive on two pizzas.
  prefs: []
  type: TYPE_NORMAL
- en: The motivations behind distributed systems should be considered before crafting
    microservices. If there is no actual need to distribute a system, it should be
    avoided. Sticking to monolithic applications with reasonable responsibilities
    is to be preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Usually the approach to craft microservice architectures is to slice up monolithic
    applications that grow too large in responsibilities, or diverged in teams and
    life cycles. This is comparable with refactoring approaches. Refactoring a class
    that grew too big into multiple delegates works well more often than trying to
    introduce a perfect scenario from the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it's always advisable to consider the business requirements, context
    map of the system with their development teams and life cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 12-factor and cloud native applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 5](a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml), *Container and Cloud
    Environments with Java EE*, introduced the approaches of 12-factor and cloud native
    applications. They heavily support microservice architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the shared-nothing approach of having interdependent, distributed
    applications is well realizable with the principles of containerized, stateless,
    and scalable enterprise applications.
  prefs: []
  type: TYPE_NORMAL
- en: The 12-factor principles and the effective nature of cloud and container environments
    support teams in developing microservices with manageable overhead and high productivity.
  prefs: []
  type: TYPE_NORMAL
- en: However, an enterprise system doesn't not have to be distributed in order to
    comply with the 12-factor or cloud native principles. The approaches are certainly
    advisable for building monolithic applications as well.
  prefs: []
  type: TYPE_NORMAL
- en: When to use and when not to use microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recent years microservice architectures have seen some hype in the software
    industry.
  prefs: []
  type: TYPE_NORMAL
- en: As always with hypes, engineers should ask themselves what is behind certain
    buzzwords and whether implementing them makes sense. It's always advisable to
    look into new technology and methodologies. It's not necessarily advisable to
    apply them immediately.
  prefs: []
  type: TYPE_NORMAL
- en: The reasons for using microservices are the same as for using distributed systems
    in general. There are technical reasons, such as applications that need independent
    deployment life cycles.
  prefs: []
  type: TYPE_NORMAL
- en: There are also reasons that are driven by the business requirements and situations
    in teams and project working modes.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is an often-cited motivation behind microservice architectures.
    As we have seen in event-driven architectures, monolithic applications aren't
    able so scale infinitely. The question is whether scalability is effectively an
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: There are big companies that handle business logic for huge amounts of users
    using monolithic applications. Before considering distribution as a relief for
    scaling issues, performance insights and statistics should be gathered.
  prefs: []
  type: TYPE_NORMAL
- en: Engineers should avoid to use microservice architectures solely because of believing
    in a *silver bullet* approach. It can easily happen as a result of *buzzword-driven*
    meetings and conversations, that solutions are chosen based on limited or no evidence
    supporting the requirement. Microservices certainly provide benefits, but also
    come with a price in time and effort. In any way, the requirements and motivations
    whether to split up responsibilities into multiple applications should be clear.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing microservices with Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now on to the question of how to build microservices with Enterprise Java.
  prefs: []
  type: TYPE_NORMAL
- en: In various discussions and meetings, Java EE has been considered as *too heavyweight*
    and cumbersome for microservices. Whereas this is certainly the case for J2EE
    technology and approaches, Java EE offers modern, lean ways of developing enterprise
    applications. [Chapter 4](f0a49441-e411-49c4-a4b6-c6193ba36094.xhtml), *Lightweight
    Java EE* and [Chapter 5](a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml), *Container
    and Cloud Environments with Java EE* covered these aspects, especially in regard
    to modern environments.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE is indeed well suited for writing microservice applications. Container
    technologies and orchestration support the platform, particularly since Java EE
    separates the business logic from the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-dependency applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices with Java EE are ideally built as zero-dependency applications.
  prefs: []
  type: TYPE_NORMAL
- en: Thin WAR applications are deployed on modern enterprise containers that can
    be shipped in containers. This minimizes deployment time. Java EE deployment artifacts
    should only contain provided dependencies, if there is a reasonable need for adding
    third-party dependencies, they should be installed in the application server.
    Container technologies simplify this approach.
  prefs: []
  type: TYPE_NORMAL
- en: This also matches the idea of shared-nothing architectures. The team is responsible
    for the application-specific technology, in this case the application server installation
    including libraries. Infrastructure as code definitions such as Dockerfiles, enable
    the development team to accomplish this in effective ways.
  prefs: []
  type: TYPE_NORMAL
- en: Application servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following this approach, the application server is shipped in a container, containing
    only a single application. The *one application per application server* approach
    also matches the idea of shared-nothing architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The question is whether application servers introduce too much overhead if a
    single server instance only contains a single application. In the past, the storage
    and memory footprint certainly was significant.
  prefs: []
  type: TYPE_NORMAL
- en: Modern application servers considerably improved in this area. There are container
    base images of servers such as **TomEE** that consume 150 MB and less, for the
    server including the Java runtime and operating system, mind you. The memory consumption
    also significantly improved due to dynamically loading functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In enterprise projects installation sizes are usually not an issue to be concerned
    with, especially if they're not exceeding all bounds. What's much more important
    is the size of the built and shipped artifacts. The application artifact, which
    in some technologies contains megabytes of dependencies, is built and transmitted
    many times. The runtime is only installed once per environment.
  prefs: []
  type: TYPE_NORMAL
- en: Container technologies such as Docker make use of layered file systems that
    encourage the moving parts to be small. Zero-dependency applications support this
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Making each and every Continuous Delivery build only shipping kilobytes of data
    is far more advisable than saving a few megabytes in the base installation.
  prefs: []
  type: TYPE_NORMAL
- en: If the installation size still needs to be shrunk down, some application vendors
    offer possibilities to tailor the container to the required standards, especially
    the MicroProfile initiative, which includes several application server vendors,
    and defines slimmed profiles.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE microservices don't need to be shipped as standalone JAR files. On the
    contrast, applications shipped in containers should leverage the use of layered
    file systems and be deployed on enterprise containers residing in the base image.
    Standalone JAR files oppose this principle.
  prefs: []
  type: TYPE_NORMAL
- en: There are possibilities to combine standalone JAR files with thin deployments,
    by so-called hollow JAR. This approach, however, is not required when using containers.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing application boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's move on to the implementation of the application boundaries with Java
    EE. This is, in fact, a more system-architectural question than an implementational
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Communication between microservices should use technology-agnostic protocols.
    As seen previously, Java EE heavily supports HTTP, for both HTTP and REST services
    use Hypermedia.
  prefs: []
  type: TYPE_NORMAL
- en: The next sub-chapter will cover asynchronous communication in CQRS systems,
    using publish/subscribe messaging implemented with Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CQRS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter we have seen the motivations and concepts behind event
    sourcing, event-driven architectures, and CQRS. CQRS offers an interesting approach
    to creating distributed applications that implement scalable, eventually consistent
    business use cases.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, there is a lot of interest in CQRS, yet little knowledge
    within companies of how to use it. Some frameworks and technologies have emerged
    that aim to implement this approach. Yet CQRS is an architectural style, and specific
    frameworks are not necessary to develop CQRS systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a close look at an approach that uses Java EE.
  prefs: []
  type: TYPE_NORMAL
- en: System interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CQRS system interfaces are used from outside the system to initiate business
    use cases. For example, a client accesses the waiter system to order a burger.
  prefs: []
  type: TYPE_NORMAL
- en: These interfaces are used externally and ideally implemented using a technology-agnostic
    protocol.
  prefs: []
  type: TYPE_NORMAL
- en: For REST-like HTTP services, this implies that the command services implement
    HTTP methods that modify resources, such as POST, DELETE, PATCH, or PUT. The query
    services usually only implement resources queried by GET.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, this means that the client POSTs a new meal order to a command
    service resource. Similarly, meal orders are retrieved via GET resources from
    query services.
  prefs: []
  type: TYPE_NORMAL
- en: These HTTP interfaces concern the external communication. Internally the application
    communicates via events that are published using an event hub.
  prefs: []
  type: TYPE_NORMAL
- en: Example scenario using Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, I will use Apache Kafka as a distributed message broker, offering
    high performance and throughput. It's one example of a messaging technology, supporting
    a publish/subscribe approach, among others.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Apache Kafka doesn't implement all JMS semantics. The
    following examples will use the Kafka's vendor-specific Client API.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka's publish/subscribe approach organizes messages in topics. It can
    be configured to enable transactional event producers and in-order event consumption,
    which is what event-driven architectures need to ensure in order to create reliable
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka brokers are distributed and use so-called consumer groups to manage message
    topics and partitions. Examining Kafka's architecture is beyond the scope of this
    book and it's advised to go further into its documentation when choosing this
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: In short, a message is published to a topic and consumed once per consumer group.
    Consumer groups contain one or more consumers and guarantee that exactly one consumer
    will process the messages that have been published using transactional producers.
  prefs: []
  type: TYPE_NORMAL
- en: A CQRS system needs to consume messages in multiple locations. The applications
    that are interested in a specific topic will consume the message and update their
    internal representations. Therefore, all these updating consumers will receive
    an event. There are also event handlers who use the event to process the business
    logic further. Exactly one event handler needs to process the event per topic,
    otherwise processes would run multiple times or not at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of Kafka consumer groups is therefore used in such a way, where
    there is one update consumer group per application and one event handler group
    per topic. This enables all instances to receive the events, but reliably one
    command service to process the business logic. By doing so, the instances are
    able to scale without affecting the overall system''s outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8c5b4a8-2c08-49e1-b66b-52bc48f16cb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Integrating Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to integrate the Apache Kafka cluster into the application this example
    will use Kafka's Java API.
  prefs: []
  type: TYPE_NORMAL
- en: The applications connect to Kafka to consume messages in their updating consumers
    and event handlers. The same is true for publishing events.
  prefs: []
  type: TYPE_NORMAL
- en: 'The used technology should be encapsulated from the rest of the application.
    In order to integrate the events, developers can use a functionality that naturally
    fits this scenario: CDI events.'
  prefs: []
  type: TYPE_NORMAL
- en: CDI events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The domain events contain the event specific data, a timestamp, and identifiers
    that reference the domain entity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows an example of an abstract `MealEvent` and the `OrderPlaced`
    event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Domain events like these are the core of the application. The domain entity
    representations are calculated from these events.
  prefs: []
  type: TYPE_NORMAL
- en: The integration into Kafka ensures that these events are fired via CDI. They
    are observed in the corresponding functionality that updates the state representations,
    or invokes subsequent commands, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Event handlers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following snippet shows an event handler of the chef system, invoking functionality
    of a command service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The event handler consumes the event and will invoke the boundary of the subsequent
    meal preparation use case. The `prepareMeal()` method itself will result in zero
    or more events, in this case either `MealPreparationStarted` or `OrderFailedInsufficientIngredients`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The event producer will reliably publish the events to the Kafka cluster. If
    the publication fails, the whole event processing has to fail, and will be retried
    later.
  prefs: []
  type: TYPE_NORMAL
- en: State representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The consumers that update the state representation consume the CDI events as
    well. The following snippet shows the bean that contains the meal order state
    representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This simple example represents the state of the meal orders in a relational
    database. As soon as a new CDI event arrives, the state of the orders is updated.
    The current state can be retrieved by the `get()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The meal order domain entity is persisted via JPA. It contains the status of
    the order that is updated via observed CDI events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Consuming Kafka messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The part that consumes the messages encapsulates the message hub from the rest
    of the application. It is integrated by firing CDI events on arriving messages.
    This certainly is specific to the Kafka API and should be considered as an example
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The updating consumer connects to a specific topic via its consumer group.
    The startup singleton bean ensures the consumer will be initiated at application
    startup. A container-managed executor service runs the event consumer in its own
    thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The application-specific Kafka properties are exposed via a CDI producer. They
    contain the corresponding consumer groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The event consumer performs the actual consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Kafka records that are consumed result in new CDI events. The configured properties
    use JSON serializers and deserializers, respectively, to map the domain event
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Events that are fired via CDI and consumed successfully are committed to Kafka.
    The CDI events are fired synchronously, to ensure that all processes are finish
    reliably before committing.
  prefs: []
  type: TYPE_NORMAL
- en: Producing Kafka messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The event producer publishes the domain events to the message hub. This happens
    synchronously to rely on the messages being in the system. Once the transmission
    is acknowledged, the `EventProducer#publish` method invocation returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Going into the details of the Kafka producer API is beyond the scope of this
    book. However, it needs to be ensured that the events are sent reliably. The event
    producer bean encapsulates this logic.
  prefs: []
  type: TYPE_NORMAL
- en: These examples demonstrate one possibility for integrating Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the **Java EE Connector Architecture** (**JCA**) is another
    possibility for integrating external concerns into the application container.
    At the time of writing, there are vendor-specific container solutions that integrate
    messaging via JCA. Existing solutions for integrating message hubs such as Kafka
    are an interesting alternative. However, application developers are advised to
    encapsulate technology specifics into single points of responsibilities and use
    standard Java EE functionality within the application.
  prefs: []
  type: TYPE_NORMAL
- en: Application boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The applications of a CQRS system communicate via events internally. Externally,
    other protocols such as HTTP can be provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The query and command functionality of, for example, the waiter system, is
    exposed via JAX-RS. The command service offers functionality to place meal orders.
    It uses the event producer to publish the resulting events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `orderMeal()` method is called by the HTTP endpoint. The other methods are
    called by the waiter system's event handler. They will result in new events that
    are delivered by the event hub.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for not directly firing events or calling functionality internally
    here is that this application resides in a distributed environment. There might
    be other instances of the waiter system consuming the event hub and updating their
    representation accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command service contains a JAX-RS resource that is used to order meals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The query service exposes the meal order representations. It loads the current
    state of the domain entities from the database as seen in the `MealOrders`. The
    JAX-RS resources of the query service use this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the waiter system is shipped as a single instance, containing both the command
    and query services, these resources can be combined. It needs to be ensured though
    that the services don''t cross-communicate, except via the eventing mechanism.
    The following code snippet shows the query service endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These examples are not exhaustive, but are meant to give the reader an idea
    of integrating CQRS concepts and message hubs into Java EE applications.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating further CQRS concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the benefits of event-sourced systems is that it's possible to take the
    full set of atomic events and replay them, for example, in test scenarios. System
    tests verify against the actual use cases that happened in production. Audit logging
    comes for free as well, being part of the core of the application.
  prefs: []
  type: TYPE_NORMAL
- en: This approach also enables us to change business functionality and replay some
    events, either to fix bugs and to correct behavior, or to apply the event information
    to new functionality. This makes it possible to apply new features on events as
    if they were part of the application since day one.
  prefs: []
  type: TYPE_NORMAL
- en: If the chef system adds functionality to continuously calculate the average
    time of meal preparation, the events can be redelivered to re-calculate the representations.
    Therefore the database contents will be reset and the events redelivered, only
    to the updating consumer, which results in new representation being calculated
    and persisted. Kafka can explicitly redeliver events.
  prefs: []
  type: TYPE_NORMAL
- en: The events, however, are solely used to update the status representations, not
    triggering new commands during replays. Otherwise, the system would end up in
    an inconsistent state. The demonstrated example realizes this by defining a dedicated
    Kafka consumer group for event handlers, which is not reset to redistribute events
    to the event handlers. Only the updating consumers re-consume the events, to recalculate
    the internal state representations.
  prefs: []
  type: TYPE_NORMAL
- en: The point is, that CQRS systems enable many more use cases, due to event sourcing
    being used. The possibilities of capturing and replaying events, as well as the
    contained context and history information, enable extensive scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE in the age of distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservice architectures and distributed systems naturally require communication
    that involves more than a single, monolithic application. There are many ways
    how to implement communication with Java EE, depending on the chosen protocols
    and communication technologies.
  prefs: []
  type: TYPE_NORMAL
- en: There are some aspects to be considered when realizing communication. External
    applications that take part of the microservice system, for example, require discovering
    the service instances. In order to not tightly couple applications and configuration,
    looking up services should be dynamic, rather than configuring static hosts or
    IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: The cloud native principle of being resilient also concerns communication. Since
    networks can potentially fail anytime, application health should not be impacted
    when connectivity decelerates or goes down. The application should guard itself
    from potential errors propagating into the application.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Service discovery can happen in various ways, from DNS lookup to more sophisticated
    scenarios where the lookup is part of business logic, providing different endpoints
    depending on the situation. It generally encapsulates addressing external systems
    from the application's concerns. Ideally, the application logic only names the
    logical service it needs to communicate with, and the actual lookup is performed
    externally.
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the used environments and runtime which possibilities enterprise
    developers have. Container technologies offer functionality to link services by
    names, taking away work and responsibility from the application. The clients connect
    against the link or service names as hostnames, which are resolved by the container
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: This approach works both for Docker containers and container orchestration such
    as Docker Compose, Kubernetes, or OpenShift. All communication concerns solely
    use logical service names and ports to establish connections. This matches the
    12-factor principles as well.
  prefs: []
  type: TYPE_NORMAL
- en: Since the lookup work is performed in the environment, the applications will
    only specify the desired service names. This is true for all outward communication,
    such as HTTP connections, databases, or message hubs. [Chapter 5](a24f88d4-1af2-4746-91c9-9717d077dd27.xhtml),
    *Container and Cloud Environments with Java EE* demonstrated examples for this.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating resiliently
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network communication is not reliable and can potentially break in all sorts
    of ways. Connections may timeout, services may be unavailable, respond slowly,
    or deliver unexpected answers.
  prefs: []
  type: TYPE_NORMAL
- en: In order to not let errors propagate into the application, the communications
    need to be resilient.
  prefs: []
  type: TYPE_NORMAL
- en: Validating responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, this implies client-side validation and handling errors. Irrelevant
    to the communication technology in use, applications can't rely on external systems
    to provide responses that are not malformed or not simply wrong.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn't mean that clients immediately have to reject all responses that
    are not perfect in the application's understanding. Responses that contain more
    information or slightly different formats than expected, but are still understandable,
    should not lead to immediate errors. Following the principle to *be conservative
    in what you do and liberal in what you accept*, messages that contain just enough
    for the application to do its job should be accepted. For example, additional,
    unknown properties in JSON responses should not lead to refusing to map the object.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking timeouts and circuits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clients that perform synchronous calls to external systems block the subsequent
    execution until the external system responds. Invocations may fail, slow down
    the execution, or in the worst case effectively bring the whole application down.
    It's crucial to keep this fact in mind when implementing clients.
  prefs: []
  type: TYPE_NORMAL
- en: First of all client connections should always set reasonable timeouts, as shown
    similarly in Chapter 3, *Implementing Modern Java Enterprise Applications*. Timeouts
    prevent the application from deadlock situations.
  prefs: []
  type: TYPE_NORMAL
- en: As seen before, Java EE interceptors can be used to prevent potential runtime
    exceptions from propagating into the business logic.
  prefs: []
  type: TYPE_NORMAL
- en: So-called **circuit breakers** take this approach of preventing cascading failure
    further. They secure client invocations by defining error or timeout thresholds
    and prevent further invocations in case of failure. The circuit breaker approach
    comes from the model of electrical engineering, circuit breakers built into buildings,
    that intercept the connection by opening their circuits to prevent further damage.
  prefs: []
  type: TYPE_NORMAL
- en: A client circuit breaker similarly opens its circuit, that is, preventing further
    invocations, to not *damage* the application or the external system. Circuit breakers
    usually allow errors and timeouts to happen up to a certain degree and then cutting
    the connections for a certain time, or until the circuit is manually closed again.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE applications can implement circuit breakers via interceptors. They can
    add sophisticated logic on when and how to open and close their circuits, for
    example, measuring the number of failures and timeouts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following demonstrates one possible circuit breaker approach in pseudo
    code. The interceptor behavior is annotated to a client method, similarly to client
    interceptor examples demonstrated earlier in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the circuit breaker could measure the service time and open its circuit
    if the service becomes too slow, additionally to HTTP client timeouts.
  prefs: []
  type: TYPE_NORMAL
- en: There are some open source Java EE libraries available for this purpose, for
    example **Breakr** by Java EE expert Adam Bien. It depends on the technical requirements
    and the complexity of the logic, when to open and close the circuit, and whether
    third-party dependencies are required.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build zero-dependency applications, potential libraries should be
    installed into the container and not shipped with the application artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Bulkheads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ships contain bulkheads that divide the vessel into several areas. If the ship
    hull gets a leak in some locations, only a single area is filled with water and
    the whole ship is likely still able to float.
  prefs: []
  type: TYPE_NORMAL
- en: The **bulkhead** pattern takes this idea to enterprise applications. If some
    component of the application fails or is working to capacity due to workload,
    then the rest of the application should still be able to fulfill its purpose.
    This, of course, highly depends on the business use case.
  prefs: []
  type: TYPE_NORMAL
- en: One example is to separate the thread execution of business processes from HTTP
    endpoints. Application servers manage a single pool of request threads. If, for
    example, a single business component fails and blocks all incoming requests, all
    available request threads will eventually be occupied. The result is in no other
    business uses cases is being able to be invoked, due to unavailable request threads.
    This could be the case if used clients don't implement proper timeouts, connect
    against a system that is down, and block the execution.
  prefs: []
  type: TYPE_NORMAL
- en: Using asynchronous JAX-RS resources together with dedicated managed executor
    services can relieve this issue. As seen earlier in this book, JAX-RS resources
    can invoke the business functionality in separate, container-managed threads to
    prevent the overall execution utilizing a request thread. Multiple components
    can use independent thread pools, which prevent failures from spreading.
  prefs: []
  type: TYPE_NORMAL
- en: Since the application server is responsible for managing threads, this approach
    should be implemented following Java EE standards. The idea is to define dedicated
    executor services that are injectable at the required positions.
  prefs: []
  type: TYPE_NORMAL
- en: The open source library **Porcupine** by Adam Bien uses this approach to create
    dedicated executor services that use `ManagedThreadFactory` to define thread pools
    with container-managed threads. The dedicated executor services can be configured
    and instrumented appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows one example of the bulkheads pattern, combining
    asynchronous JAX-RS resources with dedicated executor services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The business use case is executed in a managed thread provided by the executor
    service, in order to allow the request thread to return and to handle other requests.
    This enables other functionality of the application to still function, even if
    this part is overloaded, and utilizes all threads of the `custom-name` executer.
  prefs: []
  type: TYPE_NORMAL
- en: The following examines how the custom executor service is configured.
  prefs: []
  type: TYPE_NORMAL
- en: Shaking hands and pushing back
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach of communicating in a resilient way is **handshaking** and
    **backpressure**. The idea is that the communication partner being under load
    notifies the other side, which then backs off and eases the load. Handshaking
    here means that the calling side has a way of asking the service whether it can
    handle more requests. Backpressure reduces the load on a system by notifying clients
    when the limit is reached or pushing back requests.
  prefs: []
  type: TYPE_NORMAL
- en: The two approaches combined form a resilient and effective form of communication.
  prefs: []
  type: TYPE_NORMAL
- en: Information about the current load state of the application can be provided
    in HTTP resources or via header fields. The clients then take this information
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: A more direct way is to simply reject a client request when the server's resources
    are fully utilized. Developers are advised to pay attention to the behavior of
    pooling such as in executor services, and how they handle situations with full
    queues. Exceptionally, it's advisable to abort the client request to not unnecessarily
    run into timeouts.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows a scenario using the Porcupine library. A business
    functionality is executed using a dedicated executor service, which will be configured
    to abort rejected executions. The clients will immediately receive a `503 Service
    Unavailable` response, indicating that currently the service is not able to serve
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JAX-RS resource is similar to the previous example. The `custom-name` executor
    is configured to abort rejected executions via a specialized configurator. The
    `ExecutorConfigurator` is part of the Porcupine library. The following shows the
    custom configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Executions that are rejected due to full queues will then result in a `RejectedExecutionException`.
    This exception is mapped via JAX-RS functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Client requests that would exceed the server limits immediately result in an
    error response. The client invocation can take this into account and act appropriately.
    For example, a circuit breaker pattern-like functionality can prevent the client
    from immediate subsequent invocations.
  prefs: []
  type: TYPE_NORMAL
- en: Backpressure is helpful when crafting scenarios with multiple services that
    need to meet **service level agreements** (**SLA**). Chapter 9, *Monitoring, Performance,
    and Logging* will cover this topic.
  prefs: []
  type: TYPE_NORMAL
- en: More on being resilient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides resilience in communication, microservices also aim to improve service
    quality and availability. Applications should be able to scale and self-heal in
    cases of failures.
  prefs: []
  type: TYPE_NORMAL
- en: The use of container orchestration technology such as Kubernetes supports this
    approach. Pods that back logical services can be scaled up to handle more workload.
    The services balance the load between the containers. There are possibilities
    to auto-scale instances up or down based on the current workload on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes aims to maximize service uptime. It manages liveness and readiness
    probes to detect failures and potentially start new containers. In case of errors
    during deployments, it will leave currently running services untouched, until
    the new versions are able to serve traffic.
  prefs: []
  type: TYPE_NORMAL
- en: These approaches are managed by the runtime environment, not part of the application.
    It's advisable to minimize the non-functional, cross-cutting concerns within the
    enterprise application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There a multiple motivations behind distributing systems. Despite certain introduced
    challenges and overheads in communication, performance, and organization, distribution
    is often necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to design the system landscape, the system''s context map that represents
    the individual responsibilities needs to be taken into consideration. It''s advisable
    to design application APIs in clear, lean interfaces, ideally implemented with
    standard communication protocols. Before introducing breaking changes, engineers
    as well as business experts need to ask themselves whether it is necessary to
    force client functionality to stop working. In the same way, APIs should be designed
    in a resilient way, preventing unnecessary breaks, in other words: *be conservative
    in what you do and liberal in what you accept*.'
  prefs: []
  type: TYPE_NORMAL
- en: Engineers that build distributed applications need to be aware of the trade-off
    between consistency and scalability. The majority of applications that use synchronous
    communication involving an external system will likely scale well enough. Distributed
    transactions should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: In order to communicate asynchronously, application can be based on event-driven
    architectures. The CQRS principle combines the motivations behind event-driven
    architectures and event sourcing. Whereas CQRS certainly offers interesting solutions,
    it only makes sense if there is a need for distributing application.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures don't share common technology or data with each other.
    Shared-nothing architectures are free to choose implementations and persistence
    technology. Zero-dependency Java EE applications shipped in containers are a reasonable
    fit for microservices. The *one application per application server* approach matches
    the idea of shared-nothing architectures. There are many aspects in which Java
    EE applications running in container orchestration frameworks support developing
    microservice architectures, such as service discovery, resilient communication
    via timeout, circuit breakers or bulkheads.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapter covers the topics of performance, monitoring and logging.
  prefs: []
  type: TYPE_NORMAL
