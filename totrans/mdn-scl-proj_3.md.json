["```java\nscala> val dataSetPath = \"\\\\<<Path to the folder containing the Data File>>\"\nscala> val dataFile = dataSetPath + \"\\\\News.csv\"\n```", "```java\nscala> val result1 = spark.sparkContext.textFile(dataSetPath + \"News.csv\")\nresult1: org.apache.spark.rdd.RDD[String] = C:\\<<Path to your own Data File>>\\News.csv MapPartitionsRDD[1] at textFile at <console>:25\n```", "```java\nscala> val result2 = result1.flatMap{ partition => partition.split(\"\\n\").toList }\nresult2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:27\n```", "```java\nscala> val result2A = result2.map(_.split(\",\"))\nresult2A: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:29\n```", "```java\nscala> val spDataFrame = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"News.csv\")\nspDataFrame: org.apache.spark.sql.DataFrame = [Date: string, Label: int ... 5 more fields]\n```", "```java\nscala> val newsColsOnly = spDataFrame.drop(\"Date\", \"Label\")\nnewsColsOnly: org.apache.spark.sql.DataFrame = [Top1: string, Top2: string ... 23 more fields]\n```", "```java\nscala> val expAnalysisTestFrame = spDataFrame.describe(\"TopHeadline1\", \"TopHeadline2\", \"TopHeadline3\",\"TopHeadline4\",\"TopHeadline5\", TopHead.....)\nexpAnalysisTestFrame: org.apache.spark.sql.DataFrame = [summary: string, TopHeadline: string ... 25 more fields]\n\nscala> newsColsOnly.show\n```", "```java\nscala> val mergedNewsColumnsFrame = newsColsOnly.withColumn(\"AllMergedNews\", newsColsOnly(\"TopHeadline1\"))\nmergedNewsColumnsFrame: org.apache.spark.sql.DataFrame = [TopHeadline1: string, TopHeadline2: string ... 23 more fields]\n```", "```java\nscala> import org.apache.spark.sql.functions\nimport org.apache.spark.sql.functions\n\nscala> val mergedFrameList = for (i <- 2L to newsColsOnly.count() + 1L) yield mergedNewsColumnsFrame.withColumn(\"AllMergedNews\", functions.concat(mergedNewsColumnsFrame(\"AllMergedNews\"), functions.lit(\" \"), mergedNewsColumnsFrame(\"Top\" + i.toString)))\nmergedFrameList: scala.collection.immutable.IndexedSeq[org.apache.spark.sql.DataFrame] = Vector([Top1: string, Top2: string ... 4 more fields], [Top1: string, Top2: string ... 4 more fields])\n```", "```java\nscala> val mergedFinalFrame = mergedFrameList(0)\nmergedFinalFrame: org.apache.spark.sql.DataFrame = [Top1: string, Top2: string ... 4 more fields]\n```", "```java\nimport org.apache.spark.ml.feature.StopWordsRemover\nimport org.apache.spark.ml.param.StringArrayParam\n\n```", "```java\nval stopwordEliminator = StopWordsRemover(new StringArrayParam(\"words\",\"..), new StringArrayParam(\"stopEliminated\", \"..)\n```", "```java\nval cleanedDataFrame = stopwordEliminator.transform(mergedFinalFrame)\n cleanedDataFrame.show()\n+-----------+-----+--------------------+--------------------+\n| Date|label| words| stopEliminated|\n+-----------+-----+--------------------+--------------------+\n| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|\n|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|\n+-----------+-----+--------------------+--------------------+\n```", "```java\n//Import the feature Transformer NGram\nimport org.apache.spark.ml.feature.NGram\n\n//Create an N-gram instance; create an N-Gram of size 2\nval aNGram = new NGram(new StringArrayParam(\"stopRemoved\"..), new StringArrayParam(\"ngrams\", n=2)\n\n// transform the cleanedDataFrame (the one devoid of stop words)\nval cleanedDataFrame2 = aNGram.transform(cleanedDataFrame)\n\n//display the first 20 rows\n cleanedDataFrame2.show()\n +-----------+-----+--------------------+--------------------+--------------------+\n | Date|label| words| stopEliminated| Ngrams|\n +-----------+-----+--------------------+--------------------+--------------------+\n | 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|\n |11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|\n +-----------+-----+--------------------+--------------------+--------------------+\n```", "```java\ncleanedDataFrame3 = cleanedDataFrame2.withColumn('ndashgrams', ....)\n\ncleanedDataFrame3.show()\n+-----------+-----+--------------------+--------------------+--------------------+\n | Date|label| words| stopEliminated| Ngrams|\n +-----------+-----+--------------------+--------------------+--------------------+\n | 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|\n |11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|\n +-----------+-----+--------------------+--------------------+--------------------+\n```", "```java\nimport org.apache.spark.ml.feature.CountVectorizer \n\n//We need a so-called count vectorizer to give us a CountVectorizerModel that will convert our 'corpus' //into a sparse vector of n-gram counts\n\n val countVectorizer = new CountVectorizer\n//Set Hyper-parameters that the CountVectorizer algorithm can take\ncountVectorizer.inputCol(new StringArrayParam(\"NGrams\")\ncountVectorizer.outputCol(new StringArrayParam(\"SparseVectorCounts\")\n//set a filter to ignore rare words\ncountVectorizer.minTF(new DoubleParam(1.0))\n```", "```java\ncleanedDataFrame3 = countVectorizer.fit(cleanedDataFrame2)\n\ncleanedDataFrame3.show()\n| Date|label| words| stopEliminated| NGrams| SparseVectorCounts|\n| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|\n|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|\n +-----------+-----+--------------------+--------------------+--------------------+--------------------\n```", "```java\nimport org.apache.spark.ml.feature.StringIndexer\n\nval indexedLabel = new StringIndexer(new StringArrayParam(\"label\"), new StringArrayParam(\"label2\"), ...)\n\ncleanedDataFrame4 = indexedLabel.fit(cleanedDataFrame3).transform(cleanedDataFrame3)\n```", "```java\nval cleanedDataFrame5 = cleanedDataFrame4.drop('label')\nDataFrame[Date: string, words: array<string>, stopRemoved: array<string>, ngrams: array<string>, countVect: vector, label2: double]\n\ncleanedDataFrame5.show()\n\n| Date|label| words| stopEliminated| NGrams| SparseVectorCount|label2|\n| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|\n```", "```java\n|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|\n +-----------+-----+--------------------+--------------------+--------------------+--------------------+------+\n```", "```java\nval  cleanedDataFrame6 = cleanedDataFrame5.withColumn('label', cleanedDataFrame.label2)\ncleanedDataFrame6.show()\n\n| Date|label| words| stopRemoved| ngrams| countVect|label2|\n| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|\n|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|\n +-----------+-----+--------------------+--------------------+--------------------+--------------------+------+\n```", "```java\n//Split the dataset in two. 85% of the dataset becomes the Training (data)set and 15% becomes the testing (data) set\nval finalDataSet1: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = cleanedDataFrame6.randomSplit(Array(0.85, 0.15), 98765L)\nprintln(\"Size of the new split dataset \" + finalDataSet.size)\n\n//the testDataSet\nval testDataSet = finalDataSet1(1)\n\n//the Training Dataset\nval trainDataSet = finalDataSet1(0)\n```", "```java\nval labelIndexer = new IndexToString().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(input)\n```", "```java\nval stringIndexer = new StringIndexer().setInputCol(\"prediction\").setOutputCol(\"predictionLabel\")\n```", "```java\nval randomForestClassifier = new RandomForestClassifier().setFeaturesCol(spFeaturesIndexedLabel._1)\n.setFeatureSubsetStrategy(\"sqrt\")\n```", "```java\nimport org.apache.spark.ml.classification.RandomForestClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.param._\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.{Pipeline, PipelineStage}\n\n```", "```java\nval soPipeline = new Pipeline().\nsetStages(Array[PipelineStage](labelIndexer) ++ Array[PipelineStage](randomForestClassifier)) ++ Array[PipelineStage](stringIndexer)]\n```", "```java\n//Lets set the hyper parameter NumTrees\nval rfNum_Trees = randomForestClassifier.setNumTrees(15)\nprintln(\"Hyper Parameter num_trees is: \" + rfNum_Trees.numTrees)\n```", "```java\n//set this default parameter in the classifier's embedded param map\nval rfMax_Depth = rfNum_Trees.setMaxDepth(2)\nprintln(\"Hyper Parameter max_depth is: \" + rfMax_Depth.maxDepth)\n```", "```java\nval stockPriceModel = pipeline.fit(trainingData)\n```", "```java\n// Generate predictions.\nval predictions = stockPriceModel.transform(testData)\n```", "```java\npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n```", "```java\nmodelOutputAccuracyEvaluator: Double = new MulticlassClassificationEvaluator()\n.setLabelCol(\"indexedLabel\")\n.setPredictionCol(\"prediction\")\n.setMetricName(\"precision\")\n\nval accuracy = modelOutputAccuracyEvaluator.evaluate(predictions)\n\n```"]