- en: Kafka Connect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, instead of using the Kafka Java API for producers and consumers,
    Kafka Streams, or KSQL as in previous chapters, we are going to connect Kafka
    with Spark Structured Streaming, the Apache Spark solution to process streams
    with its Datasets API.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading Kafka from Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing to Kafka from Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the `SparkProcessor`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Connect is an open source framework, part of Apache Kafka; it is used
    to connect Kafka with other systems, such as structured databases, column stores,
    key-value stores, filesystems, and search engines.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect has a wide range of built-in connectors. If we are reading from
    the external system, it is called a **data source**; if we are writing to the
    external system, it is called a **data sink**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous chapters, we created a Java Kafka Producer that sends JSON data
    to a topic called `healthchecks` in messages like these three:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are going to process this data to calculate the machine''s uptime and
    to obtain a topic with messages like these three:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Project setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to modify our Kioto project. We have to add the dependencies
    to `build.gradle`, as shown in *Listing 8.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 8.1: Kioto gradle build file for Spark'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Apache Spark, we need the dependency, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To connect Apache Spark with Kafka, we need the dependency, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using an old Spark version, 2.2.2, for the following two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: At the moment you are reading this, surely the Spark version will be superior.
    The reason why I chose this version (and not the last one at the time of writing)
    is because the connector with Kafka works perfectly with this version (in performance
    and with regard to bugs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka connector that works with this version is several versions behind
    the most modern version of the Kafka connector. You always have to consider this
    when upgrading production environments**.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming processor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/spark` directory, create a file called `SparkProcessor.java`
    with the contents of *Listing 8.2*, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 8.2: SparkProcessor.java'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, as in previous examples, the main method invoked the `process()`
    method with the IP address and the port of the Kafka brokers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s fill the `process()` method. The first step is to initialize Spark,
    as demonstrated in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In Spark, the application name must be the same for each member in the cluster,
    so here we call it Kioto (original, isn't it?).
  prefs: []
  type: TYPE_NORMAL
- en: As we are going to run the application locally, we are setting the Spark master
    to `local[*]`, which means that we are creating a number of threads equivalent
    to the machine CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Kafka from Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several connectors for Apache Spark. In this case, we are using the
    Databricks Inc. (the company responsible for Apache Spark) connector for Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this Spark Kafka connector, we can read data with Spark Structured Streaming
    from a Kafka topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Simply by saying Kafka format, we can read a stream from the topic specified
    in the `subscribe` option, running on the brokers specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point in the code, if you invoke the `printSchema()` method on the
    `inputDataSet`, the result will be something similar to *Figure 8.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66bd4c3c-1dd9-45e3-b5e8-28a379cb8a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Print schema output'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can interpret this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The key and the value are binary data. Here in Spark, unfortunately, andunlike
    Kafka, it is not possible to specify deserializers for our data. So, it is necessary
    to use Dataframe operations to do the deserialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each message, we can know the topic, the partition, the offset, and the
    timestamp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The timestamp type is always zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with Kafka Streams, with Spark Streaming, in each step we have to generate
    a new data stream in order to apply transformations and get new ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each step, if we need to print our data stream (to debug the application),
    we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first line is optional, because we really don't need to assign the result
    to an object, just the code execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this snippet is something like *Figure 8.2*. The message value
    is certainly binary data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e9880ca-2571-4043-86cc-1fa6ef189d25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Data stream console output'
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that when we produced the data, it was in JSON format, although Spark
    reads it in binary format. To convert the binary message to string, we use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dataset` console output is now human-readable, and is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to provide the fields list to specify the data structure of
    the JSON message, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we deserialize the String in JSON format. The simplest way is to use
    the prebuilt `from_json()` function in the `org.apache.spark.sql.functions` package,
    which is demonstrated in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we print the `Dataset` at this point, we can see the columns nested as we
    indicated in the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to flatten this `Dataset`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the flattening, if we print the `Dataset`, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that we read the startup time as a string. This is because internally the
    `from_json()` function uses the Jackson library. Unfortunately, there is no way
    to specify the format of the date to be read.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these purposes, fortunately there is the `to_timestamp()` function in the
    same functions package. There is also the `to_date()` function if it is necessary
    to read only a date, ignoring the time specification. Here, we are rewriting the
    `lastStartedAt` column, similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, what we are going to do is to calculate the `uptimes`. As is to be expected,
    Spark does not have a built-in function to calculate the number of days between
    two dates, so we are going to create a user-defined function.
  prefs: []
  type: TYPE_NORMAL
- en: If we remember the KSQL chapter, it is also possible to build and use new UDFs
    in KSQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, the first thing we do is build a function that receives as
    input a `java.sql.Timestamp`, as shown in the following code (this is how timestamps
    are represented in the Spark DataSets) and returns an integer with the number
    of days from that date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to generate a Spark UDF as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: And finally, apply that UDF to the `lastStartedAt` column to create a new column
    in the `Dataset` called `uptime`.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to Kafka from Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we already processed the data and calculated the `uptime`, now all we need
    to do is to write these values in the Kafka topic called `uptimes`.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka's connector allows us to write values to Kafka. The requirement is that
    the `Dataset` to write must have a column called `key` and another column called
    `value`; each one can be of the type String or binary.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want the machine serial number to be the key, there is no problem if
    it is already of String type. Now, we just have to convert the `uptime` column
    from binary into String.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `select()` method of the `Dataset` class to calculate these two
    columns and assign them new names using the `as()` method, shown as follows (to
    do this, we could also use the `alias()` method of that class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `Dataset` is ready and it has the format expected by the Kafka connector.
    The following code is to tell Spark to write these values to Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that we added the checkpoint location in the options. This is to ensure
    the high availability of Kafka. However, this does not guarantee that messages
    are delivered in exactly once mode. Nowadays, Kafka can guarantee exactly once
    delivery; Spark for the moment, can only guarantee the at least once delivery
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we call the `awaitAnyTermination()` method, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'An important note is to mention that if Spark leaves a console output inside
    the code, it implies that all queries must call its `start()` method before calling
    any `awaitTermination()` method, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Also note that we can replace all the `awaitTermination()` calls at the end
    with a single call to `awaitAnyTermination()`, as we did in the original code.
  prefs: []
  type: TYPE_NORMAL
- en: Running the SparkProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the project, run this command from the `kioto` directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is OK, the output is something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'From a command-line terminal, move to the `Confluent` directory and start it
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a console consumer for the `uptimes` topic, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: From our IDE, run the main method of the `PlainProducer` built in previous chapters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer of the producer should be similar to the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: From our IDE, run the main method of the `SparkProcessor`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are someone who uses Spark for batch processing, Spark Structured Streaming
    is a tool you should try, as its API is similar to its batch processing counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we compare Spark to Kafka for stream processing, we must remember that
    Spark streaming is designed to handle throughput, not latency, and it becomes
    very complicated to handle streams with low latency.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark Kafka connector has always been a complicated issue. For example,
    we have to use previous versions of both, because with each new version, there
    are too many changes on both sides.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, the deployment model is always much more complicated than with Kafka
    Streams. Although Spark, Flink, and Beam can perform tasks much more complex tasks,
    than Kafka Streams, the easiest to learn and implement has always been Kafka.
  prefs: []
  type: TYPE_NORMAL
