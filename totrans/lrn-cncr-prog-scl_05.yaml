- en: Chapter 5. Data-Parallel Collections
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 数据并行集合
- en: '|   | *"Premature optimization is the root of all evil."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *"过早优化是万恶之源。"* |   |'
- en: '|   | --*Donald Knuth* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*唐纳德·克努特* |'
- en: So far, we have been composing multiple threads of computation into safe concurrent
    programs. In doing so, we focused on ensuring their correctness. We saw how to
    avoid blocking in concurrent programs, react to the completion of asynchronous
    computations, and how to use concurrent data structures to communicate information
    between threads. All these tools made organizing the structure of concurrent programs
    easier. In this chapter, we will focus mainly on achieving good performance. We
    require minimal or no changes in the organization of existing programs, but we
    will study how to reduce their running time using multiple processors. Futures
    from the previous chapter allowed doing this to a certain extent, but they are
    relatively heavyweight and inefficient when the asynchronous computation in each
    future is short.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在将多个计算线程组合成安全的并发程序。在这样做的时候，我们专注于确保它们的正确性。我们看到了如何避免在并发程序中阻塞，如何对异步计算的完成做出反应，以及如何使用并发数据结构在线程之间传递信息。所有这些工具使组织并发程序的结构变得更加容易。在本章中，我们将主要关注实现良好的性能。我们要求对现有程序的组织进行最小或没有更改，但我们将研究如何使用多个处理器来减少它们的运行时间。前一章中的Futures允许在一定程度上做到这一点，但它们在异步计算较短时相对较重且效率低下。
- en: '**Data parallelism** is a form of computation where the same computation proceeds
    in parallel on different data elements. Rather than having concurrent computation
    tasks that communicate through the use of synchronization, in data-parallel programming,
    independent computations produce values that are eventually merged together in
    some way. An input to a data-parallel operation is usually a dataset such as a
    collection, and the output can be a value or another dataset.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据并行**是一种计算形式，其中相同的计算在不同的数据元素上并行进行。在数据并行编程中，独立的计算产生值，这些值最终以某种方式合并在一起，而不是通过同步使用并发计算任务进行通信。数据并行操作的一个输入通常是一个数据集，如集合，输出可以是一个值或另一个数据集。'
- en: 'In this chapter, we will study the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究以下主题：
- en: Starting a data-parallel operation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动数据并行操作
- en: Configuring the parallelism level of a data-parallel collection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置数据并行集合的并行级别
- en: Measuring performance and why it is important
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能测量及其重要性
- en: Differences between using sequential and parallel collections
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用顺序集合和并行集合之间的差异
- en: Using parallel collections together with concurrent collections
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将并行集合与并发集合一起使用
- en: Implementing a custom parallel collection, such as a parallel string
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自定义并行集合，例如并行字符串
- en: Alternative data-parallel frameworks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的数据并行框架
- en: In Scala, data-parallel programming was applied to the standard collection framework
    to accelerate bulk operations that are, by their nature, declarative and fit data
    parallelism well. Before studying data-parallel collections, we will present a
    brief overview of the Scala collection framework.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中，数据并行编程被应用于标准集合框架，以加速本质上声明性且非常适合数据并行的批量操作。在学习数据并行集合之前，我们将简要介绍Scala集合框架。
- en: Scala collections in a nutshell
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala集合概述
- en: 'The Scala collections module is a package in the Scala standard library that
    contains a variety of general-purpose collection types. Scala collections provide
    a general and easy-to-use way of declaratively manipulating data using functional
    combinators. For example, in the following program, we use the `filter` combinator
    on a range of numbers to return a sequence of palindromes between 0 and 100,000;
    that is, numbers that are read in the same way in both the forward and reverse
    direction:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Scala集合模块是Scala标准库中的一个包，其中包含各种通用集合类型。Scala集合提供了一种通用且易于使用的方法，通过函数组合器声明性地操作数据。例如，在以下程序中，我们使用`filter`组合器对一系列数字进行操作，以返回0到100,000之间的回文数序列；即正向和反向读取方式相同的数字：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Scala collections define three basic types of collections: **sequences**, **maps**,
    and **sets**. Elements stored in sequences are ordered and can be retrieved using
    the `apply` method and an integer index. Maps store key-value pairs and can be
    used to retrieve a value associated with a specific key. Sets can be used to test
    the element membership with the `apply` method.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 集合定义了三种基本的集合类型：**序列**、**映射**和**集合**。存储在序列中的元素是有序的，可以使用 `apply` 方法和整数索引来检索。映射存储键值对，可以用来检索与特定键关联的值。集合可以使用
    `apply` 方法来测试元素成员资格。
- en: The Scala collection library makes a distinction between immutable collections,
    which cannot be modified after they are created, and mutable collections which
    can be updated after they are created. Commonly used immutable sequences are `List`
    and `Vector`, while `ArrayBuffer` is the mutable sequence of choice in most situations.
    Mutable `HashMap` and `HashSet` collections are maps and sets implemented using
    hash tables, while immutable `HashMap` and `HashSet` collections are based on
    the less widely known hash trie data structure.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 集合库在不可变集合和可变集合之间做出了区分，不可变集合在创建后不能被修改，而可变集合在创建后可以被更新。常用的不可变序列有 `List` 和
    `Vector`，而 `ArrayBuffer` 是大多数情况下选择的可变序列。可变的 `HashMap` 和 `HashSet` 集合是使用哈希表实现的映射和集合，而不可变的
    `HashMap` 和 `HashSet` 集合基于不太为人所知的哈希 trie 数据结构。
- en: 'Scala collections can be transformed into their parallel counterparts by calling
    the `par` method. The resulting collection is called a **parallel collection**,
    and its operations are accelerated by using multiple processors simultaneously.
    The previous example can run in parallel, as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 集合可以通过调用 `par` 方法转换为它们的并行对应物。得到的集合被称为**并行集合**，其操作通过同时使用多个处理器来加速。前面的例子可以并行运行，如下所示：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code line, the filter combinator is a data-parallel operation.
    In this chapter, we will study parallel collections in more detail. We will see
    when and how to create parallel collections, study how they can be used together
    with sequential collections, and conclude by implementing a custom parallel collection
    class.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码行中，过滤器组合器是一个数据并行操作。在本章中，我们将更详细地研究并行集合。我们将看到何时以及如何创建并行集合，研究它们如何与顺序集合一起使用，并以实现一个自定义并行集合类作为结论。
- en: Using parallel collections
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用并行集合
- en: 'Most of the concurrent programming utilities we have studied so far are used
    in order to enable different threads of computation to exchange information. Atomic
    variables, the `synchronized` statement, concurrent queues, futures, and promises
    are focused on ensuring the correctness of a concurrent program. On the other
    hand, the parallel collection programming model is designed to be largely identical
    to that of sequential Scala collections; parallel collections exist solely in
    order to improve the running time of the program. In this chapter, we will measure
    the relative speedup of programs using parallel collections. To make this task
    easier, we will introduce the `timed` method to the package object used for the
    examples in this chapter. This method takes a block of code body, and returns
    the running time of the executing block of code `body`. It starts by recording
    the current time with the `nanoTime` method from the JDK `System` class. It then
    runs the body, records the time after the body executes, and computes the time
    difference:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止研究的大多数并发编程工具都是为了使不同的计算线程能够交换信息。原子变量、`synchronized` 语句、并发队列、未来和承诺都专注于确保并发程序的正确性。另一方面，并行集合编程模型被设计成与顺序
    Scala 集合在很大程度上相同；并行集合的存在只是为了提高程序的运行时间。在本章中，我们将测量使用并行集合的程序相对加速速度。为了使这项任务更容易，我们将向本章中用于示例的包对象引入
    `timed` 方法。此方法接受一个代码块，并返回执行代码块 `body` 的运行时间。它首先使用 JDK `System` 类的 `nanoTime` 方法记录当前时间。然后运行代码块，记录代码块执行后的时间，并计算时间差：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Certain runtime optimizations in the JVM, such as dead-code elimination, can
    potentially remove the invocation of the `body` block, causing us to measure an
    incorrect running time. To prevent this, we assign the return value of the `body`
    block to a volatile field named `dummy`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: JVM 中的一些运行时优化，如死代码消除，可能会删除 `body` 块的调用，导致我们测量到不正确的运行时间。为了防止这种情况，我们将 `body` 块的返回值赋给一个名为
    `dummy` 的易变字段。
- en: 'Program performance is subject to many factors, and it is very hard to predict
    in practice. Whenever you can, you should validate your performance assumptions
    with measurements. In the following example, we use the Scala `Vector` class to
    create a vector with five million numbers and then shuffle that vector using the
    `Random` class from the `scala.util` package. We then compare the running time
    of the sequential and parallel `max` methods, which both find the greatest integer
    in the `numbers` collection:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 程序性能受许多因素影响，在实践中很难预测。只要可能，你应该通过测量来验证你的性能假设。在以下示例中，我们使用Scala的`Vector`类创建一个包含五百万个数字的向量，然后使用`scala.util`包中的`Random`类来打乱该向量。然后我们比较顺序和并行`max`方法的运行时间，这两个方法都在`numbers`集合中找到最大的整数：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running this program on an Intel i7-4900MQ quad-core processor with hyper-threading
    and Oracle JVM Version 1.7.0_51, we find that the sequential `max` method takes
    244 milliseconds, while its parallel version takes 35 milliseconds. This is partly
    because parallel collections are optimized better than their sequential counterparts,
    and partly because they use multiple processors. However, on different processors
    and JVM implementations, results will vary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在配备英特尔i7-4900MQ四核处理器和超线程技术以及Oracle JVM版本1.7.0_51的机器上运行此程序，我们发现顺序`max`方法需要244毫秒，而其并行版本只需要35毫秒。这主要是因为并行集合比它们的顺序版本优化得更好，部分原因是它们使用了多个处理器。然而，在不同的处理器和JVM实现中，结果可能会有所不同。
- en: Tip
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Always validate assumptions about performance by measuring the execution time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总是通过测量执行时间来验证关于性能的假设。
- en: The `max` method is particularly well-suited for parallelization. Worker threads
    can independently scan subsets of the collection, such as `numbers`. When a worker
    thread finds the greatest integer in its own subset, it notifies the other processors
    and they agree on the greatest result. This final step takes much less time than
    searching for the greatest integer in a collection subset. We say that the `max`
    method is **trivially parallelizable**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`max`方法非常适合并行化。工作线程可以独立地扫描集合的子集，例如`numbers`。当一个工作线程在其子集中找到最大的整数时，它会通知其他处理器，并就最大结果达成一致。这一步比在集合子集中搜索最大整数所需的时间要少得多。我们说`max`方法是**易于并行化**的。'
- en: 'In general, data-parallel operations require more inter-processor communication
    than the `max` method. Consider the `incrementAndGet` method on atomic variables
    from [Chapter 3](ch03.html "Chapter 3. Traditional Building Blocks of Concurrency"),
    *Traditional Building Blocks of Concurrency*. We can use this method once again
    to compute unique identifiers. This time, we will use parallel collections to
    compute a large number of unique identifiers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据并行操作比`max`方法需要更多的处理器间通信。考虑[第3章](ch03.html "第3章。并发传统的构建块")中原子变量的`incrementAndGet`方法。我们可以再次使用此方法来计算唯一标识符。这次，我们将使用并行集合来计算大量唯一标识符：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This time, we use parallel collections in a `for` loop; recall that every occurrence
    of a `for` loop is desugared into the `foreach` call by the compiler. The parallel
    `for` loop from the preceding code is equivalent to the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们在`for`循环中使用并行集合；回想一下，编译器会将每个`for`循环的出现转换为`foreach`调用。前面代码中的并行`for`循环等同于以下代码：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When the `foreach` method is called on a parallel collection, collection elements
    are processed concurrently. This means that separate worker threads simultaneously
    invoke the specified function, so proper synchronization must be applied. In our
    case, this synchronization is ensured by the atomic variable, as explained in
    [Chapter 3](ch03.html "Chapter 3. Traditional Building Blocks of Concurrency"),
    *Traditional Building Blocks of Concurrency*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当在并行集合上调用`foreach`方法时，集合元素会并发处理。这意味着不同的工作线程会同时调用指定的函数，因此必须应用适当的同步。在我们的例子中，这种同步是通过原子变量来保证的，如[第3章](ch03.html
    "第3章。并发传统的构建块")中所述，*并发传统的构建块*。
- en: Running this program on our machine reveals that there is no increase in speed.
    In fact, the parallel version of the program is even slower; our program prints
    320 milliseconds for the sequential `foreach` call, and 1,041 milliseconds for
    the parallel `foreach` call.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的机器上运行此程序显示，速度没有增加。事实上，程序的并行版本甚至更慢；我们的程序为顺序`foreach`调用打印了320毫秒，为并行`foreach`调用打印了1,041毫秒。
- en: You might be surprised to see this; shouldn't a program be running at least
    four times faster on a quad-core processor with hyper-threading? As shown by the
    preceding example, this is not always the case. The parallel `foreach` call is
    slower because the worker threads simultaneously invoke the `incrementAndGet`
    method on the atomic variable, `uid`, and write to the same memory location at
    once.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会感到惊讶看到这一点；程序在具有超线程的四核处理器上至少应该运行四倍快，对吧？正如前面的例子所示，这并不总是如此。并行`foreach`调用较慢，因为工作线程同时调用原子变量`uid`上的`incrementAndGet`方法，并一次性写入相同的内存位置。
- en: 'Memory writes do not go directly to **Random Access Memory** (**RAM**) in modern
    architectures, as this would be too slow. Instead, modern computer architectures
    separate the CPU from the RAM with multiple levels of caches: smaller, more expensive,
    and much faster memory units that hold copies of parts of the RAM that the processor
    is currently using. The cache level closest to the CPU is called the L1 cache.
    The L1 cache is divided into short contiguous parts called **cache lines**. Typically,
    a cache-line size is 64 bytes. Although multiple cores can read the same cache
    line simultaneously, in standard multicore processors, the cache line needs to
    be in exclusive ownership when a core writes to it. When another core requests
    to write to the same cache line, the cache line needs to be copied to that core''s
    L1 cache. The cache coherence protocol that enables this is called **Modified
    Exclusive Shared Invalid** (**MESI**), and its specifics are beyond the scope
    of this book. All you need to know is that exchanging cache-line ownership can
    be relatively expensive in terms of the processor''s time scale.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代架构中，内存写入不会直接写入**随机存取存储器**（**RAM**），因为这会太慢。相反，现代计算机架构通过多级缓存将CPU与RAM分开：更小、更昂贵且速度更快的内存单元，这些单元保存了处理器当前正在使用的RAM部分。最接近CPU的缓存级别称为L1缓存。L1缓存被划分为称为**缓存行**的短连续部分。通常，缓存行大小为64字节。尽管多个核心可以同时读取相同的缓存行，但在标准多核处理器中，当核心写入时，缓存行需要处于独占所有者状态。当另一个核心请求写入相同的缓存行时，需要将该缓存行复制到该核心的L1缓存。使这成为可能的缓存一致性协议称为**修改独占共享无效**（**MESI**），其具体内容超出了本书的范围。你需要知道的是，在处理器的时间尺度上，交换缓存行所有者可能相对昂贵。
- en: 'Since the `uid` variable is atomic, the JVM needs to ensure a happens-before
    relationship between the writes and reads of the `uid` variable, as we know from
    [Chapter 2](ch02.html "Chapter 2. Concurrency on the JVM and the Java Memory Model"),
    *Concurrency on the JVM and the Java Memory Model*. To ensure the happens-before
    relationship, memory writes have to be visible to other processors. The only way
    to ensure this is to obtain the cache line in exclusive mode before writing to
    it. In our example, different processor cores repetitively exchange the ownership
    of the cache line in which the `uid` variable is allocated, and the resulting
    program becomes much slower than its sequential version. This is shown in the
    following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`uid`变量是原子的，JVM需要确保`uid`变量的写入和读取之间存在happens-before关系，正如我们从[第二章](ch02.html
    "第二章. JVM和Java内存模型中的并发")，*JVM和Java内存模型中的并发*中得知。为了确保happens-before关系，内存写入必须对其他处理器可见。确保这一点的唯一方法是写入之前以独占模式获取缓存行。在我们的例子中，不同的处理器核心反复交换`uid`变量分配的缓存行的所有权，导致程序比其顺序版本慢得多。这在下图中显示：
- en: '![Using parallel collections](img/image_05_001.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![使用并行集合](img/image_05_001.jpg)'
- en: If different processors only read a shared memory location, then there is no
    slowdown. Writing to the same memory location is, on the other hand, an obstacle
    to scalability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不同的处理器只读取共享内存位置，则不会出现减速。另一方面，写入相同的内存位置是可扩展性的障碍。
- en: Tip
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Writing to the same memory location with proper synchronization leads to performance
    bottlenecks and contention; avoid this in data-parallel operations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用适当的同步写入相同的内存位置会导致性能瓶颈和竞争；在数据并行操作中避免这种情况。
- en: Parallel programs share other resources in addition to computing power. When
    different parallel computations request more resources than are currently available,
    an effect known as **resource contention** occurs. The specific kind of resource
    contention that occurs in our example is called a **memory contention**, a conflict
    over exclusive rights to write to a specific part of memory.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算能力之外，并行程序还共享其他资源。当不同的并行计算请求的资源超过当前可用资源时，就会发生一种称为**资源竞争**的现象。在我们示例中发生的特定类型的资源竞争被称为**内存竞争**，即对写入特定内存部分的排他权的冲突。
- en: We can expect the same kind of performance degradation when using multiple threads
    to concurrently start the `synchronized` statement on the same object, repetitively
    modifying the same key in a concurrent map or simultaneously enqueueing elements
    to a concurrent queue; all these actions require writes to the same memory location.
    Nonetheless, this does not mean that threads should never write to the same memory
    locations. In some applications, concurrent writes occur very infrequently; the
    ratio between the time spent writing to contended memory locations and the time
    spent doing other work determines whether parallelization is beneficial or not.
    It is difficult to predict this ratio by just looking at the program; the `ParUid`
    example serves to illustrate that we should always measure in order to see the
    impact of contention.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以预期，当使用多个线程并发地对同一对象上的`synchronized`语句进行重复修改，或在并发映射中同时修改相同的键，或在并发队列中同时入队元素时，会出现相同类型的性能下降；所有这些操作都需要写入相同的内存位置。尽管如此，这并不意味着线程永远不会写入相同的内存位置。在某些应用程序中，并发写入非常罕见；写入竞争内存位置所花费的时间与执行其他工作所花费的时间之间的比率决定了并行化是否有益。仅通过查看程序很难预测这个比率；`ParUid`示例旨在说明我们应该始终进行测量，以了解竞争的影响。
- en: Parallel collection class hierarchy
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行集合类层次结构
- en: As we saw, parallel collection operations execute on different worker threads
    simultaneously. At any point during the execution of a parallel operation, an
    element in a parallel collection can be processed by at most one worker thread
    executing that operation. The block of code associated with the parallel operation
    is executed on each of the elements separately; in the `ParUid` example, the `incrementAndGet`
    method is called concurrently many times. Whenever a parallel operation executes
    any side-effects, it must take care to use proper synchronization; the naive approach
    of using `var` to store `uid` causes data races as it did in [Chapter 2](ch02.html
    "Chapter 2. Concurrency on the JVM and the Java Memory Model"), *Concurrency on
    the JVM and the Java Memory Model*. This is not the case with sequential Scala
    collections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，并行集合操作是在不同的工作线程上同时执行的。在并行操作执行过程中的任何时刻，并行集合中的一个元素最多只能被一个执行该操作的工作线程处理。与并行操作相关的代码块是分别在每个元素上执行的；在`ParUid`示例中，`incrementAndGet`方法被并发多次调用。每当并行操作执行任何副作用时，它必须注意使用适当的同步；使用`var`来存储`uid`的简单方法会导致数据竞争，就像在[第2章](ch02.html
    "第2章。JVM和Java内存模型上的并发")中发生的那样，*JVM和Java内存模型上的并发*。这与顺序Scala集合的情况不同。
- en: The consequence is that a parallel collection cannot be a subtype of a sequential
    collection. If it were, then the *Liskov substitution principle* would be violated.
    The Liskov substitution principle states that if type `S` is a subtype of `T`,
    then the object of type `T` can be replaced with objects of type `S` without affecting
    the correctness of the program.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，并行集合不能成为顺序集合的子类型。如果是这样，那么就会违反**Liskov替换原则**。Liskov替换原则指出，如果类型`S`是类型`T`的子类型，那么类型`T`的对象可以被类型`S`的对象替换，而不会影响程序的正确性。
- en: 'In our case, if parallel collections are subtypes of sequential collections,
    then some methods can return a sequential sequence collection with the static
    type `Seq[Int]`, where the sequence object is a parallel sequence collection at
    runtime. Clients can call methods such as `foreach` on the collection without
    knowing that the body of the `foreach` method needs synchronization, and their
    programs would not work correctly. For these reasons, parallel collections form
    a hierarchy that is separate from the sequential collections, as shown in the
    following diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，如果并行集合是顺序集合的子类型，那么某些方法可以返回具有静态类型`Seq[Int]`的顺序序列集合，其中序列对象在运行时是并行序列集合。客户端可以在不知道`foreach`方法体需要同步的情况下调用集合上的方法，而他们的程序将无法正确运行。出于这些原因，并行集合形成了一个与顺序集合分开的层次结构，如下面的图所示：
- en: '![Parallel collection class hierarchy](img/image_05_002.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![并行集合类层次结构](img/image_05_002.jpg)'
- en: Scala collection hierarchy
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Scala集合层次结构
- en: 'The preceding diagram shows the simplified Scala collection hierarchy with
    sequential collections on the left. The most general collection type is called
    `Traversable`. Different collection operations such as `find`, `map`, `filter`,
    or `reduceLeft` are implemented in terms of its abstract `foreach` method. Its
    `Iterable[T]` subtype offers additional operations such as `zip`, `grouped`, `sliding`,
    and `sameElements`, implemented using its `iterator` method. The `Seq`, `Map`,
    and `Set` traits are iterable collections that represent Scala sequences, maps,
    and sets, respectively. These traits are used to write code that is generic in
    the type of the concrete Scala collection. The following `nonNull` method copies
    elements from an `xs` collection that are different from `null`. Here, the `xs`
    collection can be a `Vector[T]`, `List[T]`, or some other sequence:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图显示了简化的Scala集合层次结构，其中顺序集合位于左侧。最一般的集合类型称为`Traversable`。不同的集合操作，如`find`、`map`、`filter`或`reduceLeft`，都是基于其抽象的`foreach`方法实现的。其`Iterable[T]`子类型提供了额外的操作，如`zip`、`grouped`、`sliding`和`sameElements`，这些操作使用其`iterator`方法实现。`Seq`、`Map`和`Set`特质是可迭代的集合，分别代表Scala的序列、映射和集合。这些特质用于编写针对具体Scala集合类型的通用代码。下面的`nonNull`方法从`xs`集合中复制与`null`不同的元素。在这里，`xs`集合可以是`Vector[T]`、`List[T]`或其他序列：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Parallel collections form a separate hierarchy. The most general parallel collection
    type is called `ParIterable`. Methods such as `foreach`, `map`, or `reduce` on
    a `ParIterable` object execute in parallel. The `ParSeq`, `ParMap`, and `ParSet`
    collections are parallel collections that correspond to `Seq`, `Map`, and `Set`,
    but are not their subtypes. We can rewrite the `nonNull` method to use parallel
    collections:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合形成了一个单独的层次结构。最一般的并行集合类型称为`ParIterable`。在`ParIterable`对象上执行的方法，如`foreach`、`map`或`reduce`，将并行执行。`ParSeq`、`ParMap`和`ParSet`集合是并行集合，对应于`Seq`、`Map`和`Set`，但不是它们的子类型。我们可以将`nonNull`方法重写为使用并行集合：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Although the implementation is identical, we can no longer pass a sequential
    collection to the `nonNull` method. We can call `.par` on the sequential `xs`
    collection before passing it to the `nonNull` method, but then the `filter` method
    will execute in parallel. Can we instead write code that is agnostic in the type
    of the collection? The generic collection types: `GenTraversable`, `GenIterable`,
    `GenSeq`, `GenMap`, and `GenSet` exist for this purpose. Each of them represents
    a supertype of the corresponding sequential or parallel collection type. For example,
    the `GenSeq` generic sequence type allows us to rewrite the `nonNull` method as
    follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实现相同，但我们不能再将顺序集合传递给`nonNull`方法。我们可以在将集合传递给`nonNull`方法之前，对顺序的`xs`集合调用`.par`，但这样`filter`方法将并行执行。我们能否编写对集合类型无知的代码？存在用于此目的的通用集合类型：`GenTraversable`、`GenIterable`、`GenSeq`、`GenMap`和`GenSet`。每个都代表相应顺序或并行集合类型的超类型。例如，`GenSeq`通用序列类型允许我们将`nonNull`方法重写如下：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When using generic collection types, we need to remember that they might be
    implemented either as a sequential collection or as a parallel collection. Thus,
    as a precaution, if operations invoked on a generic collection execute any side
    effects, you should use synchronization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用通用集合类型时，我们需要记住它们可能被实现为顺序集合或并行集合。因此，作为预防措施，如果在通用集合上执行的操作有任何副作用，您应该使用同步。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Treat operations invoked on a generic collection type as if they are parallel.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将在通用集合类型上执行的操作视为并行操作。
- en: Configuring the parallelism level
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置并行级别
- en: Parallel collections use all the processors by default; their underlying executor
    has as many workers as there are processors. We can change this default behavior
    by changing the `TaskSupport` object of the parallel collection. The basic `TaskSupport`
    implementation is the `ForkJoinTaskSupport` class. It takes a `ForkJoinPool` collection
    and uses it to schedule parallel operations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集合默认使用所有处理器；其底层执行器的工作者数量与处理器数量相同。我们可以通过更改并行集合的`TaskSupport`对象来改变这种默认行为。基本的`TaskSupport`实现是`ForkJoinTaskSupport`类。它接受一个`ForkJoinPool`集合，并使用它来调度并行操作。
- en: 'Therefore, to change the parallelism level of a parallel collection, we instantiate
    a `ForkJoinPool` collection with the desired parallelism level:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要改变并行集合的并行级别，我们需要实例化一个具有所需并行级别的`ForkJoinPool`集合：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once a `TaskSupport` object is created, we can use it with different parallel
    collections. Every parallel collection has a `tasksupport` field that we use to
    assign the `TaskSupport` object to it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了一个`TaskSupport`对象，我们就可以使用它与不同的并行集合一起使用。每个并行集合都有一个`tasksupport`字段，我们使用它将`TaskSupport`对象分配给它。
- en: Measuring the performance on the JVM
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在JVM上测量性能
- en: To correctly measure the running time on the JVM is not an easy task. Under
    the hood, the JVM does a lot more than meets the eye. The Scala compiler does
    not produce machine code directly runnable on the CPU. Instead, the Scala compiler
    produces a special intermediate instruction code called **Java bytecode**. When
    bytecode from the Scala compiler gets run inside the JVM, at first it executes
    in so-called **interpreted mode**; the JVM interprets each bytecode instruction
    and simulates the execution of the program. Only when the JVM decides that the
    bytecode in a certain method was run often enough does it compile the bytecode
    to machine code, which can be executed directly on the processor. This process
    is called **just-in-time compilation**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在JVM上正确测量运行时间不是一项容易的任务。在底层，JVM做了很多超出我们视线的工作。Scala编译器不会直接产生可在CPU上运行的机器码。相反，Scala编译器产生一种特殊的中间指令代码，称为**Java字节码**。当Scala编译器的字节码在JVM内部运行时，最初它以所谓的**解释模式**执行；JVM解释每个字节码指令并模拟程序的执行。只有当JVM决定某个方法中的字节码已经运行足够频繁时，它才会将字节码编译成机器码，这可以直接在处理器上执行。这个过程称为**即时编译**。
- en: The JVM needs standardized bytecode to be cross-platform; the same bytecode
    can be run on any processor or operating system that supports the JVM. However,
    the entire bytecode of a program cannot be translated to the machine code as soon
    as the program runs; this would be too slow. Instead, the JVM translates parts
    of the programs, such as specific methods, incrementally, in short compiler runs.
    In addition, the JVM can decide to additionally optimize certain parts of the
    program that execute very frequently. As a result, programs running on the JVM
    are usually slow immediately after they start, and eventually reach their optimal
    performance. Once this happens, we say that the JVM reached its steady state.
    When evaluating the performance on the JVM, we are usually interested in the **steady
    state**; most programs run long enough to achieve it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: JVM需要标准化的字节码以实现跨平台；相同的字节码可以在支持JVM的任何处理器或操作系统上运行。然而，程序的全部字节码并不能在程序运行时立即转换为机器码；这将太慢。相反，JVM会增量地将程序的某些部分，如特定方法，在短暂的编译运行中转换。此外，JVM还可以决定额外优化程序中执行非常频繁的部分。因此，在JVM上运行的程序通常在启动后速度较慢，最终达到最佳性能。一旦发生这种情况，我们就说JVM达到了稳定状态。在评估JVM上的性能时，我们通常对**稳定状态**感兴趣；大多数程序运行足够长的时间以达到这一状态。
- en: 'To witness this effect, assume that you want to find out what the `TEXTAREA`
    tag means in HTML. You write the program that downloads the HTML specification
    and searches for the first occurrence of the `TEXTAREA` string. Having mastered
    asynchronous programming in [Chapter 4](ch04.html "Chapter 4.  Asynchronous Programming
    with Futures and Promises"), *Asynchronous Programming with Futures and Promises*,
    you can implement the `getHtmlSpec` method, which starts an asynchronous computation
    to download the HTML specification and returns a future value with the lines of
    the HTML specification. You then install a callback; once the HTML specification
    is available, you can call the `indexWhere` method on the lines to find the line
    that matches the regular expression `.*TEXTAREA.*`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了见证这种效果，假设你想找出 HTML 中的 `TEXTAREA` 标签代表什么。你编写了一个程序，下载 HTML 规范并搜索 `TEXTAREA`
    字符串的第一个出现。在掌握了 [第 4 章](ch04.html "第 4 章。使用 Future 和 Promise 进行异步编程") 中的异步编程后，*使用
    Future 和 Promise 进行异步编程*，你可以实现 `getHtmlSpec` 方法，该方法启动一个异步计算以下载 HTML 规范，并返回一个包含
    HTML 规范行的 future 值。然后你安装一个回调；一旦 HTML 规范可用，你就可以在行上调用 `indexWhere` 方法来找到匹配正则表达式
    `.*TEXTAREA.*` 的行：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running this example several times from SBT shows that the times vary. At first,
    the sequential and parallel versions execute for 45 and 16 milliseconds, respectively.
    Next time, they take 36 and 10 milliseconds, and subsequently 10 and 4 milliseconds.
    Note that we only observe this effect when running the examples inside the same
    JVM process as SBT itself.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从 SBT 运行此示例多次表明，运行时间会有所不同。最初，顺序版本和并行版本分别执行了 45 毫秒和 16 毫秒。下一次，它们分别需要 36 毫秒和 10
    毫秒，随后是 10 毫秒和 4 毫秒。请注意，我们只有在将示例运行在与 SBT 本身相同的 JVM 进程内时才会观察到这种效果。
- en: 'We can draw a false conclusion that the steady state was reached at this point.
    In truth, we should run this program many more times before the JVM properly optimizes
    it. Therefore, we add the `warmedTimed` method to our package object. This method
    runs the block of code `n` times before measuring its running time. We set the
    default value for the `n` variable to `200`; although there is no way to be sure
    that the JVM will reach a steady state after executing the block of code 200 times,
    this is a reasonable default:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会得出一个错误的结论，认为此时已经达到了稳定状态。实际上，我们应该在 JVM 正确优化程序之前运行这个程序许多次。因此，我们将 `warmedTimed`
    方法添加到我们的包对象中。此方法在测量运行时间之前运行代码块 `n` 次。我们将 `n` 变量的默认值设置为 `200`；尽管无法保证 JVM 在执行代码块
    200 次后将达到稳定状态，但这是一个合理的默认值：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now call the `warmedTimed` method instead of the `timed` method in the
    `ParHtmlSearch` example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用 `ParHtmlSearch` 示例中的 `warmedTimed` 方法，而不是 `timed` 方法：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Doing so changes the running times on our machine to 1.5 and 0.5 milliseconds
    for the sequential and parallel versions of the program, respectively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做将我们的机器上程序的顺序版本和并行版本的运行时间分别改变为 1.5 毫秒和 0.5 毫秒。
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Make sure that the JVM is in the steady state before drawing any premature conclusions
    about the running time of a program.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在对程序运行时间做出任何过早的结论之前，请确保 JVM 处于稳定状态。
- en: There are other reasons why measuring performance on the JVM is hard. Even if
    the JVM reached a steady state for the part of the program we measure, the **Just-In-Time**
    (**JIT**) compiler can at any point pause the execution and translate some other
    part of the program, effectively slowing down our measurement. Then, the JVM provides
    automatic memory management. In languages such as C++, an invocation of the `new`
    keyword, which is used to allocate an object, must be accompanied by the corresponding
    `delete` call that frees the memory occupied by the object so that it can be reused
    later. In languages such as Scala and Java, however, there is no `delete` statement;
    objects are eventually freed automatically during the process called **Garbage
    Collection** (**GC**). Periodically, the JVM stops the execution, scans the heap
    for all objects no longer used in the program, and frees the memory they occupy.
    If we measure the running time of code that frequently causes GC cycles, the chances
    are that GC will skew the measurements. In some cases, the performance of the
    same program can vary from one JVM process to the other because the objects get
    allocated in a way that causes a particular memory access pattern, impacting the
    program's performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: To get really reliable running time values, we need to run the code many times
    by starting separate JVM processes, making sure that the JVM reached a steady
    state, and taking the average of all the measurements. Frameworks such as **ScalaMeter**,
    introduced in [Chapter 9](ch09.html "Chapter 9. Concurrency in Practice"), *Concurrency
    in Practice*, go a long way toward automating this process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Caveats with parallel collections
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel collections were designed to provide a programming API similar to sequential
    Scala collections. Every sequential collection has a parallel counterpart and
    most operations have the same signature in both sequential and parallel collections.
    Still, there are some caveats when using parallel collections, and we will study
    them in this section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Non-parallelizable collections
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parallel collections use **splitters**, represented with the `Splitter[T]`
    type, in order to provide parallel operations. A splitter is a more advanced form
    of an iterator; in addition to the iterator''s `next` and `hasNext` methods, splitters
    define the `split` method, which divides the splitter `S` into a sequence of splitters
    that traverse parts of the `S` splitter:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This method allows separate processors to traverse separate parts of the input
    collection. The `split` method must be implemented efficiently, as this method
    is invoked many times during the execution of a parallel operation. In the vocabulary
    of computational complexity theory, the allowed asymptotic running time of the
    `split` method is **O**(log (*N*)), where *N* is the number of elements in the
    splitter. Splitters can be implemented for flat data structures such as arrays
    and hash tables, and tree-like data structures such as immutable hash maps and
    vectors. Linear data structures such as the Scala `List` and `Stream` collections
    cannot efficiently implement the `split` method. Dividing a long linked list of
    nodes into two parts requires traversing these nodes, which takes a time that
    is proportionate to the size of the collection.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Operations on Scala collections such as `Array`, `ArrayBuffer`, mutable `HashMap`
    and `HashSet`, `Range`, `Vector`, immutable `HashMap` and `HashSet`, and concurrent
    `TrieMap` can be parallelized. We call these collections *parallelizable*. Calling
    the `par` method on these collections creates a parallel collection that shares
    the same underlying dataset as the original collection. No elements are copied
    and the conversion is fast.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Scala collections need to be converted to their parallel counterparts
    upon calling `par`. We can refer to them as *non-parallelizable collections*.
    Calling the `par` method on non-parallelizable collections entails copying their
    elements into a new collection. For example, the `List` collection needs to be
    copied to a `Vector` collection when the `par` method is called, as shown in the
    following code snippet:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Calling `par` on `List` takes 55 milliseconds on our machine, whereas calling
    `par` on `Vector` takes 0.025 milliseconds. Importantly, the conversion from a
    sequential collection to a parallel one is not itself parallelized, and is a possible
    sequential bottleneck.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Converting a non-parallelizable sequential collection to a parallel collection
    is not a parallel operation; it executes on the caller thread.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the cost of converting a non-parallelizable collection to a parallel
    one is acceptable. If the amount of work in the parallel operation far exceeds
    the cost of converting the collection, then we can bite the bullet and pay the
    cost of the conversion. Otherwise, it is more prudent to keep the program data
    in parallelizable collections and benefit from fast conversions. When in doubt,
    measure!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Non-parallelizable operations
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While most parallel collection operations achieve superior performance by executing
    on several processors, some operations are inherently sequential, and their semantics
    do not allow them to execute in parallel. Consider the `foldLeft` method from
    the Scala collections API:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This method visits elements of the collection going from left to right and
    adds them to the accumulator of type `S`. The accumulator is initially equal to
    the zero value `z`, and is updated with the function `f` that uses the accumulator
    and a collection element of type `T` to produce a new accumulator. For example,
    given a list of integers `List(1, 2, 3)`, we can compute the sum of its integers
    with the following expression:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This `foldLeft` method starts by assigning `0` to `acc`. It then takes the first
    element in the list `1` and calls the function `f` to evaluate `0 + 1`. The `acc`
    accumulator then becomes `1`. This process continues until the entire list of
    elements is visited, and the `foldLeft` method eventually returns the result `6`.
    In this example, the `S` type of the accumulator is set to the `Int` type. In
    general, the accumulator can have any type. When converting a list of elements
    to a string, the zero value is an empty string and the function `f` concatenates
    a string and a number.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'The crucial property of the `foldLeft` operation is that it traverses the elements
    of the list by going from left to right. This is reflected in the type of the
    function `f`; it accepts an accumulator of type `S` and a list value of type `T`.
    The function `f` cannot take two values of the accumulator type `S` and merge
    them into a new accumulator of type `S`. As a consequence, computing the accumulator
    cannot be implemented in parallel; the `foldLeft` method cannot merge two accumulators
    from two different processors. We can confirm this by running the following program:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding program, we use the `getHtmlSpec` method introduced earlier
    to obtain the lines of the HTML specification. We install a callback using the
    `foreach` call to process the HTML specification once it arrives; the `allMatches`
    method calls the `foldLeft` operation to accumulate the lines of the specification
    that contain the `TEXTAREA` string. Running the program reveals that both the
    sequential and parallel `foldLeft` operations take 5.6 milliseconds.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify how the accumulators produced by different processors should be
    merged together, we need to use the `aggregate` method. The `aggregate` method
    is similar to the `foldLeft` operation, but it does not specify that the elements
    are traversed from left to right. Instead, it only specifies that subsets of elements
    are visited going from left to right; each of these subsets can produce a separate
    accumulator. The `aggregate` method takes an additional function of type `(S,
    S) => S`, which is used to merge multiple accumulators:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Running the example again shows the difference between the sequential and parallel
    versions of the program; the parallel `aggregate` method takes 1.4 milliseconds
    to complete on our machine.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: When doing these kinds of reduction operation in parallel, we can alternatively
    use the `reduce` or `fold` methods, which do not guarantee going from left to
    right. The `aggregate` method is more expressive, as it allows the accumulator
    type to be different from the type of the elements in the collection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the `aggregate` method to execute parallel reduction operations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Other inherently sequential operations include `foldRight`, `reduceLeft`, `reduceRight`,
    `reduceLeftOption`, `reduceRightOption`, `scanLeft`, `scanRight`, and methods
    that produce non-parallelizable collections such as the `toList` method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Side effects in parallel operations
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As their name implies, parallel collections execute on multiple threads concurrently.
    We have already learned in [Chapter 2](ch02.html "Chapter 2. Concurrency on the
    JVM and the Java Memory Model"), *Concurrency on the JVM and the Java Memory Model*,
    that multiple threads cannot correctly modify shared memory locations without
    the use of synchronization. Assigning to a mutable variable from a parallel collection
    operation may be tempting, but it is almost certainly incorrect. This is best
    illustrated by the following example, in which we construct two sets, `a` and
    `b`, where `b` is the subset of the elements in `a`, and then uses the `total`
    mutable variable to count the size of the intersection:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Instead of returning `250`, the parallel version nondeterministically returns
    various wrong results. Note that you might have to change the sizes of the sets
    `a` and `b` to witness this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To ensure that the parallel version returns the correct results, we can use
    an atomic variable and its `incrementAndGet` method. However, this leads to the
    same scalability problems we had before. A better alternative is to use the parallel
    `count` method:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If the amount of work executed per element is low and the matches are frequent,
    the parallel `count` method will result in better performance than the `foreach`
    method with an atomic variable.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid the need for synchronization and ensure better scalability, favor declarative-style
    parallel operations instead of the side effects in parallel `for` loops.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we must ensure that the memory locations read by a parallel operation
    are protected from concurrent writes. In the last example, the `b` set should
    not be concurrently mutated by some thread while the parallel operation is executing;
    this leads to the same incorrect results as using mutable variables from within
    the parallel operation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterministic parallel operations
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html "Chapter 2. Concurrency on the JVM and the Java Memory
    Model"), *Concurrency on the JVM and the Java Memory Model*, we saw that multithreaded
    programs can be nondeterministic; given the same inputs, they can produce different
    outputs depending on the execution schedule. The `find` collection operation returns
    an element matching a given predicate. The parallel `find` operation returns whichever
    element was found first by some processor. In the following example, we use `find`
    to search the HTML specification for occurrences of the `TEXTAREA` string; running
    the example several times gives different results, because the `TEXTAREA` string
    occurs in many different places in the HTML specification:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we want to retrieve the first occurrence of the `TEXTAREA` string, we need
    to use `indexWhere` instead:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Parallel collection operations other than `find` are deterministic as long
    as their operators are **pure functions**. A pure function is always evaluated
    to the same value, given the same inputs, and does not have any side effects.
    For example, the function `(x: Int) => x + 1` is a pure function. By contrast,
    the following function `f` is not pure, because it changes the state of the `uid`
    value:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Even if a function does not modify any memory locations, it is not pure if
    it reads memory locations that might change. For example, the following `g` function
    is not pure:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'When used with a non-pure function, any parallel operation can become nondeterministic.
    Mapping the range of values to unique identifiers in parallel gives a nondeterministic
    result, as illustrated by the following call:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The resulting sequence, `uids`, is different in separate executions. The parallel
    `map` operation retains the relative order of elements from the range `0 until
    10000`, so the tuples in `uids` are ordered by their first elements from 0 until
    10,000\. On the other hand, the second element in each tuple is assigned nondeterministically;
    in one execution, the `uids` sequence can start with the `(0, 0), (1, 2), (2,
    3), ...` and in another, with `(0, 0), (1, 4), (2, 9), ...`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Commutative and associative operators
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel collection operations such as `reduce`, `fold`, `aggregate`, and `scan`
    take binary operators as part of their input. A binary operator is a function  `op`
    that takes two arguments, `a` and `b`. We can say that the binary operator `op`
    is **commutative** if changing the order of its arguments returns the same result,
    that is, `op(a, b) == op(b, a)`. For example, adding two numbers together is a
    commutative operation. Concatenating two strings is not a commutative operation;
    we get different strings depending on the concatenation order.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Binary operators for the parallel `reduce`, `fold`, `aggregate`, and `scan`
    operations never need to be commutative. Parallel collection operations always
    respect the relative order of the elements when applying binary operators, provided
    that the underlying collections have any ordering. Elements in sequence collections,
    such as `ArrayBuffer` collections, are always ordered. Other collection types
    can order their elements but are not required to do so.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can concatenate the strings inside an `ArrayBuffer`
    collection into one long string by using the sequential `reduceLeft` operation
    and the parallel `reduce` operation. We then convert the `ArrayBuffer` collection
    into a set, which does not have an ordering:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the string is concatenated correctly when the parallel `reduce`
    operation is invoked on a parallel array, but the order of the pages is mangled
    both for the `reduceLeft` and `reduce` operations when invoked on a set; the default
    Scala set implementation does not order the elements.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Binary operators used in parallel operations do not need to be commutative.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: An `op` binary operator is **associative** if applying `op` consecutively to
    a sequence of values `a`, `b`, and `c` gives the same result regardless of the
    order in which the operator is applied, that is, `op(a, op(b, c)) == op(op(a,
    b), c)`. Adding two numbers together or computing the larger of the two numbers
    is an associative operation. Subtraction is not associative, as `1 - (2 - 3)`
    is different from `(1 - 2) - 3`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel collection operations usually require associative binary operators.
    While using subtraction with the `reduceLeft` operation means that all the numbers
    in the collection should be subtracted from the first number, using subtraction
    in the `reduce`, `fold`, or `scan` methods gives nondeterministic and incorrect
    results, as illustrated by the following code snippet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: While the `reduceLeft` operation consistently returns `-435`, the `reduce` operation
    returns meaningless results at random.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Make sure that binary operators used in parallel operations are associative.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel operations such as `aggregate` require the multiple binary operators,
    `sop` and `cop`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `sop` operator is of the same type as the operator required by the `reduceLeft`
    operation. It takes an accumulator and the collection element. The `sop` operator
    is used to fold elements within a subset assigned to a specific processor. The
    `cop` operator is used to merge the subsets together and is of the same type as
    the operators for `reduce` and `fold`. The `aggregate` operation requires that
    `cop` is associative and that `z` is the **zero element** for the accumulator,
    that is, `cop(z, a) == a`. Additionally, the `sop` and `cop` operators must give
    the same result irrespective of the order in which element subsets are assigned
    to processors, that is, `cop(sop(z, a), sop(z, b)) == cop(z, sop(sop(z, a), b))`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Using parallel and concurrent collections together
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen that parallel collection operations are not allowed to
    access mutable states without the use of synchronization. This includes modifying
    sequential Scala collections from within a parallel operation. Recall that we
    used a mutable variable in the section on side effects to count the size of the
    intersection. In the following example, we will download the URL and HTML specifications,
    convert them to sets of words, and try to find an intersection of their words.
    In the `intersection` method, we use a `HashSet` collection and update it in parallel.
    Collections in the `scala.collection.mutable` package are not thread-safe. The
    following example nondeterministically drops elements, corrupts the buffer state,
    or throws exceptions:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We learned in [Chapter 3](ch03.html "Chapter 3. Traditional Building Blocks
    of Concurrency"), *Traditional Building Blocks of Concurrency*, that concurrent
    collections can be safely modified by multiple threads without the risk of data
    corruption. We use the concurrent skip list collection from the JDK to accumulate
    words that appear in both specifications. The `decorateAsScala` object is used
    to add the `asScala` method to Java collections:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Weakly consistent iterators
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [Chapter 3](ch03.html "Chapter 3. Traditional Building Blocks of
    Concurrency"), *Traditional Building Blocks of Concurrency*, iterators on most
    concurrent collections are weakly consistent. This means that they are not guaranteed
    to correctly traverse the data structure if some thread concurrently updates the
    collection during traversal.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'When executing a parallel operation on a concurrent collection, the same limitation
    applies; the traversal is weakly consistent and might not reflect the state of
    the data structure at the point when the operation started. The Scala `TrieMap`
    collection is an exception to this rule. In the following example, we will create
    a `TrieMap` collection called `cache` containing numbers between 0 and 100, mapped
    to their string representation. We will then start a parallel operation that traverses
    these numbers and adds the mappings for their negative values to the map:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The parallel `foreach` operation does not traverse entries added after the parallel
    operation started; only positive numbers are reflected in the traversal. The `TrieMap`
    collection is implemented using the Ctrie concurrent data structure, which atomically
    creates a snapshot of the collection when the parallel operation starts. Snapshot
    creation is efficient and does not require you to copy the elements; subsequent
    update operations incrementally rebuild parts of the `TrieMap` collection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whenever program data needs to be simultaneously modified and traversed in parallel,
    use the `TrieMap` collection.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Implementing custom parallel collections
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel collections in the Scala standard library are sufficient for most tasks,
    but in some cases we want to add parallel operations to our own collections. The
    Java `String` class does not have a direct parallel counterpart in the parallel
    collections framework. In this section, we will study how to implement a custom
    `ParString` class that supports parallel operations. We will then use our custom
    parallel collection class in several example programs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in implementing a custom parallel collection is to extend the
    correct parallel collection trait. A parallel string is a sequence of characters,
    so we need to extend the `ParSeq` trait with the `Char` type argument. Once a
    string is created, it can no longer be modified; we say that the string is an
    immutable collection. For this reason, we extend a subtype of the `scala.collection.parallel.ParSeq`
    trait, the `ParSeq` trait from the `scala.collection.parallel.immutable` package:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When we extend a parallel collection, we need to implement its `apply`, `length`,
    `splitter`, and `seq` methods. The `apply` method returns an element at position
    `i` in the sequence, and the `length` method returns the total number of elements
    in the sequence. These methods are equivalent to the methods on sequential collections,
    so we use the `String` class's `charAt` and `length` methods to implement them.
    Where defining a custom regular sequence requires implementing its `iterator`
    method, custom parallel collections need a `splitter` method. Calling `splitter`
    returns an object of the `Splitter[T]` type, a special iterator that can be split
    into subsets. We implement the `splitter` method to return a `ParStringSplitter`
    object, which we will show you shortly. Finally, parallel collections need a `seq`
    method, which returns a sequential Scala collection. Since `String` itself comes
    from Java and is not a Scala collection, we will use its `WrappedString` wrapper
    class from the Scala collections library.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Our custom parallel collection class is almost complete; we only need to provide
    the implementation for the `ParStringSplitter` object. We will study how to do
    this next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Splitters
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A splitter is an iterator that can be efficiently split into disjoint subsets.
    Here, efficient means that the splitter's `split` method must have **O**(*log*(*N*))
    running time, where *N* is the number of elements in the splitter. Stated informally,
    a splitter is not allowed to copy large parts of the collection when split; if
    it did, the computational overhead from splitting would overcome any benefits
    from parallelization and become a serial bottleneck.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to define a new `Splitter` class for the Scala parallel collection
    framework is to extend the `IterableSplitter[T]` trait, which has the following
    simplified interface:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The splitter interface declares the `dup` method which duplicates the current
    splitter. This method simply returns a new splitter pointing to the same subset
    of the collection. Splitters also define the `remaining` method, which returns
    the number of elements that the splitter can traverse by calling `next` before
    the `hasNext` method returns `false`. The `remaining` method does not change the
    state of the splitter and can be called as many times as necessary.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: However, the `split` method can be called only once and it invalidates the splitter;
    none of the splitter's methods should be called after calling the `split` method.
    The `split` method returns a sequence of splitters that iterate over the disjoint
    subsets of the original splitter. If the original splitter has two or more elements
    remaining, then none of the resulting splitters should be empty, and the `split`
    method should return at least two splitters. If the original splitter has a single
    element or no elements remaining, then `split` is allowed to return empty splitters.
    Importantly, the splitters returned by `split` should be approximately equal in
    size; this helps the parallel collection scheduler achieve good performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow sequence-specific operations such as `zip`, `sameElements`, and `corresponds`,
    parallel sequence collections use a more refined subtype of the `IterableSplitter`
    trait, called the `SeqSplitter` trait:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Sequence splitters declare an additional method, `psplit`, which takes the list
    of sizes for the splitter partitions and returns as many splitters and elements
    as specified by the `sizes` parameter. If `sizes` specifies more elements than
    there are available in the splitter, additional empty splitters are returned at
    the end of the resulting sequence. For example, calling `s.psplit(10, 20, 15)`
    on a splitter with only 15 elements yields three splitters with sizes 10, five,
    and zero.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if the `sizes` parameter specifies fewer elements than there are
    in the splitter, an additional splitter with the remaining elements is appended
    at the end.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Our parallel string class is a parallel sequence, so we need to implement a
    sequence splitter. We can start by extending the `SeqSplitter` class with the
    `Char` type parameter:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We add the `s` field pointing to the underlying `String` object in the `ParStringSplitter`
    constructor. A parallel string splitter must represent a subset of the elements
    in the string, so we add an `i` field to represent the position of the next character
    that will be traversed by the splitter. Note that `i` does not need to be synchronized;
    the splitter is only used by one processor at a time. The `limit` field contains
    the position after the last character in the splitter. This way, our splitter
    class represents substrings of the original string.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing methods inherited from the `Iterator` trait is easy. As long as
    `i` is less than `limit`, `hasNext` must return `true`. The `next` method uses
    `i` to read the character at that position, increment `i`, and return the character:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `dup` and `remaining` methods are straightforward; the `dup` method creates
    a new parallel string splitter using the state of the current splitter, and the
    `remaining` method uses `limit` and `i` to compute the number of remaining elements:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The main parts of a splitter are its `split` and `psplit` methods. Luckily,
    `split` can be implemented in terms of `psplit`. If there is more than one element
    remaining, we call the `psplit` method. Otherwise, if there are no elements to
    split, we return the `this` splitter:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `psplit` method uses `sizes` to peel off parts of the original splitter.
    It does so by incrementing the `i` variable and creating a new splitter for each
    size `sz` in the `sizes` parameter. Recall that the current splitter is considered
    invalidated after calling the `split` or `psplit` method, so we are allowed to
    mutate its `i` field:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that we never copy the string underlying the splitter; instead, we update
    the indices that mark the beginning and the end of the splitter.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now completed our `ParString` class; we can use it to execute parallel
    operations on strings. We can also use it to count the number of uppercase characters
    in the string as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: On our machine, the sequential `foldLeft` call takes 57 milliseconds, and the
    parallel `aggregate` call takes 19 milliseconds. This is a good indication that
    we have implemented parallel strings efficiently.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Combiners
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Collection methods in the Scala standard library are divided into two major
    groups: **accessor** and **transformer** methods. Accessor methods, such as `foldLeft`,
    `find`, or `exists`, return a single value from the collection. By contrast, transformer
    methods, such as `map`, `filter`, or `groupBy`, create new collections and return
    them as results.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'To generically implement transformer operations, the Scala collection framework
    uses an abstraction called a **builder**, which has roughly the following interface:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Here, the `Repr` type is of a collection that a specific builder can produce,
    and `T` is the type of its elements. A builder is used by repetitively calling
    its `+=` method to add more elements, and eventually calling the `result` method
    to obtain the collection. After the `result` method is called, the contents of
    the builder are undefined. The `clear` method can be used to reset the state of
    the builder.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Every collection defines a custom builder used in various transformer operations.
    For example, the `filter` operation is defined in the `Traversable` trait, roughly
    as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the preceding example, the `filter` implementation relies on the abstract
    `newBuilder` method, which is implemented in subclasses of the `Traversable` trait.
    This design allows defining all collection methods once, and only provide the
    `foreach` method (or the iterator) and the `newBuilder` method when declaring
    a new collection type.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '**Combiners** are a parallel counterpart of standard builders, and are represented
    with the `Combiner[T, Repr]` type, which subtypes the `Builder[T, Repr]` type:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `size` method is self-explanatory. The `combine` method takes another combiner
    called `that`, and produces a third combiner that contains the elements of the
    `this` and `that` combiners. After the `combine` method returns, the contents
    of both the `this` and `that` combiners are undefined, and should not be used
    again. This constraint allows reusing the `this` or `that` combiner object as
    the resulting combiner. Importantly, if that combiner is the same runtime object
    as the `this` combiner, the `combine` method should just return the `this` combiner.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to implement a custom combiner, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '**Merging**: Some data structures have an efficient merge operation that can
    be used to implement the `combine` method.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Two-phase evaluation**: Here, elements are first partially sorted into buckets
    that can be efficiently concatenated, and placed into the final data structure
    once it is allocated.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent data structure**: The `+=` method is implemented by modifying
    a concurrent data structure shared between different combiners, and the `combine`
    method does not do anything.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most data structures do not have an efficient merge operation, so we usually
    have to use two-phase evaluation in the combiner implementation. In the following
    example, we implement the combiners for parallel strings using two-phase evaluation.
    The `ParStringCombiner` class contains a resizable array, called `chunks`, containing
    `StringBuilder` objects. Invoking the `+=` method adds a character to the rightmost
    `StringBuilder` object in this array:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `combine` method takes the `StringBuilder` objects of the `that` combiner,
    and adds them to the `chunks` array of the `this` combiner. It then returns a
    reference to the `this` combiner:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, the `result` method allocates a new `StringBuilder` object and adds
    the characters from all the chunks into the resulting string:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We test the performance of the parallel `filter` method with the following
    snippet:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Running this snippet on our machine takes 11 milliseconds for the sequential
    version, and 6 milliseconds for the parallel one.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use parallel collections to improve program
    performance. We have seen that sequential operations on large collections can
    be easily parallelized and learned the difference between parallelizable and non-parallelizable
    collections. We investigated how mutability and side effects impact correctness
    and determinism of parallel operations and saw the importance of using associative
    operators for parallel operations. Finally, we studied how to implement our custom
    parallel collection class.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: We also found, however, that tuning program performance is tricky. Effects such
    as memory contention, garbage collection, and dynamic compilation may impact the
    performance of the program in ways that are hard to predict by looking at the
    source code. Throughout this section, we urged you to confirm suspicions and claims
    about program performance by experimentally validating them. Understanding the
    performance characteristics of your program is the first step toward optimizing
    it.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Even when you are sure that parallel collections improve program performance,
    you should think twice before using them. Donald Knuth once coined the phrase
    *Premature optimization is the root of all evil*. It is neither desirable nor
    necessary to use parallel collections wherever possible. In some cases, parallel
    collections give negligible or no increase in speed. In other situations, they
    could be speeding up a part of the program that is not the real bottleneck. Before
    using parallel collections, make sure to investigate which part of the program
    takes the most time, and whether it is worth parallelizing. The only practical
    way of doing so is by correctly measuring the running time of the parts of your
    application. In [Chapter 9](ch09.html "Chapter 9. Concurrency in Practice"), *Concurrency
    in Practice*, we will introduce a framework called ScalaMeter, which offers a
    more robust way to measure program performance than what we saw in this chapter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: This chapter briefly introduced concepts such as Random Access Memory, cache
    lines, and the MESI protocol. If you would like to learn more about this, you
    should read the article, *What Every Programmer Should Know About Memory*, by
    Ulrich Drepper. To gain a more in-depth knowledge about the Scala collections
    hierarchy, we recommend you to search for the document entitled *The Architecture
    of Scala Collections*, by Martin Odersky and Lex Spoon, or the paper *Fighting
    Bit Rot with Types*, by Martin Odersky and Adriaan Moors. To understand how data-parallel
    frameworks work under the hood, consider reading the doctoral thesis entitled
    *Data Structures and Algorithms for Data-Parallel Computing in a Managed Runtime*,
    by Aleksandar Prokopec.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've assumed that all the collection elements are available when the
    data-parallel operation starts. A collection does not change its contents during
    the data-parallel operation. This makes parallel collections ideal in situations
    where we already have the dataset, and we want to process it in bulk. In other
    applications, data elements are not immediately available, but arrive asynchronously.
    In the next chapter, we will learn about an abstraction called an event stream,
    which is used when asynchronous computations produce multiple intermediate results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following exercises, you will use data-parallel collections in several
    concrete parallel collection use cases, and implement custom parallel collections.
    In all examples, a special emphasis is put on measuring the performance gains
    from parallelization. Even when it is not asked for explicitly, you should ensure
    that your program is not only correct but also faster than a corresponding sequential
    program:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Measure the average running time for allocating a simple object on the JVM.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count the occurrences of the whitespace character in a randomly generated string,
    where the probability of a whitespace at each position is determined by a `p`
    parameter. Use the parallel `foreach` method. Plot a graph that correlates the
    running time of this operation with the `p` parameter.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a program that renders the Mandelbrot set in parallel.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a program that simulates a cellular automaton in parallel.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a parallel *Barnes-Hut N-body* simulation algorithm.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain how you can improve the performance of the `result` method in the `ParStringCombiner`
    class, as shown in this chapter. Can you parallelize this method?
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a custom splitter for the binary heap data structure.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The binomial heap, described in the doctoral thesis of Chris Okasaki entitled
    *Purely Functional Data Structures*, is an immutable data structure that efficiently
    implements a priority queue with four basic operations: insert the element, find
    the smallest element, remove the smallest element, and merge two binomial heaps:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Implement the `BinomialHeap` class. Then, implement splitters and combiners
    for the binomial heap, and override the `par` operation.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implement the `Combiner` trait for the Red-Black tree from the Scala standard
    library. Use it to provide a parallel version of the `SortedSet` trait.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a `parallelBalanceParentheses` method, which returns `true` if the
    parentheses in a string are properly balanced, or `false` otherwise. Parentheses
    are balanced if, going from left to right, the count of left parenthesis occurrences
    is always larger than, or equal to, the count of right parenthesis occurrences,
    and the total count of the left parentheses is equal to the total count of the
    right parentheses. For example, string `0(1)(2(3))4` is balanced, but strings
    `0)2(1(3)` and `0((1)2` are not. You should use the `aggregate` method.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
