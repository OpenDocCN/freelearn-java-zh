- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Core Architectural Design Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to architect data engineering solutions
    for both batch-based and real-time processing using specific use cases. However,
    we haven’t discussed the various options available concerning architectural design
    patterns for batch and real-time stream processing engines.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about a few commonly used architectural patterns
    for data engineering problems. We will start by learning about a few common patterns
    in batch-based data processing and common scenarios where they are used. Then,
    we will learn about various streaming-based processing patterns in modern data
    architectures and how they can help solve business problems. We will also discuss
    the two famous hybrid data architectural patterns. Finally, we will learn about
    various serverless data ingestion patterns commonly used in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Core batch processing patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core stream processing patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid data processing patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless patterns for data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core batch processing patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at a few commonly used data engineering patterns
    to solve batch processing problems. Although there can be many variations of the
    implementation, these patterns are generic, irrespective of the technologies used
    to implement the patterns. In the following sections, we’ll discuss the commonly
    used batch processing patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The staged Collect-Process-Store pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **staged Collect-Process-Store pattern** is the most common batch processing
    pattern. It is also commonly known as the **Extract-Transform-Load** (**ETL**)
    pattern in data engineering. This architectural pattern is used to ingest data
    and store it as information. The following diagram depicts this architectural
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The staged Collect-Process-Store pattern ](img/B17084_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – The staged Collect-Process-Store pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'We can break this pattern into a series of stages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In this architectural pattern, one or more data sources are extracted and kept
    in a form of data storage called a raw zone or landing zone. The landing zone
    data is often raw data, which consists of noise such as extra spaces, junk characters,
    important fields missing, and so on. The extraction or collection job has the
    responsibility to extract and store the data in the raw zone. The data storage
    that’s used for the landing zone can vary from a filesystem, the **Hadoop Distributed
    File System** (**HDFS**), an S3 bucket, or some relational database based on the
    use case and the platform chosen to solve the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The processing jobs read the data from the raw zone and perform a series of
    transformations on the data, such as data cleansing, data standardization, and
    data validation. The job stores its output in the intermediate processing zone(s).
    There can be one or more transformation jobs, as well as multiple intermediate
    processing zones, based on the project and technology. Sometimes, processing jobs
    fetch related information from the intermediate data zone to enrich the processed
    data. In such a scenario, it reads the data from the intermediate processing zone
    or any external reference database. The final intermediate processing zone contains
    the data, which is cleansed, transformed, validated, and well organized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fetch and load process picks up the transformed data and loads it into the
    sorted dataset layer. The sorted dataset layer contains clean and usable data
    in a specific format that can easily be consumed by downstream applications for
    data analytics, reference, and so on. The sorted dataset layer is also popularly
    known as the **Organized Data Layer** (**ODL**). There is no hard and fast rule
    regarding the type of database or data store used for the sorted dataset layer.
    However, based on whether the sorted data will be used for **Online Transaction
    Processing** (**OLTP**) or **Online Analytical Processing** (**OLAP**), a database
    is chosen. Generally, this pattern is used to ingest and store data for OLAP purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The jobs for this architectural pattern typically run periodically based on
    a predetermined schedule, such as once daily or once weekly, or every Friday and
    Wednesday at 8 P.M. One of the advantages of this pattern is that it ingests the
    data and processes in a series of stages. The output of each stage is stored in
    an intermediate processing zone, and the next stage fetches data from the output
    of the previous stage. This staged architecture makes the design loosely coupled.
    Often, in production, the data processing job fails. In such a situation, we don’t
    need to rerun the full ingestion pipeline; instead, we can restart the pipeline
    from the job that failed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at a real-world use case where this pattern will be a good fit.
    A health insurance firm receives tons of insurance claims every day. To process
    these claims and determine the cost that will be paid by the insurance firm, the
    data needs to be cleansed, enriched, organized, and stored. In such a use case,
    this architectural pattern can be used to ingest different kinds of claims, such
    as medical, dental, and vision, from various sources; then, they can be extracted,
    transformed, and loaded into ODL. Another example implementation of this pattern
    was discussed in [*Chapter 4*](B17084_04.xhtml#_idTextAnchor062), *ETL Data Load
    – A Batch-Based Solution to Ingesting Data in a Data Warehouse*.
  prefs: []
  type: TYPE_NORMAL
- en: Common file format processing pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose there is a scenario where there are multiple files (say, for example,
    25 source files) for the data sources and the structure of these sources are quite
    different from each other. Now, the question is, *Can the staged collect-process-store
    pattern handle such a use case?* Yes, it can. But is it optimized to do so? No,
    it’s not. The problem is that for all 25 different kinds of source files, we need
    to have a separate set of transformation logic written to process and store them
    into a sorted dataset. We may require 25 separate data pipelines to ingest the
    data. This not only increases development effort but also increases analysis and
    testing effort. Also, we may need to fine-tune all the jobs in all 25 data pipelines.
    The **common file format processing pattern** is well suited to overcome such
    problems. The following diagram depicts how the common file format processing
    pattern works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – The common file format processing pattern ](img/B17084_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – The common file format processing pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern is divided into the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: In this architectural pattern, multiple source files with distinctively different
    source file structures are stored or sent to the landing zone from the sources.
    The landing zone can be a filesystem, a NAS mount, an SFTP location, or an HDFS
    location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A common file format conversion process runs, which takes the different incoming
    source files and converts them into a uniform structure called a **common file
    format**. The job or pipeline that runs to do this conversion is lightweight.
    It should not do cleansing or business transformation in this layer. The common
    file format conversion process writes its output in the common file format zone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that all the files are in the same format or structure, a single set of
    process and load jobs can run on top of those files present in the common file
    format zone. The process and load process can be a single job or a series of jobs
    that writes the final organized and sorted data into ODL or the sorted dataset
    layer. The process and load job may write its intermediate result to temporary
    storage zones if required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s look at a real-world scenario. A credit card company wants to generate
    and provide offers for its customers based on their buying and spending patterns,
    as well as a set of complex rules. However, transaction data can be received from
    various kinds of sources, which includes web-based payment gateways, physical
    transaction gateways, payment apps such as PayPal and Cash App, foreign payment
    gateways, and various other similar apps. However, the transaction files that
    are received from all these sources are in different formats. One option is to
    create a separate set of transformation mappings for each source and apply the
    rules differently. However, that will cause a lot of development time and costs,
    as well as a maintenance challenge. In such a scenario, the common file format
    processing pattern can be used to convert all transaction files coming from different
    source systems into a common file format. Then, a single set of rule engine jobs
    can process transactions received from different sources.
  prefs: []
  type: TYPE_NORMAL
- en: The Extract-Load-Transform pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Previously in this book, we learned about the classical ETL-based pattern,
    where we extract the data first, transform and process the data, and finally store
    it in the final data store. However, with modern processing capabilities and the
    scalability that the cloud offers, we have seen many **Massive Parallel Processing**
    (**MPP**) databases such as Snowflake, Redshift, and Google’s Big Query becoming
    popular. These MPP databases have enabled a new pattern of data ingestion where
    we extract and load the data into these MPP databases first and then process the
    data. This pattern is commonly known as the **Extract-Load-Transform** (**ELT**)
    pattern or the Collect-Store-Process pattern. This pattern is useful for building
    high-performing data warehouses that contain a huge amount of data. The following
    diagram provides an overview of the ELT pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – The Extract-Load-Transform (ELT) pattern ](img/B17084_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – The Extract-Load-Transform (ELT) pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram depicts the typical flow of an ELT pattern. This can
    be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, raw data is extracted and loaded into the
    MPP database. This data is stored in the staging zone of the MPP database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, using MPP queries and a transformation pipeline, the data is transformed
    into the final set of tables. These final tables are exposed as a data warehouse.
    For security purposes, sometimes, views are created and exposed as a data warehouse
    on top of the tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Again, let’s look at an example of how this pattern is used in the industry.
    As the customer experience continues to rise, businesses face a gap between the
    data needed to meet customer expectations and the ability to deliver using the
    current data management practice. Customer 360 involves building a complete and
    accurate repository of all the structured and unstructured data across the organization
    related to the customer. It is an aggregation of all customer data into a single
    unified location so that it can be queried and used for analytics to improve customer
    experience. To build Customer 360 solutions, we can leverage the power of MPP
    databases to create a single unified Customer 360 data warehouse. An example of
    a Customer 360 design using Snowflake on AWS is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Example of an ELT pattern on AWS ](img/B17084_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Example of an ELT pattern on AWS
  prefs: []
  type: TYPE_NORMAL
- en: Here, data from cloud storage, event streams, and third-party sources all land
    in the staging area of Snowflake (an MPP database). Then, using Snowflake pipelines,
    data is cleansed, transformed, and enriched and is stored in the final tables
    to be consumed by the organization as the centralized enterprise data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: The compaction pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data warehouses are not only built on MPP databases. For big data needs, a lot
    of the time, they are built on top of HDFS using Hive as the querying engine.
    However, in modern pipelines, a lot of data is dumped in the landing zone by real-time
    processing engines such as Kafka or Pulsar. Although the use case needs our processing
    jobs to run a few times a day or once daily, the files are landed when any records
    come in. This creates a different kind of problem. Due to the scenario described
    earlier, too many small files containing few records are created. HDFS is not
    designed to work with small files, especially if it is significantly smaller than
    the HDFS block size; for example, 128 MB. HDFS works better if a smaller number
    of huge files are stored instead of a huge number of small files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, as the small files grow, the query performance reduces, and eventually,
    Hive is unable to query those records. To overcome this problem, a pattern is
    commonly used. This is called the compaction pattern. The following diagram provides
    an overview of the compaction pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The compaction pattern ](img/B17084_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The compaction pattern
  prefs: []
  type: TYPE_NORMAL
- en: In this architectural pattern, the small files are stored in the landing zone.
    A batch-based periodical job runs and compacts those small files to create a single
    large file. In between, it uses the status and state storage to store job audit
    information. It is also used to store state information that may be used by subsequent
    compaction jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The staged report generation pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have discussed multiple patterns to show how data is ingested and stored
    as a sorted dataset or in a data warehouse. This pattern, on the other hand, focuses
    on running data analytics jobs and generating report(s) from the **ODL** or data
    warehouse. The following diagram shows the generic architecture of this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – The staged report generation pattern ](img/B17084_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – The staged report generation pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'The staged report generation pattern consists of primarily two stages and an
    auxiliary step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Report generation stage**: Various analytics jobs run on top of the sorted
    data or organized data layer. Such analytics jobs may even run on the data stored
    in the data warehouse. These jobs then save the report of the analysis in a reporting
    database. A reporting database can be a relational database, a NoSQL database,
    or a search engine such as Elasticsearch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Summary generation stage**: The summary reporting jobs fetch data from the
    reporting database and report the summary data in the summary database. Summary
    databases are usually relational databases, data warehouses, or search engines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using exporters and connectors, the data present in either the reporting database
    or the summary database can be visualized using BI tools or used for data science
    and analytics purposes, or simply used to extract flat files containing reports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s look at a real-world scenario where this pattern is suitable. Let’s
    say that a company has an on-premises data center. Every day, monitoring and resolution
    logs are generated for all the servers and storage, backup storage, and networking
    devices present in the data center. This data is ingested and stored in a data
    warehouse that contains daily, weekly, and monthly outage and resolution details.
    Using this data warehouse, the organization wants to generate various reports
    for the average SLA for various kinds of incidents, the performance or KPI ratio
    before and after the resolutions, and the team-wise velocity of closing incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the company wants to generate a summary of all incidents on a weekly,
    monthly, and quarterly basis. This use case is well-suited for using this pattern.
    In this use case, we can generate all the reports and store them in a reporting
    database and generate the summary reports to the summary database. Both general
    reports and summary reports can be visualized using BI tools such as Tableau by
    pulling the data from the reporting databases using proper connectors.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about a few popular batch processing architectural
    patterns and a few real-world scenarios that can be applied. In the next section,
    we will cover a few common patterns used for real-time stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: Core stream processing patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned about a few commonly used batch processing
    patterns. In this section, we will discuss various stream processing patterns.
    Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: The outbox pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With modern data engineering, monolithic applications have been replaced by
    a series of microservices application working in tandem. Also, it is worth noting
    that microservices usually don’t share their databases with other microservices.
    The database session commits and interservice communications should be atomic
    and in real time to avoid inconsistencies and bugs. Here, the outbox pattern comes
    in handy. The following diagram shows the generic architecture of the outbox pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – The outbox pattern ](img/B17084_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – The outbox pattern
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, a microservice (here, **Service 1**) writes a transaction to
    not only the required table where online reads and writes happen (denoted in the
    diagram as **Online Table**) but also to an **Outbox Table**, whose structure
    is where messages to the message broker should be published. Just like the physical
    trays on office desks that once held outgoing letters and documents, the outbox
    pattern uses an **Outbox Table** to send messages to the message broker. A **Change
    Data Capture** (**CDC**) publisher picks the CDC events from the **Outbox Table**
    area and publishes them to our **Message Broker**. Downstream services that need
    data from Service 1 consume the data from the topic.
  prefs: []
  type: TYPE_NORMAL
- en: The saga pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The saga pattern is a design pattern that is used to manage and handle distributed
    transactions across multiple applications or services successfully. In a real-world
    scenario, a single business transaction can never be done with one application
    or backend service. Usually, multiple applications work in tandem to complete
    a successful business transaction. However, we need to have an asynchronous, reliable,
    and scalable way to communicate between these systems. Each business transaction
    that spans multiple services is called a saga. The pattern to implement such a
    transaction is called the saga pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the saga pattern, let’s take a look at an e-commerce application.
    The following diagram shows the workflow of a simplified ordering system in an
    e-commerce application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – A simplified e-commerce ordering system ](img/B17084_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – A simplified e-commerce ordering system
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, an ordering system consists of multiple services, each of which
    has its own set of functions to perform. Essentially, there are three services:
    the ordering service, the credit management service, and the payment service.
    For a successful ordering transaction, the ordering service receives the order.
    If the order is received successfully, it goes to the credit management service,
    which checks the credit card’s balance and validates the card. If the credit check
    is successful, the system uses the payment service to request payment. If the
    payment goes through successfully, the order is marked as accepted. If it fails
    at any stage, the transaction is aborted, and the order gets rejected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how the saga pattern is implemented in this situation. Here
    inter-service communication is decoupled and made asynchronous by introducing
    a message broker platform to exchange messages between them. The following diagram
    shows how the saga pattern is used to implement the ordering system for an e-commerce
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – The saga pattern applied to implement an ordering system ](img/B17084_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – The saga pattern applied to implement an ordering system
  prefs: []
  type: TYPE_NORMAL
- en: Here, the saga pattern is applied to the saga transaction of placing an order.
    Here, the ordering service stores the orders in a local database. A CDC publisher
    is used to publish the messages containing the order to **Topic 2** in the streaming
    platform. Data sent to Topic 2 is consumed by the credit management service to
    do the credit check functionality (marked as flow *1* in the preceding diagram).
    The output of the credit check functionality is sent to a local database. The
    message containing the credit check result is sent from the local database to
    **Topic 1**. The ordering service consumes and stores the output for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: If the credit check report is positive, a payment request event is published
    in **Topic 3** using the CDC processor (depicted as flow *3* in the preceding
    diagram). The event that’s published in flow *3* is picked up by the payment service
    and requests payment. The result of the payment request is saved in the local
    payment database. The CDC publisher from the payment database produces the payment
    output to **Topic 4**, which is denoted as flow *4* in the preceding diagram.
    Using the information that’s been shared over **Topic 4**, the ordering service
    determines whether the order was placed or whether it was rejected. One of the
    interesting things that you can see is that each step of the saga pattern follows
    the outbox pattern, as described earlier. We can say a series of outbox patterns
    are knitted in a certain manner to create the saga pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The choreography pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This pattern is used specifically where each component independently takes
    part in the decision-making process to complete a business transaction. All these
    independent components talk to a centralized orchestrator application or system.
    Just like how in choreography, the choreography pattern enables all independent
    dancers to perform separately and create a wonderfully synchronized show, so an
    orchestrator orchestrates decentralized decision-making components to complete
    a business transaction. This is the reason that this pattern is called the choreography
    pattern. The following diagram provides an overview of the choreography pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – The choreography pattern ](img/B17084_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – The choreography pattern
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, events from the client are streamed to the topic. Each event
    contains a specific message header or a message key. Based on the type of message
    header value or key value, each consuming application can filter and process the
    messages required by that application. Once it processes the event, it generates
    a result event to the same topic but with a different key or header value. The
    client consumes all the resulting events to create the final output or decision.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is useful when you have scenarios where applications may be frequently
    added, removed, or updated or there is a bottleneck in the centralized orchestration
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at a real-world use case where this pattern may come in handy.
    A service provider receives different events whenever a client does a recharge.
    In a single recharge, the client can buy different bundles, such as a top-up bundle,
    data bundle, and so on. Each bundle adds a message header to the event. Different
    client applications provide customized offers for each kind of bundle. This use
    case is suitable for the choreography pattern. Suppose an event comes with both
    the top-up and data bundles; this will add two pieces of header information, so
    there will be two consuming applications based on the type of bundle that will
    be consumed; its own set of offers will be generated and sent back to the client
    using the topic. It makes sense to use the choreography pattern here as the type
    of bundles are dynamic, which can vary year-to-year and season-to-season. So,
    the consuming applications may be frequently added or removed from the ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The Command Query Responsibility Segregation (CQRS) pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a very famous pattern, where the read responsibility and write responsibility
    is segregated out. This means the data is written to a different data store and
    read from another data store. While the write data store is optimized for fast
    writes, the read data store is optimized for fast data reads. The following diagram
    shows how the CQRS pattern works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – The CQRS pattern ](img/B17084_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – The CQRS pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram depicts how the CQRS pattern works. The flow of this
    pattern is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the producer or publisher writes the event to a topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a streaming application, this record is streamed into the read database.
    While the topic is optimized for fast data writes, the read database is optimized
    for high-performance data reads. This kind of pattern is very useful for scenarios
    where we need to have a high write as well as high read speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, for a big e-commerce website such as Amazon, the traffic increases
    heavily on Amazon sale days. In this scenario, there is the possibility of a high
    number of writes as well as a high number of searches. In this case, various sources
    such as mobile apps, web portals, and so on will accept orders and update the
    inventory. Also, offers and discounts are changed hourly using the Amazon Big
    Day sale event management portal by sellers and Amazon representatives. Although
    there will be a high number of reads and writes, customers expect subsecond response
    times regarding search results. This can be achieved by maintaining separate write
    and search databases.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, this use case is ideal for the CQRS pattern. Here, when a customer searches,
    data is fetched from the search database, and when the customer orders or adds
    something to the cart, it is written to the write database. The information available
    in the write database will be streamed in real time to a search database such
    as Elasticsearch or AWS OpenSearch. So, users who are searching for products and
    discounts should get the search results in a fraction of a second.
  prefs: []
  type: TYPE_NORMAL
- en: The strangler fig pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The strangler fig pattern derives its name from a species of tropical fig plants
    that grow around their host trees, slowly strangling the host tree so that it
    dies. This pattern was first proposed by Martin Fowler. Although the basic pattern
    may be implemented in different ways, the streaming pipeline gives us an indigenous
    way to use this pattern. To understand this pattern, let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose there is a monolithic application that consists of three modules –
    A, B, and C. A, B, and C read and write data to the database. Initially, the architecture
    looked as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Initial state of the monolithic application ](img/B17084_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Initial state of the monolithic application
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, all the modules have double-ended arrows, denoting both reads
    and writes are happening. Now, using the strangler fig pattern, we can convert
    this monolithic legacy application into a microservices-based application by slowly
    migrating the individual modules as separate microservices – one at a time. The
    following diagram shows that **Module A** is being moved to the microservices
    pattern from the monolithic app:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Module A replaced with Microservice A ](img/B17084_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Module A replaced with Microservice A
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, **Microservice A** (which has successfully replaced **Module
    A**) reads and writes data to an event stream. This event stream is, in turn,
    connected to the database using an event source or sink connector. Slowly, the
    monolithic application will be strangled, and the final transformed architecture
    will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – All modules migrated using the strangler fig pattern ](img/B17084_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – All modules migrated using the strangler fig pattern
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, all the modules have been migrated from the monolithic application
    to the federated microservice pattern, allowing the monolithic application to
    retire.
  prefs: []
  type: TYPE_NORMAL
- en: The log stream analytics pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this pattern, we will learn how logs collected across various apps, web
    portals, backend services, and IoT devices are used for analytics and monitoring.
    The following diagram shows a typical log streaming pattern used to facilitate
    log analytics and monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – The log stream analytics pattern ](img/B17084_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – The log stream analytics pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s learn how this pattern works:'
  prefs: []
  type: TYPE_NORMAL
- en: As evident from the preceding diagram, all log events from various IoT devices,
    apps, web portals, and services are streamed into an event stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, using the event sink connector, the events are sent to both a search database
    and a querying database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A search database can be a search engine such as Elasticsearch, AWS OpenSearch,
    Splunk, or Apache Solar. This database facilitates quick searches with complex
    query patterns. It also enables visualization and analytics using the capabilities
    of the search engine. The query database is either an MPP database such as Redshift
    or Snowflake or a query engine such as Athena. A query engine allows users to
    run SQL queries on top of ObjectStores such as S3 objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a sample implementation of this kind of pattern
    in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Example of a log analytics pattern in AWS ](img/B17084_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Example of a log analytics pattern in AWS
  prefs: []
  type: TYPE_NORMAL
- en: Here, log events from various AWS services such as EC2, ECR, EKS, and others
    are streamed to a Kinesis topic using Kinesis Firehose. Kinesis Analytics transformation
    is done and, using Kinesis Firehose, streamed to AWS OpenSearch for search and
    analytics purposes. On the other hand, data gets streamed into S3 from the first
    Kinesis Firehose. Athena tables are created on top of the S3 objects. Athena then
    provides an easy-to-use query interface for batch-based analytic queries to be
    performed on the log data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about various streaming patterns that are popular
    and can be used to solve common data engineering problems. We also looked at a
    few examples and learned when these patterns should be used. Next, we will investigate
    a few popular patterns that are a mix of both batch and stream processing. These
    are known as hybrid data processing patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid data processing patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss two very famous patterns that support both
    batch and real-time processing. Since these patterns support both batch processing
    and stream processing, they are categorized as hybrid patterns. Let’s take a look
    at the most popular hybrid architectural patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The Lambda architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s understand the need for Lambda architecture. In distributed computing,
    the CAP theorem states that any distributed data can guarantee only two out of
    the three features of the data – that is, consistency, availability, and partition
    tolerance. However, Nathan Marz proposed a new pattern in 2011 that made it possible
    to have all three characteristics present in a distributed data store. This pattern
    is called the Lambda pattern. The Lambda architecture consists of three layers,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch layer**: This layer is responsible for batch processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed layer**: This layer is responsible for real-time processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving layer**: This layer serves as the unified serving layer where querying
    can be done by downstream applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows an overview of the Lambda architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – The Lambda architecture ](img/B17084_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – The Lambda architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the Lambda architecture, the input data or the source data is written to
    the master data store present in the batch layer, as well as the event streams
    present in the speed layer. The master data store may be a relational or NoSQL
    database or a filesystem such as HDFS. Batch processing jobs run on top of this
    data store to do any data processing, as well as to load the data into batch views
    present in the serving layer. Events written in the event stream are picked up,
    processed, and loaded into the real-time view by stream processing jobs (as shown
    in *Figure 7.17*). Queries can be made separately to query batch views and real-time
    views, or they can be queried simultaneously on both views to view the results.
    Batch views are mainly for historical data, while real-time views are for Delta
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Although it solves the problem of eventual consistency as queries can combine
    data from both real-time views and batch-based views, it comes with a few shortcomings.
    One of the major shortcomings is that we must maintain two different workflows
    – one for the batch layer and another for the speed layer. Since, in a lot of
    scenarios, the technology to implement a streaming application is quite different
    from a batch-based application, we must maintain two different source codes. Also,
    debugging and monitoring for both batch and stream processing systems becomes
    an overhead. We will discuss how to overcome these challenges in the next pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The Kappa architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the reasons the Lambda architecture is widely accepted is because it
    can overcome the limitation of the CAP theorem and enable more use of stream processing
    across the industry. Before the Lambda architecture, businesses were skeptical
    to use stream processing as they feared losing messages during real-time processing.
    However, this assumption is not true with modern distributed streaming platforms
    such as Kafka and Pulsar. Let’s take a look at how the Kappa architecture provides
    a simpler alternative to the Lambda architecture. The following diagram depicts
    the Kappa architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – The Kappa architecture ](img/B17084_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – The Kappa architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the Kappa architecture, the idea is not to use two different flows – one
    for batch and one for streaming. Instead, it proposes all processing should be
    done using a stream processing engine. This means that both batch-based workloads
    and stream-based workloads can be handled by a single pipeline. Here, input data
    or source data is written in a special event stream. This event stream is an immutable
    append-only transaction log. Since it is an append-only log, it has fast writing
    capabilities. To read data, we can read from the offset where the data read stopped
    earlier. In addition to this last read offset, it should support replayability,
    which means we can read from the first message as well.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are talking about distributed computing, this transaction log will
    be partitioned, which will improve the read and write performance as well. Stream
    processing jobs read the events, process them, and write the output to unified
    views (containing both batch and real-time data). One question that comes to mind
    is, *How does this kind of flow support high-volume batch loads?* Huge volumes
    of data are also sent to the transaction log, which is then picked up by stream
    processing jobs. The output is stored in the view present in the serving layer.
    To process such a high-volume event stream, we need to do more parallelism by
    increasing the number of partitions in the event stream.
  prefs: []
  type: TYPE_NORMAL
- en: This event stream log should have retention capabilities. Also, consumers should
    be able to replay the stream using either the event time or event offset. Each
    event has an offset and an event timestamp. The replayability feature allows consumers
    to re-read already fetched data by setting the event offset or event timestamp
    to an older value.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed commonly used batch-based, real-time, and hybrid architectural
    patterns. In the penultimate section, we will quickly look at a few common serverless
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless patterns for data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by answering the question, *What is serverless computing?* Serverless
    computing is a cloud execution model in which a cloud provider takes care of allocating
    resources such as storage and compute based on demand while taking care of the
    servers on behalf of customers. Serverless computing removes the burden of maintaining
    and managing servers and resources associated with it. Here, the customers of
    serverless computing don’t care how and where the jobs or applications are running.
    They just focus on the business logic and let the cloud provider take care of
    managing the resources for running and executing that code. A few examples of
    serverless computing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Lambda Function or Azure Function**: This is used to run any application
    or service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Glue**: This is used to run big data-based ETL jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Kinesis**: This is a serverless event streaming and analytics platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although there are many useful serverless patterns, in this section, we will
    discuss the two most relevant patterns that can help us architect data engineering
    solutions. The following are the serverless patterns that we will be discussing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The event-driven trigger pattern**: This is a very common pattern that’s
    used in cloud architectures. In this pattern, upon creating or updating any file
    in object storage such as an S3 bucket, a serverless function gets triggered.
    The following diagram provides an overview of this pattern:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.19 – The event-driven trigger pattern ](img/B17084_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – The event-driven trigger pattern
  prefs: []
  type: TYPE_NORMAL
- en: In this pattern, any change to an object, such as it being created or deleted,
    in object storage can trigger a serverless function. This serverless function
    may either directly process the data or be used to trigger a big data job. Serverless
    functions such as AWS Lambda and Azure Function can set triggers that can trigger
    them. For example, a Lambda function can be configured to have an S3 trigger from
    Bucket1 for any new object being created or updated in Bucket1\. The triggered
    Lambda function can, in turn, trigger an EMR job or a serverless Glue job, which
    transforms and processes the necessary data and writes the final output to a data
    store. Alternatively, the Lambda function can do some data processing and store
    the output result in the final data store. The final data store can be a SQL database,
    NoSQL database, MPP database, or object storage such as AWS S3.
  prefs: []
  type: TYPE_NORMAL
- en: A real-world scenario for using this pattern and its solution was explained
    in detail in [*Chapter 5*](B17084_05.xhtml#_idTextAnchor074), *Architecting a
    Batch Processing Pipeline*.
  prefs: []
  type: TYPE_NORMAL
- en: '**The serverless real-time pattern**: This is an oversimplistic serverless
    pattern that is quite popular for data ingestion in the cloud. An overview of
    this pattern can be seen in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.20 – The serverless real-time pattern ](img/B17084_07_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – The serverless real-time pattern
  prefs: []
  type: TYPE_NORMAL
- en: In the serverless real-time pattern, event or data streaming, as well as data
    processing, happens using serverless services in the cloud. Events, logs, and
    messages from different source systems publish the events to a serverless data
    streaming platform such as AWS Kinesis. The data stream triggers one or a series
    of serverless functions chained one after another to do the data processing on
    the fly. Once the data has been processed, it is written back to a final data
    store. The final data store can be SQL, NoSQL, an MPP database, object storage,
    or a search engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'A real-world example where this pattern may be used is in a real-time fraud
    detection system for credit card usage. The following diagram depicts a sample
    solution of fraud detection in AWS using this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – A sample implementation of the serverless real-time pattern
    in AWS ](img/B17084_07_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – A sample implementation of the serverless real-time pattern in
    AWS
  prefs: []
  type: TYPE_NORMAL
- en: Here, API Gateway streams real-time credit card transactions directly into Kinesis
    Data Streams (a serverless data streaming platform). The transaction events written
    in Kinesis Data Streams trigger the Lambda function to perform fraud and anomaly
    detection on the event. The Lambda function makes use of AWS SageMaker, which,
    in turn, uses the already stored data science models stored in S3 to determine
    fraud and anomalies in the transaction. The output is then passed to Kinesis Data
    Firehose, which captures the result from the Lambda function and stores it in
    an S3 bucket. This S3 bucket contains the results in real time. We can use a service
    such as Amazon QuickSight to visualize the results and take any action if required.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have discussed what serverless computing is and discussed two
    highly used patterns for serverless computing for data ingestion. Now, let’s summarize
    what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by discussing various popular batch processing patterns.
    We covered five commonly used patterns to solve batch processing problems. We
    also looked at examples of those patterns and real-world scenarios where such
    patterns are used. Then, we looked at five popular patterns available to architect
    stream processing pipelines and how they are used to solve real-world problems
    in data engineering. Next, we learned about the Lambda and Kappa architectures
    and how they are useful for both batch and stream processing. Finally, we learned
    what serverless architecture is and looked at two popular serverless architectures
    that are used to solve many data engineering problems in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we know how to implement batch and streaming solutions, as well
    as have a fair idea of different data engineering patterns that are commonly used
    across the industry. Now, it is time to put some amount of security and data governance
    into our solutions. In the next chapter, we will discuss various data governance
    techniques and tools. We will also cover how and why data security needs to be
    applied to data engineering solutions.
  prefs: []
  type: TYPE_NORMAL
