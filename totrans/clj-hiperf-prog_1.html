<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Performance by Design"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Performance by Design</h1></div></div></div><p>Clojure<a class="indexterm" id="id0"/> is a safe, functional programming language that brings great power and simplicity to the user. Clojure is also dynamically and strongly typed, and has very good performance characteristics. Naturally, every activity performed on a computer has an associated cost. What constitutes acceptable performance varies from one use-case and workload to another. In today's world, performance is even the determining factor for several kinds of applications. We will discuss Clojure (which runs on the <a class="indexterm" id="id1"/>
<span class="strong"><strong>JVM</strong></span> (<span class="strong"><strong>Java Virtual Machine</strong></span>)), and its runtime environment in the light of performance, which is the goal of this book.</p><p>The performance of Clojure applications depend on various factors. For a given application, understanding its use cases, design and implementation, algorithms, resource requirements and alignment with the hardware, and the underlying software capabilities is essential. In this chapter, we will study the basics of performance analysis, including the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Classifying the performance anticipations by the use cases types</li><li class="listitem" style="list-style-type: disc">Outlining the structured approach to analyze performance</li><li class="listitem" style="list-style-type: disc">A glossary of terms, commonly used to discuss performance aspects</li><li class="listitem" style="list-style-type: disc">The performance numbers that every programmer should know</li></ul></div><div class="section" title="Use case classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Use case classification</h1></div></div></div><p>The performance <a class="indexterm" id="id2"/>requirements and priority vary across the different kinds of use cases. We need to determine what constitutes acceptable performance for the various kinds of use cases. Hence, we classify them to identify their performance model. When it comes to details, there is no sure shot performance recipe for any kind of use case, but it certainly helps to study their general nature. Note that in real life, the use cases listed in this section may overlap with each other.</p><div class="section" title="The user-facing software"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec06"/>The user-facing software</h2></div></div></div><p>The <a class="indexterm" id="id3"/>performance of user-facing applications is <a class="indexterm" id="id4"/>strongly linked to the user's anticipation. Having a difference of a good number of milliseconds may not be perceptible for the user but at the same time, a wait of more than a few seconds may not be taken kindly. One important element in normalizing anticipation is to engage the user by providing duration-based feedback. A good idea to deal with such a scenario would be to start the task asynchronously in the background, and poll it from the UI layer to generate a duration-based feedback for the user. Another way could be to incrementally render the results to the user to even out the anticipation.</p><p>Anticipation is not the only factor in user facing performance. Common techniques like staging or precomputation of data, and other general optimization techniques can go a long way to improve the user experience with respect to performance. Bear in mind that all kinds of user facing interfaces fall into this use case category—the Web, mobile web, GUI, command line, touch, voice-operated, gesture...you name it.</p></div><div class="section" title="Computational and data-processing tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Computational and data-processing tasks</h2></div></div></div><p>Non-trivial compute<a class="indexterm" id="id5"/> intensive tasks <a class="indexterm" id="id6"/>demand a proportional amount of computational resources. All of the CPU, cache, memory, efficiency and the parallelizability of the computation algorithms would be involved in determining the performance. When the computation is combined with distribution over a network or reading from/staging to disk, I/O bound factors come into play. This class of workloads can be further subclassified into more specific use cases.</p><div class="section" title="A CPU bound computation"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec02"/>A CPU bound computation</h3></div></div></div><p>A CPU bound computation <a class="indexterm" id="id7"/>is limited by the CPU cycles spent on executing it. Arithmetic <a class="indexterm" id="id8"/>processing in a loop, small matrix multiplication, determining whether a number is a <a class="indexterm" id="id9"/>
<span class="strong"><strong>Mersenne prime</strong></span>, and so on, would be considered CPU bound jobs. If the algorithm complexity is linked to the number of iterations/operations <span class="emphasis"><em>N</em></span>, such as <span class="emphasis"><em>O(N)</em></span>, <span class="emphasis"><em>O(N</em></span>
<span class="emphasis"><em><sup>2</sup>)</em></span> and more, then the performance depends on how big <span class="emphasis"><em>N</em></span> is, and how many CPU cycles each step takes. For parallelizable algorithms, performance of such tasks may be enhanced by assigning multiple CPU cores to the task. On virtual hardware, the performance may be impacted if the CPU cycles are available in bursts.</p></div><div class="section" title="A memory bound task"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec03"/>A memory bound task</h3></div></div></div><p>A memory bound task <a class="indexterm" id="id10"/>is limited by the<a class="indexterm" id="id11"/> availability and bandwidth of the memory. Examples include large text processing, list processing, and more. For example, specifically in Clojure, the <code class="literal">(reduce f (pmap g coll))</code> operation would be memory bound if <code class="literal">coll</code> is a large sequence of big maps, even though we parallelize the operation using <code class="literal">pmap</code> here. Note that higher CPU resources cannot help when memory is the bottleneck, and vice versa. Lack of availability of memory may force you to process smaller chunks of data at a time, even if you have enough CPU resources at your disposal. If the maximum speed of your memory is <span class="emphasis"><em>X</em></span> and your algorithm on single the core accesses the memory at speed <span class="emphasis"><em>X/3</em></span>, the multicore performance of your algorithm cannot exceed three times the current performance, no matter how many CPU cores you assign to it. The memory architecture (for example, SMP and NUMA) contributes to the memory bandwidth in multicore computers. Performance with respect to memory is also subject to page faults.</p></div><div class="section" title="A cache bound task"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec04"/>A cache bound task</h3></div></div></div><p>A task is cache bound when <a class="indexterm" id="id12"/>its speed is constrained by<a class="indexterm" id="id13"/> the amount of cache available. When a task retrieves values from a small number of repeated memory locations, for example a small matrix multiplication, the values may be cached and fetched from there. Note that CPUs (typically) have multiple layers of cache, and the performance will be at its best when the processed data fits in the cache, but the processing will still happen, more slowly, when the data does not fit into the cache. It is possible to make the most of the cache using <a class="indexterm" id="id14"/>
<span class="strong"><strong>cache-oblivious</strong></span> algorithms. A higher number of concurrent cache/memory bound threads than CPU cores is likely to flush the instruction pipeline, as well as the cache at the time of context switch, likely leading to a severely degraded performance.</p></div><div class="section" title="An input/output bound task"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec05"/>An input/output bound task</h3></div></div></div><p>An <span class="strong"><strong>input/output</strong></span> (<span class="strong"><strong>I/O</strong></span>) bound task<a class="indexterm" id="id15"/> would <a class="indexterm" id="id16"/>go faster if the I/O subsystem, that it depends on, goes faster. Disk/storage and network are the most commonly used I/O subsystems in data processing, but it can be serial port, a USB-connected card reader, or any I/O device. An I/O bound task may consume very few CPU cycles. Depending on the speed of the device, connection pooling, data compression, asynchronous handling, application caching, and more, may help in performance. One notable aspect of I/O bound tasks is that performance is usually dependent on the time spent waiting for connection/seek, and the amount of serialization that we do, and hardly on the other resources.</p><p>In practice, many data <a class="indexterm" id="id17"/>processing workloads are usually a combination of <a class="indexterm" id="id18"/>CPU bound, memory bound, cache bound, and I/O bound tasks. The performance of such mixed workloads effectively depends on the even distribution of CPU, cache, memory, and I/O resources over the duration of the operation. A bottleneck situation arises only when one resource gets too busy to make way for another.</p></div></div><div class="section" title="Online transaction processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Online transaction processing</h2></div></div></div><p>
<span class="strong"><strong>Online transaction processing</strong></span> (<span class="strong"><strong>OLTP</strong></span>)<a class="indexterm" id="id19"/> systems process the business transactions on demand. They can sit behind <a class="indexterm" id="id20"/>systems such as a user-facing ATM machine, point-of-sale terminal, a network-connected ticket counter, ERP systems, and more. The OLTP systems are characterized by low latency, availability, and data integrity. They run day-to-day business transactions. Any interruption or outage is likely to have a direct and immediate impact on sales or service. Such systems are expected to be designed for resiliency rather than delayed recovery from failures. When the performance objective is unspecified, you may like to consider graceful degradation as a strategy.</p><p>It is a common mistake to ask the OLTP systems to answer analytical queries, something that they are not optimized for. It is desirable for an informed programmer to know the capability of the system, and suggest design changes as per the requirements.</p></div><div class="section" title="Online analytical processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Online analytical processing</h2></div></div></div><p><span class="strong"><strong>Online analytical processing</strong></span> (<span class="strong"><strong>OLAP</strong></span>)<a class="indexterm" id="id21"/> systems are<a class="indexterm" id="id22"/> designed to answer analytical queries in a short time. They typically get data from the OLTP operations, and their data model is optimized for querying. They basically provide for consolidation (roll-up), drill-down and slicing and dicing of data for analytical purposes. They often use specialized data stores that can optimize ad-hoc analytical queries on the fly. It is important for such databases to provide pivot-table like capability. Often, the OLAP cube is used to get fast access to the analytical data.</p><p>Feeding the OLTP data into the OLAP systems may entail workflows and multistage batch processing. The performance concern of such systems is to efficiently deal with large quantities of data while also dealing with inevitable failures and recovery.</p></div><div class="section" title="Batch processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Batch processing</h2></div></div></div><p>
<span class="strong"><strong>Batch processing</strong></span> is <a class="indexterm" id="id23"/>automated <a class="indexterm" id="id24"/>execution of predefined jobs. These are typically bulk jobs that are executed during off-peak hours. Batch processing may involve one or more stages of job processing. Often batch processing is clubbed with workflow automation, where some workflow steps are executed offline. Many of the batch processing jobs work on staging of data, and on preparing data for the next stage of processing to pick up.</p><p>Batch jobs are generally optimized for the best utilization of the computing resources. Since there is little to moderate the demand to lower the latencies of some particular subtasks, these systems tend to optimize for throughput. A lot of batch jobs involve largely I/O processing and are often distributed over a cluster. Due to distribution, the data locality is preferred when processing the jobs; that is, the data and processing should be local in order to avoid network latency in reading/writing data.</p></div></div></div>
<div class="section" title="A structured approach to the performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>A structured approach to the performance</h1></div></div></div><p>In practice, the<a class="indexterm" id="id25"/> performance of non-trivial applications is <a class="indexterm" id="id26"/>rarely a function of coincidence or prediction. For many projects, performance is not an option (it is rather a necessity), which is why this is even more important today. Capacity planning, determining performance objectives, performance modeling, measurement, and monitoring are key.</p><p>Tuning a poorly designed system to perform is significantly harder, if not practically impossible, than having a system well-designed from the start. In order to meet a performance goal, performance objectives should be known before the application is designed. The performance objectives are stated in terms of latency, throughput, resource utilization, and workload. These terms are discussed in the following section in this chapter.</p><p>The resource cost can be identified in terms of application scenarios, such as browsing of products, adding products to shopping cart, checkout, and more. Creating workload profiles that represent users performing various operations is usually helpful.</p><p>
<span class="strong"><strong>Performance modeling</strong></span>
<a class="indexterm" id="id27"/> is a reality check for whether the application design will support the performance objectives. It includes performance objectives, application scenarios, constraints, measurements (benchmark results), workload objectives and if available, the performance baseline. It is not a replacement for measurement and load testing, rather, the model is validated using these. The performance model may include the performance test cases to assert the performance characteristics of the application scenarios.</p><p>Deploying an application to production almost always needs<a class="indexterm" id="id28"/> some form of <span class="strong"><strong>capacity planning</strong></span>. It has to take into account the performance objectives for today and for the foreseeable future. It requires an idea of the application architecture, and an understanding of how the external factors translate into the internal workload. It <a class="indexterm" id="id29"/>also requires informed expectations about the<a class="indexterm" id="id30"/> responsiveness and the level of service to be provided by the system. Often, capacity planning is done early in a project to mitigate the risk of provisioning delays.</p></div>
<div class="section" title="The performance vocabulary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>The performance vocabulary</h1></div></div></div><p>There are several<a class="indexterm" id="id31"/> technical terms that are heavily used in performance engineering. It is important to understand these, as they form the cornerstone of the performance-related discussions. Collectively, these terms form a performance vocabulary. The performance is usually measured in terms of several parameters, where every parameter has roles to play—such parameters are a part of the vocabulary.</p><div class="section" title="Latency"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Latency</h2></div></div></div><p>
<span class="strong"><strong>Latency</strong></span> is the<a class="indexterm" id="id32"/> time taken by an individual unit of work to complete the task. It does<a class="indexterm" id="id33"/> not imply successful completion of a task. Latency is not collective, it is linked to a particular task. If two similar jobs—<code class="literal">j1</code> and <code class="literal">j2</code> took 3 ms and 5 ms respectively, their latencies would be treated as such. If <code class="literal">j1</code> and <code class="literal">j2</code> were dissimilar tasks, it would have made no difference. In many cases the average latency of similar jobs is used in the performance objectives, measurement, and monitoring results.</p><p>Latency is an important indicator of the health of a system. A high performance system often thrives on low latency. Higher than normal latency can be caused due to load or bottleneck. It helps to measure the latency distribution during a load test. For example, if more than 25 percent of similar jobs, under a similar load, have significantly higher latency than others, then it may be an indicator of a bottleneck scenario that is worth investigating.</p><p>When a task called <code class="literal">j1</code> consists of smaller tasks called <code class="literal">j2</code>, <code class="literal">j3</code>, and <code class="literal">j4</code>, the latency of <code class="literal">j1</code> is not necessarily the sum of the latencies of each of <code class="literal">j2</code>, <code class="literal">j3</code>, and <code class="literal">j4</code>. If any of the subtasks of <code class="literal">j1</code> are concurrent with another, the latency of <code class="literal">j1</code> will turn out to be less than the sum of the latencies of <code class="literal">j2</code>, <code class="literal">j3</code>, and <code class="literal">j4</code>. The I/O bound tasks are generally more prone to higher latency. In network systems, latency is commonly based on the round-trip to another host, including the latency from source to destination, and then back to source.</p></div><div class="section" title="Throughput"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>Throughput</h2></div></div></div><p>
<span class="strong"><strong>Throughput</strong></span> is the <a class="indexterm" id="id34"/>number of successful tasks or operations performed in a unit of time. The <a class="indexterm" id="id35"/>top-level operations performed in a unit of time are usually of a similar kind, but with a potentially different from latencies. So, what does throughput tell us about the system? It is the rate at which the system is performing. When you perform load testing, you can determine the maximum rate at which a particular system can perform. However, this is not a guarantee of the conclusive, overall, and maximum rate of performance.</p><p>Throughput is one of the factors that determine the scalability of a system. The throughput of a higher level task depends on the capacity to spawn multiple such tasks in parallel, and also on the average latency of those tasks. The throughput should be measured during load testing and performance monitoring to determine the peak-measured throughput, and the maximum-sustained throughput. These factors contribute to the scale and performance of a system.</p></div><div class="section" title="Bandwidth"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>Bandwidth</h2></div></div></div><p>
<span class="strong"><strong>Bandwidth</strong></span> is the<a class="indexterm" id="id36"/> raw data rate over a communication channel, measured in a certain number <a class="indexterm" id="id37"/>of bits per second. This includes not only the payload, but also all the overhead necessary to carry out the communication. Some examples are: Kbits/sec, Mbits/sec, and more. An uppercase B such as KB/sec denotes Bytes, as in kilobytes per second. Bandwidth is often compared to throughput. While bandwidth is the raw capacity, throughput for the same system is the successful task completion rate, which usually involves a round-trip. Note that throughput is for an operation that involves latency. To achieve maximum throughput for a given bandwidth, the communication/protocol overhead and operational latency should be minimal.</p><p>For storage systems (such as hard disks, solid-state drives, and more) the predominant way to measure performance is <a class="indexterm" id="id38"/>
<span class="strong"><strong>IOPS</strong></span> (<span class="strong"><strong>Input-output per second</strong></span>), which is multiplied by the transfer size and represented as bytes per second, or further into MB/sec, GB/sec, and more. IOPS is usually derived for sequential and random workloads for read/write operations.</p><p>Mapping the throughput of a system to the bandwidth of another may lead to dealing with an impedance mismatch between the two. For example, an order processing system may perform the following tasks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Transact with the database on disk </li><li class="listitem" style="list-style-type: disc">Post results over the network to an external system</li></ul></div><p>Depending on the bandwidth of the disk sub-system, the bandwidth of the network, and the execution model of order processing, the throughput may depend not only on the bandwidth of<a class="indexterm" id="id39"/> the disk sub-system and network, but also on how loaded they currently are. Parallelism <a class="indexterm" id="id40"/>and pipelining are common ways to increase the throughput over a given bandwidth.</p></div><div class="section" title="Baseline and benchmark"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Baseline and benchmark</h2></div></div></div><p>The performance <a class="indexterm" id="id41"/>
<span class="strong"><strong>baseline</strong></span>, or simply baseline, is the reference point, including measurements of <a class="indexterm" id="id42"/>well-characterized and understood performance parameters for a known configuration. The baseline is used to collect performance measurements for the same parameters that we may benchmark later for another configuration. For example, collecting "throughput distribution over 10 minutes at a load of 50 concurrent threads" is one such performance parameter that we can use for baseline and benchmarking. A baseline is recorded together with the hardware, network, OS and JVM configuration.</p><p>The performance <a class="indexterm" id="id43"/>
<span class="strong"><strong>benchmark</strong></span>, or simply benchmark, is the recording of the performance parameter <a class="indexterm" id="id44"/>measurements under various test conditions. A benchmark can be composed of a performance test suite. A benchmark may collect small to large amounts of data, and may take varying durations depending on the use-cases, scenarios, and environment characteristics.</p><p>A baseline is a result of the benchmark that was conducted at one point in time. However, a benchmark is independent of the baseline.</p></div><div class="section" title="Profiling"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Profiling</h2></div></div></div><p>
<span class="strong"><strong>Performance</strong></span>
<a class="indexterm" id="id45"/>
<span class="strong"><strong> profiling</strong></span>
<a class="indexterm" id="id46"/>, or simply profiling, is the analysis of the execution of a program at its runtime. A program can perform poorly for a variety of reasons. A <span class="strong"><strong>profiler</strong></span>
<a class="indexterm" id="id47"/>
can analyze and find out the execution time of various parts of the program. It is possible to put statements in a program manually to print the execution time of the blocks of code, but it gets very cumbersome as you try to refine the code iteratively.</p><p>A profiler is of great assistance to the developer. Going by how profilers work, there are three major kinds—instrumenting, sampling, and event-based.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Event-based profilers</strong></span>: These profilers <a class="indexterm" id="id48"/>work only for selected language <a class="indexterm" id="id49"/>platforms, and provide a good balance between the overhead and results; Java supports event-based profiling via the JVMTI interface.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The instrumenting profilers</strong></span>: These <a class="indexterm" id="id50"/>profilers modify code at either <a class="indexterm" id="id51"/>compile time, or runtime to inject performance counters. They are intrusive by nature and add significant performance overhead. However, you can profile the regions of code very selectively using the instrumenting profilers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The sampling profilers</strong></span>: These <a class="indexterm" id="id52"/>profilers pause the runtime and <a class="indexterm" id="id53"/>collect its state at "sampling intervals". By collecting enough samples, they get to know where the program is <a class="indexterm" id="id54"/>spending most of its time. For example, at a sampling interval of 1 millisecond, the profiler would have collected 1000 samples in a second. A sampling profiler also works for code that executes faster than the sampling interval (as in, the code may perform several iterations of work between the two sampling events), as the frequency of pausing and sampling is proportional to the overall execution time of any code.</li></ul></div><p>Profiling is not meant only for measuring execution time. Capable profilers can provide a view of memory analysis, garbage collection, threads, and more. A combination of such tools is helpful to find memory leaks, garbage collection issues, and so on.</p></div><div class="section" title="Performance optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Performance optimization</h2></div></div></div><p>Simply put, <span class="strong"><strong>optimization</strong></span>
<a class="indexterm" id="id55"/> is enhancing <a class="indexterm" id="id56"/>a program's resource <a class="indexterm" id="id57"/>consumption after a performance analysis. The symptoms of a poorly performing program are observed in terms of high latency, low throughput, unresponsiveness, instability, high memory consumption, high CPU consumption, and more. During the performance analysis, one may profile the program in order to identify the bottlenecks and tune the performance incrementally by observing the performance parameters.</p><p>Better and suitable algorithms are an all-around good way to optimize code. The CPU bound code can be optimized with computationally cheaper operations. The cache bound code can try using less memory lookups to keep a good hit ratio. The memory bound code can use an adaptive memory usage and conservative data representation to store in memory for optimization. The I/O bound code can attempt to serialize as little data as possible, and batching of operations will make the operation less chatty for better performance. Parallelism and distribution are other, overall good ways to increase performance.</p></div><div class="section" title="Concurrency and parallelism"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>Concurrency and parallelism</h2></div></div></div><p>Most of the computer hardware and operating systems that we use today provide concurrency. On the x86 architecture, hardware support for concurrency can be traced as far back as the 80286 chip. <span class="strong"><strong>Concurrency</strong></span>
<a class="indexterm" id="id58"/> is the <a class="indexterm" id="id59"/>simultaneous execution of more than one process on the same computer. In older processors, concurrency was implemented using the context switch by the operating system kernel. When concurrent parts are executed in parallel by the hardware instead of merely the switching context, it is called <span class="strong"><strong>parallelism</strong></span>. Parallelism<a class="indexterm" id="id60"/> is the property of the hardware, though <a class="indexterm" id="id61"/>the software stack must support it in order for you to leverage it in your programs. We must write your program in a concurrent way to exploit the parallelism features of the hardware.</p><p>While concurrency is a natural way to exploit hardware parallelism and speed up operations, it is worth bearing in mind that having significantly higher concurrency than the parallelism that your hardware can support is likely to schedule tasks to varying processor cores thereby, lowering the branch prediction and increasing cache misses.</p><p>At a low level, spawning the processes/threads, mutexes, semaphores, locking, shared memory, and interprocess communication are used for concurrency. The JVM has an excellent support for these concurrency primitives and interthread communication. Clojure has both—the low and higher level concurrency primitives that we will discuss in the concurrency chapter.</p></div><div class="section" title="Resource utilization"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/>Resource utilization</h2></div></div></div><p>R<span class="strong"><strong>esource utilization</strong></span> <a class="indexterm" id="id62"/>is the measure of the server, network, and storage resources <a class="indexterm" id="id63"/>that is consumed by an application. Resources include CPU, memory, disk I/O, network I/O, and more. The application can be analyzed in terms of CPU bound, memory bound, cache bound, and I/O bound tasks. Resource utilization can be derived by means of benchmarking, by measuring the utilization at a given throughput.</p></div><div class="section" title="Workload"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>Workload</h2></div></div></div><p>
<span class="strong"><strong>Workload</strong></span> is<a class="indexterm" id="id64"/> the quantification <a class="indexterm" id="id65"/>of how much work is there in hand to be carried out by the application. It is measured in the total numbers of users, the concurrent active users, the transaction volume, the data volume, and more. Processing a workload should take in to account the load conditions, such as how much data the database currently holds, how filled up the message queues are, the backlog of I/O tasks after which the new load will be processed, and more.</p></div></div>
<div class="section" title="The latency numbers that every programmer should know"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>The latency numbers that every programmer should know</h1></div></div></div><p>Hardware and software <a class="indexterm" id="id66"/>have progressed over the years. Latencies for various operations put things in perspective. The latency numbers for the year 2015, reproduced with the permission of Aurojit Panda and Colin Scott of Berkeley University (<a class="ulink" href="http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html">http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html</a>). Latency numbers that every <a class="indexterm" id="id67"/>programmer should know are as shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Operation</p>
</th><th style="text-align: left" valign="bottom">
<p>Time taken as of 2015</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>L1 cache reference</p>
</td><td style="text-align: left" valign="top">
<p>1ns (nano second)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Branch mispredict</p>
</td><td style="text-align: left" valign="top">
<p>3 ns</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>L2 cache reference</p>
</td><td style="text-align: left" valign="top">
<p>4 ns</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mutex lock/unlock</p>
</td><td style="text-align: left" valign="top">
<p>17 ns</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Compress 1KB with Zippy</p>
<p>(Zippy/Snappy: <a class="ulink" href="http://code.google.com/p/snappy/">http://code.google.com/p/snappy/</a>)</p>
</td><td style="text-align: left" valign="top">
<p>2μs (1000 ns = 1μs: micro second)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Send 2000 bytes over the commodity network</p>
</td><td style="text-align: left" valign="top">
<p>200ns (that is, 0.2μs)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SSD random read</p>
</td><td style="text-align: left" valign="top">
<p>16 μs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Round-trip in the same datacenter</p>
</td><td style="text-align: left" valign="top">
<p>500 μs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Read 1,000,000 bytes sequentially from SSD</p>
</td><td style="text-align: left" valign="top">
<p>200 μs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Disk seek</p>
</td><td style="text-align: left" valign="top">
<p>4 ms (1000 μs = 1 ms)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Read 1,000,000 bytes sequentially from disk</p>
</td><td style="text-align: left" valign="top">
<p>2 ms</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Packet roundtrip CA to Netherlands</p>
</td><td style="text-align: left" valign="top">
<p>150 ms</p>
</td></tr></tbody></table></div><p>The preceding table shows the operations in a computer vis-a-vis the latency incurred due to the operation. When a CPU core processes some data in a CPU register, it may take a few CPU cycles (for reference, a 3 GHz CPU runs 3000 cycles per nanosecond), but the moment it has to fall back on L1 or L2 cache, the latency becomes thousands of times slower. The preceding table does not show main memory access latency, which is roughly 100 ns (it varies, based on the access pattern)—about 25 times slower than the L2 cache.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Summary</h1></div></div></div><p>We learned about the basics of what it is like to think more deeply about performance. We saw the common performance vocabulary, and also the use cases by which performance aspects might vary. We concluded by looking at the performance numbers for the different hardware components, which is how performance benefits reach our applications. In the next chapter, we will dive into the performance aspects of the various Clojure abstractions.</p></div></body></html>