- en: Chapter 6. Measuring Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on the expected and actual performance, and the lack or presence
    of a measuring system, performance analysis and tuning can be a fairly elaborate
    process. Now we will discuss the analysis of performance characteristics and ways
    to measure and monitor them. In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance and understanding the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What performance tests to carry out for different purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring performance and obtaining metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling Clojure code to identify performance bottlenecks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance measurement and statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring performance is the stepping stone to performance analysis. As we noted
    earlier in this book, there are several performance parameters to be measured
    with respect to various scenarios. Clojure's built-in `time` macro is a tool to
    measure the amount of time elapsed while executing a body of code. Measuring performance
    factors is a much more involved process. The measured performance numbers may
    have linkages with each other that we need to analyze. It is a common practice
    to use statistical concepts to establish the linkage factors. We will discuss
    some basic statistical concepts in this section and use that to explain how the
    measured data gives us the bigger picture.
  prefs: []
  type: TYPE_NORMAL
- en: A tiny statistics terminology primer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we have a series of quantitative data, such as latency in milliseconds
    for the same operation (measured over a number of executions), we can observe
    a number of things. First, and the most obvious, are the minimum and maximum values
    in the data. Let''s take an example dataset to analyze further:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 23 | 19 | 21 | 24 | 26 | 20 | 22 | 21 | 25 | 168 | 23 | 20 | 29 | 172 | 22
    | 24 | 26 |'
  prefs: []
  type: TYPE_TB
- en: Median, first quartile, third quartile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can see that the minimum latency here is 19 ms whereas the maximum latency
    is 172ms. We can also observe that the average latency here is about 40ms. Let''s
    sort this data in ascending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 19 | 20 | 20 | 21 | 21 | 22 | 22 | 23 | 23 | 24 | 24 | 25 | 26 | 26 | 29
    | 168 | 172 |'
  prefs: []
  type: TYPE_TB
- en: 'The center element of the previous dataset, that is the ninth element (value
    23), is considered the **median** of the dataset. It is noteworthy that the median
    is a better representative of the center of the data than the **average** or **mean**.
    The center element of the left half, that is the fifth element (value 21), is
    considered the **first quartile**. Similarly, the value in the center of the right
    half, that is the 13th element (value 26), is considered the **third quartile**
    of the dataset. The difference between the third quartile and the first quartile
    is called **Inter Quartile Range (IQR)**, which is 5 in this case. This can be
    illustrated with a **boxplot** , as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Median, first quartile, third quartile](img/3642_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A boxplot highlights the first quartile, median and the third quartile of a
    dataset. As you can see, two "outlier" latency numbers (168 and 172) are unusually
    higher than the others. Median makes no representation of outliers in a dataset,
    whereas the mean does.
  prefs: []
  type: TYPE_NORMAL
- en: '![Median, first quartile, third quartile](img/3642_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A histogram (the diagram shown previously) is another way to display a dataset
    where we batch the data elements in **periods** and expose the **frequency** of
    such periods. A period contains the elements in a certain range. All periods in
    a histogram are generally the same size; however, it is not uncommon to omit certain
    periods when there is no data.
  prefs: []
  type: TYPE_NORMAL
- en: Percentile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **percentile** is expressed with a parameter, such as 99 percentile, or 95
    percentile etc. A percentile is the value below which all the specified percentage
    of data elements exist. For example, 95 percentile means the value *N* among a
    dataset, such that 95 percent of elements in the dataset are below *N* in value.
    As a concrete example, 85 percentile from the dataset of latency numbers we discussed
    earlier in this section is 29, because out of 17 total elements, 14 (which is
    85 percent of 17) other elements in the dataset have a value below 29\. A quartile
    splits a dataset into chunks of 25 percent elements each. Therefore, the first
    quartile is actually 25 percentile, the median is 50 percentile, and the third
    quartile is 75 percentile.
  prefs: []
  type: TYPE_NORMAL
- en: Variance and standard deviation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The spread of the data, that is, how far away the data elements are from the
    center value, gives us further insight into the data. Consider the *i^(th)* deviation
    as the difference between the *i^(th)* dataset element value (in statistics terms,
    a "variable" value) and its mean; we can represent it as ![Variance and standard
    deviation](img/image006.jpg). We can express its "variance" and "standard deviation"
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance = ![Variance and standard deviation](img/image008.jpg), standard deviation
    (σ) = ![Variance and standard deviation](img/image010.jpg) = ![Variance and standard
    deviation](img/image012.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard deviation is shown as the Greek letter "sigma", or simply "s". Consider
    the following Clojure code to determine variance and standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can use the Clojure-based platform Incanter ([http://incanter.org/](http://incanter.org/))
    for statistical computations. For example, you can find standard deviation using
    `(incanter.stats/sd tdata)` in Incanter.
  prefs: []
  type: TYPE_NORMAL
- en: The **empirical rule** states the relationship between the elements of a dataset
    and SD. It says that 68.3 percent of all elements in a dataset lie in the range
    of one (positive or negative) SD from the mean, 95.5 percent of all elements lie
    in two SDs from the mean, and 99.7 percent of all data elements lie in three SDs
    from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the latency dataset we started out with, one SD from the mean is
    ![Variance and standard deviation](img/image014.jpg)(![Variance and standard deviation](img/image016.jpg)
    range -9 to 89) containing 88 percent of all elements. Two SDs from the mean is
    ![Variance and standard deviation](img/image014.jpg) range -58 to 138) containing
    the same 88 percent of all elements. However, three SDs from the mean is(![Variance
    and standard deviation](img/image018.jpg)range -107 to 187) containing 100 percent
    of all elements. There is a mismatch between what the empirical rule states and
    the results we found, because the empirical rule applies generally to uniformly
    distributed datasets with a large number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Criterium output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 4](ch04.html "Chapter 4. Host Performance"), *Host Performance*,
    we introduced the Clojure library *Criterium* to measure the latency of Clojure
    expressions. A sample benchmarking result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the result has some familiar terms we discussed earlier in this
    section. A high mean and low standard deviation indicate that there is not a lot
    of variation in the execution times. Likewise, the lower (first) and upper (third)
    quartiles indicate that they are not too far away from the mean. This result implies
    that the body of code is more or less stable in terms of response time. Criterium
    repeats the execution many times to collect the latency numbers.
  prefs: []
  type: TYPE_NORMAL
- en: However, why does Criterium attempt to do a statistical analysis of the execution
    time? What would be amiss if we simply calculate the mean? It turns out that the
    response times of all executions are not always stable and there is often disparity
    in how the response time shows up. Only upon running sufficient times under correctly
    simulated load we can get complete data and other indicators about the latency.
    A statistical analysis gives insight into whether there is something wrong with
    the benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Guided performance objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We only briefly discussed performance objectives in [Chapter 1](ch01.html "Chapter 1. Performance
    by Design"), *Performance by Design* because that discussion needs a reference
    to statistical concepts. Let's say we identified the functional scenarios and
    the corresponding response time. Should response time remain fixed? Can we constrain
    throughput in order to prefer a stipulated response time?
  prefs: []
  type: TYPE_NORMAL
- en: The performance objective should specify the worst-case response time, that
    is, maximum latency, the average response time and the maximum standard deviation.
    Similarly, the performance objective should also mention the worst-case throughput,
    maintenance window throughput, average throughput, and the maximum standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing for performance requires us to know what we are going to test, how we
    want to test, and what environment to set up for the tests to execute. There are
    several pitfalls to be aware of, such as a lack of near-real hardware and resources
    of production use, similar OS and software environments, diversity of representative
    data for test cases, and so on. Lack of diversity in test inputs may lead to a
    monotonic branch prediction, hence introducing a "bias" in test results.
  prefs: []
  type: TYPE_NORMAL
- en: The test environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concerns about the test environment begin with the hardware representative of
    the production environment. Traditionally, the test environment hardware has been
    a scaled-down version of the production environment. The performance analysis
    done on non-representative hardware is almost certain to skew the results. Fortunately,
    in recent times, thanks to the commodity hardware and cloud systems, provisioning
    test environment hardware that is similar to the production environment is not
    too difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The network and storage bandwidth, operating system, and software used for performance
    testing should of course be the same as in production. What is also important
    is to have a "load" representative of the test scenarios. The load comes in different
    combinations including the concurrency of requests, the throughput and standard
    deviation of requests, the current population level in the database or in the
    message queue, CPU and heap usage, and so on. It is important to simulate a representative
    load.
  prefs: []
  type: TYPE_NORMAL
- en: Testing often requires quite some work on the part of the piece of code that
    carries out the test. Be sure to keep its overhead to a minimum so that it does
    not impact the benchmark results. Wherever possible, use a system other than the
    test target to generate requests.
  prefs: []
  type: TYPE_NORMAL
- en: What to test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any implementation of a non-trivial system typically involves many hardware
    and software components. Performance testing a certain feature or a service in
    the entire system needs to account for the way it interacts with the various sub-systems.
    For example, a web service call may touch multiple layers such as the web server
    (request/response marshaling and unmarshaling), URI-based routing, service handler,
    application-database connector, the database layer, logger component, and so on.
    Testing only the service handler would be a terrible mistake, because that is
    not exactly the performance what the web client will experience. The performance
    test should test at the perimeter of a system to keep the results realistic, preferably
    with a third-party observer.
  prefs: []
  type: TYPE_NORMAL
- en: The performance objectives state the criteria for testing. It would be useful
    to test what is not required by the objective, especially when the tests are run
    concurrently. Running meaningful performance tests may require a certain level
    of isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The latency obtained by executing a body of code may vary slightly on each run.
    This necessitates that we execute the code many times and collect samples. The
    latency numbers may be impacted by the JVM warm-up time, garbage collection and
    the JIT compiler kicking in. So, the test and sample collection should ensure
    that these conditions do not impact the results. Criterium follows such methods
    to produce the results. When we test a very small piece of code this way, it is
    called a **Micro-benchmark**.
  prefs: []
  type: TYPE_NORMAL
- en: As the latency of some operations may vary during runs, it is important to collect
    all samples and segregate them into periods and frequencies turning up into a
    histogram. The maximum latency is an important metric when measuring latency—it
    indicates the worst-case latency. Besides the maximum, the 99 percentile and 95
    percentile latency numbers are also important to put things in perspective. It's
    important to actually collect the latency numbers instead of inferring them from
    standard deviation, as we noted earlier that the empirical rule works only for
    normal distributions without significant outliers.
  prefs: []
  type: TYPE_NORMAL
- en: The outliers are an important data point when measuring latency. A proportionately
    higher count of outliers indicates a possibility of degradation of service.
  prefs: []
  type: TYPE_NORMAL
- en: Comparative latency measurement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When evaluating libraries for use in projects, or when coming up with alternate
    solutions against some baseline, comparative latency benchmarks are useful to
    determine the performance trade-offs. We will inspect two comparative benchmarking
    tools based on Criterium, called Perforate and Citius. Both make it easy to run
    Criterium benchmarks grouped by context, and to easily view the benchmark results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perforate ([https://github.com/davidsantiago/perforate](https://github.com/davidsantiago/perforate))
    is a Leiningen plugin that lets one define goals; a goal (defined using `perforate.core/defgoal`)
    is a common task or context having one or more benchmarks. Each benchmark is defined
    using `perforate.core/defcase`. As of version 0.3.4, a sample benchmark code may
    look like the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can declare the test environments in `project.clj` and provide the setup/cleanup
    code when defining the goal. Perforate provides ways to run the benchmarks from
    the command-line.
  prefs: []
  type: TYPE_NORMAL
- en: Citius ([https://github.com/kumarshantanu/citius](https://github.com/kumarshantanu/citius))
    is a library that provides integration hooks for clojure.test and other forms
    of invocation. It imposes more rigid constraints than Perforate, and renders additional
    comparative information about the benchmarks. It presumes a fixed number of targets
    (cases) per test suite where there may be several goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of version 0.2.1, a sample benchmark code may look like the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the previous example, the code runs the benchmarks, reports the comparative
    summary, and draws a bar chart image of the mean latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Latency measurement under concurrency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we benchmark a piece of code with Criterium, it uses just a single thread
    to determine results. That gives us a fair output in terms of single-threaded
    result, but there are many benchmarking scenarios where single-threaded latency
    is far from what we need. Under concurrency, the latency often differs quite a
    bit from single-threaded latency. Especially when we deal with *stateful* objects
    (e.g. drawing a connection from a JDBC connection pool, updating shared in-memory
    state etc.), the latency is likely to vary in proportion with the contention.
    In such scenarios it is useful to find out the latency patterns of the code under
    various concurrency levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Citius library we discussed in the previous sub-section supports tunable
    concurrency levels. Consider the following benchmark of implementations of shared
    counters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When I ran this benchmark on a 4th generation quad-core Intel Core i7 processor
    (Mac OSX 10.10), the mean latency at concurrency level 04 was 38 to 42 times the
    value of the mean latency at concurrency level 01\. Since, in many cases, the
    JVM is used to run server-side applications, benchmarking under concurrency becomes
    all the more important.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring throughput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughput is expressed per unit of time. Coarse-grained throughput, that is,
    the throughput number collected over a long period of time, may hide the fact
    when the throughput is actually delivered in bursts instead of a uniform distribution.
    Granularity of the throughput test is subject to the nature of the operation.
    A batch process may process bursts of data, whereas a web service may deliver
    uniformly distributed throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Average throughput test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Though Citius (as of version 0.2.1) shows extrapolated throughput (per second,
    per thread) in benchmark results, that throughput number may not represent the
    actual throughput very well for a variety of reasons. Let''s construct a simple
    throughput benchmark harness as follows, beginning with the helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the helper functions defined, let''s see the benchmarking
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now see how to test some code for throughput using the harness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This harness provides only a simple throughput test. To inspect the throughput
    pattern you may want to bucket the throughput across rolling fixed-duration windows
    (e.g. per second throughput.) However, that topic is beyond the scope of this
    text, though we will touch upon it in the *Performance monitoring* section later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The load, stress, and endurance tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the characteristics of tests is each run only represents the slice of
    time it is executed through. Repeated runs establish their general behavior. But
    how many runs should be enough? There may be several anticipated load scenarios
    for an operation. So, there is a need to repeat the tests at various load scenarios.
    Simple test runs may not always exhibit the long-term behavior and response of
    the operation. Running the tests under varying high load for longer duration allows
    us to observe them for any odd behavior that may not show up in a short test cycle.
  prefs: []
  type: TYPE_NORMAL
- en: When we test an operation at a load far beyond its anticipated latency and throughput
    objectives, that is **stress testing**. The intent of a stress test is to ascertain
    a reasonable behavior exhibited by the operation beyond the maximum load it was
    developed for. Another way to observe the behavior of an operation is to see how
    it behaves when run for a very long duration, typically for several days or weeks.
    Such prolonged tests are called **endurance tests**. While a stress test checks
    the graceful behavior of the operation, an endurance test checks the consistent
    behavior of the operation over a long period.
  prefs: []
  type: TYPE_NORMAL
- en: There are several tools that may help with load and stress testing. Engulf ([http://engulf-project.org/](http://engulf-project.org/))
    is a distributed HTTP-based, load-generation tool written in Clojure. Apache JMeter
    and Grinder are Java-based load-generation tools. Grinder can be scripted using
    Clojure. Apache Bench is a load-testing tool for web systems. Tsung is an extensible,
    high-performance, load-testing tool written in Erlang.
  prefs: []
  type: TYPE_NORMAL
- en: Performance monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During prolonged testing or after the application has gone to production, we
    need to monitor its performance to make sure the application continues to meet
    the performance objectives. There may be infrastructure or operational issues
    impacting the performance or availability of the application, or occasional spikes
    in latency or dips in throughput. Generally, monitoring alleviates such risk by
    generating a continuous feedback stream.
  prefs: []
  type: TYPE_NORMAL
- en: Roughly there are three kinds of components used to build a monitoring stack.
    A **collector** sends the numbers from each host that needs to be monitored. The
    collector gets host information and the performance numbers and sends them to
    an **aggregator**. An aggregator receives the data sent by the collector and persists
    them until asked by a **visualizer** on behalf of the user.
  prefs: []
  type: TYPE_NORMAL
- en: The project **metrics-clojure** ([https://github.com/sjl/metrics-clojure](https://github.com/sjl/metrics-clojure))
    is a Clojure wrapper over the **Metrics** ([https://github.com/dropwizard/metrics](https://github.com/dropwizard/metrics))
    Java framework, which acts as a collector. **Statsd** is a well-known aggregator
    that does not persist data by itself but passes it on to a variety of servers.
    One of the popular visualizer projects is **Graphite** that stores the data as
    well as produces graphs for requested periods. There are several other alternatives
    to these, notably **Riemann** ([http://riemann.io/](http://riemann.io/)) that
    is written in Clojure and Ruby. Riemann is an event processing-based aggregator.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring through logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the popular performance monitoring approaches that has emerged in recent
    times is via logs. The idea is simple—the application emits metrics data as logs,
    which are shipped from the individual machine to a central log aggregation service.
    Then, those metrics data are aggregated for each time window and further moved
    for archival and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a high-level example of such a monitoring system, you may like to use **Logstash-forwarder**
    ([https://github.com/elastic/logstash-forwarder](https://github.com/elastic/logstash-forwarder))
    to grab the application logs from the local filesystem and ship to **Logstash**
    ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)),
    where it forwards the metrics logs to **StatsD** ([https://github.com/etsy/statsd](https://github.com/etsy/statsd))
    for metrics aggregation or to Riemann ([http://riemann.io/](http://riemann.io/))
    for events analysis and monitoring alerts. StatsD and/or Riemann can forward the
    metrics data to Graphite ([http://graphite.wikidot.com/](http://graphite.wikidot.com/))
    for archival and graphing of the time-series metrics data. Often, people want
    to plug in a non-default time-series data store (such as **InfluxDB**: [https://influxdb.com/](https://influxdb.com/))
    or a visualization layer (such as **Grafana**: [http://grafana.org/](http://grafana.org/))
    with Graphite.'
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion on this topic is out of the scope of this text, but I
    think exploring this area would serve you well.
  prefs: []
  type: TYPE_NORMAL
- en: Ring (web) monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you develop web software using Ring ([https://github.com/ring-clojure/ring](https://github.com/ring-clojure/ring))
    then you may find the Ring extension of the metrics-clojure library useful: [http://metrics-clojure.readthedocs.org/en/latest/ring.html](http://metrics-clojure.readthedocs.org/en/latest/ring.html)
    —this tracks a number of useful metrics that can be queried in JSON format and
    integrated with visualization via the web browser.'
  prefs: []
  type: TYPE_NORMAL
- en: To emit a continuous stream of metrics data from the web layer, **Server-Sent
    Events** (**SSE**) may be a good idea due to its low overhead. Both **http-kit**
    ([http://www.http-kit.org/](http://www.http-kit.org/)) and **Aleph** ([http://aleph.io/](http://aleph.io/)),
    which work with Ring, support SSE today.
  prefs: []
  type: TYPE_NORMAL
- en: Introspection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Oracle JDK and OpenJDK provide two GUI tools called **JConsole** (executable
    name `jconsole`) and **JVisualVM** (executable name `jvisualvm`) that we can use
    to introspect into running JVMs for instrumentation data. There are also some
    command-line tools ([http://docs.oracle.com/javase/8/docs/technotes/tools/](http://docs.oracle.com/javase/8/docs/technotes/tools/))
    in the JDK to peek into the inner details of the running JVMs.
  prefs: []
  type: TYPE_NORMAL
- en: A common way to introspect a running Clojure application is to have an **nREPL**
    ([https://github.com/clojure/tools.nrepl](https://github.com/clojure/tools.nrepl))
    service running so that we can connect to it later using an nREPL client. Interactive
    introspection over nREPL using the Emacs editor (embedded nREPL client) is popular
    among some, whereas others prefer to script an nREPL client to carry out tasks.
  prefs: []
  type: TYPE_NORMAL
- en: JVM instrumentation via JMX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The JVM has a built-in mechanism to introspect managed resources via the extensible
    **Java Management Extensions** (**JMX**) API. It provides a way for application
    maintainers to expose manageable resources as "MBeans". Clojure has an easy-to-use
    `contrib` library called `java.jmx` ([https://github.com/clojure/java.jmx](https://github.com/clojure/java.jmx))
    to access JMX. There is a decent amount of open source tooling for visualization
    of JVM instrumentation data via JMX, such as `jmxtrans` and `jmxetric`, which
    integrate with Ganglia and Graphite.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting quick memory stats of the JVM is pretty easy using Clojure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We briefly discussed profiler types in [Chapter 1](ch01.html "Chapter 1. Performance
    by Design"), *Performance by Design*. The JVisualVM tool we discussed with respect
    to introspection in the previous section is also a CPU and memory profiler that
    comes bundled with the JDK. Let''s see them in action— consider the following
    two Clojure functions that stress the CPU and memory respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Using JVisualVM is pretty easy—open the Clojure JVM process from the left pane.
    It has sampler and regular profiler styles of profiling. Start profiling for CPU
    or memory use when the code is running and wait for it to collect enough data
    to plot on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling](img/3642_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following shows memory profiling in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling](img/3642_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that JVisualVM is a very simple, entry-level profiler. There are several
    commercial JVM profilers on the market for sophisticated needs.
  prefs: []
  type: TYPE_NORMAL
- en: OS and CPU/cache-level profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling only the JVM may not always tell the whole story. Getting down to
    OS and hardware-level profiling often provides better insight into what is going
    on with the application. On Unix-like operating systems, command-line tools such
    as `top`, `htop`, `perf`, `iota`, `netstat`, `vista`, `upstate`, `pidstat` etc
    can help. Profiling the CPU for cache misses and other information is a useful
    source to catch performance issues. Among open source tools for Linux, **Likwid**
    ([http://code.google.com/p/likwid/](http://code.google.com/p/likwid/) and [https://github.com/rrze-likwid/likwid](https://github.com/rrze-likwid/likwid))
    is small yet effective for Intel and AMD processors; **i7z** ([https://code.google.com/p/i7z/](https://code.google.com/p/i7z/)
    and [https://github.com/ajaiantilal/i7z](https://github.com/ajaiantilal/i7z))
    is specifically for Intel processors. There are also dedicated commercial tools
    such as **Intel VTune Analyzer** for more elaborate needs.
  prefs: []
  type: TYPE_NORMAL
- en: I/O profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling I/O may require special tools too. Besides `iota` and `blktrace`,
    `ioping` ([https://code.google.com/p/ioping/](https://code.google.com/p/ioping/)
    and [https://github.com/koct9i/ioping](https://github.com/koct9i/ioping)) is useful
    to measure real-time I/O latency on Linux/Unix systems. The **vnStat** tool is
    useful to monitor and log network traffic on Linux. The IOPS of a storage device
    may not tell the whole truth unless it is accompanied by latency information for
    different operations, and how many reads and writes can simultaneously happen.
  prefs: []
  type: TYPE_NORMAL
- en: In an I/O bound workload one has to look for the read and write IOPS over time
    and set a threshold to achieve optimum performance. The application should throttle
    the I/O access so that the threshold is not crossed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delivering high-performance applications not only requires care for performance
    but also systematic effort to measure, test, monitor, and optimize the performance
    of various components and subsystems. These activities often require the right
    skill and experience. Sometimes, performance considerations may even bring system
    design and architecture back to the drawing board. Early structured steps taken
    to achieve performance go a long way to ensuring that the performance objectives
    are being continuously met.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into performance optimization tools and techniques.
  prefs: []
  type: TYPE_NORMAL
