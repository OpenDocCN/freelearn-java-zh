<html><head></head><body><div class="part" title="Part&#xA0;2.&#xA0;Module 2"><div class="titlepage"><div><div><h1 class="title"><a id="part02"/>Part 2. Module 2</h1></div></div></div><div class="partintro" title="Module 2"><div/><div class="blockquote"><blockquote class="blockquote"><p>
<span class="strong"><strong>Clojure High Performance Programming, Second Edition</strong></span>
</p><p>
<span class="emphasis"><em>
Become an expert at writing fast and high performant code in Clojure 1.7.0
</em></span>
</p></blockquote></div></div></div>
<div class="chapter" title="Chapter&#xA0;1.&#xA0;Performance by Design"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 1. Performance by Design</h1></div></div></div><p>Clojure<a class="indexterm" id="id00"/> is a safe, functional programming language that brings great power and simplicity to the user. Clojure is also dynamically and strongly typed, and has very good performance characteristics. Naturally, every activity performed on a computer has an associated cost. What constitutes acceptable performance varies from one use-case and workload to another. In today's world, performance is even the determining factor for several kinds of applications. We will discuss Clojure (which runs on the <a class="indexterm" id="id01"/>
<span class="strong"><strong>JVM</strong></span> (<span class="strong"><strong>Java Virtual Machine</strong></span>)), and its runtime environment in the light of performance, which is the goal of this book.</p><p>The performance of Clojure applications depend on various factors. For a given application, understanding its use cases, design and implementation, algorithms, resource requirements and alignment with the hardware, and the underlying software capabilities is essential. In this chapter, we will study the basics of performance analysis, including the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Classifying the performance anticipations by the use cases types</li><li class="listitem" style="list-style-type: disc">Outlining the structured approach to analyze performance</li><li class="listitem" style="list-style-type: disc">A glossary of terms, commonly used to discuss performance aspects</li><li class="listitem" style="list-style-type: disc">The performance numbers that every programmer should know</li></ul></div><div class="section" title="Use case classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec008"/>Use case classification</h1></div></div></div><p>The performance <a class="indexterm" id="id02"/>requirements and priority vary across the different kinds of use cases. We need to determine what constitutes acceptable performance for the various kinds of use cases. Hence, we classify them to identify their performance model. When it comes to details, there is no sure shot performance recipe for any kind of use case, but it certainly helps to study their general nature. Note that in real life, the use cases listed in this section may overlap with each other.</p><div class="section" title="The user-facing software"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec006"/>The user-facing software</h2></div></div></div><p>The <a class="indexterm" id="id03"/>performance of user-facing applications is <a class="indexterm" id="id04"/>strongly linked to the user's anticipation. Having a difference of a good number of milliseconds may not be perceptible for the user but at the same time, a wait of more than a few seconds may not be taken kindly. One important element in normalizing anticipation is to engage the user by providing duration-based feedback. A good idea to deal with such a scenario would be to start the task asynchronously in the background, and poll it from the UI layer to generate a duration-based feedback for the user. Another way could be to incrementally render the results to the user to even out the anticipation.</p><p>Anticipation is not the only factor in user facing performance. Common techniques like staging or precomputation of data, and other general optimization techniques can go a long way to improve the user experience with respect to performance. Bear in mind that all kinds of user facing interfaces fall into this use case category—the Web, mobile web, GUI, command line, touch, voice-operated, gesture...you name it.</p></div><div class="section" title="Computational and data-processing tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec007"/>Computational and data-processing tasks</h2></div></div></div><p>Non-trivial compute<a class="indexterm" id="id05"/> intensive tasks <a class="indexterm" id="id06"/>demand a proportional amount of computational resources. All of the CPU, cache, memory, efficiency and the parallelizability of the computation algorithms would be involved in determining the performance. When the computation is combined with distribution over a network or reading from/staging to disk, I/O bound factors come into play. This class of workloads can be further subclassified into more specific use cases.</p><div class="section" title="A CPU bound computation"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec002"/>A CPU bound computation</h3></div></div></div><p>A CPU bound computation <a class="indexterm" id="id07"/>is limited by the CPU cycles spent on executing it. Arithmetic <a class="indexterm" id="id08"/>processing in a loop, small matrix multiplication, determining whether a number is a <a class="indexterm" id="id09"/>
<span class="strong"><strong>Mersenne prime</strong></span>, and so on, would be considered CPU bound jobs. If the algorithm complexity is linked to the number of iterations/operations <span class="emphasis"><em>N</em></span>, such as <span class="emphasis"><em>O(N)</em></span>, <span class="emphasis"><em>O(N</em></span>
<span class="emphasis"><em><sup>2</sup>)</em></span> and more, then the performance depends on how big <span class="emphasis"><em>N</em></span> is, and how many CPU cycles each step takes. For parallelizable algorithms, performance of such tasks may be enhanced by assigning multiple CPU cores to the task. On virtual hardware, the performance may be impacted if the CPU cycles are available in bursts.</p></div><div class="section" title="A memory bound task"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec003"/>A memory bound task</h3></div></div></div><p>A memory bound task <a class="indexterm" id="id010"/>is limited by the<a class="indexterm" id="id011"/> availability and bandwidth of the memory. Examples include large text processing, list processing, and more. For example, specifically in Clojure, the <code class="literal">(reduce f (pmap g coll))</code> operation would be memory bound if <code class="literal">coll</code> is a large sequence of big maps, even though we parallelize the operation using <code class="literal">pmap</code> here. Note that higher CPU resources cannot help when memory is the bottleneck, and vice versa. Lack of availability of memory may force you to process smaller chunks of data at a time, even if you have enough CPU resources at your disposal. If the maximum speed of your memory is <span class="emphasis"><em>X</em></span> and your algorithm on single the core accesses the memory at speed <span class="emphasis"><em>X/3</em></span>, the multicore performance of your algorithm cannot exceed three times the current performance, no matter how many CPU cores you assign to it. The memory architecture (for example, SMP and NUMA) contributes to the memory bandwidth in multicore computers. Performance with respect to memory is also subject to page faults.</p></div><div class="section" title="A cache bound task"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec004"/>A cache bound task</h3></div></div></div><p>A task is cache bound when <a class="indexterm" id="id012"/>its speed is constrained by<a class="indexterm" id="id013"/> the amount of cache available. When a task retrieves values from a small number of repeated memory locations, for example a small matrix multiplication, the values may be cached and fetched from there. Note that CPUs (typically) have multiple layers of cache, and the performance will be at its best when the processed data fits in the cache, but the processing will still happen, more slowly, when the data does not fit into the cache. It is possible to make the most of the cache using <a class="indexterm" id="id014"/>
<span class="strong"><strong>cache-oblivious</strong></span> algorithms. A higher number of concurrent cache/memory bound threads than CPU cores is likely to flush the instruction pipeline, as well as the cache at the time of context switch, likely leading to a severely degraded performance.</p></div><div class="section" title="An input/output bound task"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec005"/>An input/output bound task</h3></div></div></div><p>An <span class="strong"><strong>input/output</strong></span> (<span class="strong"><strong>I/O</strong></span>) bound task<a class="indexterm" id="id015"/> would <a class="indexterm" id="id016"/>go faster if the I/O subsystem, that it depends on, goes faster. Disk/storage and network are the most commonly used I/O subsystems in data processing, but it can be serial port, a USB-connected card reader, or any I/O device. An I/O bound task may consume very few CPU cycles. Depending on the speed of the device, connection pooling, data compression, asynchronous handling, application caching, and more, may help in performance. One notable aspect of I/O bound tasks is that performance is usually dependent on the time spent waiting for connection/seek, and the amount of serialization that we do, and hardly on the other resources.</p><p>In practice, many data <a class="indexterm" id="id017"/>processing workloads are usually a combination of <a class="indexterm" id="id018"/>CPU bound, memory bound, cache bound, and I/O bound tasks. The performance of such mixed workloads effectively depends on the even distribution of CPU, cache, memory, and I/O resources over the duration of the operation. A bottleneck situation arises only when one resource gets too busy to make way for another.</p></div></div><div class="section" title="Online transaction processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec008"/>Online transaction processing</h2></div></div></div><p>
<span class="strong"><strong>Online transaction processing</strong></span> (<span class="strong"><strong>OLTP</strong></span>)<a class="indexterm" id="id019"/> systems process the business transactions on demand. They can sit behind <a class="indexterm" id="id020"/>systems such as a user-facing ATM machine, point-of-sale terminal, a network-connected ticket counter, ERP systems, and more. The OLTP systems are characterized by low latency, availability, and data integrity. They run day-to-day business transactions. Any interruption or outage is likely to have a direct and immediate impact on sales or service. Such systems are expected to be designed for resiliency rather than delayed recovery from failures. When the performance objective is unspecified, you may like to consider graceful degradation as a strategy.</p><p>It is a common mistake to ask the OLTP systems to answer analytical queries, something that they are not optimized for. It is desirable for an informed programmer to know the capability of the system, and suggest design changes as per the requirements.</p></div><div class="section" title="Online analytical processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec009"/>Online analytical processing</h2></div></div></div><p><span class="strong"><strong>Online analytical processing</strong></span> (<span class="strong"><strong>OLAP</strong></span>)<a class="indexterm" id="id021"/> systems are<a class="indexterm" id="id022"/> designed to answer analytical queries in a short time. They typically get data from the OLTP operations, and their data model is optimized for querying. They basically provide for consolidation (roll-up), drill-down and slicing and dicing of data for analytical purposes. They often use specialized data stores that can optimize ad-hoc analytical queries on the fly. It is important for such databases to provide pivot-table like capability. Often, the OLAP cube is used to get fast access to the analytical data.</p><p>Feeding the OLTP data into the OLAP systems may entail workflows and multistage batch processing. The performance concern of such systems is to efficiently deal with large quantities of data while also dealing with inevitable failures and recovery.</p></div><div class="section" title="Batch processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec010"/>Batch processing</h2></div></div></div><p>
<span class="strong"><strong>Batch processing</strong></span> is <a class="indexterm" id="id023"/>automated <a class="indexterm" id="id024"/>execution of predefined jobs. These are typically bulk jobs that are executed during off-peak hours. Batch processing may involve one or more stages of job processing. Often batch processing is clubbed with workflow automation, where some workflow steps are executed offline. Many of the batch processing jobs work on staging of data, and on preparing data for the next stage of processing to pick up.</p><p>Batch jobs are generally optimized for the best utilization of the computing resources. Since there is little to moderate the demand to lower the latencies of some particular subtasks, these systems tend to optimize for throughput. A lot of batch jobs involve largely I/O processing and are often distributed over a cluster. Due to distribution, the data locality is preferred when processing the jobs; that is, the data and processing should be local in order to avoid network latency in reading/writing data.</p></div></div></div>
<div class="section" title="A structured approach to the performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec009"/>A structured approach to the performance</h1></div></div></div><p>In practice, the<a class="indexterm" id="id025"/> performance of non-trivial applications is <a class="indexterm" id="id026"/>rarely a function of coincidence or prediction. For many projects, performance is not an option (it is rather a necessity), which is why this is even more important today. Capacity planning, determining performance objectives, performance modeling, measurement, and monitoring are key.</p><p>Tuning a poorly designed system to perform is significantly harder, if not practically impossible, than having a system well-designed from the start. In order to meet a performance goal, performance objectives should be known before the application is designed. The performance objectives are stated in terms of latency, throughput, resource utilization, and workload. These terms are discussed in the following section in this chapter.</p><p>The resource cost can be identified in terms of application scenarios, such as browsing of products, adding products to shopping cart, checkout, and more. Creating workload profiles that represent users performing various operations is usually helpful.</p><p>
<span class="strong"><strong>Performance modeling</strong></span>
<a class="indexterm" id="id027"/> is a reality check for whether the application design will support the performance objectives. It includes performance objectives, application scenarios, constraints, measurements (benchmark results), workload objectives and if available, the performance baseline. It is not a replacement for measurement and load testing, rather, the model is validated using these. The performance model may include the performance test cases to assert the performance characteristics of the application scenarios.</p><p>Deploying an application to production almost always needs<a class="indexterm" id="id028"/> some form of <span class="strong"><strong>capacity planning</strong></span>. It has to take into account the performance objectives for today and for the foreseeable future. It requires an idea of the application architecture, and an understanding of how the external factors translate into the internal workload. It <a class="indexterm" id="id029"/>also requires informed expectations about the<a class="indexterm" id="id030"/> responsiveness and the level of service to be provided by the system. Often, capacity planning is done early in a project to mitigate the risk of provisioning delays.</p></div>
<div class="section" title="The performance vocabulary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec010"/>The performance vocabulary</h1></div></div></div><p>There are several<a class="indexterm" id="id031"/> technical terms that are heavily used in performance engineering. It is important to understand these, as they form the cornerstone of the performance-related discussions. Collectively, these terms form a performance vocabulary. The performance is usually measured in terms of several parameters, where every parameter has roles to play—such parameters are a part of the vocabulary.</p><div class="section" title="Latency"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec011"/>Latency</h2></div></div></div><p>
<span class="strong"><strong>Latency</strong></span> is the<a class="indexterm" id="id032"/> time taken by an individual unit of work to complete the task. It does<a class="indexterm" id="id033"/> not imply successful completion of a task. Latency is not collective, it is linked to a particular task. If two similar jobs—<code class="literal">j1</code> and <code class="literal">j2</code> took 3 ms and 5 ms respectively, their latencies would be treated as such. If <code class="literal">j1</code> and <code class="literal">j2</code> were dissimilar tasks, it would have made no difference. In many cases the average latency of similar jobs is used in the performance objectives, measurement, and monitoring results.</p><p>Latency is an important indicator of the health of a system. A high performance system often thrives on low latency. Higher than normal latency can be caused due to load or bottleneck. It helps to measure the latency distribution during a load test. For example, if more than 25 percent of similar jobs, under a similar load, have significantly higher latency than others, then it may be an indicator of a bottleneck scenario that is worth investigating.</p><p>When a task called <code class="literal">j1</code> consists of smaller tasks called <code class="literal">j2</code>, <code class="literal">j3</code>, and <code class="literal">j4</code>, the latency of <code class="literal">j1</code> is not necessarily the sum of the latencies of each of <code class="literal">j2</code>, <code class="literal">j3</code>, and <code class="literal">j4</code>. If any of the subtasks of <code class="literal">j1</code> are concurrent with another, the latency of <code class="literal">j1</code> will turn out to be less than the sum of the latencies of <code class="literal">j2</code>, <code class="literal">j3</code>, and <code class="literal">j4</code>. The I/O bound tasks are generally more prone to higher latency. In network systems, latency is commonly based on the round-trip to another host, including the latency from source to destination, and then back to source.</p></div><div class="section" title="Throughput"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec012"/>Throughput</h2></div></div></div><p>
<span class="strong"><strong>Throughput</strong></span> is the <a class="indexterm" id="id034"/>number of successful tasks or operations performed in a unit of time. The <a class="indexterm" id="id035"/>top-level operations performed in a unit of time are usually of a similar kind, but with a potentially different from latencies. So, what does throughput tell us about the system? It is the rate at which the system is performing. When you perform load testing, you can determine the maximum rate at which a particular system can perform. However, this is not a guarantee of the conclusive, overall, and maximum rate of performance.</p><p>Throughput is one of the factors that determine the scalability of a system. The throughput of a higher level task depends on the capacity to spawn multiple such tasks in parallel, and also on the average latency of those tasks. The throughput should be measured during load testing and performance monitoring to determine the peak-measured throughput, and the maximum-sustained throughput. These factors contribute to the scale and performance of a system.</p></div><div class="section" title="Bandwidth"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec013"/>Bandwidth</h2></div></div></div><p>
<span class="strong"><strong>Bandwidth</strong></span> is the<a class="indexterm" id="id036"/> raw data rate over a communication channel, measured in a certain number <a class="indexterm" id="id037"/>of bits per second. This includes not only the payload, but also all the overhead necessary to carry out the communication. Some examples are: Kbits/sec, Mbits/sec, and more. An uppercase B such as KB/sec denotes Bytes, as in kilobytes per second. Bandwidth is often compared to throughput. While bandwidth is the raw capacity, throughput for the same system is the successful task completion rate, which usually involves a round-trip. Note that throughput is for an operation that involves latency. To achieve maximum throughput for a given bandwidth, the communication/protocol overhead and operational latency should be minimal.</p><p>For storage systems (such as hard disks, solid-state drives, and more) the predominant way to measure performance is <a class="indexterm" id="id038"/>
<span class="strong"><strong>IOPS</strong></span> (<span class="strong"><strong>Input-output per second</strong></span>), which is multiplied by the transfer size and represented as bytes per second, or further into MB/sec, GB/sec, and more. IOPS is usually derived for sequential and random workloads for read/write operations.</p><p>Mapping the throughput of a system to the bandwidth of another may lead to dealing with an impedance mismatch between the two. For example, an order processing system may perform the following tasks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Transact with the database on disk </li><li class="listitem" style="list-style-type: disc">Post results over the network to an external system</li></ul></div><p>Depending on the bandwidth of the disk sub-system, the bandwidth of the network, and the execution model of order processing, the throughput may depend not only on the bandwidth of<a class="indexterm" id="id039"/> the disk sub-system and network, but also on how loaded they currently are. Parallelism <a class="indexterm" id="id040"/>and pipelining are common ways to increase the throughput over a given bandwidth.</p></div><div class="section" title="Baseline and benchmark"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec014"/>Baseline and benchmark</h2></div></div></div><p>The performance <a class="indexterm" id="id041"/>
<span class="strong"><strong>baseline</strong></span>, or simply baseline, is the reference point, including measurements of <a class="indexterm" id="id042"/>well-characterized and understood performance parameters for a known configuration. The baseline is used to collect performance measurements for the same parameters that we may benchmark later for another configuration. For example, collecting "throughput distribution over 10 minutes at a load of 50 concurrent threads" is one such performance parameter that we can use for baseline and benchmarking. A baseline is recorded together with the hardware, network, OS and JVM configuration.</p><p>The performance <a class="indexterm" id="id043"/>
<span class="strong"><strong>benchmark</strong></span>, or simply benchmark, is the recording of the performance parameter <a class="indexterm" id="id044"/>measurements under various test conditions. A benchmark can be composed of a performance test suite. A benchmark may collect small to large amounts of data, and may take varying durations depending on the use-cases, scenarios, and environment characteristics.</p><p>A baseline is a result of the benchmark that was conducted at one point in time. However, a benchmark is independent of the baseline.</p></div><div class="section" title="Profiling"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec015"/>Profiling</h2></div></div></div><p>
<span class="strong"><strong>Performance</strong></span>
<a class="indexterm" id="id045"/>
<span class="strong"><strong> profiling</strong></span>
<a class="indexterm" id="id046"/>, or simply profiling, is the analysis of the execution of a program at its runtime. A program can perform poorly for a variety of reasons. A <span class="strong"><strong>profiler</strong></span>
<a class="indexterm" id="id047"/>
can analyze and find out the execution time of various parts of the program. It is possible to put statements in a program manually to print the execution time of the blocks of code, but it gets very cumbersome as you try to refine the code iteratively.</p><p>A profiler is of great assistance to the developer. Going by how profilers work, there are three major kinds—instrumenting, sampling, and event-based.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Event-based profilers</strong></span>: These profilers <a class="indexterm" id="id048"/>work only for selected language <a class="indexterm" id="id049"/>platforms, and provide a good balance between the overhead and results; Java supports event-based profiling via the JVMTI interface.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The instrumenting profilers</strong></span>: These <a class="indexterm" id="id050"/>profilers modify code at either <a class="indexterm" id="id051"/>compile time, or runtime to inject performance counters. They are intrusive by nature and add significant performance overhead. However, you can profile the regions of code very selectively using the instrumenting profilers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The sampling profilers</strong></span>: These <a class="indexterm" id="id052"/>profilers pause the runtime and <a class="indexterm" id="id053"/>collect its state at "sampling intervals". By collecting enough samples, they get to know where the program is <a class="indexterm" id="id054"/>spending most of its time. For example, at a sampling interval of 1 millisecond, the profiler would have collected 1000 samples in a second. A sampling profiler also works for code that executes faster than the sampling interval (as in, the code may perform several iterations of work between the two sampling events), as the frequency of pausing and sampling is proportional to the overall execution time of any code.</li></ul></div><p>Profiling is not meant only for measuring execution time. Capable profilers can provide a view of memory analysis, garbage collection, threads, and more. A combination of such tools is helpful to find memory leaks, garbage collection issues, and so on.</p></div><div class="section" title="Performance optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec016"/>Performance optimization</h2></div></div></div><p>Simply put, <span class="strong"><strong>optimization</strong></span>
<a class="indexterm" id="id055"/> is enhancing <a class="indexterm" id="id056"/>a program's resource <a class="indexterm" id="id057"/>consumption after a performance analysis. The symptoms of a poorly performing program are observed in terms of high latency, low throughput, unresponsiveness, instability, high memory consumption, high CPU consumption, and more. During the performance analysis, one may profile the program in order to identify the bottlenecks and tune the performance incrementally by observing the performance parameters.</p><p>Better and suitable algorithms are an all-around good way to optimize code. The CPU bound code can be optimized with computationally cheaper operations. The cache bound code can try using less memory lookups to keep a good hit ratio. The memory bound code can use an adaptive memory usage and conservative data representation to store in memory for optimization. The I/O bound code can attempt to serialize as little data as possible, and batching of operations will make the operation less chatty for better performance. Parallelism and distribution are other, overall good ways to increase performance.</p></div><div class="section" title="Concurrency and parallelism"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec017"/>Concurrency and parallelism</h2></div></div></div><p>Most of the computer hardware and operating systems that we use today provide concurrency. On the x86 architecture, hardware support for concurrency can be traced as far back as the 80286 chip. <span class="strong"><strong>Concurrency</strong></span>
<a class="indexterm" id="id058"/> is the <a class="indexterm" id="id059"/>simultaneous execution of more than one process on the same computer. In older processors, concurrency was implemented using the context switch by the operating system kernel. When concurrent parts are executed in parallel by the hardware instead of merely the switching context, it is called <span class="strong"><strong>parallelism</strong></span>. Parallelism<a class="indexterm" id="id060"/> is the property of the hardware, though <a class="indexterm" id="id061"/>the software stack must support it in order for you to leverage it in your programs. We must write your program in a concurrent way to exploit the parallelism features of the hardware.</p><p>While concurrency is a natural way to exploit hardware parallelism and speed up operations, it is worth bearing in mind that having significantly higher concurrency than the parallelism that your hardware can support is likely to schedule tasks to varying processor cores thereby, lowering the branch prediction and increasing cache misses.</p><p>At a low level, spawning the processes/threads, mutexes, semaphores, locking, shared memory, and interprocess communication are used for concurrency. The JVM has an excellent support for these concurrency primitives and interthread communication. Clojure has both—the low and higher level concurrency primitives that we will discuss in the concurrency chapter.</p></div><div class="section" title="Resource utilization"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec018"/>Resource utilization</h2></div></div></div><p>R<span class="strong"><strong>esource utilization</strong></span> <a class="indexterm" id="id062"/>is the measure of the server, network, and storage resources <a class="indexterm" id="id063"/>that is consumed by an application. Resources include CPU, memory, disk I/O, network I/O, and more. The application can be analyzed in terms of CPU bound, memory bound, cache bound, and I/O bound tasks. Resource utilization can be derived by means of benchmarking, by measuring the utilization at a given throughput.</p></div><div class="section" title="Workload"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec019"/>Workload</h2></div></div></div><p>
<span class="strong"><strong>Workload</strong></span> is<a class="indexterm" id="id064"/> the quantification <a class="indexterm" id="id065"/>of how much work is there in hand to be carried out by the application. It is measured in the total numbers of users, the concurrent active users, the transaction volume, the data volume, and more. Processing a workload should take in to account the load conditions, such as how much data the database currently holds, how filled up the message queues are, the backlog of I/O tasks after which the new load will be processed, and more.</p></div></div>
<div class="section" title="The latency numbers that every programmer should know"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec011"/>The latency numbers that every programmer should know</h1></div></div></div><p>Hardware and software <a class="indexterm" id="id066"/>have progressed over the years. Latencies for various operations put things in perspective. The latency numbers for the year 2015, reproduced with the permission of Aurojit Panda and Colin Scott of Berkeley University (<a class="ulink" href="http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html">http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html</a>). Latency numbers that every <a class="indexterm" id="id067"/>programmer should know are as shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Operation</p>
</th><th style="text-align: left" valign="bottom">
<p>Time taken as of 2015</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>L1 cache reference</p>
</td><td style="text-align: left" valign="top">
<p>1ns (nano second)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Branch mispredict</p>
</td><td style="text-align: left" valign="top">
<p>3 ns</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>L2 cache reference</p>
</td><td style="text-align: left" valign="top">
<p>4 ns</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mutex lock/unlock</p>
</td><td style="text-align: left" valign="top">
<p>17 ns</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Compress 1KB with Zippy</p>
<p>(Zippy/Snappy: <a class="ulink" href="http://code.google.com/p/snappy/">http://code.google.com/p/snappy/</a>)</p>
</td><td style="text-align: left" valign="top">
<p>2μs (1000 ns = 1μs: micro second)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Send 2000 bytes over the commodity network</p>
</td><td style="text-align: left" valign="top">
<p>200ns (that is, 0.2μs)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SSD random read</p>
</td><td style="text-align: left" valign="top">
<p>16 μs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Round-trip in the same datacenter</p>
</td><td style="text-align: left" valign="top">
<p>500 μs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Read 1,000,000 bytes sequentially from SSD</p>
</td><td style="text-align: left" valign="top">
<p>200 μs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Disk seek</p>
</td><td style="text-align: left" valign="top">
<p>4 ms (1000 μs = 1 ms)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Read 1,000,000 bytes sequentially from disk</p>
</td><td style="text-align: left" valign="top">
<p>2 ms</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Packet roundtrip CA to Netherlands</p>
</td><td style="text-align: left" valign="top">
<p>150 ms</p>
</td></tr></tbody></table></div><p>The preceding table shows the operations in a computer vis-a-vis the latency incurred due to the operation. When a CPU core processes some data in a CPU register, it may take a few CPU cycles (for reference, a 3 GHz CPU runs 3000 cycles per nanosecond), but the moment it has to fall back on L1 or L2 cache, the latency becomes thousands of times slower. The preceding table does not show main memory access latency, which is roughly 100 ns (it varies, based on the access pattern)—about 25 times slower than the L2 cache.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec012"/>Summary</h1></div></div></div><p>We learned about the basics of what it is like to think more deeply about performance. We saw the common performance vocabulary, and also the use cases by which performance aspects might vary. We concluded by looking at the performance numbers for the different hardware components, which is how performance benefits reach our applications. In the next chapter, we will dive into the performance aspects of the various Clojure abstractions.</p></div>
<div class="chapter" title="Chapter&#xA0;2.&#xA0;Clojure Abstractions"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 2. Clojure Abstractions</h1></div></div></div><p>Clojure has four founding ideas. Firstly, it was set up to be a functional language. It is not pure (as in purely functional), but emphasizes immutability. Secondly, it is a dialect of Lisp; Clojure is malleable enough that users can extend the language without waiting for the language implementers to add new features and constructs. Thirdly, it was built to leverage concurrency for the new generation challenges. Lastly, it was designed to be a hosted language. As of today, Clojure implementations exist for the JVM, CLR, JavaScript, Python, Ruby, and Scheme. Clojure blends seamlessly with its host language.</p><p>Clojure is rich in abstractions. Though the syntax itself is very minimal, the abstractions are finely grained, mostly composable, and designed to tackle a wide variety of concerns in the least complicated way. In this chapter, we will discuss the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Performance characteristics of non-numeric scalars</li><li class="listitem" style="list-style-type: disc">Immutability and epochal time model paving the way for performance by isolation</li><li class="listitem" style="list-style-type: disc">Persistent data structures and their performance characteristics</li><li class="listitem" style="list-style-type: disc">Laziness and its impact on performance</li><li class="listitem" style="list-style-type: disc">Transients as a high-performance, short-term escape hatch</li><li class="listitem" style="list-style-type: disc">Other abstractions, such as tail recursion, protocols/types, multimethods, and many more</li></ul></div><div class="section" title="Non-numeric scalars and interning"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec013"/>Non-numeric scalars and interning</h1></div></div></div><p>Strings<a class="indexterm" id="id068"/> and<a class="indexterm" id="id069"/> characters<a class="indexterm" id="id070"/> in Clojure are the same as in Java. The string literals are<a class="indexterm" id="id071"/> implicitly interned. Interning is a way of storing only the unique values in the heap and sharing the reference everywhere it is required. Depending on the JVM vendor and the version of Java you use, the interned data may be stored in a string pool, Permgen, ordinary heap, or some special area in the heap marked for interned data. Interned data is subject to garbage collection when not in use, just like ordinary objects. Take a look at the following code:</p><div class="informalexample"><pre class="programlisting">user=&gt; (identical? "foo" "foo")  ; literals are automatically interned
true
user=&gt; (identical? (String. "foo") (String. "foo"))  ; created string is not interned
false
user=&gt; (identical? (.intern (String. "foo")) (.intern (String. "foo")))
true
user=&gt; (identical? (str "f" "oo") (str "f" "oo"))  ; str creates string
false
user=&gt; (identical? (str "foo") (str "foo"))  ; str does not create string for 1 arg
true
user=&gt; (identical? (read-string "\"foo\"") (read-string "\"foo\""))  ; not interned
false
user=&gt; (require '[clojure.edn :as edn])  ; introduced in Clojure 1.5
nil
user=&gt; (identical? (edn/read-string "\"foo\"") (edn/read-string "\"foo\""))
false</pre></div><p>Note that <code class="literal">identical?</code> in Clojure is the same as <code class="literal">==</code> in Java. The benefit of interning a string is that <a class="indexterm" id="id072"/>there is no memory allocation overhead for duplicate <a class="indexterm" id="id073"/>strings. Commonly, applications on the JVM spend quite some time on string processing. So, it makes sense to have them interned whenever there is a chance of duplicate strings being simultaneously processed. Most of the JVM implementations today have an extremely fast intern operation; however, you should measure the overhead for your JVM if you have an older version.</p><p>Another benefit of string interning<a class="indexterm" id="id074"/> is that when you know that two string tokens are interned, you can compare them faster for equality using <code class="literal">identical?</code> than non-interned string tokens. The equivalence function <code class="literal">=</code> first checks for identical references before conducting a content check.</p><p>Symbols<a class="indexterm" id="id075"/> in Clojure always contain interned string references within them, so generating a symbol from a given string is nearly as fast as interning a string. However, two symbols created from the same string will not be identical:</p><div class="informalexample"><pre class="programlisting">user=&gt; (identical? (.intern "foo") (.intern "foo"))
true
user=&gt; (identical? (symbol "foo") (symbol "foo"))
false
user=&gt; (identical? (symbol (.intern "foo")) (symbol (.intern "foo")))
false</pre></div><p>Keywords<a class="indexterm" id="id076"/> are, on the basis of their implementation, built on top of symbols and are designed to work with the <code class="literal">identical?</code> function for equivalence. So, comparing keywords for equality using <code class="literal">identical?</code> would be faster, just as with interned string tokens.</p><p>Clojure is increasingly <a class="indexterm" id="id077"/>being used for large-volume data processing, which includes text and composite data structures. In many cases, the data is either stored as JSON or <a class="indexterm" id="id078"/>EDN (<a class="ulink" href="http://edn-format.org">http://edn-format.org</a>). When processing such data, you can save memory by interning strings or using symbols/keywords. Remember that string tokens read from such data would not be automatically interned, whereas the symbols and keywords read from EDN data would invariably be interned. You may come across such situations when dealing with relational or NoSQL databases, web services, CSV or XML files, log parsing, and so on.</p><p>Interning is linked to the JVM <a class="indexterm" id="id079"/>
<span class="strong"><strong>Garbage Collection</strong></span> (<span class="strong"><strong>GC</strong></span>), which, in <a class="indexterm" id="id080"/>turn, is closely linked to performance. When you do not intern the string data and let duplicates exist, they end up being allocated on the heap. More heap usage leads to GC overhead. Interning a string has a tiny but measurable and upfront performance overhead, whereas GC is often unpredictable and unclear. GC performance, in most JVM implementations, has not increased in a similar proportion to the performance advances in hardware. So, often, effective performance depends on preventing GC from becoming the bottleneck, which in most cases means minimizing it.</p></div></div>
<div class="section" title="Identity, value, and epochal time model"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec014"/>Identity, value, and epochal time model</h1></div></div></div><p>One of the principal virtues of <a class="indexterm" id="id081"/>Clojure is its simple design that results in malleable, beautiful <a class="indexterm" id="id082"/>composability. Using symbols in place of pointers is a <a class="indexterm" id="id083"/>programming practice that has existed for several decades now. It has found widespread adoption in several imperative languages. Clojure dissects that notion in order to uncover the core concerns that need to be addressed. The following subsections illustrate this aspect of Clojure.</p><p>We program using logical entities to represent values. For example, a value of <code class="literal">30</code> means nothing unless it is associated with a logical entity, let's say <code class="literal">age</code>. The logical entity <code class="literal">age</code> is the identity here. Now, even though <code class="literal">age</code> represents a value, the value may change with time; this brings us to the notion of <code class="literal">state</code>, which represents the value of the identity at a certain time. Hence, <code class="literal">state</code> is a function of time and is causally related to what we do in the program. Clojure's power lies in binding an identity with its value that holds true at the time and the identity remains isolated from any new value it may represent later. We will discuss state management in <a class="link" href="ch12.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>.</p><div class="section" title="Variables and mutation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec020"/>Variables and mutation</h2></div></div></div><p>If you have previously worked with an imperative language (C/C++, Java, and so on), you may be familiar with the concept of a variable. A <span class="strong"><strong>variable</strong></span>
<a class="indexterm" id="id084"/> is a reference to a block of memory. When we update its value, we essentially update the place in memory where the value is stored. The variable continues to point to the place where the older version of the value was stored. So, essentially, a variable is an alias for the place of storage of values.</p><p>A little analysis would reveal that variables are strongly linked to the processes that read or mutate their values. Every mutation is a state transition. The processes that read/update the variable should be aware of the possible states of the variable to make sense of the state. Can you see a problem here? It conflates identity and state! It is impossible to refer to a value or a state in time when dealing with a variable—the value could change at any time unless you have complete control over the process accessing it. The mutability model does not accommodate the concept of time that causes its state transition.</p><p>The issues with mutability<a class="indexterm" id="id085"/> do not stop here. When you have a composite data structure containing mutable variables, the entire data structure becomes mutable. How can we mutate it without potentially undermining the other processes that might be observing it? How can we share this data structure with concurrent processes? How can we use this data structure as a key in a hash-map? This data structure does not convey anything. Its meaning could change with mutation! How do we send such a thing to another process without also compensating for the time, which can mutate it in different ways?</p><p>Immutability<a class="indexterm" id="id086"/> is an important tenet of functional programming. It not only simplifies the programming model, but also paves the way for safety and concurrency. Clojure supports immutability throughout the language. Clojure also supports fast, mutation-oriented data structures as well as thread-safe state management via concurrency primitives. We will discuss these topics in the forthcoming sections and chapters.</p></div><div class="section" title="Collection types"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec021"/>Collection types</h2></div></div></div><p>There are a few types of collections<a class="indexterm" id="id087"/> in Clojure, which are categorized based on their properties. The following Venn diagram depicts this categorization on the basis of whether the collections are counted (so that <code class="literal">counted?</code> returns <code class="literal">true</code>) or associative (so that <code class="literal">associative?</code> returns <code class="literal">true</code>) or sequential (so that <code class="literal">sequential?</code> returns <code class="literal">true</code>):</p><div class="mediaobject"><img alt="Collection types" src="graphics/B04596_02_01.jpg"/></div><p>The previous diagram illustrates the characteristics that different kinds of data structures share. The sequential structures<a class="indexterm" id="id088"/> let us iterate over the items in the collection, the item count of counted structures can be found constant with respect to time, and associative structures can be looked at with keys for corresponding values. The <span class="strong"><strong>CharSequence</strong></span> box<a class="indexterm" id="id089"/> shows the character sequence Java types that can be converted to a Clojure sequence using (<code class="literal">seq charseq</code>).</p></div></div>
<div class="section" title="Persistent data structures"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec015"/>Persistent data structures</h1></div></div></div><p>As we've noticed in the <a class="indexterm" id="id090"/>previous section, Clojure's data structures are not only immutable, but can produce new values without impacting the old version. Operations produce these new values in such a way that old values remain accessible; the new version is produced in compliance with the complexity guarantees of that data structure, and both the old and new versions continue to meet the complexity guarantees. The operations can be recursively applied and can still meet the complexity guarantees. Such immutable data structures as the ones provided by Clojure are called <span class="strong"><strong>persistent data structures</strong></span>. They are "persistent", as in, when a new version is created, both the old and new versions "persist" in terms of both the value and complexity guarantee. They have nothing to do with storage or durability of data. Making changes to the old version doesn't impede working with the new version and vice versa. Both versions persist in a similar way.</p><p>Among the publications that have inspired the implementation of Clojure's persistent data structures, two of them are well known. Chris Okasaki's <span class="emphasis"><em>Purely Functional Data Structures</em></span> has influenced <a class="indexterm" id="id091"/>the implementation of persistent data structures and lazy sequences/operations. Clojure's persistent queue implementation is adapted from Okasaki's <span class="emphasis"><em>Batched Queues</em></span>. Phil Bagwell's <span class="emphasis"><em>Ideal Hash Tries</em></span>, though meant for mutable and imperative data structures, was adapted to implement Clojure's persistent map/vector/set.</p><div class="section" title="Constructing lesser-used data structures"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec022"/>Constructing lesser-used data structures</h2></div></div></div><p>Clojure supports a <a class="indexterm" id="id092"/>well-known literal syntax for lists, vectors, sets, and maps. Shown in the following list are some less-used methods for creating other data structures:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Map (<code class="literal">PersistentArrayMap</code> and <code class="literal">PersistentHashMap</code>):<div class="informalexample"><pre class="programlisting">{:a 10 :b 20}  ; array-map up to 8 pairs
{:a 1 :b 2 :c 3 :d 4 :e 5 :f 6 :g 7 :h 8 :i 9}  ; hash-map for 9 or more pairs</pre></div></li><li class="listitem" style="list-style-type: disc">Sorted map (<code class="literal">PersistentTreeMap</code>):<div class="informalexample"><pre class="programlisting">(sorted-map :a 10 :b 20 :c 30)  ; (keys ..) should return sorted</pre></div></li><li class="listitem" style="list-style-type: disc">Sorted set (<code class="literal">PersistentTreeSet</code>):<div class="informalexample"><pre class="programlisting">(sorted-set :a :b :c)</pre></div></li><li class="listitem" style="list-style-type: disc">Queue (<code class="literal">PersistentQueue</code>):<div class="informalexample"><pre class="programlisting">(import 'clojure.lang.PersistentQueue)
(reduce conj PersistentQueue/EMPTY [:a :b :c :d])  ; add to queue
(peek queue)  ; read from queue
(pop queue)  ; remove from queue</pre></div></li></ul></div><p>As you can see, abstractions such as <code class="literal">TreeMap</code> (sorted by key), <code class="literal">TreeSet</code> (sorted by element), and <code class="literal">Queue</code> should be instantiated by calling their respective APIs.</p></div><div class="section" title="Complexity guarantee"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec023"/>Complexity guarantee</h2></div></div></div><p>The following table gives a summary of the complexity guarantees<a class="indexterm" id="id093"/> (using the Big-O notation) of various kinds of persistent data structures in Clojure:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Operation</p>
</th><th style="text-align: left" valign="bottom">
<p>PersistentList</p>
</th><th style="text-align: left" valign="bottom">
<p>PersistentHashMap</p>
</th><th style="text-align: left" valign="bottom">
<p>PersistentArrayMap</p>
</th><th style="text-align: left" valign="bottom">
<p>PersistentVector</p>
</th><th style="text-align: left" valign="bottom">
<p>PersistentQueue</p>
</th><th style="text-align: left" valign="bottom">
<p>PersistentTreeMap</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">count</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">conj</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">first</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">rest</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">doseq</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">nth</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">last</code>
</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top">
<p>O(n)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">get</code>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(log n)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">assoc</code>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(log n)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dissoc</code>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(log n)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">peek</code>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">pop</code>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>O(&lt;7)</p>
</td><td style="text-align: left" valign="top">
<p>O(1)</p>
</td><td style="text-align: left" valign="top"> </td></tr></tbody></table></div><p>A <span class="strong"><strong>list</strong></span>
<a class="indexterm" id="id094"/> is a sequential <a class="indexterm" id="id095"/>data structure. It provides constant time access for count and for anything regarding the first element only. For example, <code class="literal">conj</code> adds the element to the head and guarantees <span class="emphasis"><em>O(1)</em></span> complexity. Similarly, <code class="literal">first</code> and <code class="literal">rest</code> provide <span class="emphasis"><em>O(1)</em></span> guarantees too. Everything else provides an <span class="emphasis"><em>O(n)</em></span> complexity guarantee.</p><p>Persistent hash-maps<a class="indexterm" id="id096"/> and <a class="indexterm" id="id097"/>vectors use the trie data structure with a branching factor of 32 under the hood. So, even though the complexity is <span class="emphasis"><em>O(log</em></span>
<span class="emphasis"><em><sub>32</sub></em></span>
<span class="emphasis"><em> n)</em></span>, only 2<sup>32</sup> hash codes can fit into the trie nodes. Hence, log<sub>32</sub> 2<sup>32</sup>, which turns out to be <code class="literal">6.4</code> and is less than <code class="literal">7</code>, is the worst-case complexity and can be considered near-constant time. As the trie grows larger, the portion to copy gets proportionately tiny due to structure sharing. Persistent hash-set implementation is also based on hash-map; hence, the hash-sets share the characteristics of the hash-maps. In a persistent vector, the last incomplete node is placed at the tail, which is always directly accessible from the root. This makes using <code class="literal">conj</code> to the end a constant time operation.</p><p>Persistent tree-maps<a class="indexterm" id="id098"/> and tree-sets <a class="indexterm" id="id099"/>are basically sorted maps and sets respectively. Their implementation uses red-black trees and is generally more expensive than hash-maps and hash-sets. A persistent queue uses a persistent vector under the hood for adding new elements. Removing an element from a persistent queue takes the head off <code class="literal">seq</code>, which is created from the vector where new elements are added.</p><p>The complexity of an algorithm over a data structure is not an absolute measure of its performance. For example, working with hash-maps involves computing the hashCode, which is not included in the complexity guarantee. Our choice of data structures should be based on the actual use case. For example, when should we use a list instead of a vector? Probably when we need sequential or <span class="strong"><strong>last-in-first-out</strong></span> (<span class="strong"><strong>LIFO</strong></span>)<a class="indexterm" id="id0100"/> access, or when constructing an <span class="strong"><strong>abstract-syntax-tree</strong></span> (<span class="strong"><strong>AST</strong></span>)<a class="indexterm" id="id0101"/> for a function call.</p><div class="section" title="O(&lt;7) implies near constant time"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec006"/>O(&lt;7) implies near constant time</h3></div></div></div><p>You may know that the <span class="strong"><strong>Big-O</strong></span> notation<a class="indexterm" id="id0102"/> is used to express the upper bound (worst case) of the efficiency of any algorithm. The variable <span class="emphasis"><em>n</em></span> is used to express the number of elements in the algorithm. For example, a binary search on a sorted associative collection, such as a sorted vector, is a logarithmic time, that is an <span class="emphasis"><em>O(log</em></span>
<span class="emphasis"><em><sub>2</sub></em></span>
<span class="emphasis"><em> n)</em></span> or simply an <span class="emphasis"><em>O(log n)</em></span> algorithm. Since there can be a maximum of 2<sup>32</sup> (technically 2<sup>31</sup> due to a signed positive integer) elements in a Java collection and log<sub>2</sub> 2<sup>32</sup> is 32, the binary search can be <span class="emphasis"><em>O(≤32)</em></span> in the worst case. Similarly, though operations on persistent collections are O(log<sub>32</sub> n), in the worst case they actually turn out to be O(log<sub>32</sub> 2<sup>32</sup>) at maximum, which is <span class="emphasis"><em>O(&lt;7)</em></span>. Note that this is much lower than logarithmic time and approaches near constant time. This implies not so bad performance for persistent collections even in the worst possible scenario.</p></div></div><div class="section" title="The concatenation of persistent data structures"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec024"/>The concatenation of persistent data structures</h2></div></div></div><p>While persistent<a class="indexterm" id="id0103"/> data structures have excellent <a class="indexterm" id="id0104"/>performance characteristics, the concatenation of two persistent data structures has been a linear time <span class="emphasis"><em>O(N)</em></span> operation, except for some recent developments. The <code class="literal">concat</code> function, as of Clojure 1.7, still provides linear time concatenation. Experimental work on <span class="strong"><strong>Relaxed Radix Balanced</strong></span> (<span class="strong"><strong>RRB</strong></span>)<a class="indexterm" id="id0105"/> trees is going on in the <a class="indexterm" id="id0106"/>
<span class="strong"><strong>core.rrb-vector</strong></span> contrib project (<a class="ulink" href="https://github.com/clojure/core.rrb-vector">https://github.com/clojure/core.rrb-vector</a>), which may provide logarithmic time <span class="emphasis"><em>O(log N)</em></span> concatenation. Readers interested in the details should refer to the following links:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The RRB-trees paper<a class="indexterm" id="id0107"/> at <a class="ulink" href="http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf">http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf</a></li><li class="listitem" style="list-style-type: disc">Phil Bagwell's talk at <a class="ulink" href="http://www.youtube.com/watch?v=K2NYwP90bNs">http://www.youtube.com/watch?v=K2NYwP90bNs</a></li><li class="listitem" style="list-style-type: disc">Tiark Rompf's talk at <a class="ulink" href="http://skillsmatter.com/podcast/scala/fast-concatenation-immutable-vectors">http://skillsmatter.com/podcast/scala/fast-concatenation-immutable-vectors</a></li></ul></div></div></div>
<div class="section" title="Sequences and laziness"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec016"/>Sequences and laziness</h1></div></div></div><div class="blockquote"><table border="0" cellpadding="0" cellspacing="0" class="blockquote" summary="Block quote" width="100%"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>"A seq is like a logical cursor."</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td align="right" colspan="2" style="text-align: center" valign="top">--<span class="attribution"><span class="emphasis"><em>Rich Hickey</em></span></span></td></tr></table></div><p>
<span class="strong"><strong>Sequences</strong></span> (commonly known as <span class="strong"><strong>seqs</strong></span>) are a way to <a class="indexterm" id="id0108"/>sequentially consume a succession of data. As with iterators, they let a user begin consuming elements from the head and proceed realizing one element after another. However, unlike iterators, sequences are immutable. Also, since sequences are only a view of the underlying data, they do not modify the storage structure of the data.</p><p>What makes sequences stand apart is they are not data structures per se; rather, they are a data abstraction over a stream of data. The data may be produced by an algorithm or a data source connected to an I/O operation. For example, the <code class="literal">resultset-seq</code> function accepts a <code class="literal">java.sql.ResultSet</code> JDBC instance as an argument and produces lazily realized rows of data as <code class="literal">seq</code>.</p><p>Clojure data structures can be turned into sequences using the <code class="literal">seq</code> function. For example, (<code class="literal">seq [:a :b :c :d]</code>) returns a sequence. Calling <code class="literal">seq</code> over an empty collection returns nil.</p><p>Sequences can be consumed by the following functions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">first</code>: This<a class="indexterm" id="id0109"/> returns the head of the sequence</li><li class="listitem" style="list-style-type: disc"><code class="literal">rest</code>: This returns the<a class="indexterm" id="id0110"/> remaining sequence, even if it's empty, after removing the head</li><li class="listitem" style="list-style-type: disc"><code class="literal">next</code>: This returns the <a class="indexterm" id="id0111"/>remaining sequence or nil, if it's empty, after removing the head</li></ul></div><div class="section" title="Laziness"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec025"/>Laziness</h2></div></div></div><p>Clojure is a strict (as in, the opposite of "lazy") language, which can choose to explicitly make use of laziness when required. Anybody can create a lazily evaluated sequence using the <code class="literal">lazy-seq</code> macro. Some Clojure operations over collections, such as <code class="literal">map</code>, <code class="literal">filter</code>, and more are intentionally lazy.</p><p>
<span class="strong"><strong>Laziness</strong></span>
<a class="indexterm" id="id0112"/> simply means that the value is not computed until actually required. Once the value is computed, it is cached so that any future reference to the value need not re-compute it. The caching of the value is called <a class="indexterm" id="id0113"/>
<span class="strong"><strong>memoization</strong></span>. Laziness and memoization often go hand in hand.</p><div class="section" title="Laziness in data structure operations"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec007"/>Laziness in data structure operations</h3></div></div></div><p>Laziness and <a class="indexterm" id="id0114"/>memoization together form an extremely useful <a class="indexterm" id="id0115"/>combination to keep the single-threaded performance of functional algorithms comparable to its imperative counterparts. For an example, consider the following Java code:</p><div class="informalexample"><pre class="programlisting">List&lt;String&gt; titles = getTitles();
int goodCount = 0;
for (String each: titles) {
  String checksum = computeChecksum(each);
  if (verifyOK(checksum)) {
    goodCount++;
  }
}</pre></div><p>As is clear from the preceding snippet, it has a linear time complexity, that is, <span class="emphasis"><em>O(n)</em></span>, and the whole operation is performed in a single pass. The comparable Clojure code is as follows:</p><div class="informalexample"><pre class="programlisting">(-&gt;&gt; (get-titles)
  (map compute-checksum)
  (filter verify-ok?)
  count)</pre></div><p>Now, since we know <code class="literal">map</code> and <code class="literal">filter</code> are lazy, we can deduce that the Clojure version also has linear time complexity, that is, <span class="emphasis"><em>O(n)</em></span>, and finishes the task in one pass with no significant memory overhead. Imagine, for a moment, that <code class="literal">map</code> and <code class="literal">filter</code> are not lazy—what would be the complexity then? How many passes would it make? It's not just that map and filter would both have taken one pass, that is, <span class="emphasis"><em>O(n)</em></span>, each; they would each have taken as much memory as the original collection in the worst case, due to storing the intermediate results.</p><p>It is important to know the value of laziness and memoization in an immutability-emphasizing functional language such as Clojure. They form a basis for <span class="strong"><strong>amortization</strong></span>
<a class="indexterm" id="id0116"/> in persistent data structures, which is about focusing on the overall performance of a composite operation instead of microanalyzing the performance of each operation in it; the operations are tuned to perform faster in those operations that matter the most.</p><p>Another<a class="indexterm" id="id0117"/> important bit of detail is that when a lazy sequence<a class="indexterm" id="id0118"/> is realized, the data is memoized and stored. On the JVM, all the heap references that are reachable in some way are not garbage collected. So, as a consequence, the entire data structure is kept in the memory unless you lose the head of the sequence. When working with lazy sequences using local bindings, make sure you don't keep referring to the lazy sequence from any of the locals. When writing functions that may accept lazy sequence(s), take care that any reference to the lazy <code class="literal">seq</code> does not outlive the execution of the function in the form of a closure or some such.</p></div><div class="section" title="Constructing lazy sequences"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec008"/>Constructing lazy sequences</h3></div></div></div><p>Now that we know <a class="indexterm" id="id0119"/>what lazy sequences are, let's try to create a retry counter that should return true only as many times as the retry can be performed. This is shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn retry? [n]
  (if (&lt;= n 0)
    (cons false (lazy-seq (retry? 0)))
    (cons true (lazy-seq (retry? (dec n))))))</pre></div><p>The <code class="literal">lazy-seq</code> macro makes sure that the stack is not used for recursion. We can see that this function would return endless values. Hence, in order to inspect what it returns, we should limit the number of elements as shown in the following code:</p><div class="informalexample"><pre class="programlisting">user=&gt; (take 7 (retry? 5))
(true true true true true false false)</pre></div><p>Now, let's try using it in a mock fashion:</p><div class="informalexample"><pre class="programlisting">(loop [r (retry? 5)]
  (if-not (first r)
    (println "No more retries")
    (do
      (println "Retrying")
      (recur (rest r)))))</pre></div><p>As expected, the output should print <code class="literal">Retrying</code> five times before printing <code class="literal">No more retries</code> and exiting as follows:</p><div class="informalexample"><pre class="programlisting">Retrying
Retrying
Retrying
Retrying
Retrying
No more retries
nil</pre></div><p>Let's take another simpler example of constructing a lazy sequence, which gives us a countdown from a specified number to zero:</p><div class="informalexample"><pre class="programlisting">(defn count-down [n]
  (if (&lt;= n 0)
    '(0)
    (cons n (lazy-seq (count-down (dec n))))))</pre></div><p>We can inspect the values it returns as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (count-down 8)
(8 7 6 5 4 3 2 1 0)</pre></div><p>Lazy sequences<a class="indexterm" id="id0120"/> can loop indefinitely without exhausting the stack and can come in handy when working with other lazy operations. To maintain a balance between space-saving and performance, consuming lazy sequences results in the chunking of elements by a factor of 32. That means lazy seqs are realized in a chunk-size of 32, even though they are consumed sequentially.</p><div class="section" title="Custom chunking"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec003"/>Custom chunking</h4></div></div></div><p>The default <a class="indexterm" id="id0121"/>chunk size 32 may not be optimum for all lazy sequences—you can override the chunking behavior when you need to. Consider the following snippet (adapted from Kevin Downey's public gist at <a class="ulink" href="https://gist.github.com/hiredman/324145">https://gist.github.com/hiredman/324145</a>):</p><div class="informalexample"><pre class="programlisting">(defn chunked-line-seq
  "Returns the lines of text from rdr as a chunked[size] sequence of strings.
  rdr must implement java.io.BufferedReader."
  [^java.io.BufferedReader rdr size]
  (lazy-seq
    (when-let [line (.readLine rdr)]
      (chunk-cons
        (let [buffer (chunk-buffer size)]
          (chunk-append buffer line)
          (dotimes [i (dec size)]
            (when-let [line (.readLine rdr)]
              (chunk-append buffer line)))
  (chunk buffer))
(chunked-line-seq rdr size)))))</pre></div><p>As per the previous snippet, the user is allowed to pass a chunk size that is used to produce the lazy sequence. A larger chunk size may be useful when processing large text files, such as when processing CSV or log files. You would notice the following four less-known functions used in the snippet:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">clojure.core/chunk-cons</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clojure.core/chunk-buffer</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clojure.core/chunk-append</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clojure.core/chunk</code></li></ul></div><p>While <code class="literal">chunk-cons</code> is the <a class="indexterm" id="id0122"/>equivalent of <code class="literal">clojure.core/cons</code> for chunked sequences, <code class="literal">chunk-buffer</code> creates a mutable chunk buffer (controls the chunk size), <code class="literal">chunk-append</code> appends an item to the end of a mutable chunk buffer, and chunk turns a mutable chunk buffer into an immutable chunk.</p><p>The <code class="literal">clojure.core</code> namespace has several functions related to chunked sequences listed as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">chunk</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunk-rest</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunk-cons</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunk-next</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunk-first</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunk-append</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunked-seq?</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">chunk-buffer</code></li></ul></div><p>These functions are not documented, so although I would encourage you to study their source code to understand what they do, I would advise you not to make any assumptions about their support in future Clojure versions.</p></div><div class="section" title="Macros and closures"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec004"/>Macros and closures</h4></div></div></div><p>Often, we define a <a class="indexterm" id="id0123"/>macro<a class="indexterm" id="id0124"/> so as to turn the parameter body of code into a <a class="indexterm" id="id0125"/>closure and delegate it to a function. See the <a class="indexterm" id="id0126"/>following example:</p><div class="informalexample"><pre class="programlisting">(defmacro do-something
  [&amp; body]
  `(do-something* (fn [] ~@body)))</pre></div><p>When using such code, if the body binds a local to a lazy sequence it may be retained longer than necessary, likely with bad consequences on memory consumption and performance. Fortunately, this can be easily fixed:</p><div class="informalexample"><pre class="programlisting">(defmacro do-something
  [&amp; body]
  `(do-something* (^:once fn* [] ~@body)))</pre></div><p>Notice the <code class="literal">^:once</code> hint and the <code class="literal">fn*</code> macro, which make the Clojure compiler clear the closed-over references, thus avoiding the problem. Let's see this in action (Alan Malloy's example from <a class="ulink" href="https://groups.google.com/d/msg/clojure/Ys3kEz5c_eE/3St2AbIc3zMJ">https://groups.google.com/d/msg/clojure/Ys3kEz5c_eE/3St2AbIc3zMJ</a>):</p><div class="informalexample"><pre class="programlisting">user&gt; (let [x (for [n (range)] (make-array Object 10000))
      f (^:once fn* [] (nth x 1e6))]  ; using ^:once
        (f))
#&lt;Object[] [Ljava.lang.Object;@402d3105&gt;
user&gt; (let [x (for [n (range)] (make-array Object 10000))
            f (fn* [] (nth x 1e6))]         ; not using ^:once
        (f))
OutOfMemoryError GC overhead limit exceeded</pre></div><p>The manifestation<a class="indexterm" id="id0127"/> of the <a class="indexterm" id="id0128"/>previous condition depends on the<a class="indexterm" id="id0129"/> available heap space. This issue is tricky to detect as it <a class="indexterm" id="id0130"/>only raises <code class="literal">OutOfMemoryError</code>, which is easy to misunderstand as a heap space issue instead of a memory leak. As a preventive measure, I would suggest using <code class="literal">^:once</code> with <code class="literal">fn*</code> in all cases where you close over any potentially lazy sequence.</p></div></div></div></div>
<div class="section" title="Transducers"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec017"/>Transducers</h1></div></div></div><p>Clojure 1.7 introduced a new abstraction called transducers for "composable algorithmic transformations", commonly used to apply a series of transformations over collections. The idea of transducers follows from<a class="indexterm" id="id0131"/> the <span class="strong"><strong>reducing function</strong></span>, which accepts arguments of the form (<code class="literal">result, input</code>) and returns <code class="literal">result</code>. A reducing function is what we typically use with reduce. A <span class="strong"><strong>transducer</strong></span>
<a class="indexterm" id="id0132"/> accepts a reducing function, wraps/composes over its functionality to provide something extra, and returns another reducing function.</p><p>The functions in <code class="literal">clojure.core</code> that deal with collections have acquired an <code class="literal">arity-1</code> variant, which returns a transducer, namely <code class="literal">map</code>, <code class="literal">cat</code>, <code class="literal">mapcat</code>, <code class="literal">filter</code>, <code class="literal">remove</code>, <code class="literal">take</code>, <code class="literal">take-while</code>, <code class="literal">take-nth</code>, <code class="literal">drop</code>, <code class="literal">drop-while</code>, <code class="literal">replace</code>, <code class="literal">partition-by</code>, <code class="literal">partition-all</code>, <code class="literal">keep</code>, <code class="literal">keep-indexed</code>, <code class="literal">dedupe</code> and <code class="literal">random-sample</code>.</p><p>Consider the following few examples, all of which do the same thing:</p><div class="informalexample"><pre class="programlisting">user=&gt; (reduce ((filter odd?) +) [1 2 3 4 5])
9
user=&gt; (transduce (filter odd?) + [1 2 3 4 5])
9
user=&gt; (defn filter-odd? [xf]
         (fn
           ([] (xf))
           ([result] (xf result))
           ([result input] (if (odd? input)
                               (xf result input)
                               result))))
#'user/filter-odd?
user=&gt; (reduce (filter-odd? +) [1 2 3 4 5])
9</pre></div><p>Here, (<code class="literal">filter odd?</code>) returns a transducer—in the first example the transducer wraps over the reducer function <code class="literal">+</code> to return another combined reducing function. While we use the ordinary <code class="literal">reduce</code> function in the first example, in the second example we use the <code class="literal">transduce</code> function that accepts a transducer as an argument. In the third example, we write a transducer <code class="literal">filter-odd?</code>, which emulates what (<code class="literal">filter odd?</code>) does. Let's see how the performance <a class="indexterm" id="id0133"/>varies between traditional and transducer versions:</p><div class="informalexample"><pre class="programlisting">;; traditional way
user=&gt; (time (dotimes [_ 10000] (reduce + (filter odd? (range 10000)))))
"Elapsed time: 2746.782033 msecs"
nil
;; using transducer
(def fodd? (filter odd?))
user=&gt; (time (dotimes [_ 10000] (transduce fodd? + (range 10000))))
"Elapsed time: 1998.566463 msecs"
nil</pre></div><div class="section" title="Performance characteristics"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec026"/>Performance characteristics</h2></div></div></div><p>The key point behind <a class="indexterm" id="id0134"/>transducers is how orthogonal each<a class="indexterm" id="id0135"/> transformation is allowed to be, yet highly composable also. At the same time, transformations can happen in lockstep for the entire sequence instead of each operation producing lazy chunked sequences. This often causes significant performance benefits with transducers. Lazy sequences are still going to be useful when the final result is too large to realize at once—for other use cases transducers should fit the need aptly with improved performance. Since the core functions have been overhauled to work with transducers, it makes sense to model transformations more often than not in terms of transducers.</p></div></div>
<div class="section" title="Transients"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec018"/>Transients</h1></div></div></div><p>Earlier in this <a class="indexterm" id="id0136"/>chapter, we discussed the virtues of immutability and the pitfalls of mutability. However, even though mutability is fundamentally unsafe, it also has very good single-threaded performance. Now, what if there was a way to restrict the mutable operation in a local context in order to provide safety guarantees? That would be equivalent to combining the performance advantage and local safety guarantees. That is exactly the abstraction called <span class="strong"><strong>transients</strong></span>, which is provided by Clojure.</p><p>Firstly, let's verify that it is safe (up to Clojure 1.6 only):</p><div class="informalexample"><pre class="programlisting">user=&gt; (let [t (transient [:a])]
  @(future (conj! t :b)))
IllegalAccessError Transient used by non-owner thread  clojure.lang.PersistentVector$TransientVector.ensureEditable (PersistentVector.java:463)</pre></div><p>As we can see previously, up to Clojure 1.6, a transient created in one thread cannot be accessed by another. However, this operation is allowed in Clojure 1.7 in order for transducers to play well<a class="indexterm" id="id0137"/> with the <code class="literal">core.async</code> (<a class="ulink" href="https://github.com/clojure/core.async">https://github.com/clojure/core.async</a>) library —the developer should maintain operational consistency on transients across threads:</p><div class="informalexample"><pre class="programlisting">user=&gt; (let [t (transient [:a])] (seq t))

IllegalArgumentException Don't know how to create ISeq from: clojure.lang.PersistentVector$TransientVector  clojure.lang.RT.seqFrom (RT.java:505)</pre></div><p>So, transients cannot be converted to seqs. Hence, they cannot participate in the birthing of new persistent data structures and leak out of the scope of execution. Consider the following code:</p><div class="informalexample"><pre class="programlisting">(let [t (transient [])]
  (conj! t :a)
  (persistent! t)
  (conj! t :b))
IllegalAccessError Transient used after persistent! call  clojure.lang.PersistentVector$TransientVector.ensureEditable (PersistentVector.java:464)</pre></div><p>The <code class="literal">persistent!</code> function permanently converts <code class="literal">transient</code> into an equivalent persistent data structure. Effectively, transients are for one-time use only.</p><p>Conversion between <code class="literal">persistent</code> and <code class="literal">transient</code> data structures (the <code class="literal">transient</code> and <code class="literal">persistent!</code> functions) is constant time, that is, it is an <span class="emphasis"><em>O(1)</em></span> operation. Transients can be created from unsorted maps, vectors, and sets only. The functions that mutate transients are: <code class="literal">conj!</code>, <code class="literal">disj!</code>, <code class="literal">pop!</code>, <code class="literal">assoc!</code>, and <code class="literal">dissoc!</code>. Read-only operations such as <code class="literal">get</code>, <code class="literal">nth</code>, <code class="literal">count</code>, and many more work as usual on transients, but functions such as <code class="literal">contains?</code> and those that imply seqs, such as <code class="literal">first</code>, <code class="literal">rest</code>, and <code class="literal">next</code>, do not.</p><div class="section" title="Fast repetition"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec027"/>Fast repetition</h2></div></div></div><p>The function <code class="literal">clojure.core/repeatedly</code> lets us <a class="indexterm" id="id0138"/>execute a function many times and produces a lazy sequence of results. Peter Taoussanis, in his open source serialization <a class="indexterm" id="id0139"/>library <span class="strong"><strong>Nippy</strong></span> (<a class="ulink" href="https://github.com/ptaoussanis/nippy">https://github.com/ptaoussanis/nippy</a>), wrote a transient-aware variant that performs significantly better. It is reproduced, as shown, with his permission (note that the arity of the function is not the same as <code class="literal">repeatedly</code>):</p><div class="informalexample"><pre class="programlisting">(defn repeatedly*
  "Like `repeatedly` but faster and returns given collection type."
  [coll n f]
  (if-not (instance? clojure.lang.IEditableCollection coll)
    (loop [v coll idx 0]
      (if (&gt;= idx n)
        v
        (recur (conj v (f)) (inc idx))))
    (loop [v (transient coll) idx 0]
      (if (&gt;= idx n)
        (persistent! v)
        (recur (conj! v (f)) (inc idx))))))</pre></div></div></div>
<div class="section" title="Performance miscellanea"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec019"/>Performance miscellanea</h1></div></div></div><p>Besides the major<a class="indexterm" id="id0140"/> abstractions we saw earlier in the chapter, there are other smaller, but nevertheless very performance-critical, parts of Clojure that we will see in this section.</p><div class="section" title="Disabling assertions in production"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec028"/>Disabling assertions in production</h2></div></div></div><p>Assertions are very <a class="indexterm" id="id0141"/>useful to catch logical <a class="indexterm" id="id0142"/>errors in the code during development, but<a class="indexterm" id="id0143"/> they impose a runtime overhead that you may like to avoid in the production environment. Since <code class="literal">assert</code> is a compile time variable, the assertions can be silenced either by binding <code class="literal">assert</code> to false or by using <code class="literal">alter-var-root</code> before the code is loaded. Unfortunately, both the techniques are cumbersome to use. Paul Stadig's library called <span class="strong"><strong>assertions</strong></span>
<a class="indexterm" id="id0144"/> (<a class="ulink" href="https://github.com/pjstadig/assertions">https://github.com/pjstadig/assertions</a>) helps with this exact use-case by enabling or disabling assertions via the command-line argument <code class="literal">-ea</code> to the Java runtime.</p><p>To use it, you must include it in your Leiningen <code class="literal">project.clj</code> file as a dependency:</p><div class="informalexample"><pre class="programlisting">:dependencies [;; other dependencies…
                            [pjstadig/assertions "0.1.0"]]</pre></div><p>You must use this library's <code class="literal">assert</code> macro instead of Clojure's own, so each <code class="literal">ns</code> block in the application should look similar to this:</p><div class="informalexample"><pre class="programlisting">(ns example.core

  (:refer-clojure :exclude [assert])

  (:require [pjstadig.assertions :refer [assert]]))</pre></div><p>When<a class="indexterm" id="id0145"/> running the application, you<a class="indexterm" id="id0146"/> should include the <code class="literal">-ea</code> argument to the <a class="indexterm" id="id0147"/>JRE to enable assertions, whereas its exclusion implies no assertion at runtime:</p><div class="informalexample"><pre class="programlisting">$ JVM_OPTS=-ea lein run -m example.core
$ java -ea -jar example.jar</pre></div><p>Note that this usage will not automatically avoid assertions in the dependency libraries.</p></div><div class="section" title="Destructuring"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec029"/>Destructuring</h2></div></div></div><p>
<span class="strong"><strong>Destructuring</strong></span>
<a class="indexterm" id="id0148"/> is one of Clojure's built-in mini languages and, arguably, a top <a class="indexterm" id="id0149"/>productivity booster during development. This feature leads to the parsing of values to match the left-hand side of the binding forms. The more complicated the binding form, the more work there is that needs to be done. Not surprisingly, this has a little bit of performance overhead.</p><p>It is easy to avoid this overhead by using explicit functions to unravel data in the tight loops and other performance-critical code. After all, it all boils down to making the program work less and do more.</p></div><div class="section" title="Recursion and tail-call optimization (TCO)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec030"/>Recursion and tail-call optimization (TCO)</h2></div></div></div><p>Functional<a class="indexterm" id="id0150"/> languages have <a class="indexterm" id="id0151"/>this<a class="indexterm" id="id0152"/> concept of tail-call <a class="indexterm" id="id0153"/>optimization related to recursion. So, the idea is that when a recursive call is at the tail position, it does not take up space on the stack for recursion. Clojure supports a form of user-assisted recursive call to make sure the recursive calls do not blow the stack. This is kind of an imperative looping, but is extremely fast.</p><p>When carrying out computations, it may make a lot of sense to use <code class="literal">loop-recur</code> in the tight loops instead of iterating over synthetic numbers. For example, we want to add all odd integers from zero through to 1,000,000. Let's compare the code:</p><div class="informalexample"><pre class="programlisting">(defn oddsum-1 [n]  ; using iteration
  (-&gt;&gt; (range (inc n))
    (filter odd?)
    (reduce +)))
(defn oddsum-2 [n]  ; using loop-recur
  (loop [i 1 s 0]
    (if (&gt; i n)
      s
      (recur (+ i 2) (+ s i)))))</pre></div><p>When we run the code, we get interesting results:</p><div class="informalexample"><pre class="programlisting">user=&gt; (time (oddsum-1 1000000))
"Elapsed time: 109.314908 msecs"

250000000000
user=&gt; (time (oddsum-2 1000000))
"Elapsed time: 42.18116 msecs"

250000000000</pre></div><p>The <code class="literal">time</code> macro is far from perfect as the performance-benchmarking tool, but the relative numbers indicate a trend—in the subsequent chapters, we will look at the <span class="emphasis"><em>Criterium</em></span> library for more scientific benchmarking. Here, we use <code class="literal">loop-recur</code> not only to iterate faster, but we are also able to change the algorithm itself by iterating only about half as many times as we did in the other example.</p><div class="section" title="Premature end of iteration"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec009"/>Premature end of iteration</h3></div></div></div><p>When accumulating <a class="indexterm" id="id0154"/>over a collection, in some cases, we may want to end it prematurely. Prior to Clojure 1.5, <code class="literal">loop-recur</code> was the only way to do it. When using <code class="literal">reduce</code>, we can do just that using the <code class="literal">reduced</code> function introduced in Clojure 1.5 as shown:</p><div class="informalexample"><pre class="programlisting">;; let coll be a collection of numbers
(reduce (fn ([x] x) ([x y] (if (or (zero? x) (zero? y)) (reduced 0) (* x y))))
             coll)</pre></div><p>Here, we multiply<a class="indexterm" id="id0155"/> all the numbers in a collection and, upon finding any of the numbers as zero, immediately return the result zero instead of continuing up to the last element.</p><p>The function <code class="literal">reduced?</code> helps detect when a reduced value is returned. Clojure 1.7 introduces the <code class="literal">ensure-reduced</code> function to box up non-reduced values as reduced.</p></div></div><div class="section" title="Multimethods versus protocols"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec031"/>Multimethods versus protocols</h2></div></div></div><p>
<span class="strong"><strong>Multimethods</strong></span> are a<a class="indexterm" id="id0156"/> fantastic expressive abstraction for a<a class="indexterm" id="id0157"/> polymorphic dispatch on the dispatch function's return value. The <code class="literal">dispatch</code> functions associated with a multimethod are maintained at runtime and are looked up whenever a multimethod call is invoked. While multimethods provide a lot of flexibility in determining the dispatch, the performance<a class="indexterm" id="id0158"/> overhead is simply too high compared to that of protocol implementations.</p><p>Protocols (<code class="literal">defprotocol</code>) are implemented using reify, records (<code class="literal">defrecord</code>), and types (<code class="literal">deftype</code>, <code class="literal">extend-type</code>) in Clojure. This is a big discussion topic—since we are discussing the performance characteristics, it should suffice to say that protocol implementations dispatch on polymorphic types and are significantly faster than multimethods. Protocols and types are generally the implementation detail of an API, so they are usually fronted by functions.</p><p>Due to the multimethods' flexibility, they still have a place. However, in performance-critical code it is advisable to use protocols, records, and types instead.</p></div><div class="section" title="Inlining"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec032"/>Inlining</h2></div></div></div><p>It is well known that<a class="indexterm" id="id0159"/> macros<a class="indexterm" id="id0160"/> are expanded inline at the call site and avoid a function call. As a consequence, there is a small performance benefit. There is also a <code class="literal">definline</code> macro that lets you write a function just like a normal macro. It creates an actual function that gets inlined at the call site:</p><div class="informalexample"><pre class="programlisting">(def PI Math/PI)
(definline circumference [radius]
  `(* 2 PI ~radius))</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note002"/>Note</h3><p>Note that the JVM also analyzes the code it runs and does its own inlining of code at runtime. While you may choose to inline the hot functions, this technique is known to give only a modest performance boost.</p></div></div><p>When we define a <code class="literal">var</code> object, its value is looked up each time it is used. When we define a <code class="literal">var</code> object using a <code class="literal">:const</code> meta pointing to a <code class="literal">long</code> or <code class="literal">double</code> value, it is inlined from wherever it is called:</p><div class="informalexample"><pre class="programlisting">(def ^:const PI Math/PI)</pre></div><p>This is known to give a <a class="indexterm" id="id0161"/>decent <a class="indexterm" id="id0162"/>performance boost when applicable. See the following example:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a 10)
user=&gt; (def ^:const b 10)
user=&gt; (def ^:dynamic c 10)
user=&gt; (time (dotimes [_ 100000000] (inc a)))
"Elapsed time: 1023.745014 msecs"
nil
user=&gt; (time (dotimes [_ 100000000] (inc b)))
"Elapsed time: 226.732942 msecs"
nil
user=&gt; (time (dotimes [_ 100000000] (inc c)))
"Elapsed time: 1094.527193 msecs"
nil</pre></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec020"/>Summary</h1></div></div></div><p>Performance is one of the cornerstones of Clojure's design. Abstractions in Clojure are designed for simplicity, power, and safety, with performance firmly in mind. We saw the performance characteristics of various abstractions and also how to make decisions about abstractions depending on performance use cases.</p><p>In the next chapter, we will see how Clojure interoperates with Java and how we can extract Java's power to derive optimum performance.</p></div>
<div class="chapter" title="Chapter&#xA0;3.&#xA0;Leaning on Java"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 3. Leaning on Java</h1></div></div></div><p>Being hosted on the JVM, there are several aspects of Clojure that really help to understand about the Java language and platform. The need is not only due to interoperability with Java or understanding its implementation, but also for performance reasons. In certain cases, Clojure may not generate optimized JVM bytecode by default; in some other cases, you may want to go beyond the performance that Clojure data structures offer—you can use the Java alternatives via Clojure to get better performance. This chapter discusses those aspects of Clojure. In this chapter we will discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Inspecting Java and bytecode generated from a Clojure source</li><li class="listitem" style="list-style-type: disc">Numerics and primitives</li><li class="listitem" style="list-style-type: disc">Working with arrays</li><li class="listitem" style="list-style-type: disc">Reflection and type hinting</li></ul></div><div class="section" title="Inspecting the equivalent Java source for Clojure code"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec021"/>Inspecting the equivalent Java source for Clojure code</h1></div></div></div><p>Inspecting the equivalent <a class="indexterm" id="id0163"/>Java source for a given <a class="indexterm" id="id0164"/>Clojure code provides great insight into how that might impact its performance. However, Clojure generates only Java bytecodes at runtime unless we compile a namespace out to the disk. When developing with Leiningen, only selected namespaces under the <code class="literal">:aot</code> vector in the <code class="literal">project.clj</code> file are output as the compiled <code class="literal">.class</code> files containing bytecodes. Fortunately, an easy and quick way to know the equivalent Java source for the Clojure code is to AOT-compile namespaces and then decompile the bytecodes into equivalent Java sources, using a Java bytecode decompiler.</p><p>There are several commercial and open source Java bytecode decompilers available. One of the open source decompilers we will discuss here is<a class="indexterm" id="id0165"/> <span class="strong"><strong>JD-GUI</strong></span>, which you can download from<a class="indexterm" id="id0166"/> its website (<a class="ulink" href="http://jd.benow.ca/#jd-gui">http://jd.benow.ca/#jd-gui</a>). Use a version suitable for your operating system.</p><div class="section" title="Creating a new project"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec033"/>Creating a new project</h2></div></div></div><p>Let's see how exactly to<a class="indexterm" id="id0167"/> arrive at the equivalent Java source code from Clojure. Create a new project using Leiningen: <code class="literal">lein new foo</code>. Then edit the <code class="literal">src/foo/core.clj</code> file with a <code class="literal">mul</code> function to find out the product of two numbers:</p><div class="informalexample"><pre class="programlisting">(ns foo.core)

(defn mul [x y]
  (* x y))</pre></div></div><div class="section" title="Compiling the Clojure sources into Java bytecode"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec034"/>Compiling the Clojure sources into Java bytecode</h2></div></div></div><p>Now, to compile<a class="indexterm" id="id0168"/> Clojure sources into bytecodes and <a class="indexterm" id="id0169"/>output them as <code class="literal">.class</code> files, run the <code class="literal">lein compile :all</code> command. It creates the <code class="literal">.class</code> files in the <code class="literal">target/classes</code> directory of the project as follows:</p><div class="informalexample"><pre class="programlisting">target/classes/
`-- foo
    |-- core$fn__18.class
    |-- core__init.class
    |-- core$loading__4910__auto__.class
    `-- core$mul.class</pre></div><p>You can see that the <code class="literal">foo.core</code> namespace has been compiled into four <code class="literal">.class</code> files.</p></div><div class="section" title="Decompiling the .class files into Java source"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec035"/>Decompiling the .class files into Java source</h2></div></div></div><p>Assuming that you<a class="indexterm" id="id0170"/> have already installed JD-GUI, decompiling<a class="indexterm" id="id0171"/> the <code class="literal">.class</code> files is as simple as opening them using the JD-GUI application.</p><div class="mediaobject"><img alt="Decompiling the .class files into Java source" src="graphics/B04596_03_01.jpg"/></div><p>On inspection, the <a class="indexterm" id="id0172"/>code for the <code class="literal">foo.core/mul</code> function <a class="indexterm" id="id0173"/>looks as follows:</p><div class="informalexample"><pre class="programlisting">package foo;

import clojure.lang.AFunction;
import clojure.lang.Numbers;
import clojure.lang.RT;
import clojure.lang.Var;

public final class core$mul extends AFunction
{
  public static final Var const__0 = (Var)RT.var("clojure.core", "*");

  public Object invoke(Object x, Object y) { x = null; y = null; return Numbers.multiply(x, y);
  }
}</pre></div><p>It is easy to understand from the decompiled Java source that the foo.core/mul function is an instance of the core$mul class in the foo package extending the clojure.lang.AFunction class. We can also see that the argument types are of the Object type in method invoke(Object, Object), which implies the numbers will be boxed. In a similar fashion, you can decompile class files of any Clojure code to inspect the equivalent Java code. If you can combine this with knowledge about Java types and potential reflection and boxing, you can find the suboptimal spots in code and focus on what to improve upon.</p></div><div class="section" title="Compiling the Clojure source without locals clearing"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec036"/>Compiling the Clojure source without locals clearing</h2></div></div></div><p>Note the Java code<a class="indexterm" id="id0174"/> in the method invoke where it says <code class="literal">x = null; y = null;</code> —how is it possible that the code throws away the arguments, sets them to null, and effectively multiplies two null objects? This misleading decompilation happens due to locals clearing, a feature of the JVM bytecode implementation of Clojure, which has no equivalent in the Java language.</p><p>Starting with Clojure 1.4, the compiler supports the <code class="literal">:disable-locals-clearing</code> key in the dynamic <code class="literal">clojure.core/*compiler-options*</code> var that we cannot configure in the <code class="literal">project.clj</code> file. So, we cannot use the <code class="literal">lein compile</code> command, but we can start a <span class="strong"><strong>REPL</strong></span>
<a class="indexterm" id="id0175"/> with the <code class="literal">lein repl</code> command to compile the classes:</p><div class="informalexample"><pre class="programlisting">user=&gt; (binding [*compiler-options* {:disable-locals-clearing true}] (compile 'foo.core))
foo.core</pre></div><p>This generates the class files in the same location as we saw earlier in this section, but without <code class="literal">x = null; y = null;</code> because locals clearing is omitted.</p></div></div></div>
<div class="section" title="Numerics, boxing, and primitives"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec022"/>Numerics, boxing, and primitives</h1></div></div></div><p>
<span class="strong"><strong>Numerics</strong></span> <a class="indexterm" id="id0176"/>are scalars. The discussion on numerics was deferred till this chapter for the sole reason that the numerics implementation in Clojure has strong Java underpinnings. Since version 1.3, Clojure has settled with 64-bit numerics as the default. Now, <code class="literal">long</code> and <code class="literal">double</code> are idiomatic and the default numeric types. Note that these are primitive Java types, not objects. Primitives in Java lead to high performance and have several optimizations associated with them at compiler and runtime levels. A local primitive is created on the stack (hence does not contribute to heap allocation and GC) and can be accessed directly without any kind of dereferencing. In Java, there also exist object equivalents of the numeric primitives, known as<a class="indexterm" id="id0177"/> <span class="strong"><strong>boxed numerics</strong></span>—these are regular objects that are allocated on the heap. The boxed numerics are also immutable objects, which mean not only does the JVM need to dereference the stored value when reading it, but also needs to create a new boxed object when a new value needs to be created.</p><p>It should be obvious that boxed numerics are slower than their primitive equivalents. The Oracle HotSpot JVM, when started with the <code class="literal">-server</code> option, aggressively inlines those functions (on frequent invocation) that contain a call to primitive operations. Clojure automatically uses <span class="strong"><strong>primitive numerics</strong></span>
<a class="indexterm" id="id0178"/> at several levels. In the <code class="literal">let</code> blocks, <code class="literal">loop</code> blocks, arrays, and arithmetic operations (<code class="literal">+</code>, <code class="literal">-</code>, <code class="literal">*</code>, <code class="literal">/</code>, <code class="literal">inc</code>, <code class="literal">dec</code>, <code class="literal">&lt;</code>, <code class="literal">&lt;=</code>, <code class="literal">&gt;</code>, <code class="literal">&gt;=</code>), primitive numerics are detected and retained. The following table describes the primitive numerics with their boxed equivalents:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Primitive numeric type</p>
</th><th style="text-align: left" valign="bottom">
<p>Boxed equivalent</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>byte (1 byte)</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">java.lang.Byte</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>short (2 bytes)</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">java.lang.Short</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>int (4 bytes)</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">java.lang.Integer</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>float (4 bytes)</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">java.lang.Float</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>long (8 bytes)</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">java.lang.Long</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>double (8 bytes)</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">java.lang.Double</code>
</p>
</td></tr></tbody></table></div><p>In Clojure, sometimes you may find the numerics are passed or returned as boxed objects to or from functions due to the lack of type information at runtime. Even if you have no control over such functions, you can coerce the values to be treated as primitives. The <code class="literal">byte</code>, <code class="literal">short</code>, <code class="literal">int</code>, <code class="literal">float</code>, <code class="literal">long</code>, and <code class="literal">double</code> functions create primitive equivalents from given boxed numeric values.</p><p>One of the Lisp traditions is to <a class="indexterm" id="id0179"/>provide correct (<a class="ulink" href="http://en.wikipedia.org/wiki/Numerical_tower">http://en.wikipedia.org/wiki/Numerical_tower</a>) arithmetic implementation. A lower type should not truncate values when overflow or underflow happens, but rather should be promoted to construct a higher type to maintain correctness. Clojure follows this constraint and provides <a class="indexterm" id="id0180"/>
<span class="strong"><strong>autopromotion</strong></span> via prime (<a class="ulink" href="http://en.wikipedia.org/wiki/Prime_(symbol)">http://en.wikipedia.org/wiki/Prime_(symbol)</a>) functions: <code class="literal">+'</code>, <code class="literal">-'</code>, <code class="literal">*'</code>, <code class="literal">inc'</code>, and <code class="literal">dec'</code>. Autopromotion<a class="indexterm" id="id0181"/> provides correctness at the cost of some performance.</p><p>There are also arbitrary length or precision numeric types in Clojure that let us store unbounded numbers but have poorer performance compared to primitives. The <code class="literal">bigint</code> and <code class="literal">bigdec</code> functions let us create numbers of arbitrary length and precision.</p><p>If we try to carry out any operations with primitive numerics that may result in a number beyond its maximum capacity, the operation maintains correctness by throwing an exception. On the other hand, when we use the prime functions, they autopromote to provide correctness. There is another set of operations called unchecked operations, which do not check for overflow or underflow and can potentially return incorrect results.</p><p>In some cases, they may be faster than regular and prime functions. Such functions are <code class="literal">unchecked-add</code>, <code class="literal">unchecked-subtract</code>, <code class="literal">unchecked-multiply</code>, <code class="literal">unchecked-divide</code>, <code class="literal">unchecked-inc</code>, and <code class="literal">unchecked-dec</code>. We can also enable unchecked math behavior for regular arithmetic functions using the <code class="literal">*unchecked-math*</code> var; simply include the following in your source code file:</p><div class="informalexample"><pre class="programlisting">(set! *unchecked-math* true)</pre></div><p>One of the common needs in the arithmetic is the division used to find out the quotient and remainder after a natural number division. Clojure's <code class="literal">/</code> function provides a rational number division yielding a ratio, and the <code class="literal">mod</code> function provides a true modular arithmetic division. These functions are slower than the <code class="literal">quot</code> and <code class="literal">rem</code> functions that compute the division quotient and the remainder, respectively.</p></div>
<div class="section" title="Arrays"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec023"/>Arrays</h1></div></div></div><p>Besides<a class="indexterm" id="id0182"/> objects and primitives, Java has a special type of collection storage structure called <span class="strong"><strong>arrays</strong></span>. Once created, arrays cannot be grown or shrunk without copying data and creating another array to hold the result. Array elements are always homogeneous in type. The array elements are similar to places where you can mutate them to hold new values. Unlike collections such as list and vector, arrays can contain primitive elements, which make them a very fast storage mechanism without GC overhead.</p><p>Arrays often form a basis for mutable data structures. For example, Java's <code class="literal">java.lang.ArrayList</code> implementation uses arrays internally. In Clojure, arrays can be used for fast numeric storage and processing, efficient algorithms, and so on. Unlike collections, arrays<a class="indexterm" id="id0183"/> can have one or more dimensions. So you could layout data in an array such as a matrix or cube. Let's see Clojure's support for arrays:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Description</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th><th style="text-align: left" valign="bottom">
<p>Notes</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Create array</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(make-array Integer 20)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Array of type (boxed) integer</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">(make-array Integer/TYPE 20)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Array of primitive type integer</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">(make-array Long/TYPE 20 10)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Two-dimensional array of primitive long</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Create array of primitives</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(int-array 20)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Array of primitive integer of size 20</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">(int-array [10 20 30 40])</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Array of primitive integer created from a vector</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Create array from coll</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(to-array [10 20 30 40])</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Array from sequable</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">(to-array-2d [[10 20 30][40 50 60]])</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Two-dimensional array from collection</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Clone an array</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(aclone (to-array [:a :b :c]))</code>
</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>Get array element</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(aget array-object 0 3)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Get element at index [0][3] in a 2-D array</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mutate array element</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(aset array-object 0 3 :foo)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Set obj :foo at index [0][3] in a 2-D array</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mutate primitive array element</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(aset-int int-array-object 2 6 89)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Set value 89 at index [2][6] in 2-D array</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Find length of array</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(alength array-object)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">alength</code> is significantly faster than count</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Map over an array</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(def a (int-array [10 20 30 40 50 60]))</code>
</p>
<p>
<code class="literal">(seq</code>
</p>
<p>
<code class="literal">  (amap a idx ret</code>
</p>
<p>
<code class="literal">    (do (println idx (seq ret))</code>
</p>
<p>
<code class="literal">      (inc (aget a idx)))))</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Unlike map, <code class="literal">amap</code> returns a non-lazy array, which is significantly faster over array elements. Note that <code class="literal">amap</code> is faster only when properly type hinted. See next section for type hinting.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Reduce over an array</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(def a (int-array [10 20 30 40 50 60]))</code>
</p>
<p>
<code class="literal">(areduce a idx ret 0</code>
</p>
<p>
<code class="literal">  (do (println idx ret)</code>
</p>
<p>
<code class="literal">    (+ ret idx)))</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Unlike reduce, <code class="literal">areduce</code> is significantly faster over array elements. Note that reduce is faster only when properly type hinted. See next section for type hinting.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Cast to primitive arrays</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(ints int-array-object)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Used with type hinting (see next section)</p>
</td></tr></tbody></table></div><p>Like <code class="literal">int-array</code> and <code class="literal">ints</code>, there are <a class="indexterm" id="id0184"/>functions for other types as well:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Array construction function</p>
</th><th style="text-align: left" valign="bottom">
<p>Primitive-array casting function</p>
</th><th style="text-align: left" valign="bottom">
<p>Type hinting (does not work for vars)</p>
</th><th style="text-align: left" valign="bottom">
<p>Generic array type hinting</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>boolean-array</p>
</td><td style="text-align: left" valign="top">
<p>booleans</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^booleans</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[Z"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>byte-array</p>
</td><td style="text-align: left" valign="top">
<p>bytes</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^bytes</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[B"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>short-array</p>
</td><td style="text-align: left" valign="top">
<p>shorts</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^shorts</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[S"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>char-array</p>
</td><td style="text-align: left" valign="top">
<p>chars</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^chars</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[C"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>int-array</p>
</td><td style="text-align: left" valign="top">
<p>ints</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^ints</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[I"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>long-array</p>
</td><td style="text-align: left" valign="top">
<p>longs</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^longs</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[J"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>float-array</p>
</td><td style="text-align: left" valign="top">
<p>floats</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^floats</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[F"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>double-array</p>
</td><td style="text-align: left" valign="top">
<p>doubles</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^doubles</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[D"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>object-array</p>
</td><td style="text-align: left" valign="top">
<p>   ––</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^objects</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">^"[Ljava.lang.Object"</code>
</p>
</td></tr></tbody></table></div><p>Arrays are favored over other data structures mainly due to performance, and sometimes due to interop. Extreme care should be taken to type hint the arrays and use the appropriate functions to work with them.</p></div>
<div class="section" title="Reflection and type hints"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec024"/>Reflection and type hints</h1></div></div></div><p>Sometimes, as Clojure is dynamically typed, the Clojure compiler is unable to figure out the type of object to invoke a certain method. In such cases, Clojure uses <a class="indexterm" id="id0185"/>
<span class="strong"><strong>reflection</strong></span>, which is considerably slower than the direct method dispatch. Clojure's solution to this is something called <a class="indexterm" id="id0186"/>
<span class="strong"><strong>type hints</strong></span>. Type hints are a way to annotate arguments and objects with static types, so that the Clojure compiler can emit bytecodes for efficient dispatch.</p><p>The easiest way to know where to put type hints is to turn on reflection warning in the code. Consider this code that determines the length of a string:</p><div class="informalexample"><pre class="programlisting">user=&gt; (set! *warn-on-reflection* true)
true
user=&gt; (def s "Hello, there")
#'user/s
user=&gt; (.length s)
Reflection warning, NO_SOURCE_PATH:1 - reference to field length can't be resolved.
12
user=&gt; (defn str-len [^String s] (.length s))
#'user/str-len
user=&gt; (str-len s)
12
user=&gt; (.length ^String s)  ; type hint when passing argument
12
user=&gt; (def ^String t "Hello, there")  ; type hint at var level
#'user/t
user=&gt; (.length t)  ; no more reflection warning
12
user=&gt; (time (dotimes [_ 1000000] (.length s)))
Reflection warning, /private/var/folders/cv/myzdv_vd675g4l7y92jx9bm5lflvxq/T/form-init6904047906685577265.clj:1:28 - reference to field length can't be resolved.
"Elapsed time: 2409.155848 msecs"
nil
user=&gt; (time (dotimes [_ 1000000] (.length t)))
"Elapsed time: 12.991328 msecs"
nil</pre></div><p>In the previous snippet, we can clearly see there is a very big difference in performance in the code that uses reflection versus the code that does not. When working on a project, you may want reflection warning to be turned on for all files. You can do it easily in Leiningen. Just put the following entry in your <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">:profiles {:dev {:global-vars {*warn-on-reflection* true}}}</pre></div><p>This will automatically turn on warning reflection every time you begin any kind of invocation via Leiningen in the dev workflow such as REPL and test.</p><div class="section" title="An array of primitives"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec037"/>An array of primitives</h2></div></div></div><p>Recall the <a class="indexterm" id="id0187"/>examples on <code class="literal">amap</code> and <code class="literal">areduce</code> from the previous section. If we run them with reflection warning on, we'd be warned that it uses reflection. Let's type hint them:</p><div class="informalexample"><pre class="programlisting">(def a (int-array [10 20 30 40 50 60]))
;; amap example
(seq
<span class="strong"><strong>  (amap ^ints a idx ret</strong></span>
    (do (println idx (seq ret))
      (inc (aget ^ints a idx)))))
;; areduce example
(areduce ^ints a idx ret 0
  (do (println idx ret)
    (+ ret idx)))</pre></div><p>Note that the primitive array hint <code class="literal">^ints</code> does not work at the var level. So, it would not work if you defined the var <code class="literal">a</code>, as in the following:</p><div class="informalexample"><pre class="programlisting">(def ^ints a (int-array [10 20 30 40 50 60]))  ; wrong, will complain later
(def ^"[I" a (int-array [10 20 30 40 50 60]))  ; correct
(def ^{:tag 'ints} a (int-array [10 20 30 40 50 60])) ; correct</pre></div><p>This notation is for an array of integers. Other primitive array types have similar type hints. Refer to the previous section for type hinting for various primitive array types.</p></div><div class="section" title="Primitives"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec038"/>Primitives</h2></div></div></div><p>The type hinting of <a class="indexterm" id="id0188"/>primitive locals is neither required nor allowed. However, you can type hint function arguments as primitives. Clojure allows up to four arguments in functions to be type hinted:</p><div class="informalexample"><pre class="programlisting">(defn do-something
  [^long a ^long b ^long c ^long d]
  ..)</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note003"/>Note</h3><p>Boxing may result in something not always being a primitive. In those cases, you can coerce those using respective primitive types.</p></div></div></div><div class="section" title="Macros and metadata"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec039"/>Macros and metadata</h2></div></div></div><p>In <a class="indexterm" id="id0189"/>macros, type hinting does not work the way it does in the other parts of the code. Since macros are about transforming the <a class="indexterm" id="id0190"/>
<span class="strong"><strong>Abstract Syntax Tree</strong></span> (<span class="strong"><strong>AST</strong></span>), we need to have a mental map of the transformation and we <a class="indexterm" id="id0191"/>should add type hints as metadata in the code. For example, if <code class="literal">str-len</code> is a macro to find the length of a string, we make use of the following code:</p><div class="informalexample"><pre class="programlisting">(defmacro str-len
  [s]
  `(.length ~(with-meta s {:tag String})))
;; below is another way to write the same macro
(defmacro str-len
  [s]
  `(.length ~(vary-meta s assoc :tag `String)))</pre></div><p>In the preceding code, we alter the metadata of the symbol <code class="literal">s</code> by tagging it with the type <code class="literal">String</code>, which happens to be the <code class="literal">java.lang.String</code> class in this case. For array types, we can use <code class="literal">[Ljava.lang.String</code> for an array of string objects and similarly for others. If you try to use <code class="literal">str-len</code> listed previously, you may notice this works only when we pass the string bound to a local or a var, not as a string literal. To mitigate this, we can write the macro as follows:</p><div class="informalexample"><pre class="programlisting">(defmacro str-len
  [s]
  `(let [^String s# ~s] (.length s#)))</pre></div><p>Here we bind the argument to a type-hinted gensym local, hence calling <code class="literal">.length</code> on it does not use reflection and there is no reflection warning emitted as such.</p><p>Type hinting via metadata also works with functions, albeit in a different notation:</p><div class="informalexample"><pre class="programlisting">(defn foo [] "Hello")
(defn foo ^String [] "Hello")
(defn foo (^String [] "Hello") (^String [x] (str "Hello, " x)))</pre></div><p>Except for the first example in the preceding snippet, they are type hinted to return the <code class="literal">java.lang.String</code> type.</p><div class="section" title="String concatenation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec010"/>String concatenation</h3></div></div></div><p>The <code class="literal">str</code> function in Clojure is <a class="indexterm" id="id0192"/>used to concatenate and convert to string tokens. In Java, when we write <code class="literal">"hello" + e</code>, the Java compiler translates this to an equivalent code that uses <code class="literal">StringBuilder</code> and is considerably faster than the <code class="literal">str</code> function in micro-benchmarks. To obtain close-to-Java performance, in Clojure we can use a similar mechanism with a macro directly using Java interop to avoid the indirection via the <code class="literal">str</code> function. The<a class="indexterm" id="id0193"/> <span class="strong"><strong>Stringer</strong></span> (<a class="ulink" href="https://github.com/kumarshantanu/stringer">https://github.com/kumarshantanu/stringer</a>) library adopts the same technique to come up with<a class="indexterm" id="id0194"/> fast string concatenation in Clojure:</p><div class="informalexample"><pre class="programlisting">(require '[stringer.core :as s])
user=&gt; (time (dotimes [_ 10000000] (str "foo" :bar 707 nil 'baz)))
"Elapsed time: 2044.284333 msecs"
nil
user=&gt; (time (dotimes [_ 10000000] (s/strcat "foo" :bar 707 nil 'baz)))
"Elapsed time: 555.843271 msecs"
nil</pre></div><p>Here, Stringer also aggressively concatenates the literals during the compile phase.</p></div></div><div class="section" title="Miscellaneous"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec040"/>Miscellaneous</h2></div></div></div><p>In a type (as in <code class="literal">deftype</code>), the mutable instance variables can be optionally annotated as <code class="literal">^:volatile-mutable</code> or <code class="literal">^:unsynchronized-mutable</code>. For example:</p><div class="informalexample"><pre class="programlisting">(deftype Counter [^:volatile-mutable ^long now]
  ..)</pre></div><p>Unlike <code class="literal">defprotocol</code>, the <code class="literal">definterface</code> macro lets us provide a return type hint for methods:</p><div class="informalexample"><pre class="programlisting">(definterface Foo
  (^long doSomething [^long a ^double b]))</pre></div><p>The <code class="literal">proxy-super</code> macro (which is used inside the <code class="literal">proxy</code> macro) is a <a class="indexterm" id="id0195"/>special case where you cannot directly apply a type hint. The reason being that it relies on the implicit this object that is automatically created by the <code class="literal">proxy</code> macro. In this case, you must explicitly bind this to a type:</p><div class="informalexample"><pre class="programlisting">(proxy [Object][]
  (equals [other]
    (let [^Object this this]
      (proxy-super equals other))))</pre></div><p>Type hinting is quite important for performance in Clojure. Fortunately, we need to type hint only when required and it's easy to find out when. In many cases, a gain from type hinting overshadows the gains from code inlining.</p></div></div>
<div class="section" title="Using array/numeric libraries for efficiency"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec025"/>Using array/numeric libraries for efficiency</h1></div></div></div><p>You may have<a class="indexterm" id="id0196"/> noticed in the <a class="indexterm" id="id0197"/>previous sections, when working with numerics, performance depends a lot on whether the data is based on arrays and primitives. It may take a lot of meticulousness on the programmer's part to correctly coerce data into primitives and arrays at all stages of the computation in order to achieve optimum efficiency. Fortunately, the high-performance enthusiasts from the Clojure community realized this issue early on and created some dedicated open source libraries to mitigate the problem.</p><div class="section" title="HipHip"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec041"/>HipHip</h2></div></div></div><p>
<span class="strong"><strong>HipHip</strong></span>
<a class="indexterm" id="id0198"/> is a Clojure library used to work with arrays of primitive types. It provides a safety net, that is, it strictly accepts only primitive array arguments to work with. As a result, passing silently boxed primitive arrays as arguments always results in an exception. HipHip macros and functions rarely need the programmer to type hint anything during the operations. It supports arrays of primitive types such as <code class="literal">int</code>, <code class="literal">long</code>, <code class="literal">float</code>, and <code class="literal">double</code>.</p><p>The HipHip<a class="indexterm" id="id0199"/> project is available at <a class="ulink" href="https://github.com/Prismatic/hiphip">https://github.com/Prismatic/hiphip</a>.</p><p>As of writing, HipHip's most recent version is 0.2.0 that supports Clojure 1.5.x or above, and is tagged as an Alpha release. There is a standard set of operations provided by HipHip for arrays of all of the four primitive types: integer array operations are in the namespace <code class="literal">hiphip.int</code>; double precision array operations in <code class="literal">hiphip.double</code>; and so on. The operations are all type hinted for the respective types. All of the operations for <code class="literal">int</code>, <code class="literal">long</code>, <code class="literal">float</code>, and <code class="literal">double</code> in respective namespaces are essentially the same except for the array type:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Category</p>
</th><th style="text-align: left" valign="bottom">
<p>Function/macro</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Core functions</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">aclone</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/aclone</code>, for primitives</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">alength</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/alength</code>, for primitives</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">aget</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/aget</code>, for primitives</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">aset</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/aset</code>, for primitives</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">ainc</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Increment array element by specified value</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Equiv hiphip.array operations</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">amake</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Make a new array and fill values computed by expression</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">areduce</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/areduce</code>, with HipHip array bindings</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">doarr</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/doseq</code>, with HipHip array bindings</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amap</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">clojure.core/for</code>, creates new array</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">afill!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like preceding <code class="literal">amap</code>, but overwrites array argument</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mathy operations</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">asum</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Compute sum of array elements using expression</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">aproduct</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Compute product of array elements using expression</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amean</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Compute<a class="indexterm" id="id0200"/> mean over array elements</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">dot-product</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Compute dot product of two arrays</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Finding minimum/maximum, Sorting</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">amax-index</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Find maximum value in array and return the index</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amax</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Find maximum value in array and return it</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amin-index</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Find minimum value in array and return the index</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amin</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Find minimum value in array and return it</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">apartition!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Three-way partition of array: less, equal, greater than pivot</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">aselect!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Gather smallest <code class="literal">k</code> elements at the beginning of array</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">asort!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Sort array in-place using Java's built-in implementation</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">asort-max!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Partial in-place sort gathering top <code class="literal">k</code> elements to the end</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">asort-min!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Partial in-place sort gathering min <code class="literal">k</code> elements to the top</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">apartition-indices!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">apartition!</code> but mutates index-array instead of values</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">aselect-indices!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">aselect!</code> but mutates index-array instead of values</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">asort-indices!</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Like <code class="literal">asort!</code> but mutates index-array instead of values</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amax-indices</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Get index-array; last <code class="literal">k</code> indices pointing to max <code class="literal">k</code> values</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">amin-indices</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Get index-array; first <code class="literal">k</code> indices pointing to min <code class="literal">k</code> values</p>
</td></tr></tbody></table></div><p>To include HipHip as a <a class="indexterm" id="id0201"/>dependency in your Leiningen project, specify it in <code class="literal">project.clj</code>:</p><div class="informalexample"><pre class="programlisting">:dependencies [;; other dependencies
               [prismatic/hiphip "0.2.0"]]</pre></div><p>As an example of how to use HipHip, let's see how to compute the normalized values of an array:</p><div class="informalexample"><pre class="programlisting">(require '[hiphip.double :as hd])

(def xs (double-array [12.3 23.4 34.5 45.6 56.7 67.8]))

(let [s (hd/asum xs)] (hd/amap [x xs] (/ x s)))</pre></div><p>Unless we make sure that <code class="literal">xs</code> is an array of primitive doubles, HipHip will throw <code class="literal">ClassCastException</code> when the type is incorrect, and <code class="literal">IllegalArgumentException</code> in other cases. I recommend exploring the HipHip project to gain more insight into using it effectively.</p></div><div class="section" title="primitive-math"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec042"/>primitive-math</h2></div></div></div><p>We can set <code class="literal">*warn-on-reflection*</code> to true to <a class="indexterm" id="id0202"/>let Clojure warn us when the reflection is used at invocation boundaries. However, when Clojure has to implicitly use reflection to perform math, the only resort is to either use a profiler or compile the Clojure source down to bytecode, and analyze boxing and reflection with a decompiler. This is where the <code class="literal">primitive-math</code> library helps, by producing extra warnings and throwing exceptions.</p><p>The <code class="literal">primitive-math</code> library <a class="indexterm" id="id0203"/>is available at <a class="ulink" href="https://github.com/ztellman/primitive-math">https://github.com/ztellman/primitive-math</a>.</p><p>As of writing, primitive-math is at version 0.1.4; you can include it as a dependency in your Leiningen project by editing <code class="literal">project.clj</code> as follows:</p><div class="informalexample"><pre class="programlisting">:dependencies [;; other dependencies
               [primitive-math "0.1.4"]]</pre></div><p>The following code is how it can be used (recall the example from the <span class="emphasis"><em>Decompiling the .class files into Java source</em></span> section):</p><div class="informalexample"><pre class="programlisting">;; must enable reflection warnings for extra warnings from primitive-math
(set! *warn-on-reflection* true)
(require '[primitive-math :as pm])
(defn mul [x y] (pm/* x y))  ; primitive-math produces reflection warning
(mul 10.3 2)                        ; throws exception
(defn mul [^long x ^long y] (pm/* x y))  ; no warning after type hinting
(mul 10.3 2)  ; returns 20</pre></div><p>While <code class="literal">primitive-math</code> is a<a class="indexterm" id="id0204"/> useful library, the problem it solves is mostly taken care of by the boxing detection feature in Clojure 1.7 (see next section <span class="emphasis"><em>Detecting boxed math</em></span>). However, this library is still useful if you are unable to use Clojure 1.7 or higher.</p><div class="section" title="Detecting boxed math"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec011"/>Detecting boxed math</h3></div></div></div><p>
<span class="strong"><strong>Boxed math</strong></span> is <a class="indexterm" id="id0205"/>hard to detect and is a source of performance issues. Clojure 1.7 introduces a way to warn the user when boxed math happens. This can be configured in the following way:</p><div class="informalexample"><pre class="programlisting">(set! *unchecked-math* :warn-on-boxed)

(defn sum-till [n] (/ (* n (inc n)) 2))  ; causes warning
Boxed math warning, /private/var/folders/cv/myzdv_vd675g4l7y92jx9bm5lflvxq/T/form-init3701519533014890866.clj:1:28 - call: public static java.lang.Number clojure.lang.Numbers.unchecked_inc(java.lang.Object).
Boxed math warning, /private/var/folders/cv/myzdv_vd675g4l7y92jx9bm5lflvxq/T/form-init3701519533014890866.clj:1:23 - call: public static java.lang.Number clojure.lang.Numbers.unchecked_multiply(java.lang.Object,java.lang.Object).
Boxed math warning, /private/var/folders/cv/myzdv_vd675g4l7y92jx9bm5lflvxq/T/form-init3701519533014890866.clj:1:20 - call: public static java.lang.Number clojure.lang.Numbers.divide(java.lang.Object,long).

;; now we define again with type hint
(defn sum-till [^long n] (/ (* n (inc n)) 2))</pre></div><p>When working with Leiningen, you can enable boxed math warnings by putting the following entry in the <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">:global-vars {*unchecked-math* :warn-on-boxed}</pre></div><p>The math operations in <code class="literal">primitive-math</code> (like HipHip) are implemented via macros. Therefore, they cannot be used as higher order functions and, as a consequence, may not compose well <a class="indexterm" id="id0206"/>with other code. I recommend exploring the project to see what suits your program use case. Adopting Clojure 1.7 obviates the boxing discovery issues by means of a boxed-warning feature.</p></div></div></div>
<div class="section" title="Resorting to Java and native code"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec026"/>Resorting to Java and native code</h1></div></div></div><p>In a handful of cases, where the <a class="indexterm" id="id0207"/>lack of imperative, stack-based, mutable variables in <a class="indexterm" id="id0208"/>Clojure may make the code not perform as well as Java, we may need to evaluate alternatives to make it faster. I would advise you to consider writing such code directly in Java for better performance.</p><p>Another consideration is to use native OS capabilities, such as <a class="indexterm" id="id0209"/>memory-mapped buffers (<a class="ulink" href="http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html">http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html</a>) or files and unsafe operations (<a class="ulink" href="http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/">http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/</a>). Note that unsafe operations are potentially hazardous and not recommended in general. Such times are also an opportunity to consider writing performance-critical pieces of code in C or C++ and then access them via the <a class="indexterm" id="id0210"/>
<span class="strong"><strong>Java Native Interface</strong></span> (<span class="strong"><strong>JNI</strong></span>).</p><div class="section" title="Proteus – mutable locals in Clojure"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec043"/>Proteus – mutable locals in Clojure</h2></div></div></div><p>Proteus <a class="indexterm" id="id0211"/>is an open source <a class="indexterm" id="id0212"/>Clojure library that lets you treat a local as a local variable, thereby allowing its unsynchronized mutation within the local scope only. Note that this library depends on the internal implementation structure of Clojure as of Clojure 1.5.1. The <a class="indexterm" id="id0213"/>
<span class="strong"><strong>Proteus</strong></span> project is available at <a class="ulink" href="https://github.com/ztellman/proteus">https://github.com/ztellman/proteus</a>.</p><p>You can include Proteus as a dependency in the Leiningen project by editing <code class="literal">project.clj</code>:</p><div class="informalexample"><pre class="programlisting">:dependencies [;;other dependencies
               [proteus "0.1.4"]]</pre></div><p>Using Proteus in code is straightforward, as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">(require '[proteus :as p])
(p/let-mutable [a 10]
  (println a)
  (set! a 20)
  (println a))
;; Output below:
;; 10
;; 20</pre></div><p>Since Proteus allows mutation only in the local scope, the following throws an exception:</p><div class="informalexample"><pre class="programlisting">(p/let-mutable [a 10 add2! (fn [x] (set! x (+ 2 x)))]
  (add2! a)
  (println a))</pre></div><p>The mutable locals <a class="indexterm" id="id0214"/>are very fast and may be quite useful in tight loops. Proteus is unconventional by Clojure idioms, but it may give the required performance boost without having to write Java code.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec027"/>Summary</h1></div></div></div><p>Clojure has strong Java interoperability and underpinning, due to which programmers can leverage the performance benefits nearing those of Java. For performance-critical code, it is sometimes necessary to understand how Clojure interacts with Java and how to turn the right knobs. Numerics is a key area where Java interoperability is required to get optimum performance. Type hints are another important performance trick that is frequently useful. There are several open source Clojure libraries that make such activities easier for the programmer.</p><p>In the next chapter, we will dig deeper below Java and see how the hardware and the JVM stack play a key role in offering the performance we get, what their constraints are, and how to use their understanding to get better performance.</p></div>
<div class="chapter" title="Chapter&#xA0;4.&#xA0;Host Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 4. Host Performance</h1></div></div></div><p>In the previous chapters, we noted how Clojure interoperates with Java. In this chapter we will go a bit deeper to understand the internals better. We will touch upon several layers of the entire stack, but our major focus will be the JVM, in particular the Oracle HotSpot JVM, though there are several <a class="indexterm" id="id0215"/>JVM vendors to choose from (<a class="ulink" href="http://en.wikipedia.org/wiki/List_of_Java_virtual_machines">http://en.wikipedia.org/wiki/List_of_Java_virtual_machines</a>). At the time of writing this, Oracle JDK 1.8 is the latest stable release and early OpenJDK 1.9 builds are available. In this chapter we will discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How the hardware subsystems function from the performance viewpoint</li><li class="listitem" style="list-style-type: disc">Organization of the JVM internals and how that is related to performance</li><li class="listitem" style="list-style-type: disc">How to measure the amount of space occupied by various objects in the heap</li><li class="listitem" style="list-style-type: disc">Profile Clojure code for latency using Criterium</li></ul></div><div class="section" title="The hardware"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec028"/>The hardware</h1></div></div></div><p>There are various hardware components<a class="indexterm" id="id0216"/> that may impact the performance of software in different ways. The processors, caches, memory subsystem, I/O subsystems, and so on, all have varying degrees of performance impact depending upon the use cases. In the following sections we look into each of those aspects.</p><div class="section" title="Processors"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec044"/>Processors</h2></div></div></div><p>Since about the late 1980s, microprocessors <a class="indexterm" id="id0217"/>have been employing pipelining and <a class="indexterm" id="id0218"/>instruction-level parallelism to speed up their performance. Processing an instruction at the <a class="indexterm" id="id0219"/>CPU level consists of typically four cycles: <span class="strong"><strong>fetch</strong></span>, <a class="indexterm" id="id0220"/>
<span class="strong"><strong>decode</strong></span>, <a class="indexterm" id="id0221"/>
<span class="strong"><strong>execute</strong></span>, and<a class="indexterm" id="id0222"/> <span class="strong"><strong>writeback</strong></span>. Modern processors optimize the cycles by running them in parallel—while one instruction is executed, the next instruction is being decoded, and the one after that is being fetched, and so on. This style is called<a class="indexterm" id="id0223"/> <span class="strong"><strong>instruction pipelining</strong></span>.</p><p>In practice, in order to speed<a class="indexterm" id="id0224"/> up execution even further, the stages are subdivided into many shorter stages, thus leading to deeper super-pipeline architecture. The length of the longest stage in the pipeline limits the clock speed of the CPU. By splitting stages into substages, the processor can be run at a higher clock speed, where more cycles are required for each instruction, but the processor still completes one instruction per cycle. Since there are more cycles per second now, we get better performance in terms of throughput per second even though the latency of each instruction is now higher.</p><div class="section" title="Branch prediction"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec012"/>Branch prediction</h3></div></div></div><p>The <a class="indexterm" id="id0225"/>processor <a class="indexterm" id="id0226"/>must fetch and decode instructions in advance even when it encounters instructions of the conditional <code class="literal">if-then</code> form. Consider an equivalent of the (<code class="literal">if (test a) (foo a) (bar a)</code>) Clojure expression. The processor must choose a branch to fetch and decode, the question is should it fetch the <code class="literal">if</code> branch or the <code class="literal">else</code> branch? Here, the processor makes a guess as to which instruction to fetch/decode. If the guess turns out to be correct, it is a performance gain as usual; otherwise, the processor has to throw away the result of the fetch/decode process and start on the other branch afresh.</p><p>Processors deal with branch prediction using an on-chip branch prediction table. It contains recent code branches and two bits per branch, indicating whether or not the branch was taken, while also accommodating one-off, not-taken occurrences.</p><p>Today, branch prediction is extremely important in processors for performance, so modern processors dedicate hardware resources and special predication instructions to improve the prediction accuracy and lower the cost of a mispredict penalty.</p></div><div class="section" title="Instruction scheduling"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec013"/>Instruction scheduling</h3></div></div></div><p>High-latency <a class="indexterm" id="id0227"/>instructions and branching usually lead to empty cycles in the instruction<a class="indexterm" id="id0228"/> pipeline known as <span class="strong"><strong>stalls</strong></span>
<a class="indexterm" id="id0229"/> or<a class="indexterm" id="id0230"/> <span class="strong"><strong>bubbles</strong></span>. These cycles are often used to do other work by the means of instruction reordering. Instruction reordering is implemented at the hardware level via out of order execution and at the compiler level via compile time instruction scheduling (also called <a class="indexterm" id="id0231"/>
<span class="strong"><strong>static instruction scheduling</strong></span>).</p><p>The processor needs to remember the dependencies between instructions when carrying out the out-of-order execution. This cost is somewhat mitigated by using renamed registers, wherein register values are stored into / loaded from memory locations, potentially on different physical registers, so that they can be executed in parallel. This necessitates that out-of-order processors always maintain a mapping of instructions and corresponding registers they use, which makes their design complex and power hungry. With a few exceptions, almost all high-performance CPUs today have out-of-order designs.</p><p>Good compilers are usually extremely aware of processors, and are capable of optimizing the code by rearranging processor instructions in a way that there are fewer bubbles in the processor instruction pipeline. A<a class="indexterm" id="id0232"/> few high-performance CPUs still rely on only static instruction reordering instead of out-of-order instruction reordering and, in turn, save chip area due to simpler design—the saved area is used to accommodate extra cache or CPU cores. Low-power processors, such as those from the ARM and Atom family, use in-order design. Unlike most CPUs, the modern GPUs use in-order design with deep pipelines, which is compensated by very fast context switching. This leads to high latency and high<a class="indexterm" id="id0233"/> throughput on GPUs.</p></div><div class="section" title="Threads and cores"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec014"/>Threads and cores</h3></div></div></div><p>Concurrency and <a class="indexterm" id="id0234"/>parallelism via context switches, hardware threads, and cores are very common today and we have accepted them as a norm to implement in our programs. However, we should understand why we needed such a design in the first place. Most of the<a class="indexterm" id="id0235"/> real-world code we write today does not have more than a modest scope for<a class="indexterm" id="id0236"/> instruction-level parallelism. Even with hardware-based, out-of-order execution and static instruction reordering, no more than two instructions per cycle are truly parallel. Hence, another potential source of instructions that can be pipelined and executed in parallel are the programs other than the currently running one.</p><p>The empty cycles in a pipeline can be<a class="indexterm" id="id0237"/> dedicated to other running programs, which assume that there are other currently running programs that need the processor's attention. <span class="strong"><strong>Simultaneous multithreading</strong></span> (<span class="strong"><strong>SMT</strong></span>)<a class="indexterm" id="id0238"/> is a hardware design that enables such kinds of parallelism. Intel implements SMT named as <span class="strong"><strong>HyperThreading</strong></span>
<a class="indexterm" id="id0239"/> in some of its processors. While SMT presents a single physical processor as two or more logical processors, a true multiprocessor system executes one thread per processor, thus achieving simultaneous execution. A multicore processor includes two or more processors per chip, but has the properties of a multiprocessor system.</p><p>In general, multicore processors significantly outperform SMT processors. Performance on SMT processors can vary by the use case. It peaks in those cases where code is highly variable or threads do not compete for the same hardware resources, and dips when the threads are cache-bound on the same processor. What is also important is that some programs are simply not inherently parallel. In such cases it may be hard to make them go faster without the explicit use of threads in the program.</p></div></div><div class="section" title="Memory systems"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec045"/>Memory systems</h2></div></div></div><p>It is important to <a class="indexterm" id="id0240"/>understand the <a class="indexterm" id="id0241"/>memory performance characteristics to know the likely impact on the programs we write. Data-intensive programs that are also inherently parallel, such as audio/video processing and scientific computation, are largely limited by memory bandwidth, not by the processor. Adding processors would not make them faster unless the memory bandwidth is also increased. Consider another class of programs, such as 3D graphics rendering or database systems that are limited mainly by memory latency but not the memory bandwidth. SMT can be highly suitable for such programs, where threads do not compete for the same hardware resources.</p><p>Memory access roughly constitutes a quarter of all instructions executed by a processor. A code block typically begins with memory-load instructions and the remainder portion depends on the loaded data. This stalls the instructions and prevents large-scale, instruction-level parallelism. As if that was not bad enough, even superscalar processors (which can issue more than one instruction per clock cycle) can issue, at most, two memory instructions per cycle. Building fast memory systems is limited by natural factors such as the speed of light. It impacts the signal round trip to the RAM. This is a natural hard limit and any optimization can only work around it.</p><p>Data transfer between the processor and motherboard chipset is one of the factors that induce memory latency. This is countered using a <a class="indexterm" id="id0242"/>
<span class="strong"><strong>faster front-side bus</strong></span> (<span class="strong"><strong>FSB</strong></span>). Nowadays, most modern processors fix this problem by integrating the memory controller directly at the chip level. The significant difference between the processor versus memory latencies is known as the <a class="indexterm" id="id0243"/>
<span class="strong"><strong>memory wall</strong></span>. This has plateaued in recent times due to processor clock speeds hitting power and heat limits, but notwithstanding this, memory latency continues to be a significant problem.</p><p>Unlike CPUs, GPUs typically realize a sustained high-memory bandwidth. Due to latency hiding, they utilize the bandwidth even during a high number-crunching workload.</p><div class="section" title="Cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec015"/>Cache</h3></div></div></div><p>To overcome the memory <a class="indexterm" id="id0244"/>latency, modern processors employ a special type of very <a class="indexterm" id="id0245"/>fast memory placed onto the processor chip or close to the chip. The purpose of the cache is to store the most recently used data from the memory. Caches are of different levels: <span class="strong"><strong>L1</strong></span> cache is located on the processor chip; <span class="strong"><strong>L2</strong></span> cache is bigger and located farther away from the processor compared to L1. There is often an <span class="strong"><strong>L3</strong></span> cache, which is even bigger and located farther from the processor than L2. In Intel's Haswell processor, the L1 cache is generally 64 kilobytes (32 KB instruction plus 32 KB data) in size, L2 is 256 KB per core, and L3 is 8 MB.</p><p>While memory latency is very bad, fortunately caches seem to work very well. The L1 cache is much faster than accessing the main memory. The reported cache hit rates in real-world programs is 90 percent, which makes a strong case for caches. A cache works like a dictionary of memory addresses to a block of data values. Since the value is a block of memory, the caching of<a class="indexterm" id="id0246"/> adjacent memory locations has mostly no additional <a class="indexterm" id="id0247"/>overhead. Note that L2 is slower and bigger than L1, and L3 is slower and bigger than L2. On Intel Sandybridge processors, register lookup is instantaneous; L1 cache lookup takes three clock cycles, L2 takes nine, L3 takes 21, and main memory access takes 150 to 400 clock cycles.</p></div><div class="section" title="Interconnect"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec016"/>Interconnect</h3></div></div></div><p>A processor<a class="indexterm" id="id0248"/> communicates with the memory and other processors via interconnect that are generally of two types of architecture: <span class="strong"><strong>Symmetric multiprocessing</strong></span> (<span class="strong"><strong>SMP</strong></span>) <a class="indexterm" id="id0249"/>and <a class="indexterm" id="id0250"/>
<span class="strong"><strong>Non-uniform memory access</strong></span> (<span class="strong"><strong>NUMA</strong></span>). In SMP, a bus interconnects processors and memory with the help of bus controllers. The bus acts as a broadcast device for the end points. The bus often becomes a bottleneck with a large number of processors and memory banks. SMP systems are cheaper to build and harder to scale to a large number of cores compared to NUMA. In a NUMA system, collections of processors and memory are connected point to point to other such groups of processors and memory. Every such group is called a node. Local memory of a node is accessible by other nodes and vice versa. Intel's <span class="strong"><strong>HyperTransport</strong></span> <a class="indexterm" id="id0251"/>and <span class="strong"><strong>QuickPath</strong></span>
<a class="indexterm" id="id0252"/> interconnect technologies support NUMA.</p></div></div><div class="section" title="Storage and networking"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec046"/>Storage and networking</h2></div></div></div><p>Storage<a class="indexterm" id="id0253"/> and networking<a class="indexterm" id="id0254"/> are the most commonly used hardware <a class="indexterm" id="id0255"/>components besides the processor, cache, and memory. Many of<a class="indexterm" id="id0256"/> the real-world applications are more often I/O bound than execution-bound. Such I/O technologies are continuously advancing and there is a wide variety of components available in the market. The consideration of such devices should be based on the exact performance and reliability characteristics for the use case. Another important criterion is to know how well they are supported by the target operating system drivers. Current day storage technologies mostly build upon hard disks and solid state drives. The applicability of network devices and protocols vary widely as per the business use case. A detailed discussion of I/O hardware is beyond the scope of this book.</p></div></div></div>
<div class="section" title="The Java Virtual Machine"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec029"/>The Java Virtual Machine</h1></div></div></div><p>The Java Virtual Machine<a class="indexterm" id="id0257"/> is a bytecode-oriented, garbage-collected virtual machine that specifies its own instruction set. The instructions have equivalent bytecodes that are interpreted and compiled to the underlying OS and hardware by the <a class="indexterm" id="id0258"/>
<span class="strong"><strong>Java Runtime Environment</strong></span> (<span class="strong"><strong>JRE</strong></span>). Objects are referred to using symbolic references. The data types in the JVM are fully standardized as a single spec across all JVM implementations on all platforms and architectures. The JVM also follows the network byte order, which means communication between Java programs on different architectures can happen using the big-endian byte order. <span class="strong"><strong>Jvmtop</strong></span> (<a class="ulink" href="https://code.google.com/p/jvmtop/">https://code.google.com/p/jvmtop/</a>) is a handy <a class="indexterm" id="id0259"/>JVM monitoring tool similar to the top command in <a class="indexterm" id="id0260"/>Unix-like systems.</p><div class="section" title="The just-in-time compiler"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec047"/>The just-in-time compiler</h2></div></div></div><p>The <span class="strong"><strong>just-in-time</strong></span> (<span class="strong"><strong>JIT</strong></span>) compiler<a class="indexterm" id="id0261"/> is part of the JVM. When the JVM starts up, the JIT compiler knows hardly anything about the running code so it simply interprets the JVM bytecodes. As the program keeps running, the JIT compiler starts profiling the code by collecting statistics and analyzing the call and bytecode patterns. When a method call count exceeds a certain threshold, the JIT compiler applies a number of optimizations to the code. Most common optimizations are inlining and native code generating. The final and static methods and classes are great candidates for inlining. JIT compilation does not come without a cost; it occupies memory to store the profiled code and sometimes it has to revert the wrong speculative optimization. However, JIT compilation is almost always amortized over a long duration of code execution. In rare cases, turning off JIT compilation may be useful if either the code is too large or there are no hotspots in the code due to infrequent execution.</p><p>A JRE has typically two kinds of JIT compilers: client and server. Which JIT compiler is used by default depends on the type of hardware and platform. The client JIT compiler is meant for client programs such as command-line and desktop applications. We can start the JRE with the <code class="literal">-server</code> option to invoke the server JIT compiler, which is really meant for long-running programs on a server. The threshold for JIT compilation is higher in the server than the client. The difference in the two kinds of JIT compilers is that the client targets upfront, visible lower latency, and the server is assumed to be running on a high-resource hardware and tries to optimize for throughput.</p><p>The JIT compiler in the Oracle HotSpot JVM observes the code execution to determine the most frequently invoked methods, which are hotspots. Such hotspots are usually just a fraction of the entire code that can be cheap to focus on and optimize. The <a class="indexterm" id="id0262"/>
<span class="strong"><strong>HotSpot JIT</strong></span> compiler is lazy and adaptive. It is lazy because it compiles only those methods to native code that have crossed a certain threshold, and not all the code that it encounters. Compiling to native code is a time-consuming process and compiling all code would be wasteful. It is adaptive to gradually increasing the aggressiveness of its compilation on frequently called code, which implies that the code is not optimized only once but many times over as the code gets executed repeatedly. After a method call crosses the first JIT compiler threshold, it is optimized and the counter is reset to zero. At the same time, the optimization count for the code is set to one. When the call exceeds the threshold yet again, the counter is reset to zero and the optimization count is incremented; and this time a more aggressive optimization is applied. This cycle continues until the<a class="indexterm" id="id0263"/> code cannot be optimized anymore.</p><p>The HotSpot JIT compiler<a class="indexterm" id="id0264"/> does a whole bunch of optimizations. Some of the most prominent ones are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Inlining</strong></span>: Inlining of methods—very small methods, the static and final methods, methods in final classes, and small methods involving only primitive numerics are prime candidates for inlining.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Lock elimination</strong></span>: Locking is a performance overhead. Fortunately, if the lock object monitor is not reachable from other threads, the lock is eliminated.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Virtual call elimination</strong></span>: Often, there is only one implementation for an interface in a program. The JIT compiler eliminates the virtual call and replaces that with a direct method call on the class implementation object.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Non-volatile memory write elimination</strong></span>: The non-volatile data members and references in an object are not guaranteed to be visible by the threads other than the current thread. This criterion is utilized not to update such references in memory and rather use hardware registers or the stack via native code.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Native code generation</strong></span>: The JIT compiler generates native code for frequently invoked methods together with the arguments. The generated native code is stored in the code cache.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Control flow and local optimizations</strong></span>: The JIT compiler frequently reorders and splits the code for better performance. It also analyzes the branching of control and optimizes code based on that.</li></ul></div><p>There should rarely be any reason to disable JIT compilation, but it can be done by passing the <code class="literal">-Djava.compiler=NONE</code> parameter when starting the JRE. The default compile threshold can be changed by passing <code class="literal">-XX:CompileThreshold=9800</code> to the JRE executable where <code class="literal">9800</code> is the example threshold. The <code class="literal">XX:+PrintCompilation</code> and <code class="literal">-XX:-CITime</code> options make the JIT compiler print the JIT statistics and time spent on JIT.</p></div><div class="section" title="Memory organization"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec048"/>Memory organization</h2></div></div></div><p>The memory <a class="indexterm" id="id0265"/>used by the JVM is divided into several segments. JVM, being<a class="indexterm" id="id0266"/> a stack-based execution model, one of the memory segments is the stack area. Every thread is given a stack where the stack frames are stored in <span class="strong"><strong>Last-in-First-out</strong></span> (<span class="strong"><strong>LIFO</strong></span>)<a class="indexterm" id="id0267"/> order. The stack includes a <span class="strong"><strong>program counter</strong></span> (<span class="strong"><strong>PC</strong></span>)<a class="indexterm" id="id0268"/> that points to the instruction in the JVM memory currently being executed. When a method is called, a new stack frame is created containing the local variable array and the operand stack. Contrary to conventional stacks, the operand stack holds instructions to load local variable / field values and computation results—a mechanism that is also used to prepare method parameters before a call and to store the return value. The stack frame itself may be allocated on the heap. The easiest way to inspect the order of stack frames in the current thread is to execute the following code:</p><div class="informalexample"><pre class="programlisting">(require 'clojure.repl)
(clojure.repl/pst (Throwable.))</pre></div><p>When a thread requires more stack space than what the JVM can provide, <code class="literal">StackOverflowError</code> is thrown.</p><p>The heap is the main memory area where the object and array allocations are done. It is shared across all JVM threads. The heap may be of a fixed size or expanding, depending on the arguments passed to the JRE on startup. Trying to allocate more heap space than what the JVM can make room for results in <code class="literal">OutOfMemoryError</code> to be thrown. The allocations in the heap are subject to garbage collection. When an object is no more reachable via any reference, it is garbage collected, with the notable exception of weak, soft, and phantom references. Objects pointed to by non-strong references take longer to GC.</p><p>The method area is logically a part of the heap memory and contains per-class structures such as the field and method information, runtime constant pool, code for method, and constructor bodies. It is shared across all JVM threads. In the Oracle HotSpot JVM (up to Version 7), the method area is found in a memory area called the <a class="indexterm" id="id0269"/>
<span class="strong"><strong>permanent generation</strong></span>. In HotSpot Java 8, the permanent generation is replaced by a native memory area called <a class="indexterm" id="id0270"/>
<span class="strong"><strong>Metaspace</strong></span>.</p><div class="mediaobject"><img alt="Memory organization" src="graphics/3642_04_01.jpg"/></div><p>The JVM contains the native code and the Java bytecode to be provided to the Java API implementation and the JVM implementation. The native code call stack is maintained separately for each thread stack. The JVM stack contains the Java method calls. Please note that the<a class="indexterm" id="id0271"/> JVM spec for Java SE 7 and 8 does not imply a <a class="indexterm" id="id0272"/>native method stack, but for Java SE 5 and 6, it does.</p></div><div class="section" title="HotSpot heap and garbage collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec049"/>HotSpot heap and garbage collection</h2></div></div></div><p>The <a class="indexterm" id="id0273"/>Oracle HotSpot JVM uses a<a class="indexterm" id="id0274"/> generational heap. The three main generations are <span class="strong"><strong>Young</strong></span>, <span class="strong"><strong>Tenured</strong></span> (old), and <span class="strong"><strong>Perm</strong></span> (permanent) (up to HotSpot JDK 1.7 only). As objects survive garbage<a class="indexterm" id="id0275"/> collection, they move from <span class="strong"><strong>Eden</strong></span> to <span class="strong"><strong>Survivor</strong></span> and from <span class="strong"><strong>Survivor</strong></span> to <span class="strong"><strong>Tenured</strong></span> <a class="indexterm" id="id0276"/>spaces. The new instances are allocated in the <span class="strong"><strong>Eden</strong></span> segment, which is a very cheap operation (as cheap as a pointer bump, and faster than a C <code class="literal">malloc</code> call), if it already has sufficient free space. When the Eden area does not have enough free space, a minor GC is triggered. This copies the live objects from <span class="strong"><strong>Eden</strong></span> into the <span class="strong"><strong>Survivor</strong></span> space. In the same operation, live objects are checked in <span class="strong"><strong>Survivor-1</strong></span> and copied over to <span class="strong"><strong>Survivor-2</strong></span>, thus keeping the live objects only in <span class="strong"><strong>Survivor-2</strong></span>. This scheme keeps <span class="strong"><strong>Eden</strong></span> and <span class="strong"><strong>Survivor-1</strong></span> empty and unfragmented to make new allocations, and is known<a class="indexterm" id="id0277"/> as <span class="strong"><strong>copy collection</strong></span>.</p><div class="mediaobject"><img alt="HotSpot heap and garbage collection" src="graphics/3642_04_02.jpg"/></div><p>After a certain survival threshold in the young generation, the objects are moved to the tenured/old generation. If it is not possible to do a minor GC, a major GC is attempted. The major GC does not use copying, but rather relies on mark-and-sweep algorithms. We can use throughput collectors (<span class="strong"><strong>Serial</strong></span>, <span class="strong"><strong>Parallel</strong></span>, and <span class="strong"><strong>ParallelOld</strong></span>) or low-pause collectors (<span class="strong"><strong>Concurrent</strong></span> and <span class="strong"><strong>G1</strong></span>) for the old generation. The following table shows a non-exhaustive list of options to be used for each collector type:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Collector name</p>
</th><th style="text-align: left" valign="bottom">
<p>JVM flag</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Serial</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseSerialGC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Parallel</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseParallelGC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Parallel Compacting</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseParallelOldGC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Concurrent</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseConcMarkSweepGC</p>
<p>-XX:+UseParNewGC</p>
<p>-XX:+CMSParallelRemarkEnabled</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>G1</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseG1GC</p>
</td></tr></tbody></table></div><p>The<a class="indexterm" id="id0278"/> previously mentioned flags can be used to start the<a class="indexterm" id="id0279"/> Java<a class="indexterm" id="id0280"/> runtime. For example, in the following command, we<a class="indexterm" id="id0281"/> start the server JVM with a 4 GB heap using Parallel compacting GC:</p><div class="informalexample"><pre class="programlisting">java \
  -server \
  -Xms4096m -Xmx4096m \
  -XX:+UseParallelOldGC XX:ParallelGCThreads=4 \
  -jar application-standalone.jar</pre></div><p>Sometimes, due to running full GC multiple times, the tenured space may have become so fragmented that it may not be feasible to move objects from Survivor to Tenured spaces. In those cases, a full GC with compaction is triggered. During this period, the application may appear unresponsive due to the full GC in action.</p></div><div class="section" title="Measuring memory (heap/stack) usage"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec050"/>Measuring memory (heap/stack) usage</h2></div></div></div><p>One of the <a class="indexterm" id="id0282"/>prime reasons for a performance <a class="indexterm" id="id0283"/>hit in the JVM is garbage collection. It certainly helps to know how heap memory is used by the objects we create and how to reduce the impact on GC by means of a lower footprint. Let's inspect how the representation of an object may lead to heap space.</p><p>Every (uncompressed) object or array reference on a 64-bit JVM is 16 bytes long. On a 32-bit JVM, every reference is 8 bytes long. As the 64-bit architecture is becoming more commonplace now, the 64-bit JVM is more likely to be used on servers. Fortunately, for a heap size of up to 32 GB, the JVM (Java 7) can use compressed pointers (default behavior) that are only 4 bytes in size. Java 8 VMs can address up to 64 GB heap size via compressed pointers as seen in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="top"> </th><th style="text-align: left" valign="bottom">
<p>Uncompressed</p>
</th><th style="text-align: left" valign="bottom">
<p>Compressed</p>
</th><th style="text-align: left" valign="bottom">
<p>32-bit</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Reference (pointer)</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Object header</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Array header</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>12</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Superclass padding</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr></tbody></table></div><p>This table illustrates pointer sizes in different modes (reproduced with permission from Attila Szegedi: <a class="ulink" href="http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20">http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20</a>).</p><p>We saw in the<a class="indexterm" id="id0284"/> previous chapter how <a class="indexterm" id="id0285"/>many bytes each primitive type takes. Let's see how the memory consumption of the composite types looks with compressed pointers (a common case) on a 64-bit JVM with a heap size smaller than 32 GB:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Java Expression</p>
</th><th style="text-align: left" valign="bottom">
<p>64-bit memory usage</p>
</th><th style="text-align: left" valign="bottom">
<p>Description (b = bytes, padding toward memory word size in approximate multiples of 8)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new Object()</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 4 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new byte[0]</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes</p>
</td><td style="text-align: left" valign="top">
<p>12 b <code class="literal">obj</code> header + 4 b <code class="literal">int</code> length = 16 b array header</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new String("foo")</code>
</p>
</td><td style="text-align: left" valign="top">
<p>40 bytes (interned for literals)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + (12 b array header + 6 b char-array content + 4 b length + 2 b padding = 24 b) + 4 b hash</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new Integer(3)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes (boxed integer)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 4 b <code class="literal">int</code> value</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new Long(4)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>24 bytes (boxed long)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 8 b <code class="literal">long</code> value + 4 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">class A { byte x; }</code>
</p>
<p>
<code class="literal">new A();</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 1 b value + 3 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">class B extends A {byte y;}</code>
</p>
<p>
<code class="literal">new B();</code>
</p>
</td><td style="text-align: left" valign="top">
<p>24 bytes (subclass padding)</p>
</td><td style="text-align: left" valign="top">
<p>12 b reference + (1 b value + 7 b padding = 8 b) for A + 1 b for value of <code class="literal">y</code> + 3 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">clojure.lang.Symbol.intern("foo")</code>
</p>
<p>
<code class="literal">// clojure 'foo</code>
</p>
</td><td style="text-align: left" valign="top">
<p>104 bytes (40 bytes interned)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 12 b ns reference + (12 b name reference + 40 b interned chars) + 4 b <code class="literal">int</code> hash + 12 b meta reference + (12 b <code class="literal">_str</code> reference + 40 b interned chars) – 40 b interned <code class="literal">str</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">clojure.lang.Keyword.intern("foo")</code>
</p>
<p>
<code class="literal">// clojure :foo</code>
</p>
</td><td style="text-align: left" valign="top">
<p>184 bytes (fully interned by factory method)</p>
</td><td style="text-align: left" valign="top">
<p>12 b reference + (12 b symbol reference + 104 b interned value) + 4 b <code class="literal">int</code> hash + (12 b <code class="literal">_str</code> reference + 40 b interned <code class="literal">char</code>)</p>
</td></tr></tbody></table></div><p>A comparison of space taken by a symbol and a keyword created from the same given string demonstrates that even though a keyword has slight overhead over a symbol, the keyword is fully interned and would provide better guard against memory consumption and thus GC <a class="indexterm" id="id0286"/>over time. Moreover, the <a class="indexterm" id="id0287"/>keyword is interned as a weak reference, which ensures that it is garbage collected when no keyword in memory is pointing to the interned value anymore.</p><div class="section" title="Determining program workload type"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec017"/>Determining program workload type</h3></div></div></div><p>We often need to <a class="indexterm" id="id0288"/>determine whether a program is CPU/cache bound, memory bound, I/O bound or contention bound. When a program is I/O or contention bound, the CPU usage is generally low. You may have to use a profiler (we will see this in <a class="link" href="ch14.html" title="Chapter 7. Performance Optimization">Chapter 7</a>, <span class="emphasis"><em>Performance Optimization</em></span>) to find out whether threads are stuck due to resource contention. When a program is CPU/cache or memory bound, CPU usage may not be a clear indicator of the source of the bottleneck. In such cases, you may want to make an educated guess by inspecting cache misses in the program. On Linux systems tools such as <a class="indexterm" id="id0289"/>
<span class="strong"><strong>perf</strong></span> (<a class="ulink" href="https://perf.wiki.kernel.org/">https://perf.wiki.kernel.org/</a>), <a class="indexterm" id="id0290"/>
<span class="strong"><strong>cachegrind</strong></span> (<a class="ulink" href="http://valgrind.org/info/tools.html#cachegrind">http://valgrind.org/info/tools.html#cachegrind</a>) and <a class="indexterm" id="id0291"/>
<span class="strong"><strong>oprofile</strong></span> (<a class="ulink" href="http://oprofile.sourceforge.net/">http://oprofile.sourceforge.net/</a>) can help determine the volume of cache misses—a higher threshold may imply that the program is memory bound. However, using these tools with Java is not straightforward because Java's JIT compiler needs a warm-up until meaningful behavior can be observed. The project <a class="indexterm" id="id0292"/>
<span class="strong"><strong>perf-map-agent</strong></span> (<a class="ulink" href="https://github.com/jrudolph/perf-map-agent">https://github.com/jrudolph/perf-map-agent</a>) can help generate method mappings that you can correlate using the <code class="literal">perf</code> utility.</p></div></div><div class="section" title="Tackling memory inefficiency"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec051"/>Tackling memory inefficiency</h2></div></div></div><p>In earlier sections in this<a class="indexterm" id="id0293"/> chapter we discussed that unchecked memory access may become a bottleneck. As of Java 8, due to the way the heap and object references work, we cannot fully control the object layout and memory access patterns. However, we can take care of the frequently executed blocks of code to consume less memory and attempt to make them cache-bound instead of memory-bound at runtime. We can consider a few techniques to lower memory consumption and randomness in access:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Primitive locals (long, double, boolean, char, etc) in the JVM are created on the stack. The rest of the objects are created on the heap and only their references are stored in the stack. Primitives have a low overhead and do not require memory indirection for access, and are hence recommended.</li><li class="listitem" style="list-style-type: disc">Data laid out in the main memory in a sequential fashion is faster to access than randomly laid out data. When we use a large (say more than eight elements) persistent map, the data stored in tries may not be sequentially laid out in memory, rather they would be randomly laid out in the heap. Moreover both keys and values are stored and accessed. When you use records (<code class="literal">defrecord</code>) and types (<code class="literal">deftype</code>), not only do they provide array/class semantics for the layout of fields within them, they do not store the keys, which is very efficient compared to regular maps.</li><li class="listitem" style="list-style-type: disc">Reading large<a class="indexterm" id="id0294"/> content from a disk or the network may have an adverse impact on performance due to random memory roundtrips. In <a class="link" href="ch10.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span>, we briefly discussed memory-mapped byte buffers. You can leverage memory-mapped buffers to minimize fragmented object allocation/access on the heap. While libraries such as<a class="indexterm" id="id0295"/> <code class="literal">nio</code> (<a class="ulink" href="https://github.com/pjstadig/nio/">https://github.com/pjstadig/nio/</a>) and<a class="indexterm" id="id0296"/> <code class="literal">clj-mmap</code> (<a class="ulink" href="https://github.com/thebusby/clj-mmap">https://github.com/thebusby/clj-mmap</a>) help us deal with memory-mapped<a class="indexterm" id="id0297"/> buffers, <code class="literal">bytebuffer</code> (<a class="ulink" href="https://github.com/geoffsalmon/bytebuffer">https://github.com/geoffsalmon/bytebuffer</a>), and<a class="indexterm" id="id0298"/> <code class="literal">gloss</code> (<a class="ulink" href="https://github.com/ztellman/gloss">https://github.com/ztellman/gloss</a>) let us work with byte buffers. There are also alternate abstractions such as <a class="indexterm" id="id0299"/>iota (<a class="ulink" href="https://github.com/thebusby/iota">https://github.com/thebusby/iota</a>) that help us deal with large files as collections.</li></ul></div><p>Given that memory bottleneck is a potential performance issue in data-intensive programs, lowering memory overhead goes a long way in avoiding performance risk. Understanding low-level details of the hardware, the JVM and Clojure's implementation helps us choose the appropriate techniques to tackle the memory bottleneck issue.</p></div></div>
<div class="section" title="Measuring latency with Criterium"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec030"/>Measuring latency with Criterium</h1></div></div></div><p>Clojure has a<a class="indexterm" id="id0300"/> neat little macro called <code class="literal">time</code> that evaluates the<a class="indexterm" id="id0301"/> body of code passed to it, and then prints out the time it took and simply returns the value. However, we can note that often the time taken to execute the code varies quite a bit across various runs:</p><div class="informalexample"><pre class="programlisting">user=&gt; (time (reduce + (range 100000)))
"Elapsed time: 112.480752 msecs"
4999950000
user=&gt; (time (reduce + (range 1000000)))
"Elapsed time: 387.974799 msecs"
499999500000</pre></div><p>There are several <a class="indexterm" id="id0302"/>reasons associated to this variance in behavior. When<a class="indexterm" id="id0303"/> cold started, the JVM has its heap segments empty and is unaware of the code path. As the JVM keeps running, the heap fills up and the GC patterns start becoming noticeable. The JIT compiler gets a chance to profile the different code paths and optimize them. Only after quite some GC and JIT compilation rounds, does the JVM performance become less unpredictable.</p><p>Criterium <a class="indexterm" id="id0304"/>(<a class="ulink" href="https://github.com/hugoduncan/criterium">https://github.com/hugoduncan/criterium</a>) is a Clojure library to scientifically measure the latency of Clojure expressions on a machine. A summary of how it works can be found at the Criterium project page. The easiest way to use Criterium is to use it with Leiningen. If you want Criterium to be available only in the REPL and not as a project dependency, add the following entry to the <code class="literal">~/.lein/profiles.clj</code> file:</p><div class="informalexample"><pre class="programlisting">{:user {:plugins [[criterium "0.4.3"]]}}</pre></div><p>Another way is to include <code class="literal">criterium</code> in your project in the <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">:dependencies [[org.clojure/clojure "1.7.0"]
               [criterium "0.4.3"]]</pre></div><p>Once done with the editing of the file, launch REPL using <code class="literal">lein repl</code>:</p><div class="informalexample"><pre class="programlisting">user=&gt; (require '[criterium.core :as c])
nil
user=&gt; (c/bench (reduce + (range 100000)))
Evaluation count : 1980 in 60 samples of 33 calls.
             Execution time mean : 31.627742 ms
    Execution time std-deviation : 431.917981 us
   Execution time lower quantile : 30.884211 ms ( 2.5%)
   Execution time upper quantile : 32.129534 ms (97.5%)
nil</pre></div><p>Now, we can see that, on average, the expression took 31.6 ms on a certain test machine.</p><div class="section" title="Criterium and Leiningen"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec052"/>Criterium and Leiningen</h2></div></div></div><p>By default, Leiningen <a class="indexterm" id="id0305"/>starts the JVM in a low-tiered compilation mode, which causes it to start up faster but impacts the optimizations that the JRE can perform at runtime. To get the best effects when running tests with Criterium and Leiningen for a server-side use case, be sure to override the defaults in <code class="literal">project.clj</code> as follows:</p><div class="informalexample"><pre class="programlisting">:jvm-opts ^:replace ["-server"]</pre></div><p>The <code class="literal">^:replace</code> hint causes Leiningen to replace its own defaults with what is provided under the <code class="literal">:jvm-opts</code> key. You may like to add more parameters as needed, such as a minimum and maximum heap size to run the tests.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec031"/>Summary</h1></div></div></div><p>The performance of a software system is directly impacted by its hardware components, so understanding how the hardware works is crucial. The processor, caches, memory, and I/O subsystems have different performance behaviors. Clojure, being a hosted language, understanding the performance properties of the host, that is, the JVM, is equally important. The Criterium library is useful for measuring the latency of the Clojure code—we will discuss Criterium again in <a class="link" href="ch13.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span>. In the next chapter we will look at the concurrency primitives in Clojure and their performance characteristics.</p></div>
<div class="chapter" title="Chapter&#xA0;5.&#xA0;Concurrency"><div class="titlepage"><div><div><h1 class="title"><a id="ch12"/>Chapter 5. Concurrency</h1></div></div></div><p>Concurrency was one of the chief design goals of Clojure. Considering the concurrent programming <a class="indexterm" id="id0306"/>model in Java (the comparison with Java is due to it being the predominant language on the JVM), it is not only too low level, but rather tricky to get right that without strictly following the patterns, one is more likely to shoot oneself in the foot. Locks, synchronization, and unguarded mutation are recipes for the concurrency pitfalls, unless exercised with extreme caution. Clojure's design choices deeply influence the way in which the concurrency patterns can be achieved in a safe and functional manner. In this chapter, we will discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The low level concurrency support at the hardware and JVM level</li><li class="listitem" style="list-style-type: disc">The concurrency primitives of Clojure—atoms, agents, refs and vars</li><li class="listitem" style="list-style-type: disc">The built-in concurrency that features in Java safe, and its usefulness with Clojure</li><li class="listitem" style="list-style-type: disc">Parallelization with the Clojure features and reducers</li></ul></div><div class="section" title="Low-level concurrency"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec032"/>Low-level concurrency</h1></div></div></div><p>Concurrency<a class="indexterm" id="id0307"/> cannot be achieved without explicit hardware support. We discussed about SMT and the multi-core processors in the previous chapters. Recall that every processor core has its own L1 cache, and several cores share the L2 cache. The shared L2 cache provides a fast mechanism to the processor cores to coordinate their cache access, eliminating the comparatively expensive memory access. Additionally, a processor buffers the writes to memory into something known as a <span class="strong"><strong>dirty write-buffer</strong></span>. This <a class="indexterm" id="id0308"/>helps the processor to issue a batch of memory update requests, reorder the instructions, and determine the final value to write to memory, known<a class="indexterm" id="id0309"/> as <span class="strong"><strong>write absorption</strong></span>.</p><div class="section" title="Hardware memory barrier (fence) instructions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec053"/>Hardware memory barrier (fence) instructions</h2></div></div></div><p>Memory access reordering is great for a sequential (single-threaded) program performance, but it is hazardous for the concurrent programs where the order of memory access in one thread may disrupt the expectations in another thread. The processor needs the means of synchronizing the access in such a way that memory reordering is either compartmentalized in code segments that do not care, or is prevented where it might have undesirable consequences. The hardware supports such a safety measure in terms of a "memory barrier" (also known as "fence").</p><p>There<a class="indexterm" id="id0310"/> are several kinds of memory barrier instructions found in different architectures, with potentially different performance characteristics. The compiler (or the JIT compiler in the case of the JVM) usually knows about the fence instructions on the architectures that it runs on. The common fence instructions are read, write, acquire, and release barrier, and more. The barriers do not guarantee the latest data, rather they only control the relative ordering of memory access. Barriers cause the write-buffer to be flushed after all the writes are issued, before the barrier is visible to the processor that issued it.</p><p>Read and write barriers control the order of reads and writes respectively. Writes happen via a write-buffer; but reads may happen out of order, or from the write-buffer. To guarantee the correct ordering, acquire, and release, blocks/barriers are used. Acquire and release are considered "half barriers"; both of them together (acquire and release) form a "full barrier". A full barrier is more expensive than a half barrier.</p></div><div class="section" title="Java support and the Clojure equivalent"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec054"/>Java support and the Clojure equivalent</h2></div></div></div><p>In Java, the <a class="indexterm" id="id0311"/>memory barrier instructions are inserted by<a class="indexterm" id="id0312"/> the higher level coordination primitives. Even though fence instructions are expensive (hundreds of cycles) to run, they provide a safety net that makes accessing shared variables safe within the critical sections. In Java, the <code class="literal">synchronized</code> keyword marks a "critical section", which can be executed by only one thread at a time, thus making is a tool for "mutual exclusion". In Clojure, the equivalent of Java's <code class="literal">synchronized</code> is the <code class="literal">locking</code> macro:</p><div class="informalexample"><pre class="programlisting">// Java example
synchronized (someObject) {
    // do something
}
;; Clojure example
(locking some-object
  ;; do something
  )
  </pre></div><p>The <code class="literal">locking</code> macro builds upon two special forms, <code class="literal">monitor-enter</code> and <code class="literal">monitor-exit</code>. Note that the <code class="literal">locking</code> macro is a low-level and imperative solution just like Java's <code class="literal">synchronized</code> – their use is not considered idiomatic Clojure. The special forms <code class="literal">monitor-enter</code> and <code class="literal">monitor-exit</code> respectively enter and exit the lock object's "monitor" – they are even lower level and not recommended for direct use.</p><p>Someone<a class="indexterm" id="id0313"/> measuring the performance of the code that uses<a class="indexterm" id="id0314"/> such locking should be aware of its single-threaded versus the multi-threaded latencies. Locking in a single thread is cheap. However, the performance penalty starts kicking in when there are two or more threads contending for a lock on the same object monitor. A lock is acquired on the monitor of an object called the "intrinsic" or "monitor" lock. Object equivalence (that is, when the <code class="literal">=</code> function returns as true) is never used for the purpose of locking. Make sure that the object references are the same (that is, when <code class="literal">identical?</code> returns as true) when locking from different threads.</p><p>Acquiring a monitor lock by a thread entails a read barrier, which invalidates the thread-local cached data, the corresponding processor registers, and the cache lines. This forces a reread from the memory. On the other hand, releasing the monitor lock results in a write barrier, which flushes all the changes to memory. These are expensive operations that impact parallelism, but they ensure consistency of data for all threads.</p><p>Java supports a <code class="literal">volatile</code> keyword for the data members in a class that guarantees read and write to an attribute outside of a synchronized block that would not be reordered. It is interesting to note that unless an attribute is declared <code class="literal">volatile</code>, it is not guaranteed to be visible in all the threads that are accessing it. The Clojure equivalent of Java's <code class="literal">volatile</code> is the metadata called <code class="literal">^:volatile-mutable</code> that we discussed in <a class="link" href="ch10.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span>. An example of <code class="literal">volatile</code> in Java and Clojure is as follows:</p><div class="informalexample"><pre class="programlisting">// Java example
public class Person {
    volatile long age;
}
;; Clojure example
(deftype Person [^:volatile-mutable ^long age])</pre></div><p>Reading and writing a <code class="literal">volatile</code> data requires read-acquire or write-release respectively, which means we need only a half-barrier to individually read or write the value. Note that due to a half-barrier, the read-followed-by-write operations are not guaranteed to be atomic. For example, the <code class="literal">age++</code> expression first reads the value, then increments and sets it. This makes two memory operations, which is no more a half-barrier.</p><p>Clojure 1.7 introduced a first class support for the volatile data using a new set of functions: <code class="literal">volatile!</code>, <code class="literal">vswap!</code>, <code class="literal">vreset!,</code> and <code class="literal">volatile?</code> These functions define volatile (mutable) data and work with that. However, make a note that these functions do not work with the volatile fields in <code class="literal">deftype</code>. You can see how to use them as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a (volatile! 10))
#'user/a
user=&gt; (vswap! a inc)
11
user=&gt; @a
11
user=&gt; (vreset! a 20)
20
user=&gt; (volatile? a)
true</pre></div><p>Operations <a class="indexterm" id="id0315"/>on volatile data are not atomic, which is <a class="indexterm" id="id0316"/>why even creating a volatile (using <code class="literal">volatile!</code>) is considered potentially unsafe. In general, volatiles may be useful where read consistency is not a high priority but writes must be fast, such as real-time trend analysis, or other such analytics reporting. Volatiles may also be very useful when writing stateful transducers (refer to <a class="link" href="ch09.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span>), serving as very fast state containers. In the next sub-section, we will see the other state abstractions that are safer (and mostly slower) than volatiles.</p></div></div></div>
<div class="section" title="Atomic updates and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec033"/>Atomic updates and state</h1></div></div></div><p>It is a <a class="indexterm" id="id0317"/>common use case to read a data element, execute some logic, and <a class="indexterm" id="id0318"/>update with a new value. For single-threaded programs, it bears no consequences; but for concurrent scenarios, the entire operation must be carried out in a lockstep, as an atomic operation. This case is so common that many processors support this at the hardware level using a special Compare-and-swap (CAS) instruction, which is much cheaper than locking. On x86/x64 architectures, the instruction is called CompareExchange (CMPXCHG).</p><p>Unfortunately, it is possible that another thread updates the variable with the same value that the thread, which is working on the atomic update, is going to compare the old value against. This is known as the "ABA" problem. The set of instructions such as "Load-linked" (LL) and "Store-conditional" (SC), which are found in some other architectures, provide an alternative to CAS without the ABA problem. After the LL instruction reads the value from an address, the SC instruction to update the address with a new value will only go through if the address has not been updated since the LL instruction was successful.</p><div class="section" title="Atomic updates in Java"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec055"/>Atomic updates in Java</h2></div></div></div><p>Java has a<a class="indexterm" id="id0319"/> bunch of built-in lock free, atomic, thread safe compare-and-swap abstractions for the state management. They live in the <code class="literal">java.util.concurrent.atomic</code> package. For primitive types, such as boolean, integer, and long, there are the <code class="literal">AtomicBoolean</code>, <code class="literal">AtomicInteger</code>, and <code class="literal">AtomicLong</code> classes respectively. The latter two classes support additional atomic add/subtract operations. For atomic reference updates, there are the <code class="literal">AtomicReference</code>, <code class="literal">AtomicMarkableReference</code>, and <code class="literal">AtomicStampedReference</code> classes for the arbitrary objects. There is also a support available for arrays where the array elements can be updated atomically—<code class="literal">AtomicIntegerArray</code>, <code class="literal">AtomicLongArray</code>, and <code class="literal">AtomicReferenceArray</code>. They are easy to use; here is the example:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.atomic.AtomicReference)
(def ^AtomicReference x (AtomicReference. "foo"))
(.compareAndSet x "foo" "bar")
(import 'java.util.concurrent.atomic.AtomicInteger)
(def ^AtomicInteger y (AtomicInteger. 10))
(.getAndAdd y 5)</pre></div><p>However, where<a class="indexterm" id="id0320"/> and how to use it is subjected to the update points and the logic in the code. The atomic updates are not guaranteed to be non-blocking. Atomic updates are not a substitute to locking in Java, but rather a convenience, only when the scope is limited to a compare and swap operation for one mutable variable.</p></div><div class="section" title="Clojure's support for atomic updates"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec056"/>Clojure's support for atomic updates</h2></div></div></div><p>Clojure's atomic <a class="indexterm" id="id0321"/>update abstraction is called "atom". It uses <code class="literal">AtomicReference</code> under the hood. An operation on <code class="literal">AtomicInteger</code> or <code class="literal">AtomicLong</code> may be slightly faster than on the Clojure <code class="literal">atom</code>, because the former uses primitives. But neither of them are too cheap, due to the compare-and-swap instruction that they use in the CPU. The speed really depends on how frequently the mutation happens, and how the JIT compiler optimizes the code. The benefit of speed may not show up until the code is run several hundred thousand times, and having an atom mutated very frequently will increase the latency due to the retries. Measuring the latency under actual (or similar to actual) load can tell better. An example of using an atom is as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a (atom 0))
#'user/a
user=&gt; (swap! a inc)
1
user=&gt; @a
1
user=&gt; (compare-and-set! a 1 5)
true
user=&gt; (reset! a 20)
20</pre></div><p>The <code class="literal">swap!</code> function provides a notably different style of carrying out atomic updates than the <code class="literal">compareAndSwap(oldval, newval)</code> methods. While <code class="literal">compareAndSwap()</code> compares and sets the value, returning true if it's a success and false if it's a failure, <code class="literal">swap!</code> keeps on trying to update in an endless loop until it succeeds. This style is a popular pattern that is followed among Java developers. However, there is also a potential pitfall associated with the update-in-loop style. As the concurrency of the updaters gets higher, the performance of the update may gradually degrade. Then again, high concurrency on the atomic updates raises a question of whether or not uncoordinated updates was a good idea at all for the use-case. The <code class="literal">compare-and-set!</code> and <code class="literal">reset!</code> are pretty straightforward.</p><p>The function<a class="indexterm" id="id0322"/> passed to <code class="literal">swap!</code> is required to be pure (as in side effect free), because it is retried several times in a loop during contention. If the function is not pure, the side effect may happen as many times as the retries.</p><p>It is noteworthy that atoms are not "coordinated", which means that when an atom is used concurrently by different threads, we cannot predict the order in which the operations work on it, and we cannot guarantee the end result as a consequence. The code we write around atoms should be designed with this constraint in mind. In many scenarios, atoms may not be a good fit due to the lack of coordination—watch out for that in the program design. Atoms support meta data and basic validation mechanism via extra arguments. The following examples illustrate these features:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def a (atom 0 :meta {:foo :bar}))
user=&gt; (meta a)
{:foo :bar}
user=&gt; (def age (atom 0 :validator (fn [x] (if (&gt; x 200) false true))))
user=&gt; (reset! age 200)
200
user=&gt; (swap! age inc)
IllegalStateException Invalid reference state  clojure.lang.ARef.validate (ARef.java:33)</pre></div><p>The second important thing is that atoms support is adding and removing watches on them. We will discuss watches later in the chapter.</p><div class="section" title="Faster writes with atom striping"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec018"/>Faster writes with atom striping</h3></div></div></div><p>We know<a class="indexterm" id="id0323"/> that atoms present contention when multiple threads try to update the state at the same time. This implies that atoms have great performance when the writes are infrequent. There are some use cases, for example metrics counters, where the writes need to be fast and frequent, but the reads are fewer and can tolerate some inconsistency. For such use cases, instead of directing all the updates to a single atom, we can maintain a bunch of atoms where each thread updates a different atom, thus reducing contention. Reads from these atoms cannot be guaranteed to be consistent. Let's develop an example of such a counter:</p><div class="informalexample"><pre class="programlisting">(def ^:const n-cpu (.availableProcessors (Runtime/getRuntime)))
(def counters (vec (repeatedly n-cpu #(atom 0))))
(defn inc! []
  ;; consider java.util.concurrent.ThreadLocalRandom in Java 7+
  ;; which is faster than Math/random that rand-int is based on
  (let [i (rand-int n-cpu)]
    (swap! (get counters i) inc)))
(defn value []
  (transduce (map deref) + counters))</pre></div><p>In the previous example, we created a vector called <code class="literal">counters</code> of the same size as the number of CPU cores in the computer, and initialize each element with an atom of initial value 0. The function called <code class="literal">inc!</code> updates the counter by picking up a random atom from <code class="literal">counters</code>, and incrementing the value by 1. We also assumed that <code class="literal">rand-int</code> distributes the picking<a class="indexterm" id="id0324"/> up of atom uniformly across all the processor<a class="indexterm" id="id0325"/> cores, so that we have almost zero contention. The <code class="literal">value</code> function simply walks over all the atoms and adds up their <code class="literal">deref</code>'ed values to return the counter value. The example uses <code class="literal">clojure.core/rand-int</code>, which depends on <code class="literal">java.lang.Math/random</code> (due to Java 6 support) to randomly find out the next counter atom. Let's see how we can optimize this when using Java 7 or above:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.ThreadLocalRandom)
(defn inc! []
  (let [i (.nextLong (ThreadLocalRandom/current) n-cpu)]
    (swap! (get counters i) inc)))</pre></div><p>Here, we <code class="literal">import</code> the <code class="literal">java.util.concurrent.ThreadLocalRandom</code> class, and define the <code class="literal">inc!</code> function to pick up the next random atom using <code class="literal">ThreadLocalRandom</code>. Everything else remains the same.</p></div></div></div>
<div class="section" title="Asynchronous agents and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec034"/>Asynchronous agents and state</h1></div></div></div><p>While<a class="indexterm" id="id0326"/> atoms are synchronous, agents are the asynchronous <a class="indexterm" id="id0327"/>mechanism in Clojure to effect any change in the state. Every agent is associated with a mutable state. We pass a function (known as "action") to an agent with the optional additional arguments. This function gets queued for processing in another thread by the agent. All the agents share two common thread pools—one for the low-latency (potentially CPU-bound, cache-bound, or memory-bound) jobs, and one for the blocking (potentially I/O related or lengthy processing) jobs. Clojure provides the <code class="literal">send</code> function for the low-latency actions, <code class="literal">send-off</code> for blocking actions, and <code class="literal">send-via</code> to have the action executed on the user-specified thread-pool, instead of either of the preconfigured thread pools. All of <code class="literal">send</code>, <code class="literal">send-off</code>, and <code class="literal">send-via</code> return immediately. Here is how we can use them:</p><div class="informalexample"><pre class="programlisting">(def a (agent 0))
;; invoke (inc 0) in another thread and set state of a to result
(send a inc)
@a  ; returns 1
;; invoke (+ 1 2 3) in another thread and set state of a to result
(send a + 2 3)
@a  ; returns 6

(shutdown-agents)  ; shuts down the thread-pools
;; no execution of action anymore, hence no result update either
(send a inc)
@a  ; returns 6</pre></div><p>When we inspect<a class="indexterm" id="id0328"/> the Clojure (as of version 1.7.0) source code, we <a class="indexterm" id="id0329"/>can find that the thread-pool for the low-latency actions is named as <code class="literal">pooledExecutor</code> (a bounded thread-pool, initialized to max '2 + number of hardware processors' threads), and the thread-pool for the high-latency actions is named as <code class="literal">soloExecutor</code> (an unbounded thread pool). The premise of this default configuration is that the CPU/cache/memory-bound actions run most optimally on a bounded thread-pool, with the default number of threads. The I/O bound tasks do not consume CPU resources. Hence, a relatively larger number of such tasks can execute at the same time, without significantly affecting the performance of the CPU/cache/memory-bound jobs. Here is how you can access and override the thread-pools:</p><div class="informalexample"><pre class="programlisting">(import 'clojure.lang.Agent)
Agent/pooledExecutor  ; thread-pool for low latency actions
Agent/soloExecutor  ; thread-pool for I/O actions
(import 'java.util.concurrent.Executors)
(def a-pool (Executors/newFixedThreadPool 10))  ; thread-pool with 10 threads
(def b-pool (Executors/newFixedThreadPool 100)) ; 100 threads pool
(def a (agent 0))
(send-via a-pool a inc)  ; use 'a-pool' for the action
(set-agent-send-executor! a-pool)  ; override default thread-pool
(set-agent-send-off-executor! b-pool)  ; override default pool</pre></div><p>If a program carries out a large number of I/O or blocking operations through agents, it probably makes sense to limit the number of threads dedicated for such actions. Overriding the <code class="literal">send-off</code> thread-pool using <code class="literal">set-agent-send-off-executor!</code> is the easiest way to limit the thread-pool size. A more granular way to isolate and limit the I/O actions on the agents is to use <code class="literal">send-via</code> with the thread-pools of appropriate sizes for various kinds of I/O and blocking operations.</p><div class="section" title="Asynchrony, queueing, and error handling"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec057"/>Asynchrony, queueing, and error handling</h2></div></div></div><p>Sending <a class="indexterm" id="id0330"/>an<a class="indexterm" id="id0331"/> action to an agent returns immediately<a class="indexterm" id="id0332"/> without blocking. If the agent is not already busy in executing any action, it "reacts" by enqueuing the action that triggers the execution of the action, in a thread, from the respective thread-pool. If the agent is busy in executing another action, the new action is simply enqueued. Once an action is executed from the action queue, the queue is checked for more entries and triggers the next action, if found. This whole "reactive" mechanism of triggering actions obviates the need of a message loop, polling the queue. This is only possible, because the entry points to an agent's queue are controlled.</p><p>Actions <a class="indexterm" id="id0333"/>are <a class="indexterm" id="id0334"/>executed asynchronously on agents, which<a class="indexterm" id="id0335"/> raises the question of how the errors are handled. Error cases need to be handled with an explicit, predefined function. When using a default agent construction, such as <code class="literal">(agent :foo)</code>, the agent is created without any error handler, and gets suspended in the event of any exception. It caches the exception, and refuses to accept any more actions. It throws the cached exception upon sending any action until the agent is restarted. A suspended agent can be reset using the <code class="literal">restart-agent</code> function. The objective of such suspension is safety and supervision. When the asynchronous actions are executed on an agent and suddenly an error occurs, it will require attention. Check out the following code:</p><div class="informalexample"><pre class="programlisting">(def g (agent 0))
(send g (partial / 10))  ; ArithmeticException due to divide-by-zero
@g  ; returns 0, because the error did not change the old state
(send g inc)  ; throws the cached ArithmeticException
(agent-error g)  ; returns (doesn't throw) the exception object
(restart-agent g @g)  ; clears the suspension of the agent
(agent-error g)  ; returns nil
(send g inc)  ; works now because we cleared the cached error
@g  ; returns 1
(dotimes [_ 1000] (send-off g long-task))
;; block for 100ms or until all actions over (whichever earlier)
(await-for 100 g)
(await g)  ; block until all actions dispatched till now are over</pre></div><p>There are two optional parameters <code class="literal">:error-handler</code> and <code class="literal">:error-mode, which</code> we can configure on an agent to have finer control over the error handling and suspension as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">(def g (agent 0 :error-handler (fn [x] (println "Found:" x))))  ; incorrect arity
(send g (partial / 10))  ; no error encountered because error-handler arity is wrong
(def g (agent 0 :error-handler (fn [ag x] (println "Found:" x))))  ; correct arity
(send g (partial / 10))  ; prints the message
(set-error-handler! g (fn [ag x] (println "Found:" x)))  ; equiv of :error-handler arg
(def h (agent 0 :error-mode :continue))
(send h (partial / 10))  ; error encountered, but agent not suspended
(send h inc)
@h  ; returns 1
(set-error-mode! h :continue)  ; equiv of :error-mode arg, other possible value :fail</pre></div></div><div class="section" title="Why you should use agents"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec058"/>Why you should use agents</h2></div></div></div><p>Just as the "atom" implementation uses only compare-and-swap instead of locking, the underlying "agent" specific implementation uses mostly the compare-and-swap operations. The<a class="indexterm" id="id0336"/> agent implementation uses locks only when dispatching action in a transaction (discussed in the next section), or when restarting an agent. All the actions are queued and dispatched serially in the agents, regardless of the concurrency level. The serial nature makes it possible to execute the actions in an independent and contention-free manner. For the same agent, there can never be more than one action being executed. Since there is no locking, reads (<code class="literal">deref</code> or <code class="literal">@</code>) on agents are never blocked due to writes. However, all the actions are independent of each other—there is no overlap in their execution.</p><p>The implementation goes so far as to ensure that the execution of an action blocks other actions, which follow in the queue. Even though the actions are executed in a thread-pool, actions for the same agent are never executed concurrently. This is an excellent ordering guarantee that also extends a natural coordination mechanism, due to its serial nature. However, note that this ordering coordination is limited to only a single agent. If an agent action sends actions to two other agents, they are not automatically coordinated. In this situation, you may want to use transactions (which will be covered in the next section).</p><p>Since agents distinguish between the low-latency and blocking jobs, the jobs are executed in an appropriate kind of thread-pools. Actions on different agents may execute concurrently, thereby making optimum use of the threading resources. Unlike atoms, the performance of the agents is not impeded by high contention. In fact, for many cases, agents make a lot of sense due to the serial buffering of actions. In general, agents are great for high volume I/O tasks, or where the ordering of operations provides a win in the high contention scenarios.</p></div><div class="section" title="Nesting"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec059"/>Nesting</h2></div></div></div><p>When an<a class="indexterm" id="id0337"/> agent action sends another action to the same agent, that is a case of nesting. This would have been nothing special if agents didn't participate in STM transactions (which will be covered in the next section). However, agents do participate in STM transactions and that places certain constraints on agent implementation that warrants a second-layer buffering of actions. For now, it should suffice to say that the nested sends are queued in a thread-local queue instead of the regular queue in the agent. The thread-local queue is visible only to the thread in which the action is executed. Upon executing an action, unless there was an error, the agent implicitly calls the equivalent of <code class="literal">release-pending-sends</code> function, which transfers the actions from second level thread-local queue to the normal action queue. Note that nesting is simply an implementation detail of agents and <a class="indexterm" id="id0338"/>has no other impact.</p></div></div>
<div class="section" title="Coordinated transactional ref and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec035"/>Coordinated transactional ref and state</h1></div></div></div><p>We saw in <a class="indexterm" id="id0339"/>an earlier section that an atom provides <a class="indexterm" id="id0340"/>atomic read-and-update operation. What if we need to perform an atomic read-and-update operation across two or even more number of atoms? This clearly poses a coordination problem. Some entity has to watch over the process of reading and updating, so that the values are not corrupted. This is what a ref provides—a <span class="strong"><strong>Software Transactional Memory</strong></span> (<span class="strong"><strong>STM</strong></span>) based system that takes care of concurrent atomic read-and-update operations across multiple refs, such that either all the updates go through, or in the case of failure, none does. Like atoms, on failure, refs retry the whole operation from scratch with the new values.</p><p>Clojure's STM implementation is coarse grained. It works at the application level objects and aggregates (that is, references to aggregates), scoped to only all the refs in a program, constituting the "Ref world". Any update to a ref can only happen synchronously, in a transaction, in a <code class="literal">dosync</code> block of code, within the same thread. It cannot span beyond the current thread. The implementation detail reveals that a thread-local transaction context is maintained during a lifetime of a transaction. The same context ceases to be available, the moment the control reaches another thread.</p><p>Like the other reference types in Clojure, reads on a ref are never blocked by the updates, and vice versa. However, unlike the other reference types, the implementation of ref does not depend on a lock-free spinning, but rather, it internally uses locks, a low-level wait/notify, a deadlock detection, and the age-based barging.</p><p>The <code class="literal">alter</code> function is used to read-and-update the value of a ref, and <code class="literal">ref-set</code> is used to reset the value. Roughly, <code class="literal">alter</code> and <code class="literal">ref-set,</code> for the refs, are analogous to <code class="literal">swap!</code> and <code class="literal">reset!</code> for the atoms. Just like <code class="literal">swap!</code>, <code class="literal">alter</code> accepts a function (and arguments) with no side effects, and may be retried several times during the contention. However, unlike with the atoms, not only <code class="literal">alter</code> but also <code class="literal">ref-set</code> and simple <code class="literal">deref</code>, may cause a transaction to be retried during the contention. Here is a very simple example on how we may use a transaction:</p><div class="informalexample"><pre class="programlisting">(def r1 (ref [:a :b :c]))
(def r2 (ref [1 2 3]))
(alter r1 conj :d)  ; IllegalStateException No transaction running...
(dosync (let [v (last @r1)] (alter r1 pop) (alter r2 conj v)))
@r1  ; returns [:a :b]
@r2  ; returns [1 2 3 :c]
(dosync (ref-set r1 (conj @r1 (last @r2))) (ref-set r2 (pop @r2)))
@r1  ; returns [:a :b :c]
@r2  ; returns [1 2 3]</pre></div><div class="section" title="Ref characteristics"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec060"/>Ref characteristics</h2></div></div></div><p>Clojure <a class="indexterm" id="id0341"/>maintains the <span class="strong"><strong>Atomicity</strong></span>, <span class="strong"><strong>Consistency</strong></span>, and <span class="strong"><strong>Isolation</strong></span> (<span class="strong"><strong>ACI</strong></span>) characteristics in a transaction. This overlaps with A, C, and I of the ACID guarantee that many databases provide. Atomicity implies that either all of the updates in a transaction will complete successfully or none of them do. Consistency means that the transaction must maintain general correctness, and should honor the constraints set by the validation—any exception or validation error should roll back the transaction. Unless a shared state is guarded, concurrent updates on it may lead a multi-step transaction into seeing different values at different steps. Isolation implies that all the steps in a transaction will see the same value, no matter how concurrent the updates are.</p><p>The Clojure refs use something known as <span class="strong"><strong>Multi Version Concurrency Control</strong></span> (<span class="strong"><strong>MVCC</strong></span>) to provide <span class="strong"><strong>Snapshot Isolation</strong></span> to the transactions. In MVCC, instead of locking (which could block the transactions), the queues are maintained, so that each transaction can occur using its own snapshot copy, taken at its "read point", independent of other transactions. The main benefit of this approach is that the read-only out-of-transaction operations can go through without any contention. Transactions without the ref contention go through concurrently. In a rough comparison with the database systems, the Clojure ref isolation level is "Read Committed" for reading a Ref outside of a transaction, and "Repeatable Read" by default when inside the transaction.</p></div><div class="section" title="Ref history and in-transaction deref operations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec061"/>Ref history and in-transaction deref operations</h2></div></div></div><p>We discussed earlier that both, read and update operations, on a ref, may cause a transaction<a class="indexterm" id="id0342"/> to be retried. The reads in a transaction <a class="indexterm" id="id0343"/>can be configured to use the ref history in such a manner that the snapshot isolation instances are stored in the history queues, and are used by the read operations in the transactions. The default, which is not supposed to use the history queues, conserves heap space, and provides strong consistency (avoids the staleness of data) in the transactions.</p><p>Using the ref history reduces the likelihood of the transaction retries caused by read contention, thereby providing a weak consistency. Therefore, it is a tool for performance optimization, which comes at the cost of consistency. In many scenarios, programs do not need strong consistency—we can choose appropriately if we know the trade-off, and what we need. The snapshot isolation mechanism in the Clojure ref implementation is backed by the adaptive history queues. The history queues grow dynamically to meet the read requests, and do not overshoot the maximum limit that is set for the ref. By default, the history is not enabled, so we need to specify it during the initialization or set it later. Here is an example of how to use the history:</p><div class="informalexample"><pre class="programlisting">(def r (ref 0 :min-history 5 :max-history 10))
(ref-history-count r)  ; returns 0, because no snapshot instances are queued so far
(ref-min-history r)  ; returns 5
(ref-max-history r)  ; returns 10
(future (dosync (println "Sleeping 20 sec") (Thread/sleep 20000) (ref-set r 10)))
(dosync (alter r inc))  ; enter this within few seconds after the previous expression
;; The message "Sleeping 20 sec" should appear twice due to transaction-retry
(ref-history-count r)  ; returns 2, the number of snapshot history elements
(.trimHistory ^clojure.lang.Ref r)
(ref-history-count r)  ; returns 0 because we wiped the history
(ref-min-history r 10)  ; reset the min history
(ref-max-history r 20)  ; reset the max history count</pre></div><p>Minimum/maximum history limits are proportional to the length of the staleness window of the data. It<a class="indexterm" id="id0344"/> also depends on the relative latency<a class="indexterm" id="id0345"/> difference of the update and read operations to see what the range of the min-history and the max-history works well on a given host system. It may take some amount of trial and error to get the range right. As a ballpark figure, read operations only need as many min-history elements to avoid the transaction retries, as many updates can go through during one read operation. The max-history elements can be a multiple of min-history to cover for any history overrun or underrun. If the relative latency difference is unpredictable, then we have to either plan a min-history for the worst case scenario, or consider other approaches.</p></div><div class="section" title="Transaction retries and barging"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec062"/>Transaction retries and barging</h2></div></div></div><p>A <a class="indexterm" id="id0346"/>transaction can internally be in one <a class="indexterm" id="id0347"/>of the five distinct states—Running, Committing, Retry, Killed, and Committed. A transaction can be killed for various reasons. Exceptions are the common reasons for killing a transaction. But let's consider the corner case where a transaction is retried many times, but it does not appear to commit successfully—what is the resolution? Clojure supports age-based barging, wherein an older transaction automatically tries to abort a younger transaction, so that the younger transaction is retried later. If the barging still doesn't work, as a last resort, the transaction is killed after a hard limit of 10,000 retry attempts, and then the exception is thrown.</p></div><div class="section" title="Upping transaction consistency with ensure"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec063"/>Upping transaction consistency with ensure</h2></div></div></div><p>Clojure's transactional consistency is a good balance between performance and safety. However,<a class="indexterm" id="id0348"/> at times, we may need the <span class="strong"><strong>Serializable</strong></span> consistency in order to preserve the correctness of the transaction. Concretely, in the face of the transaction retries, when a transaction's correctness depends on the state of a ref, in the transaction, wherein the ref is updated simultaneously in another transaction, we have a condition called "write skew". The Wikipedia entry on the write skew, <a class="ulink" href="https://en.wikipedia.org/wiki/Snapshot_isolation">https://en.wikipedia.org/wiki/Snapshot_isolation</a>, describes it well, but let's see a<a class="indexterm" id="id0349"/> more concrete example. Let's say we want to design a flight simulation system with two engines, and one of the system level constraints is not to switch off both engines at the same time. If we model each engine as a ref, and certain maneuvers do require us to switch off an engine, we must ensure that the other engine is on. We can do it with <code class="literal">ensure</code>. Usually, <code class="literal">ensure</code> is required when maintaining a consistent relationship (invariants) across the refs is necessary. This cannot be ensured by the validator functions, because they do not come into play until the transaction commits. The validator functions will see the same value hence cannot help.</p><p>The write-skew can be solved using the namesake <code class="literal">ensure</code> function that essentially prevents a ref from modification by other transactions. It is similar to a locking operation, but in practice, it provides better concurrency than the explicit read-and-update operations, when the retries are expensive. Using <code class="literal">ensure</code> is quite simple—<code class="literal">(ensure ref-object).</code> However, it may be performance-wise expensive, due to the locks it holds during the transaction. Managing performance with <code class="literal">ensure</code> involves a trade-off between the retry latency, and the lost throughput due to the ensured state.</p></div><div class="section" title="Lesser transaction retries with commutative operations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec064"/>Lesser transaction retries with commutative operations</h2></div></div></div><p>Commutative<a class="indexterm" id="id0350"/> operations are independent of the order in which they are applied. For example, incrementing a counter ref c1 from transactions t1 and t2 would have the same effect irrespective of the order in which t1 and t2 commit their changes. Refs have a special optimization for changing functions that are commutative for transactions—the <code class="literal">commute</code> function, which is similar to <code class="literal">alter</code> (same syntax), but with different semantics. Like <code class="literal">alter</code>, the <code class="literal">commute</code> functions are applied atomically during the transaction commit. However, unlike <code class="literal">alter</code>, <code class="literal">commute</code> does not cause the transaction retry on contention, and there is no guarantee about the order in which the <code class="literal">commute</code> functions are applied. This effectively makes <code class="literal">commute</code> nearly useless for returning a meaningful value as a result of the operation. All the commute functions in a transaction are reapplied with the final in transaction ref values during the transaction commit.</p><p>As we can see, commute reduces the contention, thereby optimizing the performance of the overall transaction throughput. Once we know that an operation is commutative and we are not going to use its return value in a meaningful way, there is hardly any trade-off deciding on whether to use it—we should just go ahead and use it. In fact, a program design, with respect to the ref<a class="indexterm" id="id0351"/> transactions, with commute in mind, is not a bad idea.</p></div><div class="section" title="Agents can participate in transactions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec065"/>Agents can participate in transactions</h2></div></div></div><p>In the <a class="indexterm" id="id0352"/>previous section on agents, we discussed how agents work with the queued change functions. Agents can also participate in the ref transactions, thereby making it possible to combine the use of refs and agents in the transactions. However, agents are not included in the "Ref world", hence a transaction scope is not extended till the execution of the change function in an agent. Rather, the transactions only make sure that the changes sent to the agents are queued until the transaction commit happens.</p><p>The <span class="emphasis"><em>Nesting</em></span> sub-section, in the earlier section on agents, discusses about a second-layer thread-local queue. This thread-local queue is used during a transaction to hold the sent changes to an agent until the commit. The thread-local queue does not block the other changes that are being sent to an agent. The out-of-transaction changes are never buffered in the thread-local queue; rather, they are added to the regular queue in the agent.</p><p>The participation of agents in the transactions provides an interesting angle of design, where the coordinated and independent/sequential operations can be pipelined as a workflow for better throughput and performance.</p></div><div class="section" title="Nested transactions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec066"/>Nested transactions</h2></div></div></div><p>Clojure transactions are nesting aware and they compose well. But, why would one need a nested <a class="indexterm" id="id0353"/>transaction? Often, independent units of code may have their own low-granularity transactions that a higher level code can make use of. When the higher level caller itself needs to wrap actions in a transaction, nested transactions occur. Nested transactions have their own lifecycle and run-state. However, an outer transaction can abort an inner transaction on the detection of failure.</p><p>The "ref world" snapshot <code class="literal">ensure</code>s and <code class="literal">commute</code>s are shared among all (that is, outer and inner) levels of a nested transaction. Due to this, the inner transaction is treated as any other ref change operation (similar to <code class="literal">alter</code>, <code class="literal">ref-set</code> and so on) within an outer transaction. The watches and internal lock implementation are handled at the respective nesting level. The detection of contention in the inner transactions causes a restart of not only the inner but also the outer transaction. Commits at all the levels are effected as a global state finally when the outermost transaction commits. The watches, even though tracked at each individual transaction level, are finally effected during the commit. A closer look at the nested transaction implementation shows that nesting has little or no impact on the performance <a class="indexterm" id="id0354"/>of transactions.</p></div><div class="section" title="Performance considerations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec067"/>Performance considerations</h2></div></div></div><p>Clojure Ref is<a class="indexterm" id="id0355"/> likely to be the most complex reference type implemented yet. Due to its characteristics, especially its transaction retry mechanism, it may not be immediately apparent that such a system would have good performance during the high-contention scenarios. </p><p>Understanding its nuances and best ways of use should help:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We do not use changes with the side effects in a transaction, except for possibly sending the I/O changes to agents, where the changes are buffered until the commit. So by definition, we do not carry out any expensive I/O work in a transaction. Hence, a retry of this work would be cheap as well.</li><li class="listitem" style="list-style-type: disc">A change function for a transaction should be as small as possible. This lowers the latency and hence, the retries will also be cheaper.</li><li class="listitem" style="list-style-type: disc">Any ref that is not updated along with at least one more ref simultaneously needs not be a ref—atoms would do just fine in this case. Now that the refs make sense only in a group, their contention is directly proportional to the group size. Small groups of refs used in the transactions lead to a low contention, lower latency, and a higher throughput.</li><li class="listitem" style="list-style-type: disc">Commutative functions provide a good opportunity to enhance the transaction throughput without any penalty. Identifying such cases and designing with commute in mind can help performance significantly.</li><li class="listitem" style="list-style-type: disc">Refs are very coarse grained—they work at the application aggregate level. Often a program may need to have more fine-grained control over the transaction<a class="indexterm" id="id0356"/> resources. This can be enabled by Ref striping, such as Megaref (<a class="ulink" href="https://github.com/cgrand/megaref">https://github.com/cgrand/megaref</a>), by providing a scoped view on the associative refs, thereby allowing higher concurrency.</li><li class="listitem" style="list-style-type: disc">In the high contention scenarios in which the ref group size in a transaction cannot be small, consider using agents, as they have no contention due to the serial nature. Agents may not be a replacement for the transactions, but rather we can employ a pipeline consisting of atoms, refs, and agents to ease out the contention versus latency concerns.</li></ul></div><p>Refs and transactions have an intricate implementation. Fortunately, we can inspect the source code, and browse through available online and offline resources.</p></div></div>
<div class="section" title="Dynamic var binding and state"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec036"/>Dynamic var binding and state</h1></div></div></div><p>The fourth<a class="indexterm" id="id0357"/> kind among the Clojure's reference types is the dynamic var. Since Clojure 1.3, all the vars are static by default. A var must be explicitly declared so<a class="indexterm" id="id0358"/> in order to be dynamic. Once declared, a dynamic var can be bound to new values on per-thread basis. Binding on different threads do not block each other. An example is shown here:</p><div class="informalexample"><pre class="programlisting">(def ^:dynamic *foo* "bar")
(println *foo*)  ; prints bar
(binding [*foo* "baz"] (println *foo*))  ; prints baz
(binding [*foo* "bar"] (set! *foo* "quux") (println *foo*))  ; prints quux</pre></div><p>As the dynamic binding is thread-local, it may be tricky to use in multi-threaded scenarios. Dynamic vars have been long abused by libraries and applications as a means to pass in a common argument to be used by several functions. However, this style is acknowledged to be an anti-pattern, and is discouraged. Typically, in the anti-pattern dynamic, vars are wrapped by a macro to contain the dynamic thread-local binding in the lexical scope. This causes problems with the multi-threading and lazy sequences.</p><p>So, how can the dynamic vars be used effectively? A dynamic var lookup is more expensive than looking up a static var. Even passing a function argument is performance-wise much cheaper than looking up a dynamic var. Binding a dynamic var incurs additional cost. Clearly, in performance sensitive code, dynamic vars are best not used at all. However, dynamic vars may prove to be useful to hold a temporary thread-local state in a complex, or recursive call-graph scenario, where the performance does not matter significantly, without being advertised or leaked into the public API. The dynamic var bindings can nest and unwind like a stack, which makes them both attractive and suitable for such tasks.</p></div>
<div class="section" title="Validating and watching the reference types"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec037"/>Validating and watching the reference types</h1></div></div></div><p>Vars (both static and dynamic), atoms, refs, and agents provide a way to validate the value being<a class="indexterm" id="id0359"/> set as state—a <code class="literal">validator</code> function that accepts<a class="indexterm" id="id0360"/> new value as argument, and returns the logical as true if it succeeds, or throws exception/returns logical as false (the false and nil values) if there's an error. They all honor what the validator function returns. If it is a success, the update goes through, and if an error, an exception is thrown instead. Here is the syntax on how the validators can be declared and associated with the reference types:</p><div class="informalexample"><pre class="programlisting">(def t (atom 1 :validator pos?))
(def g (agent 1 :validator pos?))
(def r (ref 1 :validator pos?))
(swap! t inc)  ; goes through, because value after increment (2) is positive
(swap! t (constantly -3))  ; throws exception
(def v 10)
(set-validator! (var v) pos?)
(set-validator! t (partial &lt; 10)) ; throws exception
(set-validator! g (partial &lt; 10)) ; throws exception
(set-validator! r #(&lt; % 10)) ; works</pre></div><p>Validators<a class="indexterm" id="id0361"/> cause actual failure within a reference type while <a class="indexterm" id="id0362"/>updating them. For vars and atoms, they simply prevent the update by throwing an exception. In an agent, a validation failure causes agent failure, and needs the agent to restart. Inside a ref, the validation failure causes the transaction to rollback and rethrow the exception.</p><p>Another mechanism to observe the changes to the reference types is a "watcher". Unlike validators, a watcher is passive—it is notified of the update after the fact. Hence, a watcher cannot prevent updates from going through, because it is only a notification mechanism. For transactions, a watcher is invoked only after the transaction commit. While only one validator can be set on a reference type, it is possible to associate multiple watchers to a reference type on the other hand. Secondly, when adding a watch, we can specify a key, so that the notifications can be identified by the key, and be dealt accordingly by the watcher. Here is the syntax on how to use watchers:</p><div class="informalexample"><pre class="programlisting">(def t (atom 1))
(defn w [key iref oldv newv] (println "Key:" key "Old:" oldv "New:" newv))
(add-watch t :foo w)
(swap! t inc)  ; prints "Key: :foo Old: 1 New: 2"</pre></div><p>Like validators, the watchers are executed synchronously in the thread of the reference type. For atoms and refs, this may be fine, since the notification to the watchers goes on, the other threads may proceed with their updates. However in agents, the notification happens in the same thread where the update happens—this makes the update latency higher, and the throughput potentially lower.</p></div>
<div class="section" title="Java concurrent data structures"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec038"/>Java concurrent data structures</h1></div></div></div><p>Java has<a class="indexterm" id="id0363"/> a number of mutable data structures that are meant for concurrency and thread-safety, which implies that multiple callers can safely access these data structures at the same time, without blocking each other. When we need only the highly concurrent access without the state management, these data structures may be a very good fit. Several of these employ lock free algorithms. We discussed about the Java atomic state classes in the <span class="emphasis"><em>Atomic updates and state section</em></span>, so we will not repeat them here. Rather, we will only discuss the concurrent queues and other collections. </p><p>All of these data structures live in the <code class="literal">java.util.concurrent</code> package. These concurrent data structures are tailored to leverage<a class="indexterm" id="id0364"/> the JSR 133 "Java Memory Model and <a class="indexterm" id="id0365"/>Thread Specification Revision" (<a class="ulink" href="http://gee.cs.oswego.edu/dl/jmm/cookbook.html">http://gee.cs.oswego.edu/dl/jmm/cookbook.html</a>) implementation that first appeared in Java 5.</p><div class="section" title="Concurrent maps"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec068"/>Concurrent maps</h2></div></div></div><p>Java has a mutable concurrent hash map—<code class="literal">java.util.concurrent.ConcurrentHashMap</code> (CHM in short). The concurrency level can be optionally <a class="indexterm" id="id0366"/>specified when instantiating the class, which is 16 by default. The CHM implementation internally partitions the map entries into the hash buckets, and uses multiple locks to reduce the contention on each bucket. Reads are never blocked by writes, therefore they may be stale or inconsistent—this is countered by built-in detection of such situations, and issuing a lock in order to read the data again in the synchronized fashion. This is an optimization for the scenarios, where reads significantly outnumber writes. In CHM, all the individual operations are near constant-time unless stuck in a retry loop due to the lock contention.</p><p>In contrast with Clojure's persistent map, CHM cannot accept <code class="literal">null</code> (<code class="literal">nil</code>) as the key or value. Clojure's immutable scalars and collections are automatically well-suited for use with CHM. An important thing to note is that only the individual operations in CHM are atomic, and exhibit strong consistency. As CHM operations are concurrent, the aggregate operations provide a rather weak consistency than the true operation-level consistency. Here is how we can use CHM. The individual operations in CHM, which provide a better consistency, are safe to use. The aggregate operations should be reserved for when we know its consistency characteristics, and the related trade-off:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.ConcurrentHashMap)
(def ^ConcurrentHashMap m (ConcurrentHashMap.))
(.put m :english "hi")                    ; individual operation
(.get m :english)                           ; individual operation
(.putIfAbsent m :spanish "alo")    ; individual operation
(.replace m :spanish "hola")         ; individual operation
(.replace m :english "hi" "hello")  ; individual compare-and-swap atomic operation
(.remove m :english)                     ; individual operation
(.clear m)    ; aggregate operation
(.size m)      ; aggregate operation
(count m)    ; internally uses the .size() method
;; aggregate operation
(.putAll m {:french "bonjour" :italian "buon giorno"})
(.keySet m)  ; aggregate operation
(keys m)      ; calls CHM.entrySet() and on each pair java.util.Map.Entry.getKey()
(vals m)       ; calls CHM.entrySet() and on each pair java.util.Map.Entry.getValue()</pre></div><p>The <code class="literal">java.util.concurrent.ConcurrentSkipListMap</code> class (CSLM in short) is another concurrent mutable map data structure in Java. The difference between CHM and CSLM is that CSLM offers a sorted view of the map at all times with the O(log N) time complexity. The <a class="indexterm" id="id0367"/>sorted view has the natural order of keys by default, which can be overridden by specifying a Comparator implementation when instantiating CSLM. The implementation of CSLM is based on the Skip List, and provides navigation operations.</p><p>The <code class="literal">java.util.concurrent.ConcurrentSkipListSet</code> class (CSLS in short) is a concurrent mutable set based on the CSLM implementation. While CSLM offers the map API, CSLS behaves as a set data structure while borrowing features of CSLM.</p></div><div class="section" title="Concurrent queues"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec069"/>Concurrent queues</h2></div></div></div><p>Java has a built-in implementation of several kinds of mutable and concurrent in-memory queues. The<a class="indexterm" id="id0368"/> queue data structure is a useful tool for buffering, producer-consumer style implementation, and for pipelining such units together to form the high-performance workflows. We should not confuse them with durable queues that are used for similar purpose in the batch jobs for a high throughput. Java's in-memory queues are not transactional, but they provide atomicity and strong consistency guarantee for the individual queue operations only. Aggregate operations offer weaker consistency.</p><p>The <code class="literal">java.util.concurrent.ConcurrentLinkedQueue</code> (CLQ) is a lock-free, wait-free unbounded "First In First Out" (FIFO) queue. FIFO implies that the order of the queue elements will not change once added to the queue. CLQ's <code class="literal">size()</code> method is not a constant time operation; it depends on the concurrency level. Few examples of using CLQ are here:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.ConcurrentLinkedQueue)
(def ^ConcurrentLinkedQueue q (ConcurrentLinkedQueue.))
(.add q :foo)
(.add q :bar)
(.poll q)  ; returns :foo
(.poll q)  ; returns :bar</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Queue</p>
</th><th style="text-align: left" valign="bottom">
<p>Blocking?</p>
</th><th style="text-align: left" valign="bottom">
<p>Bounded?</p>
</th><th style="text-align: left" valign="bottom">
<p>FIFO?</p>
</th><th style="text-align: left" valign="bottom">
<p>Fairness?</p>
</th><th style="text-align: left" valign="bottom">
<p>Notes</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>CLQ</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Wait-free, but the size() is not constant time</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ABQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>The capacity is fixed at instantiation</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>DQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>The elements implement the Delayed interface</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LBQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>The capacity is flexible, but with no fairness option</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PBQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>The elements are consumed in a priority order</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>SQ</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>–</p>
</td><td style="text-align: left" valign="top">
<p>–</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>It has no capacity; it serves as a channel</p>
</td></tr></tbody></table></div><p>In the <code class="literal">java.util.concurrent</code> package, <code class="literal">ArrayBlockingQueue</code> (ABQ), <code class="literal">DelayQueue</code> (DQ), <code class="literal">LinkedBlockingQueue</code> (LBQ), <code class="literal">PriorityBlockingQueue</code> (PBQ), and <code class="literal">SynchronousQueue</code> (SQ) implement the <code class="literal">BlockingQueue</code> (BQ) interface. Its Javadoc describes the <a class="indexterm" id="id0369"/>characteristics of its method calls. ABQ is a fixed-capacity, FIFO queue backed by an array. LBQ is also a FIFO queue, backed by the linked nodes, and is optionally bounded (default <code class="literal">Integer.MAX_VALUE</code>). ABQ and LBQ generate "Back pressure" by blocking the enqueue operations on full capacity. ABQ supports optional fairness (with performance overhead) in the order of the threads that access it.</p><p>DQ is an unbounded queue that accepts the elements associated with the delay. The queue elements cannot be null, and must implement the <code class="literal">java.util.concurrent.Delayed</code> interface. Elements are available for removal from the queue only after the delay has been expired. DQ can be very useful for scheduling the processing of the elements at different times.</p><p>PBQ is unbounded and blocking while letting elements be consumed from the queue as per priority. Elements have the natural ordering by default that can be overridden by specifying a Comparator implementation when instantiating the queue.</p><p>SQ is not really a queue at all. Rather, it's just a barrier for a producer or consumer thread. The producer blocks until a consumer removes the element and vice versa. SQ does not have a capacity. However, SQ supports optional fairness (with performance overhead), in the order, in which the threads access it.</p><p>There are some new concurrent queue types introduced after Java 5. Since JDK 1.6, in the <code class="literal">java.util.concurrent</code> package Java has <span class="strong"><strong>BlockingDeque</strong></span> (<span class="strong"><strong>BD</strong></span>) with <span class="strong"><strong>LinkedBlockingDeque</strong></span> (<span class="strong"><strong>LBD</strong></span>) as the only available implementation. BD builds on BQ by adding the <span class="strong"><strong>Deque</strong></span> (<span class="strong"><strong>double-ended queue</strong></span>) operations, that is, the ability to add elements and consume the elements from both the ends of the queue. LBD can be instantiated with an optional capacity (bounded) to block the overflow. JDK 1.7 introduced <span class="strong"><strong>TransferQueue</strong></span> (<span class="strong"><strong>TQ</strong></span>) with <span class="strong"><strong>LinkedTransferQueue</strong></span> (<span class="strong"><strong>LTQ</strong></span>) as the only implementation. TQ extends the concept of SQ in such a way that the producers and consumers block a queue of elements. This will help utilize the producer and consumer threads better by keeping them busy. LTQ is an unbounded implementation of TQ where the <code class="literal">size()</code> method is<a class="indexterm" id="id0370"/> not a constant time operation.</p></div><div class="section" title="Clojure support for concurrent queues"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec070"/>Clojure support for concurrent queues</h2></div></div></div><p>We covered the <a class="indexterm" id="id0371"/>persistent queue in <a class="link" href="ch09.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span> earlier. Clojure has a built-in <code class="literal">seque</code> function that builds over a BQ implementation (LBQ by default) to expose a write-ahead sequence. The sequence is potentially lazy, and the write-ahead buffer throttles how many elements to realize. As opposed to the chunked sequences (of chunk size 32), the size of the write-ahead buffer is controllable and potentially populated at all times until the source sequence is exhausted. Unlike the chunked sequences, the realization doesn't happen suddenly for a chunk of 32 elements. It does so gradually and smoothly.</p><p>Under the hood, Clojure's <code class="literal">seque</code> uses an agent to the backfill data in the write-ahead buffer. In the arity-2 variant of <code class="literal">seque</code>, the first argument should either be a positive integer, or an instance of BQ (ABQ, LBQ, and more) that is preferably bounded.</p></div></div>
<div class="section" title="Concurrency with threads"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec039"/>Concurrency with threads</h1></div></div></div><p>On<a class="indexterm" id="id0372"/> the JVM, threads are the de-facto fundamental instrument of concurrency. Multiple threads live in the same JVM; they share the heap space, and compete for the resources.</p><div class="section" title="JVM support for threads"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec071"/>JVM support for threads</h2></div></div></div><p>The JVM <a class="indexterm" id="id0373"/>threads are the Operating System threads. Java wraps an underlying OS thread as an instance of the <code class="literal">java.lang.Thread</code> class, and builds up an API around it to work with threads. A thread on the JVM has a number of states: New, Runnable, Blocked, Waiting, Timed_Waiting, and Terminated. A thread is instantiated by overriding the <code class="literal">run()</code> method of the <code class="literal">Thread</code> class, or by passing an instance of the <code class="literal">java.lang.Runnable</code> interface to the constructor of the <code class="literal">Thread</code> class. </p><p>Invoking the <code class="literal">start()</code> method of a <code class="literal">Thread</code> instance starts its execution in a new thread. Even if just a single thread runs in the JVM, the JVM would not shut down. Calling the <code class="literal">setDaemon(boolean)</code> method of a thread with argument <code class="literal">true</code> tags the thread as a daemon that can be automatically shut down if no other non-daemon thread is running.</p><p>All Clojure functions implement the <code class="literal">java.lang.Runnable</code> interface. Therefore, invoking a function in a new thread is very easy:</p><div class="informalexample"><pre class="programlisting">(defn foo5 [] (dotimes [_ 5] (println "Foo")))
(defn barN [n] (dotimes [_ n] (println "Bar")))
(.start (Thread. foo5))  ; prints "Foo" 5 times
(.start (Thread. (partial barN 3)))  ; prints "Bar" 3 times</pre></div><p>The <code class="literal">run()</code> method does not accept any argument. We can work around it by creating a higher order <a class="indexterm" id="id0374"/>function that needs no arguments, but internally applies the argument <code class="literal">3</code>.</p></div><div class="section" title="Thread pools in the JVM"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec072"/>Thread pools in the JVM</h2></div></div></div><p>Creating<a class="indexterm" id="id0375"/> threads leads to the Operating System API calls, which is not always a cheap operation. The general practice is to create a pool of threads that can be recycled for different tasks. Java has a built-in support for threads pools. The interface called <code class="literal">java.util.concurrent.ExecutorService</code> represents the API for a thread pool. The most common way to create a thread pool is to use a factory method in the <code class="literal">java.util.concurrent.Executors</code> class:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.Executors)
(import 'java.util.concurrent.ExecutorService)
(def ^ExecutorService a (Executors/newSingleThreadExecutor))  ; bounded pool
(def ^ExecutorService b (Executors/newCachedThreadPool))  ; unbounded pool
(def ^ExecutorService c (Executors/newFixedThreadPool 5))  ; bounded pool
(.execute b #(dotimes [_ 5] (println "Foo")))  ; prints "Foo" 5 times</pre></div><p>The previous example is equivalent of the examples with raw threads that we saw in the previous sub-section. Thread pools are also capable of helping to track the completion, and the return value of a function, executed in a new thread. An ExecutorService accepts an instance of the <code class="literal">java.util.concurrent.Callable</code> instance as an argument to several methods that launch a task, and return <code class="literal">java.util.concurrent.Future</code> to track the final result. </p><p>All the Clojure functions also implement the <code class="literal">Callable</code> interface, so we can use them as follows:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.Callable)
(import 'java.util.concurrent.Future)
(def ^ExecutorService e (Executors/newSingleThreadExecutor))
(def ^Future f (.submit e (cast Callable #(reduce + (range 10000000)))))
(.get f)  ; blocks until result is processed, then returns it</pre></div><p>The thread pools described here are the same as the ones that we saw briefly in the Agents section earlier. Thread pools need to be shut down by calling the <code class="literal">shutdown()</code> method when no longer required.</p></div><div class="section" title="Clojure concurrency support"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec073"/>Clojure concurrency support</h2></div></div></div><p>Clojure has some nifty built-in features to deal with concurrency. We already discussed about the <a class="indexterm" id="id0376"/>agents, and how they use the thread pools, in an earlier section. There are some more concurrency features in Clojure to deal with the various use cases.</p><div class="section" title="Future"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec019"/>Future</h3></div></div></div><p>We saw <a class="indexterm" id="id0377"/>earlier in this section how to use the Java API to launch a new thread, to execute a function. Also, we learned how to get the result back. Clojure has a built-in support called "futures" to do these things in a much smoother and integrated manner. The basis of the futures is the function <code class="literal">future-call</code> (it takes a <code class="literal">no-arg</code> function as an argument), and the macro <code class="literal">future</code> (it takes the body of code) that builds on the former. Both of them immediately start a thread to execute the supplied code. The following snippet illustrates the functions that work with the future, and how to use them:</p><div class="informalexample"><pre class="programlisting">;; runs body in new thread
(def f (future (println "Calculating") (reduce + (range 1e7))))
(def g (future-call #(do (println "Calculating") (reduce + (range 1e7)))))  ; takes no-arg fn
(future? f)                  ; returns true
(future-cancel g)        ; cancels execution unless already over (can stop mid-way)
(future-cancelled? g) ; returns true if canceled due to request
(future-done? f)         ; returns true if terminated successfully, or canceled
(realized? f)               ; same as future-done? for futures
@f                              ; blocks if computation not yet over (use deref for timeout)</pre></div><p>One of the interesting aspects of <code class="literal">future-cancel</code> is that it can sometimes not only cancel tasks that haven't started yet, but may also abort those that are halfway through execution:</p><div class="informalexample"><pre class="programlisting">(let [f (future (println "[f] Before sleep")
                (Thread/sleep 2000)
                (println "[f] After sleep")
                2000)]
  (Thread/sleep 1000)
  (future-cancel f)
  (future-cancelled? f))
;; [f] Before sleep  ← printed message (second message is never printed)
;; true  ← returned value (due to future-cancelled?)</pre></div><p>The previous scenario happens because Clojure's <code class="literal">future-cancel</code> cancels a future in such a way that if the execution has already started, it may be interrupted causing <code class="literal">InterruptedException</code>, which, if not explicitly caught, would simply abort the block of code. Beware <a class="indexterm" id="id0378"/>of exceptions arising from the code that is executed in a future, because, by default, they are not reported verbosely! Clojure futures use the "solo" thread pool (used to execute the potentially blocking actions) that we discussed earlier with respect to the agents.</p></div><div class="section" title="Promise"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec020"/>Promise</h3></div></div></div><p>A promise is <a class="indexterm" id="id0379"/>a placeholder for the result of a computation that may or may not have occurred. A promise is not directly associated with any computation. By definition, a promise does not imply when the computation might occur, hence realizing the promise.</p><p>Typically, a promise originates from one place in the code, and is realized by some other portion of the code that knows when and how to realize the promise. Very often, this happens in a multi-threaded code. If a promise is not realized yet, any attempt to read the value blocks all callers. If a promise is realized, then all the callers can read the value without being blocked. As with futures, a promise can be read with a timeout using <code class="literal">deref</code>. </p><p>Here is a very simple example showing how to use promises:</p><div class="informalexample"><pre class="programlisting">(def p (promise))
(realized? p)  ; returns false
@p  ; at this point, this will block until another thread delivers the promise
(deliver p :foo)
@p  ; returns :foo (for timeout use deref)</pre></div><p>A promise is a very powerful tool that can be passed around as function arguments. It can be stored in a reference type, or simply be used for a high level coordination.</p></div></div></div>
<div class="section" title="Clojure parallelization and the JVM"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec040"/>Clojure parallelization and the JVM</h1></div></div></div><p>We <a class="indexterm" id="id0380"/>observed in <a class="link" href="ch15.html" title="Chapter 8. Application Performance">Chapter 1</a>, <span class="emphasis"><em>Performance by Design</em></span> that parallelism is a function<a class="indexterm" id="id0381"/> of the hardware, whereas concurrency is a function of the software, assisted by the hardware support. Except for the algorithms that are purely sequential by nature, concurrency is the favored means to facilitate parallelism, and achieve better performance. Immutable and stateless data is a catalyst to concurrency, as there is no contention between threads, due to absence of mutable data.</p><div class="section" title="Moore's law"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec074"/>Moore's law</h2></div></div></div><p>In 1965, Intel's cofounder, Gordon Moore, made an observation that the number of transistors<a class="indexterm" id="id0382"/> per square inch on Integrated Circuits doubles every 24 months. He also predicted that the trend would continue for 10 years, but in practice, it has continued till now, marking almost half a century. More transistors have resulted in more computing power. With a greater number of transistors in the same area, we need higher clock speed to transmit signals to all of the transistors. Secondly, transistors need to get smaller in size to fit in. Around 2006-2007, the clock speed that the circuitry could work with topped out at about 2.8GHz, due to the heating issues and the laws of physics. Then, the multi-core processors were born.</p></div><div class="section" title="Amdahl's law"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec075"/>Amdahl's law</h2></div></div></div><p>The multi-core<a class="indexterm" id="id0383"/> processors naturally require splitting up computation in order to achieve parallelization. Here begins a conflict—a program that was made to be run sequentially cannot make use of the parallelization features of the multi-core processors. The program must be altered to find the opportunity to split up computation at every step, while keeping the cost of coordination in mind. This results in a limitation that a program can be no more faster than its longest sequential part (<span class="emphasis"><em>contention</em></span>, or <span class="emphasis"><em>seriality</em></span>), and the coordination overhead. This characteristic was described by Amdahl's law.</p></div><div class="section" title="Universal Scalability Law"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec076"/>Universal Scalability Law</h2></div></div></div><p>Dr Neil Gunther's Universal Scalability Law (USL) is a superset of Amdahl's Law that makes<a class="indexterm" id="id0384"/> both: <span class="emphasis"><em>contention (α)</em></span> and <span class="emphasis"><em>coherency (β)</em></span> the first class concerns in quantifying the scalability very closely to the realistic parallel systems. Coherency implies the coordination overhead (latency) in making the result of one part of a parallelized program to be available to another. While Amdahl's Law states that contention (seriality) causes performance to level off, USL goes to show that the performance actually degrades with excessive parallelization. USL is described with the following formula:</p><p>C(N) = N / (1 + α ((N – 1) + β N (N – 1)))</p><p>Here, C(N) implies relative capacity or throughput in terms of the source of concurrency, such as physical processors, or the users driving the software application. α implies the degree of contention because of the shared data or the sequential code, and β implies penalty <a class="indexterm" id="id0385"/>incurred for maintaining the consistency of shared data. I would encourage you to pursue USL further (<a class="ulink" href="http://www.perfdynamics.com/Manifesto/USLscalability.html">http://www.perfdynamics.com/Manifesto/USLscalability.html</a>), as this is a very important resource for studying the impact of concurrency on scalability and the performance of the systems.</p></div><div class="section" title="Clojure support for parallelization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec077"/>Clojure support for parallelization</h2></div></div></div><p>A <a class="indexterm" id="id0386"/>program that relies on mutation cannot parallelize its parts without creating contention on the mutable state. It requires coordination overhead, which makes the situation worse. Clojure's immutable nature is better suited to parallelize the parts of a program. Clojure also has some constructs that are suited for parallelism by the virtue of Clojure's consideration of available hardware resources. The result is, the operations execute optimized for certain use case scenarios.</p><div class="section" title="pmap"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec021"/>pmap</h3></div></div></div><p>The <code class="literal">pmap</code> function (similar to <code class="literal">map</code>) accepts as arguments a function and one, or more collections<a class="indexterm" id="id0387"/> of data elements. The function is applied to each of the data elements in such a way that some of the elements are processed by the function in parallel. The parallelism factor is chosen at runtime by the <code class="literal">pmap</code> implementation, as two greater than the total number of available processors. It still processes the elements lazily, but the realization factor is same as the parallelism factor. </p><p>Check out the following code:</p><div class="informalexample"><pre class="programlisting">(pmap (partial reduce +)
        [(range 1000000)
         (range 1000001 2000000)
         (range 2000001 3000000)])</pre></div><p>To use <code class="literal">pmap</code> effectively, it is imperative that we understand what it is meant for. As the documentation says, it is meant for computationally intensive functions. It is optimized for CPU-bound and cache-bound jobs. High latency and low CPU tasks, such as blocking I/O, are a gross misfit for <code class="literal">pmap</code>. Another pitfall to be aware of is whether the function used in <code class="literal">pmap</code> performs a lot of memory operations or not. Since the same function will be applied across all the threads, all the processors (or cores) may compete for the memory interconnect and the sub-system bandwidth. If the parallel memory access becomes a bottleneck, <code class="literal">pmap</code> cannot make the operation truly parallel, due to the contention on the memory access.</p><p>Another concern is what happens when several <code class="literal">pmap</code> operations run concurrently? Clojure does not attempt to detect multiple <code class="literal">pmap</code>s running concurrently. The same number of threads will be launched afresh for every new <code class="literal">pmap</code> operation. The developer is responsible to ensure the performance characteristics, and the response time of the program resulting from the concurrent pmap executions. Usually, when the latency reasons are paramount, it is advisable to limit the concurrent instances of <code class="literal">pmap</code> running in the program.</p></div><div class="section" title="pcalls"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec022"/>pcalls</h3></div></div></div><p>The <code class="literal">pcalls</code> function <a class="indexterm" id="id0388"/>is built using <code class="literal">pmap</code>, so it borrows properties from the latter. However, the <code class="literal">pcalls</code> function accepts zero or more functions as arguments and executes them in parallel, returning the result values<a class="indexterm" id="id0389"/> of the calls as a list.</p></div><div class="section" title="pvalues"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec023"/>pvalues</h3></div></div></div><p>The <code class="literal">pvalues</code> macro is built using <code class="literal">pcalls</code>, so it transitively shares the properties of <code class="literal">pmap</code>. It's<a class="indexterm" id="id0390"/> behavior is similar to <code class="literal">pcalls</code>, but instead of functions, it accepts zero or more S-expressions that are evaluated in the parallel using <code class="literal">pmap</code>.</p></div></div><div class="section" title="Java 7's fork/join framework"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec078"/>Java 7's fork/join framework</h2></div></div></div><p>Java 7 introduced <a class="indexterm" id="id0391"/>a new framework for parallelism called "fork/join," based on divide-and-conquer and the work-stealing scheduler algorithms. The basic idea of how to use the fork/join framework is fairly simple—if the work is small enough, then do it directly in the same thread; otherwise, split the work into two pieces, invoke them in a fork/join thread pool, and wait for the results to combine. </p><p>This way, the job gets recursively split into smaller parts such as an inverted tree, until the smallest part can be carried out in just a single thread. When the leaf/subtree jobs return, the parent combines the result of all children, and returns the results.</p><p>The fork/join framework is implemented in Java 7 in terms of a special kind of thread pool; check out <code class="literal">java.util.concurrent.ForkJoinPool</code>. The specialty of this thread pool is that it accepts the jobs of <code class="literal">java.util.concurrent.ForkJoinTask</code> type, and whenever these jobs block, waiting for the child jobs to finish, the threads used by the waiting jobs are allocated to the child jobs. When the child finishes its work, the thread is allocated back to the blocked parent jobs in order to continue. This style of dynamic thread allocation is described as "work-stealing". The fork/join framework can be used from within Clojure. The <code class="literal">ForkJoinTask</code> interface has two implementations: <code class="literal">RecursiveAction</code> and <code class="literal">RecursiveTask</code> in the <code class="literal">java.util.concurrent</code> package. Concretely, <code class="literal">RecursiveTask</code> maybe more useful with Clojure, as <code class="literal">RecursiveAction</code> is designed to work with mutable data, and does not return any value from its operation.</p><p>Using the fork-join framework entails choosing the batch size to split a job into, which is a crucial factor in parallelizing a long job. Too large a batch size may not utilize all the CPU cores enough; on the other hand, a small batch size may lead to a longer overhead, coordinating across the parent/child batches. As we will see in the next section, Clojure integrates with the Fork/join framework to parallelize the reducers implementation.</p></div></div>
<div class="section" title="Parallelism with reducers"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec041"/>Parallelism with reducers</h1></div></div></div><p>Reducers <a class="indexterm" id="id0392"/>are a new abstraction introduced in Clojure 1.5, and are likely to have a wider impact on the rest of the Clojure implementation in the future versions. They depict a different way of thinking about processing collections in Clojure—the key concept is to break down the notion that collections can be processed only sequentially, lazily, or producing a seq, and more. Moving away from such a behavior guarantee raises the potential for eager and parallel operations on one hand, whereas incurring constraints on the other. Reducers are compatible with the existing collections.</p><p>For an <a class="indexterm" id="id0393"/>example, a keen observation of the regular <code class="literal">map</code> function reveals that its classic definition is tied to the mechanism (recursion), order (sequential), laziness (often), and representation (list/seq/other) aspects of producing the result. Most of this actually defines "how" the operation is performed, rather than "what" needs to be done. In the case of <code class="literal">map</code>, the "what" is all about applying a function to each element of its collection arguments. But since the collection types can be of various types (tree-structured, sequence, iterator, and more), the operating function cannot know how to navigate the collection. Reducers decouple the "what" and "how" parts of the operation.</p><div class="section" title="Reducible, reducer function, reduction transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec079"/>Reducible, reducer function, reduction transformation</h2></div></div></div><p>Collections <a class="indexterm" id="id0394"/>are of various kinds, hence only a <a class="indexterm" id="id0395"/>collection knows how to navigate itself. In the reducers model at a fundamental level, an internal "reduce" operation in<a class="indexterm" id="id0396"/> each collection type has access to its properties and behavior, and access to what it returns. This makes all the collection types essentially "reducible". All the operations that work with collections can be modeled in terms of the internal "reduce" operation. The new modeled form of such operations is a "reducing function", which is typically a function of two arguments, the first argument being the accumulator, and the second being the new input.</p><p>How does it work when we need to layer several functions upon another, over the elements of a collection? For an example, let's say first we need to "filter", "map," and then "reduce". In such cases, a "transformation function" is used to model a reducer function (for example, for "filter") as another reducer function (for "map") in such a way that it adds the functionality during the transformation. This is called "reduction transformation".</p></div><div class="section" title="Realizing reducible collections"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec080"/>Realizing reducible collections</h2></div></div></div><p>While <a class="indexterm" id="id0397"/>the reducer functions retain the purity of the abstraction, they are not useful all by themselves. The reducer operations in the namespace called as <code class="literal">clojure.core.reducers</code> similar to <code class="literal">map</code>, <code class="literal">filter</code>, and more, basically return a reducible collection that embed the reducer functions within themselves. A reducible collection is not realized, not even lazily realized—rather, it is just a recipe that is ready to be realized. In order to realize a reducible collection, we must use one of the <code class="literal">reduce</code> or <code class="literal">fold</code> operations.</p><p>The <code class="literal">reduce</code> operation that realizes a reducible collection is strictly sequential, albeit with the performance gains compared to <code class="literal">clojure.core/reduce</code>, due to reduced object allocations on the heap. The <code class="literal">fold</code> operation, which realizes a reducible collection, is potentially <a class="indexterm" id="id0398"/>parallel, and uses a "reduce-combine" approach over the fork-join framework. Unlike the traditional "map-reduce" style, the use of fork/join the reduce-combine approach reduces at the bottom, and subsequently combines by the means of reduction again. This makes the <code class="literal">fold</code> implementation less wasteful and better performing.</p></div><div class="section" title="Foldable collections and parallelism"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec081"/>Foldable collections and parallelism</h2></div></div></div><p>Parallel reduction by <code class="literal">fold</code> puts certain constraints on the collections and operations. The tree-based <a class="indexterm" id="id0399"/>collection types (persistent map, persistent vector, and persistent set) are amenable to parallelization. At the same time, the sequences may not be parallelized by <code class="literal">fold</code>. Secondly, <code class="literal">fold</code> requires that the<a class="indexterm" id="id0400"/> individual reducer functions should be "associative", that is, the order of the input arguments applied to the reducer function should not matter. The reason being, <code class="literal">fold</code> can segment the elements of the collection to process in parallel, and the order in which they may be combined is not known in advance.</p><p>The <code class="literal">fold</code> function accepts few extra arguments, such as the "combine function," and the partition batch size (default being 512) for the parallel processing. Choosing the optimum partition size depends on the jobs, host capabilities, and the performance benchmarking. There are certain functions that are foldable (that is, parallelizable by <code class="literal">fold</code>), and there are others that are not, as shown here. They live in the <code class="literal">clojure.core.reducers</code> namespace:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Foldable</strong></span>: <code class="literal">map</code>, <code class="literal">mapcat</code>, <code class="literal">filter</code>, <code class="literal">remove</code>, and <code class="literal">flatten</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Non-foldable</strong></span>: <code class="literal">take-while</code>, <code class="literal">take</code>, and <code class="literal">drop</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Combine functions</strong></span>: <code class="literal">cat</code>, <code class="literal">foldcat</code>, and <code class="literal">monoid</code></li></ul></div><p>A notable aspect of reducers is that it is foldable in parallel only when the collection is a tree type. This implies that the entire data set must be loaded in the heap memory when folding over them. This has the downside of memory consumption during the high load on a system. On the other hand, a lazy sequence is a perfectly reasonable solution for such scenarios. When processing large amount of data, it may make sense to use a combination of lazy sequences and reducers for performance.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec042"/>Summary</h1></div></div></div><p>Concurrency and parallelism are extremely important for performance in this multi-core age. Effective use of concurrency requires substantial understanding of the underlying principles and details. Fortunately, Clojure provides safe and elegant ways to deal with concurrency and state. Clojure's new feature called "reducers" provides a way to achieve granular parallelism. In the coming years, we are likely to see more and more processor cores, and an increasing demand to write code that takes advantage of these. Clojure places us in the right spot to meet such challenges.</p><p>In the next chapter, we will look at the performance measurement, analysis, and monitoring.</p></div>
<div class="chapter" title="Chapter&#xA0;6.&#xA0;Measuring Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch13"/>Chapter 6. Measuring Performance</h1></div></div></div><p>Depending on the expected and actual performance, and the lack or presence of a measuring system, performance analysis and tuning can be a fairly elaborate process. Now we will discuss the analysis of performance characteristics and ways to measure and monitor them. In this chapter we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Measuring performance and understanding the results</li><li class="listitem" style="list-style-type: disc">What performance tests to carry out for different purposes</li><li class="listitem" style="list-style-type: disc">Monitoring performance and obtaining metrics</li><li class="listitem" style="list-style-type: disc">Profiling Clojure code to identify performance bottlenecks</li></ul></div><div class="section" title="Performance measurement and statistics"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec043"/>Performance measurement and statistics</h1></div></div></div><p>Measuring <a class="indexterm" id="id0401"/>performance is the stepping stone to performance <a class="indexterm" id="id0402"/>analysis. As we noted earlier in this book, there are several performance parameters to be measured with respect to various scenarios. Clojure's built-in <code class="literal">time</code> macro is a tool to measure the amount of time elapsed while executing a body of code. Measuring performance factors is a much more involved process. The measured performance numbers may have linkages with each other that we need to analyze. It is a common practice to use statistical concepts to establish the linkage factors. We will discuss some basic statistical concepts in this section and use that to explain how the measured data gives us the bigger picture.</p><div class="section" title="A tiny statistics terminology primer"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec082"/>A tiny statistics terminology primer</h2></div></div></div><p>When we have a series<a class="indexterm" id="id0403"/> of quantitative data, such as latency in milliseconds for the same operation (measured over a number of executions), we can observe a number of things. First, and the most obvious, are the minimum and maximum values in the data. Let's take an example dataset to analyze further:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>19</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td><td style="text-align: left" valign="top">
<p>168</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>29</p>
</td><td style="text-align: left" valign="top">
<p>172</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td></tr></tbody></table></div><div class="section" title="Median, first quartile, third quartile"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec024"/>Median, first quartile, third quartile</h3></div></div></div><p>We can see that the minimum latency here is 19 ms whereas the maximum latency is 172ms. We can also observe that the average latency here is about 40ms. Let's sort this data in ascending order:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>19</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td><td style="text-align: left" valign="top">
<p>26</p>
</td><td style="text-align: left" valign="top">
<p>29</p>
</td><td style="text-align: left" valign="top">
<p>168</p>
</td><td style="text-align: left" valign="top">
<p>172</p>
</td></tr></tbody></table></div><p>The center element of the <a class="indexterm" id="id0404"/>previous dataset, that is the ninth element (value 23), is considered the <span class="strong"><strong>median</strong></span>
<a class="indexterm" id="id0405"/> of the dataset. It is noteworthy that the median is a better representative of the center of the data than the <a class="indexterm" id="id0406"/>
<span class="strong"><strong>average</strong></span> or<a class="indexterm" id="id0407"/> <span class="strong"><strong>mean</strong></span>. The center element of the <a class="indexterm" id="id0408"/>left half, that is the fifth element (value 21), is considered the <a class="indexterm" id="id0409"/>
<span class="strong"><strong>first quartile</strong></span>. Similarly, the value in the center of the right half, that is the 13th element (value 26), is considered the <a class="indexterm" id="id0410"/>
<span class="strong"><strong>third quartile</strong></span>
<a class="indexterm" id="id0411"/> of the dataset. The difference between the third quartile and the first quartile is called<a class="indexterm" id="id0412"/> <span class="strong"><strong>Inter Quartile Range (IQR)</strong></span>, which is 5 in this case. This can be <a class="indexterm" id="id0413"/>illustrated with a <span class="strong"><strong>boxplot</strong></span>
<a class="indexterm" id="id0414"/>, as follows:</p><div class="mediaobject"><img alt="Median, first quartile, third quartile" src="graphics/3642_06_01.jpg"/></div><p>A boxplot highlights the first quartile, median and the third quartile of a dataset. As you can see, two "outlier" latency numbers (168 and 172) are unusually higher than the others. Median makes no representation of outliers in a dataset, whereas the mean does.</p><div class="mediaobject"><img alt="Median, first quartile, third quartile" src="graphics/3642_06_02.jpg"/></div><p>A histogram (the diagram shown previously) is another way to display a dataset where we batch the data elements in <a class="indexterm" id="id0415"/>
<span class="strong"><strong>periods</strong></span>
<a class="indexterm" id="id0416"/> and expose the<a class="indexterm" id="id0417"/> <span class="strong"><strong>frequency</strong></span> <a class="indexterm" id="id0418"/>of such periods. A period contains the elements in a certain range. All periods in a histogram are generally the same size; however, it is not uncommon to omit certain periods when there is no data.</p></div><div class="section" title="Percentile"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec025"/>Percentile</h3></div></div></div><p>A <span class="strong"><strong>percentile</strong></span>
<a class="indexterm" id="id0419"/> is expressed with a parameter, such as 99 percentile, or<a class="indexterm" id="id0420"/> 95 percentile etc. A percentile is the value below which all the specified percentage of data elements exist. For example, 95 percentile means the value <span class="emphasis"><em>N</em></span> among a dataset, such that 95 percent of elements in the dataset are below <span class="emphasis"><em>N</em></span> in value. As a concrete example, 85 percentile from the dataset of latency numbers we discussed earlier in this section is 29, because out of 17 total elements, 14 (which is 85 percent of 17) other elements in the dataset have a value below 29. A quartile splits a dataset into chunks of 25 percent elements each. Therefore, the first quartile is actually 25 percentile, the median is 50 percentile, and the third quartile is 75 percentile.</p></div><div class="section" title="Variance and standard deviation"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec026"/>Variance and standard deviation</h3></div></div></div><p>The spread of the data, that is, how <a class="indexterm" id="id0421"/>far away the data elements<a class="indexterm" id="id0422"/> are<a class="indexterm" id="id0423"/> from the<a class="indexterm" id="id0424"/> center value, gives us further insight into the data. Consider the <span class="emphasis"><em>i<sup>th</sup></em></span> deviation as the difference between the <span class="emphasis"><em>i<sup>th</sup></em></span> dataset element value (in statistics terms, a "variable" value) and its mean; we can represent it as <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image006.jpg"/></span>. We can express its "variance" and "standard deviation" as follows:</p><p>Variance = <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image008.jpg"/></span>, standard deviation (σ) = <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image010.jpg"/></span> = <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image012.jpg"/></span>
</p><p>Standard deviation is shown as the Greek letter "sigma", or simply "s". Consider the following Clojure code to determine variance and standard deviation:</p><div class="informalexample"><pre class="programlisting">(def tdata [23 19 21 24 26 20 22 21 25 168 23 20 29 172 22 24 26])

(defn var-std-dev
  "Return variance and standard deviation in a vector"
  [data]
  (let [size (count data)
        mean (/ (reduce + data) size)
        sum (-&gt;&gt; data
                 (map #(let [v (- % mean)] (* v v)))                 (reduce +))
        variance (double (/ sum (dec size)))]
    [variance (Math/sqrt variance)]))

user=&gt; (println (var-std-dev tdata))
[2390.345588235294 48.89116063497873]</pre></div><p>You can use the <a class="indexterm" id="id0425"/>Clojure-based platform<a class="indexterm" id="id0426"/> Incanter (<a class="ulink" href="http://incanter.org/">http://incanter.org/</a>) for statistical <a class="indexterm" id="id0427"/>computations. For <a class="indexterm" id="id0428"/>example, you can find standard<a class="indexterm" id="id0429"/> deviation using <code class="literal">(incanter.stats/sd tdata)</code> in Incanter.</p><p>The <span class="strong"><strong>empirical rule</strong></span>
<a class="indexterm" id="id0430"/> states the relationship between the elements of a dataset and SD. It says that 68.3 percent of all elements in a dataset lie in the range of one (positive or negative) SD from the mean, 95.5 percent of all elements lie in two SDs from the mean, and 99.7 percent of all data elements lie in three SDs from the mean.</p><p>Looking at the latency dataset we started out with, one SD from the mean is <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image014.jpg"/></span>(<span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image016.jpg"/></span> range -9 to 89) containing 88 percent of all elements. Two SDs from the mean is <span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image014.jpg"/></span> range -58 to 138) containing the same 88 percent of all elements. However, three SDs from the mean is(<span class="inlinemediaobject"><img alt="Variance and standard deviation" src="graphics/image018.jpg"/></span>range -107 to 187) containing 100 percent of all elements. There is a mismatch between what the empirical rule states and the results we found, because the empirical rule applies generally to uniformly distributed datasets with a large number of elements.</p></div></div><div class="section" title="Understanding Criterium output"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec083"/>Understanding Criterium output</h2></div></div></div><p>In <a class="link" href="ch11.html" title="Chapter 4. Host Performance">Chapter 4</a>, <span class="emphasis"><em>Host Performance</em></span>, we <a class="indexterm" id="id0431"/>introduced the Clojure library <span class="emphasis"><em>Criterium</em></span> to measure the latency of Clojure expressions. A sample benchmarking result is as follows:</p><div class="informalexample"><pre class="programlisting">user=&gt; (require '[criterium.core :refer [bench]])
nil
user=&gt; (bench (reduce + (range 1000)))
Evaluation count : 162600 in 60 samples of 2710 calls.
             Execution time mean : 376.756518 us
    Execution time std-deviation : 3.083305 us
   Execution time lower quantile : 373.021354 us ( 2.5%)
   Execution time upper quantile : 381.687904 us (97.5%)

Found 3 outliers in 60 samples (5.0000 %)
low-severe 2 (3.3333 %)
low-mild 1 (1.6667 %)
 Variance from outliers : 1.6389 % Variance is slightly inflated by outliers</pre></div><p>We can see that the<a class="indexterm" id="id0432"/> result has some familiar terms we discussed earlier in this section. A high mean and low standard deviation indicate that there is not a lot of variation in the execution times. Likewise, the lower (first) and upper (third) quartiles indicate that they are not too far away from the mean. This result implies that the body of code is more or less stable in terms of response time. Criterium repeats the execution many times to collect the latency numbers.</p><p>However, why does Criterium attempt to do a statistical analysis of the execution time? What would be amiss if we simply calculate the mean? It turns out that the response times of all executions are not always stable and there is often disparity in how the response time shows up. Only upon running sufficient times under correctly simulated load we can get complete data and other indicators about the latency. A statistical analysis gives insight into whether there is something wrong with the benchmark.</p></div><div class="section" title="Guided performance objectives"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec084"/>Guided performance objectives</h2></div></div></div><p>We only briefly discussed <a class="indexterm" id="id0433"/>performance objectives in <a class="link" href="ch15.html" title="Chapter 8. Application Performance">Chapter 1</a>, <span class="emphasis"><em>Performance by Design</em></span> because that discussion needs a reference to statistical concepts. Let's say we identified the functional scenarios and the corresponding response time. Should response time remain fixed? Can we constrain throughput in order to prefer a stipulated response time?</p><p>The performance objective should specify the worst-case response time, that is, maximum latency, the average response time and the maximum standard deviation. Similarly, the performance objective should also mention the worst-case throughput, maintenance window throughput, average throughput, and the maximum standard deviation.</p></div></div></div>
<div class="section" title="Performance testing"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec044"/>Performance testing</h1></div></div></div><p>Testing for performance <a class="indexterm" id="id0434"/>requires us to know what we are going to test, how we want to test, and what environment to set up for the tests to execute. There are several pitfalls to be aware of, such as a lack of near-real hardware and resources of production use, similar OS and software environments, diversity of representative data for test cases, and so on. Lack of diversity in test inputs may lead to a monotonic branch prediction, hence introducing a "bias" in test results.</p><div class="section" title="The test environment"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec085"/>The test environment</h2></div></div></div><p>Concerns about the <a class="indexterm" id="id0435"/>test environment begin with the hardware representative of the production environment. Traditionally, the test environment hardware has been a scaled-down version of the production environment. The performance analysis done on non-representative hardware is almost certain to skew the results. Fortunately, in recent times, thanks to the commodity hardware and cloud systems, provisioning test environment hardware that is similar to the production environment is not too difficult.</p><p>The network and storage bandwidth, operating system, and software used for performance testing should of course be the same as in production. What is also important is to have a "load" representative of the test scenarios. The load comes in different combinations including the concurrency of requests, the throughput and standard deviation of requests, the current population level in the database or in the message queue, CPU and heap usage, and so on. It is important to simulate a representative load.</p><p>Testing often requires quite some work on the part of the piece of code that carries out the test. Be sure to keep its overhead to a minimum so that it does not impact the benchmark results. Wherever possible, use a system other than the test target to generate requests.</p></div><div class="section" title="What to test"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec086"/>What to test</h2></div></div></div><p>Any implementation of a non-trivial system typically involves many hardware and software components. Performance testing a certain feature or a service in the entire system needs to account for the way it interacts with the various sub-systems. For example, a web service call may touch multiple layers such as the web server (request/response marshaling and unmarshaling), URI-based routing, service handler, application-database connector, the database layer, logger component, and so on. Testing only the service handler would be a terrible mistake, because that is not exactly the performance what the web client will experience. The performance test should test at the perimeter of a system to keep the results realistic, preferably with a third-party observer.</p><p>The performance objectives state the criteria for testing. It would be useful to test what is not required by the objective, especially when the tests are run concurrently. Running meaningful performance tests may require a certain level of isolation.</p></div><div class="section" title="Measuring latency"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec087"/>Measuring latency</h2></div></div></div><p>The latency obtained<a class="indexterm" id="id0436"/> by executing a body of code may vary <a class="indexterm" id="id0437"/>slightly on each run. This necessitates that we execute the code many times and collect samples. The latency numbers may be impacted by the JVM warm-up time, garbage collection and the JIT compiler kicking in. So, the test and sample collection should ensure that these conditions do not impact the results. Criterium follows such methods to produce the results. When we test a very small piece of code this way, it is called a <a class="indexterm" id="id0438"/>
<span class="strong"><strong>Micro-benchmark</strong></span>.</p><p>As the latency of some operations may vary during runs, it is important to collect all samples and segregate them into periods and frequencies turning up into a histogram. The maximum latency is an important metric when measuring latency—it indicates the worst-case latency. Besides the maximum, the 99 percentile and 95 percentile latency numbers are also important to put things in perspective. It's important to actually collect the latency numbers instead of inferring them from standard deviation, as we noted earlier that the empirical rule works only for normal distributions without significant outliers.</p><p>The outliers are an important data point when measuring latency. A proportionately higher count of outliers indicates a possibility of degradation of service.</p><div class="section" title="Comparative latency measurement"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec027"/>Comparative latency measurement</h3></div></div></div><p>When evaluating libraries for<a class="indexterm" id="id0439"/> use in projects, or when coming up with alternate solutions against some baseline, comparative latency benchmarks are useful to determine the performance trade-offs. We will inspect two comparative benchmarking tools based on Criterium, called Perforate and Citius. Both make it easy to run Criterium benchmarks grouped by context, and to easily view the benchmark results.</p><p>Perforate (<a class="ulink" href="https://github.com/davidsantiago/perforate">https://github.com/davidsantiago/perforate</a>) is <a class="indexterm" id="id0440"/>a Leiningen <a class="indexterm" id="id0441"/>plugin that lets one define goals; a goal (defined using <code class="literal">perforate.core/defgoal</code>) is a common task or context having one or more benchmarks. Each benchmark is defined using <code class="literal">perforate.core/defcase</code>. As of version 0.3.4, a sample benchmark code may look like the following code snippet:</p><div class="informalexample"><pre class="programlisting">(ns foo.bench
  (:require [perforate.core :as p]))

(p/defgoal str-concat "String concat")
(p/defcase str-concat :apply
  [] (apply str ["foo" "bar" "baz"]))
(p/defcase str-concat :reduce
  [] (reduce str ["foo" "bar" "baz"]))

(p/defgoal sum-numbers "Sum numbers")
(p/defcase sum-numbers :apply
  [] (apply + [1 2 3 4 5 6 7 8 9 0]))
(p/defcase sum-numbers :reduce
  [] (reduce + [1 2 3 4 5 6 7 8 9 0]))</pre></div><p>You can declare the test environments in <code class="literal">project.clj</code> and provide the setup/cleanup code when defining the goal. Perforate provides ways to run the benchmarks from the command-line.</p><p>Citius (<a class="ulink" href="https://github.com/kumarshantanu/citius">https://github.com/kumarshantanu/citius</a>) is<a class="indexterm" id="id0442"/> a library that provides integration <a class="indexterm" id="id0443"/>hooks for clojure.test and other forms of invocation. It<a class="indexterm" id="id0444"/> imposes more rigid constraints than Perforate, and renders additional comparative information about the benchmarks. It presumes a fixed number of targets (cases) per test suite where there may be several goals. </p><p>As of version 0.2.1, a sample benchmark code may look like the following code snippet:</p><div class="informalexample"><pre class="programlisting">(ns foo.bench
  (:require [citius.core :as c]))

(c/with-bench-context ["Apply" "Reduce"]
  {:chart-title "Apply vs Reduce"
   :chart-filename "bench-simple.png"}
  (c/compare-perf
    "concat strs"
    (apply str ["foo" "bar" "baz"])
    (reduce str ["foo" "bar" "baz"]))
  (c/compare-perf
    "sum numbers"
    (apply + [1 2 3 4 5 6 7 8 9 0])
    (reduce + [1 2 3 4 5 6 7 8 9 0])))</pre></div><p>In the previous example, the code runs the benchmarks, reports the comparative summary, and draws a bar chart image of the mean latencies.</p></div><div class="section" title="Latency measurement under concurrency"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec028"/>Latency measurement under concurrency</h3></div></div></div><p>When we benchmark <a class="indexterm" id="id0445"/>a piece of code with Criterium, it uses just a single thread to determine results. That gives us a fair output in terms of single-threaded result, but there are many benchmarking scenarios where single-threaded latency is far from what we need. Under concurrency, the latency often differs quite a bit from single-threaded latency. Especially when we deal with <span class="emphasis"><em>stateful</em></span> objects (e.g. drawing a connection from a JDBC connection pool, updating shared in-memory state etc.), the latency is likely to vary in proportion with the contention. In such scenarios it is useful to find out the latency patterns of the code under various concurrency levels.</p><p>The Citius library we discussed in the previous sub-section supports tunable concurrency levels. Consider the following benchmark of implementations of shared counters:</p><div class="informalexample"><pre class="programlisting">(ns foo.bench
  (:require
    [clojure.test :refer [deftest]]
    [citius.core :as c])
  (:import [java.util.concurrent.atomic AtomicLong]))

(def a (atom 0))
(def ^AtomicLong b (AtomicLong. 0))

(deftest test-counter
  (c/with-bench-context ["Atom" "AtomicLong"] {}
    (c/compare-perf "counter"
      (swap! a unchecked-inc) (.incrementAndGet b))))

;; Under Unix-like systems run the following command in terminal:
;; CITIUS_CONCURRENCY=4,4 lein test</pre></div><p>When I ran<a class="indexterm" id="id0446"/> this benchmark on a 4th generation quad-core Intel Core i7 processor (Mac OSX 10.10), the mean latency at concurrency level 04 was 38 to 42 times the value of the mean latency at concurrency level 01. Since, in many cases, the JVM is used to run server-side applications, benchmarking under concurrency becomes all the more important.</p></div></div><div class="section" title="Measuring throughput"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec088"/>Measuring throughput</h2></div></div></div><p>Throughput is expressed per <a class="indexterm" id="id0447"/>unit of time. Coarse-grained<a class="indexterm" id="id0448"/> throughput, that is, the throughput number collected over a long period of time, may hide the fact when the throughput is actually delivered in bursts instead of a uniform distribution. Granularity of the throughput test is subject to the nature of the operation. A batch process may process bursts of data, whereas a web service may deliver uniformly distributed throughput.</p><div class="section" title="Average throughput test"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec029"/>Average throughput test</h3></div></div></div><p>Though Citius (as of version 0.2.1) shows <a class="indexterm" id="id0449"/>extrapolated throughput (per second, per thread) in benchmark results, that throughput number may not represent the actual throughput very well for a variety of reasons. Let's construct a simple<a class="indexterm" id="id0450"/> throughput benchmark harness as follows, beginning with the helper functions:</p><div class="informalexample"><pre class="programlisting">(import '[java.util.concurrent ExecutorService Executors Future])
(defn concurrently
  ([n f]
    (concurrently n f #(mapv deref %)))
  ([n f g]
    (let [^ExecutorService
          thread-pool (Executors/newFixedThreadPool n)
          future-vals (transient [])]
      (dotimes [i n]
        (let [^Callable task (if (coll? f) (nth f i) f)
              ^Future each-future (.submit thread-pool task)]
          (conj! future-vals each-future)))
      (try
        (g (persistent! future-vals))
        (finally
          (.shutdown thread-pool))))))

(defn call-count
  []
  (let [stats (atom 0)]
    (fn
      ([] (deref stats))
      ([k] (if (identical? :reset k)
             (reset! stats 0)
             (swap! stats unchecked-inc))))))

(defn wrap-call-stats
  [stats f]
  (fn [&amp; args]
    (try
      (let [result (apply f args)]
        (stats :count)
        result))))

(defn wait-until-millis
  ([^long timeout-millis]
    (wait-until-millis timeout-millis 100))
  ([^long timeout-millis ^long progress-millis]
    (while (&lt; (System/currentTimeMillis) timeout-millis)
      (let [millis (min progress-millis
                     (- timeout-millis (System/currentTimeMillis)))]
        (when (pos? millis)
          (try
            (Thread/sleep millis)
            (catch InterruptedException e
              (.interrupt ^Thread (Thread/currentThread))))
          (print \.)
          (flush))))))</pre></div><p>Now that we have<a class="indexterm" id="id0451"/> the helper functions defined, let's see the benchmarking code:</p><div class="informalexample"><pre class="programlisting">(defn benchmark-throughput*
  [^long concurrency ^long warmup-millis ^long bench-millis f]
  (let [now        #(System/currentTimeMillis)
        exit?      (atom false)
        stats-coll (repeatedly concurrency call-count)
        g-coll     (-&gt;&gt; (repeat f)
                     (map wrap-call-stats stats-coll)
                     (map-indexed (fn [i g]
                                    (fn []
                                      (let [r (nth stats-coll i)]
                                        (while (not (deref exit?))
                                          (g))
                                        (r)))))
                     vec)
        call-count (-&gt;&gt; (fn [future-vals]
                          (print "\nWarming up")
                          (wait-until-millis (+ (now) warmup-millis))
                          (mapv #(% :reset) stats-coll) ; reset count
                          (print "\nBenchmarking")
                          (wait-until-millis (+ (now) bench-millis))
                          (println)
                          (swap! exit? not)
                          (mapv deref future-vals))
                     (concurrently concurrency g-coll)
                     (apply +))]
    {:concurrency concurrency
     :calls-count call-count
     :duration-millis bench-millis
     :calls-per-second (-&gt;&gt; (/ bench-millis 1000)
                         double
                         (/ ^long call-count)
                         long)}))

(defmacro benchmark-throughput
  "Benchmark a body of code for average throughput."
  [concurrency warmup-millis bench-millis &amp; body]
  `(benchmark-throughput*
    ~concurrency ~warmup-millis ~bench-millis (fn [] ~@body)))</pre></div><p>Let's now see how to test some code for throughput using the harness:</p><div class="informalexample"><pre class="programlisting">(def a (atom 0))
(println
  (benchmark-throughput
    4 20000 40000 (swap! a inc)))</pre></div><p>This harness <a class="indexterm" id="id0452"/>provides only a simple throughput test. To inspect the throughput pattern you may want to bucket the throughput across rolling fixed-duration windows (e.g. per second throughput.) However, that topic is beyond the scope of this text, though we will touch upon it in the <span class="emphasis"><em>Performance monitoring</em></span> section later in this chapter.</p></div></div><div class="section" title="The load, stress, and endurance tests"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec089"/>The load, stress, and endurance tests</h2></div></div></div><p>One of the characteristics of tests is each run only represents the slice of time it is executed through. Repeated <a class="indexterm" id="id0453"/>runs establish their general behavior. But how many runs should be enough? There may be several anticipated load scenarios for an operation. So, there is a need to repeat the tests at various load scenarios. Simple test runs may not always exhibit the long-term behavior and response of the operation. Running the tests under varying high load for longer duration allows us to observe them for any odd behavior that may not show up in a short test cycle.</p><p>When we test an <a class="indexterm" id="id0454"/>operation at a load far beyond its anticipated latency and throughput objectives, that is<a class="indexterm" id="id0455"/> <span class="strong"><strong>stress testing</strong></span>. The intent of a stress test is to ascertain a reasonable behavior exhibited by the operation beyond the maximum load it was developed for. Another way to observe the behavior of an operation is to see how it behaves when run for a very long duration, typically for several days or weeks. Such prolonged tests are called <a class="indexterm" id="id0456"/>
<span class="strong"><strong>endurance tests</strong></span>. While <a class="indexterm" id="id0457"/>a stress test checks the graceful behavior of the operation, an endurance test checks the consistent behavior of the operation over a long period.</p><p>There are several tools that may help with load and stress testing. Engulf (<a class="ulink" href="http://engulf-project.org/">http://engulf-project.org/</a>) is <a class="indexterm" id="id0458"/>a distributed<a class="indexterm" id="id0459"/> HTTP-based, load-generation tool written in Clojure. Apache JMeter and Grinder are Java-based load-generation tools. Grinder can be scripted using Clojure. Apache Bench is a load-testing tool for web systems. Tsung is an extensible, high-performance, load-testing tool written in Erlang.</p></div></div>
<div class="section" title="Performance monitoring"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec045"/>Performance monitoring</h1></div></div></div><p>During prolonged testing or after <a class="indexterm" id="id0460"/>the application has gone to production, we need to monitor its performance to make sure the application continues to meet the performance objectives. There may be infrastructure or operational issues impacting the performance or availability of the application, or occasional spikes in latency or dips in throughput. Generally, monitoring alleviates such risk by generating a continuous feedback stream.</p><p>Roughly there are three kinds of components used to build a monitoring stack. A <span class="strong"><strong>collector</strong></span> sends the numbers from each host that needs to be monitored. The collector gets host information and the performance numbers and sends them to an <span class="strong"><strong>aggregator</strong></span>. An aggregator receives the data sent by the collector and persists them until asked by a <span class="strong"><strong>visualizer</strong></span> on behalf of the user.</p><p>The<a class="indexterm" id="id0461"/> project <span class="strong"><strong>metrics-clojure</strong></span> (<a class="ulink" href="https://github.com/sjl/metrics-clojure">https://github.com/sjl/metrics-clojure</a>) is a Clojure wrapper over the <span class="strong"><strong>Metrics</strong></span> (<a class="ulink" href="https://github.com/dropwizard/metrics">https://github.com/dropwizard/metrics</a>) Java<a class="indexterm" id="id0462"/> framework, which acts as a collector. <span class="strong"><strong>Statsd</strong></span>
<a class="indexterm" id="id0463"/> is a well-known aggregator that does not persist data by itself but passes it on to a variety of servers. One of the popular visualizer projects is <span class="strong"><strong>Graphite</strong></span>
<a class="indexterm" id="id0464"/> that stores the data as well as produces graphs for requested periods. There are several other alternatives to these, notably <a class="indexterm" id="id0465"/>
<span class="strong"><strong>Riemann</strong></span> (<a class="ulink" href="http://riemann.io/">http://riemann.io/</a>) that is written in Clojure and Ruby. Riemann is an event processing-based aggregator.</p><div class="section" title="Monitoring through logs"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec090"/>Monitoring through logs</h2></div></div></div><p>One of the popular <a class="indexterm" id="id0466"/>performance monitoring approaches that has emerged in recent times is via logs. The idea is simple—the application emits metrics data as logs, which are shipped from the individual machine to a central log aggregation service. Then, those metrics data are aggregated for each time window and further moved for archival and visualization.</p><p>As a high-level example of such a monitoring system, you may like to use<a class="indexterm" id="id0467"/> <span class="strong"><strong>Logstash-forwarder</strong></span> (<a class="ulink" href="https://github.com/elastic/logstash-forwarder">https://github.com/elastic/logstash-forwarder</a>) to grab the application logs from the local filesystem and ship to <a class="indexterm" id="id0468"/>
<span class="strong"><strong>Logstash</strong></span> (<a class="ulink" href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a>), where it forwards the metrics logs to <span class="strong"><strong>StatsD</strong></span>
<a class="indexterm" id="id0469"/> (<a class="ulink" href="https://github.com/etsy/statsd">https://github.com/etsy/statsd</a>) for metrics aggregation or to Riemann<a class="indexterm" id="id0470"/> (<a class="ulink" href="http://riemann.io/">http://riemann.io/</a>) for events analysis and monitoring alerts. StatsD and/or Riemann can forward the metrics data to Graphite<a class="indexterm" id="id0471"/> (<a class="ulink" href="http://graphite.wikidot.com/">http://graphite.wikidot.com/</a>) for archival and graphing of the time-series metrics data. Often, people want to plug in a non-default time-series data store (such as <span class="strong"><strong>InfluxDB</strong></span>: <a class="ulink" href="https://influxdb.com/">https://influxdb.com/</a>) or a visualization layer (such as <span class="strong"><strong>Grafana</strong></span>: <a class="ulink" href="http://grafana.org/">http://grafana.org/</a>) with Graphite.</p><p>A detailed<a class="indexterm" id="id0472"/> discussion on this topic is out of the scope of this text, but I think exploring this area would serve you well.</p></div><div class="section" title="Ring (web) monitoring"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec091"/>Ring (web) monitoring</h2></div></div></div><p>If you develop web <a class="indexterm" id="id0473"/>software using<a class="indexterm" id="id0474"/> Ring (<a class="ulink" href="https://github.com/ring-clojure/ring">https://github.com/ring-clojure/ring</a>) then you may find the Ring extension of the metrics-clojure library<a class="indexterm" id="id0475"/> useful: <a class="ulink" href="http://metrics-clojure.readthedocs.org/en/latest/ring.html">http://metrics-clojure.readthedocs.org/en/latest/ring.html</a> —this tracks a number of useful metrics that can be queried in JSON format and integrated with visualization via the web browser.</p><p>To emit a continuous stream of metrics data from the web layer, <span class="strong"><strong>Server-Sent Events</strong></span> (<span class="strong"><strong>SSE</strong></span>) may be a good idea due to its low overhead. Both <span class="strong"><strong>http-kit</strong></span>
<a class="indexterm" id="id0476"/> (<a class="ulink" href="http://www.http-kit.org/">http://www.http-kit.org/</a>) and <span class="strong"><strong>Aleph</strong></span>
<a class="indexterm" id="id0477"/> (<a class="ulink" href="http://aleph.io/">http://aleph.io/</a>), which work with Ring, support SSE today.</p></div><div class="section" title="Introspection"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec092"/>Introspection</h2></div></div></div><p>Both Oracle JDK and OpenJDK<a class="indexterm" id="id0478"/> provide two GUI tools called <a class="indexterm" id="id0479"/>
<span class="strong"><strong>JConsole</strong></span> (executable name <code class="literal">jconsole</code>) and <span class="strong"><strong>JVisualVM</strong></span> (executable name <code class="literal">jvisualvm</code>) that we can use to introspect into running JVMs for instrumentation data. There are also some command-line tools (<a class="ulink" href="http://docs.oracle.com/javase/8/docs/technotes/tools/">http://docs.oracle.com/javase/8/docs/technotes/tools/</a>) in the JDK to peek into the inner details of the running JVMs.</p><p>A common way to introspect a running <a class="indexterm" id="id0480"/>Clojure application is to have an <span class="strong"><strong>nREPL</strong></span> (<a class="ulink" href="https://github.com/clojure/tools.nrepl">https://github.com/clojure/tools.nrepl</a>) service running so that we can connect <a class="indexterm" id="id0481"/>to it later using an nREPL client. Interactive introspection over nREPL using the Emacs editor (embedded nREPL client) is popular among some, whereas others prefer to script an nREPL client to carry out tasks.</p><div class="section" title="JVM instrumentation via JMX"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec030"/>JVM instrumentation via JMX</h3></div></div></div><p>The JVM has a <a class="indexterm" id="id0482"/>built-in mechanism to introspect managed resources via the extensible <span class="strong"><strong>Java Management Extensions</strong></span> (<span class="strong"><strong>JMX</strong></span>) <a class="indexterm" id="id0483"/>API. It provides a way for application maintainers to expose manageable resources as "MBeans". Clojure has an easy-to-use <code class="literal">contrib</code> library <a class="indexterm" id="id0484"/>called <code class="literal">java.jmx</code> (<a class="ulink" href="https://github.com/clojure/java.jmx">https://github.com/clojure/java.jmx</a>) to access JMX. There is a decent amount of open source tooling for visualization of JVM instrumentation data via JMX, such as <code class="literal">jmxtrans</code> and <code class="literal">jmxetric</code>, which integrate with Ganglia and Graphite.</p><p>Getting quick memory<a class="indexterm" id="id0485"/> stats of the JVM is pretty easy using Clojure:</p><div class="informalexample"><pre class="programlisting">(let [^Runtime r (Runtime/getRuntime)]
  (println "Maximum memory" (.maxMemory r))
  (println "Total memory" (.totalMemory r))
  (println "Free memory" (.freeMemory r)))
Output:
Maximum memory 704643072
Total memory 291373056
Free memory 160529752</pre></div></div></div></div>
<div class="section" title="Profiling"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec046"/>Profiling</h1></div></div></div><p>We briefly discussed<a class="indexterm" id="id0486"/> profiler types in <a class="link" href="ch15.html" title="Chapter 8. Application Performance">Chapter 1</a>, <span class="emphasis"><em>Performance by Design</em></span>. The JVisualVM tool we discussed with respect to introspection in the previous section is also a CPU and memory profiler that comes bundled with the JDK. Let's see them in action— consider the following two Clojure functions that stress the CPU and memory respectively:</p><div class="informalexample"><pre class="programlisting">(defn cpu-work []
  (reduce + (range 100000000)))

(defn mem-work []
  (-&gt;&gt; (range 1000000)
       (map str)
       vec
       (map keyword)
       count))</pre></div><p>Using JVisualVM is pretty easy—open the Clojure JVM process from the left pane. It has sampler and regular profiler styles of profiling. Start profiling for CPU or memory use when the code is running and wait for it to collect enough data to plot on the screen.</p><div class="mediaobject"><img alt="Profiling" src="graphics/3642_06_03.jpg"/></div><p>The following shows <a class="indexterm" id="id0487"/>memory profiling in action:</p><div class="mediaobject"><img alt="Profiling" src="graphics/3642_06_04.jpg"/></div><p>Note that JVisualVM is a very simple, entry-level profiler. There are several commercial JVM profilers on the market for sophisticated needs.</p><div class="section" title="OS and CPU/cache-level profiling"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec093"/>OS and CPU/cache-level profiling</h2></div></div></div><p>Profiling only the JVM may not <a class="indexterm" id="id0488"/>always tell the whole story. Getting down to OS and hardware-level profiling often provides better insight into what is going on with the application. On Unix-like operating systems, command-line tools such as <code class="literal">top</code>, <code class="literal">htop</code>, <code class="literal">perf</code>, <code class="literal">iota</code>, <code class="literal">netstat</code>, <code class="literal">vista</code>, <code class="literal">upstate</code>, <code class="literal">pidstat</code> etc can help. Profiling the CPU for cache misses and other information is a useful source to catch performance issues. Among open source tools for <a class="indexterm" id="id0489"/>Linux, <span class="strong"><strong>Likwid </strong></span>(<a class="ulink" href="http://code.google.com/p/likwid/">http://code.google.com/p/likwid/</a> and <a class="ulink" href="https://github.com/rrze-likwid/likwid">https://github.com/rrze-likwid/likwid</a>) is small yet effective for Intel and AMD processors; <span class="strong"><strong>i7z</strong></span> (<a class="ulink" href="https://code.google.com/p/i7z/">https://code.google.com/p/i7z/</a> <a class="indexterm" id="id0490"/>and <a class="ulink" href="https://github.com/ajaiantilal/i7z">https://github.com/ajaiantilal/i7z</a>) is specifically for Intel processors. There are also dedicated commercial tools such as <span class="strong"><strong>Intel VTune Analyzer</strong></span>
<a class="indexterm" id="id0491"/> for more elaborate needs.</p></div><div class="section" title="I/O profiling"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec094"/>I/O profiling</h2></div></div></div><p>Profiling I/O may <a class="indexterm" id="id0492"/>require special<a class="indexterm" id="id0493"/> tools too. Besides <code class="literal">iota</code> and <code class="literal">blktrace</code>, <code class="literal">ioping</code> (<a class="ulink" href="https://code.google.com/p/ioping/">https://code.google.com/p/ioping/</a> and <a class="ulink" href="https://github.com/koct9i/ioping">https://github.com/koct9i/ioping</a>) is useful to measure real-time I/O latency on Linux/Unix systems. The <span class="strong"><strong>vnStat</strong></span> tool<a class="indexterm" id="id0494"/> is useful to monitor and log network <a class="indexterm" id="id0495"/>traffic on Linux. The IOPS of a storage device may not tell the whole truth unless it is accompanied by latency information for different operations, and how many reads and writes can simultaneously happen.</p><p>In an I/O bound workload one has to look for the read and write IOPS over time and set a threshold to achieve optimum performance. The application should throttle the I/O access so that the threshold is not crossed.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec047"/>Summary</h1></div></div></div><p>Delivering high-performance applications not only requires care for performance but also systematic effort to measure, test, monitor, and optimize the performance of various components and subsystems. These activities often require the right skill and experience. Sometimes, performance considerations may even bring system design and architecture back to the drawing board. Early structured steps taken to achieve performance go a long way to ensuring that the performance objectives are being continuously met.</p><p>In the next chapter, we will look into performance optimization tools and techniques.</p></div>
<div class="chapter" title="Chapter&#xA0;7.&#xA0;Performance Optimization"><div class="titlepage"><div><div><h1 class="title"><a id="ch14"/>Chapter 7. Performance Optimization</h1></div></div></div><p>Performance optimization<a class="indexterm" id="id0496"/> is additive by nature, as in it works by adding performance tuning to the knowledge of how the underlying system works, and to the result of performance measurement. This chapter builds on the previous ones that covered "how the underlying system works" and "performance measurement". Though you will notice some recipe-like sections in this chapter, you already know the pre-requisite in order to exploit those well. Performance tuning is an iterative process of measuring performance, determining bottlenecks, applying knowledge in order to experiment with tuning the code, and repeating it all until performance improves. In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Setting up projects for better performance</li><li class="listitem" style="list-style-type: disc">Identifying performance bottlenecks in the code</li><li class="listitem" style="list-style-type: disc">Profiling code with VisualVM</li><li class="listitem" style="list-style-type: disc">Performance tuning of Clojure code</li><li class="listitem" style="list-style-type: disc">JVM performance tuning</li></ul></div><div class="section" title="Project setup"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec048"/>Project setup</h1></div></div></div><p>While finding<a class="indexterm" id="id0497"/> bottlenecks is essential to fixing performance problems in the code, there are several things one can do right from the start to ensure better performance.</p><div class="section" title="Software versions"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec095"/>Software versions</h2></div></div></div><p>Usually, new software versions include bug fixes, new features, and performance improvements. Unless <a class="indexterm" id="id0498"/>advised to the contrary, it is better to use newer versions. For development with Clojure, consider the following software versions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>JVM version</strong></span>: As of this writing, Java 8 (Oracle JDK, OpenJDK, Zulu) has been released as the latest stable production-ready version. It is not only stable, it also has better performance in several areas (especially concurrency) than the earlier<a class="indexterm" id="id0499"/> versions. If you have a choice, choose Java 8 over the older versions of Java.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Clojure version</strong></span>: As of this writing, Clojure 1.7.0 is the latest stable version that has several performance improvements over the older versions. There are also new<a class="indexterm" id="id0500"/> features (transducers, volatile) that can make your code perform better. Choose Clojure 1.7 over the older versions unless you have no choice.</li></ul></div></div><div class="section" title="Leiningen project.clj configuration"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec096"/>Leiningen project.clj configuration</h2></div></div></div><p>As of version 2.5.1, the default Leiningen template (<code class="literal">lein new foo</code>, <code class="literal">lein new app foo</code>) needs few tweaks<a class="indexterm" id="id0501"/> to make the project amenable to performance. Ensure your Leiningen <code class="literal">project.clj</code> file has<a class="indexterm" id="id0502"/> the following entries, as appropriate.</p><div class="section" title="Enable reflection warning"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec031"/>Enable reflection warning</h3></div></div></div><p>One of the most<a class="indexterm" id="id0503"/> common pitfalls in Clojure programming is to inadvertently let the code resort to reflection. Recall that we discussed this in <a class="link" href="ch10.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java. Enabling</em></span>, reflection warning is quite easy, let's fix it by adding the following entry to <code class="literal">project.clj</code>:</p><div class="informalexample"><pre class="programlisting">:global-vars {*unchecked-math* :warn-on-boxed ; in Clojure 1.7+
              *warn-on-reflection* true}</pre></div><p>In the previous configuration, the first setting <code class="literal">*unchecked-math* :warn-on-boxed</code> works only in Clojure 1.7—it emits numeric boxing warnings. The second setting <code class="literal">*warn-on-reflection* true</code> works on earlier Clojure versions as well as Clojure 1.7, and emits reflection warning messages in the code.</p><p>However, including these settings in <code class="literal">project.clj</code> may not be enough. Reflection warnings are emitted only when a namespace is loaded. You need to ensure that all namespaces are loaded in order to search for reflection warnings throughout the project. This can be done by writing tests that refer to all namespaces, or via scripts that do so.</p></div><div class="section" title="Enable optimized JVM options when benchmarking"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec032"/>Enable optimized JVM options when benchmarking</h3></div></div></div><p>In <a class="link" href="ch11.html" title="Chapter 4. Host Performance">Chapter 4</a>, <span class="emphasis"><em>Host Performance</em></span> we<a class="indexterm" id="id0504"/> discussed that Leiningen enables tiered compilation by default, which provides low startup time at the cost of poor JIT compiler optimization. The default setting is quite misleading for performance benchmarking, so you should enable JVM options that are representative of what you would use in production:</p><div class="informalexample"><pre class="programlisting">:profiles {:perf {:test-paths ^:replace ["perf-test"]
                  :jvm-opts ^:replace ["-server"
                                       "-Xms2048m" "-Xmx2048m"]}}</pre></div><p>For example, the previous setting defines a Leiningen profile that overrides the default JVM options to configure a <code class="literal">server</code> Java runtime with 2 GB of fixed-size heap space. It also sets test paths to a directory <code class="literal">perf-test</code>. Now you can run performance tests as follows:</p><div class="informalexample"><pre class="programlisting">lein with-profile perf test</pre></div><p>If your project has<a class="indexterm" id="id0505"/> performance test suites that require different JVM options, you should define multiple profiles for running tests, as appropriate.</p></div></div><div class="section" title="Distinguish between initialization and runtime"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec097"/>Distinguish between initialization and runtime</h2></div></div></div><p>Most non-trivial projects<a class="indexterm" id="id0506"/> need a lot of context to be set up before they can function. Examples of such contexts could be app configuration, in-memory state, I/O resources, thread pools, caches, and so on. While many projects start with ad hoc configuration and initialization, eventually projects need to isolate the initialization phase from runtime. The purpose of this distinction is not only to sanitize the organization of code, but also to pre-compute as much as possible once before the runtime can take over to repeatedly respond to demands. This distinction also allows the initialization phase to easily (and conditionally, based on configuration) instrument the initialized code for performance logging and monitoring.</p><p>Non-trivial programs are usually divided into layers, such as business logic, caching, messaging, database access, and so on. Each layer has a dependency relationship with one or more of the other layers. It is feasible to carry out the isolation of the initialization phase by writing code using first principles, and many projects actually do that. However, there are a few libraries that simplify this process by letting you declare the dependency relationship between<a class="indexterm" id="id0507"/> layers. <span class="strong"><strong>Component</strong></span> (<a class="ulink" href="https://github.com/stuartsierra/component">https://github.com/stuartsierra/component</a>) and <span class="strong"><strong>Prismatic Graph</strong></span> (<a class="ulink" href="https://github.com/Prismatic/plumbing">https://github.com/Prismatic/plumbing</a>) are notable <a class="indexterm" id="id0508"/>examples of such libraries.</p><p>The Component library is well documented. It may not be easily apparent how to use Prismatic Graph for dependency resolution; following is a contrived example for illustration:</p><div class="informalexample"><pre class="programlisting">(require '[plumbing.core :refer [fnk]])
(require '[plumbing.graph :as g])

(def layers
  {:db      (fnk [config]    (let [pool (db-pool config)]
                               (reify IDatabase ...)))
   :cache   (fnk [config db] (let [cache-obj (mk-cache config)]
                               (reify ICache    ...)))
   :service (fnk [config db cache] (reify IService  ...))
   :web     (fnk [config service]  (reify IWeb      ...))})

(defn resolve-layers
  "Return a map of reified layers"
  [app-config]
  (let [compiled (g/compile layers)]
    (compiled {:config app-config})))</pre></div><p>This example merely shows the construction of a layer dependency graph, but often you may need different construction scope and order for testing. In that case you may define different graphs and resolve them, as and when appropriate. If you need teardown logic for testing, you can<a class="indexterm" id="id0509"/> add extra <code class="literal">fnk</code> entries for each teardown step and use those for teardown.</p></div></div></div>
<div class="section" title="Identifying performance bottlenecks"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec049"/>Identifying performance bottlenecks</h1></div></div></div><p>We discussed in previous chapters that random performance tuning of code rarely works, because we may not be<a class="indexterm" id="id0510"/> tuning in the right place. It is crucial to find the performance bottlenecks before we can tune those areas in the code. Upon finding the bottleneck, we can experiment with alternate solutions around it. In this section we will look into finding the bottlenecks.</p><div class="section" title="Latency bottlenecks in Clojure code"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec098"/>Latency bottlenecks in Clojure code</h2></div></div></div><p>Latency is the starting, and the most obvious, metric to drill-down in order to find bottlenecks. For Clojure code, we observed in <a class="link" href="ch13.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span> that code profiling tools can help<a class="indexterm" id="id0511"/> us find the areas of bottleneck. Profilers are, of course, very useful. Once you discover hotspots <a class="indexterm" id="id0512"/>via profilers, you may find ways to tune those for latency to a certain extent.</p><p>Most profilers work on aggregates, a batch of runs, ranking the hotspots in code by resources consumed. However, often the opportunity to tune latency lies in the long tail that may not be highlighted by the profilers. In such circumstances, we may employ a direct drill-down technique. Let's see how to carry out such drill-down using <span class="strong"><strong>Espejito</strong></span> (<a class="ulink" href="https://github.com/kumarshantanu/espejito">https://github.com/kumarshantanu/espejito</a>), a Clojure<a class="indexterm" id="id0513"/> library for measuring latency (as of version 0.1.0) across measurement points in single-threaded execution paths. There are two parts of <a class="indexterm" id="id0514"/>using <span class="strong"><strong>Espejito</strong></span>, both requiring change to your code—one to wrap the code being measured, and the other to report the collected measurement data. The following code illustrates a contrived E-commerce use case of adding an item to a cart:</p><div class="informalexample"><pre class="programlisting">(require '[espejito.core :as e])

;; in the top level handler (entry point to the use case)
(e/report e/print-table
  ...)

;; in web.clj
(e/measure "web/add-cart-item"
  (biz/add-item-to-cart (resolve-cart request) item-code qty)
  ...)

;; in service.clj (biz)
(defn add-item-to-cart
  [cart item qty]
  (e/measure "biz/add-cart-item"
    (db/save-cart-item (:id cart) (:id item) qty)
    ...))

;; in db.clj (db)
(defn save-cart-item
  [cart-id item-id qty]
  (e/measure "db/save-cart-item"
    ...))</pre></div><p>Reporting a call is required to be made only once at the outermost (top-level) layer of the code. Measurement calls can be made at any number of places in the call path. Be careful not to put measurement calls inside tight loops, which may shoot memory consumption up. When this execution path is triggered, the functionality works as usual, while the latencies are<a class="indexterm" id="id0515"/> measured and recorded alongside transparently in memory. The <code class="literal">e/report</code> call prints a table of recorded metrics. An example output (edited to fit) would be:</p><div class="informalexample"><pre class="programlisting">|                 :name|:cumulat|:cumul%|:indiv |:indiv%|:thrown?|
|----------------------+--------+-------+-------+-------+--------|
|    web/add-cart-item |11.175ms|100.00%|2.476ms|22.16% |        |
| biz/add-item-to-cart | 8.699ms| 77.84%|1.705ms|15.26% |        |
|    db/save-cart-item | 6.994ms| 62.59%|6.994ms|62.59% |        |</pre></div><p>Here we can observe that the database call is the most expensive (individual latency), followed by the web layer. Our tuning preference may be guided by the order of expensiveness of the measurement points.</p><div class="section" title="Measure only when it is hot"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec033"/>Measure only when it is hot</h3></div></div></div><p>One important <a class="indexterm" id="id0516"/>aspect we did not cover in the drill-down measurement is whether the environment is ready for measurement. The <code class="literal">e/report</code> call is invoked unconditionally every time, which would not only have its own overhead (table printing), but the JVM may not be warmed up and the JIT compiler may not have kicked in to correctly report the latencies. To ensure that we report only meaningful latencies, let's trigger the <code class="literal">e/report</code> call on an example condition:</p><div class="informalexample"><pre class="programlisting">(defmacro report-when
  [test &amp; body]
  `(if ~test
    (e/report e/print-table
      ~@body)
    ~@body))</pre></div><p>Now, let's assume it is a <span class="strong"><strong>Ring</strong></span>-based (<a class="ulink" href="https://github.com/ring-clojure/ring">https://github.com/ring-clojure/ring</a>) web app and you <a class="indexterm" id="id0517"/>want to trigger the reporting only when the web request contains a parameter <code class="literal">report</code> with a value <code class="literal">true</code>. In that case, your call might look like the following:</p><div class="informalexample"><pre class="programlisting">(report-when (= "true" (get-in request [:params "report"]))
  ...)</pre></div><p>Condition-based invocation expects the JVM to be up across several calls, so it may not work with command-line apps.</p><p>This technique can also be used in performance tests, where non-reporting calls may be made during a certain warm-up period, followed by a reporting call that provides its own reporter function instead of <code class="literal">e/print-table</code>. You may even write a sampling reporter function that <a class="indexterm" id="id0518"/>aggregates the samples over a duration and finally reports the latency metrics. Not only for performance testing, you can use this for latency monitoring where the reporter function logs the metrics instead of printing a table, or sends the latency breakup to a metrics aggregation system.</p></div></div><div class="section" title="Garbage collection bottlenecks"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec099"/>Garbage collection bottlenecks</h2></div></div></div><p>Since Clojure runs <a class="indexterm" id="id0519"/>on the JVM, one has to be aware of the GC behavior in the application. You can print out the GC details at runtime by specifying the respective JVM options in <code class="literal">project.clj</code> or on the Java <a class="indexterm" id="id0520"/>command-line:</p><div class="informalexample"><pre class="programlisting">:jvm-options ^:replace [..other options..
<span class="strong"><strong>                        "-verbose:gc" "-XX:+PrintGCDetails"</strong></span>
<span class="strong"><strong>                        "-XX:+PrintGC" "-XX:+PrintGCTimeStamps"</strong></span>
                        ..other options..]</pre></div><p>This causes a detailed summary of GC events to be printed as the application runs. To capture the output in a file, you can specify the following parameter:</p><div class="informalexample"><pre class="programlisting">:jvm-options ^:replace [..other options..
                        "-verbose:gc" "-XX:+PrintGCDetails"
                        "-XX:+PrintGC" "-XX:+PrintGCTimeStamps"
<span class="strong"><strong>                        "-Xloggc:./memory.log"</strong></span>
                        ..other options..]</pre></div><p>It is also useful to see the time between and during full GC events:</p><div class="informalexample"><pre class="programlisting">:jvm-options ^:replace [..other options..
                        "-verbose:gc" "-XX:+PrintGCDetails"
                        "-XX:+PrintGC" "-XX:+PrintGCTimeStamps"
<span class="strong"><strong>                        "-XX:+PrintGCApplicationStoppedTime"</strong></span>
<span class="strong"><strong>                        "-XX:+PrintGCApplicationConcurrentTime"</strong></span>
                        ..other options..]</pre></div><p>The other useful options to troubleshoot GC are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">-XX:+HeapDumpOnOutOfMemoryError</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">-XX:+PrintTenuringDistribution</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">-XX:+PrintHeapAtGC</code></li></ul></div><p>The output of the <a class="indexterm" id="id0521"/>previous options may help you identify GC bottlenecks that you can try to fix by choosing the<a class="indexterm" id="id0522"/> right garbage collector, other generational heap options, and code changes. For easy viewing of GC logs, you may like to use GUI tools such as<a class="indexterm" id="id0523"/> <span class="strong"><strong>GCViewer</strong></span> (<a class="ulink" href="https://github.com/chewiebug/GCViewer">https://github.com/chewiebug/GCViewer</a>) for this purpose.</p><div class="section" title="Threads waiting at GC safepoint"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec034"/>Threads waiting at GC safepoint</h3></div></div></div><p>When there is a long<a class="indexterm" id="id0524"/> tight loop (without any I/O operation) in the code, the thread executing it cannot be brought to safepoint if GC happens when the loop ends or goes out of memory (for example, fails to allocate). This may have a disastrous effect of stalling other critical threads during GC. You can identify this category of bottleneck by enabling safepoint logs using the following JVM option:</p><div class="informalexample"><pre class="programlisting">:jvm-options ^:replace [..other options..
                        "-verbose:gc" "-XX:+PrintGCDetails"
                        "-XX:+PrintGC" "-XX:+PrintGCTimeStamps"
<span class="strong"><strong>                        "-XX:+PrintSafepointStatistics"</strong></span>
                        ..other options..]</pre></div><p>The safepoint logs emitted by the previous option may help you identify the impact of a tight-loop thread on other threads during GC.</p></div><div class="section" title="Using jstat to probe GC details"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec035"/>Using jstat to probe GC details</h3></div></div></div><p>The Oracle<a class="indexterm" id="id0525"/> JDK (also OpenJDK, Azul's Zulu) comes with a utility called <code class="literal">jstat</code> that can be handy to inspect GC details. You can find details on<a class="indexterm" id="id0526"/> this utility at <a class="ulink" href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html">https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html</a> —the following examples show how to use it:</p><div class="informalexample"><pre class="programlisting">jstat -gc -t &lt;process-id&gt; 10000
jstat -gccause -t &lt;process-id&gt; 10000</pre></div><p>The first <a class="indexterm" id="id0527"/>command mentioned previously monitors object allocations and freeing in various heap generations, together with other GC statistics, one in every 10 seconds. The second command also prints the reason for GC, along with other details.</p></div></div><div class="section" title="Inspecting generated bytecode for Clojure source"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0100"/>Inspecting generated bytecode for Clojure source</h2></div></div></div><p>We discussed in <a class="link" href="ch10.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span> how to see the generated equivalent Java code for any Clojure <a class="indexterm" id="id0528"/>code. Sometimes, there may not be a direct correlation between the generated bytecode and Java, which is when inspecting the generated bytecode is very useful. Of course, it requires the reader to know<a class="indexterm" id="id0529"/> at least a bit about the JVM instruction set (<a class="ulink" href="http://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html">http://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html</a>). This tool can allow you to very effectively analyze the cost of the generated bytecode instructions.</p><p>The project <span class="strong"><strong>no.disassemble</strong></span> (<a class="ulink" href="https://github.com/gtrak/no.disassemble">https://github.com/gtrak/no.disassemble</a>) is a very useful tool to<a class="indexterm" id="id0530"/> discover the generated bytecode. Include it in your <code class="literal">project.clj</code> file as a Leiningen plugin:</p><div class="informalexample"><pre class="programlisting">:plugins [[lein-nodisassemble "0.1.3"]]</pre></div><p>Then, at the REPL, you can inspect the generated bytecodes one by one:</p><div class="informalexample"><pre class="programlisting">(require '[no.disassemble :as n])
(println (n/disassemble (map inc (range 10))))</pre></div><p>The previous snippet prints out the bytecode of the Clojure expression entered there.</p></div><div class="section" title="Throughput bottlenecks"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0101"/>Throughput bottlenecks</h2></div></div></div><p>The throughput <a class="indexterm" id="id0531"/>bottlenecks usually arise from shared resources, which could be CPU, cache, memory, mutexes and locks, GC, disk, and other I/O devices. Each of these resources has a different way to find utilization, saturation, and load level. This also heavily depends on the operating system in use, as it manages the resources. Delving into the OS-specific ways of determining those factors is beyond the scope of this text. However, we will look at profiling some of these for bottlenecks in the next section.</p><p>The net effect of throughput shows up as an inverse relationship with latency. This is natural as per Little's law—as we will see in the next chapter. We covered throughput testing and latency testing under concurrency in <a class="link" href="ch13.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span>. This should be roughly a good indicator of the throughput trend.</p></div></div>
<div class="section" title="Profiling code with VisualVM"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec050"/>Profiling code with VisualVM</h1></div></div></div><p>The Oracle JDK (also OpenJDK) comes with a powerful profiler called <span class="strong"><strong>VisualVM</strong></span>; the distribution that comes<a class="indexterm" id="id0532"/> with the JDK is known as Java VisualVM and can be invoked using the binary executable:</p><div class="informalexample"><pre class="programlisting">jvisualvm</pre></div><p>This launches the GUI profiler app where you can connect to running instances of the JVM. The profiler has powerful<a class="indexterm" id="id0533"/> features (<a class="ulink" href="https://visualvm.java.net/features.html">https://visualvm.java.net/features.html</a>) that can be useful for finding various bottlenecks in code. Besides analyzing heap dump and thread dump, VisualVM can interactively graph CPU and heap consumption, and thread status in near real time. It also has sampling and tracing profilers for both CPU and memory.</p></div>
<div class="section" title="The Monitor tab"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec051"/>The Monitor tab</h1></div></div></div><p>The <span class="strong"><strong>Monitor</strong></span> tab has a<a class="indexterm" id="id0534"/> graphical overview of the runtime, including CPU, heap, threads and loaded classes:</p><div class="mediaobject"><img alt="The Monitor tab" src="graphics/3642_07_01.jpg"/></div><p>This tab is useful for "at a glance" information, leaving further drill-down for other tabs.</p><div class="section" title="The Threads tab"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0102"/>The Threads tab</h2></div></div></div><p>In the following screenshot, the <span class="strong"><strong>Threads</strong></span> tab shows the status of all threads:</p><div class="mediaobject"><img alt="The Threads tab" src="graphics/3642_07_02.jpg"/></div><p>It is very useful to find out if any threads are undergoing contention, entering deadlock, are underutilized, or they are taking up more<a class="indexterm" id="id0535"/> CPU. Especially in concurrent apps with in-memory state, and in apps that use limited I/O resources (such as connection pools, or network calls to other hosts) shared by threads, this feature provides a great insight if you set the thread names:</p><p>Notice the threads named <span class="strong"><strong>citius-RollingStore-store-1</strong></span> through <span class="strong"><strong>citius-RollingStore-store - 4</strong></span>. In an ideal no-contention scenario, those threads would have a green <span class="strong"><strong>Running</strong></span> status. See the legend at the bottom right of the image, which explains thread state:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Running</strong></span>: A thread is running, which<a class="indexterm" id="id0536"/> is the ideal condition.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sleeping</strong></span>: A thread has <a class="indexterm" id="id0537"/>yielded control temporarily.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Wait</strong></span>: A thread is waiting for notification in a critical section. <code class="literal">Object.wait()</code> was called, and is now<a class="indexterm" id="id0538"/> waiting for <code class="literal">Object.notify()</code> or <code class="literal">Object.notifyAll()</code> to wake it up.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Park</strong></span>: A thread is parked <a class="indexterm" id="id0539"/>on a permit (binary semaphore) waiting for some condition. Usually seen with concurrent blocking calls in the <code class="literal">java.util.concurrent</code> API.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Monitor</strong></span>: A thread has<a class="indexterm" id="id0540"/> reached object monitor waiting for some lock, perhaps waiting to enter or exit a critical section.</li></ul></div><p>You can install the<a class="indexterm" id="id0541"/> <span class="emphasis"><em>Threads Inspector</em></span> plugin for details on threads of interest. To inspect thread dumps from the command line you can use the <code class="literal">jstack</code> or <code class="literal">kill -3</code> commands.</p></div><div class="section" title="The Sampler tab"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0103"/>The Sampler tab</h2></div></div></div><p>The <span class="strong"><strong>Sampler</strong></span> tab is the lightweight sampling profiler tab that can sample both CPU and memory consumption. You<a class="indexterm" id="id0542"/> can easily find hotspots in code that may benefit from tuning. However, sampler profiling is limited by sampling period and frequency, inability to detect inlined code, and so on. It is a good general indicator of the bottlenecks <a class="indexterm" id="id0543"/>and looks similar to the screenshots we saw in <a class="link" href="ch13.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span>. You can profile either CPU or memory at a time.</p><p>The <span class="strong"><strong>CPU</strong></span> tab<a class="indexterm" id="id0544"/> displays both the overall CPU time distribution and per-thread CPU consumption. You can take a thread dump while sampling is in progress and analyze the dump. There are several VisualVM plugins available for more analysis.</p><p>The <span class="strong"><strong>Memory</strong></span> tab displays heap histogram metrics with distribution and instance count of objects. It also shows a PermGen histogram and per thread allocation data. It is a very good idea and highly recommended to set thread names in your project so that it is easy to locate those names in such tools. In this tab, you can force a GC, take a heap dump for analysis, and view memory metrics data in several ways.</p><div class="section" title="Setting the thread name"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec036"/>Setting the thread name</h3></div></div></div><p>Setting a thread name in <a class="indexterm" id="id0545"/>Clojure is quite straightforward using Java interop:</p><div class="informalexample"><pre class="programlisting">(.setName ^Thread (Thread/currentThread) "service-thread-12")</pre></div><p>However, since threads often transcend several contexts, in most cases you should do so in a limited scope as follows:</p><div class="informalexample"><pre class="programlisting">(defmacro with-thread-name
  "Set current thread name; execute body of code in that context."
  [new-name &amp; body]
  `(let [^Thread thread# (Thread/currentThread)
         ^String t-name# thread#]
     (.setName thread# ~new-name)
     (try
       ~@body
       (finally
         (.setName thread# t-name#)))</pre></div><p>Now you can use this macro to execute any body of code with a specified thread name:</p><div class="informalexample"><pre class="programlisting">(with-thread-name (str "process-order-" order-id)
  ;; business code
  )</pre></div><p>This style of<a class="indexterm" id="id0546"/> setting a thread name makes sure that the original name is restored before leaving the thread-local scope. If your code has various sections and you are setting a different thread name for each section, you can detect which code sections are causing contention by looking at the name when any contention appears on profiling and monitoring tools.</p></div></div><div class="section" title="The Profiler tab"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0104"/>The Profiler tab</h2></div></div></div><p>The <span class="strong"><strong>Profiler</strong></span> tab lets you instrument the running code in the JVM, and profile both CPU and memory consumption. This option adds a larger overhead than the <span class="strong"><strong>Sampler</strong></span> tab, and poses a different trade<a class="indexterm" id="id0547"/> off in terms of JIT compilation, inlining, and accuracy. This tab does not have as much diversity in visualization as the <span class="strong"><strong>Sampler</strong></span> tab. The main difference this tab has with the <span class="strong"><strong>Sampler</strong></span> tab is it changes the bytecode of the running code for accurate measurement. When you choose CPU profiling, it starts instrumenting the code for CPU profiling. If <a class="indexterm" id="id0548"/>you switch from CPU to memory profiling, it re-instruments the running code for memory profiling, and re-instruments every time you want a different profiling. One downside of such instrumentation is that it may massively slow down everything if your code is deployed in application containers, such as Tomcat.</p><p>While you can get most of the common CPU bottleneck information from <span class="strong"><strong>Sampler</strong></span>, you may need the <span class="strong"><strong>Profiler</strong></span> to investigate hotspots already discovered by <span class="strong"><strong>Sampler</strong></span> and other profiling techniques. You can selectively profile and drill-down only the known bottlenecks using the instrumenting profiler, thereby restricting its ill-effects to only small parts of the code.</p></div><div class="section" title="The Visual GC tab"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0105"/>The Visual GC tab</h2></div></div></div><p>The<a class="indexterm" id="id0549"/> <span class="strong"><strong>Visual GC</strong></span> is a <a class="indexterm" id="id0550"/>VisualVM plugin that visually depicts the GC status in near real time.</p><div class="mediaobject"><img alt="The Visual GC tab" src="graphics/3642_07_03.jpg"/></div><p>If your application uses<a class="indexterm" id="id0551"/> a lot of memory and potentially has GC bottlenecks, this plugin may be very useful for various troubleshooting purposes.</p></div><div class="section" title="The Alternate profilers"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0106"/>The Alternate profilers</h2></div></div></div><p>Besides VisualVM, there are<a class="indexterm" id="id0552"/> several third-party profilers and performance-monitoring tools for the Java platform. Among open source tools, Prometheus (<a class="ulink" href="http://prometheus.io/">http://prometheus.io/</a>) and<a class="indexterm" id="id0553"/> Moskito (<a class="ulink" href="http://www.moskito.org/">http://www.moskito.org/</a>) are<a class="indexterm" id="id0554"/> relatively popular. A non-exhaustive list of <a class="indexterm" id="id0555"/>Open Source performance tools is here: <a class="ulink" href="http://java-source.net/open-source/profilers">http://java-source.net/open-source/profilers</a>
</p><p>There are several commercial proprietary profilers that you may want to know about. The YourKit (<a class="ulink" href="https://www.yourkit.com/">https://www.yourkit.com/</a>) Java<a class="indexterm" id="id0556"/> profiler is probably the most notable profiler that many people have found much success with for profiling Clojure code. There are also other<a class="indexterm" id="id0557"/> profiling tools for the JVM, such as JProfiler (<a class="ulink" href="https://www.ej-technologies.com/products/jprofiler/overview.html">https://www.ej-technologies.com/products/jprofiler/overview.html</a>), which is a desktop-based profiler and web-based hosted <a class="indexterm" id="id0558"/>solutions such as New Relic<a class="indexterm" id="id0559"/> (<a class="ulink" href="http://newrelic.com/">http://newrelic.com/</a>) and<a class="indexterm" id="id0560"/> AppDynamics (<a class="ulink" href="https://www.appdynamics.com/">https://www.appdynamics.com/</a>).</p></div></div>
<div class="section" title="Performance tuning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec052"/>Performance tuning</h1></div></div></div><p>Once we get insight into the code via testing and profiling results, we need to analyze the bottlenecks worth considering for optimization. A better approach is to find the most under-performing portion and optimize it, thereby eliminating the weakest link. We discussed performance aspects of hardware and JVM/Clojure in previous chapters. Optimization and tuning requires rethinking the design and code in light of those aspects, and then refactoring for performance objectives.</p><p>Once we establish the<a class="indexterm" id="id0561"/> performance bottlenecks, we have to pinpoint the root cause and experiment with improvisations, one step at a time, to see what works. Tuning for performance is an iterative process that is backed by measurement, monitoring and experimentation.</p><div class="section" title="Tuning Clojure code"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0107"/>Tuning Clojure code</h2></div></div></div><p>Identifying the nature of the performance bottleneck helps a lot in order to experiment with the right aspects <a class="indexterm" id="id0562"/>of the code. The key is to determine the origin of cost and <a class="indexterm" id="id0563"/>whether the cost is reasonable.</p><div class="section" title="CPU/cache bound"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec037"/>CPU/cache bound</h3></div></div></div><p>As we noted in the <a class="indexterm" id="id0564"/>beginning of this chapter, setting up a project with the right JVM options and project settings informs us of reflection and boxing, the common sources of CPU-bound performance issues after poor design and algorithm choice. As a general rule, we have to see <a class="indexterm" id="id0565"/>whether we are doing unnecessary or suboptimal operations, especially inside loops. For example, transducers are amenable to better performance than lazy sequences in CPU-bound operations.</p><p>While public functions are recommended to work with immutable data structures, the implementation details can afford to use transients and arrays when performance is necessary. Records are a great alternative to maps, where appropriate, due to type hints and tight field layout in the former. Operations on primitive data types is faster (hence recommended) than their boxed equivalents.</p><p>In tight loops, besides transients and arrays you may prefer loop-recur with unchecked math for performance. You may also like to avoid using multi-methods and dynamic vars in tight loops, rather than pass arguments around. Using Java and macros may be the last resort, but still an option if there is such a need for performance.</p></div><div class="section" title="Memory bound"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec038"/>Memory bound</h3></div></div></div><p>Allocating less memory<a class="indexterm" id="id0566"/> in code is always going to reduce memory-related performance issues. Optimization of memory-bound code is not only about reducing memory consumption, but it is also about memory layout and utilizing the CPU and cache well. We have to see whether we are using the data types that fit well in CPU registers and cache lines. For cache and memory-bound code, we have to know whether there are cache misses and the reason—often the data might be too large to fit in a cache line. For memory-bound code we have to care about data locality, whether the <a class="indexterm" id="id0567"/>code is hitting the interconnect too often, and whether memory representation of data can be slimmed down.</p></div><div class="section" title="Multi-threaded"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec039"/>Multi-threaded</h3></div></div></div><p>Shared resources with side effects are the main source of contention and performance bottlenecks in multi-threaded code. As we saw in the <span class="emphasis"><em>Profiling VisualVM code </em></span>section in this chapter, profiling the threads better informs us about the bottlenecks. The best way to improve performance of multi-threaded <a class="indexterm" id="id0568"/>code is to reduce contention. The easy way to reduce contention is to increase the resources and reduce concurrency, though only optimal levels of resources and concurrency would be good for performance. While designing for concurrency, append only, single writer, and shared nothing approaches work well.</p><p>Another way to reduce contention may be to exploit thread-local queueing of data until resources are available. This technique is similar to what Clojure agents use, though it is an involved technique. <a class="link" href="ch12.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span> covers agents in some detail. I would encourage you to study the agents source code for better understanding. When using CPU-bound resources (for example <code class="literal">java.util.concurrent.atomic.AtomicLong</code>) you may use the contention-striping technique used by some Java 8 classes (such as <code class="literal">java.util.concurrent.atomic.LongAdder</code>, which also balances between memory consumption and contention striping across processors.) This technique is also quite involved and generic contention-striping solutions may have to trade off read consistency to allow fast updates.</p></div></div><div class="section" title="I/O bound"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0108"/>I/O bound</h2></div></div></div><p>I/O-bound tasks could be limited by bandwidth or IOPS/latency. Any I/O bottleneck usually manifests in chatty I/O calls or unconstrained data serialization. Restricting I/O to only minimum required<a class="indexterm" id="id0569"/> data is a common opportunity to minimize serialization and reduce latency. I/O operations can often be batched for higher throughput, for example <span class="emphasis"><em>SpyMemcached</em></span> library employs an asynchronous batched operation for high throughput.</p><p>I/O-bound bottlenecks are often coupled with multi-threaded scenarios. When the I/O calls are synchronous (for example, the JDBC API), one naturally has to depend upon multiple threads working on a bounded resource pool. Asynchronous I/O can relieve our threads from blocking, letting the threads do other useful work until the I/O response arrives. In synchronous I/O, we pay the cost of having threads (each allocated with memory) block on I/O calls while the kernel schedules them.</p></div><div class="section" title="JVM tuning"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0109"/>JVM tuning</h2></div></div></div><p>Often Clojure applications might inherit bloat from Clojure/Java libraries or frameworks, which cause poor performance. Hunting down unnecessary abstractions and unnecessary layers of code may bring decent performance gains. Reasoning about the performance of dependency libraries/frameworks before inclusion in a project is a good approach.</p><p>The JIT compiler, garbage collector and safepoint (in Oracle HotSpot JVM) have a significant impact on<a class="indexterm" id="id0570"/> the performance of applications. We discussed the JIT compiler and garbage collector in <a class="link" href="ch11.html" title="Chapter 4. Host Performance">Chapter 4</a>, <span class="emphasis"><em>Host Performance</em></span>. When the HotSpot JVM reaches a point when it cannot carry out concurrent, incremental GC anymore, it needs to suspend the JVM safely in order to carry out a full GC. It is also called the stop-the-world GC pause that may run up to several minutes while the JVM appears frozen.</p><p>The Oracle and OpenJDK JVMs accept many command-line options when invoked, to tune and monitor the way components in the JVM behave. Tuning GC is common among people who want to extract optimum performance from the JVM. </p><p>You may like to experiment with the following JVM options (Oracle JVM or OpenJDK) for performance:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>JVM option</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-XX:+AggressiveOpts</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Aggressive options that enable <a class="indexterm" id="id0571"/>compressed heap pointers</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-server</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Server class<a class="indexterm" id="id0572"/> JIT thresholds (use -client for GUI apps)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-XX:+UseParNewGC</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Use <a class="indexterm" id="id0573"/>Parallel GC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-Xms3g</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specify min<a class="indexterm" id="id0574"/> heap size (keep it less on desktop apps)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-Xmx3g</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specify <a class="indexterm" id="id0575"/>max heap size (keep min/max same on servers)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-XX:+UseLargePages</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Reduce<a class="indexterm" id="id0576"/> Translation-Lookaside Buffer misses (if OS supports), see <a class="ulink" href="http://www.oracle.com/technetwork/java/javase/tech/largememory-jsp-137182.html">http://www.oracle.com/technetwork/java/javase/tech/largememory-jsp-137182.html</a> for details</p>
</td></tr></tbody></table></div><p>On the Java 6 HotSpot JVM, the<a class="indexterm" id="id0577"/> <span class="strong"><strong>Concurrent Mark and Sweep</strong></span> (<span class="strong"><strong>CMS</strong></span>) garbage collector is well regarded for its GC performance. On the Java 7 and Java 8 HotSpot JVM, the default GC is a parallel collector (for better throughput), whereas at the time of writing this, there is a proposal to use the G1 collector (for lower pauses) by default in the upcoming Java 9. Note that the JVM GC can be tuned for different objectives, hence the same exact configuration for one application may not work well for another. Refer to the documents Oracle published for tuning the JVM at the<a class="indexterm" id="id0578"/> following links:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://www.oracle.com/technetwork/java/tuning-139912.html">http://www.oracle.com/technetwork/java/tuning-139912.html</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/">https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/</a></li></ul></div></div><div class="section" title="Back pressure"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec0110"/>Back pressure</h2></div></div></div><p>It is not uncommon to see applications behaving poorly under load. Typically, the application server simply appears unresponsive, which is often a combined result of high resource utilization, GC pressure, more threads that lead to busier thread scheduling, and cache misses. If the capacity of a system is known, the solution is to apply <span class="strong"><strong>back pressure</strong></span> by denying <a class="indexterm" id="id0579"/>services after the capacity is reached. Note that back pressure cannot be applied optimally until the system is load-tested for optimum capacity. The capacity threshold that triggers back pressure may or may not be directly associated with individual services, but rather can be defined as load criteria.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec053"/>Summary</h1></div></div></div><p>It is worth reiterating that performance optimization begins with learning about how the underlying system works, and measuring the performance of systems we build under representative hardware and load. The chief component of performance optimization is identifying the bottlenecks using various kinds of measurements and profiling. Thereafter, we can apply experiments to tune the performance of code and measure/profile once again to verify. The tuning mechanism varies depending on the type of bottleneck.</p><p>In the next chapter, we will see how to address performance concerns when building applications. Our focus will be on the several common patterns that impact performance.</p></div>
<div class="chapter" title="Chapter&#xA0;8.&#xA0;Application Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch15"/>Chapter 8. Application Performance</h1></div></div></div><p>The earliest computing devices were built to perform automatic computations and, as computers grew in power, they became increasingly popular because of how much and how fast they could compute. Even today, this essence lives on in our anticipation that computers can execute our business calculations faster than before by means of the applications we run on them.</p><p>Compared to performance analysis and optimization at a smaller component level, as we saw in previous chapters, it takes a holistic approach to improve performance at the application level. The higher-level concerns, such as serving a certain number of users in a day, or handling an identified quantum of load through a multi-layered system, requires us to think about how the components fit together and how the load is designed to flow through it. In this chapter, we will discuss such high-level concerns. Like the previous chapter, by and large this chapter applies to applications written in any JVM language, but with a focus on Clojure. In this chapter, we will discuss general performance techniques that apply to all layers of the code:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Choosing libraries</li><li class="listitem" style="list-style-type: disc">Logging</li><li class="listitem" style="list-style-type: disc">Data sizing</li><li class="listitem" style="list-style-type: disc">Resource pooling</li><li class="listitem" style="list-style-type: disc">Fetch and compute in advance</li><li class="listitem" style="list-style-type: disc">Staging and batching</li><li class="listitem" style="list-style-type: disc">Little's law</li></ul></div><div class="section" title="Choosing libraries"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec054"/>Choosing libraries</h1></div></div></div><p>Most non-trivial applications <a class="indexterm" id="id0580"/>depend a great deal on third-party libraries for various functionality, such as logging, serving web requests, connecting to databases, writing to message queues, and so on. Many of these libraries not only carry out parts of critical business functionality but also appear in the performance-sensitive areas of our code, impacting the overall performance. It is imperative that we choose libraries wisely (with respect to features versus performance trade off) after due performance analysis.</p><p>The crucial factor in choosing libraries is not identifying which library to use, rather it is having a performance model of our applications and having the use cases benchmarked under representative load. Only benchmarks can tell us whether the performance is problematic or acceptable. If the performance is below expectation, a drill-down profiling can show us whether a third-party library is causing the performance issue. In <a class="link" href="ch13.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span> and <a class="link" href="ch14.html" title="Chapter 7. Performance Optimization">Chapter 7</a>, <span class="emphasis"><em>Performance Optimization</em></span> we discussed how to measure performance and identify bottlenecks. You can evaluate multiple libraries for performance-sensitive use cases and choose what suits.</p><p>Libraries often <a class="indexterm" id="id0581"/>improve (or occasionally lose) performance with new releases, so measurement and profiling (comparative, across versions) should be an ongoing practice for the development and maintenance lifecycle of our applications. Another factor to note is that libraries may show different performance characteristics based on the use case, load, and the benchmark. The devil is in the benchmark details. Be sure that your benchmarks are as close as possible to the representative scenario for your application.</p><div class="section" title="Making a choice via benchmarks"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0111"/>Making a choice via benchmarks</h2></div></div></div><p>Let's take a brief look at a few<a class="indexterm" id="id0582"/> general use cases where performance of third-party libraries are exposed via benchmarks.</p><div class="section" title="Web servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec040"/>Web servers</h3></div></div></div><p>Web servers<a class="indexterm" id="id0583"/> are<a class="indexterm" id="id0584"/> typically subject to quite a bit of performance benchmarking due to their generic nature and scope. One such benchmark for Clojure web servers<a class="indexterm" id="id0585"/> exists here:</p><p>
<a class="ulink" href="https://github.com/ptaoussanis/clojure-web-server-benchmarks">https://github.com/ptaoussanis/clojure-web-server-benchmarks</a>
</p><p>Web servers are complex pieces of software and they may exhibit different characteristics under various conditions. As you will notice, the performance numbers vary based on keep-alive versus non-keep-alive modes and request volume—at the time of writing, Immutant-2 came out better in keep-alive mode but fared poorly in the non-keep-alive benchmark. In production, people often front their application servers with reverse proxy servers, for example Nginx or HAProxy, which make keep-alive connections to application servers.</p></div><div class="section" title="Web routing libraries"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec041"/>Web routing libraries</h3></div></div></div><p>There are several<a class="indexterm" id="id0586"/> web routing libraries<a class="indexterm" id="id0587"/> for Clojure, as listed here:</p><p>
<a class="ulink" href="https://github.com/juxt/bidi#comparison-with-other-routing-libraries">https://github.com/juxt/bidi#comparison-with-other-routing-libraries</a>
</p><p>The same document also shows a performance benchmark with <span class="strong"><strong>Compojure</strong></span> as the baseline, in which (at the time of writing) Compojure<a class="indexterm" id="id0588"/> turns out to be performing better than <span class="strong"><strong>Bidi</strong></span>. However, another benchmark compares Compojure, <span class="strong"><strong>Clout</strong></span> (the library that Compojure internally uses), and <span class="strong"><strong>CalfPath</strong></span>
<a class="indexterm" id="id0589"/> routing <a class="indexterm" id="id0590"/>here:</p><p>
<a class="ulink" href="https://github.com/kumarshantanu/calfpath#development">https://github.com/kumarshantanu/calfpath#development</a>
</p><p>In this benchmark, as of this writing, Clout performs better than Compojure, and CalfPath outperforms Clout. However, you should be aware of any caveats in the faster libraries.</p></div><div class="section" title="Data serialization"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec042"/>Data serialization</h3></div></div></div><p>There are <a class="indexterm" id="id0591"/>several ways to<a class="indexterm" id="id0592"/> serialize data in Clojure, for example EDN and Fressian. Nippy is another serialization library with benchmarks to demonstrate how well it performs over EDN and<a class="indexterm" id="id0593"/> Fressian:</p><p>
<a class="ulink" href="https://github.com/ptaoussanis/nippy#performance">https://github.com/ptaoussanis/nippy#performance</a>
</p><p>We covered Nippy in <a class="link" href="ch09.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span> to show how it uses transients to speed up its internal computations. Even within Nippy, there are several flavors of serialization that have different features/performance trade-offs.</p></div><div class="section" title="JSON serialization"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec043"/>JSON serialization</h3></div></div></div><p>Parsing and generating <a class="indexterm" id="id0594"/>JSON is a <a class="indexterm" id="id0595"/>very common use case in RESTful services and web applications. The Clojure contrib library clojure/data.json (<a class="ulink" href="https://github.com/clojure/data.json">https://github.com/clojure/data.json</a>) provides this functionality. However, many people have found out that the Cheshire library<a class="indexterm" id="id0596"/> <a class="ulink" href="https://github.com/dakrone/cheshire">https://github.com/dakrone/cheshire</a> performs much better than the former. The included benchmarks in Cheshire can be run using the following command:</p><div class="informalexample"><pre class="programlisting">lein with-profile dev,benchmark test</pre></div><p>Cheshire internally uses the Jackson Java library<a class="indexterm" id="id0597"/> <a class="ulink" href="https://github.com/FasterXML/jackson">https://github.com/FasterXML/jackson</a>, which is known for its good performance.</p></div><div class="section" title="JDBC"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec044"/>JDBC</h3></div></div></div><p>JDBC access is <a class="indexterm" id="id0598"/>another very common use case <a class="indexterm" id="id0599"/>among applications using relational databases. The Clojure contrib library <code class="literal">clojure/java.jdbc</code> <a class="ulink" href="https://github.com/clojure/java.jdbc">https://github.com/clojure/java.jdbc</a> provides a Clojure JDBC API. Asphalt <a class="ulink" href="https://github.com/kumarshantanu/asphalt">https://github.com/kumarshantanu/asphalt</a> is an<a class="indexterm" id="id0600"/> alternative JDBC library where the comparative benchmarks can be run as follows:</p><div class="informalexample"><pre class="programlisting">lein with-profile dev,c17,perf test</pre></div><p>As of this writing, Asphalt outperforms <code class="literal">clojure/java.jdbc</code> by several micro seconds, which may be useful in low-latency applications. However, note that JDBC performance is usually dominated by SQL queries/joins, database latency, connection pool parameters, and so on. We will discuss more about JDBC in later sections.</p></div></div></div></div>
<div class="section" title="Logging"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec055"/>Logging</h1></div></div></div><p>Logging<a class="indexterm" id="id0601"/> is a prevalent activity that almost all non-trivial applications do. Logging calls are quite frequent, hence it is important to make sure our logging configuration is tuned well for performance. If you are not familiar with logging systems (especially on the JVM), you<a class="indexterm" id="id0602"/> may want to take some time to get familiar with those first. We will cover the use of <code class="literal">clojure/tools.logging</code>, <span class="strong"><strong>SLF4J</strong></span>
<a class="indexterm" id="id0603"/> and <span class="strong"><strong>LogBack</strong></span>
<a class="indexterm" id="id0604"/> libraries (as a combination) for logging, and look into<a class="indexterm" id="id0605"/> how to make them perform well:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Clojure/tools.logging <a class="ulink" href="https://github.com/clojure/tools.logging">https://github.com/clojure/tools.logging</a></li><li class="listitem" style="list-style-type: disc">SLF4J: <a class="ulink" href="http://www.slf4j.org/">http://www.slf4j.org/</a></li><li class="listitem" style="list-style-type: disc">LogBack: <a class="ulink" href="http://logback.qos.ch/">http://logback.qos.ch/</a></li></ul></div><div class="section" title="Why SLF4J/LogBack?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0112"/>Why SLF4J/LogBack?</h2></div></div></div><p>Besides SLF4J/LogBack, there are several logging libraries to choose from in the Clojure application, for <a class="indexterm" id="id0606"/>example Timbre, Log4j and java.util.logging. While there is nothing <a class="indexterm" id="id0607"/>wrong with these libraries, we are often constrained into choosing something that covers most other third-party libraries (also including Java libraries) in our applications for logging purposes. SLF4J is a Java logger facade that detects any available implementation (LogBack, Log4j, and so on) —we choose LogBack simply because it performs well and is highly configurable. The library clojure/tools.logging provides a Clojure logging API that detects SLF4J, Log4j or java.util.logging (in that order) in the classpath and uses whichever implementation is found first.</p></div><div class="section" title="The setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0113"/>The setup</h2></div></div></div><p>Let's walk through <a class="indexterm" id="id0608"/>how to set up a logging system for your application using LogBack, SLF4J <a class="indexterm" id="id0609"/>and <code class="literal">clojure/tools.logging</code> for a project built using Leiningen.</p><div class="section" title="Dependencies"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec045"/>Dependencies</h3></div></div></div><p>Your <code class="literal">project.clj</code> file should have<a class="indexterm" id="id0610"/> the LogBack, SLF4J and <code class="literal">clojure/tools.logging</code> dependencies under the <code class="literal">:dependencies</code> key:</p><div class="informalexample"><pre class="programlisting">[ch.qos.logback/logback-classic "1.1.2"]
[ch.qos.logback/logback-core    "1.1.2"]
[org.slf4j/slf4j-api            "1.7.9"]
[org.codehaus.janino/janino     "2.6.1"]  ; for Logback-config
[org.clojure/tools.logging      "0.3.1"]</pre></div><p>The previously mentioned versions are current and work as of the time of writing. You may want to use updated versions, if available.</p></div><div class="section" title="The logback configuration file"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec046"/>The logback configuration file</h3></div></div></div><p>You need to create a <code class="literal">logback.xml</code> file<a class="indexterm" id="id0611"/> in the <code class="literal">resources</code> directory:</p><div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;configuration&gt;

  &lt;appender name="FILE"
            class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
    &lt;file&gt;${logfile.general.name:-logs/application.log}&lt;/file&gt;
    &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt;
      &lt;!-- daily rollover --&gt;
      &lt;fileNamePattern&gt;${logfile.general.name:-logs/application.log}.%d{yyyy-MM-dd}.%i.gz&lt;/fileNamePattern&gt;
      &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt;
        &lt;!-- or whenever the file size reaches 100MB --&gt;
        &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt;
      &lt;/timeBasedFileNamingAndTriggeringPolicy&gt;
      &lt;!-- keep 30 days worth of history --&gt;
      &lt;maxHistory&gt;30&lt;/maxHistory&gt;
    &lt;/rollingPolicy&gt;
    &lt;append&gt;true&lt;/append&gt;
    &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt;
      &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt;
        &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;
      &lt;/layout&gt;
<span class="strong"><strong>      &lt;immediateFlush&gt;false&lt;/immediateFlush&gt;</strong></span>
    &lt;/encoder&gt;
  &lt;/appender&gt;

<span class="strong"><strong>  &lt;appender name="AsyncFile" class="ch.qos.logback.classic.AsyncAppender"&gt;</strong></span>
<span class="strong"><strong>    &lt;queueSize&gt;500&lt;/queueSize&gt;</strong></span>
<span class="strong"><strong>    &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt;</strong></span>
<span class="strong"><strong>    &lt;appender-ref ref="FILE" /&gt;</strong></span>
<span class="strong"><strong>  &lt;/appender&gt;</strong></span>

  &lt;!-- You may want to set the level to DEBUG in development --&gt;
  &lt;root level="ERROR"&gt;
<span class="strong"><strong>    &lt;appender-ref ref="AsyncFile" /&gt;</strong></span>
  &lt;/root&gt;

  &lt;!-- Replace com.example with base namespace of your app --&gt;
  &lt;logger name="com.example" additivity="false"&gt;
    &lt;!-- You may want to set the level to DEBUG in development --&gt;
    &lt;level value="INFO"/&gt;
<span class="strong"><strong>    &lt;appender-ref ref="AsyncFile" /&gt;</strong></span>
  &lt;/logger&gt;

&lt;/configuration&gt;</pre></div><p>The previous <code class="literal">logback.xml</code> file<a class="indexterm" id="id0612"/> is simple on purpose (for illustration) and has just enough configuration to get you started with logging using LogBack.</p></div><div class="section" title="Optimization"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec047"/>Optimization</h3></div></div></div><p>The optimization points <a class="indexterm" id="id0613"/>are highlighted in the <code class="literal">logback.xml</code> file we saw earlier in this section. We set the <code class="literal">immediateFlush</code> attribute to <code class="literal">false</code> such that the messages are buffered before flushing to the appender. We also wrapped the regular file appender with an asynchronous appender and edited the <code class="literal">queueSize</code> and <code class="literal">discardingThreshold</code> attributes, which gets us much better results than the default.</p><p>Unless optimized, logging<a class="indexterm" id="id0614"/> configurations are usually a common source of suboptimal performance in many applications. Usually, the performance problems show up only at high load when the log volume is very high. The optimizations discussed previously are only a few of the many possible optimizations that one can experiment with. The chapters in LogBack documentation, such as <a class="indexterm" id="id0615"/>
<span class="strong"><strong>encoders</strong></span> (<a class="ulink" href="http://logback.qos.ch/manual/encoders.html">http://logback.qos.ch/manual/encoders.html</a>), <span class="strong"><strong>appenders</strong></span> (<a class="ulink" href="http://logback.qos.ch/manual/appenders.html">http://logback.qos.ch/manual/appenders.html</a>) and<a class="indexterm" id="id0616"/> <span class="strong"><strong>configuration</strong></span> (<a class="ulink" href="http://logback.qos.ch/manual/configuration.html">http://logback.qos.ch/manual/configuration.html</a>) have useful <span class="strong"><strong>information</strong></span>. There are also tips <a class="ulink" href="http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/">http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/</a> on the Internet that may provide useful pointers.</p></div></div></div>
<div class="section" title="Data sizing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec056"/>Data sizing</h1></div></div></div><p>The cost of abstractions in terms of the<a class="indexterm" id="id0617"/> data size plays an important role. For example, whether or not a data element can fit into a processor cache line depends directly upon its size. On a Linux system, we can find out the cache line size and other parameters by inspecting the values in the files under the <code class="literal">/sys/devices/system/cpu/cpu0/cache/</code> directory. Refer to <a class="link" href="ch11.html" title="Chapter 4. Host Performance">Chapter 4</a>, <span class="emphasis"><em>Host Performance</em></span>, where we discussed how to compute the size of primitives, objects, and data elements.</p><p>Another concern we generally find with data sizing is how much data we hold at any time in the heap. As we noted in earlier chapters, GC has direct consequences on the application performance. While processing data, often we do not really need all the data we hold on to. Consider the example of generating a summary report of sold items for a certain period (months) of time. After the subperiod (month-wise) summary data is computed, we do not need the item details anymore, hence it's better to remove the unwanted data while we add the summaries. See the following example:</p><div class="informalexample"><pre class="programlisting">(defn summarize [daily-data]  ; daily-data is a map
  (let [s (items-summary (:items daily-data))]
    (-&gt; daily-data
      (select-keys [:digest :invoices])  ; keep required k/v pairs
      (assoc :summary s))))

;; now inside report generation code
(-&gt; (fetch-items period-from period-to :interval-day)
  (map summarize)
  generate-report)</pre></div><p>Had we not <a class="indexterm" id="id0618"/>used <code class="literal">select-keys</code> in the previous <code class="literal">summarize</code> function, it would have returned a map with extra :<code class="literal">summary</code> data along with all other existing keys in the map. Now, such a thing is often combined with lazy sequences, so for this scheme to work it is important not to hold onto the head of the lazy sequence. Recall that in <a class="link" href="ch09.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span> we discussed the perils of holding onto the head of a lazy sequence.</p><div class="section" title="Reduced serialization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0114"/>Reduced serialization</h2></div></div></div><p>We discussed in earlier chapters <a class="indexterm" id="id0619"/>that serialization over an I/O channel is a common source of latency. The perils of over-serialization cannot be overstated. Whether we read or write data from a data source over an I/O channel, all of that data needs to be prepared, encoded, serialized, de-serialized, and parsed before being worked upon. The less data that is involved, the better it is for every step in order to lower the overhead. Where there is no I/O involved (such as in-process communication), it generally makes no sense to serialize.</p><p>A common example of over-serialization is when working with SQL databases. Often, there are common SQL query functions that fetch all columns of a table or a relation—they are called by various functions that implement business logic. Fetching data that we do not need is wasteful and detrimental to performance for the same reason that we discussed in the previous paragraph. While it may seem more work to write one SQL statement and one database-query function for each use case, it pays off with better performance. Code that uses NoSQL databases is also subject to this anti-pattern—we have to take care to fetch only what we need even though it may lead to additional code.</p><p>There's a pitfall to be aware of when reducing serialization. Often, some information needs to be inferred in the absence of serialized data. In such cases, where some of the serialization is dropped so that we can infer other information, we must compare the cost of inference versus the serialization overhead. The comparison may not necessarily be only per operation, but rather on the whole, such that we can consider the resources we can allocate in order to achieve capacities for various parts of our systems.</p></div><div class="section" title="Chunking to reduce memory pressure"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0115"/>Chunking to reduce memory pressure</h2></div></div></div><p>What happens when we <a class="indexterm" id="id0620"/>slurp a text file regardless of its size? The contents of the entire file will sit in the JVM heap. If the file is larger than the JVM heap capacity, the JVM will terminate, throwing <code class="literal">OutOfMemoryError</code>. If the file is large, but not enough to force the JVM into OOM error, it leaves relatively less JVM heap space for other operations to continue in the application. Similar situations take place when we carry out any operation disregarding the JVM heap capacity. Fortunately, this can be fixed by reading data in chunks and processing them before reading more. In <a class="link" href="ch10.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span>, we briefly discussed memory<a class="indexterm" id="id0621"/> mapped<a class="indexterm" id="id0622"/> buffers, which is another complementary solution that you may like to explore.</p><div class="section" title="Sizing for file/network operations"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec048"/>Sizing for file/network operations</h3></div></div></div><p>Let's take the example of a data ingestion process where a semi-automated job uploads large <span class="strong"><strong>Comma Separated File (CSV)</strong></span>
<a class="indexterm" id="id0623"/> files via <span class="strong"><strong>File Transfer Protocol (FTP)</strong></span>
<a class="indexterm" id="id0624"/> to a file server, and another automated job (written in Clojure) <a class="indexterm" id="id0625"/>runs periodically to detect the arrival of files via a Network File System (NFS). After detecting a new file, the Clojure program processes the file, updates the result in a database, and archives the file. The program detects and processes several files concurrently. The size of the CSV files is not known in advance, but the format is predefined.</p><p>As per the previous description, one potential problem is, since there could be multiple files being processed concurrently, how do we distribute the JVM heap among the concurrent file-processing jobs? Another issue at hand could be that the operating system imposes a limit on how many files could be open at a time; on Unix-like systems you can use the <code class="literal">ulimit</code> command to extend the limit. We cannot arbitrarily slurp the CSV file contents—we must limit each job to a certain amount of memory, and also limit the number of jobs that can run concurrently. At the same time, we cannot read a very small number of rows from a file at a time because this may impact performance:</p><div class="informalexample"><pre class="programlisting">(def ^:const K 1024)

;; create the buffered reader using custom 128K buffer-size
(-&gt; filename
  java.io.FileInputStream.
  java.io.InputStreamReader.
  (java.io.BufferedReader. (* K 128)))</pre></div><p>Fortunately, we can specify the buffer size when reading from a file (or even from a network stream) so as to tune the memory usage and performance as appropriate. In the previous code example, we explicitly set the buffer size of the reader to facilitate the same.</p></div><div class="section" title="Sizing for JDBC query results"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec049"/>Sizing for JDBC query results</h3></div></div></div><p>Java's interface standard for SQL <a class="indexterm" id="id0626"/>databases, JDBC (which is technically not an acronym), supports <span class="emphasis"><em>fetch size</em></span> for fetching query results via JDBC drivers. The default fetch size depends on the JDBC driver. Most of the JDBC drivers keep a low default value to avoid high memory usage and for internal performance optimization reasons. A notable exception to this norm is the MySQL JDBC driver that completely fetches and stores all rows in memory by default:</p><div class="informalexample"><pre class="programlisting">(require '[clojure.java.jdbc :as jdbc])

;; using prepare-statement directly
(with-open
  [stmt (jdbc/prepare-statement
          conn sql :fetch-size 1000 :max-rows 9000)
   rset (resultset-seq (.executeQuery stmt))]
  (vec rset))

;; using query
(jdbc/query db [{:fetch-size 1000}
           "SELECT empno FROM emp WHERE country=?" 1])</pre></div><p>When using the Clojure contrib<a class="indexterm" id="id0627"/> library <code class="literal">java.jdbc</code> (<a class="ulink" href="https://github.com/clojure/java.jdbc">https://github.com/clojure/java.jdbc</a> as of version 0.3.7), the fetch size can be set while preparing a <a class="indexterm" id="id0628"/>statement as shown in the previous example. Note that the fetch size does not guarantee proportional latency; however, it can be used safely for memory sizing. We must test any performance-impacting latency changes due to fetch size at different loads and use cases for the particular database and JDBC driver. Another important factor to note is that the benefit of <code class="literal">:fetch-size</code> can be useful only if the query result set is consumed incrementally and lazily—if a function extracts all rows from a result set to create a vector, then the benefit of <code class="literal">:fetch-size</code> is nullified from a memory conservation point of view. Besides fetch size, we can also pass the <code class="literal">:max-rows</code> argument to limit the maximum rows to be returned by a query—however, this implies that the extra rows will be truncated from the result, and not whether the database will internally limit the number of rows to realize.</p></div></div></div>
<div class="section" title="Resource pooling"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec057"/>Resource pooling</h1></div></div></div><p>There are several types<a class="indexterm" id="id0629"/> of resources on the JVM that are rather expensive to initialize. Examples are HTTP connections, execution threads, JDBC connections, and so on. The Java API recognizes such resources and has built-in support for creating a pool of some of those resources, such that the consumer code borrows a resource from a pool when required and at the end of the job simply returns it to the pool. Java's thread pools (discussed in <a class="link" href="ch12.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>) and JDBC data sources are prominent examples. The idea is to preserve the initialized objects for reuse. Even though Java does not support pooling of a resource type directly, one can always create a pool abstraction around custom expensive resources. Note that the pooling technique is common in I/O activities, but can be equally applicable to non-I/O purposes where initialization cost is high.</p><div class="section" title="JDBC resource pooling"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0116"/>JDBC resource pooling</h2></div></div></div><p>Java supports the obtaining<a class="indexterm" id="id0630"/> of JDBC connections <a class="indexterm" id="id0631"/>via the <code class="literal">javax.sql.DataSource </code>interface, which can be pooled. A JDBC connection pool implements this interface. Typically, a JDBC connection pool is implemented by third-party libraries or a JDBC driver itself. Generally, very few JDBC drivers implement a connection pool, so Open Source third-party JDBC resource pooling libraries such as Apache DBCP, c3p0, BoneCP, HikariCP, and so on are popular. They also support validation queries for eviction of stale connections that might result from network timeouts and firewalls, and guard against connection leaks. Apache DBCP and HikariCP are accessible from Clojure via their respective Clojure wrapper libraries <a class="indexterm" id="id0632"/>Clj-DBCP (<a class="ulink" href="https://github.com/kumarshantanu/clj-dbcp">https://github.com/kumarshantanu/clj-dbcp</a>) and HikariCP<a class="indexterm" id="id0633"/> (<a class="ulink" href="https://github.com/tomekw/hikari-cp">https://github.com/tomekw/hikari-cp</a>), and there are Clojure examples describing how to construct C3P0 and BoneCP pools (<a class="ulink" href="http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html">http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html</a>).</p><p>Connections are not the only JDBC resources that need to be pooled. Every time we create a new JDBC prepared statement, depending on the JDBC driver implementation, often the entire statement template is sent to the database server in order to obtain a reference to the prepared statement. As the database servers are generally deployed on separate hardware, there may be network latency involved. Hence, the pooling of prepared statements is a very desirable property of JDBC resource pooling libraries. Apache DBCP, C3P0, and BoneCP all support statement pooling, and the Clj-DBCP wrapper enables the pooling of prepared statements out-of-the-box for better performance. HikariCP has the opinion that statement pooling, nowadays, is already done internally by JDBC drivers, hence explicit pooling is not required. I would strongly advise running your benchmarks with the connection pooling libraries to determine whether or not it really works for your JDBC driver and application.</p></div></div>
<div class="section" title="I/O batching and throttling"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec058"/>I/O batching and throttling</h1></div></div></div><p>It is well known that chatty<a class="indexterm" id="id0634"/> I/O calls generally lead to poor performance. In general, the solution <a class="indexterm" id="id0635"/>is to batch together several messages and send them in one payload. In databases and network calls, batching is a common and useful technique to improve throughput. On the other hand, large batch sizes may actually harm throughput as they tend to incur memory overhead, and components may not be ready to handle a large batch at once. Hence, sizing the batches and throttling are just as important as batching. I would strongly advise conducting your own tests to determine the optimum batch size under representative load.</p><div class="section" title="JDBC batch operations"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0117"/>JDBC batch operations</h2></div></div></div><p>JDBC has long had batch-update <a class="indexterm" id="id0636"/>support in its API, which includes the <code class="literal">INSERT</code>, <code class="literal">UPDATE</code>, <code class="literal">DELETE</code> statements. The Clojure contrib library <code class="literal">java.jdbc</code> supports JDBC batch operations via its own API, as we can see as follows:</p><div class="informalexample"><pre class="programlisting">(require '[clojure.java.jdbc :as jdbc])

;; multiple SQL statements
(jdbc/db-do-commands
  db true
  ["INSERT INTO emp (name, countrycode) VALUES ('John Smith', 3)"
   "UPDATE emp SET countrycode=4 WHERE empid=1379"])

;; similar statements with only different parametrs
(jdbc/db-do-prepared
  db true
  "UPDATE emp SET countrycode=? WHERE empid=?"
  [4 1642]
  [9 1186]
  [2 1437])</pre></div><p>Besides batch-update support, we can also batch JDBC queries. One of the common techniques is to use the SQL <code class="literal">WHERE</code> clause to avoid the <code class="literal">N+1</code> selects issue. The <code class="literal">N+1</code> issue indicates the situation when we execute one query in another child table for every row in a rowset from a master table. A similar technique can be used to combine several similar queries on the same table into just one, and segregate the data in the program afterwards. </p><p>Consider the following example that uses clojure.java.jdbc 0.3.7 and the MySQL database:</p><div class="informalexample"><pre class="programlisting">(require '[clojure.java.jdbc :as j])

(def db {:subprotocol "mysql"
         :subname "//127.0.0.1:3306/clojure_test"
         :user "clojure_test" :password "clojure_test"})

;; the snippet below uses N+1 selects
;; (typically characterized by SELECT in a loop)
(def rq "select order_id from orders where status=?")
(def tq "select * from items where fk_order_id=?")
(doseq [order (j/query db [rq "pending"])]
  (let [items (j/query db [tq (:order_id order)])]
    ;; do something with items
    …))

;; the snippet below avoids N+1 selects,
;; but requires fk_order_id to be indexed
(def jq "select t.* from orders r, items t
  where t.fk_order_id=r.order_id and r.status=? order by t.fk_order_id")
(let [all-items (group-by :fk_order_id (j/query db [jq "pending"]))]
  (doseq [[order-id items] all-items]
    ;; do something with items
    ...))</pre></div><p>In the previous<a class="indexterm" id="id0637"/> example there are two tables: <code class="literal">orders</code> and <code class="literal">items</code>. The first snippet reads all order IDs from the <code class="literal">orders</code> table, and then iterates through them to query corresponding entries in the <code class="literal">items</code> table in a loop. This is the <code class="literal">N+1</code> selects performance anti-pattern you should keep an eye on. The second snippet avoids <code class="literal">N+1</code> selects by issuing a single SQL query, but may not perform very well unless the column <code class="literal">fk_order_id</code> is indexed.</p></div><div class="section" title="Batch support at API level"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0118"/>Batch support at API level</h2></div></div></div><p>When designing any <a class="indexterm" id="id0638"/>service, it is very useful to provide an API for batch operations. This builds flexibility in the API such that batch sizing and throttling can be controlled in a fine-grained manner. Not surprisingly, it is also an effective recipe for building high-performance services. A common overhead we encounter when implementing batch operations is the identification of each item in the batch and their correlation across requests and responses. The problem becomes more prominent when requests are asynchronous.</p><p>The solution to the item identification issue is resolved either by assigning a canonical or global ID to each item in the request (batch), or by assigning every request (batch) a unique ID and each item in the request an ID that is local to the batch.</p><p>The choice of the exact solution usually depends on the implementation details. When requests are synchronous, you can do away with identification of each request item (see the Facebook API<a class="indexterm" id="id0639"/> for reference: <a class="ulink" href="http://developers.facebook.com/docs/reference/api/batch/">http://developers.facebook.com/docs/reference/api/batch/</a>) where the items in response follow the same order as in the request. However, in asynchronous requests, items may have to be tracked via status-check call or callbacks. The desired tracking granularity typically guides the appropriate item identification strategy.</p><p>For example, if we have a batch API for order processing, every order would have a unique Order-ID that can be used in subsequent status-check calls. In another example, let's say there is a batch API for creating API keys for <a class="indexterm" id="id0640"/>
<span class="strong"><strong>Internet of Things</strong></span> (<span class="strong"><strong>IoT</strong></span>) devices—here, the API keys are not known beforehand, but they can <a class="indexterm" id="id0641"/>be generated and returned in a synchronous response. However, if this has to be an asynchronous batch API, the service should respond with a batch request ID that can be used later to find the status of the request. In a batch response for the request ID, the server can include request item IDs (for example device IDs, which may be unique for the client but not unique across all clients) with their respective status.</p></div><div class="section" title="Throttling requests to services"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0119"/>Throttling requests to services</h2></div></div></div><p>As every service can <a class="indexterm" id="id0642"/>handle only a certain capacity, the rate at which <a class="indexterm" id="id0643"/>we send requests to a service is important. The expectations about the service behavior are generally in terms of both throughput and latency. This requires us to send requests at a specified rate, as a rate lower than that may lead to under-utilization of the service, and a higher rate may overload the service or result in failure, thus leading to client-side under-utilization.</p><p>Let's say a third-party service can accept 100 requests per second. However, we may not know how robustly the service is implemented. Though sometimes it is not exactly specified, sending 100 requests at once (within 20ms, let's say) during each second may lead to lower throughput than expected. Evenly distributing the requests across the one-second duration, for example sending one request every 10ms (1000ms / 100 = 10ms), may increase the chance of attaining the optimum throughput.</p><p>For throttling, <span class="strong"><strong>Token bucket</strong></span>
<a class="indexterm" id="id0644"/> (<a class="ulink" href="https://en.wikipedia.org/wiki/Token_bucket">https://en.wikipedia.org/wiki/Token_bucket</a>) and <span class="strong"><strong>Leaky bucket</strong></span>
<a class="indexterm" id="id0645"/> (<a class="ulink" href="https://en.wikipedia.org/wiki/Leaky_bucket">https://en.wikipedia.org/wiki/Leaky_bucket</a>) algorithms can be useful. Throttling at a very fine-grained level requires that we buffer the items so that we can maintain a uniform rate. Buffering consumes memory and often requires ordering; queues (covered in <a class="link" href="ch12.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>), pipeline and persistent storage usually serve that purpose well. Again, buffering and queuing may be subject to back pressure due to system constraints. We will discuss pipelines, back pressure and buffering in a later section in this chapter.</p></div></div>
<div class="section" title="Precomputing and caching"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec059"/>Precomputing and caching</h1></div></div></div><p>While <a class="indexterm" id="id0646"/>processing data, we<a class="indexterm" id="id0647"/> usually come across instances where few common computation steps precede several kinds of subsequent steps. That is to say, some amount of computation is common and the remaining is different. For high-latency common computations (I/O to access the data and memory/CPU to process it), it makes a lot of sense to compute them once and store in digest form, such that the subsequent steps can simply use the digest data and proceed from that point onward, thus resulting in reduced overall latency. This is also known as staging of semi-computed data and is a common technique to optimize processing of non-trivial data.</p><p>Clojure has decent support for caching. The built-in <code class="literal">clojure.core/memoize</code> function performs basic caching of computed results with no flexibility in using specific caching strategies and pluggable backends. The Clojure contrib library <code class="literal">core.memoize</code> offsets the lack of flexibility in <code class="literal">memoize</code> by providing several configuration options. Interestingly, the features in <code class="literal">core.memoize</code> are also useful as a separate caching library, so the common portion is factored out as a Clojure contrib library called <code class="literal">core.cache</code> on top of which <code class="literal">core.memoize</code> is implemented.</p><p>As many <a class="indexterm" id="id0648"/>applications are deployed on multiple servers for availability, scaling<a class="indexterm" id="id0649"/> and maintenance reasons, they need distributed caching that is fast and space efficient. The open source memcached project is a popular in-memory, distributed key-value/object store that can act as a caching server for web applications. It hashes the keys to identify the server to store the value on, and has no out-of-the-box replication or persistence. It is used to cache database query results, computation results, and so on. For Clojure, there is a memcached client library called <a class="indexterm" id="id0650"/>SpyGlass (<a class="ulink" href="https://github.com/clojurewerkz/spyglass">https://github.com/clojurewerkz/spyglass</a>). Of course, memcached is not limited to just web applications; it can be used for other purposes too.</p></div>
<div class="section" title="Concurrent pipelines"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec060"/>Concurrent pipelines</h1></div></div></div><p>Imagine a situation where<a class="indexterm" id="id0651"/> we have to carry out jobs at a certain throughput, such that each job includes the same sequence of differently sized I/O task (task A), a memory-bound task (task B) and, again, an I/O task (task C). A naïve approach would be to create a thread pool and run each job off it, but soon we realize that this is not optimum because we cannot ascertain the utilization of each I/O resource due to unpredictability of the threads being scheduled by the OS. We also observe that even though several concurrent jobs have similar I/O tasks, we are unable to batch them in our first approach.</p><p>As the next iteration, we split each job in stages (A, B, C), such that each stage corresponds to one task. Since the tasks are well known, we create one thread pool (of appropriate size) per stage and execute tasks in them. The result of task A is required by task B, and B's result is required by task C—we enable this communication via queues. Now, we can tune the thread pool size for each stage, batch the I/O tasks, and throttle them for an optimum throughput. This kind of an arrangement is a concurrent pipeline. Some readers may find this feebly resembling the actor model or <span class="strong"><strong>Staged Event Driven Architecture</strong></span> (<span class="strong"><strong>SEDA</strong></span>)<a class="indexterm" id="id0652"/> model, which are more refined models for this kind of approach. Recall that we discussed several kinds of in-process queues in <a class="link" href="ch12.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>.</p><div class="section" title="Distributed pipelines"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0120"/>Distributed pipelines</h2></div></div></div><p>With this approach, it is possible<a class="indexterm" id="id0653"/> to scale out the job execution to<a class="indexterm" id="id0654"/> multiple hosts in a cluster using network queues, thereby offloading memory consumption, durability, and delivery to the queue infrastructure. For example, in a given scenario there could be several nodes in a cluster, all of them running the same code and exchanging messages (requests and intermediate result data) via network queues. </p><p>The following diagram depicts how a simple invoice-generation system might be connected to network queues:</p><div class="mediaobject"><img alt="Distributed pipelines" src="graphics/3642_08_03.jpg"/></div><p>RabbitMQ, HornetQ, ActiveMQ, Kestrel and Kafka are some well-known Open Source queue systems. Once in a while, the jobs may require distributed state and coordination. The <a class="indexterm" id="id0655"/>Avout (<a class="ulink" href="http://avout.io/">http://avout.io/</a>) project implements the distributed version of Clojure's atom and ref, which can be used for this purpose. Tesser <a class="indexterm" id="id0656"/>(<a class="ulink" href="https://github.com/aphyr/tesser">https://github.com/aphyr/tesser</a>) is another library for local and distributed parallelism using Clojure. The<a class="indexterm" id="id0657"/> Storm (<a class="ulink" href="http://storm-project.net/">http://storm-project.net/</a>) and Onyx<a class="indexterm" id="id0658"/> (<a class="ulink" href="http://www.onyxplatform.org/">http://www.onyxplatform.org/</a>) projects are distributed, real-time stream processing systems implemented using Clojure.</p></div></div>
<div class="section" title="Applying back pressure"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec061"/>Applying back pressure</h1></div></div></div><p>We discussed <a class="indexterm" id="id0659"/>back pressure briefly in the last chapter. Without back pressure we cannot build a reasonable load-tolerant system with predictable stability and performance. In this section, we will see how to apply back pressure in different scenarios in an application. At a fundamental level, we should have a threshold of a maximum number of concurrent jobs in the system and, based on that threshold, we should reject new requests above a certain arrival rate. The rejected messages may either be retried by the client or ignored if there is no control over the client. When applying back pressure to user-facing services, it may be useful to detect system load and deny auxiliary services first in order to conserve capacity and degrade gracefully in the face of high load.</p><div class="section" title="Thread pool queues"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0121"/>Thread pool queues</h2></div></div></div><p>JVM thread pools are backed by<a class="indexterm" id="id0660"/> queues, which means that when we submit a job into a thread pool that already has the maximum jobs running, the new job lands in the queue. The queue is by default an unbounded queue, which is not suitable for applying back pressure. So, we have to create the thread pool backed by a bounded queue:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.LinkedBlockingDeque)
(import 'java.util.concurrent.TimeUnit)
(import 'java.util.concurrent.ThreadPoolExecutor)
(import 'java.util.concurrent.ThreadPoolExecutor$AbortPolicy)
(def tpool
  (let [q (LinkedBlockingDeque. 100)
        p (ThreadPoolExecutor$AbortPolicy.)]
    (ThreadPoolExecutor. 1 10 30 TimeUnit/SECONDS q p)))</pre></div><p>Now, on this pool, whenever there is an attempt to add more jobs than the capacity of the queue, it will throw an exception. The caller should treat the exception as a buffer-full condition and wait until the buffer has idle capacity again by periodically pooling the <code class="literal">java.util.concurrent.BlockingQueue.remainingCapacity()</code> method.</p></div><div class="section" title="Servlet containers such as Tomcat and Jetty"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0122"/>Servlet containers such as Tomcat and Jetty</h2></div></div></div><p>In the synchronous <a class="indexterm" id="id0661"/>
<span class="strong"><strong>Tomcat</strong></span> <a class="indexterm" id="id0662"/>and <a class="indexterm" id="id0663"/>
<span class="strong"><strong>Jetty</strong></span> <a class="indexterm" id="id0664"/>versions, each HTTP request is given a dedicated thread from a common thread pool that a user can configure. The number of simultaneous requests being served is limited by the thread pool size. A common way to control the arrival rate is to set the thread pool size of the server. The <span class="strong"><strong>Ring</strong></span> library<a class="indexterm" id="id0665"/> uses an embedded jetty server by default in development mode. The embedded Jetty adapter (in Ring) can be programmatically configured with a thread pool size.</p><p>In the asynchronous (Async Servlet 3.0) versions of Tomcat and Jetty beside the thread pool size, it is also possible to specify the timeout for processing each request. However, note that the thread pool size does not limit the number of requests in asynchronous versions in the way it does on synchronous versions. The request processing is transferred to an ExecutorService (thread pool), which may buffer requests until a thread is available. This buffering behavior is tricky because this may cause system overload—you can override the default behavior by defining your own thread pool instead of using the servlet container's thread pool to return a HTTP error at a certain threshold of waiting requests.</p></div><div class="section" title="HTTP Kit"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0123"/>HTTP Kit</h2></div></div></div><p>
<span class="strong"><strong>HTTP Kit</strong></span> (<a class="ulink" href="http://http-kit.org/">http://http-kit.org/</a>) is a<a class="indexterm" id="id0666"/> high-performance asynchronous (based on Java NIO implementation) web server for Clojure. It has built-in support for applying <a class="indexterm" id="id0667"/>back pressure to new requests via a specified queue length. As of HTTP Kit 2.1.19, see the following snippet:</p><div class="informalexample"><pre class="programlisting">(require '[org.httpkit.server :as hk])

;; handler is a typical Ring handler
(hk/run-server handler {:port 3000 :thread 32 :queue-size 600})</pre></div><p>In the previous snippet, the worker thread pool size is 32 and the max queue length is specified as 600. When not specified, 20480 is the default maximum queue length for applying back pressure.</p></div><div class="section" title="Aleph"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0124"/>Aleph</h2></div></div></div><p>Aleph (<a class="ulink" href="http://aleph.io/">http://aleph.io/</a>) is another <a class="indexterm" id="id0668"/>high-performance asynchronous web server based on the Java Netty<a class="indexterm" id="id0669"/> (<a class="ulink" href="http://netty.io/">http://netty.io/</a>) library, which in<a class="indexterm" id="id0670"/> turn is based on Java NIO. Aleph extends Netty with its own primitives compatible with Netty. The worker thread pool in Aleph is specified via an option, as we can see in the following snippet as of Aleph 0.4.0:</p><div class="informalexample"><pre class="programlisting">(require '[aleph.http :as a])

;; handler is a typical Ring handler
(a/start-server handler {:executor tpool})</pre></div><p>Here, <code class="literal">tpool</code> refers to a bounded thread pool as discussed in the subsection <span class="emphasis"><em>Thread pool queues</em></span>. By default, Aleph uses a dynamic thread pool capped at maximum 512 threads aimed at 90 percent system utilization via the <a class="indexterm" id="id0671"/>
<span class="strong"><strong>Dirigiste</strong></span> (<a class="ulink" href="https://github.com/ztellman/dirigiste">https://github.com/ztellman/dirigiste</a>) library.</p><p>Back pressure not only involves enqueuing a limited number of jobs, but slows down the processing rate of a job when the peer is slow. Aleph deals with per-request back pressure (for example, when streaming response data) by "not accepting data until it runs out of memory" — it falls back to blocking instead of dropping data, or raising exceptions and closing connections</p></div></div>
<div class="section" title="Performance and queueing theory"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec062"/>Performance and queueing theory</h1></div></div></div><p>If we observe the performance<a class="indexterm" id="id0672"/> benchmark numbers across a number of runs, even though <a class="indexterm" id="id0673"/>the hardware, loads and OS remain the same, the numbers are rarely exactly the same. The difference between each run may be as much as -8 percent to 8 percent for no apparent reason. This may seem surprising, but the deep-rooted reason is that the performances of computer systems are <span class="emphasis"><em>stochastic</em></span> by nature. There are many small factors in a computer system that make performance unpredictable at any given point of time. At best, the performance variations can be explained by a series of probabilities over random variables.</p><p>The basic premise is that each subsystem is more or less like a queue where requests await their turn to be served. The CPU has an instruction queue with unpredictable fetch/decode/branch-predict timings, the memory access again depends on cache hit ratio and whether it needs to be dispatched via the interconnect, and the I/O subsystem works using interrupts that may again depend on mechanical factors of the I/O device. The OS schedules threads that wait while not executing. The software built on the top of all this basically waits in various queues to get the job done.</p><div class="section" title="Little's law"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec0125"/>Little's law</h2></div></div></div><p>Little's law<a class="indexterm" id="id0674"/> states that, over steady state, the following holds true:</p><div class="mediaobject"><img alt="Little's law" src="graphics/3642_08_01.jpg"/></div><div class="mediaobject"><img alt="Little's law" src="graphics/3642_08_02.jpg"/></div><p>This is a rather important law that gives us insight into the system capacity as it is independent of other factors. For an example, if the average time to satisfy a request is 200 ms and the service rate is about 70 per second, then the mean number of requests being served is <span class="emphasis"><em>70 req/second x 0.2 second = 14 requests</em></span>.</p><p>Note that Little's law<a class="indexterm" id="id0675"/> does not talk about spikes in request arrival rate or spikes in latency (due to GC and/or other bottlenecks) or system behavior in response to these factors. When the arrival rate spikes at one point, your system must have enough resources to handle the number of concurrent tasks required to serve the requests. We can infer here that Little's law is helpful to measure and tune average system behavior over a duration, but we cannot plan capacity based solely on this.</p><div class="section" title="Performance tuning with respect to Little's law"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec050"/>Performance tuning with respect to Little's law</h3></div></div></div><p>In order to maintain <a class="indexterm" id="id0676"/>good throughput, we should strive to maintain an upper <a class="indexterm" id="id0677"/>limit on the total number of tasks in the system. Since there can be many kinds of tasks in a system and lot of tasks can happily co-exist in the absence of bottlenecks, a better way to say it is to ensure that the system utilization and bottlenecks remain in limit.</p><p>Often, the arrival rate may not be within the control of a system. For such scenarios, the only option is to minimize the latency as much as possible and deny new requests after a certain threshold of total jobs in the system. You may be able to know the right threshold only through performance and load tests. If you can control the arrival rate, you can throttle the arrival (based on performance and load tests) so as to maintain a steady flow.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec063"/>Summary</h1></div></div></div><p>Designing an application for performance should be based on the use cases and patterns of anticipated system load and behavior. Measuring performance is extremely important to guide optimization in the process. Fortunately, there are several well-known optimization patterns to tap into, such as resource pooling, data sizing, pre-fetch and pre-compute, staging, batching, and so on. As it turns out, application performance is not only a function of the use cases and patterns—the system as a whole is a continuous stochastic turn of events that can be assessed statistically and is guided by probability. Clojure is a fun language to do high-performance programming. This book prescribes many pointers and practices for performance, but there is no mantra that can solve everything. The devil is in the details. Know the idioms and patterns, experiment to see what works for your applications, and know which rules you can bend for performance.</p></div></body></html>