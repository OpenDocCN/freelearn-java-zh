- en: Chapter 6. Efficient Sorting – quicksort and mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we explored a few simple sorting algorithms. The problem
    with those is that they are not efficient enough. In this chapter, we will cover
    two efficient sorting algorithms and we will also see how they are efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: quicksort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mergesort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimality of efficiency in sorting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quicksort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We want to develop an algorithm that sorts an array of elements efficiently.
    Our strategy will be simple; we will somehow try to divide the array into two
    halves in such a way that sorting each half will complete the sorting. If we can
    achieve this, we can recursively call the sorting algorithm this way. We already
    know that the number of levels of recursive call will be of the order of *lg n*
    where *lg m* is the logarithm of *m* with the base *2*. So, if we can manage to
    cut the array in a time in the order of *n*, we will still have a complexity of
    *O(n lg n)* This is much better than *O(n* ²*)*, which we saw in the previous
    chapter. But how do we cut the array that way? Let''s try to cut the following
    array as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we trivially cut this array, each part will contain all sorts of values.
    Sorting these individual parts would then not cause the entire array to be sorted.
    Instead, we have to cut the array in such a way that all the elements in the left
    part are less than all the elements in the right part. If we can achieve this,
    sorting the parts will achieve sorting the whole. But, of course, we need to make
    some swaps for us to be able to cut the array in such a way. So, we use the following
    trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '![quicksort](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is an example of pivot positioning in quicksort. The one headed arrows
    represent comparison and double headed arrows represent swap.
  prefs: []
  type: TYPE_NORMAL
- en: We first choose the last element and call it *pivot*. Our aim is to make all
    the elements that are less than the pivot to be on its left and all the elements
    that are greater than the pivot to be on its right. Note that this means that
    the pivot is already at the correct position of the array. The preceding figure
    shows how to do it with an example. In the figure, our pivot is **30**. We start
    comparing with the first element and move on. The moment we find an inversion,
    we swap the pivot with that element and keep comparing in the opposite direction.
    We continue this way, swapping every time there is an inversion and reversing
    the direction of comparison each time. We stop when all the elements have been
    compared. Note that after this process, all the elements less than the pivot are
    on its left, and all the elements greater than the pivot are on its right. Why
    does this work? Let's take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we start comparing from the left. After the first swap, the pivot is
    at the position of the element that was found to be greater than it. All the elements
    that were on the left of it have already been compared with the pivot and found
    to be less than it. The previous figure shows the parts that have already been
    compared. Now it starts comparing from its earlier position in the opposite direction.
    After the second swap, it sits in the position of the element that it has been
    swapped with. All the elements on the right have already been compared with it
    and found to be greater than it. We keep going in the same way, always ensuring
    that the parts that have already been compared follow the rule that the part on
    the left of the pivot contains only elements that are less than or equal to it
    and the part on the right of the pivot contains only elements that are greater
    than or equal to it. So, when we are done with the comparisons, we still hold
    this condition, thus achieving the result. Once we know that all the elements
    on the left of the pivot are less than or equal to all the elements that are on
    its right, we can sort these parts separately and the entire array will be sorted
    as a result. Of course, we sort these parts recursively in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before we dive into the code, I would like to introduce a different
    interface for comparison. It is called `java.util.Comparator` and allows us to
    specify any comparison logic while sorting, thus providing more flexibility. Here
    is how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is, of course, a very simplified version of the actual interface but has
    got all that we care about. As you can see, this is a functional interface and
    thus can be implemented using a lambda. It should return the same value that is
    conceptually similar to the value returned by `o1.compareTo(o2)`, but a different
    sorting can use a different comparison lambda. The compare method has to follow
    the same rules that are there for the `compareTo` method in the `java.util.Comparable`
    interface we studied in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's jump into the code for quicksort. We know we don't have to sort any
    more when the array to be processed is empty, which would be our base case. Otherwise,
    We create two indexes `i` and `j`, one storing the current end of the left part
    and the other storing the current beginning of the right part that has already
    been compared with the pivot at any given point of time, while the pivot is being
    placed in its correct position. Both the index variables store the indexes that
    are to be compared next. At any given point of time, one of these variables holds
    the position of the pivot and the other stores the current value being compared
    with it. The variable that currently stores the position of the pivot is flagged
    by the Boolean variable, `movingI`. If it is true, it means that we are currently
    moving `i` and hence, `j` is pointing to the pivot. We update the position variables
    and keep comparing, in a loop, until both indexes point to the pivot, when the
    comparison suggests that there is an inversion, we swap and reverse the direction
    of movement. We reverse the direction of movement because the pivot has moved
    to the position indexed by the opposite variable, marked by `movingI` switching
    its value. Otherwise, we just keep updating the appropriate position variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'When `movingI` is false, it means that `i` is storing the position of the pivot.
    And finally, when the pivot is at the correct position and all the elements on
    its left are less than or equal to all the elements on its right, we recursively
    call **quicksort** on each part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap this method to avoid having to pass the start and end parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this method to sort an array. Let''s see how to sort an integer
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following would be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note how we passed the simple comparator using a lambda. If we pass a lambda
    `(a,b)->b-a` instead, we will get the array reversed. In fact, this flexibility
    lets us sort arrays containing complex objects according to any comparison we
    like. For example, it is easy to sort an array of `Person` objects by age using
    the lambda, `(p1, p2)->p1.getAge() - p2.getAge()`.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of quicksort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like always, we will try to figure out the worst case of quicksort. To begin
    with, we notice that after the pivot has been positioned correctly, it is not
    positioned in the middle of the array. In fact, its final position depends on
    what value it has with respect to the other elements of the array. Since it is
    always positioned as per its rank, its rank determines the final position. We
    also notice that the worst case for quicksort would be when the pivot does not
    cut the array at all, that is, when all the other elements are either to its left
    or to its right. This will happen when the pivot is the largest or the smallest
    element. This will happen when the highest or the lowest element is at the end
    of the array. So, for example, if the array is already sorted, the highest element
    would be at the end of the array in every step, and we will choose this element
    as our pivot. This gives us the counter intuitive conclusion that an array that
    is already sorted would be the worst case for the quicksort algorithm. An array
    that is sorted in the opposite direction is also one of the worst cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is the complexity if the worst case happens? Since it is the worst
    case where every step is made out of two recursive calls, one of which is with
    an empty array and thus needing a constant time to process, and another having
    an array with one less element. Also, in each step, the pivot is compared with
    every other element, thus taking time proportional to *(n-1)* for an *n*-element
    step. So, we have the recursive equation for the time `T(n)` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is valid for all values of `n`, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Summing both sides, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is not very good. It is still `O(n²` `).` Is it really an efficient algorithm?
    Well, to answer that, we need to consider the average case. The average case is
    the probabilistically weighted average of the complexities for all possible inputs.
    This is quite complicated. So, we will use something that we can call a typical
    case, which is sort of the complexity of the usual case. So, what would happen
    in a typical randomly unsorted array, that is, where the input array is arranged
    quite randomly? The rank of the pivot will be equally likely to be any value from
    `1` to `n`, where `n` is the length of the array. So, it will sort of split the
    array near the middle in general. So, what is the complexity if we do manage to
    cut the array in halves? Let''s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a little difficult to solve, so we take `n/2` instead of `(n-1)/2`,
    which can only increase the estimate of complexity. So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let `m = lg n` and `S(m) = T(n)`, and hence, `n = 2m`. So, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is valid for all `m`, we can apply the same formula for `S(m-1)`
    as well. So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Proceeding similarly, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is pretty good. In fact, this is way better than the quadratic complexity
    we saw in the previous chapter. In fact, *n lg n* grows so slow that *n lg n =
    O(na)* for any `a` greater than *1*. That is to say that the function *n1.000000001*
    grows faster than *n lg n*. So, we have found an algorithm that performs quite
    well in most cases. Remember that the worst case for quicksort is still *O(n²)*.
    We will try to address this problem in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Random pivot selection in quicksort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The problem with quicksort is that it performs really badly if the array is
    already sorted or sorted in the reverse direction. This is because we would be
    always choosing the pivot to be the smallest or the largest element of the array.
    If we can avoid that, we can avoid the worst case time as well. Ideally, we want
    to select the pivot that is the median of all the elements of the array, that
    is, the middle element when the array is sorted. But it is not possible to compute
    the median efficiently enough. One trick is to choose an element randomly among
    all the elements and use it as a pivot. So, in each step, we randomly choose an
    element and swap it with the end element. After this, we can perform the quicksort
    as we did earlier. So, we update the quicksort method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Even now we can be very unlucky and pick the end element every time, but it
    is very unlikely to happen. In this case, we will almost always get an *n lg n*
    complexity, as desired.
  prefs: []
  type: TYPE_NORMAL
- en: mergesort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we tried to divide the array in such a way that when
    we sort each part, the entire array is sorted. We faced the problem that when
    we try to do that, the two parts are not equal in size causing the algorithm to
    sometimes take a quadratic amount of time. What if, instead of trying to divide
    the array in a way that sorting the parts would sort the whole, we just divide
    the array into two equal halves? Of course then, sorting the parts will not sort
    the entire array. However, if we have two array parts sorted on their own, can
    we merge them together to produce a sorted array as a whole? If we can do this
    efficiently enough, we would have an algorithm guaranteed to be efficient. As
    it turns out, it is possible. But we need to think about where the merged array
    will be stored. Since the values are being copied from the source array, the result
    needs to be stored in a separate place. So, we will need another storage of equal
    size for mergesort.
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '![mergesort](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Merge of sorted arrays
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows a part of the merging operation. We keep the current
    position of each of the arrays. In each step, we compare the values in the current
    positions in both the input sorted arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'We copy whichever of them is smaller to the target location and increment the
    corresponding current position. We keep doing this until we finish one of the
    arrays, after which the elements of the other array can just be copied over. The
    following shows the code for the merge operation. One thing to note is that, since
    the merge will be used for a mergesort, it presumes both the input arrays to be
    the same array with different indexes and the target arrays to have the same size.
    The source has three indexes: `start`, `mid`, and `end`. It is assumed that the
    source parts are residing side by side in the source array. The variable `start`
    points to the start of the first part. The integer `mid` stores the index of the
    start of the second part and also acts as the end of the first part, as the parts
    are contiguous. Finally, the `end` variable stores the end of the second array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The first two cases are for the time when one of the source arrays has been
    exhausted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If none of the arrays are exhausted, copy from the correct array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the target location must also be incremented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With this `merge` function available to us, we can now proceed to do the mergesort.
    It involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the array into two equal parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mergesort the parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the sorted parts into a full sorted array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Of course, we do not need to do anything for an array with zero or one element.
    So, that will be our exit case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Just return to the calling function for an array of zero or one element. This
    is our base case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For any array of size bigger than `1`, divide the array into two halves–start
    to mid and mid to end. Then merge-sort them separately, and then merge the two
    sorted subarrays to a combined sorted array in `tempArray`, which is an auxiliary
    space that we are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, copy the contents of `tempArray` to `sourceArray` so that the source
    is now sorted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The complexity of mergesort
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start from the complexity of the merge operation. The merge operation
    is not recursive. In every step, it increments either `i` or `j`. When both these
    variables reach the end of the respective arrays, the merge ends. The comparison
    happens at most once per any of these increments. This means that there are at
    most as many comparisons as there are elements in both the sub-arrays combined.
    The copying of the contents of `tempArray` to `sourceArray` also, of course, takes
    operations proportional to the number of elements in `tempArray`, which is the
    same as the number of elements in the `sourceArray`. So, the number of operations
    in each step is proportional to *n*, apart from the recursive call. The recursive
    call works on both parts of the array, which are themselves half the size of the
    entire array. Thus, if *T(n)* is the time taken, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, `a` and `b` are constants.
  prefs: []
  type: TYPE_NORMAL
- en: This is the same equation as the one obtained for the typical case of the quicksort
    algorithm, and we know that the solution gives us *T(n) = θ(n lg n)*. This is
    the estimate for the both the average case and the worst case because, in both
    cases, the array will always be divided into two equal halves irrespective of
    the contents of the array. In fact, the worst case is when all the copying also
    requires a comparison, which is the case we considered.
  prefs: []
  type: TYPE_NORMAL
- en: In the best case, one of the arrays will have all its elements copied before
    even the first element of the second array is copied, thus requiring only half
    as many comparisons. But this case gives the same complexity of *T(n) = θ(n lg
    n)*. So, irrespective of the actual contents of the array we started with, mergesort
    will always have the same asymptotic complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the copying of tempArray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our rather simplistic example, we first merged the subarrays into `tempArray`,
    and then we copied it back to `sourceArray`. Can the copying be avoided? Can we
    use `tempArray` itself as the result of the merge? It turns out that we can. In
    this case, both `sourceArray` and `tempArray` would be used rather symmetrically,
    the only difference being that `sourceArray` holds the original input array. Otherwise,
    they are two pre-allocated arrays of the same size. However, the code will get
    a little more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Let us first consider what would happen if we do not copy the contents of `tempArray`
    to `sourceArray` and try to use `tempArray` itself as the content of the sorted
    array. Then, in each step, `sourceArray` and `tempArray` would need to be swapped,
    that is, `tempArray` would become `sourceArray` and vice versa. Since in each
    step, `tempArray` and `sourceArray` are getting swapped, the actual array that
    holds the result depends on whether the number of steps required to sort the array
    is odd or even.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if the array we started with had a number of elements equal to an exact
    integral power of 2, the source array could always be divided into two sub-arrays
    of the exact same size. This means, the number of steps required to sort each
    of these sub-arrays would be exactly the same. This means that the actual array
    that holds the sorted result would be the same after sorting either sub-array.
    However, in reality, the number of elements in the array is not an exact power
    of 2 most of the time, and hence, one sub-array is a little bigger than the other.
    This results in a different number of steps being required to sort either sub-array,
    causing them to potentially store the resultant sorted array in different arrays.
    We have to consider these cases as well. So, when the result of sorting either
    sub-array is stored in the same array, we store the output of the merge in the
    other array. If not, we always store the output of the merge in the array that
    holds the result of sorting the second part of the array.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we change the `merge` function to handle two different arrays holding
    the contents of two different inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With this `merge` function available, we write our efficient mergesort in the
    following way. Note that we need some way to inform the calling function about
    which pre-allocated array contains the result, so we return that array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'First, split and merge-sort the sub-arrays as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If both the sorted sub-arrays are stored in the same pre-allocated array, use
    the other pre-allocated array to store the result of the merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we store the result in `sortedPart2` because it has the first
    portion empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use this mergesort as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that this time, we had to ensure that we use the output returned by the
    method as, in some cases, `anotherArray` may contain the final sorted values.
    The efficient no-copy version of the mergesort does not have any asymptotic performance
    improvement, but it improves the time by a constant. This is something worth doing.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of any comparison-based sorting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen two algorithms for sorting that are more efficient than
    the ones described in the previous chapter, how do we know that they are as efficient
    as a sorting can be? Can we make algorithms that are even faster? We will see
    in this section that we have reached our asymptotic limit of efficiency, that
    is, a comparison-based sorting will have a minimum time complexity of *θ(m lg
    m)*, where *m* is the number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we start with an array of *m* elements. For the time being, let's assume
    they are all distinct. After all, if such an array is a possible input, we need
    to consider this case as well. The number of different arrangements possible with
    these elements is *m!*. One of these arrangements is the correct sorted one. Any
    algorithm that will sort this array using comparison will have to be able to distinguish
    this particular arrangement from all others using only comparison between pairs
    of elements. Any comparison divides the arrangements into two sets–one that causes
    an inversion as per the comparison between those two exact values and one that
    does not. This is to say that given any two values *a* and *b* from the arrays,
    a comparison that returns *a<b* will divide the set of arrangements into two partitions;
    the first set will contain all the arrangements where *b* comes before *a*, and
    the second set will contain all the arrangements where *a* comes before *b*. The
    sorted arrangement is, of course, a member of the second set. Any algorithm that
    sorts based on comparisons will have to do enough of them to pin down on the single
    correct arrangement, that is, the sorted arrangement. Basically, it will first
    perform a comparison, choose the correct subset, then perform another comparison
    and choose the correct subset of the subset, and so on, until it reaches a set
    of arrangements containing just one arrangement. This particular arrangement is
    the sorted version of the array. What is the minimum number of comparisons that
    are required to find one particular arrangement out of *m!* arrangements? This
    is the same as asking how many times you have to halve a set of *m!* elements
    to reach a set of only one element. It is, of course, *lg (m!)*. This is a rough
    estimation; in fact, the number of comparisons required would be a bit more than
    this because the two subsets that each comparison creates may not be equal in
    size. But we know that the number of comparisons required is *lg (m!)* at the
    minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how much is *lg m!*? Well, it is *(ln (m!)) (lg e)*, where *ln (x)* is
    the natural logarithm of *x*. We will find a simpler asymptotic complexity for
    the function *ln(m!)*. It requires a little bit of calculus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Area under the curve y = ln x.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram shows some plots. We know that the integral measures the area under
    the curve of a function. Now, the area under the curve *y=ln b* between *a* and
    *b* is *(b-a)ln b*, and the area under the curve *y=ln a* is *(b-a) ln a*. The
    area under the curve *y=lg x* in the same interval is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the graph in the preceding figure, the following is clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In particular, having *b=a+1*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we set *a = 1* and move up to *a = m-1*, to get the following set of inequalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Adding respective sides, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, of course we know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have the following inequalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the left inequality, if we put m instead of *m-1*, we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This, combined with the right inequality, gives the following relations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complexity of any comparison-based sorting](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This gives a pretty good upper bound and lower bound on the value of *ln(m!)*.
    Both the upper bound and lower bound are *θ(m ln m)*. So, we can conclude that
    *ln(m!) = θ(m ln m)*. This means that *lg(m!) = (ln (m!))(lg e)* is also *θ(m
    ln m) = θ(m lg m)* because *lg(m) = (ln m )(lg e)*.
  prefs: []
  type: TYPE_NORMAL
- en: So, the minimum time complexity of a comparison-based sorting algorithm would
    have to be at least *θ(m lg m)* just because of the minimum number of comparisons
    that would be needed to do this. Therefore, mergesort and the typical case of
    quicksort are asymptotically optimal.
  prefs: []
  type: TYPE_NORMAL
- en: The stability of a sorting algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stability of a sorting algorithm is the property that the elements that
    compare to be equal preserve their original order after sorting. For example,
    if we have an array of objects containing the ID number and the age of some people
    and we want to sort them in increasing order of age, a stable sorting algorithm
    will preserve the original order of the people with the same age. This can be
    helpful if we are trying to sort multiple times. For example, if we want the IDs
    to be in increasing order for people with the same age, we can first sort the
    array by ID and then sort it again by age. If the sorting algorithm is stable,
    it will ensure that the final sorted array is in increasing order of age, and
    for the same age, it is in increasing order of ID. Of course, this effect can
    also be achieved by having a more complex comparison with a single sorting operation.
    Quicksort is not stable, but mergesort is. It is easy to see why mergesort is
    stable. During merging, we preserve the order, that is, values from the left half
    precede values from the right half when they compare as equal.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored two efficient sorting algorithms. The basic principle,
    in both cases, was to divide the array and to sort the parts separately. If we
    ensure that sorting the parts will cause the entire array to be sorted by readjusting
    the elements, it is quicksort. If we just divide the array into two equal parts
    first and–after sorting each part–merge the results to cause the entire array
    to be sorted, it is a mergesort. This way of dividing the input into smaller parts,
    solving the problem for the smaller parts and then combining the results to find
    the solution for the entire problem is a common pattern in solving computational
    problems, and it is called the divide and conquer pattern.
  prefs: []
  type: TYPE_NORMAL
- en: We have also seen an asymptotic lower bound for any sorting algorithm that works
    using comparisons. Both quicksort and mergesort achieve this lower bound and hence,
    are asymptotically optimal. In the next chapter, we will move to a different kind
    of data structures called trees.
  prefs: []
  type: TYPE_NORMAL
