<html><head></head><body>
		<div id="_idContainer103">
			<h1 id="_idParaDest-155"><em class="italic"><a id="_idTextAnchor155"/>Chapter 9</em>: Distributed Logging, Tracing, and Monitoring</h1>
			<p>A microservice application often runs multiple microservices on a varied range of multiple hosts. For upstream consumers, the API gateway provides a one-stop-shop interface to access all the application endpoints. Any request to the API gateway is dispersed to one or more microservices. This distributed diffusion of request handling escalates challenges in maintaining microservices-based applications. If any anomaly or error occurs, it is hard to dig which microservice or distributed component is at fault. In addition, any effective microservices implementation must handle the maintenance challenges proactively.  </p>
			<p>In this chapter, we will explore the following topics:</p>
			<ul>
				<li><strong class="bold">Distributed logging</strong>: How we can implement the log aggregation for distributed microservices so that application logs can be accessed and indexed in one place?</li>
				<li><strong class="bold">Distributed tracing</strong>: How we can trace the execution of a user request that could be dispersed onto multiple microservices running on multiple host environments?</li>
				<li><strong class="bold">Distributed monitoring</strong>: How we can continuously monitor the key performance indicators for all the service components to get a holistic picture of the system's health?</li>
			</ul>
			<p>By collecting these three different kinds of data – logging, tracing, and monitoring – we enhance the system observability. By accessing this telemetry data at any point in time, we can intuitively and precisely get a complete context of what and how a request was executed in the system. To learn more about observability, we will explore the microservices patterns on distributed logging, tracing, and monitoring with the hands-on <strong class="source-inline">pet-clinic</strong> application. </p>
			<p>By the end of this chapter, you will have good knowledge of implementing these observability patterns in the Micronaut framework<a id="_idTextAnchor156"/>.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor157"/>Technical requirements</h1>
			<p>All the commands and technical instructions in this chapter are run on Windows 10 and macOS. Code examples covered in this chapter are available in the book's GitHub repository at <a href="https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09">https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09</a>.</p>
			<p>The following tools need to be installed and set up in the development environment:</p>
			<ul>
				<li><strong class="bold">Java SDK</strong>: Version 13 or above (we used Java 14).</li>
				<li><strong class="bold">Maven</strong>: This is optional and only required if you would like to use Maven as the build system. However, we recommend having Maven set up on any development machine. Instructions to download and install Maven can be found at <a href="https://maven.apache.org/download.cgi">https://maven.apache.org/download.cgi</a>.</li>
				<li><strong class="bold">Development IDE</strong>: Based on your preferences, any Java-based IDE can be used, but for the purpose of writing this chapter, IntelliJ was used. </li>
				<li><strong class="bold">Git</strong>: Instructions to download and install Git can be found at <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a>. </li>
				<li><strong class="bold">PostgreSQL</strong>: Instructions to download and install PostgreSQL can be found at <a href="https://www.postgresql.org/download/">https://www.postgresql.org/download/</a>.</li>
				<li><strong class="bold">MongoDB</strong>: MongoDB Atlas provides a free online database-as-a-service with up to 512 MB storage. However, if a local database is preferred, then instructions to download and install can be found at <a href="https://docs.mongodb.com/manual/administration/install-community/">https://docs.mongodb.com/manual/administration/install-community/</a>. We used a local installation for writing this chapter.</li>
				<li><strong class="bold">REST client</strong>: Any HTTP REST client can be used. We used the Advanced REST Client Chrome plugin.</li>
				<li><strong class="bold">Docker</strong>: Instructions to download and install Docker can be found at https://docs.docker.com/get-docker/.</li>
			</ul>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor158"/>Distributed logging in Micronaut microservices</h1>
			<p>As we<a id="_idIndexMarker668"/> discussed in the chapter introduction, in <a id="_idIndexMarker669"/>a microservices-based application, a user request is executed on multiple microservices running on different host environments. Therefore, the log messages are spread across multiple host machines. This brings a unique challenge to a developer or admin maintaining the application. If there's a failure, then it will be hard to zero down on the issue as you have to sign into multiple host machines/environments, grep the logs, and put them together to make sense. </p>
			<p>In this section, we will dive into log aggregation for distributed logging in microservices. </p>
			<p>Log aggregation, as the name suggests, is combining the logs produced by various microservices and components in the application. Log aggregation typically involves the following components:</p>
			<ul>
				<li><strong class="bold">Log producer</strong>: This is<a id="_idIndexMarker670"/> any microservice or a distributed component that's producing logs while executing the control flow. </li>
				<li><strong class="bold">Log dispatcher</strong>: The log dispatcher is responsible for collecting the logs produced by the log producer and dispatching them to the centralized storage.</li>
				<li><strong class="bold">Log storage</strong>: The log storage persists and indexes the logs produced by all the application components and microservices.</li>
				<li><strong class="bold">Log visualizer</strong>: The log visualizer provides a user interface for accessing, searching, and filtering the logs stored in the log storage. </li>
			</ul>
			<p>In the <strong class="source-inline">pet-clinic</strong> application<a id="_idIndexMarker671"/> context, we will implement the <strong class="bold">ELK</strong> (short for <strong class="bold">Elasticsearch</strong>, <strong class="bold">Logstash</strong>, <strong class="bold">Kibana</strong>) Stack for distributed logging. Refer to the following figure:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/Figure_9.1_B16585_Fixed_edited.jpg" alt="Figure 9.1 – Distributed logging using the ELK Stack&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Distributed logging using the ELK Stack</p>
			<p>In the<a id="_idIndexMarker672"/> preceding<a id="_idIndexMarker673"/> diagram, the ELK Stack is used to implement distributed logging in the <strong class="source-inline">pet-clinic</strong> application. <strong class="bold">Logstash</strong> dispatches the logs into the <strong class="bold">Elasticsearch</strong> engine, which is then used by <strong class="bold">Kibana</strong> to provide a user interface. </p>
			<p>In the next section, we will explore how we can set up an ELK Stack in a dockerized container.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor159"/>Setting up ELK in Docker</h2>
			<p>To set u<a id="_idIndexMarker674"/>p ELK in Docker, follow<a id="_idIndexMarker675"/> these instructions:</p>
			<ol>
				<li>Check out <strong class="source-inline">docker-elk</strong> from <a href="https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro">https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-elk</a>.</li>
				<li>Open any Bash terminal (we used Git Bash).</li>
				<li>Change directory to where you have checked out <strong class="source-inline">docker-elk</strong>. </li>
				<li>Run the <strong class="source-inline">docker compose up –d</strong> command.</li>
				<li>Wait for Docker to download the images and instantiate the ELK container.</li>
			</ol>
			<p>The preceding instructions will boot up an ELK app in Docker. You can verify the installation by going to the Docker Dashboard | <strong class="bold">Containers / Apps</strong>, as shown here:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/Figure_9.2_B16585_Fixed.jpg" alt="Figure 9.2 – Verifying ELK in the Docker Dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Verifying ELK in the Docker Dashboard</p>
			<p>Here, you <a id="_idIndexMarker676"/>can verify ELK <a id="_idIndexMarker677"/>instantiation. By default, Elasticsearch runs on port <strong class="source-inline">9200</strong>, Logstash on <strong class="source-inline">5000</strong>, and Kibana on port <strong class="source-inline">5601</strong>. </p>
			<p>In the next section, we will modify our <strong class="source-inline">pet-clinic</strong> microservice to dispatch the logs to the Logstash instance. </p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor160"/>Integrating Logstash with Micronaut microservices</h2>
			<p>To <a id="_idIndexMarker678"/>integrate <a id="_idIndexMarker679"/>Logstash into the <strong class="source-inline">pet-clinic</strong> microservice, we will leverage Logback. We will introduce a new appender to Logback that can dispatch the logs to the previously created Logstash instance. </p>
			<p>In the locally<a id="_idIndexMarker680"/> checked out <strong class="source-inline">docker-elk</strong> directory, you <a id="_idIndexMarker681"/>can verify that Logstash is configured with the following settings:</p>
			<p class="source-code">input {</p>
			<p class="source-code">    tcp {</p>
			<p class="source-code">        port =&gt; 5000</p>
			<p class="source-code">        type =&gt; syslog</p>
			<p class="source-code">        codec =&gt; json_lines</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p class="source-code">filter {</p>
			<p class="source-code">    grok {</p>
			<p class="source-code">        match =&gt; [ "message", "%{GREEDYDATA}" ]</p>
			<p class="source-code">    }</p>
			<p class="source-code">    mutate {</p>
			<p class="source-code">        add_field =&gt; { "instance_name" =&gt; "%{app_name}-</p>
			<p class="source-code">         %{host}:%{app_port}" }</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p class="source-code">output {</p>
			<p class="source-code">    stdout { # This will log all messages so that we can </p>
			<p class="source-code">     confirm that Logstash is receiving them</p>
			<p class="source-code">        codec =&gt; rubydebug</p>
			<p class="source-code">    }</p>
			<p class="source-code">    elasticsearch {</p>
			<p class="source-code">        hosts =&gt; [</p>
			<p class="source-code">         "${XPACK_MONITORING_ELASTICSEARCH_HOSTS}" ]</p>
			<p class="source-code">        user =&gt; </p>
			<p class="source-code">         "${XPACK_MONITORING_ELASTICSEARCH_USERNAME}"</p>
			<p class="source-code">        password =&gt; </p>
			<p class="source-code">          "${XPACK_MONITORING_ELASTICSEARCH_PASSWORD}"</p>
			<p class="source-code">        index =&gt; "logstash-%{+YYYY.MM.dd}"</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>In <strong class="source-inline">logstash.config</strong>, we have the following three sections:</p>
			<ul>
				<li><strong class="source-inline">input</strong>: Logstash<a id="_idIndexMarker682"/> has the power<a id="_idIndexMarker683"/> to aggregate more than 50 different kinds of log sources. <strong class="source-inline">input</strong> configures Logstash for one or more input sources. In our configuration, we are enabling <strong class="source-inline">tcp</strong> input on port <strong class="source-inline">5000</strong>.</li>
				<li><strong class="source-inline">filter</strong>: Logstash's <strong class="source-inline">filter</strong> provides an easy way to transform the incoming logs into filter-defined log events. These events are then pushed to log storage. In the preceding configuration, we are using a <strong class="source-inline">grok</strong> filter along with <strong class="source-inline">mutate</strong> to add extra information (<strong class="source-inline">app_name</strong> and <strong class="source-inline">app_port</strong>) to the log events.</li>
				<li><strong class="source-inline">output</strong>: The <strong class="source-inline">output</strong> section configures the receiving sources so that Logstash can push the log events to the configured output sources. In the preceding configuration, we are configuring the standard output and Elasticsearch to receive the produced log events. </li>
			</ul>
			<p>So far, we have booted up an ELK Docker instance with Logstash configured to receive, transform, and send log events into Elasticsearch. Next, we will make required amends in the <strong class="source-inline">pet-clinic</strong> microservices so that logs can be shipped to Logstash.</p>
			<h3>Configuring microservices for distributed logging </h3>
			<p>In order<a id="_idIndexMarker684"/> to make the <strong class="source-inline">pet-clinic</strong> microservice<a id="_idIndexMarker685"/> aggregate and ship logs to Logstash, we need to add the following <strong class="source-inline">logstash-logback-encoder</strong> dependency to all the microservice <strong class="source-inline">pom.xml</strong> files:</p>
			<p class="source-code">&lt;dependency&gt;</p>
			<p class="source-code">  &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt;</p>
			<p class="source-code">  &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt;</p>
			<p class="source-code">  &lt;version&gt;6.3&lt;/version&gt;</p>
			<p class="source-code">&lt;/dependency&gt;</p>
			<p>By importing <strong class="source-inline">logstash-logback-encoder</strong>, we can leverage the <strong class="source-inline">net.logstash.logback.appender.LogstashTcpSocketAppender</strong> class in <strong class="source-inline">logback.xml</strong>. This class provides the <strong class="source-inline">logstash</strong> appender, which can ship logs to the Logstash server from the microservice. </p>
			<p>Modify <strong class="source-inline">logback.xml</strong> for all microservices by adding the Logstash appender as follows:</p>
			<p class="source-code">&lt;appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt;</p>
			<p class="source-code">    &lt;param name="Encoding" value="UTF-8"/&gt;</p>
			<p class="source-code">    &lt;remoteHost&gt;host.docker.internal&lt;/remoteHost&gt;</p>
			<p class="source-code">    &lt;port&gt;5000&lt;/port&gt;</p>
			<p class="source-code">    &lt;encoder </p>
			<p class="source-code">     class=»net.logstash.logback.encoder.LogstashEncoder»/&gt;</p>
			<p class="source-code">&lt;/appender&gt;</p>
			<p class="source-code">…</p>
			<p class="source-code">&lt;root level="debug"&gt;</p>
			<p class="source-code">    &lt;appender-ref ref="logstash"/&gt;</p>
			<p class="source-code">    &lt;appender-ref ref="stdout"/&gt;</p>
			<p class="source-code">&lt;/root&gt;</p>
			<p>The Logstash appender will help in shipping the logs to <strong class="source-inline">localhost:5000</strong> and as we are running Logstash in a Docker container, we provide the address as <strong class="source-inline">host.docker.internal</strong>. </p>
			<p>Also, we need to add the appender to the root level by using <strong class="source-inline">appender-ref</strong>.</p>
			<p>Furthermore, we need to define two properties for <strong class="source-inline">app_name</strong> and <strong class="source-inline">app_port</strong>. These are the filter configurations that will be used by Logstash to create the desired log events with app information. This is how we do it:</p>
			<p class="source-code">&lt;property scope="context" name="app_name" value="pet-owner "/&gt;</p>
			<p class="source-code">&lt;property scope="context" name="app_port" value="32581"/&gt;</p>
			<p>In the <a id="_idIndexMarker686"/>preceding<a id="_idIndexMarker687"/> code snippet, we have added the required properties for the <strong class="source-inline">pet-owner</strong> microservice. We need to add similar properties in all the services so Logstash can generate service-specific log events. </p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor161"/>Verifying the distributed logging in the pet-clinic application</h2>
			<p>To verify<a id="_idIndexMarker688"/> that Logstash is receiving <a id="_idIndexMarker689"/>the logs from all the microservices in the <strong class="source-inline">pet-clinic</strong> application, we would need to re-build the Docker images and redeploy the <strong class="source-inline">pet-clinic</strong> application. Perform the following steps:</p>
			<ol>
				<li value="1">Open the terminal in the <strong class="source-inline">pet-owner</strong> microservice root directory:<p>a. Run the <strong class="source-inline">jib</strong> command to build Docker <strong class="source-inline">mvn compile jib:dockerBuild</strong>.</p><p>b. Wait for <strong class="source-inline">jib</strong> to build and upload the Docker image to the local Docker images repository. </p></li>
				<li>Open the terminal in the <strong class="source-inline">pet-clinic</strong> microservice root directory:<p>a. Run the <strong class="source-inline">jib</strong> command to build Docker <strong class="source-inline">mvn compile jib:dockerBuild</strong>.</p><p>b. Wait for <strong class="source-inline">jib</strong> to build and upload the Docker image to the local Docker images repository.</p></li>
				<li>Open the terminal in the <strong class="source-inline">pet-clinic-reviews</strong> microservice root directory:<p>a. Run the <strong class="source-inline">jib</strong> command to build Docker <strong class="source-inline">mvn compile jib:dockerBuild</strong>.</p><p>b. Wait for <strong class="source-inline">jib</strong> to build and upload the Docker image to the local Docker images repository.</p></li>
				<li>Open the <a id="_idIndexMarker690"/>terminal in <a id="_idIndexMarker691"/>the <strong class="source-inline">pet-clinic-concierge</strong> microservice root directory:<p>a. Run the <strong class="source-inline">jib</strong> command to build Docker <strong class="source-inline">mvn compile jib:dockerBuild</strong>.</p><p>b. Wait for <strong class="source-inline">jib</strong> to build and upload the Docker image to the local Docker images repository.</p></li>
				<li>Open any Bash terminal and change the directory to where you have checked out the <strong class="source-inline">pet-clinic</strong> <strong class="source-inline">docker-compose.yml</strong> file:<p>a. Run <strong class="source-inline">docker compose up –d</strong>.</p><p>b. Wait for Docker to finish booting up the <strong class="source-inline">pet-clinic</strong> stack. </p></li>
			</ol>
			<p>Once the <strong class="source-inline">pet-clinic</strong> application is instantiated and running in Docker, we need to configure Kibana to index and show the logs. To index logs in Kibana, perform the following steps:</p>
			<ol>
				<li value="1">Navigate to Kibana at <strong class="source-inline">http://localhost:5601</strong> and log in using Elasticsearch credentials as mentioned in the <strong class="source-inline">.env</strong> file in the <strong class="source-inline">docker-elk</strong> directory. </li>
				<li>Open the home page and click on the <strong class="bold">Connect to your Elasticsearch</strong> <strong class="bold">index</strong> hyperlink. After clicking on <strong class="bold">Connect to your Elasticsearch</strong> <strong class="bold">index</strong>, Kibana will provide a setup page to connect your index (see the following screenshot):<div id="_idContainer096" class="IMG---Figure"><img src="image/Figure_9.3_B16585_Fixed.jpg" alt="Figure 9.3 – Connecting the Elasticsearch index in Kibana&#13;&#10;"/></div><p class="figure-caption">Figure 9.3 – Connecting the Elasticsearch index in Kibana</p><p>Kibana <a id="_idIndexMarker692"/>provides an intuitive user <a id="_idIndexMarker693"/>interface to connect with your Elasticsearch index. Click on the highlighted portion in the screenshot and follow the steps as presented by Kibana.</p></li>
				<li>When the setup page loads, enter <strong class="source-inline">logstash</strong> in the <strong class="bold">Index Patterns</strong> textbox. </li>
				<li>Click on the <strong class="bold">Next step</strong> button and select <strong class="bold">@timestamp</strong> in the configure settings. </li>
				<li>Then, click on <strong class="bold">Create index pattern</strong>. </li>
			</ol>
			<p>After a successful index connection, you can go to the <strong class="bold">Discover</strong> page and view the application logs as follows:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/Figure_9.4_B16585_Fixed.jpg" alt="Figure 9.4 – Viewing the application logs in Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Viewing the application logs in Discover</p>
			<p>On the <strong class="bold">Discover</strong> page, we <a id="_idIndexMarker694"/>can access the live<a id="_idIndexMarker695"/> streaming of logs from various microservices. As we mutated the log events to include <strong class="source-inline">app_name</strong> and <strong class="source-inline">app_port</strong>, we can drill down on both the parameters to see the logs to a specific microservice. </p>
			<p>So now, we have implemented an ELK Stack distributed logging that intuitively provides a common place to live access the microservices' logs. If there's any fault in any microservice, you can directly access Kibana and view/search the logs. As you add more microservice instances and components to your runtime topology, ELK will simplify the log management.</p>
			<p>In the next section, we will dive into distributed tracing and how we can implement distributed tracing in the <strong class="source-inline">pet-clinic</strong> application. </p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor162"/>Distributed tracing in Micronaut microservices</h1>
			<p>Distributed<a id="_idIndexMarker696"/> tracing is the capability of <a id="_idIndexMarker697"/>the system to track and observe the execution flow of a request in distributed systems by collecting data as the request furthers from one service component to another. This trace data compiles metrics such as the time taken at each service along with end-to-end execution flow. Time metrics can help to zero down performance issues such as which service component is a bottleneck to the execution flow and why. </p>
			<p>A trace is a Gantt chart-like data structure that stores the trace information in spans. Each span will keep a trace for the execution flow in a particular service component. Furthermore, a span can have a reference to parent span and child spans. Refer to the following figure:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/Figure_9.5_B16585_Fixed.jpg" alt="Figure 9.5 – Distributed tracing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Distributed tracing</p>
			<p>In the preceding diagram, we can see the traces/spans for loading the <strong class="source-inline">foo</strong> page on the user interface app. It first calls <strong class="bold">Service A</strong> to get the <strong class="source-inline">foo</strong> object, which in turn calls <strong class="bold">Service B</strong> and <strong class="bold">Service C</strong> to get <strong class="source-inline">Bars</strong> and <strong class="source-inline">Bazs</strong> for <strong class="source-inline">foo</strong>, respectively. The time taken for the whole execution will be the cumulative total of execution times at various service components.</p>
			<p>In the next section, we will implement a distributed tracing solution in the <strong class="source-inline">pet-clinic</strong> application.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor163"/>Implementing distributed tracing in Micronaut</h2>
			<p>In order to <a id="_idIndexMarker698"/>get hands-on with<a id="_idIndexMarker699"/> distributed tracing in Micronaut, we will implement Zipkin-based tracing in the <strong class="source-inline">pet-clinic</strong> application. </p>
			<p>We will run a Zipkin instance in Docker. To run Zipkin in Docker, perform the following steps:</p>
			<ol>
				<li value="1">Open any Bash terminal.</li>
				<li>Run the <strong class="source-inline">docker run -d -p 9411:9411 openzipkin/zipkin</strong> command.<p>Wait for Docker to download and instantiate Zipkin on port <strong class="source-inline">9411</strong>. After successful instantiation, you can verify Zipkin by accessing <strong class="source-inline">http://localhost:9411/zipkin</strong>.</p></li>
				<li>Next, we will begin with the <strong class="source-inline">pet-clinic-concierge</strong> service, which is the API gateway. Add the following dependencies to the <strong class="source-inline">pet-clinic-concierge</strong> POM:<p class="source-code">&lt;!-- Distributed tracing --&gt;</p><p class="source-code">&lt;dependency&gt;</p><p class="source-code">  &lt;groupId&gt;io.micronaut&lt;/groupId&gt;</p><p class="source-code">  &lt;artifactId&gt;micronaut-tracing&lt;/artifactId&gt;</p><p class="source-code">  &lt;version&gt;${micronaut.version}&lt;/version&gt;</p><p class="source-code">  &lt;scope&gt;compile&lt;/scope&gt;</p><p class="source-code">&lt;/dependency&gt;</p><p class="source-code">&lt;dependency&gt;</p><p class="source-code">  &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt;</p><p class="source-code">  &lt;artifactId&gt;brave-instrumentation-http&lt;/artifactId&gt;</p><p class="source-code">  &lt;version&gt;5.12.3&lt;/version&gt;</p><p class="source-code">  &lt;scope&gt;runtime&lt;/scope&gt;</p><p class="source-code">&lt;/dependency&gt;</p><p class="source-code">&lt;dependency&gt;</p><p class="source-code">  &lt;groupId&gt;io.zipkin.reporter2&lt;/groupId&gt;</p><p class="source-code">  &lt;artifactId&gt;zipkin-reporter&lt;/artifactId&gt;</p><p class="source-code">  &lt;version&gt;2.15.0&lt;/version&gt;</p><p class="source-code">  &lt;scope&gt;runtime&lt;/scope&gt;</p><p class="source-code">&lt;/dependency&gt;</p><p class="source-code">&lt;dependency&gt;</p><p class="source-code">  &lt;groupId&gt;io.opentracing.brave&lt;/groupId&gt;</p><p class="source-code">  &lt;artifactId&gt;brave-opentracing&lt;/artifactId&gt;</p><p class="source-code">  &lt;version&gt;0.37.2&lt;/version&gt;</p><p class="source-code">  &lt;scope&gt;compile&lt;/scope&gt;</p><p class="source-code">&lt;/dependency&gt;</p><p>By <a id="_idIndexMarker700"/>importing <a id="_idIndexMarker701"/>the preceding dependencies, we can leverage Micronaut as well as third-party code artifacts for distributed tracing. </p></li>
				<li>To enable the distributed tracing, we also need to amend <strong class="source-inline">application.properties</strong>. Add the following properties related to Zipkin:<p class="source-code">tracing:</p><p class="source-code">  zipkin:</p><p class="source-code">    http:</p><p class="source-code">      url: http://host.docker.internal:9411</p><p class="source-code">    enabled: true</p><p class="source-code">    sampler:</p><p class="source-code">      probability: 1</p><p>The preceding application properties for Zipkin are added at the root level. In <strong class="source-inline">url</strong>, we specified a Docker instance of Zipkin running on localhost. Furthermore, in <strong class="source-inline">sampler.probability</strong>, we specify the value as <strong class="source-inline">1</strong> that will enable the tracing for all the user requests. This probability can be reduced to any value between 0 and 1 wherein 0 means never sample and 1 means sample every request. </p></li>
				<li>Next, we need to tag the controller methods for spans. For managing the spans, we have the following two tags in Micronaut:<p>a. <strong class="source-inline">@NewSpan</strong>: This will create a new span beginning from the method it's tagged on.</p><p>b. <strong class="source-inline">@ContinueSpan</strong>: This will continue the previous span.</p></li>
			</ol>
			<p>Since all<a id="_idIndexMarker702"/> the <a id="_idIndexMarker703"/>client controllers in <strong class="source-inline">pet-clinic-concierge</strong> are the interfacing points to any upstream consumers, we will use <strong class="source-inline">@NewSpan</strong> on these methods so that a new trace can begin. The following are the span-related changes in <strong class="source-inline">OwnerResourceClientController</strong>:</p>
			<p class="source-code">@Controller("/api")</p>
			<p class="source-code">public class OwnerResourceClientController {</p>
			<p class="source-code">    @Inject</p>
			<p class="source-code">    OwnerResourceClient;</p>
			<p class="source-code">    @NewSpan</p>
			<p class="source-code">    @Post("/owners")</p>
			<p class="source-code">    public HttpResponse&lt;OwnerDTO&gt; </p>
			<p class="source-code">     createOwner(@SpanTag("owner.dto") OwnerDTO ownerDTO) {</p>
			<p class="source-code">        return ownerResourceClient.createOwner(ownerDTO);</p>
			<p class="source-code">    }</p>
			<p class="source-code">    @NewSpan</p>
			<p class="source-code">    @Put("/owners")</p>
			<p class="source-code">    HttpResponse&lt;OwnerDTO&gt; updateOwner</p>
			<p class="source-code">     (@SpanTag("owner.dto") @Body OwnerDTO ownerDTO) {</p>
			<p class="source-code">        return ownerResourceClient.updateOwner(ownerDTO);</p>
			<p class="source-code">    }</p>
			<p class="source-code">    ...</p>
			<p class="source-code">}</p>
			<p>Similar<a id="_idIndexMarker704"/> changes to annotate <a id="_idIndexMarker705"/>client controller methods should be made in all the other clients for the <strong class="source-inline">pet-owner</strong>, <strong class="source-inline">pet-clinic</strong>, and <strong class="source-inline">pet-clinic-reviews</strong> microservices. </p>
			<p>Next, we need to modify the <strong class="source-inline">pet-clinic</strong> microservice for distributed tracing. </p>
			<h3>Modifying the pet-clinic microservice for distributed tracing</h3>
			<p>Continuing<a id="_idIndexMarker706"/> with distributed<a id="_idIndexMarker707"/> tracing changes, we need to make the required amends in the <strong class="source-inline">pet-owner</strong>, <strong class="source-inline">pet-clinic</strong>, and <strong class="source-inline">pet-clinic-reviews</strong> microservices project POM and application properties as explained in the previous section. </p>
			<p>Furthermore, to continue the tracing, we need to annotate controller methods with <strong class="source-inline">@ContinueSpan</strong> tags. Refer to the following code block:</p>
			<p class="source-code">@Post("/owners")</p>
			<p class="source-code">@ExecuteOn(TaskExecutors.IO)</p>
			<p class="source-code">@ContinueSpan</p>
			<p class="source-code">public HttpResponse&lt;OwnerDTO&gt; createOwner(@Body OwnerDTO ownerDTO) throws URISyntaxException {</p>
			<p class="source-code">    ...</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">@ContinueSpan</strong> must be annotated on all the controller methods in all the microservices (excluding <strong class="source-inline">pet-clinic-concierge</strong>, which is an API gateway). <strong class="source-inline">@ContinueSpan</strong> will continue the span/trace from the previous span/trace. In <strong class="source-inline">pet-clinic-concierge</strong>, we annotate the <strong class="source-inline">createOwner()</strong> method with <strong class="source-inline">@NewSpan</strong>, and in<a id="_idIndexMarker708"/> the <strong class="source-inline">pet-owner</strong> microservice, we <a id="_idIndexMarker709"/>use <strong class="source-inline">@ContinueSpan</strong>. Using these tags in tandem will trace the end-to-end execution flow.</p>
			<p>In the next section, we will verify the end-to-end trace for an HTTP request in the <strong class="source-inline">pet-clinic</strong> application. </p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor164"/>Verifying the distributed tracing in the pet-clinic application</h2>
			<p>To verify<a id="_idIndexMarker710"/> the distributed tracing<a id="_idIndexMarker711"/> in the <strong class="source-inline">pet-clinic</strong> application, you must have the <strong class="source-inline">pet-clinic</strong> microservice running. We will fetch a list of owners via the API gateway. For this, perform the following steps:</p>
			<ol>
				<li value="1">Go to <strong class="source-inline">http://localhost:32584/api/owners</strong> in any browser tab or REST client.</li>
				<li>Navigate to Zipkin to verify the trace for the preceding HTTP <strong class="source-inline">GET</strong> call at <strong class="source-inline">http://localhost:9411/zipkin</strong>.</li>
				<li>Click on the <strong class="bold">Run Query</strong> button.</li>
				<li>Go to the <strong class="source-inline">get /api/owners</strong> request in the returned results and click <strong class="bold">Show</strong>. </li>
			</ol>
			<p>After successfully performing these steps, you will see the following screen:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/Figure_9.6_B16585_Fixed.jpg" alt="Figure 9.6 – GET owners distributed tracing in Zipkin &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – GET owners distributed tracing in Zipkin </p>
			<p>Zipkin provides<a id="_idIndexMarker712"/> an intuitive user interface <a id="_idIndexMarker713"/>for accessing the request execution traces. You can see at first the request reaches <strong class="source-inline">pet-clinic-concierge</strong>, which is further passed on to the <strong class="source-inline">pet-owner</strong> microservice. In total it took approximately 948 ms to complete the request with the majority of the time spent on the <strong class="source-inline">pet-owner</strong> microservice.</p>
			<p>In the next section, we will focus on distributed monitoring and how to implement distributed monitoring in the Micronaut framework. </p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor165"/>Distributed monitoring in Micronaut microservices</h1>
			<p>Monitoring is <a id="_idIndexMarker714"/>simply recording the key<a id="_idIndexMarker715"/> performance metrics to enhance visibility into the application state. By recording and surfacing the system performance metrics such as CPU usage, thread pools, memory usage, and database connections for all the distributed components, it can provide a holistic picture of how a microservice system is performing at a given point in time. The distributed nature of microservices requires a shift in how the system is monitored. Instead of relying on the host environment monitoring tools, we need a unified monitoring solution that can combine performance metrics from various services and present a one-stop interface. In this section, we will explore how to implement such a distributed monitoring solution for the <strong class="source-inline">pet-clinic</strong> application.</p>
			<p>To implement distributed monitoring, we will use a very popular stack of Prometheus and Grafana. Let's look at our system components for distributed monitoring:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/Figure_9.7_B16585_Fixed.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Distributed monitoring using Prometheus and Grafana </p>
			<p>As shown in the preceding diagram, the <strong class="source-inline">pet-clinic</strong> microservice will be communicating the metrics to the <strong class="bold">Prometheus</strong> server and <strong class="bold">Grafana</strong> will get the metrics to <a id="_idIndexMarker716"/>present <a id="_idIndexMarker717"/>the user interface. Prometheus configurations will be stored in a YAML file. </p>
			<p>In the next section, we will begin with setting up Prometheus and Grafana in Docker.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor166"/>Setting up Prometheus and Grafana in Docker</h2>
			<p>Before <a id="_idIndexMarker718"/>we <a id="_idIndexMarker719"/>instantiate<a id="_idIndexMarker720"/> Prometheus <a id="_idIndexMarker721"/>and Grafana in Docker, we need to define configurations for Prometheus so that it can pull required metrics from the <strong class="source-inline">pet-clinic</strong> microservice. You can check out <strong class="source-inline">docker-prometheus docker-compose</strong> and <strong class="source-inline">prometheus.yml</strong> from <a href="https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro">https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus</a>.</p>
			<p>Once checked out locally, you can review the <strong class="source-inline">prometheus.yml</strong> file to be as follows:</p>
			<p class="source-code">global:</p>
			<p class="source-code">  scrape_interval:     15s</p>
			<p class="source-code">  evaluation_interval: 15s</p>
			<p class="source-code">  external_labels:</p>
			<p class="source-code">      monitor: 'pet-clinic-monitor'</p>
			<p class="source-code">scrape_configs:</p>
			<p class="source-code">  - job_name: 'prometheus'</p>
			<p class="source-code">    scrape_interval: 10s</p>
			<p class="source-code">    static_configs:</p>
			<p class="source-code">         - targets: ['host.docker.internal:9090','node-</p>
			<p class="source-code">            exporter:9110']</p>
			<p class="source-code">  - job_name: 'micronaut'</p>
			<p class="source-code">    metrics_path: '/metrics'</p>
			<p class="source-code">    scrape_interval: 10s</p>
			<p class="source-code">    static_configs:</p>
			<p class="source-code">         - targets: ['host.docker.internal:32581', 'host.docker.internal:32582', 'host.docker.internal:32583', 'host.docker.internal:32584']</p>
			<p>In <strong class="source-inline">prometheus.yml</strong>, we mainly need to configure <strong class="source-inline">scrape_configs</strong>. This will be responsible for invoking the microservice endpoints to get the metrics. We can specify the <strong class="source-inline">pet-clinic</strong> microservice in the targets. In addition, you can note that the scrape interval is <strong class="source-inline">10</strong> seconds. This will configure Prometheus to fetch metrics every 10 seconds. </p>
			<p>Next, let's <a id="_idIndexMarker722"/>set <a id="_idIndexMarker723"/>up <a id="_idIndexMarker724"/>our<a id="_idIndexMarker725"/> distributed monitoring stack in Docker. </p>
			<p>To set up Prometheus and Grafana in Docker, follow these instructions:</p>
			<ol>
				<li value="1">Check out <strong class="source-inline">docker-prometheus</strong> from <a href="https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro">https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus</a>.</li>
				<li>Open any Bash terminal (we used Git Bash).</li>
				<li>Change directory to where you have checked out <strong class="source-inline">docker-prometheus</strong>.</li>
				<li>Run <strong class="source-inline">docker compose up –d</strong>.</li>
				<li>Wait for Docker to download the images and instantiate the Prometheus app container.</li>
			</ol>
			<p>These instructions will boot up the monitoring app in Docker. You can verify the installation by going to the Docker Dashboard and <strong class="bold">Containers / Apps</strong>.</p>
			<p>In the next section, we will explore how we can integrate the <strong class="source-inline">pet-clinic</strong> microservice into Prometheus.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor167"/>Configuring microservices for distributed monitoring</h2>
			<p>To <a id="_idIndexMarker726"/>configure the <strong class="source-inline">pet-clinic</strong> microservice<a id="_idIndexMarker727"/> for distributed monitoring, we need to update the <strong class="source-inline">project</strong> POM with <strong class="source-inline">Micrometer</strong> dependencies. </p>
			<p>Add the following dependencies to the <strong class="source-inline">pet-owner</strong> project POM:</p>
			<p class="source-code">&lt;!-- Micrometer --&gt;</p>
			<p class="source-code">&lt;dependency&gt;</p>
			<p class="source-code">  &lt;groupId&gt;io.micronaut.micrometer&lt;/groupId&gt;</p>
			<p class="source-code">  &lt;artifactId&gt;micronaut-micrometer-core&lt;/artifactId&gt;</p>
			<p class="source-code">&lt;/dependency&gt;</p>
			<p class="source-code">&lt;dependency&gt;</p>
			<p class="source-code">  &lt;groupId&gt;io.micronaut.micrometer&lt;/groupId&gt;</p>
			<p class="source-code">  &lt;artifactId&gt;micronaut-micrometer-registry-</p>
			<p class="source-code">   prometheus&lt;/artifactId&gt;</p>
			<p class="source-code">&lt;/dependency&gt;</p>
			<p class="source-code">&lt;dependency&gt;</p>
			<p class="source-code">  &lt;groupId&gt;io.micronaut&lt;/groupId&gt;</p>
			<p class="source-code">  &lt;artifactId&gt;micronaut-management&lt;/artifactId&gt;</p>
			<p class="source-code">&lt;/dependency&gt;</p>
			<p>By importing the <strong class="source-inline">micronaut-micrometer</strong> dependencies, we can leverage a distributed monitoring toolkit in the <strong class="source-inline">pet-owner</strong> microservice. </p>
			<p>To expose service metrics for Prometheus, we need to expose the <strong class="source-inline">metrics</strong> endpoint in all the <strong class="source-inline">pet-clinic</strong> microservices. We will add a new controller called <strong class="source-inline">PrometheusController</strong> to the <strong class="source-inline">com.packtpub.micronaut.web.rest.commons</strong> package as follows:</p>
			<p class="source-code">@RequiresMetrics</p>
			<p class="source-code">@Controller("/metrics")</p>
			<p class="source-code">public class PrometheusController {</p>
			<p class="source-code">    private final PrometheusMeterRegistry;</p>
			<p class="source-code">    @Inject</p>
			<p class="source-code">    public PrometheusController(PrometheusMeterRegistry </p>
			<p class="source-code">     prometheusMeterRegistry) {</p>
			<p class="source-code">        this.prometheusMeterRegistry = </p>
			<p class="source-code">         prometheusMeterRegistry;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    @Get</p>
			<p class="source-code">    @Produces("text/plain")</p>
			<p class="source-code">    public String metrics() {</p>
			<p class="source-code">        return prometheusMeterRegistry.scrape();</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">PrometheusController</strong> will expose <strong class="source-inline">prometheusMeterRegistry.scrape()</strong> on the <strong class="source-inline">/metrics</strong> endpoint. </p>
			<p><strong class="source-inline">prometheusMeterRegistry.scrape()</strong> will<a id="_idIndexMarker728"/> provide<a id="_idIndexMarker729"/> service performance metrics as configured in the <strong class="source-inline">application.properties</strong> file.</p>
			<p>We need to configure the <strong class="source-inline">application.properties</strong> file as follows:</p>
			<p class="source-code">micronaut:</p>
			<p class="source-code">  ...</p>
			<p class="source-code">  metrics:</p>
			<p class="source-code">    enabled: true</p>
			<p class="source-code">    export:</p>
			<p class="source-code">      prometheus:</p>
			<p class="source-code">        enabled: true</p>
			<p class="source-code">        step: PT1M</p>
			<p class="source-code">        descriptions: true</p>
			<p class="source-code">endpoints:</p>
			<p class="source-code">  metrics:</p>
			<p class="source-code">    enabled: false</p>
			<p class="source-code">  prometheus:</p>
			<p class="source-code">    enabled: false</p>
			<p>In <strong class="source-inline">application.properties</strong>, we are enabling the metrics and exporting the metrics in <a id="_idIndexMarker730"/>Prometheus <a id="_idIndexMarker731"/>format. Furthermore, since we are providing our custom <strong class="source-inline">/metrics</strong> endpoint, we are disabling the <strong class="source-inline">metrics</strong> and <strong class="source-inline">prometheus</strong> endpoints in the application properties. </p>
			<p>Similarly, we need to modify the project POM, add <strong class="source-inline">PrometheusController</strong>, and update the application properties for the <strong class="source-inline">pet-clinic</strong>, <strong class="source-inline">pet-clinic-reviews</strong>, and <strong class="source-inline">pet-clinic-concierge</strong> microservices. Afterward, we need to rebuild the Docker images for all service projects running the <strong class="source-inline">mvn compile jib:dockerBuild</strong> command in the terminal. Once the Docker images are built and uploaded to the local Docker repository, we need to decommission the old <strong class="source-inline">pet-clinic</strong> application in Docker and rerun <strong class="source-inline">docker compose up –d</strong> to re-instantiate the modified <strong class="source-inline">pet-clinic</strong> application. </p>
			<p>In the next section, we will verify the distributed monitoring implementation in the <strong class="source-inline">pet-clinic</strong> application.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/>Verifying the distributed monitoring in the pet-clinic application</h2>
			<p>To verify<a id="_idIndexMarker732"/> the distributed monitoring <a id="_idIndexMarker733"/>in the <strong class="source-inline">pet-clinic</strong> application, you must have the <strong class="source-inline">pet-clinic</strong> and Prometheus applications running in Docker. You need to follow these instructions to verify integration between Prometheus and the <strong class="source-inline">pet-clinic</strong> application:</p>
			<ol>
				<li value="1">Access the <strong class="source-inline">/metrics</strong> endpoints for all the microservices to verify that services are exposing metrics to Prometheus.</li>
				<li>Verify the <strong class="source-inline">pet-owner</strong> metrics by accessing <strong class="source-inline">http://localhost:32581/metrics</strong>.</li>
				<li>Verify the <strong class="source-inline">pet-clinic</strong> metrics by accessing <strong class="source-inline">http://localhost:32582/metrics</strong>.</li>
				<li>Verify <a id="_idIndexMarker734"/>the <strong class="source-inline">pet-clinic-reviews</strong> metrics <a id="_idIndexMarker735"/>by accessing <strong class="source-inline">http://localhost:32583/metrics</strong>.</li>
				<li>Verify the <strong class="source-inline">pet-clinic-concierge</strong> metrics by accessing <strong class="source-inline">http://localhost:32584/metrics</strong>.</li>
				<li>Navigate to <strong class="source-inline">http://localhost:9090/graph</strong> and check whether you can see the <strong class="source-inline">system_cpu_usage</strong> metric. <p>After successful completion of the preceding steps, you will see the following screen:</p></li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/Figure_9.8_B16585_Fixed.jpg" alt="Figure 9.8 – Accessing the system CPU usage graph for the pet-clinic application in Prometheus&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Accessing the system CPU usage graph for the pet-clinic application in Prometheus</p>
			<p>In the <a id="_idIndexMarker736"/>preceding <a id="_idIndexMarker737"/>screenshot, we can verify that the <strong class="source-inline">pet-clinic</strong> microservice is able to expose the performance metrics on the endpoint and Prometheus can invoke the <strong class="source-inline">/metrics</strong> endpoints. We can see the system CPU usage graph in Prometheus graphs but as a system admin or developer, you probably need a system dashboard with all the metric graphs in one place. </p>
			<p>In the following instructions, we will integrate Grafana with the Prometheus server: </p>
			<ol>
				<li value="1">Navigate to <strong class="source-inline">http://localhost:3000/</strong> and log in with the username as <strong class="source-inline">admin</strong> and password as <strong class="source-inline">pass</strong>.</li>
				<li>After logging in, navigate to <strong class="source-inline">http://localhost:3000/datasources</strong>.</li>
				<li>Click on the <strong class="bold">Add data source</strong> button.</li>
				<li>Under<a id="_idIndexMarker738"/> the <strong class="bold">Time series databases</strong> list, select <strong class="bold">Prometheus</strong>. </li>
				<li>In <strong class="bold">URL</strong>, provide <a id="_idIndexMarker739"/>the value as <strong class="source-inline">http://prometheus:9090</strong>.<p>Keep the rest of the values as the defaults.</p></li>
				<li>Click on the <strong class="bold">Save and test</strong> button. You should get a successful message. </li>
				<li>Go to the adjacent <strong class="bold">Dashboards</strong> tab and click on <strong class="bold">Prometheus 2.0 stats</strong>. </li>
			</ol>
			<p>After successful completion of these steps, you should see the following dashboard:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/Figure_9.9_B16585_Fixed.jpg" alt="Figure 9.9 – Prometheus dashboard in Grafana&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Prometheus dashboard in Grafana</p>
			<p>As shown <a id="_idIndexMarker740"/>in<a id="_idIndexMarker741"/> the preceding screenshot, Grafana provides a very intuitive, unified dashboard for accessing the vital system metrics for all the service components in the <strong class="source-inline">pet-clinic</strong> application. One-stop access to this telemetry data is very handy in addressing any performance issues and system failures in production environments.</p>
			<p>In this section, we explored what distributed monitoring is and how we can implement distributed monitoring using Prometheus and Grafana in the Micronaut framework. </p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Summary</h1>
			<p>In this chapter, we began with distributed logging and why it is important in any microservice implementation. We implemented an ELK Stack for distributed logging in the <strong class="source-inline">pet-clinic</strong> application. Furthermore, we dived into using the Kibana user interface for connecting to the Elasticsearch application logs index. </p>
			<p>Later, we explored what distributed tracing is and how to implement distributed tracing using Zipkin in the Micronaut framework. We also verified the trace of an HTTP call in the Zipkin user interface. </p>
			<p>In the end, we dived into the world of distributed monitoring and implemented a distributed monitoring solution for the <strong class="source-inline">pet-clinic</strong> application using a Prometheus and Grafana stack. </p>
			<p>This chapter enhanced your Micronaut microservices journey with the observability patterns that are distributed logging, distributed tracing, and distributed monitoring by enabling you with hands-on knowledge on how to implement these patterns in the Micronaut framework.</p>
			<p>In the next chapter, we will explore how to implement an IoT solution in the Micronaut framework.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor170"/>Questions</h1>
			<ol>
				<li value="1">What is distributed logging in microservices?</li>
				<li>How do you run an ELK Stack in Docker?</li>
				<li>How do you implement distributed logging in the Micronaut framework?</li>
				<li>How do you connect to a Docker Logstash from the Micronaut microservice?</li>
				<li>What is distributed tracing in microservices?</li>
				<li>How do you implement distributed tracing in the Micronaut framework?</li>
				<li>What is distributed monitoring in microservices? </li>
				<li>How do you run a Prometheus and Grafana stack in Docker?</li>
				<li>How do you implement distributed monitoring in the Micronaut framework?</li>
			</ol>
		</div>
	</body></html>