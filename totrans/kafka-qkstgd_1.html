<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Configuring Kafka</h1>
                </header>
            
            <article>
                
<p>This chapter describes what Kafka is and the concepts related to this technology: brokers, topics, <span>producers,</span> and consumers. It also talks about how to build a simple producer and consumer from the command line, as well as how to install Confluent Platform. The information in this chapter is fundamental to the following chapters.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Kafka in a nutshell</li>
<li>Installing Kafka (Linux and macOS)</li>
<li>Installing the Confluent Platform</li>
<li>Running Kafka</li>
<li>Running Confluent Platform</li>
<li>Running Kafka brokers</li>
<li>Running Kafka topics</li>
<li>A command–line message producer</li>
<li>A command–line message consumer</li>
<li>Using kafkacat</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka in a nutshell</h1>
                </header>
            
            <article>
                
<p>Apache Kafka is an open source streaming platform. If you are reading this book, maybe you already know that Kafka scales very well in a horizontal way without compromising speed and efficiency.</p>
<p>The Kafka core is written in Scala, and Kafka Streams and KSQL are written in Java. A Kafka server can run in several operating systems: Unix, Linux, macOS, and even Windows. As it usually runs in production on Linux servers, the examples in this book are designed to run on Linux environments. The examples in this book also consider bash environment usage.</p>
<p class="mce-root"/>
<p>This chapter explains how to install, configure, and run Kafka. As this is a Quick Start Guide, it does not cover Kafka's theoretical details. At the moment, it is appropriate to mention these three points:</p>
<ul>
<li><strong>Kafka is a </strong><strong>service bus</strong>: To connect heterogeneous applications, we need to implement a message publication mechanism to send and receive messages among them. A message router is known as message broker. Kafka is a message broker, a solution to deal with routing messages among clients in a quick way.</li>
<li><strong>Kafka architecture has two directives</strong>: The first is to not block the producers (in order to deal with the back pressure). The second is to isolate producers and consumers. The producers should not know who their consumers are, hence Kafka follows the dumb broker and smart clients <span>model</span>.</li>
<li><strong>Kafka is a real-time messaging system</strong>:<strong> </strong>Moreover, Kafka is a software solution with a publish-subscribe model: open source, distributed, partitioned, replicated, and commit-log-based.</li>
</ul>
<p>There are some concepts and nomenclature in Apache Kafka:</p>
<ul>
<li><strong>Cluster</strong>: This is a set of Kafka brokers.</li>
<li><strong>Zookeeper</strong>: <span>This is a</span> cluster coordinator—a tool with different services that are part of the Apache ecosystem.</li>
<li><strong>Broker</strong>: <span>This is a</span> Kafka server, also the Kafka server process itself.</li>
<li><strong>Topic</strong>: <span>This is a</span> queue (that has log partitions); a broker can run several topics.</li>
<li><strong>Offset</strong>: <span>This is a</span>n identifier for each message.</li>
<li><strong>Partition</strong>: <span>This is a</span>n immutable and ordered sequence of records continually appended to a structured commit log.</li>
<li><strong>Producer</strong>: This is the program that publishes data to topics.</li>
<li><strong>Consumer</strong>: Th<span>is is the</span> program that processes data from the topics.</li>
<li><strong>Retention period</strong>: <span>Th</span><span>is is the t</span>ime to keep messages available for consumption.</li>
</ul>
<p>In Kafka, there are three types of clusters:</p>
<ul>
<li>Single node–single broker</li>
<li>Single node–multiple broker</li>
<li>Multiple node–multiple broker</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In Kafka, there are three (and just three) ways to deliver messages:</p>
<ul>
<li><strong>Never redelivered</strong>: The messages may be lost because, once delivered, they are not sent again.</li>
<li><strong>May be redelivered</strong>: The messages are never lost because, if it is not received, the message can be sent again.</li>
<li><strong>Delivered once</strong>: The message is delivered exactly once. This is the most difficult form of delivery; since the message is only sent once and never redelivered, it implies that there is zero loss of any message.</li>
</ul>
<p>The message log can be compacted in two ways:</p>
<ul>
<li><strong>Coarse-grained</strong>: Log compacted by time</li>
<li><strong>Fine-grained</strong>: Log compacted by message</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka installation</h1>
                </header>
            
            <article>
                
<p>There are three ways to install a Kafka environment:</p>
<ul>
<li>Downloading the executable files</li>
<li>Using <kbd>brew</kbd> (in macOS) or <kbd>yum</kbd> (in Linux)</li>
<li>Installing Confluent Platform</li>
</ul>
<p>For all three ways, the first step is to install Java; we need Java 8. Download and install the latest JDK 8 from the Oracle's website: </p>
<p><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<p>At the time of writing, the latest Java 8 JDK version is 8u191.</p>
<p>For Linux users :</p>
<ol>
<li>Change the file mode to executable as follows, follows these steps:</li>
</ol>
<pre><strong>      &gt; chmod +x jdk-8u191-linux-x64.rpm</strong></pre>
<ol start="2">
<li>Go to the directory in which you want to install Java:</li>
</ol>
<pre><strong>      &gt; cd &lt;directory path&gt;</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Run the <kbd>rpm</kbd> installer with the following command:</li>
</ol>
<pre><strong>      &gt; rpm -ivh jdk-8u191-linux-x64.rpm</strong></pre>
<p class="mce-root"/>
<ol start="4">
<li>Add to your environment the <kbd>JAVA_HOME</kbd> <span>variable.</span> The following command writes the <kbd>JAVA_HOME</kbd> environment variable to the <kbd>/etc/profile</kbd> <span>file:</span></li>
</ol>
<pre><strong>      &gt; echo "export JAVA_HOME=/usr/java/jdk1.8.0_191" &gt;&gt; /etc/profile</strong></pre>
<ol start="5">
<li>Validate the Java installation as follows:</li>
</ol>
<pre><strong>      &gt; java -version<br/>      java version "1.8.0_191"<br/>      Java(TM) SE Runtime Environment (build 1.8.0_191-b12)<br/>      Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</strong></pre>
<p>At the time of writing, the latest Scala version is 2.12.6. <span>To install Scala in Linux, perform the following steps</span>:</p>
<ol>
<li>Download the latest Scala binary from <a href="http://www.scala-lang.org/download">http://www.scala-lang.org/download</a></li>
<li>Extract the <span>downloaded file, </span><kbd>scala-2.12.6.tgz</kbd><span>, as follows:</span></li>
</ol>
<pre><strong>      &gt; tar xzf scala-2.12.6.tgz</strong></pre>
<ol start="3">
<li>Add the  <kbd>SCALA_HOME</kbd> <span>variable </span>to your environment as follows:</li>
</ol>
<pre><strong>      &gt; export SCALA_HOME=/opt/scala</strong></pre>
<ol start="4">
<li>Add the Scala bin directory to your <kbd>PATH</kbd> environment variable as follows:</li>
</ol>
<pre><strong>      &gt; export PATH=$PATH:$SCALA_HOME/bin</strong></pre>
<ol start="5">
<li>To validate the Scala installation, do the following:</li>
</ol>
<pre><strong>      &gt;  scala -version<br/>      Scala code runner version 2.12.6 -- Copyright 2002-2018,<br/>      LAMP/EPFL and Lightbend, Inc.<br/></strong></pre>
<p>To install Kafka on your machine, ensure that you have at least 4 GB of RAM, and the installation directory will be <kbd>/usr/local/kafka/</kbd> for macOS users and <kbd>/opt/kafka/</kbd> for Linux users. Create these directories according to your operating system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka installation on Linux</h1>
                </header>
            
            <article>
                
<p>Open the Apache Kafka download page, <a href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a>, as in <em>Figure 1.1</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34e74b94-4c67-4c00-9148-ade9cdd2ddfc.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1.1: Apache Kafka download page</div>
<p>At the time of writing, the <span>current </span>Apache Kafka version is 2.0.0 as a stable release. Remember that, since version 0.8.x, Kafka is not backward-compatible. So, we cannot replace this version for one prior to 0.8. Once you've downloaded the latest available release, let's proceed with the installation.</p>
<p class="mce-root"/>
<div class="packt_tip">Remember for macOS users, replace the directory <kbd>/opt/</kbd> with <kbd>/usr/local</kbd><span>.</span></div>
<p>Follow these steps to install Kafka in Linux:</p>
<ol>
<li>Extract the downloaded file, <kbd>kafka_2.11-2.0.0.tgz</kbd>, in the <kbd>/opt/</kbd> directory as follows:</li>
</ol>
<pre><strong>      &gt; tar xzf kafka_2.11-2.0.0.tgz</strong></pre>
<ol start="2">
<li>Create the <kbd>KAFKA_HOME</kbd> environment variable as follows:</li>
</ol>
<pre><strong>      &gt; export KAFKA_HOME=/opt/kafka_2.11-2.0.0</strong></pre>
<ol start="3">
<li>Add the Kafka bin directory to the <kbd>PATH</kbd> variable as follows:</li>
</ol>
<pre><strong>      &gt; export PATH=$PATH:$KAFKA_HOME/bin</strong></pre>
<p style="padding-left: 60px">Now Java, Scala, and Kafka are installed.</p>
<p>To do all of the previous steps from the command line, there is a powerful tool for macOS users called <kbd>brew</kbd> (the equivalent in Linux would be <kbd>yum</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka installation on macOS</h1>
                </header>
            
            <article>
                
<p>To install from the command line in macOS (<kbd>brew</kbd> must be installed), perform the following steps:</p>
<ol>
<li>To install <kbd>sbt</kbd> (the Scala build tool) with <kbd>brew</kbd>, execute the following:</li>
</ol>
<pre><strong>      &gt; brew install sbt</strong></pre>
<p style="padding-left: 60px">If already have it in your environment (downloaded previously), run the following <span>to upgrade it</span>:</p>
<pre><strong>      &gt; brew upgrade sbt</strong></pre>
<p style="padding-left: 60px">The output is similar to that shown in <em>Figure 1.2</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/eb334cc9-9b67-41c5-9757-b1d11ff83232.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 1.2: The Scala build tool installation output</div>
<ol start="2">
<li>To install Scala with <kbd>brew</kbd>, execute the following:</li>
</ol>
<pre><strong>      &gt; brew install scala</strong></pre>
<p style="padding-left: 60px">If you already have it in your environment (downloaded previously), to upgrade it, run the following command:</p>
<pre><strong>      &gt; brew upgrade scala</strong></pre>
<p style="padding-left: 60px">The output is similar to that shown in <em>Figure 1.3</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a3c6c893-4b71-40a4-88a5-ab8149fdb1b8.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 1.3: The Scala installation output</div>
<ol start="3">
<li>To install Kafka with <kbd>brew</kbd>, (it also installs Zookeeper), do the following:</li>
</ol>
<pre><strong>      &gt; brew install kafka</strong></pre>
<p style="padding-left: 60px">If you already have it (downloaded in the past), upgrade it as follows:</p>
<pre><strong>      &gt; brew upgrade kafka</strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The output is similar to that shown in <em>Figure 1.4</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8d8c9dd3-7924-4ab3-b806-d3b568f8f721.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 1.4: Kafka installation output</div>
<p><span>Visit </span><a href="https://brew.sh/">https://brew.sh/</a> for more about <kbd>brew</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confluent Platform installation</h1>
                </header>
            
            <article>
                
<p>The third way to install Kafka is through Confluent Platform. In the rest of this book, we will be using Confluent Platform open source version.</p>
<p>Confluent Platform is an integrated platform that includes the following components:</p>
<ul>
<li>Apache Kafka</li>
<li>REST proxy</li>
<li>Kafka Connect API</li>
<li>Schema Registry</li>
<li>Kafka Streams API</li>
<li>Pre-built connectors</li>
<li>Non-Java clients</li>
<li>KSQL</li>
</ul>
<p>If the reader notices, almost every one of the components has its own chapter in this book.</p>
<p>The <span>commercially licensed Confluent Platform</span><span> includes, in addition to all of the components of the open source version, the following:</span></p>
<ul>
<li><strong>Confluent Control Center</strong> (<strong>CCC</strong>)</li>
<li>Kafka operator (for Kubernetes)</li>
<li>JMS client</li>
<li>Replicator</li>
<li>MQTT proxy</li>
<li>Auto data balancer</li>
<li>Security features</li>
</ul>
<p>It is important to mention that the training on the components of the non-open source version is beyond the scope of this book.</p>
<p>Confluent Platform is available also in Docker images, but here we are going to install it in local.</p>
<p>Open Confluent Platform download page: <a href="https://www.confluent.io/download/">https://www.confluent.io/download/</a> .</p>
<p>At the time of this writing, the <span>current version of Confluent Platform</span><span> is 5.0.0 as a stable release. Remember that, since the Kafka core runs on Scala, there are two versions: for Scala 2.11 and Scala 2.12.</span></p>
<p>We could run Confluent Platform from our desktop directory, but following this book's conventions, let's use <kbd>/opt/</kbd> for Linux users and <kbd>/usr/local</kbd> for macOS users.</p>
<p>To install Confluent Platform, extract the downloaded file, <kbd>confluent-5.0.0-2.11.tar.gz</kbd>, in the directory, as follows:</p>
<pre><strong>&gt; tar xzf confluent-5.0.0-2.11.tar.gz</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Kafka</h1>
                </header>
            
            <article>
                
<p>There are two ways to run Kafka, depending on whether we install it directly or through Confluent Platform.</p>
<p>If we install it directly, the steps to run Kafka are as follows.</p>
<div class="packt_infobox">For macOS users, your paths might be different if you've installed using <kbd>brew</kbd>. Check the output of <kbd>brew install kafka</kbd> command for the exact command that you can use to start Zookeeper and Kafka.</div>
<p>Go to the Kafka installation directory (<kbd>/usr/local/kafka</kbd> for <span>macOS </span>users and <kbd>/opt/kafka/</kbd> for Linux users), as in the example:</p>
<pre><strong>&gt; cd /usr/local/kafka</strong></pre>
<p>First of all, we need to start Zookeeper (the Kafka dependency with Zookeeper is and will remain strong). Type the following:</p>
<pre><strong>&gt; ./bin/zookeeper-server-start.sh ../config/zookeper.properties</strong><br/><br/><strong>ZooKeeper JMX enabled by default</strong><br/><br/><strong>Using config: /usr/local/etc/zookeeper/zoo.cfg</strong><br/><br/><strong>Starting zookeeper ... STARTED</strong></pre>
<p>To check whether Zookeeper is running, use the <kbd>lsof</kbd> command over the <kbd>9093</kbd> <span>port </span>(default port) as follows:</p>
<pre><strong>&gt; lsof -i :9093</strong><br/><br/><strong>COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME</strong><br/><br/><strong>java 12529 admin 406u IPv6 0xc41a24baa4fedb11 0t0 TCP *:9093 (LISTEN)</strong></pre>
<p>Now run the Kafka server that comes with the installation by going to <kbd>/usr/local/kafka/</kbd> for <span>macOS </span>users and <kbd>/opt/kafka/</kbd> for Linux users:</p>
<pre><strong>&gt; ./bin/kafka-server-start.sh ./config/server.properties</strong></pre>
<p>Now there is an Apache Kafka broker running in your machine.</p>
<p>Remember that Zookeeper must be running on the machine before starting Kafka. If you don't want to start Zookeeper manually every time you need to run Kafka, install it as an operation system auto-start service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Confluent Platform</h1>
                </header>
            
            <article>
                
<p>Go to the Confluent Platform installation directory (<kbd>/usr/local/kafka/</kbd> for <span>macOS </span>users and <kbd>/opt/kafka/</kbd> for Linux users) and type the following:</p>
<pre><strong>&gt; cd /usr/local/confluent-5.0.0</strong></pre>
<p>To start Confluent Platform, run the following:</p>
<pre><strong>&gt; bin/confluent start</strong></pre>
<p>This command-line interface is intended for development only, not for production:</p>
<p><a href="https://docs.confluent.io/current/cli/index.html">https://docs.confluent.io/current/cli/index.html</a></p>
<p class="mce-root">The output is similar to what is shown in the following code snippet:</p>
<pre><strong>Using CONFLUENT_CURRENT: /var/folders/nc/4jrpd1w5563crr_np997zp980000gn/T/confluent.q3uxpyAt</strong><br/><br/><strong>Starting zookeeper</strong><br/><strong>zookeeper is [UP]</strong><br/><strong>Starting kafka</strong><br/><strong>kafka is [UP]</strong><br/><strong>Starting schema-registry</strong><br/><strong>schema-registry is [UP]</strong><br/><strong>Starting kafka-rest</strong><br/><strong>kafka-rest is [UP]</strong><br/><strong>Starting connect</strong><br/><strong>connect is [UP]</strong><br/><strong>Starting ksql-server</strong><br/><strong>ksql-server is [UP]</strong><br/><strong>Starting control-center</strong><br/><strong>control-center is [UP]</strong></pre>
<p>As indicated by the command output, Confluent Platform automatically starts in this order: Zookeeper, Kafka, Schema Registry, REST proxy, Kafka Connect, KSQL, and the Confluent Control Center.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To access the Confluent Control Center running in your local, go to <kbd>http://localhost:9021</kbd>, as shown in <em>Figure 1.5</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1bb16534-016c-411f-9dc7-468c78e7e346.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1.5: Confluent Control Center main page</div>
<p>There are other commands for Confluent Platform.</p>
<p>To get the status of all services or the status of a specific service along with its dependencies, enter the following:</p>
<pre><strong>&gt; bin/confluent status</strong></pre>
<p>To stop all services or a specific service along with the services depending on it, enter<span> the following</span>:</p>
<pre><strong>&gt; bin/confluent stop</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To delete the data and logs of the current Confluent Platform, type<span> the following</span>:</p>
<pre><strong>&gt; bin/confluent destroy</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Kafka brokers</h1>
                </header>
            
            <article>
                
<p>The real art behind a server is in its configuration. In this section, we will examine how to deal with the basic configuration of a Kafka broker in standalone mode. Since we are learning, at the moment, we will not review the cluster configuration.</p>
<p>As we can suppose, there are two types of configuration: standalone and cluster. The real power of Kafka is unlocked when running with replication in cluster mode and all topics are correctly partitioned.</p>
<p>The cluster mode has two main advantages: parallelism and redundancy. Parallelism is the capacity to run tasks simultaneously among the cluster members. The redundancy warrants that, when a Kafka node goes down, the cluster is safe and accessible from the other running nodes.</p>
<p>This section shows how to configure a cluster with several nodes on our local machine although, in practice, it is always better to have several machines with multiple nodes sharing clusters.</p>
<p>Go to the Confluent Platform installation directory, referenced from now on as<span> </span><kbd>&lt;confluent-path&gt;</kbd><span>. </span></p>
<p>As mentioned in the beginning of this chapter, a broker is a server instance. A server (or broker) is actually a process running in the operating system and starts based on its configuration file.</p>
<p>The people of Confluent have kindly provided us with a template of a <span>standard </span>broker configuration. This file, which is called <kbd>server.properties</kbd>, is located in the Kafka installation directory in the <kbd>config</kbd> subdirectory:</p>
<ol>
<li>Inside <kbd>&lt;confluent-path&gt;</kbd>, make a directory with the name mark.</li>
</ol>
<ol start="2">
<li>For each Kafka broker (server) that we want to run, we need to make a copy of the configuration file template and rename it accordingly. In this example, our cluster is going to be called <kbd>mark</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; cp config/server.properties &lt;confluent-path&gt;/mark/mark-1.properties</strong></pre>
<pre style="padding-left: 60px"><strong>&gt; cp config/server.properties &lt;confluent-path&gt;/mark/mark-2.properties</strong></pre>
<ol start="3">
<li>Modify each properties file accordingly. If the file is called <kbd>mark-1</kbd>, the <kbd>broker.id</kbd> should be <kbd>1</kbd>. Then, specify the port in which the server will run; the recommendation is <kbd>9093</kbd> for <kbd><span>mark-1</span></kbd> and <kbd>9094</kbd> for <kbd>mark-2</kbd>. Note that the port property is not set in the template, so add the line. Finally, specify the location of the Kafka logs (a Kafka log is a specific archive to store all of the Kafka broker operations); in this case, we use the <kbd>/tmp</kbd> <span>directory. </span><span>Here, it is common to have problems with write permissions. Do not forget to give write and execute permissions to the user with whom these processes are executed over the log directory, as in the examples:</span></li>
</ol>
<ul>
<li>In <kbd>mark-1.properties</kbd>, set the following:</li>
</ul>
<pre class="mce-root" style="padding-left: 60px"><strong>      broker.id=1</strong><br/><strong>      port=9093</strong><br/><strong>      log.dirs=/tmp/mark-1-logs</strong></pre>
<ul>
<li>In <kbd>mark-2.properties</kbd>, set <span>the following</span>:</li>
</ul>
<pre class="mce-root" style="padding-left: 60px"><strong>      broker.id=2</strong><br/><strong>      port=9094</strong><br/><strong>      log.dirs=/tmp/mark-2-logs</strong></pre>
<ol start="4">
<li>Start the Kafka brokers using the <kbd>kafka-server-start</kbd> command with the corresponding configuration file passed as the parameter. Don't forget that Confluent Platform must be already running and the ports should not be in use by another process. Start <span>the Kafka brokers as follows:</span></li>
</ol>
<pre><strong>      &gt; &lt;confluent-path&gt;/bin/kafka-server-start &lt;confluent-<br/>      path&gt;/mark/mark-1.properties &amp;</strong></pre>
<p style="padding-left: 60px">And, in another command-line window, run the following command:</p>
<pre><strong>      &gt; &lt;confluent-path&gt;/bin/kafka-server-start &lt;confluent-<br/>      path&gt;/mark/mark-2.properties &amp;</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Don't forget that the trailing <kbd>&amp;</kbd> is to specify that you want your command line back. If you want to see the broker output, it is recommended to run each command separately in its own command-line window.</p>
<p>Remember that the properties file contains the server configuration and that the <kbd>server.properties</kbd> file located in the <kbd>config</kbd> directory is just a template.</p>
<p>Now there are two brokers, <kbd>mark-1</kbd> and <kbd>mark-2</kbd> , running in the same machine in the same cluster.</p>
<p>Remember, there are no dumb questions, as in the following examples:</p>
<p><strong>Q</strong>: How does each broker know which cluster it belongs to?</p>
<p><strong>A</strong>: The brokers know that they belong to the same cluster because, in the configuration, both point to the same Zookeeper cluster.</p>
<p><strong>Q</strong>: How does each broker differ from the others within the same cluster? </p>
<p><strong>A</strong>: Every broker is identified inside the cluster by the name specified in the <kbd>broker.id</kbd> property.</p>
<p><strong>Q</strong>: What happens if the port number is not specified?</p>
<p><strong>A</strong>: If the port property is not specified, Zookeeper will assign the same port number and will overwrite the data.</p>
<p><strong>Q</strong>: What happens if the log directory is not specified?</p>
<p><strong>A</strong>: If <kbd>log.dir</kbd> is not specified, all the brokers will write to the same default <kbd>log.dir</kbd>. If the brokers are planned to run in different machines, then the port and <kbd>log.dir</kbd> properties might not be specified (because they run in the same port and log file but in different machines).</p>
<p><strong>Q</strong>: How can I check that there is not a process already running in the port where I want to start my broker?</p>
<p><strong>A</strong>: As shown in the previous section, there is a useful command to see what process is running on specific port, in this case the <kbd>9093</kbd><span> port:</span></p>
<pre><strong>&gt; lsof -i :9093</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output of the previous command is something like this:</p>
<pre><strong>COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME</strong><br/><br/><strong>java 12529 admin 406u IPv6 0xc41a24baa4fedb11 0t0 TCP *:9093 (LISTEN)</strong></pre>
<p>Your turn: try to run this command before starting the Kafka brokers, and run it after starting them to see the change. Also, try to start a broker on a port in use to see how it fails.</p>
<p>OK, what if I want my cluster to run on several machines?</p>
<p>To run Kafka nodes on different machines but in the same cluster, adjust the Zookeeper connection string in the configuration file; its default value is as follows:</p>
<pre>zookeeper.connect=localhost:2181</pre>
<p><span>Remember that the machines must be able to be found by each other by DNS and that there are no network security restrictions between them.</span></p>
<p>The default value for Zookeeper connect is correct only if you are running the Kafka broker in the same machine as Zookeeper. Depending on the architecture, it will be necessary to decide if there will be a broker running on the same Zookeeper machine.</p>
<p>To specify that Zookeeper might run in other machines, do the following:</p>
<pre>zookeeper.connect=localhost:2181, 192.168.0.2:2183, 192.168.0.3:2182</pre>
<p>The previous line specifies that Zookeeper is running in the local host machine on port <kbd>2181</kbd>, in the machine with IP address <kbd>192.168.0.2</kbd> on port <kbd>2183</kbd> , and in the machine with IP address, the <kbd>192.168.0.3</kbd>, on port <kbd>2182</kbd>. The Zookeeper default port is <kbd>2181</kbd>, so normally it runs there.</p>
<p>Your turn: as an exercise, try to start a broker with incorrect information about the Zookeeper cluster. Also, using the <kbd>lsof</kbd> command, try to raise Zookeeper on a port in use.</p>
<p>If you have doubts about the configuration, or it is not clear what values to change, the <kbd>server.properties</kbd> template (as all of the Kafka project) is open sourced in the following:</p>
<p><a href="https://github.com/apache/kafka/blob/trunk/config/server.properties">https://github.com/apache/kafka/blob/trunk/config/server.properties</a></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Kafka topics</h1>
                </header>
            
            <article>
                
<p>The power inside a broker is the topic, namely the queues inside it. Now that we have two brokers running, let's create a Kafka topic on them.</p>
<p>Kafka, like almost all modern infrastructure projects, has three ways of building things: through the command line, through programming, and through a web console (in this case the Confluent Control Center). The management (creation, modification, and destruction) of Kafka brokers can be done through programs written in most modern programming languages. If the language is not supported, it could be managed through the Kafka REST API. The previous section showed how to build a broker using the command line. In later chapters, we will see how to do this process through programming.</p>
<p>Is it possible to only manage (create, modify, or destroy) brokers through programming? No, we can also manage the topics. The topics can also be created through the command line. Kafka has pre-built utilities to manage brokers as we already saw and to manage topics, as we will see next.</p>
<p>To create a topic called <kbd>amazingTopic</kbd> in our running cluster, use the following command:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic amazingTopic</strong></pre>
<p>The output should be as follows:</p>
<pre><strong>Created topic amazingTopic</strong></pre>
<p>Here, the <kbd>kafka-topics</kbd> command is used. With the <kbd>--create</kbd> parameter it is specified that we want to create a new topic. The <kbd>--topic</kbd> parameter sets the name of the topic, in this case, <kbd>amazingTopic</kbd>.</p>
<p>Do you remember the terms parallelism and redundancy? Well, the <kbd>–-partitions</kbd> parameter controls the parallelism and the <kbd>--replication-factor</kbd> parameter controls the redundancy.</p>
<p>The <kbd>--replication-factor</kbd> parameter is fundamental as it specifies in how many servers of the cluster the topic is going to replicate (for example, running). On the other hand, one broker can run just one replica.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Obviously, if a greater number than the number of running servers on the cluster is specified, it will result in an error (you don't believe me? Try it in your environment). The error will be like this:</p>
<pre>Error while executing topic command: replication factor: 3 larger than available brokers: 2<br/><br/>[2018-09-01 07:13:31,350] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: replication factor: 3 larger than available brokers: 2<br/><br/>(kafka.admin.TopicCommand$)</pre>
<p>To be considered, the broker should be running (don't be shy and test all this theory in your environment).</p>
<p>The <kbd>--partitions</kbd> parameter, as its name implies, says how many partitions the topic will have. The number determines the parallelism that can be achieved on the consumer's side. This parameter is very important when doing cluster fine-tuning.</p>
<p>Finally, as expected, the <kbd>--zookeeper</kbd> parameter indicates where the Zookeeper cluster is running.</p>
<p>When a topic is created, the output in the broker log is something like this:</p>
<pre>[2018-09-01 07:05:53,910] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions amazingTopic-0 (kafka.server.ReplicaFetcherManager)<br/><br/>[2018-09-01 07:05:53,950] INFO Completed load of log amazingTopic-0 with 1 log segments and log end offset 0 in 21 ms (kafka.log.Log)</pre>
<p>In short, this message reads like a new topic has been born in our cluster.</p>
<p>How can I check my new and shiny topic? By using the same command: <kbd>kafka-topics</kbd><span>.</span></p>
<p>There are more parameters than <kbd>--create</kbd>. To check the status of a topic, run the <kbd>kafka-topics</kbd> command with the <kbd>--list</kbd> parameter, as follows:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-topics.sh --list --zookeeper localhost:2181</strong></pre>
<p>The output is the list of topics, as we know, is as follows:</p>
<pre><strong>amazingTopic</strong></pre>
<p>This command returns the list with the names of all of the running topics in the cluster. </p>
<p class="mce-root"/>
<p>How can I get details of a topic? Using the same command: <kbd>kafka-topics</kbd><span>.</span></p>
<p>For a particular topic, run the <kbd>kafka-topics</kbd> command with the <kbd>--describe</kbd> parameter, as follows:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-topics --describe --zookeeper localhost:2181 --topic amazingTopic</strong></pre>
<p>The command output is as follows:</p>
<pre><strong>Topic:amazingTopic PartitionCount:1 ReplicationFactor:1 Configs: Topic: amazingTopic Partition: 0 Leader: 1 Replicas: 1 Isr: 1</strong></pre>
<p>Here is a brief explanation of the output:</p>
<ul>
<li><kbd>PartitionCount</kbd>: Number of partitions on the topic (parallelism)</li>
<li><kbd>ReplicationFactor</kbd>: Number of replicas on the topic (redundancy)</li>
<li><kbd>Leader</kbd>: Node responsible for reading and writing operations of a given partition</li>
<li><kbd>Replicas</kbd>: List of brokers replicating this topic data; some of these might even be dead</li>
<li><kbd>Isr</kbd>: List of nodes that are currently in-sync replicas</li>
</ul>
<p>Let's create a topic with multiple replicas (for example, we will run with more brokers in the cluster); we type the following:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 2 --partitions 1 --topic redundantTopic</strong></pre>
<p>The output is as follows:</p>
<pre><strong>Created topic redundantTopic</strong></pre>
<p>Now, call the <kbd>kafka-topics</kbd> command with the <kbd>--describe</kbd> parameter to check the topic details, as follows:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-topics --describe --zookeeper localhost:2181 --topic redundantTopic</strong><br/><br/><br/><strong>Topic:redundantTopic PartitionCount:1 ReplicationFactor:2 Configs:</strong><br/><br/><strong>Topic: redundantTopic Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2</strong></pre>
<p>As you can see, <kbd>Replicas</kbd> and <kbd>Isr</kbd> are the same lists; we infer that all of the nodes are in-sync.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Your turn: play with the <kbd>kafka-topics</kbd> command, and try to create replicated topics on dead brokers and see the output. Also, create topics on running servers and then kill them to see the results. Was the output what you expected?</p>
<p>As mentioned before, all of these commands executed through the command line can be executed programmatically or performed through the Confluent Control Center web console.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A command-line message producer</h1>
                </header>
            
            <article>
                
<p>Kafka also has a command to send messages through the command line; the input can be a text file or the console standard input. Each line typed in the input is sent as a single message to the cluster.</p>
<p>For this section, the execution of the previous steps <span>is needed</span>. The Kafka brokers must be up and running and a topic created inside them.</p>
<p>In a new command-line window, run the following command, followed by the lines to be sent as messages to the server:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-console-producer --broker-list localhost:9093 --topic amazingTopic<br/></strong><br/><strong>Fool me once shame on you</strong><br/><strong>Fool me twice shame on me</strong></pre>
<p>These lines push two messages into the <kbd>amazingTopic</kbd> running on the localhost cluster on the <kbd>9093</kbd> <span>port.</span></p>
<p>This command is also the simplest way to check whether a broker with a specific topic is up and running as it is expected.</p>
<p>As we can see, the <kbd>kafka-console-producer</kbd> command receives the following parameters:</p>
<ul>
<li><kbd>--broker-list</kbd>: This specifies the Zookeeper servers specified as a comma-separated list in the form, hostname:port.</li>
<li><kbd>--topic</kbd>: This parameter is followed by the name of the target topic.</li>
<li><kbd>--sync</kbd>: This specifies whether the messages should be sent synchronously.</li>
<li><kbd>--compression-codec</kbd>: This specifies the compression codec used to produce the messages. The possible options are: <kbd>none</kbd>, <kbd>gzip</kbd>, <kbd>snappy</kbd>, or lz4. If not specified, the default is gzip.</li>
<li><kbd>--batch-size</kbd>: If the messages are not sent synchronously, but the message size is sent in a single batch, this value is specified in bytes.</li>
<li><kbd>--message-send-max-retries</kbd>: As the brokers can fail receiving messages, this parameter specifies the number of retries before a producer gives up and drops the message. This number must be a positive integer.</li>
<li><kbd>--retry-backoff-ms</kbd>: In case of failure, the node leader election might take some time. This parameter is the time to wait before producer retries after this election. The number is the time in milliseconds.</li>
<li><kbd>--timeout</kbd>: If the producer is running in asynchronous mode and this parameter is set, it indicates the maximum amount of time a message will queue awaiting for the sufficient batch size. This value is expressed in milliseconds.</li>
<li><kbd>--queue-size</kbd>: If the producer is running in asynchronous mode and this parameter is set, it gives the maximum amount of messages will queue awaiting the sufficient batch size.</li>
</ul>
<p>In case of a server fine tuning, <kbd>batch-size</kbd>, <kbd>message-send-max-retries</kbd>, and <kbd>retry-backoff-ms</kbd> are very important; take in consideration these parameters to achieve the desired behavior.</p>
<p>If you don't want to type the messages, the command could receive a file where each line is considered a message, as shown in the following example:</p>
<pre><strong>&lt;confluent-path&gt;/bin/kafka-console-producer --broker-list localhost:9093 –topic amazingTopic &lt; aLotOfWordsToTell.txt</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A command-line message consumer</h1>
                </header>
            
            <article>
                
<p>The last step is how to read the generated messages. Kafka also has a powerful command that enables messages to be consumed from the command line. Remember that all of these command-line tasks can also be done programmatically. As the producer, each line in the input is considered a message from the producer. </p>
<p>For this section, the execution of the previous steps <span>is needed</span>. The Kafka brokers must be up and running and a topic created inside them. Also, some messages need to be produced with the message console producer, to begin consuming these messages from the console.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Run the following command:</p>
<pre><strong>&gt; &lt;confluent-path&gt;/bin/kafka-console-consumer --topic amazingTopic --bootstrap-server localhost:9093 --from-beginning</strong></pre>
<p>The output should be as follows:</p>
<pre><strong>Fool me once shame on you</strong><br/><strong>Fool me twice shame on me</strong></pre>
<p>The parameters are the topic's name and the name of the broker producer. Also, the <kbd>--from-beginning</kbd> parameter indicates that messages should be consumed from the beginning instead of the last messages in the log (now test it, generate many more messages, and don't specify this parameter).</p>
<p>There are more useful parameters for this command, some important ones are as follows:</p>
<ul>
<li><kbd>--fetch-size</kbd>: This is the amount of data to be fetched in a single request. The size in bytes follows as argument. The default value is 1,024 x 1,024.</li>
<li><kbd>--socket-buffer-size</kbd>: This is the size of the TCP RECV. The size in bytes follows this parameter. The default value is 2 x 1024 x 1024.</li>
<li><kbd>--formater</kbd>: This is the name of the class to use for formatting messages for display. The default value is <kbd>NewlineMessageFormatter</kbd>.</li>
<li><kbd>--autocommit.interval.ms</kbd>: This is the time interval at which to save the current offset in milliseconds. The time in milliseconds follows as argument. The default value is 10,000.</li>
<li><kbd>--max-messages</kbd>: This is the maximum number of messages to consume before exiting. If not set, the consumption is continuous. The number of messages follows as the argument.</li>
<li><kbd>--skip-message-on-error</kbd>: If there is an error while processing a message, the system should skip it instead of halting.</li>
</ul>
<p>The most requested forms of this command are as follows:</p>
<ul>
<li>To consume just one message, use the following:</li>
</ul>
<pre><strong>      &gt; &lt;confluent-path&gt;/bin/kafka-console-consumer --topic <br/>      amazingTopic --<br/>      bootstrap-server localhost:9093 --max-messages 1</strong></pre>
<p class="mce-root"/>
<ul>
<li>To consume one message from an offset<span>, use the following</span>:</li>
</ul>
<pre><strong>      &gt; &lt;confluent-path&gt;/bin/kafka-console-consumer --topic <br/>      amazingTopic --<br/>      bootstrap-server localhost:9093 --max-messages 1 --formatter <br/>      'kafka.coordinator.GroupMetadataManager$OffsetsMessageFormatter'</strong></pre>
<ul>
<li>To consume messages from a specific consumer group<span>, use the followi</span><span>ng</span>:</li>
</ul>
<pre><strong>      &lt;confluent-path&gt;/bin/kafka-console-consumer –topic amazingTopic -<br/>      - bootstrap-server localhost:9093 --new-consumer --consumer-<br/>      property <br/>      group.id=my-group</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using kafkacat</h1>
                </header>
            
            <article>
                
<p>kafkacat is a generic command-line non-JVM utility used to test and debug Apache Kafka deployments. kafkacat can be used to produce, consume, and list topic and partition information for Kafka. kafkacat is netcat for Kafka, and it is a tool for inspecting and creating data in Kafka.</p>
<p>kafkacat is similar to the Kafka console producer and Kafka console consumer, but more powerful.</p>
<p>kafkacat is an open source utility and it is not included in Confluent Platform. It is available at <a href="https://github.com/edenhill/kafkacat">https://github.com/edenhill/kafkacat</a>.</p>
<p>To install <kbd>kafkacat</kbd> on modern Linux, type the following:</p>
<pre><strong><span>apt-get install kafkacat</span></strong></pre>
<p>To install <kbd>kafkacat</kbd> on macOS with <kbd>brew,</kbd> type <span>the following</span>:</p>
<pre><strong><span>brew install kafkacat</span></strong></pre>
<p>To subscribe to <kbd>amazingTopic</kbd> and <kbd>redundantTopic</kbd> and print to <kbd>stdout</kbd>, type <span>the following</span>:</p>
<pre><strong>kafkacat -b localhost:9093 –t amazingTopic redundantTopic</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've learned what Kafka is, how to install and run Kafka in Linux and macOS and how to install and run Confluent Platform.</p>
<p>Also, we've reviewed how to run Kafka brokers and topics, how to run a command-line message producer and consumer, and how to use kafkacat.</p>
<p>In <a href="0f0c3c87-3860-4247-97b7-3ce070640dc3.xhtml" target="_blank"/><a href="0f0c3c87-3860-4247-97b7-3ce070640dc3.xhtml" target="_blank">Chapter 2</a>, <em>Message Validation</em>, we will analyze how to build a producer and a consumer from Java.</p>


            </article>

            
        </section>
    </body></html>