- en: Configuring Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter describes what Kafka is and the concepts related to this technology:
    brokers, topics, producers, and consumers. It also talks about how to build a
    simple producer and consumer from the command line, as well as how to install
    Confluent Platform. The information in this chapter is fundamental to the following
    chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka in a nutshell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Kafka (Linux and macOS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the Confluent Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Confluent Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Kafka brokers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Kafka topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A command–line message producer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A command–line message consumer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using kafkacat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka is an open source streaming platform. If you are reading this book,
    maybe you already know that Kafka scales very well in a horizontal way without
    compromising speed and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kafka core is written in Scala, and Kafka Streams and KSQL are written
    in Java. A Kafka server can run in several operating systems: Unix, Linux, macOS,
    and even Windows. As it usually runs in production on Linux servers, the examples
    in this book are designed to run on Linux environments. The examples in this book
    also consider bash environment usage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter explains how to install, configure, and run Kafka. As this is
    a Quick Start Guide, it does not cover Kafka''s theoretical details. At the moment,
    it is appropriate to mention these three points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kafka is a** **service bus**: To connect heterogeneous applications, we need
    to implement a message publication mechanism to send and receive messages among
    them. A message router is known as message broker. Kafka is a message broker,
    a solution to deal with routing messages among clients in a quick way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka architecture has two directives**: The first is to not block the producers
    (in order to deal with the back pressure). The second is to isolate producers
    and consumers. The producers should not know who their consumers are, hence Kafka
    follows the dumb broker and smart clients model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka is a real-time messaging system**:Moreover, Kafka is a software solution
    with a publish-subscribe model: open source, distributed, partitioned, replicated,
    and commit-log-based.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some concepts and nomenclature in Apache Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster**: This is a set of Kafka brokers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zookeeper**: This is a cluster coordinator—a tool with different services
    that are part of the Apache ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broker**: This is a Kafka server, also the Kafka server process itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic**: This is a queue (that has log partitions); a broker can run several
    topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offset**: This is an identifier for each message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition**: This is an immutable and ordered sequence of records continually
    appended to a structured commit log.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Producer**: This is the program that publishes data to topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer**: This is the program that processes data from the topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retention period**: This is the time to keep messages available for consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Kafka, there are three types of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Single node–single broker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single node–multiple broker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple node–multiple broker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Kafka, there are three (and just three) ways to deliver messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Never redelivered**: The messages may be lost because, once delivered, they
    are not sent again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**May be redelivered**: The messages are never lost because, if it is not received,
    the message can be sent again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delivered once**: The message is delivered exactly once. This is the most
    difficult form of delivery; since the message is only sent once and never redelivered,
    it implies that there is zero loss of any message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The message log can be compacted in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coarse-grained**: Log compacted by time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained**: Log compacted by message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three ways to install a Kafka environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the executable files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `brew` (in macOS) or `yum` (in Linux)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Confluent Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For all three ways, the first step is to install Java; we need Java 8\. Download
    and install the latest JDK 8 from the Oracle''s website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.oracle.com/technetwork/java/javase/downloads/index.html](http://www.oracle.com/technetwork/java/javase/downloads/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the latest Java 8 JDK version is 8u191.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Linux users :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the file mode to executable as follows, follows these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Go to the directory in which you want to install Java:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `rpm` installer with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Add to your environment the `JAVA_HOME` variable. The following command writes
    the `JAVA_HOME` environment variable to the `/etc/profile` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Validate the Java installation as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At the time of writing, the latest Scala version is 2.12.6\. To install Scala
    in Linux, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest Scala binary from [http://www.scala-lang.org/download](http://www.scala-lang.org/download)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the downloaded file, `scala-2.12.6.tgz`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `SCALA_HOME` variable to your environment as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the Scala bin directory to your `PATH` environment variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the Scala installation, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To install Kafka on your machine, ensure that you have at least 4 GB of RAM,
    and the installation directory will be `/usr/local/kafka/` for macOS users and
    `/opt/kafka/` for Linux users. Create these directories according to your operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka installation on Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open the Apache Kafka download page, [http://kafka.apache.org/downloads](http://kafka.apache.org/downloads),
    as in *Figure 1.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34e74b94-4c67-4c00-9148-ade9cdd2ddfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Apache Kafka download page'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the current Apache Kafka version is 2.0.0 as a stable
    release. Remember that, since version 0.8.x, Kafka is not backward-compatible.
    So, we cannot replace this version for one prior to 0.8\. Once you've downloaded
    the latest available release, let's proceed with the installation.
  prefs: []
  type: TYPE_NORMAL
- en: Remember for macOS users, replace the directory `/opt/` with `/usr/local`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to install Kafka in Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the downloaded file, `kafka_2.11-2.0.0.tgz`, in the `/opt/` directory
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `KAFKA_HOME` environment variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the Kafka bin directory to the `PATH` variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now Java, Scala, and Kafka are installed.
  prefs: []
  type: TYPE_NORMAL
- en: To do all of the previous steps from the command line, there is a powerful tool
    for macOS users called `brew` (the equivalent in Linux would be `yum`).
  prefs: []
  type: TYPE_NORMAL
- en: Kafka installation on macOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install from the command line in macOS (`brew` must be installed), perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `sbt` (the Scala build tool) with `brew`, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If already have it in your environment (downloaded previously), run the following
    to upgrade it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to that shown in *Figure 1.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb334cc9-9b67-41c5-9757-b1d11ff83232.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: The Scala build tool installation output'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Scala with `brew`, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you already have it in your environment (downloaded previously), to upgrade
    it, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to that shown in *Figure 1.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3c6c893-4b71-40a4-88a5-ab8149fdb1b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: The Scala installation output'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Kafka with `brew`, (it also installs Zookeeper), do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If you already have it (downloaded in the past), upgrade it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to that shown in *Figure 1.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d8c9dd3-7924-4ab3-b806-d3b568f8f721.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Kafka installation output'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://brew.sh/](https://brew.sh/) for more about `brew`.
  prefs: []
  type: TYPE_NORMAL
- en: Confluent Platform installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third way to install Kafka is through Confluent Platform. In the rest of
    this book, we will be using Confluent Platform open source version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Confluent Platform is an integrated platform that includes the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: REST proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schema Registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Streams API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-built connectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Java clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the reader notices, almost every one of the components has its own chapter
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The commercially licensed Confluent Platform includes, in addition to all of
    the components of the open source version, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confluent Control Center** (**CCC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka operator (for Kubernetes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JMS client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replicator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MQTT proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto data balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to mention that the training on the components of the non-open
    source version is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Confluent Platform is available also in Docker images, but here we are going
    to install it in local.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Confluent Platform download page: [https://www.confluent.io/download/](https://www.confluent.io/download/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of this writing, the current version of Confluent Platform is 5.0.0
    as a stable release. Remember that, since the Kafka core runs on Scala, there
    are two versions: for Scala 2.11 and Scala 2.12.'
  prefs: []
  type: TYPE_NORMAL
- en: We could run Confluent Platform from our desktop directory, but following this
    book's conventions, let's use `/opt/` for Linux users and `/usr/local` for macOS
    users.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Confluent Platform, extract the downloaded file, `confluent-5.0.0-2.11.tar.gz`,
    in the directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Running Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two ways to run Kafka, depending on whether we install it directly
    or through Confluent Platform.
  prefs: []
  type: TYPE_NORMAL
- en: If we install it directly, the steps to run Kafka are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: For macOS users, your paths might be different if you've installed using `brew`.
    Check the output of `brew install kafka` command for the exact command that you
    can use to start Zookeeper and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the Kafka installation directory (`/usr/local/kafka` for macOS users
    and `/opt/kafka/` for Linux users), as in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, we need to start Zookeeper (the Kafka dependency with Zookeeper
    is and will remain strong). Type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether Zookeeper is running, use the `lsof` command over the `9093`
    port (default port) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the Kafka server that comes with the installation by going to `/usr/local/kafka/`
    for macOS users and `/opt/kafka/` for Linux users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now there is an Apache Kafka broker running in your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that Zookeeper must be running on the machine before starting Kafka.
    If you don't want to start Zookeeper manually every time you need to run Kafka,
    install it as an operation system auto-start service.
  prefs: []
  type: TYPE_NORMAL
- en: Running Confluent Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the Confluent Platform installation directory (`/usr/local/kafka/` for
    macOS users and `/opt/kafka/` for Linux users) and type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To start Confluent Platform, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This command-line interface is intended for development only, not for production:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.confluent.io/current/cli/index.html](https://docs.confluent.io/current/cli/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is similar to what is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As indicated by the command output, Confluent Platform automatically starts
    in this order: Zookeeper, Kafka, Schema Registry, REST proxy, Kafka Connect, KSQL,
    and the Confluent Control Center.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the Confluent Control Center running in your local, go to `http://localhost:9021`,
    as shown in *Figure 1.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bb16534-016c-411f-9dc7-468c78e7e346.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Confluent Control Center main page'
  prefs: []
  type: TYPE_NORMAL
- en: There are other commands for Confluent Platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the status of all services or the status of a specific service along
    with its dependencies, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To stop all services or a specific service along with the services depending
    on it, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To delete the data and logs of the current Confluent Platform, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Running Kafka brokers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The real art behind a server is in its configuration. In this section, we will
    examine how to deal with the basic configuration of a Kafka broker in standalone
    mode. Since we are learning, at the moment, we will not review the cluster configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can suppose, there are two types of configuration: standalone and cluster.
    The real power of Kafka is unlocked when running with replication in cluster mode
    and all topics are correctly partitioned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cluster mode has two main advantages: parallelism and redundancy. Parallelism
    is the capacity to run tasks simultaneously among the cluster members. The redundancy
    warrants that, when a Kafka node goes down, the cluster is safe and accessible
    from the other running nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: This section shows how to configure a cluster with several nodes on our local
    machine although, in practice, it is always better to have several machines with
    multiple nodes sharing clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Confluent Platform installation directory, referenced from now on
    as `<confluent-path>`.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the beginning of this chapter, a broker is a server instance.
    A server (or broker) is actually a process running in the operating system and
    starts based on its configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The people of Confluent have kindly provided us with a template of a standard
    broker configuration. This file, which is called `server.properties`, is located
    in the Kafka installation directory in the `config` subdirectory:'
  prefs: []
  type: TYPE_NORMAL
- en: Inside `<confluent-path>`, make a directory with the name mark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each Kafka broker (server) that we want to run, we need to make a copy
    of the configuration file template and rename it accordingly. In this example,
    our cluster is going to be called `mark`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify each properties file accordingly. If the file is called `mark-1`, the
    `broker.id` should be `1`. Then, specify the port in which the server will run;
    the recommendation is `9093` for `mark-1` and `9094` for `mark-2`. Note that the
    port property is not set in the template, so add the line. Finally, specify the
    location of the Kafka logs (a Kafka log is a specific archive to store all of
    the Kafka broker operations); in this case, we use the `/tmp` directory. Here,
    it is common to have problems with write permissions. Do not forget to give write
    and execute permissions to the user with whom these processes are executed over
    the log directory, as in the examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `mark-1.properties`, set the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In `mark-2.properties`, set the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Kafka brokers using the `kafka-server-start` command with the corresponding
    configuration file passed as the parameter. Don''t forget that Confluent Platform
    must be already running and the ports should not be in use by another process.
    Start the Kafka brokers as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And, in another command-line window, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget that the trailing `&` is to specify that you want your command
    line back. If you want to see the broker output, it is recommended to run each
    command separately in its own command-line window.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the properties file contains the server configuration and that
    the `server.properties` file located in the `config` directory is just a template.
  prefs: []
  type: TYPE_NORMAL
- en: Now there are two brokers, `mark-1` and `mark-2` , running in the same machine
    in the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, there are no dumb questions, as in the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q**: How does each broker know which cluster it belongs to?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: The brokers know that they belong to the same cluster because, in the
    configuration, both point to the same Zookeeper cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q**: How does each broker differ from the others within the same cluster?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: Every broker is identified inside the cluster by the name specified
    in the `broker.id` property.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q**: What happens if the port number is not specified?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: If the port property is not specified, Zookeeper will assign the same
    port number and will overwrite the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q**: What happens if the log directory is not specified?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: If `log.dir` is not specified, all the brokers will write to the same
    default `log.dir`. If the brokers are planned to run in different machines, then
    the port and `log.dir` properties might not be specified (because they run in
    the same port and log file but in different machines).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q**: How can I check that there is not a process already running in the port
    where I want to start my broker?'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: As shown in the previous section, there is a useful command to see what
    process is running on specific port, in this case the `9093` port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Your turn: try to run this command before starting the Kafka brokers, and run
    it after starting them to see the change. Also, try to start a broker on a port
    in use to see how it fails.'
  prefs: []
  type: TYPE_NORMAL
- en: OK, what if I want my cluster to run on several machines?
  prefs: []
  type: TYPE_NORMAL
- en: 'To run Kafka nodes on different machines but in the same cluster, adjust the
    Zookeeper connection string in the configuration file; its default value is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the machines must be able to be found by each other by DNS and
    that there are no network security restrictions between them.
  prefs: []
  type: TYPE_NORMAL
- en: The default value for Zookeeper connect is correct only if you are running the
    Kafka broker in the same machine as Zookeeper. Depending on the architecture,
    it will be necessary to decide if there will be a broker running on the same Zookeeper
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify that Zookeeper might run in other machines, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The previous line specifies that Zookeeper is running in the local host machine
    on port `2181`, in the machine with IP address `192.168.0.2` on port `2183` ,
    and in the machine with IP address, the `192.168.0.3`, on port `2182`. The Zookeeper
    default port is `2181`, so normally it runs there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your turn: as an exercise, try to start a broker with incorrect information
    about the Zookeeper cluster. Also, using the `lsof` command, try to raise Zookeeper
    on a port in use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have doubts about the configuration, or it is not clear what values
    to change, the `server.properties` template (as all of the Kafka project) is open
    sourced in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/apache/kafka/blob/trunk/config/server.properties](https://github.com/apache/kafka/blob/trunk/config/server.properties)'
  prefs: []
  type: TYPE_NORMAL
- en: Running Kafka topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The power inside a broker is the topic, namely the queues inside it. Now that
    we have two brokers running, let's create a Kafka topic on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka, like almost all modern infrastructure projects, has three ways of building
    things: through the command line, through programming, and through a web console
    (in this case the Confluent Control Center). The management (creation, modification,
    and destruction) of Kafka brokers can be done through programs written in most
    modern programming languages. If the language is not supported, it could be managed
    through the Kafka REST API. The previous section showed how to build a broker
    using the command line. In later chapters, we will see how to do this process
    through programming.'
  prefs: []
  type: TYPE_NORMAL
- en: Is it possible to only manage (create, modify, or destroy) brokers through programming?
    No, we can also manage the topics. The topics can also be created through the
    command line. Kafka has pre-built utilities to manage brokers as we already saw
    and to manage topics, as we will see next.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a topic called `amazingTopic` in our running cluster, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `kafka-topics` command is used. With the `--create` parameter it is
    specified that we want to create a new topic. The `--topic` parameter sets the
    name of the topic, in this case, `amazingTopic`.
  prefs: []
  type: TYPE_NORMAL
- en: Do you remember the terms parallelism and redundancy? Well, the `–-partitions`
    parameter controls the parallelism and the `--replication-factor` parameter controls
    the redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: The `--replication-factor` parameter is fundamental as it specifies in how many
    servers of the cluster the topic is going to replicate (for example, running).
    On the other hand, one broker can run just one replica.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, if a greater number than the number of running servers on the cluster
    is specified, it will result in an error (you don''t believe me? Try it in your
    environment). The error will be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: To be considered, the broker should be running (don't be shy and test all this
    theory in your environment).
  prefs: []
  type: TYPE_NORMAL
- en: The `--partitions` parameter, as its name implies, says how many partitions
    the topic will have. The number determines the parallelism that can be achieved
    on the consumer's side. This parameter is very important when doing cluster fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as expected, the `--zookeeper` parameter indicates where the Zookeeper
    cluster is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a topic is created, the output in the broker log is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In short, this message reads like a new topic has been born in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can I check my new and shiny topic? By using the same command: `kafka-topics`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more parameters than `--create`. To check the status of a topic,
    run the `kafka-topics` command with the `--list` parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the list of topics, as we know, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This command returns the list with the names of all of the running topics in
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can I get details of a topic? Using the same command: `kafka-topics`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a particular topic, run the `kafka-topics` command with the `--describe`
    parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The command output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a brief explanation of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PartitionCount`: Number of partitions on the topic (parallelism)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReplicationFactor`: Number of replicas on the topic (redundancy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Leader`: Node responsible for reading and writing operations of a given partition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Replicas`: List of brokers replicating this topic data; some of these might
    even be dead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Isr`: List of nodes that are currently in-sync replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s create a topic with multiple replicas (for example, we will run with
    more brokers in the cluster); we type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, call the `kafka-topics` command with the `--describe` parameter to check
    the topic details, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `Replicas` and `Isr` are the same lists; we infer that all of
    the nodes are in-sync.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your turn: play with the `kafka-topics` command, and try to create replicated
    topics on dead brokers and see the output. Also, create topics on running servers
    and then kill them to see the results. Was the output what you expected?'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, all of these commands executed through the command line
    can be executed programmatically or performed through the Confluent Control Center
    web console.
  prefs: []
  type: TYPE_NORMAL
- en: A command-line message producer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka also has a command to send messages through the command line; the input
    can be a text file or the console standard input. Each line typed in the input
    is sent as a single message to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For this section, the execution of the previous steps is needed. The Kafka brokers
    must be up and running and a topic created inside them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new command-line window, run the following command, followed by the lines
    to be sent as messages to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: These lines push two messages into the `amazingTopic` running on the localhost
    cluster on the `9093` port.
  prefs: []
  type: TYPE_NORMAL
- en: This command is also the simplest way to check whether a broker with a specific
    topic is up and running as it is expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the `kafka-console-producer` command receives the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--broker-list`: This specifies the Zookeeper servers specified as a comma-separated
    list in the form, hostname:port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--topic`: This parameter is followed by the name of the target topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--sync`: This specifies whether the messages should be sent synchronously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--compression-codec`: This specifies the compression codec used to produce
    the messages. The possible options are: `none`, `gzip`, `snappy`, or lz4\. If
    not specified, the default is gzip.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--batch-size`: If the messages are not sent synchronously, but the message
    size is sent in a single batch, this value is specified in bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--message-send-max-retries`: As the brokers can fail receiving messages, this
    parameter specifies the number of retries before a producer gives up and drops
    the message. This number must be a positive integer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--retry-backoff-ms`: In case of failure, the node leader election might take
    some time. This parameter is the time to wait before producer retries after this
    election. The number is the time in milliseconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--timeout`: If the producer is running in asynchronous mode and this parameter
    is set, it indicates the maximum amount of time a message will queue awaiting
    for the sufficient batch size. This value is expressed in milliseconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--queue-size`: If the producer is running in asynchronous mode and this parameter
    is set, it gives the maximum amount of messages will queue awaiting the sufficient
    batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case of a server fine tuning, `batch-size`, `message-send-max-retries`, and
    `retry-backoff-ms` are very important; take in consideration these parameters
    to achieve the desired behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t want to type the messages, the command could receive a file where
    each line is considered a message, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: A command-line message consumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last step is how to read the generated messages. Kafka also has a powerful
    command that enables messages to be consumed from the command line. Remember that
    all of these command-line tasks can also be done programmatically. As the producer,
    each line in the input is considered a message from the producer.
  prefs: []
  type: TYPE_NORMAL
- en: For this section, the execution of the previous steps is needed. The Kafka brokers
    must be up and running and a topic created inside them. Also, some messages need
    to be produced with the message console producer, to begin consuming these messages
    from the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The parameters are the topic's name and the name of the broker producer. Also,
    the `--from-beginning` parameter indicates that messages should be consumed from
    the beginning instead of the last messages in the log (now test it, generate many
    more messages, and don't specify this parameter).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more useful parameters for this command, some important ones are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--fetch-size`: This is the amount of data to be fetched in a single request.
    The size in bytes follows as argument. The default value is 1,024 x 1,024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--socket-buffer-size`: This is the size of the TCP RECV. The size in bytes
    follows this parameter. The default value is 2 x 1024 x 1024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--formater`: This is the name of the class to use for formatting messages
    for display. The default value is `NewlineMessageFormatter`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--autocommit.interval.ms`: This is the time interval at which to save the
    current offset in milliseconds. The time in milliseconds follows as argument.
    The default value is 10,000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--max-messages`: This is the maximum number of messages to consume before
    exiting. If not set, the consumption is continuous. The number of messages follows
    as the argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--skip-message-on-error`: If there is an error while processing a message,
    the system should skip it instead of halting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most requested forms of this command are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To consume just one message, use the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To consume one message from an offset, use the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To consume messages from a specific consumer group, use the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Using kafkacat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: kafkacat is a generic command-line non-JVM utility used to test and debug Apache
    Kafka deployments. kafkacat can be used to produce, consume, and list topic and
    partition information for Kafka. kafkacat is netcat for Kafka, and it is a tool
    for inspecting and creating data in Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: kafkacat is similar to the Kafka console producer and Kafka console consumer,
    but more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: kafkacat is an open source utility and it is not included in Confluent Platform.
    It is available at [https://github.com/edenhill/kafkacat](https://github.com/edenhill/kafkacat).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `kafkacat` on modern Linux, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'To install `kafkacat` on macOS with `brew,` type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To subscribe to `amazingTopic` and `redundantTopic` and print to `stdout`,
    type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned what Kafka is, how to install and run Kafka in
    Linux and macOS and how to install and run Confluent Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we've reviewed how to run Kafka brokers and topics, how to run a command-line
    message producer and consumer, and how to use kafkacat.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](0f0c3c87-3860-4247-97b7-3ce070640dc3.xhtml), *Message Validation*,
    we will analyze how to build a producer and a consumer from Java.
  prefs: []
  type: TYPE_NORMAL
