<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Host Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Host Performance</h1></div></div></div><p>In the previous chapters, we noted how Clojure interoperates with Java. In this chapter we will go a bit deeper to understand the internals better. We will touch upon several layers of the entire stack, but our major focus will be the JVM, in particular the Oracle HotSpot JVM, though there are several <a class="indexterm" id="id215"/>JVM vendors to choose from (<a class="ulink" href="http://en.wikipedia.org/wiki/List_of_Java_virtual_machines">http://en.wikipedia.org/wiki/List_of_Java_virtual_machines</a>). At the time of writing this, Oracle JDK 1.8 is the latest stable release and early OpenJDK 1.9 builds are available. In this chapter we will discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How the hardware subsystems function from the performance viewpoint</li><li class="listitem" style="list-style-type: disc">Organization of the JVM internals and how that is related to performance</li><li class="listitem" style="list-style-type: disc">How to measure the amount of space occupied by various objects in the heap</li><li class="listitem" style="list-style-type: disc">Profile Clojure code for latency using Criterium</li></ul></div><div class="section" title="The hardware"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>The hardware</h1></div></div></div><p>There are various hardware components<a class="indexterm" id="id216"/> that may impact the performance of software in different ways. The processors, caches, memory subsystem, I/O subsystems, and so on, all have varying degrees of performance impact depending upon the use cases. In the following sections we look into each of those aspects.</p><div class="section" title="Processors"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Processors</h2></div></div></div><p>Since about the late 1980s, microprocessors <a class="indexterm" id="id217"/>have been employing pipelining and <a class="indexterm" id="id218"/>instruction-level parallelism to speed up their performance. Processing an instruction at the <a class="indexterm" id="id219"/>CPU level consists of typically four cycles: <span class="strong"><strong>fetch</strong></span>, <a class="indexterm" id="id220"/>
<span class="strong"><strong>decode</strong></span>, <a class="indexterm" id="id221"/>
<span class="strong"><strong>execute</strong></span>, and<a class="indexterm" id="id222"/> <span class="strong"><strong>writeback</strong></span>. Modern processors optimize the cycles by running them in parallel—while one instruction is executed, the next instruction is being decoded, and the one after that is being fetched, and so on. This style is called<a class="indexterm" id="id223"/> <span class="strong"><strong>instruction pipelining</strong></span>.</p><p>In practice, in order to speed<a class="indexterm" id="id224"/> up execution even further, the stages are subdivided into many shorter stages, thus leading to deeper super-pipeline architecture. The length of the longest stage in the pipeline limits the clock speed of the CPU. By splitting stages into substages, the processor can be run at a higher clock speed, where more cycles are required for each instruction, but the processor still completes one instruction per cycle. Since there are more cycles per second now, we get better performance in terms of throughput per second even though the latency of each instruction is now higher.</p><div class="section" title="Branch prediction"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec12"/>Branch prediction</h3></div></div></div><p>The <a class="indexterm" id="id225"/>processor <a class="indexterm" id="id226"/>must fetch and decode instructions in advance even when it encounters instructions of the conditional <code class="literal">if-then</code> form. Consider an equivalent of the (<code class="literal">if (test a) (foo a) (bar a)</code>) Clojure expression. The processor must choose a branch to fetch and decode, the question is should it fetch the <code class="literal">if</code> branch or the <code class="literal">else</code> branch? Here, the processor makes a guess as to which instruction to fetch/decode. If the guess turns out to be correct, it is a performance gain as usual; otherwise, the processor has to throw away the result of the fetch/decode process and start on the other branch afresh.</p><p>Processors deal with branch prediction using an on-chip branch prediction table. It contains recent code branches and two bits per branch, indicating whether or not the branch was taken, while also accommodating one-off, not-taken occurrences.</p><p>Today, branch prediction is extremely important in processors for performance, so modern processors dedicate hardware resources and special predication instructions to improve the prediction accuracy and lower the cost of a mispredict penalty.</p></div><div class="section" title="Instruction scheduling"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec13"/>Instruction scheduling</h3></div></div></div><p>High-latency <a class="indexterm" id="id227"/>instructions and branching usually lead to empty cycles in the instruction<a class="indexterm" id="id228"/> pipeline known as <span class="strong"><strong>stalls</strong></span>
<a class="indexterm" id="id229"/> or<a class="indexterm" id="id230"/> <span class="strong"><strong>bubbles</strong></span>. These cycles are often used to do other work by the means of instruction reordering. Instruction reordering is implemented at the hardware level via out of order execution and at the compiler level via compile time instruction scheduling (also called <a class="indexterm" id="id231"/>
<span class="strong"><strong>static instruction scheduling</strong></span>).</p><p>The processor needs to remember the dependencies between instructions when carrying out the out-of-order execution. This cost is somewhat mitigated by using renamed registers, wherein register values are stored into / loaded from memory locations, potentially on different physical registers, so that they can be executed in parallel. This necessitates that out-of-order processors always maintain a mapping of instructions and corresponding registers they use, which makes their design complex and power hungry. With a few exceptions, almost all high-performance CPUs today have out-of-order designs.</p><p>Good compilers are usually extremely aware of processors, and are capable of optimizing the code by rearranging processor instructions in a way that there are fewer bubbles in the processor instruction pipeline. A<a class="indexterm" id="id232"/> few high-performance CPUs still rely on only static instruction reordering instead of out-of-order instruction reordering and, in turn, save chip area due to simpler design—the saved area is used to accommodate extra cache or CPU cores. Low-power processors, such as those from the ARM and Atom family, use in-order design. Unlike most CPUs, the modern GPUs use in-order design with deep pipelines, which is compensated by very fast context switching. This leads to high latency and high<a class="indexterm" id="id233"/> throughput on GPUs.</p></div><div class="section" title="Threads and cores"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec14"/>Threads and cores</h3></div></div></div><p>Concurrency and <a class="indexterm" id="id234"/>parallelism via context switches, hardware threads, and cores are very common today and we have accepted them as a norm to implement in our programs. However, we should understand why we needed such a design in the first place. Most of the<a class="indexterm" id="id235"/> real-world code we write today does not have more than a modest scope for<a class="indexterm" id="id236"/> instruction-level parallelism. Even with hardware-based, out-of-order execution and static instruction reordering, no more than two instructions per cycle are truly parallel. Hence, another potential source of instructions that can be pipelined and executed in parallel are the programs other than the currently running one.</p><p>The empty cycles in a pipeline can be<a class="indexterm" id="id237"/> dedicated to other running programs, which assume that there are other currently running programs that need the processor's attention. <span class="strong"><strong>Simultaneous multithreading</strong></span> (<span class="strong"><strong>SMT</strong></span>)<a class="indexterm" id="id238"/> is a hardware design that enables such kinds of parallelism. Intel implements SMT named as <span class="strong"><strong>HyperThreading</strong></span>
<a class="indexterm" id="id239"/> in some of its processors. While SMT presents a single physical processor as two or more logical processors, a true multiprocessor system executes one thread per processor, thus achieving simultaneous execution. A multicore processor includes two or more processors per chip, but has the properties of a multiprocessor system.</p><p>In general, multicore processors significantly outperform SMT processors. Performance on SMT processors can vary by the use case. It peaks in those cases where code is highly variable or threads do not compete for the same hardware resources, and dips when the threads are cache-bound on the same processor. What is also important is that some programs are simply not inherently parallel. In such cases it may be hard to make them go faster without the explicit use of threads in the program.</p></div></div><div class="section" title="Memory systems"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec45"/>Memory systems</h2></div></div></div><p>It is important to <a class="indexterm" id="id240"/>understand the <a class="indexterm" id="id241"/>memory performance characteristics to know the likely impact on the programs we write. Data-intensive programs that are also inherently parallel, such as audio/video processing and scientific computation, are largely limited by memory bandwidth, not by the processor. Adding processors would not make them faster unless the memory bandwidth is also increased. Consider another class of programs, such as 3D graphics rendering or database systems that are limited mainly by memory latency but not the memory bandwidth. SMT can be highly suitable for such programs, where threads do not compete for the same hardware resources.</p><p>Memory access roughly constitutes a quarter of all instructions executed by a processor. A code block typically begins with memory-load instructions and the remainder portion depends on the loaded data. This stalls the instructions and prevents large-scale, instruction-level parallelism. As if that was not bad enough, even superscalar processors (which can issue more than one instruction per clock cycle) can issue, at most, two memory instructions per cycle. Building fast memory systems is limited by natural factors such as the speed of light. It impacts the signal round trip to the RAM. This is a natural hard limit and any optimization can only work around it.</p><p>Data transfer between the processor and motherboard chipset is one of the factors that induce memory latency. This is countered using a <a class="indexterm" id="id242"/>
<span class="strong"><strong>faster front-side bus</strong></span> (<span class="strong"><strong>FSB</strong></span>). Nowadays, most modern processors fix this problem by integrating the memory controller directly at the chip level. The significant difference between the processor versus memory latencies is known as the <a class="indexterm" id="id243"/>
<span class="strong"><strong>memory wall</strong></span>. This has plateaued in recent times due to processor clock speeds hitting power and heat limits, but notwithstanding this, memory latency continues to be a significant problem.</p><p>Unlike CPUs, GPUs typically realize a sustained high-memory bandwidth. Due to latency hiding, they utilize the bandwidth even during a high number-crunching workload.</p><div class="section" title="Cache"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec15"/>Cache</h3></div></div></div><p>To overcome the memory <a class="indexterm" id="id244"/>latency, modern processors employ a special type of very <a class="indexterm" id="id245"/>fast memory placed onto the processor chip or close to the chip. The purpose of the cache is to store the most recently used data from the memory. Caches are of different levels: <span class="strong"><strong>L1</strong></span> cache is located on the processor chip; <span class="strong"><strong>L2</strong></span> cache is bigger and located farther away from the processor compared to L1. There is often an <span class="strong"><strong>L3</strong></span> cache, which is even bigger and located farther from the processor than L2. In Intel's Haswell processor, the L1 cache is generally 64 kilobytes (32 KB instruction plus 32 KB data) in size, L2 is 256 KB per core, and L3 is 8 MB.</p><p>While memory latency is very bad, fortunately caches seem to work very well. The L1 cache is much faster than accessing the main memory. The reported cache hit rates in real-world programs is 90 percent, which makes a strong case for caches. A cache works like a dictionary of memory addresses to a block of data values. Since the value is a block of memory, the caching of<a class="indexterm" id="id246"/> adjacent memory locations has mostly no additional <a class="indexterm" id="id247"/>overhead. Note that L2 is slower and bigger than L1, and L3 is slower and bigger than L2. On Intel Sandybridge processors, register lookup is instantaneous; L1 cache lookup takes three clock cycles, L2 takes nine, L3 takes 21, and main memory access takes 150 to 400 clock cycles.</p></div><div class="section" title="Interconnect"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec16"/>Interconnect</h3></div></div></div><p>A processor<a class="indexterm" id="id248"/> communicates with the memory and other processors via interconnect that are generally of two types of architecture: <span class="strong"><strong>Symmetric multiprocessing</strong></span> (<span class="strong"><strong>SMP</strong></span>) <a class="indexterm" id="id249"/>and <a class="indexterm" id="id250"/>
<span class="strong"><strong>Non-uniform memory access</strong></span> (<span class="strong"><strong>NUMA</strong></span>). In SMP, a bus interconnects processors and memory with the help of bus controllers. The bus acts as a broadcast device for the end points. The bus often becomes a bottleneck with a large number of processors and memory banks. SMP systems are cheaper to build and harder to scale to a large number of cores compared to NUMA. In a NUMA system, collections of processors and memory are connected point to point to other such groups of processors and memory. Every such group is called a node. Local memory of a node is accessible by other nodes and vice versa. Intel's <span class="strong"><strong>HyperTransport</strong></span> <a class="indexterm" id="id251"/>and <span class="strong"><strong>QuickPath</strong></span>
<a class="indexterm" id="id252"/> interconnect technologies support NUMA.</p></div></div><div class="section" title="Storage and networking"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec46"/>Storage and networking</h2></div></div></div><p>Storage<a class="indexterm" id="id253"/> and networking<a class="indexterm" id="id254"/> are the most commonly used hardware <a class="indexterm" id="id255"/>components besides the processor, cache, and memory. Many of<a class="indexterm" id="id256"/> the real-world applications are more often I/O bound than execution-bound. Such I/O technologies are continuously advancing and there is a wide variety of components available in the market. The consideration of such devices should be based on the exact performance and reliability characteristics for the use case. Another important criterion is to know how well they are supported by the target operating system drivers. Current day storage technologies mostly build upon hard disks and solid state drives. The applicability of network devices and protocols vary widely as per the business use case. A detailed discussion of I/O hardware is beyond the scope of this book.</p></div></div></div>
<div class="section" title="The Java Virtual Machine"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>The Java Virtual Machine</h1></div></div></div><p>The Java Virtual Machine<a class="indexterm" id="id257"/> is a bytecode-oriented, garbage-collected virtual machine that specifies its own instruction set. The instructions have equivalent bytecodes that are interpreted and compiled to the underlying OS and hardware by the <a class="indexterm" id="id258"/>
<span class="strong"><strong>Java Runtime Environment</strong></span> (<span class="strong"><strong>JRE</strong></span>). Objects are referred to using symbolic references. The data types in the JVM are fully standardized as a single spec across all JVM implementations on all platforms and architectures. The JVM also follows the network byte order, which means communication between Java programs on different architectures can happen using the big-endian byte order. <span class="strong"><strong>Jvmtop</strong></span> (<a class="ulink" href="https://code.google.com/p/jvmtop/">https://code.google.com/p/jvmtop/</a>) is a handy <a class="indexterm" id="id259"/>JVM monitoring tool similar to the top command in <a class="indexterm" id="id260"/>Unix-like systems.</p><div class="section" title="The just-in-time compiler"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec47"/>The just-in-time compiler</h2></div></div></div><p>The <span class="strong"><strong>just-in-time</strong></span> (<span class="strong"><strong>JIT</strong></span>) compiler<a class="indexterm" id="id261"/> is part of the JVM. When the JVM starts up, the JIT compiler knows hardly anything about the running code so it simply interprets the JVM bytecodes. As the program keeps running, the JIT compiler starts profiling the code by collecting statistics and analyzing the call and bytecode patterns. When a method call count exceeds a certain threshold, the JIT compiler applies a number of optimizations to the code. Most common optimizations are inlining and native code generating. The final and static methods and classes are great candidates for inlining. JIT compilation does not come without a cost; it occupies memory to store the profiled code and sometimes it has to revert the wrong speculative optimization. However, JIT compilation is almost always amortized over a long duration of code execution. In rare cases, turning off JIT compilation may be useful if either the code is too large or there are no hotspots in the code due to infrequent execution.</p><p>A JRE has typically two kinds of JIT compilers: client and server. Which JIT compiler is used by default depends on the type of hardware and platform. The client JIT compiler is meant for client programs such as command-line and desktop applications. We can start the JRE with the <code class="literal">-server</code> option to invoke the server JIT compiler, which is really meant for long-running programs on a server. The threshold for JIT compilation is higher in the server than the client. The difference in the two kinds of JIT compilers is that the client targets upfront, visible lower latency, and the server is assumed to be running on a high-resource hardware and tries to optimize for throughput.</p><p>The JIT compiler in the Oracle HotSpot JVM observes the code execution to determine the most frequently invoked methods, which are hotspots. Such hotspots are usually just a fraction of the entire code that can be cheap to focus on and optimize. The <a class="indexterm" id="id262"/>
<span class="strong"><strong>HotSpot JIT</strong></span> compiler is lazy and adaptive. It is lazy because it compiles only those methods to native code that have crossed a certain threshold, and not all the code that it encounters. Compiling to native code is a time-consuming process and compiling all code would be wasteful. It is adaptive to gradually increasing the aggressiveness of its compilation on frequently called code, which implies that the code is not optimized only once but many times over as the code gets executed repeatedly. After a method call crosses the first JIT compiler threshold, it is optimized and the counter is reset to zero. At the same time, the optimization count for the code is set to one. When the call exceeds the threshold yet again, the counter is reset to zero and the optimization count is incremented; and this time a more aggressive optimization is applied. This cycle continues until the<a class="indexterm" id="id263"/> code cannot be optimized anymore.</p><p>The HotSpot JIT compiler<a class="indexterm" id="id264"/> does a whole bunch of optimizations. Some of the most prominent ones are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Inlining</strong></span>: Inlining of methods—very small methods, the static and final methods, methods in final classes, and small methods involving only primitive numerics are prime candidates for inlining.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Lock elimination</strong></span>: Locking is a performance overhead. Fortunately, if the lock object monitor is not reachable from other threads, the lock is eliminated.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Virtual call elimination</strong></span>: Often, there is only one implementation for an interface in a program. The JIT compiler eliminates the virtual call and replaces that with a direct method call on the class implementation object.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Non-volatile memory write elimination</strong></span>: The non-volatile data members and references in an object are not guaranteed to be visible by the threads other than the current thread. This criterion is utilized not to update such references in memory and rather use hardware registers or the stack via native code.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Native code generation</strong></span>: The JIT compiler generates native code for frequently invoked methods together with the arguments. The generated native code is stored in the code cache.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Control flow and local optimizations</strong></span>: The JIT compiler frequently reorders and splits the code for better performance. It also analyzes the branching of control and optimizes code based on that.</li></ul></div><p>There should rarely be any reason to disable JIT compilation, but it can be done by passing the <code class="literal">-Djava.compiler=NONE</code> parameter when starting the JRE. The default compile threshold can be changed by passing <code class="literal">-XX:CompileThreshold=9800</code> to the JRE executable where <code class="literal">9800</code> is the example threshold. The <code class="literal">XX:+PrintCompilation</code> and <code class="literal">-XX:-CITime</code> options make the JIT compiler print the JIT statistics and time spent on JIT.</p></div><div class="section" title="Memory organization"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"/>Memory organization</h2></div></div></div><p>The memory <a class="indexterm" id="id265"/>used by the JVM is divided into several segments. JVM, being<a class="indexterm" id="id266"/> a stack-based execution model, one of the memory segments is the stack area. Every thread is given a stack where the stack frames are stored in <span class="strong"><strong>Last-in-First-out</strong></span> (<span class="strong"><strong>LIFO</strong></span>)<a class="indexterm" id="id267"/> order. The stack includes a <span class="strong"><strong>program counter</strong></span> (<span class="strong"><strong>PC</strong></span>)<a class="indexterm" id="id268"/> that points to the instruction in the JVM memory currently being executed. When a method is called, a new stack frame is created containing the local variable array and the operand stack. Contrary to conventional stacks, the operand stack holds instructions to load local variable / field values and computation results—a mechanism that is also used to prepare method parameters before a call and to store the return value. The stack frame itself may be allocated on the heap. The easiest way to inspect the order of stack frames in the current thread is to execute the following code:</p><div class="informalexample"><pre class="programlisting">(require 'clojure.repl)
(clojure.repl/pst (Throwable.))</pre></div><p>When a thread requires more stack space than what the JVM can provide, <code class="literal">StackOverflowError</code> is thrown.</p><p>The heap is the main memory area where the object and array allocations are done. It is shared across all JVM threads. The heap may be of a fixed size or expanding, depending on the arguments passed to the JRE on startup. Trying to allocate more heap space than what the JVM can make room for results in <code class="literal">OutOfMemoryError</code> to be thrown. The allocations in the heap are subject to garbage collection. When an object is no more reachable via any reference, it is garbage collected, with the notable exception of weak, soft, and phantom references. Objects pointed to by non-strong references take longer to GC.</p><p>The method area is logically a part of the heap memory and contains per-class structures such as the field and method information, runtime constant pool, code for method, and constructor bodies. It is shared across all JVM threads. In the Oracle HotSpot JVM (up to Version 7), the method area is found in a memory area called the <a class="indexterm" id="id269"/>
<span class="strong"><strong>permanent generation</strong></span>. In HotSpot Java 8, the permanent generation is replaced by a native memory area called <a class="indexterm" id="id270"/>
<span class="strong"><strong>Metaspace</strong></span>.</p><div class="mediaobject"><img alt="Memory organization" src="graphics/3642_04_01.jpg"/></div><p>The JVM contains the native code and the Java bytecode to be provided to the Java API implementation and the JVM implementation. The native code call stack is maintained separately for each thread stack. The JVM stack contains the Java method calls. Please note that the<a class="indexterm" id="id271"/> JVM spec for Java SE 7 and 8 does not imply a <a class="indexterm" id="id272"/>native method stack, but for Java SE 5 and 6, it does.</p></div><div class="section" title="HotSpot heap and garbage collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"/>HotSpot heap and garbage collection</h2></div></div></div><p>The <a class="indexterm" id="id273"/>Oracle HotSpot JVM uses a<a class="indexterm" id="id274"/> generational heap. The three main generations are <span class="strong"><strong>Young</strong></span>, <span class="strong"><strong>Tenured</strong></span> (old), and <span class="strong"><strong>Perm</strong></span> (permanent) (up to HotSpot JDK 1.7 only). As objects survive garbage<a class="indexterm" id="id275"/> collection, they move from <span class="strong"><strong>Eden</strong></span> to <span class="strong"><strong>Survivor</strong></span> and from <span class="strong"><strong>Survivor</strong></span> to <span class="strong"><strong>Tenured</strong></span> <a class="indexterm" id="id276"/>spaces. The new instances are allocated in the <span class="strong"><strong>Eden</strong></span> segment, which is a very cheap operation (as cheap as a pointer bump, and faster than a C <code class="literal">malloc</code> call), if it already has sufficient free space. When the Eden area does not have enough free space, a minor GC is triggered. This copies the live objects from <span class="strong"><strong>Eden</strong></span> into the <span class="strong"><strong>Survivor</strong></span> space. In the same operation, live objects are checked in <span class="strong"><strong>Survivor-1</strong></span> and copied over to <span class="strong"><strong>Survivor-2</strong></span>, thus keeping the live objects only in <span class="strong"><strong>Survivor-2</strong></span>. This scheme keeps <span class="strong"><strong>Eden</strong></span> and <span class="strong"><strong>Survivor-1</strong></span> empty and unfragmented to make new allocations, and is known<a class="indexterm" id="id277"/> as <span class="strong"><strong>copy collection</strong></span>.</p><div class="mediaobject"><img alt="HotSpot heap and garbage collection" src="graphics/3642_04_02.jpg"/></div><p>After a certain survival threshold in the young generation, the objects are moved to the tenured/old generation. If it is not possible to do a minor GC, a major GC is attempted. The major GC does not use copying, but rather relies on mark-and-sweep algorithms. We can use throughput collectors (<span class="strong"><strong>Serial</strong></span>, <span class="strong"><strong>Parallel</strong></span>, and <span class="strong"><strong>ParallelOld</strong></span>) or low-pause collectors (<span class="strong"><strong>Concurrent</strong></span> and <span class="strong"><strong>G1</strong></span>) for the old generation. The following table shows a non-exhaustive list of options to be used for each collector type:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Collector name</p>
</th><th style="text-align: left" valign="bottom">
<p>JVM flag</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Serial</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseSerialGC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Parallel</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseParallelGC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Parallel Compacting</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseParallelOldGC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Concurrent</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseConcMarkSweepGC</p>
<p>-XX:+UseParNewGC</p>
<p>-XX:+CMSParallelRemarkEnabled</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>G1</p>
</td><td style="text-align: left" valign="top">
<p>-XX:+UseG1GC</p>
</td></tr></tbody></table></div><p>The<a class="indexterm" id="id278"/> previously mentioned flags can be used to start the<a class="indexterm" id="id279"/> Java<a class="indexterm" id="id280"/> runtime. For example, in the following command, we<a class="indexterm" id="id281"/> start the server JVM with a 4 GB heap using Parallel compacting GC:</p><div class="informalexample"><pre class="programlisting">java \
  -server \
  -Xms4096m -Xmx4096m \
  -XX:+UseParallelOldGC XX:ParallelGCThreads=4 \
  -jar application-standalone.jar</pre></div><p>Sometimes, due to running full GC multiple times, the tenured space may have become so fragmented that it may not be feasible to move objects from Survivor to Tenured spaces. In those cases, a full GC with compaction is triggered. During this period, the application may appear unresponsive due to the full GC in action.</p></div><div class="section" title="Measuring memory (heap/stack) usage"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"/>Measuring memory (heap/stack) usage</h2></div></div></div><p>One of the <a class="indexterm" id="id282"/>prime reasons for a performance <a class="indexterm" id="id283"/>hit in the JVM is garbage collection. It certainly helps to know how heap memory is used by the objects we create and how to reduce the impact on GC by means of a lower footprint. Let's inspect how the representation of an object may lead to heap space.</p><p>Every (uncompressed) object or array reference on a 64-bit JVM is 16 bytes long. On a 32-bit JVM, every reference is 8 bytes long. As the 64-bit architecture is becoming more commonplace now, the 64-bit JVM is more likely to be used on servers. Fortunately, for a heap size of up to 32 GB, the JVM (Java 7) can use compressed pointers (default behavior) that are only 4 bytes in size. Java 8 VMs can address up to 64 GB heap size via compressed pointers as seen in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="top"> </th><th style="text-align: left" valign="bottom">
<p>Uncompressed</p>
</th><th style="text-align: left" valign="bottom">
<p>Compressed</p>
</th><th style="text-align: left" valign="bottom">
<p>32-bit</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Reference (pointer)</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Object header</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Array header</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>12</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Superclass padding</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr></tbody></table></div><p>This table illustrates pointer sizes in different modes (reproduced with permission from Attila Szegedi: <a class="ulink" href="http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20">http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20</a>).</p><p>We saw in the<a class="indexterm" id="id284"/> previous chapter how <a class="indexterm" id="id285"/>many bytes each primitive type takes. Let's see how the memory consumption of the composite types looks with compressed pointers (a common case) on a 64-bit JVM with a heap size smaller than 32 GB:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Java Expression</p>
</th><th style="text-align: left" valign="bottom">
<p>64-bit memory usage</p>
</th><th style="text-align: left" valign="bottom">
<p>Description (b = bytes, padding toward memory word size in approximate multiples of 8)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new Object()</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 4 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new byte[0]</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes</p>
</td><td style="text-align: left" valign="top">
<p>12 b <code class="literal">obj</code> header + 4 b <code class="literal">int</code> length = 16 b array header</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new String("foo")</code>
</p>
</td><td style="text-align: left" valign="top">
<p>40 bytes (interned for literals)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + (12 b array header + 6 b char-array content + 4 b length + 2 b padding = 24 b) + 4 b hash</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new Integer(3)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes (boxed integer)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 4 b <code class="literal">int</code> value</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">new Long(4)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>24 bytes (boxed long)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 8 b <code class="literal">long</code> value + 4 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">class A { byte x; }</code>
</p>
<p>
<code class="literal">new A();</code>
</p>
</td><td style="text-align: left" valign="top">
<p>16 bytes</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 1 b value + 3 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">class B extends A {byte y;}</code>
</p>
<p>
<code class="literal">new B();</code>
</p>
</td><td style="text-align: left" valign="top">
<p>24 bytes (subclass padding)</p>
</td><td style="text-align: left" valign="top">
<p>12 b reference + (1 b value + 7 b padding = 8 b) for A + 1 b for value of <code class="literal">y</code> + 3 b padding</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">clojure.lang.Symbol.intern("foo")</code>
</p>
<p>
<code class="literal">// clojure 'foo</code>
</p>
</td><td style="text-align: left" valign="top">
<p>104 bytes (40 bytes interned)</p>
</td><td style="text-align: left" valign="top">
<p>12 b header + 12 b ns reference + (12 b name reference + 40 b interned chars) + 4 b <code class="literal">int</code> hash + 12 b meta reference + (12 b <code class="literal">_str</code> reference + 40 b interned chars) – 40 b interned <code class="literal">str</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">clojure.lang.Keyword.intern("foo")</code>
</p>
<p>
<code class="literal">// clojure :foo</code>
</p>
</td><td style="text-align: left" valign="top">
<p>184 bytes (fully interned by factory method)</p>
</td><td style="text-align: left" valign="top">
<p>12 b reference + (12 b symbol reference + 104 b interned value) + 4 b <code class="literal">int</code> hash + (12 b <code class="literal">_str</code> reference + 40 b interned <code class="literal">char</code>)</p>
</td></tr></tbody></table></div><p>A comparison of space taken by a symbol and a keyword created from the same given string demonstrates that even though a keyword has slight overhead over a symbol, the keyword is fully interned and would provide better guard against memory consumption and thus GC <a class="indexterm" id="id286"/>over time. Moreover, the <a class="indexterm" id="id287"/>keyword is interned as a weak reference, which ensures that it is garbage collected when no keyword in memory is pointing to the interned value anymore.</p><div class="section" title="Determining program workload type"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec17"/>Determining program workload type</h3></div></div></div><p>We often need to <a class="indexterm" id="id288"/>determine whether a program is CPU/cache bound, memory bound, I/O bound or contention bound. When a program is I/O or contention bound, the CPU usage is generally low. You may have to use a profiler (we will see this in <a class="link" href="ch07.html" title="Chapter 7. Performance Optimization">Chapter 7</a>, <span class="emphasis"><em>Performance Optimization</em></span>) to find out whether threads are stuck due to resource contention. When a program is CPU/cache or memory bound, CPU usage may not be a clear indicator of the source of the bottleneck. In such cases, you may want to make an educated guess by inspecting cache misses in the program. On Linux systems tools such as <a class="indexterm" id="id289"/>
<span class="strong"><strong>perf</strong></span> (<a class="ulink" href="https://perf.wiki.kernel.org/">https://perf.wiki.kernel.org/</a>), <a class="indexterm" id="id290"/>
<span class="strong"><strong>cachegrind</strong></span> (<a class="ulink" href="http://valgrind.org/info/tools.html#cachegrind">http://valgrind.org/info/tools.html#cachegrind</a>) and <a class="indexterm" id="id291"/>
<span class="strong"><strong>oprofile</strong></span> (<a class="ulink" href="http://oprofile.sourceforge.net/">http://oprofile.sourceforge.net/</a>) can help determine the volume of cache misses—a higher threshold may imply that the program is memory bound. However, using these tools with Java is not straightforward because Java's JIT compiler needs a warm-up until meaningful behavior can be observed. The project <a class="indexterm" id="id292"/>
<span class="strong"><strong>perf-map-agent</strong></span> (<a class="ulink" href="https://github.com/jrudolph/perf-map-agent">https://github.com/jrudolph/perf-map-agent</a>) can help generate method mappings that you can correlate using the <code class="literal">perf</code> utility.</p></div></div><div class="section" title="Tackling memory inefficiency"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec51"/>Tackling memory inefficiency</h2></div></div></div><p>In earlier sections in this<a class="indexterm" id="id293"/> chapter we discussed that unchecked memory access may become a bottleneck. As of Java 8, due to the way the heap and object references work, we cannot fully control the object layout and memory access patterns. However, we can take care of the frequently executed blocks of code to consume less memory and attempt to make them cache-bound instead of memory-bound at runtime. We can consider a few techniques to lower memory consumption and randomness in access:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Primitive locals (long, double, boolean, char, etc) in the JVM are created on the stack. The rest of the objects are created on the heap and only their references are stored in the stack. Primitives have a low overhead and do not require memory indirection for access, and are hence recommended.</li><li class="listitem" style="list-style-type: disc">Data laid out in the main memory in a sequential fashion is faster to access than randomly laid out data. When we use a large (say more than eight elements) persistent map, the data stored in tries may not be sequentially laid out in memory, rather they would be randomly laid out in the heap. Moreover both keys and values are stored and accessed. When you use records (<code class="literal">defrecord</code>) and types (<code class="literal">deftype</code>), not only do they provide array/class semantics for the layout of fields within them, they do not store the keys, which is very efficient compared to regular maps.</li><li class="listitem" style="list-style-type: disc">Reading large<a class="indexterm" id="id294"/> content from a disk or the network may have an adverse impact on performance due to random memory roundtrips. In <a class="link" href="ch03.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span>, we briefly discussed memory-mapped byte buffers. You can leverage memory-mapped buffers to minimize fragmented object allocation/access on the heap. While libraries such as<a class="indexterm" id="id295"/> <code class="literal">nio</code> (<a class="ulink" href="https://github.com/pjstadig/nio/">https://github.com/pjstadig/nio/</a>) and<a class="indexterm" id="id296"/> <code class="literal">clj-mmap</code> (<a class="ulink" href="https://github.com/thebusby/clj-mmap">https://github.com/thebusby/clj-mmap</a>) help us deal with memory-mapped<a class="indexterm" id="id297"/> buffers, <code class="literal">bytebuffer</code> (<a class="ulink" href="https://github.com/geoffsalmon/bytebuffer">https://github.com/geoffsalmon/bytebuffer</a>), and<a class="indexterm" id="id298"/> <code class="literal">gloss</code> (<a class="ulink" href="https://github.com/ztellman/gloss">https://github.com/ztellman/gloss</a>) let us work with byte buffers. There are also alternate abstractions such as <a class="indexterm" id="id299"/>iota (<a class="ulink" href="https://github.com/thebusby/iota">https://github.com/thebusby/iota</a>) that help us deal with large files as collections.</li></ul></div><p>Given that memory bottleneck is a potential performance issue in data-intensive programs, lowering memory overhead goes a long way in avoiding performance risk. Understanding low-level details of the hardware, the JVM and Clojure's implementation helps us choose the appropriate techniques to tackle the memory bottleneck issue.</p></div></div>
<div class="section" title="Measuring latency with Criterium"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Measuring latency with Criterium</h1></div></div></div><p>Clojure has a<a class="indexterm" id="id300"/> neat little macro called <code class="literal">time</code> that evaluates the<a class="indexterm" id="id301"/> body of code passed to it, and then prints out the time it took and simply returns the value. However, we can note that often the time taken to execute the code varies quite a bit across various runs:</p><div class="informalexample"><pre class="programlisting">user=&gt; (time (reduce + (range 100000)))
"Elapsed time: 112.480752 msecs"
4999950000
user=&gt; (time (reduce + (range 1000000)))
"Elapsed time: 387.974799 msecs"
499999500000</pre></div><p>There are several <a class="indexterm" id="id302"/>reasons associated to this variance in behavior. When<a class="indexterm" id="id303"/> cold started, the JVM has its heap segments empty and is unaware of the code path. As the JVM keeps running, the heap fills up and the GC patterns start becoming noticeable. The JIT compiler gets a chance to profile the different code paths and optimize them. Only after quite some GC and JIT compilation rounds, does the JVM performance become less unpredictable.</p><p>Criterium <a class="indexterm" id="id304"/>(<a class="ulink" href="https://github.com/hugoduncan/criterium">https://github.com/hugoduncan/criterium</a>) is a Clojure library to scientifically measure the latency of Clojure expressions on a machine. A summary of how it works can be found at the Criterium project page. The easiest way to use Criterium is to use it with Leiningen. If you want Criterium to be available only in the REPL and not as a project dependency, add the following entry to the <code class="literal">~/.lein/profiles.clj</code> file:</p><div class="informalexample"><pre class="programlisting">{:user {:plugins [[criterium "0.4.3"]]}}</pre></div><p>Another way is to include <code class="literal">criterium</code> in your project in the <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">:dependencies [[org.clojure/clojure "1.7.0"]
               [criterium "0.4.3"]]</pre></div><p>Once done with the editing of the file, launch REPL using <code class="literal">lein repl</code>:</p><div class="informalexample"><pre class="programlisting">user=&gt; (require '[criterium.core :as c])
nil
user=&gt; (c/bench (reduce + (range 100000)))
Evaluation count : 1980 in 60 samples of 33 calls.
             Execution time mean : 31.627742 ms
    Execution time std-deviation : 431.917981 us
   Execution time lower quantile : 30.884211 ms ( 2.5%)
   Execution time upper quantile : 32.129534 ms (97.5%)
nil</pre></div><p>Now, we can see that, on average, the expression took 31.6 ms on a certain test machine.</p><div class="section" title="Criterium and Leiningen"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec52"/>Criterium and Leiningen</h2></div></div></div><p>By default, Leiningen <a class="indexterm" id="id305"/>starts the JVM in a low-tiered compilation mode, which causes it to start up faster but impacts the optimizations that the JRE can perform at runtime. To get the best effects when running tests with Criterium and Leiningen for a server-side use case, be sure to override the defaults in <code class="literal">project.clj</code> as follows:</p><div class="informalexample"><pre class="programlisting">:jvm-opts ^:replace ["-server"]</pre></div><p>The <code class="literal">^:replace</code> hint causes Leiningen to replace its own defaults with what is provided under the <code class="literal">:jvm-opts</code> key. You may like to add more parameters as needed, such as a minimum and maximum heap size to run the tests.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Summary</h1></div></div></div><p>The performance of a software system is directly impacted by its hardware components, so understanding how the hardware works is crucial. The processor, caches, memory, and I/O subsystems have different performance behaviors. Clojure, being a hosted language, understanding the performance properties of the host, that is, the JVM, is equally important. The Criterium library is useful for measuring the latency of the Clojure code—we will discuss Criterium again in <a class="link" href="ch06.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span>. In the next chapter we will look at the concurrency primitives in Clojure and their performance characteristics.</p></div></body></html>