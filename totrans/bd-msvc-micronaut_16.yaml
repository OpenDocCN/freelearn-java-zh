- en: '*Chapter 10*: IoT with Micronaut'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Internet of Things** (**IoT**) is one of the fastest emerging technologies.
    It is a network of devices or things. These devices have the same capabilities
    as sensors or software and can communicate with other devices over the internet.
    A device or thing can be from various fields and can include things such as light
    bulbs, door locks, heartbeat monitors, location sensors, and many devices that
    can be enabled with sensors. It is an ecosystem of smart devices or things with
    internet capabilities. IoT is trending in various fields. A few of the top trending
    fields are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Home automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manufacturing and industrial applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Healthcare and medical science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Military and defense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automotive, transportation, and logistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Along with these fields, in this chapter, we will learn about the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basics of IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Micronaut Alexa skills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be well versed in the preceding aspects
    concerning IoT with Micronaut integration.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the commands and technical instructions in this chapter can be run on Windows
    10 and macOS. The code examples in this chapter are available in this book's GitHub
    repository at [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter10/](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter10/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tools need to be installed and set up in the development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java SDK**: Version 13 or above (we used Java 14).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maven**: This is optional and only required if you would like to use Maven
    as the build system. However, we recommend having Maven set up on any development
    machine. Instructions to download and install Maven can be found at [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development IDE**: Based on your preference, any Java-based IDE can be used,
    but for the purpose of writing this chapter, IntelliJ was used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git**: Instructions for downloading and installing Git can be found at [https://git-scm.com/downloads](https://git-scm.com/downloads).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PostgreSQL**: Instructions for downloading and installing PostgreSQL can
    be found at [https://www.postgresql.org/download/](https://www.postgresql.org/download/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDB**: MongoDB Atlas provides a free online Database-as-a-Service (DBaaS)
    with up to 512 MB storage. However, if you would prefer to use a local database,
    then the instructions for downloading and installing MongoDB can be found at [https://docs.mongodb.com/manual/administration/install-community/](https://docs.mongodb.com/manual/administration/install-community/).
    We used a local installation to write this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**REST client**: Any HTTP REST client can be used. We used the Advanced REST
    Client Chrome plugin in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker**: Instructions for downloading and installing Docker can be found
    at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon**: You will need an Amazon account for Alexa, which you can set up
    at [https://developer.amazon.com/alexa](https://developer.amazon.com/alexa).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**IoT** is a network of devices or things. These things can be anything – it
    can be a human wearing a health monitor, a pet wearing a geolocation sensor, a
    car with a tire pressure sensor, a television with voice/visual capability, or
    a smart speaker. IoT can also use advanced **machine learning** (**ML**) and **artificial
    intelligence** (**AI**) capabilities in the cloud to provide next-level services.
    IoT can make things smart with data collection and automation. The following diagram
    illustrates IoT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Internet of Things (IoT)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.1_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Internet of Things (IoT)
  prefs: []
  type: TYPE_NORMAL
- en: These devices or things have internet capabilities and are interconnected, so
    they act as an ecosystem. This ecosystem can collect, send, and act based on data
    it acquires from other things. For example, you can turn on the lights at your
    home when you arrive.
  prefs: []
  type: TYPE_NORMAL
- en: IoT provides significant benefits to individuals, businesses, and organizations.
    IoT can reduce manual work and intervention with seamless data transfer between
    two systems or devices. IoT devices are become more significant every day in the
    consumer market, be it as locks, doorbells, light bulbs, speakers, televisions,
    healthcare products, or fitness systems. IoT is mainly accessed now in voice enabled
    ecosystems such as Google Home, Apple Siri, Amazon Alexa, Microsoft Cortana, Samsung
    Bixby, and more. There are numerous positive aspects of IoT; however, there are
    a few cons regarding security and privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the basics of IoT and its applications, let's
    understand the basics of Alexa skills.
  prefs: []
  type: TYPE_NORMAL
- en: Working on the basics of Alexa skills
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alexa is a cloud-based voice recognition service available on millions of devices
    from Amazon and third-party device manufacturers, such as televisions, Bluetooth
    speakers, headphones, automobiles, and so on. You can build interactive voice-based
    request-response applications using Alexa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexa can be integrated into various applications. Alexa also has screen capabilities
    for displaying responses visually, and Echo Show is an Alexa speaker with a display
    screen. The following diagram illustrates the Amazon Alexa architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Amazon Alexa architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.2_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Amazon Alexa architecture
  prefs: []
  type: TYPE_NORMAL
- en: Users can say the wake-up word for the device, which is **Alexa**, and perform
    an operation. For example, to find the weather in your current location, you can
    say *Alexa, what is the current weather?* and you will receive a response, such
    as *The current weather in your location is 28 degrees*. Alexa skills are like
    apps, and you can enable or disable skills using the Alexa app for a specific
    device. Skills are voice-based Alexa capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexa can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Set an alarm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Play music from Spotify, Apple Music, or Google Music.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a to-do list and add items to your shopping list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the weather.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check your calendar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read news briefings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check bank accounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order in a restaurant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check facts on the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are a few of the many things that Alexa can perform. Now, let's move on
    and understand more about Alexa.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of Alexa skills
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Users with any voice-based assistants or tools can use the wake-up word to open
    the skill or application. For example, with Google Home, we use *Hey Google* or
    *OK Google*, for Apple Siri, we use *Hey Siri* or *Siri*, and for Amazon Alexa,
    we use *Alexa*. This wake-up word can be replaced with *Amazon*, *Echo*, or *computer*.
    All Alexa skills have been designed based on the voice interaction model; that
    is, phrases you can say to make the skill do something you want, such as *Alexa,
    turn on the lights* or *Alexa, what is the current temperature?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexa supports the following two types of voice interaction models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-built voice interaction model**: Alexa defines the phrases for each skill
    for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom voice interaction model**: You define the phrases that the user can
    say to interact with your skills.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our working example code, we will use the custom voice interaction model.
    The following diagram illustrates the process of opening a skill using a custom
    voice interaction model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Opening a skill'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.3_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Opening a skill
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about the wake-up word, the phrase following it is the **launch**
    word, followed by the **invocation name**. For our sample application, **Pet Clinic**,
    the launch word will be **open**, followed by the invocation **Pet Clinic**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the relationship between utterances and intent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Opening a skill'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.4_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Opening a skill
  prefs: []
  type: TYPE_NORMAL
- en: An utterance is a word users say to Alexa to convey what they want to do such
    as *Turn on the lights*, *What is the current temperature?*, and so on. Users
    can say the same thing in different ways, such as *find the temperature*, *current
    temperature*, *outside temperature*, *the temperature in [location]*, and Alexa
    will provide pre-build utterances and associated requests as part of the custom
    voice interaction model. This list of utterances can be mapped to a request or
    intent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the custom voice interaction model with a
    **wake word**, **launch**, **invocation name**, **utterance**, and **intent**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Opening a skill – Pet Clinic'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.5_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Opening a skill – Pet Clinic
  prefs: []
  type: TYPE_NORMAL
- en: For our code example, we will use the sequences *Alexa, Open Pet Clinic*, and
    *Alexa, find nearby Pet Clinics*. Here, the wake-up word is **Alexa**, the launch
    word is **Open**, and the invocation name is **Pet Clinic**. The utterance can
    be **find the nearest pet clinic**. We can also have other variations of utterances,
    such as **find pet clinic**. All these utterances can be mapped to **GetFactByPetClinicIntent**.
    We will learn about intents in the next section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of intents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the fundamental designs for voice in Alexa is intents. Intents capture
    events the end user wants to do with voice. Intents represent an action that is
    triggered by the user's spoken request. Intents in Alexa are specified in a JSON
    structure called an **intent schema**. The built-in intents include **Cancel**,
    **Help**, **Stop**, **Navigate Home**, and **Fallback**. Some intents are basic,
    such as help, and the skills should have a Help Intent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the built-in intents in the Alexa developer
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Built-in intents'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.6_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Built-in intents
  prefs: []
  type: TYPE_NORMAL
- en: If we have a website for logging in that has username and password fields and
    a submit button, there will be a submit intent in the Alexa skill world. However,
    one big difference is that users can say the submit in different ways; for example,
    *Submit*, *submit it*, *confirm*, *ok*, *get*, *continue*, and so on. These different
    ways of saying the same thing are called **utterances**. Each intent should include
    a list of utterances; that is, all the things a user might say to invoke these
    intents. Intents can have arguments called **slots**, which will not be discussed
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the basics of Alexa skills by covering utterances,
    intents, and built-in intents, let's create our first functional Alexa skill.
  prefs: []
  type: TYPE_NORMAL
- en: Your first HelloWorld Alexa skill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start creating our Alexa skill, we must navigate to [https://developer.amazon.com/](https://developer.amazon.com/),
    select **Amazon Alexa**, and click **Create Alexa Skills**. This will open the
    Alexa developer console. If you don''t have an Amazon developer account, you can
    create one for free. The following screenshot illustrates the **Create Skill**
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Create Skill screen'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.7_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Create Skill screen
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see how to create a new skill name called
    `Pet Clinic`, choose a model to add to your skill option called `Custom`, and
    choose a method to host your skill's backend resources called `Provision your
    own`. Choose a template to add to your skill called `Start from Scratch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the custom voice interaction model, we have learned that we need to
    create and configure our wake word, launch, invocation name, utterances, and intent.
    The wake word is configured for the device and is the same for all the skills,
    so we don''t need to change it. In our configuration, we will configure the code
    launch, invocation, utterances, and intent. The following diagram illustrates
    the basics of developing Alexa skills:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Alexa skills'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.8_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Alexa skills
  prefs: []
  type: TYPE_NORMAL
- en: 'Alexa skills are based on the **voice interaction model** and **programming
    logic**. Programming logic can be created using Node.js, Java, Python, C#, or
    Go. This programming logic allows us to connect to web services, microservices,
    APIs, and interfaces. With this, you can invoke an internet-accessible endpoint
    for Alexa skills. The following diagram illustrates the skills developer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Alexa skills'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.9_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Alexa skills
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set `pet clinic` and save it. You can also select the `HelloWorldIntent`
    intent and rename it `PetClinicWelcomeIntent`. There will be sample utterances
    listed in the intent that you can modify manually or use the JSON Editor and copy
    the `alexa_petclinic_intent_schema.json` code from this book''s GitHub repository.
    The following code illustrates the JSON schema for the intent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can configure the intent's invocation name and sample utterance using the
    JSON configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have copied the JSON file to the Alexa developer console's JSON editor,
    click **Save Model** and then **Build Model and Evaluate Model**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding configuration is a sample from this chapter's folder on GitHub.
    The actual schema can be copied from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have built the model, click **Test** in the Alexa developer console
    and enable the skill testing process. Now, we need to develop our backend code
    for the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Maven Java project using your favorite IDE. The following dependencies
    are required for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using Amazon''s `ask-sdk` for our backend Java development. You
    can also configure the dependencies using Gradle. A sample Gradle configuration
    can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create a Java class for all the intents. In our JSON schema, we
    have defined the `CancelIntent`, `HelpIntent`, `StopIntent`, `NavigateHomeIntent`,
    `FallbackIntent`, and `PetClinicWelcomeIntent` intents. For every intent, we need
    to create a handler; for example, `PetClinicWelcomeIntent` should have `PetClicWelcomeIntentHandler`.
    The handler''s name will be added to the end of each intent name. We must also
    create one additional handler that hasn''t been configured in the JSON schema,
    and this is called `LaunchRequestHandler`. This is the first intent that is triggered
    whenever their skill is launched. The following code illustrates `LaunchRequestHandler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`LaunchRequestHandler` will override the handler method and the response voice
    message when the skill is launched. This is defined in the code block. In the
    code, we have a speech text response of *Welcome to Pet Clinic, you can say find
    near by Pet Clinics*, along with the title of `PetClinic`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have created the handlers (`CancelandStopIntentHandler`, `HelpIntentHandler`,
    `LaunchRequestHandler`, `PetClinicWelcomeIntentHandler`, and `SessionEndedRequestHandler`),
    we need to create `StreamHandler`. `StreamHandler` is the entry point for the
    AWS Lambda function. All requests that are sent by the end user to Alexa, which
    invokes your skill, will pass through this class. You need to configure the copy
    of the skill ID from the Amazon Alexa developer console in the endpoint. Refer
    to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have learned about how to use stream handlers and how to invoke
    intent handlers. Let''s learn about the use of the skill ID, which is where you
    can get information about the skill ID. The following screenshot illustrates the
    skill ID''s location in the developer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Endpoint skill ID and default region'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.10_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – Endpoint skill ID and default region
  prefs: []
  type: TYPE_NORMAL
- en: 'The skill ID can be found on the Alexa developer console''s `.jar` file with
    dependencies for the code. You can execute the `mvn assembly:assembly -DdescriptorId=jar-with-dependencies
    package` command to create the `.jar` file. This `.jar` file will be located in
    the target directory, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Maven JAR file location'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.11_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.11 – Maven JAR file location
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to create the Amazon Lambda function, which is our backend
    service code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to [https://console.aws.amazon.com/lambda/](https://console.aws.amazon.com/lambda/)
    to create a function. Name the function `lambda_for_petclinic`, set options to
    `Author from scratch`, and set runtime to `Java 11`. The user interface is illustrated
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Alexa Create function screen'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.12_B16585_edited.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.12 – Alexa Create function screen
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create the trigger with **Trigger configuration** set to
    **Alexa Skills Kit**, as shown in the following screenshot. You need to copy the
    Alexa **Skill Id** from the Alexa developer console''s **Endpoint** screen. Also,
    you need to copy the **Function ARN** (**Amazon Resource Name**) property from
    the Lambda developer console to the Alexa skill developer console''s **Endpoint**
    screen. The following screenshot illustrates the location of the AWS Lambda function''s
    ARN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – AWS Function ARN\'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure10.13_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.13 – AWS Function ARN\
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Function ARN** value must be copied from the Alexa developer console
    **Endpoint** screen to the **Default Region** section or to the location-specific
    regions, as shown in *Figure 10.10*. The skill ID shown in *Figure 10.10* should
    be copied to the AWS Lambda trigger screen, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – AWS Lambda – Add trigger'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure10.14_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.14 – AWS Lambda – Add trigger
  prefs: []
  type: TYPE_NORMAL
- en: Once the trigger has been added with the necessary skill ID and the `.jar` file,
    along with any dependencies (`petclinic-Alexa-maven-1.0-SNAPSHOT-jar-with-dependencies.jar`),
    to the Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates the process of uploading the `.jar` file
    to the Amazon Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 –Uploading code to Amazon Lambda'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure10.15_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.15 –Uploading code to Amazon Lambda
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created our first Alexa skill and uploaded the necessary code,
    let's test it out.
  prefs: []
  type: TYPE_NORMAL
- en: Testing your code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last and final process is to test the code with the Amazon Alexa Simulator,
    which is located in the developer console. The following screenshot illustrates
    how to request the Alexa Simulator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Alexa Simulator request/response'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure10.16_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.16 – Alexa Simulator request/response
  prefs: []
  type: TYPE_NORMAL
- en: The request/response testing screen accepts text or speech. You can type or
    say `open pet clinic` and `find nearby pet clinics` here. You should be able to
    see the response from the Java code in the **JSON Output 1** section. Once you
    can see the response, this means we have successfully created our first basic
    IoT pet clinic example with a request and a response.
  prefs: []
  type: TYPE_NORMAL
- en: We will integrate Micronaut and Alexa in the next section. You can find the
    complete working example for the `petclinic-alexa-maven` project in this book's
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Micronaut with Alexa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in the preceding section, in this section, we will start to
    understand how to integrate Micronaut with Alexa. Micronaut provides various extensions
    that support `micronaut-function-aws-alexa` module includes support for building
    Alexa skills with Micronaut. Micronaut Alexa support can wire up your Alexa application
    with **AlexaFunction** and supports dependency injection for the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`com.amazon.ask.dispatcher.request.handler.RequestHandler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.amazon.ask.dispatcher.request.interceptor.RequestInterceptor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.amazon.ask.dispatcher.exception.ExceptionHandler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`com.amazon.ask.builder.SkillBuilder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Micronaut''s `aws-alexa` module simplifies how we can develop Alexa skills
    with Java, Kotlin, or Groovy. The following code is the Java Maven dependency
    for the `aws-alexa` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As we learned in the previous chapters, Micronaut uses Java annotations. To
    change any Alexa Java handler so that it can work with Micronaut, all we need
    to do is add the necessary `@Singleton` annotation; that is, `javax.inject.Singleton`.
    A sample `LaunchRequestHandler` with the `Singleton` annotation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of Micronaut, you can perform unit testing for your intents easily.
    This is because the `@MicronautTest` annotation provides seamless unit testing
    capabilities. Here, we can inject the handler into the unit test cases. The Micronaut
    framework leverages the Amazon `LaunchRequest` class to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the complete working example for the `petclinic-alexa-micronaut-maven`
    project in this book''s GitHub repository. You can connect to a web service or
    to a backend database in the handler to send a request and receive a response.
    The following diagram illustrates the design for Alexa skill integration with
    the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Alexa skills with a custom backend'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.17_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.17 – Alexa skills with a custom backend
  prefs: []
  type: TYPE_NORMAL
- en: 'Your `speechText`, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `speechText` handle method can be added from a microservice call and can
    retrieve information from a database or service.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have integrated Micronaut with Alexa, we can control IoT devices
    with the Voice and Micronaut microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to use the basics of IoT and Amazon Alexa.
    Then, we dived into creating a Micronaut microservice and integrating it with
    Amazon Alexa. With this integration, we can control IoT devices with the Voice
    and Micronaut microservices.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter enhanced your Micronaut microservices journey in IoT. It has equipped
    you with first-hand knowledge of IoT and Amazon Alexa. Micronaut also supports
    **Speech Synthesis Markup Language** (**SSML**) and Flash Briefings.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will bring all the topics that we have learned about
    together and take things to the next level by architecting enterprise microservices,
    looking at OpenAPI, scaling Micronaut, and deep diving into building enterprise-grade
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is IoT?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name a few devices that are IoT devices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an Alexa skill?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Alexa intents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which programming languages does Alexa support?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the default launch handler class's name?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the one change you will need to make to your annotate handlers so that
    they're compatible with Micronaut?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
