<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Applications on the Cloud with OpenShift</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we showed you how to develop microservices with WildFly Swarm. In this chapter, you will learn how to deploy those services to the cloud, and you will use OpenShift to achieve that. However, why should we bother? What are the features and benefits of cloud computing? Let's start by saying a few words about it.</p>
<p class="mce-root">Before we move on to the next section, you need to be provided with some important information. This chapter describes a lot of theoretical concepts that explain the internals of OpenShift. Don't worry if some of those concepts sound too complex to configure because (spoiler alert!), in the end, OpenShift will do most of the stuff for you. The goal of this chapter is to provide you with knowledge, which will allow you to understand all the magic that OpenShift can do, and. In later chapters, modify, and reconfigure that behavior. So, let's begin.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud computing</h1>
                </header>
            
            <article>
                
<p>OK. Let's start at the beginning. So, what actually is cloud computing?</p>
<p>Cloud computing is an IT paradigm that advocates the use of shared pools of configurable resources and services provided over the internet. Those services are provided on demand, rapidly, and with minimal management. As a result, cloud computing allows for flexible architecture, optimized resource usage, and the possibility of decoupling the infrastructure provider from a consumer, enabling the separation of concerns. Let's examine those statements in greater detail.</p>
<p>Resources provisioned on demand give you, as a developer or an architect, flexibility in configuring your technical infrastructure. Starting the project is cheap, as you don't have to begin with infrastructure investments. Also, when your project is in production, computing resources may be scaled automatically to fill your application's demands. In each case, you are paying only for the resources used.</p>
<p class="mce-root">Furthermore, cloud computing introduces the separation of concerns. The cloud provider becomes specialized in provisioning the infrastructure. The developers are provided with the interface of the cloud infrastructure and will use it for the development. As a result, they don't need to be concerned with the details of the infrastructure configuration as long as they comply with the interface provided by the cloud. Cloud providers, on the other hand, have to compete to provide the most convenient and robust cloud infrastructure while minimizing the costs.</p>
<p class="mce-root">Throughout this chapter, we will describe the cloud interface provided by OpenShift. Before that, we will describe the following two cloud infrastructure characteristics, which will enable the use of a consistent set of concepts during the following chapters: deployment and service models. Let's start with the first one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The cloud infrastructure deployment model</h1>
                </header>
            
            <article>
                
<p>The cloud deployment model specifies how cloud infrastructure is organized. Let's look at commonly used models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The public cloud</h1>
                </header>
            
            <article>
                
<p>The <strong>public cloud</strong> is a model in which services are provided over a network for public usage.</p>
<p class="mce-root">Public cloud computing is sometimes described as an evolution of computing resources into utility such as electricity. Using the example of a power provider may give you an alternative insight into the topics described in this paragraph: the power distribution company is responsible, among other, for ensuring that the network can deal with demand peaks, or provide network redundancy in case of traction breakdown. However, in the end, we don't think about it too muchâ€”we insert the plug into the socket when needed and get billed only for electricity we used.</p>
<p>We shouldn't stretch that analogy too far though. Computing resources are <span>obviously</span> not a commodity like electricity, and they cannot be provided to everyone in a uniform manner because of different customer requirements.</p>
<p>First of all, not all customers are able, for various reasons, to move their workloads to the public cloud. One of the reasons may be the need for a higher level of security or greater control over the architecture. Such customers can take advantage of the private cloud discuss it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The private cloud</h1>
                </header>
            
            <article>
                
<p>In the <strong>private cloud</strong>, the computing resources used to create the cloud are dedicated to a single customer. The resources may be located on-site or off-site, owned by the customer, or managed by a third party. A<span>ll of the resources are provisioned for a single client and are not shared by multiple clients.</span> Such a configuration allows for greater control and security but requires the company to invest in and manage the cloud resources, removing the decoupling benefit of the public cloud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The hybrid cloud</h1>
                </header>
            
            <article>
                
<p>The <strong>hybrid cloud</strong> is a model in which public and private clouds are connected together. As a result, you can take advantage of both solutions. For example, you can run services that deal with sensitive data on the private cloud, but use the public cloud to scale other services.</p>
<div class="packt_infobox">We would like our cloud provider to provide cloud abstraction that will enable users to see a homogeneous cloud view, abstracting away the infrastructure details of the hybrid cloud. We will return to this thought later, when we describe the architecture of OpenShift.</div>
<p>Let's now focus on another cloud characteristic, that is, the <strong>service model</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The service model</h1>
                </header>
            
            <article>
                
<p>As you learned at the beginning of this chapter, the cloud provider is responsible for providing computing resources on demand. Computing resources can be provisioned in different ways, and characterized by different levels of abstraction. This feature of cloud computing infrastructure is called a service model. Let's describe the commonly used models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Infrastructure as a Service</h1>
                </header>
            
            <article>
                
<p>In <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>), a customer is able to install arbitrary applications (including operating systems) on the provisioned resources. The customer does not control the infrastructure that is provided by the cloud provider. For example, the customer may get access to a remote virtual machine, which they can fully operate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Platform as a Service</h1>
                </header>
            
            <article>
                
<p class="mce-root">In <strong>Platform as a Service</strong> (<strong>PaaS</strong>), as its name suggests, <span>the</span> cloud provider is responsible for providing the customer with a ready-for-use the platform on which the customer can deploy and run their applications. Let's suppose that you want to use the database. In this case, the platform provider is responsible for giving you access to <span>an</span> up-to-date and configured database on which you can start working immediately. You don't have to mess with the entire configuration, and can start working with the database (or any other technology; WildFly Swarm, for example) straightaway.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software as a Service</h1>
                </header>
            
            <article>
                
<p class="mce-root">Finally, there is <strong>Software as a Service</strong> (<strong>SaaS</strong>). In this model, the customer is able to use the application provided by the cloud provider. An example of SaaS may be disc storage, email, or an office suite provided over the internet.</p>
<p>OK, now that we've clarified the nomenclature, we can finally dig into OpenShift architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The OpenShift architecture</h1>
                </header>
            
            <article>
                
<p>So far, we have been talking in abstract terms. In this <span>section,</span> we will give you an overview of the OpenShift architecture. As a result, you will gain a practical understanding of the functioning cloud PaaS infrastructure.</p>
<p>Let's start with a bird's-eye view architecture diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a028067-a462-464b-b063-d3b65ab55eb6.png" style=""/></div>
<p>The preceding diagram sketches the layers of OpenShift architecture. <strong>Docker</strong> runs on top of the operating system and provides layers of containers. Containers are lightweight, standalone, and executable pieces of software (<em>Further reading</em>, link 2) that can be run anywhere on the cloud. Those containers are orchestrated by <strong>Kubernetes</strong>, which provides a unified view of heterogeneous computing resources. Finally, <strong>OpenShift</strong> builds on top of <strong>Kubernetes</strong>, providing developer tools that automate most configuration tasks. If this short description sounded cryptic to you, don't worry. Everything will be clear by the end of this chapter. Let's start with containers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Containerization</h1>
                </header>
            
            <article>
                
<p>As we said before, in cloud computing, the cloud provider is responsible for providing the server resources to users on demand. However, what does that actually mean? How are those resources provided? How is the server with all the necessary configurations created, and how it is isolated from other users? To understand this, we must understand how containers work.</p>
<p>Containers are basically a type of <strong>virtualization</strong>. Let's discuss this concept.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Virtualization</h1>
                </header>
            
            <article>
                
<p>Virtualization is a technology that allows the running of many isolated virtual machines on one server. A virtual machine is an emulation of a computer system governed by the virtualization application running on the server's operating system. The user to whom the virtual machine has been provisioned has access to the fully operational host. The fact that the host is not physical, and that it shares the resources of the physical server with other virtual machines, is being abstracted away from the user.</p>
<p>Another key feature of virtualization is the ability to serialize the virtual machine into the image. This feature enables portability. The image can be moved to different servers, thus ensuring that the state of the virtual machine will be preserved.</p>
<p><span>Why is virtualization important to the cloud provider, then?</span></p>
<p><span>Firstly, t</span><span>he cloud provider is able to provide access to the virtual machine to the user.</span> <span>As a result, the user will obtain access to a fraction of server resources.</span> <span>From the user's perspective, they will have access to the isolated server</span></p>
<p><span>Secondly, t</span><span>he user has the ability to use a preconfigured image of the platform that they want to use. Do you need Fedora with WildFly AS?</span> <span>Here is your configured image.</span> <span>We will run it on our server, and you are ready to go.</span></p>
<p><span>Thirdly, t</span><span>he cloud provider is able to optimize the resource usage.</span> <span>There can be many virtual machines running on one server, minimizing the idle time.</span></p>
<p><span>Fourthly,</span> <span>Virtual machines can be moved freely between different servers when necessary.</span> <span>If more resources are needed, some virtual machines can be</span> transferred <span>to another server.</span> <span>Again, from the user's point of view, you will have access to an isolated preconfigured server and won't have to worry about the details.</span></p>
<p>Then, what about the implementation?</p>
<p>Some of you may identify virtualization with hardware (of full) virtualization. In this architecture, the virtualization application is responsible for emulating the whole operating system with all the necessary process<span>es and libraries.</span></p>
<p class="mce-root">This solution has some performance problems, most of them resulting from the fact that the operating system was initially designed to run on a physical host. Firstly, to start the virtual machine the whole OS has to be started and, as a result, the startup time may be substantial (minutes). Secondly, OS processes and libraries have to be duplicated in each virtual machine, which leads to non-optimal resources usage.</p>
<p>Let's think about it from the cloud provider's perspective, taking special consideration of the microservices architecture that we described in <a href="77a69da4-3aff-46c3-aa00-fa4d02db5061.xhtml">Chapter 1</a>, <em>Java EE and Modern Architectural Methodologies</em>. We would like to have a solution that will enable us to provide a large number of ephemeral virtual machines. We would like to ensure that they can be started and stopped immediately, optimizing the use of resources, and store image data effectively. It turns out that we need another tool. Let's discuss containers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Containers</h1>
                </header>
            
            <article>
                
<p class="mce-root">Containers are the implementation of a system-level virtualization (or paravirtualization). In this kind of virtualization, the operating system is not emulated on each virtual machine. Instead, virtual machines share the same operating system instance, using the tools provided by it to achieve isolation. As a result<span>, in this model, we can think of virtual machines as isolated user-space instances running on top of the same operating system. Such instances are called container.</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d290ce5b-8edd-49c8-b9ce-f9e85d271b98.png" style=""/></div>
<p>The preceding diagram highlights the main differences between full virtualization and containers. Containers share the same operating system and, as you will learn in the next sections, are able to share common libraries effectively. In the preceding diagram, the horizontal line represents the layers that have to be created when the new virtual machine/container is created.</p>
<p>Before we describe the Docker implementation of containers, let's talk about the isolation tools that the Linux kernel provides.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kernel isolation tools</h1>
                </header>
            
            <article>
                
<p>The Linux kernel provides a wide array of tools that enable the isolation of processes, and put a resource usage limit on those groups. The main tools that enable that (and are used by Docker containers) are namespaces (isolation), and <kbd>cgroups</kbd> (limits). Let's learn more about it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Namespaces</h1>
                </header>
            
            <article>
                
<p>Each kernel process can be assigned to a namespaceâ€”processes with the same namespace share the same view some of the system resources. For example, a PID namespace provides the ability to isolate processesâ€”processes in the same PID namespace can see each other, but cannot see processes from different namespaces.</p>
<p>There is a group of namespaces in Linux kernel that provides PID, network, mount point, and username isolation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">cgroups</h1>
                </header>
            
            <article>
                
<p>The <kbd>cgroups</kbd> are responsible for limiting the resource usage for the group of processes. <kbd>cgroups</kbd> allow you to assign processes to a number of groups and configure a resource quota for those groups. The resources that can be controlled are, among others, CPU, memory usage, network, and disk bandwidths. In the event of resource congestion, the <kbd>cgroups</kbd> mechanism will make sure that the group won't exceed its quota for that resource.</p>
<div class="packt_infobox">In the case of containers, a group may be created for each container. As a result, we would be able to provide a quota for all of the containers. We may, for example, assign 1/5 of the CPU to one of the containers. This would guarantee that, in the case of a congestion, this container would have access to that amount of the CPU cycles. As a result, we are able to guarantee resource access. The <kbd>cgroups</kbd> limit is only enforced during the congestion. In our example, if all other containers are idle, the container that was assigned a fraction of CPU quota may use more CPU cycles.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Docker implementation of containers</h1>
                </header>
            
            <article>
                
<p>You have just learned that the Linux kernel provides tools that enable isolation of resources, laying the ground for the following the implementation of system-level virtualization. But we have to ask ourselves some questions: which applications will be run inside the container? Which libraries and files will be visible in individual containers?</p>
<p>To answer these questions, we will introduce you to Docker images. As we have hinted at before, virtual machines in hardware virtualization can be stored as images, which make it possible to store the state of the virtual machine and, as a result, allow for the creation of reusable preconfigured virtual machines and portability.</p>
<p>The same feature concerns containers and the implementation that we will use: Docker. As you will learn in the following sections, this idea has been developed to a whole different level, providing us with an <span>efficient and</span> convenient image ecosystem and, as a result it, provides a base environment for our cloud infrastructure. However, let's start at the beginning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Images and containers</h1>
                </header>
            
            <article>
                
<p>In Docker nomenclature, there is a distinction between an image and a container. An image is an immutable, unambiguously identifiable collection of files and metadata. A container, on the other hand, is a runtime instance of an image. There can be many container instances of the same image, each of which is mutable and has its own state.</p>
<p>Let's make it clear it with the help of an example. You can start a Fedora distribution in a container. To do so, you have to download and build a Fedora image. After the build, the image will be located on your machine and will contain the Fedora distribution. As we mentioned in the preceding paragraph, this image is an immutable template that can be used to start a container. When you start the container based on the Fedora image and log in to it, you will see that you have access to the bare Fedora distribution. You can, among other things, install software and create files there. When you do this, you modify only that specific container. If you run another container from the same Fedora image, you will again have access to the bare Fedora distribution.</p>
<p>The preceding example gives you a bird's-eye view of the behavior of containers and images. Let's now take a closer look at the architecture of both.</p>
<p>Images are described by <kbd>Dockerfiles</kbd>. A <kbd>Dockerfile</kbd> is a text file, which contains a list of commands that instruct you on how to assemble an image.</p>
<p>Images have a layered structure. The commands executed in a <kbd>Dockerfile</kbd> result in the creation of additional layersâ€”every subsequent layer is different from the previous one.</p>
<p>Every image must derive from another image (possibly, an explicitly empty <em>scratch</em> image), and it adds its layers on top of it. Image layers are built directly on top of the kernel code.</p>
<p>Let's clarify all those concepts by taking a look at a practical example. We will create a bunch of simple images.</p>
<p>Firstly, we will create a number of files in the local directory from which we will build the images:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4495867f-c8f7-4ee7-b602-fa939e73ef28.png"/></div>
<p>Let's create the first image now (note that we are aiming at architecture description; so, although we will briefly explain the <span>commands</span> used, refer to <span>(Further reading, link 1)</span> if you are interested in the details):</p>
<pre><span>FROM centos:7<br/>RUN useradd -ms /bin/bash tomek <br/>USER tomek <br/>WORKDIR /home/tomek</span></pre>
<p>The preceding <kbd>Dockerfile</kbd> represents the base image. It derives from the <kbd>centos:7</kbd> image, which is a bare Centos distribution, adds the <kbd>tomek</kbd> user (<kbd>#2</kbd>), and switches the previous user to <kbd>tomek</kbd> so that all subsequent commands will be run as this user (<kbd>#3</kbd>), therefore the directory from any further commands will be executed to <kbd>tomek</kbd>'s <kbd>homedir</kbd>.</p>
<p>In order to build an image from <kbd>Dockerfile</kbd>, we have to execute the following command in the directory in which the image is located:</p>
<pre><strong>docker build -t base </strong></pre>
<p>In the preceding command, we have tagged the image as base. As a result, we are able to refer to it by its base name. Let's move on to the second Dockerfile:</p>
<pre><span>#1<br/>FROM base <br/>#2<br/>COPY A A <br/>#3<br/>COPY B B<br/></span></pre>
<p>The preceding image derives from the base image created before that. It copies directories <kbd>A</kbd> (<kbd>#2</kbd>) and <kbd>B</kbd> (<kbd>#3</kbd>) from our local filesystem to the image. Similarly, let's build and tag the image:</p>
<pre><span>docker build -t middle .</span></pre>
<p>Finally, take a look at the last image:</p>
<pre><span>#1<br/>FROM middle <br/>#2<br/>COPY C C<br/></span></pre>
<p><span>It derives from the middle image (<kbd>#1</kbd>) and copies the <kbd>C</kbd> directory (<kbd>#2</kbd>) from the local filesystem to the image.</span></p>
<p>The following diagram presents the layers of the top image. We have included all the commands from the image hierarchy so that you can take a look at how the top image is assembled from scratch:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34b0d337-fa20-4cf8-bf9c-b461bb638d3f.png" style=""/></div>
<p>As you will notice in the preceding diagram, each command from all the <kbd>Dockerfiles</kbd> is translated into an additional layer. The base image, for example, derives from the <kbd>centos:7</kbd> images and, as a result, its first layer is added on top of <kbd>centos</kbd> layers. Similarly, the top image is based on the middle image, and as a result, the layer resulting from the execution of the <kbd>COPY C</kbd> command is added on top of layers from the middle image.</p>
<p>We now understand how <kbd>Dockerfiles</kbd> translate to the image layer structure. However, why is this structure important? It is important because it enables images to share the layers. Let's find out how.</p>
<p>When we build the image, the commands from the <kbd>Dockerfile</kbd> will be executed and all the layers will be created. As you saw in the preceding example, images are connected to each other and can share layers. A layer that is used by multiple images has to be created only once.</p>
<p>In the preceding example, the middle and top images share all the layers from the middle image. If we decide to build the middle image, all its layers will also be created. If we later build the top image, none of the middle layers have to be created again.</p>
<p>To understand why such sharing of layers is possible, we must understand how <span>the layers influence filesystem</span> behavior <span>after the container is started.</span></p>
<p>When we start the container based on an image, we are inserting another layer on top of an image from which the container was started. The container will write its changes to this layer, but the result won't change the image, which is immutable. To understand why this is the case, we must understand how layers influence filesystem behavior at runtime. Let's find out.</p>
<p>When Docker starts a container, it creates a filesystem that will be mounted as the root filesystem of the container. Such a filesystem overlays all the layers from the image and the writable layer, creating the filesystem that appears to combine the files and directories from all the layers that constitute the image. Let's return to our example to show how it works.</p>
<p>Let's create three containers based on the images from our previous example:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9b04ba0e-cf32-43ce-8d89-99676233e13e.png" style=""/></div>
<p>In the preceding diagram, the top rectangle represents the images that we have created. The middle image has added two layers corresponding to directories <kbd>A</kbd> and <kbd>B</kbd>. The top image has added one layer with the <kbd>C</kbd> directory. There are three containers. Two containers (<strong>2</strong> and <strong>3</strong>) are based on the top image, and <strong>CONTAINER1</strong> is based on the middle image. The rectangles at the bottom represent each container's writable layer.</p>
<p>Since all layers have been combined into one filesystem, all containers will see the whole operating system distribution. The content of the home directory will be different for <strong>CONTAINER1</strong>, <strong>CONTAINER2</strong>, and <strong>CONTAINER3</strong>: <strong>CONTAINER1</strong> will see only <kbd>A</kbd> and <kbd>B</kbd> directories in its <kbd>home</kbd> folder, whereas <strong>CONTAINER2</strong> and <strong>CONTAINER3</strong> will see <kbd>A</kbd>, <kbd>B</kbd>, and <kbd>C</kbd> directories.</p>
<p>Let's explain how this overlay is implemented. When a container reads a file, the storage driver (a component responsible for the union filesystem implementation) looks for that file starting from the top layer. If the file is not found, it moves to the layer below it. The cycle repeats till either the file is found or there are no more layers. If a file is not found in any of the layers, we will get a <kbd>File not found</kbd> error.</p>
<p>Let's suppose that <strong>CONTAINER1</strong> wants to read a <kbd>~/C/c.txt</kbd> <span>file</span>. The storage driver starts searching in <strong>CONTAINER3</strong>'s writable layer. Since the file is not there, it moves to the <kbd>COPY C</kbd> layer from the top image. The file is found there and is read.</p>
<p>What happens if <strong>CONTAINER1</strong> wants to read the same file?</p>
<p>The storage driver starts from <strong>CONTAINER1</strong>'s writable layer. Again, it cannot find the file, but this time it moves to the <kbd>COPY B</kbd> layer, which is the top layer of the middle image from which the container was created. The file cannot be found there, nor in any layers below it. We will end up with a <kbd>File not found</kbd> a message.</p>
<p>What if <strong>CONTAINER1</strong> and <strong>CONTAINER2</strong> want to read <kbd>~/B/b.txt</kbd>?</p>
<p>After reading the supposition in the preceding paragraph, you will know that both files can be read. Note, however, that both containers were reading the same file. The <kbd>"COPY B"</kbd> <span>layer</span> is reused by the middle and top images, and the <kbd>b.txt</kbd> <span>file</span> reads from the same image for both containers. Thanks to layers, containers are able to reuse data.</p>
<p>Now, what about writing to a file?</p>
<p>The storage controller uses a copy-on-write strategy when writing a file to a filesystem. The driver looks for a file in all the layers, again from top to bottom. If the file is present in the container's writable layer, it can be directly open for write. If it is present in one of the image's layers, it is copied to a writable layer and opened for write.</p>
<p>Let's return to our example. Let's suppose that <strong>CONTAINER1</strong> wants to write to the <kbd>~/A/a.txt</kbd> <span>file</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/32e39a1d-0bd7-4f77-b630-ecb5c1eac610.png" style=""/></div>
<p>The storage driver has found the <kbd>~/A/A.txt</kbd> file in <strong>COPY A</strong> layer and copied it to the writable layer of <strong>CONTAINER1</strong>. Subsequent reads and writes of the <kbd>~/A/A.txt</kbd> <span>file</span> from <strong>CONTAINER1</strong> will read/write the file from <strong>CONTAINER1</strong>'s writable layer.</p>
<p>Let's suppose that <strong>CONTAINER3</strong> wants to write to the <kbd>~/A/A.txt</kbd> <span>file</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dd7edaf7-abb0-4548-8adb-59f579baa3b3.png" style=""/></div>
<p>The situation is analogous; the file is copied to <strong>CONTAINER3</strong>'s writable layer. Let's look at the current situation. Each container has had access to the Fedora distribution and modified some of its parts.</p>
<div class="packt_tip"><strong>CONTAINER1</strong> and <strong>CONTAINER3</strong> still share most of the data, as only the files modified by the given container are copied to its writable layer.</div>
<p>As you will noticed in the preceding diagram, the implementation of Docker images provides an effective way of storing multiple images on the same host. Then, what about container startup time? The container uses the resources of the underlying Linux kernel and reuses the image layers (provided that they are presentâ€”if not, they are downloaded only once). Because of that, starting the container means creating a writable layer and running the container processes using Kernel isolation features. As you are able to see, such processes are very lightweight compared to the hardware virtualization. As a result, containers can be started and stopped immediately.</p>
<p>By this point, we have described the implications of Docker image architecture on container virtualization performance. You may have doubts regarding the performance of such an implementation in other circumstances. Surely, a layered filesystem would have a performance penalty if we decided to run a database on it. That's a good point that has to be clarified. A Docker-layered filesystem is used to work effectively with Docker containers. The layered filesystem is not meant to store data that requires high performance; this is the role of volumes, which we will learn about in the next chapter.</p>
<div class="packt_infobox"><span>There may be different implementations of a storage driver. For example, copy on write strategy may be implemented on the file or page-cache level. Docker provides a number of implementations, and choosing</span> <span>the correct one depends on your use case.</span></div>
<p><span>If you are interested in the architecture of specific storage drivers, or you are researching which driver is best for your use case, refer to the Docker documentation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker registries</h1>
                </header>
            
            <article>
                
<p>Let's return to the build process of our example container. When we were building the base image for the first time, the following could be seen in the build log:</p>
<pre><span>Step 1 : FROM fedora:26 <br/>Trying to pull repository docker.io/library/fedora ...  <br/>sha256:b27b4c551b1d06be25a3c76c1a9ceefd7ff189f6f8b1711d3e4b230c2081bff3: Pulling from docker.io/library/fedora <br/>Digest: sha256:b27b4c551b1d06be25a3c76c1a9ceefd7ff189f6f8b1711d3e4b230c2081bff3 <br/>Status: Downloaded newer image for docker.io/fedora:26<br/>(...)</span></pre>
<p>As it turns out, the <kbd>fedora:26</kbd> images were downloaded from the <kbd>docker.io</kbd> server. Which service that enables users to download images?</p>
<p>Docker images, just like Maven artifacts or operating system packages, create an ecosystem of interconnected reusable entities. Just like in a Maven or operating system scenario, we need a service that will store and distribute such images. Such service is called a Docker registry.</p>
<p>Docker provides a default registry called DockerHub. DockerHub is a publicly available free registry. If not configured, Docker will use DockerHub as the default registry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker wrap-up</h1>
                </header>
            
            <article>
                
<p>As you can see, Docker is a tool that provides a number of capabilities that are essential building blocks for our cloud architecture, as follows:</p>
<ul>
<li>The isolation implemented on the operating system level, combined with a layered filesystem implementation, makes it possible to share server resources effectively and allows for immediate container startup.</li>
<li>The image ecosystem provides a vast number of images that can be downloaded and used immediately.</li>
<li>The containers run from the same images and operate on different Docker environments, providing consistency in all your environments. As you will learn in the rest of this chapter, this is the key feature of a dynamic cloud environment.</li>
</ul>
<p>All these features make Docker containers a great building block for cloud infrastructure. However, we need more than that. First, there will be a lot of containers in nontrivial environments. We are aiming for a dynamic environment, which enables, among other things, automatic scaling, high availability, and continuous integration as we expect a large number of those containers to be started and stopped within short periods of time. Finally, no matter how cool and efficient Docker images are, we would preferably like to generate them automatically during the build of our applications. All of those issues are <span>resolved</span> for you by OpenShift. Let's continue learning about the OpenShift stack. The next thing that you need to learn is orchestration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Orchestrating Docker</h1>
                </header>
            
            <article>
                
<p><span>We have just learned about a tool that enables us to provide the containersâ€”lightweight, virtual machines using operating system-level virtualization and providing us with isolation, effective resource usage, immediate creation time, and the most repeatable behavior across different environments. This is the first layer in our cloud environment, but it is not our target platform. In a more complex production system, we would have to manage a large number of containers and, obviously, we don't want to do it manually. We will need a tool that will manage our containers in a clever way. Let's meet Kubernetes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes</h1>
                </header>
            
            <article>
                
<p>In order to show you the interface that Kubernetes provides, let's introduce its main architectural concepts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nodes and master services</h1>
                </header>
            
            <article>
                
<p>Kubernetes creates a cluster from a group of computers. The computers that constitute the cluster can be heterogeneous. A cluster may be created on your laptop, a group of workstations, or virtual machines. Also, all types of worker machines can be mixed in one cluster.</p>
<p>In Kubernetes nomenclature, each worker machine is a node and the whole cluster is governed by the master node. Each node runs the Kubelet, which is a daemon that enables communication with the master and the Docker engine so that Kubernetes can deploy containers on it.</p>
<p>The master, on the other hand, is a group of services that coordinate the whole cluster. From the user's point of view, the most important part of the master is its REST API service, which provides an endpoint that allows users to interact with the whole cluster.</p>
<p>The following diagram presents a sample Kubernetes cluster, which we will use in the further description. The master services are represented by the blue circle and each node is represented by a rectangle. The cluster consists of two workstations (cyan), two virtual machines (green), and one laptop. Each node runs Kubelet so that it can connect with the master, and the Docker engine so that it can start containers:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3ebf00e0-807c-47cd-8b39-d7f8eda63e95.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Volumes</h1>
                </header>
            
            <article>
                
<p>In Kubernetes, containers are ephemeral which means that they may be started and stopped often. If the container data is not committed, it will be erased when the container is stopped. As a result, <span>we will need another tool</span> for the storage of data. In Kubernetes, such a functionality is provided by volumes. Volume is a persistent storage implementation, which has an independent life cycle and can be mounted in a number of containers. We will discuss volumes in detail in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pods</h1>
                </header>
            
            <article>
                
<p class="mce-root">In Kubernetes, a pod is a group of containers and volumes, all of which share the same IP address among the cluster.</p>
<p>All content of the pod is guaranteed to run on the same host. As a result, a pod can be thought of as an atomic unit of deployment and scheduling.</p>
<p>The pod concept is needed in order to provide us with the ability to implement decoupled containers. With pods, we are able to collocate a bunch of containers with different functionalities that have to be located together (and possibly share data). Those distinct functionalities can be encapsulated in each container. If, on the other hand, a container would have to be used as an atomic unit of deployment, we may be forced to collocate distinct functionalities in one container in order to ensure that they will be deployed and scaled together, breaking good design principles of low coupling and high cohesion.</p>
<div class="packt_tip packt_infobox">In a large number of scenarios, a pod will contain only one container, and this will be perfectly fine. Pods don't have to comprise many containers, but you are provided with such a possibility so that you can use it when it is necessary for your application.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployments</h1>
                </header>
            
            <article>
                
<p>We already <span>know about</span> the building blocks of a Kubernetes cluster. Now it's time to look at the thing that interests us the most: deploying applications to it.</p>
<p>When you want to deploy an application to the Kubernetes cluster, you have to create the deployment object that contains information about it. Among other things, Kubernetes must know which containers constitute the pod and how many replicas of the pod have to be created. Given that knowledge, Kubernetes will decide on which node's application pods will be deployed, and it will deploy them there:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f856f535-ef0b-4f97-b3aa-a84dcc0bd159.png" style=""/></div>
<p>In the preceding diagram, the deployment, which requires that t<span>he pod be replicated on three hosts,</span> has been created. Let's assume that Kubernetes has decided that the pod will be run on <strong>WORKSTATION1</strong> and the two virtual machines. The deployment object has become a part of the master model. What do we actually mean by that?</p>
<p>It has to be strongly emphasized that deployment isn't an operation that will finish after execution, leaving no further impact on the cluster status. Instead, the deployment effectively adds objects to the description of the desired state of the cluster. It is the role of Kubernetes to make sure that this state will be maintained.</p>
<p>We have hinted that Kubernetes master provides the REST API. This API allows for the creation of an object describing the desired state of the cluster. The deployment is one of those objects. As a result, deploying an application to the cluster is the equivalent of adding one more deployment object to the cluster description.</p>
<p>Kubernetes monitors the state of the cluster and is responsible for making sure that it is equivalent to that description. To clarify it a little more, let's look at a few simple examples.</p>
<p>Let's suppose that one node in the cluster has gone down. As a result, the group of pods that were deployed on it has to be moved to different nodes so that the number of deployed pods matches the description of the deployment. To present it in our example, let's suppose that <strong>VIRTUAL MACHINE 1</strong> has gone down:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a73437f2-7de1-4cae-93e2-50c56c5bad7e.png" style=""/></div>
<p>In such a scenario, master would find out that the <strong>VIRTUAL MACHINE 1</strong> node has failed and, in order to keep the number of pod replicas aligned with the description, it will deploy one more pod on another machineâ€”<strong>LAPTOP 1</strong>, in our example.</p>
<p>Then, what about deploying a new version of an application? We would have to change the version in the deployment description. Kubernetes will find out that the version has changed. It will roll down the pods with the preceding version of an application and start pods with the new application. Let's present this example in our diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5be6545f-50ca-4703-8de4-e11bc0ee2684.png" style=""/></div>
<p>Kubernetes has undeployed the pods that constitute the preceding application, replaced the old deployment object with the new one, chosen on which nodes the updated pods have to be deployed, and, finally, performed deployment on those nodes.</p>
<div class="packt_tip">Because this chapter is aimed at explaining OpenShift architecture, we are using general theoretical examples. We will show you how scaling and deployments are done in practical examples in the following chapters.</div>
<p>The key point to understand now is the principle on which Kubernetes operates: it is responsible for keeping the cluster synchronized with the desired state described by the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Services</h1>
                </header>
            
            <article>
                
<p class="mce-root">You may <span>already</span> have noticed that there is a problem with the dynamic pod deployments described in the preceding section. How are we supposed to connect to the application if we have no idea where its pods are located? To solve this problem, Kubernetes introduced the service concept.</p>
<p class="mce-root">A service is an object that monitors the group of pods that constitutes an application. It contains search criteria that define which pods are the part of an application, and monitors the cluster so that it knows where these pods are located. A service has its own IP address, which is visible from outside the cluster. As a result, the client <span>only</span> has to know the address of a service. The actual cluster status (number of pods and their location) is abstracted away from the client.</p>
<p>Let's present it <span>again</span> in our example:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/952d5674-b267-4779-9183-6afe3088210e.png" style=""/></div>
<p>In the preceding diagram, the call to the master has created the service object in the master's object model, which has resulted in the creation of the service. The created service has an IP address, which is reachable by the external clients. When invoked, the service will load balance the invocation to one of the pods from the cluster. Note that the service constantly monitors the state of the cluster. If, for example, there is a failure in one of the nodes, the service will learn the new locations of the pods and will continue to work correctly.</p>
<p>This example combines together all the concepts that we have introduced. Let's recap them: Kubernetes creates a cluster of heterogeneous worker machines. All machines in a cluster have to be able to run Docker containers and communicate with Kubernetes master services that govern all the clusters. The unit of deployment is a pod, which consists of one or more containers and can have many replicas. A user creates the description of such deployments using the master's API, and Kubernetes is responsible for ensuring that the actual cluster state matches this description. To do that, it has to, among other things, perform health checks of nodes and the application, and redeploy them as necessary. It also has to react to all model changes and modify the cluster accordingly. Owing to all the reasons mentioned in this paragraph, application pods can be located on different nodes, and those locations can change dynamically. As a result, Kubernetes introduces the service concept, which provides the proxy for the external clients to the pods that constitute the application to which the client wants to connect.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Labels</h1>
                </header>
            
            <article>
                
<p>Each object in a OpenShift cluster model can have any number of labels, which are basically key-value properties. This is a very simple, yet powerful, feature.</p>
<p>We will need a way to be able to categorize different types of objects. We can build a directory structure for that, but the problem with that solution is that it's not flexible enough. A directory structure provides one view of objects, but there may be many of them, depending on the user or on the current usage scenario.</p>
<p>Labels, on the other hand, provide full flexibility. Any object can have any number of labels applied. Those labels can be used in queries to find an OpenShift object based on a wide range of characteristics.</p>
<p>As an example, let's take a look at a service object again. The service has to find all the pods that run the application represented by the service. The service finds those pods by <span>querying the</span> labels: it's a common practice for the pod to have an app label set to the name of an application that runs on it. As a result, the service can query for pods with an appropriate app label.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits</h1>
                </header>
            
            <article>
                
<p>Now that we know the most important concepts regarding the architecture of Kubernetes, it is time to look at the big picture of the interface that it provides. Kubernetes takes the group of heterogeneous worker machines and provides its user with a view of a homogeneous container execution environment. Let's think about this for a moment. As a user, you tell Kubernetes, <em>I want to deploy my application, which consists of N pods.</em> Kubernes will place those somewhere in the cluster. If pods have to be scaled up or down, or if there are failures, Kubernetes will take care of moving those pods between the underlying machines, but the technical details of those machines are abstracted away from the user. With Kubernetes, a user sees the cluster as a pool of container execution resources. Furthermore, this view is the same for all the clusters that you may want to run in your environment. The Kubernetes development cluster on your laptop will have the same interface as your production environment.</p>
<p>We must also emphasize the role that containers playâ€”Docker guarantees that two containers built from the same image will behave identically. Combining that fact with the role that Kubernetes plays, we are able to see the cloud view provided by Docker and Kubernetes: a pool of container execution computing resources, which further guarantees repeatable behavior on all environments on which it is deployed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenShift</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we covered the powerful cloud abstraction that is provided by Kubernetes and Docker. On the other hand, we hinted in a number of places that in order to use Kubernetes effectively, you have to, among other things, deal directly with Kubernetes object configurations or Docker files. As we wrote before, we would preferably like to have a tool that will abstract those things away from us, make them either happen automatically, or be configured using easy-to-use tools. Here is where OpenShift steps in. OpenShift adds another layer of abstraction on top of Kubernetes, providing it with additional cluster model features, such as builds, or tools, such as web console. As you will see, the layer added by OpenShift makes all the cloud operations very simple and effectively allows you to focus on the development of your code.</p>
<p>Returning for a moment to the cloud computing types section at the beginning of this chapter, we can say that Kubernetes and Docker provide you with IaaS, whereas OpenShift transforms it into powerful programmer-oriented PaaS.</p>
<p>You will learn about the most important features provided by OpenShift in the following paragraphs. Now, let's quickly highlight the most important features of OpenShift. We will start with the build infrastructure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The build infrastructure</h1>
                </header>
            
            <article>
                
<p>In the previous paragraphs, we suggested that direct Docker images are abstracted away by OpenShift. Now it's time to look at it a little more closely.</p>
<p>OpenShift provides builds and deployment abstractions. Builds are responsible for creating images, and deployments are responsible for deploying those images to the cluster, providing the abstraction above Kubernetes objects, such as replication controllers.</p>
<p>One of the first questions that come to mind now is how are those images built? There are a bunch of options, but the most interesting <span>one</span> for us is source-to-image build. As its name suggests, this kind of build is responsible for automatically turning your code into a Docker image.</p>
<p>As a result, your interaction with OpenShift may be configured as follows: you write your application and push the changes into the GitHub repository. This triggers the source-to-image build, which creates a Docker image, and the creation of the Docker image may trigger automatic deployment. Also, all those steps may be integrated into Jenkins based on a continuous delivery pipeline. As you can see, OpenShift build tools allow you to concentrate only on code development. The interaction with cloud will be done automatically by the build infrastructure.</p>
<p>You will learn more about deployments and about builds (including source-to-image and pipeline builds) in <a href="e5cfc996-0fbf-40e8-a89b-fc9ee72c8861.xhtml">Chapter 9</a>, <em>Configuring Continuous Integration Using Jenkins</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Management of projects and users</h1>
                </header>
            
            <article>
                
<p>This feature is not as fresh as the preceding one, but it's still very important. In order to make OpenShift able to work in an enterprise environment, the concept of the project is necessary.</p>
<p>Kubernetes provides the namespace concept, which enables it to separate the cluster into a group of virtual clusters. This concept namespace doesn't implement access control though. As a result, OpenShift creates the notion of a project, which is a Kubernetes namespace identified by specific annotation and implementing access control policies based on users and user groups.</p>
<p>You will learn more about security microservice applications in <a href="2ad0780e-aeb3-40a8-8af7-f2d26341bb35.xhtml">Chapter 10</a>, <em>Providing Security Using Keycloak</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Minishift</h1>
                </header>
            
            <article>
                
<p>We have already said a lot of good things about OpenShift. It is high time we saw how it works in practice. However, how are you actually able to do that? As a developer, you have access to OpenShift Online. It is a publicly available OpenShift cloud where anyone can open an account and test OpenShift itself.</p>
<p>There is also another option: Minishift. Minishiftis a tool that starts a virtual machine on your local computer and creates an OpenShift cluster inside it. As a result, it enables you to try and test a fully featured OpenShift cluster on your local machine. This is the option that we will use in this book. Let's start by installing it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation</h1>
                </header>
            
            <article>
                
<p>You are able to download its latest version from the GitHub page. You also have to install the virtual machine that you will use and configure your environment variables accordingly. The process is very simple and takes a few minutes to complete. The details of the particular installation steps differ a bit between operating systems. They are described thoroughly in the attached installation guide.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Starting the cluster</h1>
                </header>
            
            <article>
                
<p>After you have installed the cluster, you can start it using the minishift start command. It is a good practice to boost the default parameters to provide enough memory and disk space for the services that we will develop and use:</p>
<pre><strong>minishift start --memory=4096 </strong><span><strong>--disk-size=30gb</strong><br/></span></pre>
<p>After you run the preceding command, you have to wait few minutes for the cluster to start:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4f11a892-ad0c-4edd-95aa-6511ba0fa8b1.png" style=""/></div>
<p>After minishift has started, we <span>can access it using the provided address in the startup log.</span> The first screen is a login screen. On this screen, you can use any credentials (as Minishift is a test tool) and click the <span class="packt_screen">Login</span> button. After you do this, you will see the <kbd>web console</kbd>, which is one way of managing the OpenShift cluster. Let's learn more about it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Web console</h1>
                </header>
            
            <article>
                
<p>A <kbd>web console</kbd> is a graphical tool that enables you to view and manage the content of an OpenShift project. From a technical point of view, the console is a graphical interface that provides convenient abstraction over the OpenShift REST API, which it uses to modify the cluster model according to user operations.</p>
<p>Let's take a look at the main console window:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b37e1787-e8e8-4a6f-9b1c-ae4b14f0fb06.png"/></div>
<p>As you can see in the preceding screenshot, the console allows you to manage projects, view their content, and modify it. The overview (presented in the <span>preceding screenshot</span>) contains an <span>application deployed in the</span> <span>petstore namespace</span>. The menu on the left allows you to view and modify different aspects of the cluster, such as builds, deployments, or persistent resources.</p>
<p>We will use the <kbd>web console</kbd> extensively in the following chapters, where you will be able to take a look at most of its features and capabilities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">YAML notation</h1>
                </header>
            
            <article>
                
<p>Although the majority of configuration can be done using the graphical interface, sometimes it will be necessary to edit the internal representation of OpenShift objects: YAML.</p>
<p>Each object in an OpenShift model can be represented using this kind of notation. If you click on <span class="packt_screen">Applications</span> | <span class="packt_screen">Deployments</span>, choose one of them, click on <span class="packt_screen">Actions</span> in the top-right corner, you will be able to choose the <span class="packt_screen">Edit YAML</span> option. This applies to all objects in the console.</p>
<p>We will be performing this from time to time when such an edit is necessary, informing you about the meaning of the performed operation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CLI</h1>
                </header>
            
            <article>
                
<p>Sometimes, it is more convenient to use a command-line tool instead of graphical interface. OpenShift provides it too. The OpenShift CLI implements the <kbd>oc</kbd> command-line tool, which allows for managing the cluster from the terminal.</p>
<p>You can install the CLI using the instruction provided in.</p>
<p>The first thing that you have to do in order to use <kbd>oc</kbd> is to log in to the cluster, as follows:</p>
<pre>oc login</pre>
<p>You will be asked for your credentials, and will have to provide the same credentials that were used to create your project in the web console.</p>
<p>There are a number of operations that the <kbd>oc</kbd> tool provides. We will use it extensively to get and describe operations. Let's introduce them now.</p>
<p>The <kbd>get</kbd> operation allows you to obtain available information about the availability of a given type of object; let's invoke the command:</p>
<pre>oc get</pre>
<p>The tool will suggest a type of object that you can inspect; let's take a look:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/82c54c38-0cf9-4a6e-8f63-66f9b80e74e0.png" style=""/></div>
<p>Wow! That's a lot, but don't worry, you will learn a lot about most of these in the next chapters. Let's use the <kbd>oc get</kbd> command to inspect the services available in the cluster:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ffcd64b9-21ac-44ea-b131-e980aa585215.png" style=""/></div>
<p>You can also take advantage of labels. If you write:</p>
<pre><span>oc get all -l app=catalog-service<br/></span></pre>
<p>Then you will be able to see all kinds of objects associated with the service.</p>
<p>As you can see in the preceding code, we are able to list the objects that we are interested in <span>using the <kbd>get</kbd> command</span>. If we want to get some more information about them, we need to use the <kbd>oc describe</kbd> command, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c7f26657-45ae-42e4-9737-f5c2f03d58de.png" style=""/></div>
<p>The describe command allows you to read all the information about the given type of object.</p>
<p>You have now learned all the essential information needed to understand OpenShift. Now, it's finally time to try it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Catalog-service on the OpenShift example</h1>
                </header>
            
            <article>
                
<p>We have covered a lot of theory in this chapter and have introduced many concepts that will help you to better understand how OpenShift works under the hood. It is finally time to try it in practice.</p>
<p>We will deploy a <kbd>catalog-service</kbd> with a <kbd>h2</kbd> database. In this example, we will only use the web console and deploy the applications from the book's code repository.</p>
<div class="packt_infobox">Examples reference: <kbd>chapter6/catalog-service-openshift-h2</kbd></div>
<p>Let's start. Let's enter the host address. You will find it in the log of the minishift start command. After entering it into the web browser, you will be welcomed by the user login screen. Let's enter our <span class="packt_screen">Username</span> and <span class="packt_screen">Password</span>.</p>
<p>We will be directed to the welcome screen, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2414d182-561f-4292-b79a-9a2e2dd6f7f8.png" style=""/></div>
<p>Enter petstore as the name of the project. In order to deploy catalog-service to OpenShift, we will use the source-to-image build using the CLI. To start with, ensure that you have logged in to the cluster, as follows:</p>
<pre>oc login</pre>
<p>Then, you need to execute the following command:</p>
<pre><strong>oc create -f https://raw.githubusercontent.com/wildfly-swarm/sti-wildflyswarm/master/1.0/wildflyswarm-sti-all.json</strong></pre>
<p>The preceding command creates a bunch of OpenShift objects that are necessary to start an OpenShift build.</p>
<p>Finally, it is time to start an application:</p>
<pre><span>oc new-app wildflyswarm-10-centos7~https://github.com/PacktPublishing/Hands-On-Cloud-Development-with-WildFly.git --context-dir=chapter6/catalog-service-openshift-h2/ --name=catalog-service<br/></span></pre>
<p>We are fully aware that, at this point, these commands look cryptic. For some time, we will use them as a magical service deployment spell. Don't worry though, in <a href="988f96cb-b157-41a0-8c0c-81c5ec440927.xhtml">Chapter 8</a>, <em>Scaling and Connecting Your Services</em>, they will be fully explained, and you will be able to understand each part of the process. To give you a quick overview now: we are making OpenShift create a build of our service directly from source code. In order to do that, we have to specify the GitHub repository. Because our book repository contains many subdirectories, we have to specify the subdirectory in which this example is locatedâ€”we use <kbd>--context-dir</kbd> for that. We also provide the name, using the <kbd>--name</kbd> command.</p>
<p>For now, let's use the web console to check whether the application has been deployed.</p>
<p>Login to the web console again and navigate to <span class="packt_screen">Builds<span><span> | </span></span></span><span class="packt_screen">Builds</span> on the left-hand side. You will be able to see that the build has indeed started:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2df1108-d9ad-44bb-81e5-fdf606af338c.jpg"/></div>
<p>Wait till the build has finished, then navigate to <span class="packt_screen">Applications | </span><span class="packt_screen">Services</span>, and select <span class="packt_screen">catalog-service</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2aaea04d-8621-43f9-b60a-92ff25c8207f.png"/></div>
<p>As you can see in the preceding screenshot, OpenShift has deployed the application on one pod and made a service for it.</p>
<p>Before we can check the application, we have to do one more simple thing. The services are not visible outside of the cluster, and as a result, in order to use them, we have to expose them on the address visible from the external network. OpenShift provides a tool that enables us to use these routes. In the preceding view, click on the <span class="packt_screen">Create route</span> link, don't change anything, and click the <span class="packt_screen">Create</span> button. After that, you will be able to see the external address of the service:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ebb4abc6-5a4f-45f3-96f0-8581f9d95df9.png"/><br/></div>
<p>Finally, we are ready to check the operation of our service. Copy the external address of the host and add the REST path to it:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f65364c-e200-4736-9f47-311f28d1fc4f.png" style=""/></div>
<p>It works. Congratulations! You have deployed your first service in OpenShift.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned a lot about OpenShift architecture. You were introduced to cloud computing and was provided the most essential information about it. Later, you learned about the architecture of OpenShift: a Kubernetes cluster using Docker images with an OpenShift layer, making cluster easy, to use and allowing developers to focus on coding.</p>
<p>Later in the chapter, you started your own local OpenShift instance using the Minishift tool and deployed your first service on it.</p>
<p>In the next chapter, you will learn how to configure persistent storage for microservices deployed in the cloud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ol>
<li><span><a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a></span></li>
<li><a href="https://www.docker.com/">https://www.docker.com/</a></li>
<li><a href="https://openshift.io/">https://openshift.io/</a></li>
<li><a href="https://github.com/minishift/minishift">https://github.com/minishift/minishift</a></li>
<li><a href="https://docs.openshift.org/latest/minishift/getting-started/installing.html">https://docs.openshift.org/latest/minishift/getting-started/installing.html</a></li>
<li><a href="https://docs.openshift.com/enterprise/3.1/cli_reference/get_started_cli.html">https://docs.openshift.com/enterprise/3.1/cli_reference/get_started_cli.html</a></li>
</ol>
<p> </p>


            </article>

            
        </section>
    </body></html>