- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Storage and Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we understood the foundations of modern data engineering
    and what architects are supposed to do. We also covered how data is growing at
    an exponential rate. However, to make use of that data, we need to understand
    how to store it efficiently and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on learning how to store data. We will start
    by learning about various types of data and the various formats of the available
    data. We will briefly discuss encoding and compression and how well they work
    with various data types. Then, we will learn about file and object storage and
    compare these data storage techniques. After that, we will cover the various kinds
    of databases that are available in modern data engineering. We will briefly discuss
    the techniques and tricks to choose the correct database for a particular use
    case. However, choosing the correct database doesn’t guarantee a well-built solution.
    As a data architect, it is important to know how to best design a solution around
    the database so that we can make the most of the technology we have chosen and
    have an effective, robust, and scalable data engineering solution in place.
  prefs: []
  type: TYPE_NORMAL
- en: We will end this chapter by discussing how to design data models for different
    kinds of databases. To help you understand these critical concepts, we will provide
    hands-on real-world scenarios wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data types, formats, and encodings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding file, block, and object storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data lake, data warehouse, and data mart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases and their types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data model design considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data types, formats, and encodings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about the various data types and data formats.
    We will also cover compression and how compression and formats go together. After
    that, we will briefly discuss data encodings. This section will prepare you to
    understand these basic features of data, which will be of use when we discuss
    data storage and databases in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All datasets that are used in modern-day data engineering can be broadly classified
    into one of three categories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured data**: This is a type of dataset that can easily be mapped to
    a predefined structure or schema. It usually refers to the relational data model,
    where each data element can be mapped to a predefined field. In a structured dataset,
    usually, the number of fields, their data type, and the order of the fields are
    well defined. The most common example of this is a relational data structure where
    we model the data structure in terms of an *entity* and a *relationship*. Such
    a relational data structure can be denoted by crows-foot notation. If you are
    interested in learning the basics of crows-foot notation, please refer to [https://vertabelo.com/blog/crow-s-foot-notation](https://vertabelo.com/blog/crow-s-foot-notation).The
    following diagram shows an example of structured data in crows-feet notation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Structured data representation ](img/B17084_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Structured data representation
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see a system consisting of three structured
    datasets called `Customer`, `Order`, and `Product`. Each of these datasets has
    a fixed number of fields and their corresponding data types. We can also see the
    relationship between the datasets. For example, here, `Order` is related to `Customer`
    via `customer_id` and `Order` is related to `Product` via `product_id`. Since
    structured datasets have relationships between them, they are also called relational
    data models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unstructured data**: This is a type of data or a dataset that doesn’t conform
    to any predefined data model. Due to the lack of any internal structure, they
    cannot be stored by any relational data stores such as **Relational Database Management
    Systems** (**RDBMSs**). Also, since there is no schema attached to it, querying
    and searching is not as easy as in a structured data model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Around 70% of the data that’s generated by systems is unstructured. They can
    either be generated by humans or by machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human-generated unstructured data**: A few examples of human-generated unstructured
    datasets are media files such as audio and video files, chat, instant messaging,
    phone call transcriptions, and text messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine-generated unstructured data**: A few examples of machine-generated
    unstructured data are scientific data such as seismic imagery, digital surveillance,
    and satellite imagery'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-structured data**: This is a type of dataset that, unlike a relational
    data model, doesn’t contain a tabular structure, but still contains markers or
    tags to define the hierarchy and field names of the data model. Semi-structured
    data is hierarchical. Semi-structured data is especially useful for platforms
    and programming language-agnostic communication between different systems. Before
    we discuss a few types of semi-structured data, let’s look at a real-world example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mastercard, Visa, and **American Express** (**Amex**) are card networks that
    connect payment processors with issuers. Usually, there are a lot of **business-to-business**
    (**B2B**) sales on card networks, where a merchant buys subscription plans to
    accept a card network, thus increasing the card networks’ revenue stream. For
    example, my dentist accepts only Mastercard and Amex, while Costco stores now
    only accept Visa all over the US. Each of these huge card networks has many Salesforce
    orgs or business units such as accounting, sales, and marketing.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose Visa wants to generate a saleability score and the best time to reach
    a B2B customer. Information gathered from marketing and accounting via Salesforce
    will be used by a real-time **machine learning** (**ML**)-based application, which
    will generate and attach the saleability score and best time to reach the customer.
    The enriched record, along with this extra information, must flow to the Salesforce
    org for sales. Salesforce usually uses APEX as a language on the Salesforce cloud
    (which may be hosted in a different OS), while the AI application that generates
    the score and best call time is written in Java and Python and sits over an on-premises
    Kubernetes cluster. For messages to communicate easily between these disparate
    systems (with different OSs and different languages), we would use a form of semi-structured
    data (JSON) that is independent of the OS or the language of the different applications
    involved in this use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at a few of the most popular types of semi-structured data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**JSON** is the short form of **JavaScript Object Notation**. According to
    Wikipedia, it is “*an open standard format that uses human-readable text to transmit
    data objects consisting of attribute-value pairs*.” The following example consists
    of key-value pairs, where the value can be another JSON as well. This JSON has
    at least one key-value pair and is a value; this is known as a nested JSON. An
    array of JSON objects is called a **JSON array**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of a JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are tags denoting field names such as `‘customerId’`.
    The `‘bills’` tag’s value is an array of JSON objects, so, its value is a JSON
    array. Also, since `‘bills’` is not a primitive data type but instead another
    JSON, the preceding JSON is a nested JSON object that shows how JSON has a hierarchical
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: '**XML** denotes **Extensible Markup Language**. As is evident from the name,
    it is an open data format that is both human and machine-readable, in which each
    data value is tagged or marked by a tag that denotes the name of the field. XML
    is very similar to JSON in passing information between disparate systems in a
    platform and language-agnostic way. Like JSON, XML is also a hierarchical data
    structure. XML is the de facto standard for wsdl SOAP APIs. The following is the
    XML structure for the JSON described earlier:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The full source code for the preceding code snippet is available on GitHub at
    [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/sample.xml](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/sample.xml).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, each XML starts with a `root` tag, and each value is encapsulated
    by the tag names. We explored various data types in this section, but we need
    to understand how these types of data are formatted. So, in the next section,
    we will discuss various data formats.
  prefs: []
  type: TYPE_NORMAL
- en: Data formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In data engineering, a dataset can be stored in different kinds of file formats.
    Each format has its pros and cons. However, it is important to know which data
    format is more suitable for certain types of use cases over others. In this section,
    we will discuss the various characteristics of data formats, a few popular data
    formats, and their suitable use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of data formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s review the various characteristics of the data format that makes
    them different. These characteristics also determine when a particular data type
    should be selected over others to solve a business problem. The main characteristics
    of data formats are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.jpg` file can only be opened by applications such as Photos. Text files,
    on the other hand, can only contain characters, can be opened in any text editor,
    and are human-readable. For example, any `.txt` file that can be opened by text
    editors such as Notepad is a text file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema support**: A schema is an outline, diagram, or model that defines
    the structure of various types of data. The schema stores field-level information
    such as the *data type*, *max size*, or *default value*. A schema can be associated
    with the data, which helps with the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data serialization and compression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to communicate the data to all its consumers to easily understand and
    interpret it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data formats may or may not support schema enforcement. Also, a schema can be
    included along with the data, or it can be shared separately with the schema registry.
    The schema registry is a centralized registry of schemas so that different applications
    can add or remove fields independently, enabling better decoupling. This makes
    it suitable for schema evolution and validation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Schema evolution**: As a business grows, more columns get added or changes
    are made to the column data type, which results in the schema changing over time.
    Even as the schema evolves, it is important to have backward compatibility for
    your old data. Schema evolution provides a mechanism to update the schema while
    maintaining backward compatibility. A few data formats, such as **Avro**, support
    schema evolution, which is helpful in an agile business; as a dataset’s schema
    can change over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Row versus columnar storage**: To understand row versus columnar storage,
    let’s take a look at the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Row versus columnar storage ](img/B17084_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Row versus columnar storage
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows how the same data will be stored in row-based storage
    versus column-based storage. This example shows how sales data is stored in a
    columnar format versus a row-based data storage format. In a row-based format,
    all data is stored row-wise – that is, the columns of a specific row are stored
    adjacent to each other. Since the data is stored row-wise, it is ideal for scenarios
    where it is preferable to read or write data row-wise, such as in **Online Transaction
    Processing** (**OLTP**).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, as evident in the preceding diagram, columnar storage stores
    the values of the same column in an adjacent memory block. So, columns are stored
    together. Since it stores data in a columnar fashion, it can optimize storage
    space by storing repetitive column values once and pointers for each row (this
    is indicated by a striking of the repetitive `Clothes` value in the `Columnar
    Storage` part of the preceding diagram). This kind of storage is very useful for
    scenarios where only a subset of columns is read repetitively and doesn’t expect
    transactional writes. Two typical use cases that are widely used are **Online
    Analytical Processing** (**OLAP**) and big data processing. Both are mainly used
    for analytical queries on huge datasets. In big data, the columnar format gives
    the added advantage of being splittable and enables partition creation, which
    helps process the data faster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Splittable**: Another important factor is whether the file can be partitioned
    or split into multiple files. This factor plays a role when the data volume is
    huge or the velocity is too high, as in the case of big data. Big data files can
    be stored in a distributed filesystem such as **HDFS** if the underlying data
    format is splittable. By doing so, processing such partitioned big data becomes
    much faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression**: Data processing performance often depends on the data’s size.
    Compression reduces the size of the data on disk, which increases network I/O
    performance (however, it might take more time to decompress it while processing).
    It also reduces the data packet size when this data flows over the network, and
    hence the data transfer rates as well. The following table shows a few popular
    data compression algorithms and their features:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Name** | **Lossless Compress** | **Compression Ratio** | **Splitable**
    | **Compression Speed** | **Decompress Speed** |'
  prefs: []
  type: TYPE_TB
- en: '| **Gzip** | Yes | 2.7x‒3x | No | 100 MBps | 440 MBps |'
  prefs: []
  type: TYPE_TB
- en: '| **Snappy** | Yes | 2x | No | 580 MBps | 2020 MBps |'
  prefs: []
  type: TYPE_TB
- en: '| **LZ4** | Yes | 2.5x | No | 800 MBps | 4220 MBps |'
  prefs: []
  type: TYPE_TB
- en: '| **Zstd** | Yes | 2.8x | Yes | 530 MBps | 1360 MBps |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Different compression techniques
  prefs: []
  type: TYPE_NORMAL
- en: '**Companion technologies**: Sometimes, the choice of data format is dependent
    on a companion technology. For example, in a Hadoop environment, if we are planning
    to process the data using a Hive MapReduce job, it might be a good idea to use
    ORC format over Parquet format. But on the other hand, if all our transformations
    are done using Apache Spark, Parquet may be a better choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned about the features and characteristics of various
    data formats and how they affect the storage and processing of data elements.
    However, it is important for an architect to be aware of the popular data formats
    and how they can be used judiciously.
  prefs: []
  type: TYPE_NORMAL
- en: Popular data formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will discuss a few popular data formats that are worth
    knowing about. You will encounter these when you try to develop a data engineering
    solution. They are as follows (we covered two popular data formats, JSON and XML,
    in the *Semi-structured data* section, in the *Data types* subsection):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Delimiter Separated Format**: This is a text data format where newline is
    used as a record delimiter and there can be specific field delimiters based on
    which type of delimiter-separated file we are dealing with. Two of the most popular
    delimiter-separated formats are **Comma Separated Value** (**CSV**) and **Tab
    Separated Value** (**TSV**). While in CSV, the field delimiter is a comma, for
    TSV it is a tab. Optionally, they can have a header record. Although it doesn’t
    support splitting, it provides a very good compression ratio. This format doesn’t
    support null values or schema evolution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the simplicity of the format, it is quite popular in batch processing
    scenarios as well as real-time stream processing. However, the lack of schema
    evolution, partitioning capabilities, and non-standardized formatting makes its
    usage limited and not recommended for many use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '`avro-tools-<version>.jar`. The command to convert Avro into a human-readable
    JSON format is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`avro` data is always accompanied by its schema, which can be read using `avro-tools-<version>.jar`,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had a binary Avro file equivalent to that of the JSON described while
    explaining Semi-structured data in the *Data types* section, the `avro` schema
    would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The full source code for the preceding code snippet is available on GitHub at
    [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/avroschema.json](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/blob/main/Chapter02/avroschema.json).
  prefs: []
  type: TYPE_NORMAL
- en: '**Parquet** is an open source column-based data storage that’s ideal for analytical
    loads. It was created by Cloudera in collaboration with Twitter. Parquet is very
    popular in *big data engineering* because it provides a lot of storage optimization
    options, as well as provides great columnar compression and optimization. Like
    Avro, it also supports splittable files and schema evolution. It is quite flexible
    and has very good support for nested data structures. Parquet gives great read
    performance; it especially works very well with Apache Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s try to understand how a Parquet file is structured. The following diagram
    shows the Parquet file structure:![Figure 2.3 – Parquet file format ](img/B17084_02_004.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 2.3 – Parquet file format
  prefs: []
  type: TYPE_NORMAL
- en: A Parquet file contains a header and a footer. The header only consists of a
    marker called **PAR1**, denoting that it’s a parquet file. Then, the file is divided
    into row groups, where each row group denotes a set of rows contained in a chunk
    of data. Each chunk of data is equal to the block size of the Parquet file (128
    MB by default). Each row group contains a chunk for every column. Again, each
    column chunk consists of one or more pages. Each page in a column consists of
    *n* number of rows whose size is less than or equal to a configured page size.
    Each column chunk stores metadata as well (min/max value, number of nulls, and
    so on). The footer contains metadata for all row groups, as well as the schema
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimized Row Columnar** (**ORC**): This is yet another open source file
    format developed in the Hadoop ecosystem by Hortonworks in collaboration with
    Facebook. ORC is another form of columnar data storage that supports excellent
    compression and column optimization. Let’s look at the ORC file structure to understand
    how it is different from the Parquet format. The following diagram shows the structure
    of an ORC file format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.4 – ORC file structure ](img/B17084_02_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – ORC file structure
  prefs: []
  type: TYPE_NORMAL
- en: In ORC format, each file consists of multiple strips. The default stripe size
    is 250 MB. Each stripe is subdivided into index data, row data, and a stripe footer.
    The index data contains indexes and the row data consists of actual data, but
    both of them are stored in columnar format. The stripe footer contains column
    encodings and their location. The file footer contains information about the list
    of stripes, the number of rows in each stripe, and the data type of each column.
    Apart from that, it contains stripe-level statistical information such as min,
    max, and sum. Finally, the postscript contains information regarding the length
    of the file’s footer, metadata section, and compression-related information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now understand how to choose from the different data formats.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose between Avro, Parquet, and ORC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To choose the correct data format, we must consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Read or write-intensive query pattern**: For a write-intensive use case,
    a row-based format works better as appending new records becomes easier. So, Avro
    would be a better choice for write-intensive use cases. On the other hand, if
    a read-intensive use case needs to read a subset of columns more frequently, a
    columnar data format such as Parquet or ORC is a suitable choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression**: This is a very important aspect when choosing a data format
    because compression reduces both the time and storage required to store or transmit
    data. For big data use cases, compression involves a huge role. Row-based storage
    is not suitable for such scenarios. So, for big data analytical use cases, columnar
    storage such as Parquet or ORC is preferred. Also, more compression is required
    if transforming/processing big data creates a lot of intermediate reads and writes.
    In such a scenario, ORC is preferred because it gives a better compression ratio
    than Parquet. For example, if you are running a MapReduce job or a **Hive Query
    Language** (**HQL**) query on Hive with the MapReduce engine, ORC will perform
    better than Parquet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema Evolution**: In many data engineering use cases, schemas change frequently
    over time as new columns get added or dropped as business requirement changes.
    If there are frequent changes in the data schema and you need backward schema
    compatibility, Avro is the best choice. Avro supports very advanced schema evolution,
    compatibility, and versioning while keeping the schema definition simple in JSON
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nested Columns**: If your use case is suitable for a row-based format, Avro
    works great with the nested column structure. Otherwise, if the use case is suitable
    for a columnar data format and you have a lot of nested complex columns, then
    Parquet is the ideal data format for such use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform Support**: Finally, the platform or framework plays a very important
    role. Hive works best with ORC, while Apache Spark and Delta Lake have great support
    for Parquet. Avro or JSON is often a good choice for Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned about various data formats such as text, Parquet,
    Avro, and others. In the next section, we will learn how data (which can be in
    text, Parquet, or any other format) can be stored using different data storage
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding file, block, and object storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the various data storage formats that are essential
    for an architect who is planning to store their data. Data storage formats organize,
    keep, and present data in different ways, each with its pros and cons. The available
    data storage formats are file, block, and object.
  prefs: []
  type: TYPE_NORMAL
- en: File storage organizes and exposes data as a hierarchy of files and folders,
    whereas block storage divides the data into chunks and stores them in organized,
    evenly sized volumes. Finally, object storage manages the data in a space-optimized
    fashion and links it to its associated metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s dive deeper by discussing their basic concepts, pros and cons, and
    the use cases where they are applied. Let’s begin by discussing the simplest and
    the oldest of them all: file storage.'
  prefs: []
  type: TYPE_NORMAL
- en: File storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In file-level storage, data is stored as a single piece of information inside
    a file. This file is given a name, can contain metadata, and resides inside a
    directory or subdirectory. When you need to find a file, the computer needs to
    know the absolute path of the file to search and read the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pros and cons of file storage are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: It’s simple, has broad capabilities, and can store anything'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Not ideal for storing huge volumes as there is no option to scale
    up, only to scale out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few typical use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: File storage is ideal for file sharing in offices and other environments for
    its sheer simplicity; for example, NAS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local archiving. NAS provides excellent support for storing archival data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data protection and security. File-level storage is an old technology, but due
    to the test of time and a broad variety of uses, its policy, standard, and protection
    capabilities are all advanced. This makes it a great candidate for data protection
    use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now take a look at block-level storage.
  prefs: []
  type: TYPE_NORMAL
- en: Block storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In block-level storage, the data is divided into small chunks of data and assigned
    unique chunk identifiers. Since the chunk of data is small and has a unique identifier,
    it can be stored anywhere. Also, a group of data chunks consists of a logical
    unit called a volume. In block-level storage, you can add a volume of data easily
    to scale up the infrastructure by adding blocks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the interesting things is how it handles metadata. Unlike a file-based
    architecture, there are no additional details associated with block storage other
    than its address. Here, the operating system controls the storage management,
    which makes it ideal storage for high-performance use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pros and cons of block-level storage are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: It provides metadata handling by controlling the OS or database,
    making it highly performant. You can also easily scale storage up and down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: It can be expensive. Also, externalizing metadata handling in the
    application layer means more headaches when managing metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few typical use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Databases**: Databases usually use block storage. For example, AWS Relational
    Data Service uses AWS Elastic Block Storage volumes as its storage to store data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtualization**: Virtualization software such as **VMware**, **Hyper-V**,
    and **Oracle VirtualBox** use block storage as their filesystems for the virtual
    operating system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud-based instances**: Cloud-based instances such as AWS EC2 use block
    storage (AWS Elastic Block Storage) as their hard disk storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Email servers**: Microsoft’s email server, Exchange, uses block storage as
    its standard storage system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at object-level storage next.
  prefs: []
  type: TYPE_NORMAL
- en: Object storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object-level storage stores data in isolated containers called objects, which
    have unique identifiers and flat structures. This makes data retrieval super easy
    as you can retrieve an object by using the unique identifier, irrespective of
    the location it is stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pros and cons of object-level storage are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: Object storage provides great *metadata flexibility*. For example,
    you can customize metadata so that the application is associated with an object
    or you can set the priority of an application to an object. You can pretty much
    do any customization. This flexibility makes object storage strong and super easy
    to manage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from metadata flexibility, object storage is known for its *accessibility*
    as it has a REST API to access, which makes it accessible from any platform or
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Object storage is extremely *scalable*. Scaling out object architecture is as
    simple as adding nodes to an existing storage cluster. With the rapid growth of
    data and the cloud’s pay-as-you-go model, this feature has helped object storage
    become the most sought-after storage for data engineering needs of the present
    and future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cons**: With so many positives, there are certain drawbacks to object storage.
    The biggest and most notable one is that objects can’t be modified. However, you
    can create a newer version of the object. In some use cases such as big data processing,
    this is a boon instead of a headache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few typical use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Big data**: Due to scalability and metadata flexibility, huge volumes, as
    well as unstructured data, can be stored and read easily from object storage.
    This makes it suitable for big data storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud**: Again, due to scalability, object storage is a perfect candidate
    for cloud systems. Amazon S3 is Amazon’s object storage solution and is very popular.
    Also, customizable metadata helps Amazon S3 objects have a life cycle defined
    through the AWS console or its SDKs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web Apps**: Object storage’s easy accessibility using a REST API makes it
    a perfect candidate to be used as a backend for web apps. For example, AWS S3
    alone is used as a cheap and quick backend for static websites.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we have covered the various kinds of data storage. In the next section,
    we will learn how enterprise data (stored in any of the aforementioned storage
    formats) is organized into different kinds of data repositories, which enables
    other applications to retrieve, analyze, and query that data.
  prefs: []
  type: TYPE_NORMAL
- en: The data lake, data warehouse, and data mart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build a data architecture, an architect needs to understand the basic concept
    and differences between a data lake, data warehouse, and data mart. In this section,
    we will cover the modern data architectural ecosystem and where the data lake,
    data warehouse, and data mart fit into that landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the landscape of a modern data architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Landscape of a modern data architecture ](img/B17084_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Landscape of a modern data architecture
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, various types of data get ingested into the data lake, where
    it lands in the raw zone. The data lake consists of structured, semi-structured,
    and unstructured data ingested directly from data sources. Data lakes have a zone
    consisting of cleansed, transformed, and sorted datasets that serve various downstream
    data processing activities such as data analytics, advanced analytics, publishing
    as Data-as-a-Service, AI, ML, and many more. This is called the **curated zone**.
    The data lake acts as a source for creating a data warehouse, which is a structured
    data repository built for a specific line of business.
  prefs: []
  type: TYPE_NORMAL
- en: Data lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a modern data architecture, data from various sources is ingested into a
    data lake. A data lake is a data storage repository that contains structured,
    semi-structured, and unstructured data. In most cases, the usage of the data in
    a data lake is not predefined. Usually, once data is ingested and stored in a
    data lake, various teams use that data for analytics, reports, business intelligence,
    and other usages.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, internally, a data lake contains different data zones. The following
    are the different data zones that are available in a data lake:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Raw data zone**: The raw data from various data sources is loaded into this
    zone. Here, the data that’s loaded is in raw form. This data can be unstructured,
    uncleaned, and unformatted. This is also known as the landing zone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Master data zone**: This data zone usually contains reference data that augments
    the analytical or transformational activities of data present in the raw zone
    or curated zone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User data zone**: Sometimes, in certain data lakes, the user can manually
    drop certain data. They are usually static. This portion of the data lake is called
    the user data zone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Curated data zone**: This is the data publishing layer of the data lake.
    It contains cleansed, transformed, and sorted data. The data present in this layer
    is usually structured. Data may be stored in large flat files, as key-value stores,
    as data documents, in a star schema, or in a denormalized format. All data governance,
    data management, and security policies apply to this layer as this is the main
    consumption layer of the data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Archived data zone**: The archive zone consists of data that has been offloaded
    by other systems such as a data warehouse or the curated zone due to aging. Data
    in this zone can’t usually be modified but can be appended. This kind of data
    is used for historical analysis or auditing purposes. Usually, a cheaper data
    storage technology is used to store archived data. Technologies such as Amazon
    S3 provide more advanced capabilities to progressively move data to cheaper solutions
    automatically as time progresses using an S3 bucket’s life cycle policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to data warehouses next.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data warehouse is a sorted central repository that contains information that
    has been curated from multiple data sources in a structured user-friendly fashion
    for data analytics. A good amount of discovery, analysis, planning, and data modeling
    is required before ingesting the data in a data warehouse. It is highly cleansed,
    transformed, and structured. As evident from *Figure 2.6*, the data warehouse
    is built from a data lake in modern data engineering pipelines. While data lakes
    are usually centralized raw data zones for the enterprise or organization, data
    warehouses are usually built per business unit or department. Each data warehouse
    is structured such that it caters to the need of that particular department. A
    deep dive into data warehouses and their schema types will be discussed in [*Chapter
    4*](B17084_04.xhtml#_idTextAnchor062), *ETL Data Load – A Batch-Based Solution
    to Ingest Data in a Data Warehouse*.
  prefs: []
  type: TYPE_NORMAL
- en: Data marts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data marts are usually a subset of a data warehouse that focuses on a single
    line of business. While a data warehouse is typically few 100 GBs to TBs in size,
    data marts are usually less than 100 GB in size. Data marts provide great read
    performance as it contains data which is analyzed, designed, and stored for a
    very specific line of business. For example, from a centralized company data warehouse,
    there can be a specific data mart for the HR department, one for the finance department,
    and another for the sales department.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table captures the difference between a data lake and a data
    warehouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Characteristics** | **Data Lake** | **Data warehouse** |'
  prefs: []
  type: TYPE_TB
- en: '| **Load Pattern** | ETL(Extract, Load, and Transform) | ETL(Extract, Transform,
    and Load) |'
  prefs: []
  type: TYPE_TB
- en: '| **Type Of Data Stored** | Structured, semi-structured and unstructured |
    Structured |'
  prefs: []
  type: TYPE_TB
- en: '| **Analysis Pattern** | Acquire, analyze, and then determine structure of
    curated data | Create the structure first and then acquire the data for insights
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Ingestion Pattern** | Batch processing, real-time, Batch processing
    near real-time processing | Batch processing |'
  prefs: []
  type: TYPE_TB
- en: '| **Schema Application Time** | Schema-on-read i.e., schema is applied while
    reading the data | Schema-on-write i.e., schema is determined and is available
    when data is written |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Data lake versus a data warehouse
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how various data repositories are used and how they
    enable enterprise data platforms. Data in these repositories can be stored as
    files or objects, but they can be stored in an organized data collection called
    a database, which can retrieve, manage, and search data easily. In the next section,
    we will discuss databases in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Databases and their types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the various types of databases that are commonly
    used to create modern data engineering solutions. We will also try to explore
    the possible scenario when a specific type of database will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'A database is a systematic collection of data or information that’s stored
    in such a way that it can easily be accessed, retrieved, and managed. In modern-day
    data engineering, primarily, databases can be broadly classified into two categories,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relational database**: This is a kind of database known for storing structured
    datasets. Each type of dataset is related to another, and relational databases
    provide an easy way to establish a relationship between different kinds of datasets.
    We will discuss relational databases in detail later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NoSQL databases** or **non-relational databases**: NoSQL databases are non-relational
    databases, where data can be stored in some form other than a tabular format.
    NoSQL supports unstructured, semi-structured, and structured data. No wonder NoSQL
    stands for *Not only SQL*!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the types of databases employed in a modern data
    engineering context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Types of databases  ](img/B17084_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Types of databases
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss the various database types in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Relational database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A relational database, as discussed earlier, stores structural data. Each type
    of data in a relational database is stored in a container called a database table
    or, simply, a table. Each table needs to be defined first before data is loaded
    into that table. The table definition contains the column names or field names,
    their data type, and their size (optionally). Relational databases are further
    subdivided into two types: hierarchical databases and RDBMSs.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are databases where data is stored in a tree-like structure. The databases
    consist of a series of data records. Each record contains a set of fields that
    are determined by the type of records (this is also called a segment). Each segment
    can be related to another segment by relationships called *links*. These types
    of databases are known for *parent-child relationships*. The model is simple but
    can only support one-to-many relationships. The following diagram shows an example
    of a hierarchical database model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – An example of a hierarchical data model ](img/B17084_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – An example of a hierarchical data model
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, `Member` is the root segment. One record
    of the `Member` segment contains the ID `001`. There are two child segments to
    the root segment called `Address` and `Language`. In the `Address Segment` part,
    we can see three record instances – that is, `Address Mail`, `Address Home`, and
    `Address Work`. The `Language Segment` part also has instances such as `Spoken`
    and `Written`.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of hierarchical databases include IBM **Information Management System**
    (**IMS**) and RDM Mobile.
  prefs: []
  type: TYPE_NORMAL
- en: RDBMS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RDBMS is a relational database management system that uses SQL as its programming
    and querying interface. It is the most popular and established kind of database
    across the industry. Data is stored in tables, which represent specific entities.
    Tables have a clearly defined set of columns, along with their data type. Each
    row in a table is called a record. Each table can contain primary keys that uniquely
    identify a record. Each table supports a variety of indexes. One table can be
    linked to another table by a foreign key index. RDBMS can support one-to-many
    and many-to-many relationships. They are very powerful and have been the powerhouses
    behind most modern applications for many decades.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of RDBMSs include MySQL, Oracle, PostgreSQL, Amazon RDS, and Azure
    SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '*When to use*: RDBMS is pretty much used everywhere you need multi-row ACID
    transactions and where you require complex joins. Web applications, employee management
    systems, and financial organization''s online transactions are a few examples
    of where RDBMS is used.'
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NoSQL, as discussed earlier in this section, supports unstructured as well
    as semi-structured data. This is possible because it supports flexible schema.
    Also, NoSQL databases store and process data in a distributed manner and hence
    can scale out infinitely. The usage of distributed computing in NoSQL database
    architectures helps them support a tremendous volume of data and makes them a
    great choice for big data processing. The different ways a relational database
    and a NoSQL database handle scaling can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Scale-up versus scale-out ](img/B17084_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Scale-up versus scale-out
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, a relational database scales up the same instance. However, that
    creates a limitation of scaling. Also, scaling up is a costlier operation. On
    the other hand, NoSQL uses commodity hardware, which is cheap, and the architecture
    is such that to scale it, it needs to be scaled out. This means that NoSQL can
    scale infinitely and is cheaper to scale.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL databases can be further categorized into specific kinds of databases.
    We will discuss each of them briefly while providing examples and usages.
  prefs: []
  type: TYPE_NORMAL
- en: Key-value store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Key-value stores are the simplest kind of NoSQL databases. The data that’s stored
    is in a key and value format. The attribute name is stored in the *key*, while
    the value of the attribute is stored in the *value*. Here, the key needs to be
    a string, but the value can be an object of any type. This means that the value
    can be a JSON, an XML, or some custom serialized object.
  prefs: []
  type: TYPE_NORMAL
- en: A few examples of key-value stores are Redis, Memcached, and RocksDB.
  prefs: []
  type: TYPE_NORMAL
- en: '*When to use*:'
  prefs: []
  type: TYPE_NORMAL
- en: In a microservice or an application. If you need a lookup table that needs to
    be read fast, then an in-memory key-value store such as Redis and Memcached is
    a good choice. Again, while Memcached supports concurrent reads, it doesn’t support
    values that are complex like Redis does. Cloud services such as AWS ElastiCache
    support both of these databases. If you are interested, you can find a more detailed
    comparison between Redis and Memcached at [https://aws.amazon.com/elasticache/redis-vs-memcached/](https://aws.amazon.com/elasticache/redis-vs-memcached/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real-time event stream processing, the state needs to be maintained if the
    current event processing is dependent on the state of an older event. This kind
    of real-time processing is called stateful stream processing. In stateful stream
    processing, RocksDB is a great choice to maintain the state as a key-value pair.
    Kafka Streams uses RocksDB internally to maintain the state for stateful stream
    processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s take a look at document-based databases.
  prefs: []
  type: TYPE_NORMAL
- en: Document-based database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Document databases are NoSQL databases that give you an easy way to store and
    query data from a document. A document is defined as a semi-structured data format
    such as JSON or XML. Document databases support nested elements as well, such
    as nested JSON and JSON arrays. Each document in a document database is stored
    in a key-value pair, where the key is a unique ID for the document and the value
    is the document that is stored. Document databases support indexing on any field
    of the document, even if it is a nested field.
  prefs: []
  type: TYPE_NORMAL
- en: A few examples of document-based databases are MongoDB, Apache CouchDB, Azure
    Cosmos DB, AWS DocumentDB, and ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: '*When to use*:'
  prefs: []
  type: TYPE_NORMAL
- en: When you want to publish curated data from a data lake or a data mart to web
    applications by using a microservice or REST API. Since web applications run on
    JavaScript, they can easily parse a JSON document. Storing a well-designed JSON
    document in a document database such as MongoDB or AWS DocumentDB gives the web
    application amazing performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are receiving feeds from multiple dynamic data feeds, such as social
    media feeds from Twitter, LinkedIn, and Facebook, and the schema of these feeds
    is evolving, you must process and publish this data together by extracting certain
    data points or running some kind of aggregation over them, then Apache CouchDB
    may be an excellent choice. Simply put, if you are consuming document data and
    have no control over the inbound schema, a document-based data store is a great
    choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your lookup needs are not catered by a key-value store. If the value is a
    document that has a very complex schema or that the cost of the storage in the
    key-value store is becoming too high because of the volume of the data, then a
    document-based database is the next most obvious choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are creating a search repository for a business, then you might want
    to store the data in a search engine storage such as Elasticsearch, a document-based
    database. It creates a reverse text index (called the Lucene index) while storing
    the data. This is a special document-based database where each record is stored
    as a document, along with a unique key. Elasticsearch provides amazing search
    performance. However, data should only be stored in Elasticsearch if you want
    to perform a high-performance text-based search over the data or to create some
    visualization out of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now explore columnar databases.
  prefs: []
  type: TYPE_NORMAL
- en: Columnar database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A columnar database stores data in a columnar format. Columnar databases are
    created using Big table. According to a paper published by Google that introduced
    Bigtable, it is *a sparse, distributed, persistent multidimensional sorted map*.
    At its core, each columnar database is a map. Here, each data record is associated
    with a key called the row key. These keys are unique and lexicographically sorted.
    The data that’s stored in a columnar database is persisted in a distributed filesystem
    that provides high availability of the data. Instead of columns, we define column
    families in a columnar database. Each column family can consist of any number
    of columns. The columns inside a column family are not fixed for all records and
    can be added dynamically. This means that in most data records, one or more columns
    may be empty or non-existent, so this data structure is sparse.  This allows you
    to dynamically add columns to a record. This makes columnar databases a great
    choice for storing unstructured data. The following diagram tries to capture the
    essence of a columnar database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Columnar database structure ](img/B17084_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Columnar database structure
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, the records are divided into regions. One
    or more regions reside on a node in a distributed filesystem such as HDFS or GFS.
    Each column family inside a region is stored as a separate file. Again, each column
    inside a column family can support versioning, which makes columnar storage truly
    multi-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: Examples include Apache HBase, Cassandra, and Apache Kudu.
  prefs: []
  type: TYPE_NORMAL
- en: '*When to use*:'
  prefs: []
  type: TYPE_NORMAL
- en: In ad agency and marketing campaigns, a columnar data store is used to store
    the events of user clicks and user choices in real time. These real-time events
    are used on the fly to optimize the ads shown to a user or offers to send to a
    customer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another example is data received from Kafka as a stream of events that are small
    in size. These need to be stored in HDFS so that they can be analyzed or processed
    periodically using some form of batch application. Here, a columnar database is
    preferred since storing the data directly in HDFS or a warehouse such as Hive
    will create too many small files, which, in turn, will create too much metadata,
    thus slowing down the Hadoop cluster’s overall performance. Columnar storage is
    written to disk when the region size is reached and is usually placed in sequential
    files, so they are ideal for this kind of storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are great databases for massive dynamic spikes in data. For example, they
    are great for handling massive data surges when sales are on during the holiday
    season.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s take a look at the graph database.
  prefs: []
  type: TYPE_NORMAL
- en: Graph database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A graph database is a database where data is stored in a graph structure. Essentially,
    this means that graph databases not only store the data but also the relationships
    between it. With the advent of social networking and since the data of every domain
    has become more connected, there is a growing need to not only query data but
    query the connections between the data. Also, in social networks, it is necessary
    to explore neighboring data points (for example, LinkedIn needs to explore data
    adjacency to show whether a person is connected to your profile as a 1st level,
    2nd level, or 3rd level connection). Although relational databases can be used
    to get relationships using joins, a database can store, process, and query connections
    efficiently only if it natively supports relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most graph databases use a popular modeling approach called the **property
    graph model**. Here, data is organized into nodes, relationships, and properties.
    The following diagram shows an example of data stored using the property graph
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Example of a property graph model ](img/B17084_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Example of a property graph model
  prefs: []
  type: TYPE_NORMAL
- en: In the property graph model, there are nodes and relationships. `name`, `data_of_birth`,
    and `employee_ID`.
  prefs: []
  type: TYPE_NORMAL
- en: Relationships are directed and use named connections between two named entities
    or nodes. For example, as shown in the preceding diagram, `HAS_CEO` is a relationship
    between `HAS_CEO` relationship has a property called `start_date`.
  prefs: []
  type: TYPE_NORMAL
- en: Just like SQL standards, which are used to query RDBMS, graph databases can
    be queried using GQL. GQL is a newly announced ISO standard that helps query graph
    databases. One of the more popular open source GQLs available is openCypher. (You
    can learn more about openCypher at [https://opencypher.org/](https://opencypher.org/).)
    Other popular graph database query languages include Cypher, TinkerPop3, and SPARQL.
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of graph databases are Neo4J, ArangoDB, RedisGraph, Amazon Neptune,
    and GraphDB.
  prefs: []
  type: TYPE_NORMAL
- en: '*When to use*:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraud call detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation engines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer engagement on travel websites.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Referral relationships. For example, using a graph database, a healthcare provider
    can identify the various other providers they can get a referral from. This helps
    target specific clients and build a relationship that can be beneficial for both
    providers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps in a marketing campaign to identify influencers in a connected network
    by querying the number of incoming connections to a particular node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed various types of databases and when they should
    be used. We covered a few examples and sample use cases where a particular database
    should be chosen and why. In the next section, we will look at a few considerations
    a data architect should keep in mind while designing data models for various databases.
  prefs: []
  type: TYPE_NORMAL
- en: Data model design considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will briefly discuss various design considerations you
    should consider while designing a data model for the various databases discussed
    in the previous section. The following aspects need to be considered while designing
    a data model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized versus denormalized**: Normalization is a data organization technique.
    It is used to reduce redundancy in a relationship or set of relationships. This
    is highly used in RDBMS, and it is always a best practice in RDBMS to create a
    normalized data model. In a normalized data model, you store a column in one of
    the tables (which is most suitable), rather than storing the same column in multiple
    tables. When fetching data, if you need the data of that column, you can join
    the tables to fetch that column. The following diagram shows an example of normalized
    data modeling using the crows-feet notation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![ Figure 2.11 – Normalized data modeling ](img/B17084_02_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Normalized data modeling
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, none of the columns are repeated or redundant. Now,
    suppose we need to show an order that should display `customer name`, `Customer
    ID`, `Order ID`, `item name`, and `Order date`. To fetch this information, we
    can write a join query, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, if we are designing the same thing for a NoSQL database,
    the focus should not be on reducing redundancy or normalizing the data. Instead,
    we should focus on the read speed. Such a change in design mindset is triggered
    by two important factors. First, NoSQL works with a huge volume of data and stores
    the data in distributed commodity hardware. So, data storage is not costly, and
    joins may not work efficiently if the volume of data is in hundreds of TBs or
    PBs. Second, NoSQL doesn’t have the `JOIN` kind of queries, because NoSQL databases
    are non-relational. The following is an example of a document data model storing
    the same information that needs to be fetched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, a single document contains all the necessary information, so
    this means a lot of redundant data. However, for big data scenarios, NoSQL works
    perfectly fine and provides great performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Query-first versus schema-first pattern**: While designing a data model in
    NoSQL, you must ask yourself what queries are going to run on this data model.
    The data model design in NoSQL usually starts with the kind of analytical query
    that will run on the model. This helps design a correct key for a document or
    a record in a NoSQL database. Also, in the case of a columnar database, it helps
    group columns in a column family based on the query that will run on the data.
    Such optimization helps NoSQL databases run queries with awesome performance on
    big data or unstructured data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, RDBMS is designed to store data in a predefined schema that
    has been normalized and the relationships are very well defined. Since SQL is
    a declarative language and can query tables along with any related table during
    runtime, queries are not considered while designing an RDBMS data model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost versus speed optimization**: With the advent of cloud databases and
    cloud-based solutions, understanding the cost considerations is a very important
    factor for a modern data architect. For example, when it comes to storage versus
    **Input/Output Operations Per Second** (**IOPS**), IOPS are always costlier than
    storage in cloud-based models. However, understanding the difference in how IOPS
    is calculated for an RDBMS or document store can help you save costs and effort
    in the longer term. An RDBMS IOPS is based on the page or the block size. So,
    RDBMS IOPS is determined by the number of pages it has accessed. However, in a
    document database, IOPS is based on the number of DB read/writes that happen in
    that database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another example is that if, in an AWS DocumentDB, you give a greater number
    of indexes, you might get a better speed, but too many indexes will increase IOPS
    considerably, so it might cost you more. A safe limit of indexes per collection
    is five.
  prefs: []
  type: TYPE_NORMAL
- en: '**Indexes**: If you have a database, where you have a huge number of reads
    and you need to have great read performance, then you should consider having indexes
    in your database. Indexes help improve your read and update performance in your
    database. On the other hand, if you have a write-heavy application, indexes can
    slow down your insert performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data distributions**: NoSQL databases are based on the scale-out architecture,
    where the data is stored and distributed across commodity nodes. One of the reasons
    that NoSQL databases have great performance for huge data volumes is that they
    can read or write data in parallel in the distributed nodes. However, if not designed
    properly, the data can be stored unevenly, which can cause a huge volume of data
    to be present in one node. This kind of uneven distribution of data in a distributed
    database is called **data skew**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, the problem of a node containing unusually high amounts of data, which
    can cause read or write bottlenecks for the database, is called **hotspotting**.
    Often, this happens due to a lack of understanding of the design principles of
    NoSQL databases and poor key design. In columnar databases, choosing an incremental
    sequence number as a key often leads to hotspotting. Instead, in both document
    and columnar databases, unique keys should be chosen and a combination of a few
    key column values should be concatenated in a particular order, preferably at
    least one of them being a text value. Techniques such as salting and MD5 encryption
    are used while designing the keys to help avoid hotspotting.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the most obvious design considerations you should
    look at after you have chosen a database. While these considerations are basic
    for any data model design, there are other finer data model design techniques
    that are specific to the database you are choosing. We strongly recommend that
    you go over the official documentation of the database you’ve chosen for your
    solutions before you design your data model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the various data types and data formats that are
    available. We also discussed the various popular data formats that are used in
    modern data engineering and the compression techniques that are compatible with
    each. Once we understood the data types and formats, we explored various data
    storage formats – file, block, and object storage – we can use to store the data.
    Then, we discussed various kinds of enterprise data repositories in detail – data
    lake, data warehouse, and data marts. Once we covered the basics of data, including
    the different types and their storage, we briefly discussed databases and their
    types. We discussed various examples of databases, the USP of each kind of database,
    and when a particular kind of database should be chosen over another. We explored
    possible use cases when a database should be used.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we briefly covered the basic design considerations that a data architect
    should keep in mind while designing their data model using any chosen database.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about data types, formats, databases, and when to use what,
    in the next chapter, we will explore the various platforms where data engineering
    solutions can be deployed and run.
  prefs: []
  type: TYPE_NORMAL
