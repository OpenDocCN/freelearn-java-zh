- en: Chapter 10.  Scaling up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we built a Scala and Play framework application. We
    used the most effective frameworks and tools around the Scala ecosystem, such
    as Play framework and Akka; and we used the Reactive and Functional Programming
    techniques using Futures and RxScala. Additionally, we created reports with Jasper
    and Chat with WebSockets. This is the final chapter, and we will learn how to
    deploy and scale our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Standalone deploy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reactive drivers and discoverability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mid-Tier load-balancer, timeouts, Back pressure, and caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up microservices with an Akka cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up the infrastructure with Docker and AWS cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standalone deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we used the Activator and SBT build and development tools.
    However, when we talk about production, we can't run the application with SBT
    or Activator; we need to do a standalone deploy.
  prefs: []
  type: TYPE_NORMAL
- en: What about standard Java Servlet containers, such as Tomcat? Tomcat is great;
    however, Play is greater. You won't get better performance by deploying your app
    on Tomcat. The standalone play uses Netty, which has superior network stack.
  prefs: []
  type: TYPE_NORMAL
- en: There are some small changes that we will need to make in order to deploy our
    application for Jasper reports. Don't worry; these changes are very simple and
    straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Reports folder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to move the reports template (JRXML files) from the source folder to
    the public folder. Why do we need to do this? Because when we generate the standalone
    deploy, they won't be included in the application jars. What is inside the public
    folder will be packed and deployed into a proper JAR file. That's why we need
    to make this change.
  prefs: []
  type: TYPE_NORMAL
- en: Create a folder called `reports` at `ReactiveWebStore/public/`. Then move all
    the JRXML files there.
  prefs: []
  type: TYPE_NORMAL
- en: Changing report builder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As our templates will be inside a JAR file, we need to change the loading logic
    in order to get the templates properly. Under `ReactiveWebStore/app/report/`,
    we need to change `ReportBuilder.scala`, which should look something like this
    after editing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The main changes that we made were around the `compile` function. Now, we are
    using the Jasper JRXmlLoader in order to load the Jasper template from an `InputStream`
    method. Passing the `InputStream` method provided by the `Play.resourceAsStream`
    function. As you can see, we are passing the new path, `/public/reports/`, in
    order to get the templates. The rest of the code is pretty much the same as the
    one we executed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to deploy. In order to do so, we will need to run a command
    on `activator`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Changing report builder](img/image00332.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As soon as the task is finished, our app will be packed into the `ReactiveWebStore/target/universal/`
    directory, and you will see a `reactivewebstore-1.0-SNAPSHOT.tgz` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you need to extract the file and you shall have the following directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reactivewebstore-1.0-SNAPSHOT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bin`: Scripts to run the app'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conf`: All config files: routes, logging, messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bib`: All JARs including third-party dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`share`: All documentation about the app'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the secret
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we run our standalone application, we will need to apply one more change.
    We need to change the default secret. Locate the `reactivewebstore-1.0-SNAPSHOT/conf/application.conf`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the secret to following in the `application.conf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will need to provide a different value. It can be anything, as long as you
    don't call it `changeme`. If you don't change this, your application will not
    boot up. You can get more information at [http://www.playframework.com/documentation/latest/ApplicationSecret](http://www.playframework.com/documentation/latest/ApplicationSecret).
  prefs: []
  type: TYPE_NORMAL
- en: If you just want to test the deploy for now, let's call it `playworks`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are all set to start the application.
  prefs: []
  type: TYPE_NORMAL
- en: Running the standalone deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to run the app, we will use the script generated by the universal
    task. Go to the `reactivewebstore-1.0-SNAPSHOT` directory and then run `$ bin/reactivewebstore`, which
    would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the standalone deploy](img/image00333.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, you can open your browser and go to `http://localhost:9000/`.
  prefs: []
  type: TYPE_NORMAL
- en: That's it; we have our app up and running. Feel free to test all the features
    we built--they all work!
  prefs: []
  type: TYPE_NORMAL
- en: 'It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the standalone deploy](img/image00334.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Architecture principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalability is about handling more users, traffic, and data; and in order to
    do it, we will need to apply some principles and techniques. Our application is
    already using the most modern techniques and technologies, such as functional
    and ReactiveX programming, RxScala, Akka framework, Play, and much more. However,
    in order to scale, we will need to have an infrastructure in place and other kinds
    of system that will allow us to handle more users.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good application architecture should be created around the following principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Separation of Concerns** (**SOC**) (more details at [https://en.wikipedia.org/wiki/Separation_of_concerns](https://en.wikipedia.org/wiki/Separation_of_concerns))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service Orientation (SOA/microservices)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability/Resiliency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see these principles in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Service orientation (SOA/microservices)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Service orientation is about having a higher level of abstraction, which is
    also called services or microservices. SOA is not about a specific technology,
    but about principles such as shared services, flexibility, and intrinsic operability.
    If you want to learn more about SOA, check out the SOA Manifesto at [http://www.soa-manifesto.org/](http://www.soa-manifesto.org/).
    Microservices is a particular flavor of SOA where the main difference is the focus
    on the granularity, autonomy, and isolation. If you want to learn more about microservices,
    you can check out [https://www.linkedin.com/pulse/soa-microservices-isolation-evolution-diego-pacheco](https://www.linkedin.com/pulse/soa-microservices-isolation-evolution-diego-pacheco)
    as well as [http://microservices.io/](http://microservices.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The right algorithm might make your application work smoothly and the wrong
    algorithm might make your users have a poor experience. Performance is achieved
    by design--first of all, choose the right set of collections and the right set
    of algorithms and frameworks. However, performance needs to be measured and tuned
    eventually. The practice you should do in your application with regard of performance
    is stress testing. The best stress testing tools in the Scala ecosystem is Gatling.
    Gatling ([http://gatling.io/#/](http://gatling.io/#/)) allows you to code in Scala
    using a very simple yet powerful DSL. Gatling focuses on HTTP and latency percentiles
    and distributions, which is the right thing to do nowadays. Latency is not only
    used for the sake of performance and scalability, but it is also heavily related
    to user experience as everything is online.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability/Resiliency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scalability is one of the main reasons why we do software architecture, because
    an architecture that does not scale does not have business value. We will continue
    talking about Scalability principles during this chapter. Resiliency is about
    how much the system can resist and keep operating under the most adverse situations,
    such as hardware failure or infrastructure failure. Resiliency is an old term.
    Currently, there is a new and more modern and accurate principle called antifragility.
    This principle was well developed and used in practice by Netflix. Antifragility
    is about systems and architecture that can adapt and fail over to other systems
    and other operational modes to keep working no matter what. If you want to know
    more about antifragility, you can visit [http://diego-pacheco.blogspot.com.br/2015/09/devops-is-about-anti-fragility-not-only.html](http://diego-pacheco.blogspot.com.br/2015/09/devops-is-about-anti-fragility-not-only.html)
    and [http://diego-pacheco.blogspot.com.br/2015/11/anti-fragility-requires-chaos.html](http://diego-pacheco.blogspot.com.br/2015/11/anti-fragility-requires-chaos.html).
  prefs: []
  type: TYPE_NORMAL
- en: Scalability principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having architecture around these principles makes it possible to scale your
    application up. However, we will still need to rely on other principles and techniques
    to scale it.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several principles and techniques for scalability, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical and horizontal scaling (up and out)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throttling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud computing/containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto Scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reactive drivers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical and horizontal scaling (up and out)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can add more resources, have better hardware, or you can add more boxes.
    These are the two basic ways to scale. You can always improve and tune your app
    to use fewer resources and get more from a single box. Recently, there were several
    improvements in this area around reactive programming that uses fewer resources
    and delivers more throughput. However, there are always limits to which a single
    box can provide in sense of scaling up, which is why we always need to be able
    to scale out.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Databases are great. However, there is a latency cost to call a traditional
    database. A great way to fix this is having a memory cache, which you can use
    as a subset of your data and get the benefit of fast retrieval. Play framework
    has cache support. If you want to learn more, check out [https://www.playframework.com/documentation/2.5.x/ScalaCache](https://www.playframework.com/documentation/2.5.x/ScalaCache).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other options in sense of caching. There are lots of companies that
    use the memory as a definitive data store nowadays. For this, you can consider
    tools such as Redis ([http://redis.io/](http://redis.io/)) and Memcached ([https://memcached.org/](https://memcached.org/)).
    However, if you want to scale Redis and Memcached, you will need something like
    Netflix/Dynomite ([https://github.com/Netflix/dynomite](https://github.com/Netflix/dynomite)).
    Dynomite provides a cluster based on AWS Dynamo paper for Redis, which has the
    following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: High throughput and low latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-region support (AWS cloud)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token aware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent hashing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to learn more about dynomite, check out [https://github.com/Netflix/dynomite/wiki](https://github.com/Netflix/dynomite/wiki).
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A load balancer is a key tool to scale servers. So, let's say, you have 10 boxes
    with our Play framework application or 10 Docker containers. We will need something
    in front of our application to distribute the traffic. There are several servers
    that can do this, such as NGINX ([https://nginx.org/](https://nginx.org/)) and
    Apache HTTP Server ([https://httpd.apache.org/](https://httpd.apache.org/)). If
    you want to scale your application, this is the easiest solution for it. Configuration
    and more details can be found at [https://www.playframework.com/documentation/2.5.x/HTTPServer#Setting-up-a-front-end-HTTP-server](https://www.playframework.com/documentation/2.5.x/HTTPServer#Setting-up-a-front-end-HTTP-server).
  prefs: []
  type: TYPE_NORMAL
- en: Load balancers are often proxy servers as well. You can use them to have HTTPS
    support. If you want, you can have HTTPS on Play framework as well ([https://www.playframework.com/documentation/2.5.x/ConfiguringHttps](https://www.playframework.com/documentation/2.5.x/ConfiguringHttps)).
    Keep in mind that you will need to change swagger embedded installation as all
    the code that we have points to the HTTP interface. If you are doing deploys in
    the AWS cloud, you will need to change some of the configuration to forward the
    proxies, which you can find at [https://www.playframework.com/documentation/2.5.x/HTTPServer#Setting-up-a-front-end-HTTP-server](https://www.playframework.com/documentation/2.5.x/HTTPServer#Setting-up-a-front-end-HTTP-server).
  prefs: []
  type: TYPE_NORMAL
- en: Throttling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is also known as Back pressure. We covered throttling in [Chapter 9](part0110.xhtml#aid-38STS1
    "Chapter 9. Design Your REST API"), *Design Your REST API*. You can get more details
    there. However, the main idea is to limit the request for each user. This is also
    a way to make sure that a single user does not steal all computational resources.
    This is also important from the security point of view, especially for the services
    that are Internet-facing or also known as edge. Another great way to protect and
    have this capability is using Netflix/Zuul ([https://github.com/Netflix/zuul](https://github.com/Netflix/zuul)).
  prefs: []
  type: TYPE_NORMAL
- en: Database cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, the problem is not on the application side, but in the database.
    When we talk about scalability, we need be able to scale everything. We need to
    have the same concepts for databases that we have for the Mid-Tier. For databases,
    it is important to work with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Materialized views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data partition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our application, we used the MySQL database. Here are some resources that
    can help you scale the database and apply the previous concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://dev.mysql.com/doc/refman/5.7/en/faqs-mysql-cluster.html](http://dev.mysql.com/doc/refman/5.7/en/faqs-mysql-cluster.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.fromdual.com/mysql-materialized-views](http://www.fromdual.com/mysql-materialized-views)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://dev.mysql.com/doc/refman/5.7/en/optimization-indexes.html](http://dev.mysql.com/doc/refman/5.7/en/optimization-indexes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://dev.mysql.com/doc/refman/5.7/en/partitioning.html](http://dev.mysql.com/doc/refman/5.7/en/partitioning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html](https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud computing/containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scaling up application in traditional data centers is always hard because we
    need to have the hardware in place. This gets done by the practice of capacity
    planning. Capacity planning is great to make sure we don''t spend money beyond
    our budget. However, it is very hard to get it done right. Software is hard to
    predict, and that''s a great advantage of the cloud. Cloud is just another level
    of abstraction. Hardware and networks become logical, and they are encapsulated
    behind APIs. This makes it easier to scale our application as we can rely on cloud
    elasticity and scale on demand when we need to. However, the architecture needs
    to be ready for this moment and use the tools and techniques described in this
    chapter. Currently, there are several public clouds; the best options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS--Amazon Cloud ([https://aws.amazon.com/](https://aws.amazon.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud ([https://cloud.google.com/](https://cloud.google.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure Cloud ([https://azure.microsoft.com/en-us/](https://azure.microsoft.com/en-us/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Today, Cloud is not the only big elephant in the room. We also have the Linux
    containers, such as Docker ([https://www.docker.com/](https://www.docker.com/))
    and LXC ([https://linuxcontainers.org/](https://linuxcontainers.org/)). Containers
    provide another level of abstraction, and they can run on the cloud or on premises.
    This makes your application more portable and also more cost effective. Containers
    also scale. The main advantage around containers is speed and flexibility. It's
    way faster to boot up a container in comparison with a virtualized image in any
    public cloud. They are also portable and can run everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Auto Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, this is one of the greatest resources of cloud computing. Basically,
    you can define a base image, which is a state of an operational system such as
    Linux, and the cloud will create and destroy instances for you on demand. These
    instances can be created by the increase in computational resources, such as memory,
    CPU, network, or even based on custom rules. This is the key concern in order
    to have elasticity. If you want to learn more about Auto Scaling, check out [https://aws.amazon.com/autoscaling/](https://aws.amazon.com/autoscaling/).
  prefs: []
  type: TYPE_NORMAL
- en: A note about automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to use all these techniques and technologies at scale, we need to have
    full automation ([https://en.wikipedia.org/wiki/List_of_build_automation_software](https://en.wikipedia.org/wiki/List_of_build_automation_software))
    because it is impossible to handle all this with manual work. When we are using
    the cloud or containers, there is no other way around; everything needs to be
    automated. There are several tools that help us achieve this goal, such as Ansible
    ([https://www.ansible.com/](https://www.ansible.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget about telemetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have all infrastructures in place, you will also need to have monitoring,
    alerting, and proper dashboards. There are plenty of great tools for containers
    and public clouds, such as Sensu ([https://sensuapp.org/](https://sensuapp.org/))
    and Prometheus ([https://prometheus.io/](https://prometheus.io/)).
  prefs: []
  type: TYPE_NORMAL
- en: Reactive Drivers and discoverability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reactive Drivers**: We talked a lot and did a lot of reactive code using
    Play framework and RxScala. However, to have full benefits of ReactiveX programming,
    you need to make sure everything is a non-blocking IO and reactive. In other words,
    we need to have all of our drivers reactive. Slick is great because it gives us
    reactivity with the MySQL database. We will need to apply the same principles
    everywhere we have a driver or connection point. There are lots of libraries becoming
    reactive these days. For instance, if you want to cache using Redis, you can use
    Lettuce ([https://github.com/mp911de/lettuce](https://github.com/mp911de/lettuce)),
    which is reactive.'
  prefs: []
  type: TYPE_NORMAL
- en: When we work with microservices, we tend to have hundreds of microservice instances.
    These microservices will run on containers and/or cloud computing units. You can't
    point to specific IPs because the code will not be managed and will not survive
    in a cloud/container environment. Cloud/container infrastructure is ephemeral,
    and you don't know when an instance will be terminated. That's why you need to
    be able to switch to another availability zone or region at any moment.
  prefs: []
  type: TYPE_NORMAL
- en: There are tools that can help us apply these changes in our code. These tools
    are Netflix/Eureka ([https://github.com/Netflix/eureka](https://github.com/Netflix/eureka))
    and Consul ([https://www.consul.io/](https://www.consul.io/)), or even Apache
    Zookeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/)). Eureka
    has one advantage--it is easier to use and has tools around the JVM ecosystem,
    which was battle tested by Netflix.
  prefs: []
  type: TYPE_NORMAL
- en: Eureka is a central registry where microservices register their IP and metadata.
    Eureka has a REST API. Microservices can use the Eureka API to query and search
    existing applications. Eureka can run in a multi-vpc/multi-region environment.
    There are other JVM components, such as ribbon ([https://github.com/Netflix/ribbon](https://github.com/Netflix/ribbon))
    and karyon ([https://github.com/Netflix/karyon](https://github.com/Netflix/karyon)),
    which can automatically register and retrieve eureka information and metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the Eureka information, you can perform microservice load balancing
    and fail over to other availability zones and regions automatically. Why use Eureka
    if I can use DNS? DNS for Mid-Tier load balancing is not the right choice as DNS
    is not flexible and the timeout is quite big. If you want know more about discoverability,
    check out [http://microservices.io/patterns/service-registry.html](http://microservices.io/patterns/service-registry.html).
  prefs: []
  type: TYPE_NORMAL
- en: '![Reactive Drivers and discoverability](img/image00335.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Eureka overview - Eureka architecture overview on the AWS cloud
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, you will deploy the Eureka server at
    least in three **Availability Zones** (**AZs**) in order to have availability.
    Then, Eureka data will be replicated to each server. Our applications or microservices
    will register in Eureka, and other applications/microservices can retrieve this
    metadata, such as IP address, to them to the REST calls. If you want learn more,
    you can check out [https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance](https://github.com/Netflix/eureka/wiki/Eureka-at-a-glance).
  prefs: []
  type: TYPE_NORMAL
- en: Mid-Tier load balancer, timeouts, Back pressure, and caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eureka, Zookeeper, or Consul are only one part of the equation. We still need
    some software on the client side that can use the Eureka information in order
    to do Mid-Tier load balancing, fail over, and caching. The Netflix stack has a
    component for that, which is called ribbon ([https://github.com/Netflix/ribbon](https://github.com/Netflix/ribbon)).
    With ribbon, you can automatically resolve the microservice IPs from Eureka, do
    retries, and failover to other AZs and regions. Ribbon has a cache concept; however,
    it is on preloaded cache.
  prefs: []
  type: TYPE_NORMAL
- en: Ribbon ideas are simple. The great thing about ribbon is that everything is
    reactive, and you can use RxJava and RxScala in order to work with the stack.
    If you don't want to use ribbon, you can still create a simple integration layer
    with Scala and perform the same concerns, such as load balancing, failover, and
    caching.
  prefs: []
  type: TYPE_NORMAL
- en: What about Back pressure? Back pressure can be done with RxJava and Rxscala,
    and you will be able to do it on the client side as well. You can learn more about
    Back pressure in Rx at [https://github.com/ReactiveX/RxJava/wiki/Backpressure](https://github.com/ReactiveX/RxJava/wiki/Backpressure).
  prefs: []
  type: TYPE_NORMAL
- en: So, if I have client-side load balancing, failover, caching, and Back pressure,
    am I good to go? Yes, you are; however, we can always do better. Working with
    microservices is not easy as everything is a remote call, and remote calls can
    fail, hang, or timeout. These cons are hard and dangerous if not managed well.
    There is another solution that can help us a lot with this concept; it is called
    Hystrix ([https://github.com/Netflix/Hystrix](https://github.com/Netflix/Hystrix)).
  prefs: []
  type: TYPE_NORMAL
- en: Hystrix is a library for the JVM designed for latency and fault tolerance protection.
    At a glance, Hystrix is a wrapper around any remote code that can take time or
    go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Hystrix has thread isolation and provides a dedicated thread pool for each resource.
    This is great because it prevents you from running out of resources. It has an
    execution pattern called circuit breaker. Circuit breaker will prevent requests
    from tearing down the whole system. Additionally, it has a dashboard where we
    can visualize the circuits, so, at runtime, we can see what's going on. This capability
    is great not only for sense of telemetry, but also because it is easy to troubleshoot
    and visualize where the problem is.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be further explained with the help of the following flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mid-Tier load balancer, timeouts, Back pressure, and caching](img/image00336.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The code you want to protect will be around a Hystrix command. This command
    can be manipulated in sync or async programming models. The first thing that Hystrix
    will check is if the circuit is closed, which is good, and how it should be. Then,
    it checks if there are threads available for that command, and if there are available
    threads, then the command will be executed. If this fails, it tries to get a fallback
    code, which is a second option that you can provide in case of failure. This fallback
    should be static; however, you can be loading data in the background and then
    return on the fallback. Another option is fallback to other AZ or Region.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a snapshot of how a Hystrix dashboard circuit breaker view would
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mid-Tier load balancer, timeouts, Back pressure, and caching](img/image00337.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding image, we can see the Hystrix dashboard sample, where we can
    visualize critical information, such as success and error rate and if the circuit
    is open or closed. If you want learn more about the Hystrix dashboard, check out
    [https://github.com/Netflix/Hystrix/wiki/Dashboard](https://github.com/Netflix/Hystrix/wiki/Dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up microservices with an Akka cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our application also uses Akka. In order to scale Akka, we will need to use
    an Akka cluster. The Akka cluster allows us to clusterize several Actor systems
    in several machines. It has special Actor routers that are cluster aware, and
    we can use these Actors to route requests to the whole cluster; more details can
    be found at [http://doc.akka.io/docs/akka/2.4.9/java/cluster-usage.html#Cluster_Aware_Routers](http://doc.akka.io/docs/akka/2.4.9/java/cluster-usage.html#Cluster_Aware_Routers).
  prefs: []
  type: TYPE_NORMAL
- en: The Akka cluster provides membership protocol and life cycle. Basically, we
    can be notified by the cluster when a new member joins or when a member leaves
    the cluster. Given this capability, it is possible for us to code a scalable solution
    around these semantics. As we know when a member joins, we can deploy more nodes,
    and we can also drop nodes on demand.
  prefs: []
  type: TYPE_NORMAL
- en: A simple sample would be to create an Actor called frontend, and when we see
    this Actor, we could deploy three backend Actors across the cluster. If the frontend
    Actor leaves, we could undeploy the other Actors. All this logic can be archived
    using the membership protocol and clusters events that Akka generates for us.
    A frontend Actor is a not a UI or web application, it is just an Actor that receives
    work. So, let's say we want to generate analytics around our products catalog.
    We could have a frontend Actor who receives that request and delegates the work
    to backend Actors, which will be deployed across the cluster and deliver the analytical
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is the process view of an Akka cluster membership protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling up microservices with an Akka cluster](img/image00338.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding image, there is a set of states. First of all,
    the node is joining the cluster; then the node can be up. Once the node is up,
    it can leave the cluster. There are intermediate states, such as leaving and existing.
  prefs: []
  type: TYPE_NORMAL
- en: The Akka cluster provides many options to scale our Actor system. Another interesting
    option is to use the pattern of distributed Pub/Sub. If you are familiar with
    JMS Topics, it is almost the same idea. For those who are not familiar, you can
    check out [http://doc.akka.io/docs/akka/2.4.9/java/distributed-pub-sub.html](http://doc.akka.io/docs/akka/2.4.9/java/distributed-pub-sub.html).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want learn more about the Akka cluster, you can check out [http://doc.akka.io/docs/akka/2.4.9/common/cluster.html](http://doc.akka.io/docs/akka/2.4.9/common/cluster.html).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the infrastructure with Docker and AWS cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling up with the AWS cloud is easy, as at any moment, with a simple click
    on the AWS console, you can change the hardware and use more memory, CPU, or better
    network. Scale-out is not hard; however, we need to have good automation in place.
    The key principle to scale is to have the Auto Scaling groups in place with good
    policies. You can learn more about it at [http://docs.aws.amazon.com/autoscaling/latest/userguide/policy_creating.html](http://docs.aws.amazon.com/autoscaling/latest/userguide/policy_creating.html).
  prefs: []
  type: TYPE_NORMAL
- en: There are other interesting services and components that can help you scale
    your application. However, you will need to keep in mind that this can lead to
    coupling. The IT industry is moving toward the container direction because it
    is faster, and it's easy to deploy in other public clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can scale out with Docker as well, because there are cluster managers that
    can help us scale our containers. Currently, there are several solutions. In the
    sense of capabilities and maturity, the following are the better solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm ([https://docs.docker.com/swarm/overview/](https://docs.docker.com/swarm/overview/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes ([http://kubernetes.io/](http://kubernetes.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mesos ([http://mesos.apache.org/](http://mesos.apache.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Swarm**: This is a cluster for Docker. Docker Swarm is very flexible
    and integrates well with other Docker ecosystem tools, such as Docker machine,
    Docker compose, and Consul. It can handle hundreds of nodes, and you can learn
    more about them at [https://blog.docker.com/2015/11/scale-testing-docker-swarm-30000-containers/](https://blog.docker.com/2015/11/scale-testing-docker-swarm-30000-containers/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes**: This was created by Google, and it is a full solution for development
    automation, operation, and scaling Docker containers. The Kubernetes cluster has
    two roles, a master node that coordinates the cluster, schedules applications,
    and keeps applications on a desired state; and there are nodes, that are workers
    that run applications. It can handle hundreds of containers and scale very well.
    To learn more about it, check out [http://blog.kubernetes.io/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12.html](http://blog.kubernetes.io/2016/03/1000-nodes-and-beyond-updates-to-Kubernetes-performance-and-scalability-in-12.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Mesos**: This was created by Twitter. It is very interesting, as you
    can run a bare metal on a premises datacenter or on a public cloud. Mesos allows
    you to use Docker containers as well. If you want to learn more about mesos, check
    out the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mesos.berkeley.edu/mesos_tech_report.pdf](http://mesos.berkeley.edu/mesos_tech_report.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to deploy your Play framework application as
    a standalone distribution. Additionally, you learned several architectural principles,
    techniques, and tools, to help you scale out your application to thousands of
    users.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we also reach the end of this book. I hope you enjoyed this journey.
    We built a nice application using Scala, Play Framework, Slick, REST, Akka, Jasper,
    and RxScala. Thank you for your time. I wish you the best in your coding career
    with the Scala language.
  prefs: []
  type: TYPE_NORMAL
