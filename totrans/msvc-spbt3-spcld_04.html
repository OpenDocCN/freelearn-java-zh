<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer117" class="Basic-Text-Frame">
    <h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-121" class="chapterTitle">Deploying Our Microservices Using Docker</h1>
    <p class="normal">In this chapter, we will start using Docker and put our microservices into containers!</p>
    <p class="normal">By the end of this chapter, we will have run fully automated tests of our microservice landscape that start all our microservices as Docker containers, requiring no infrastructure other than Docker Engine. We will also have run a number of tests to verify that the microservices work together as expected, and finally, shut down all the microservices, leaving no traces of the tests we executed.</p>
    <p class="normal">Being able to test a number of cooperating microservices in this way is very useful. As developers, we can verify that the microservices work on our local developer machines. We can also run exactly the same tests in a build server to automatically verify that changes to the source code won’t break the tests at a system level. Additionally, we don’t need to have a dedicated infrastructure allocated to run these types of tests. In the upcoming chapters, we will see how we can add databases and queue managers to our test landscape, all of which will run as Docker containers.</p>
    <div class="note">
      <p class="normal">This does not, however, replace the need for automated unit and integration tests, which test individual microservices in isolation. They are as important as ever.</p>
      <p class="normal">For production usage, as we mentioned earlier in this book, we need a container orchestrator such as Kubernetes. We will get back to container orchestrators and Kubernetes later in this book.</p>
    </div>
    <p class="normal">The following topics will be covered in this chapter:</p>
    <ul>
      <li class="bulletList">Introduction to Docker</li>
      <li class="bulletList">Docker and Java – Java hasn’t been very friendly to containers historically, but that changed with Java 10, so let’s see how Docker and Java fit together</li>
      <li class="bulletList">Using Docker with one microservice</li>
      <li class="bulletList">Managing a landscape of microservices using Docker Compose</li>
      <li class="bulletList">Automating tests of cooperating microservices</li>
    </ul>
    <h1 id="_idParaDest-122" class="heading-1">Technical requirements</h1>
    <p class="normal">For instructions on how to install the tools used in this book and how to access the source code for this book, see:</p>
    <ul>
      <li class="bulletList"><em class="chapterRef">Chapter 21</em>, <em class="italic">Installation Instructions for macOS</em></li>
      <li class="bulletList"><em class="chapterRef">Chapter 22,</em> <em class="italic">Installation Instructions for Microsoft Windows with WSL 2 and Ubuntu</em></li>
    </ul>
    <p class="normal">The code examples in this chapter all come from the source code in <code class="inlineCode">$BOOK_HOME/Chapter04</code>.</p>
    <p class="normal">If you want to see the changes that were applied to the source code in this chapter, that is, see what it took to add support for Docker, you can compare it with the source code for <em class="chapterRef">Chapter 3</em>, <em class="italic">Creating a Set of Cooperating Microservices</em>. You can use your favorite <code class="inlineCode">diff</code> tool and compare the two folders, <code class="inlineCode">$BOOK_HOME/Chapter03/2-basic-rest-services</code> and <code class="inlineCode">$BOOK_HOME/Chapter04</code>.</p>
    <h1 id="_idParaDest-123" class="heading-1">Introduction to Docker</h1>
    <p class="normal">As we already mentioned in <em class="chapterRef">Chapter 2</em>,<em class="italic"> Introduction to Spring Boot</em>, Docker made the concept of containers as a<a id="_idIndexMarker256"/> lightweight alternative to virtual machines very popular in 2013. To quickly recap: containers are actually processed in a Linux host <a id="_idIndexMarker257"/>that uses <strong class="keyWord">Linux namespaces</strong> to provide isolation between containers, and <strong class="keyWord">Linux Control Groups</strong> (<strong class="keyWord">cgroups</strong>) are used to limit the amount of CPU and memory that a container is allowed to consume.</p>
    <p class="normal">Compared to a virtual machine that uses a hypervisor to run a complete copy of an operating system in each virtual machine, the overhead in a container is a fraction of the overhead in a virtual machine. This leads to much faster startup times and a significantly<a id="_idIndexMarker258"/> lower footprint. Containers are, however, not considered to be as secure as virtual machines. Take a look at the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_01.png" alt="" role="presentation" width="878" height="296"/></figure>
    <p class="packt_figref">Figure 4.1: Virtual machines versus containers</p>
    <p class="normal">The diagram illustrates the difference between the resource usage of virtual machines and containers, demonstrating that the same type of server can run significantly more containers than virtual machines. The main gain is that a container doesn’t need to run its own instance of an operating system as a virtual machine does.</p>
    <h2 id="_idParaDest-124" class="heading-2">Running our first Docker commands</h2>
    <p class="normal">Let’s try to start a container by<a id="_idIndexMarker259"/> launching an Ubuntu server using Docker’s <code class="inlineCode">run</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run -it --rm ubuntu
</code></pre>
    <p class="normal">With the preceding command, we ask Docker to create a container that runs Ubuntu, based on the latest version that’s available of the official Docker image for Ubuntu. The <code class="inlineCode">-it</code> option is used so that we can interact with the container using Terminal, and the <code class="inlineCode">--rm</code> option tells Docker to remove the container once we exit the Terminal session; otherwise, the container will remain in Docker Engine with an <code class="inlineCode">Exited</code> state.</p>
    <p class="normal">The first time we use a<a id="_idIndexMarker260"/> Docker image that we haven’t built ourselves, Docker will download it from a Docker registry, which is Docker Hub by default (<a href="https://hub.docker.com"><span class="url">https://hub.docker.com</span></a>). This will take some time, but for subsequent<a id="_idIndexMarker261"/> usage of that Docker image, the container will start in just a few seconds!</p>
    <p class="normal">Once the Docker image has been downloaded and the container has been started up, the Ubuntu server should respond with a prompt such as the following:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_02.png" alt="Graphical user interface, text, application  Description automatically generated" width="381" height="85"/></figure>
    <p class="packt_figref">Figure 4.2: Ubuntu server response</p>
    <p class="normal">We can try out the container by, for example, asking what version of Ubuntu it runs:</p>
    <pre class="programlisting con"><code class="hljs-con">cat /etc/os-release | grep 'VERSION='
</code></pre>
    <p class="normal">It should respond with something like the following:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_03.png" alt="Graphical user interface, text, application  Description automatically generated" width="741" height="116"/></figure>
    <p class="packt_figref">Figure 4.3: Ubuntu version response</p>
    <p class="normal">We can leave the container with an <code class="inlineCode">exit</code> command and verify that the Ubuntu container no longer exits with the <code class="inlineCode">docker ps -a</code> command. We need to use the <code class="inlineCode">-a</code> option to see stopped containers; otherwise, only running containers are displayed.</p>
    <p class="normal">If you favor CentOS over <a id="_idIndexMarker262"/>Ubuntu, feel free to try the same with the <code class="inlineCode">docker run --rm -it centos</code> command. </p>
    <p class="normal">Once the CentOS server has started running in its container, you can, for example, ask what version of CentOS is running with the <code class="inlineCode">cat /etc/redhat-release</code> command. It should respond with something like the following:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_04.png" alt="Text  Description automatically generated" width="493" height="120"/></figure>
    <p class="packt_figref">Figure 4.4: CentOS version response</p>
    <p class="normal">Leave the container with the <code class="inlineCode">exit</code> command to remove it.</p>
    <div class="packt_tip">
      <p class="normal">If, at some point, you find that you have a lot of unwanted containers in Docker Engine and you want to get a clean sheet, that is, get rid of them all, you can run the following command:</p>
      <pre class="programlisting con"><code class="hljs-con">docker rm -f $(docker ps -aq)
</code></pre>
      <p class="normal">The <code class="inlineCode">docker rm -f</code> command stops and removes the containers whose container IDs are specified by the command. The <code class="inlineCode">docker ps -aq</code> command lists the container IDs of all the running and stopped containers in Docker Engine. The <code class="inlineCode">-q</code> option reduces the output from the <code class="inlineCode">docker ps</code> command so that it only lists the container IDs.</p>
    </div>
    <p class="normal">Now we’ve understood what Docker is, we can move on to learn how to run Java in Docker.</p>
    <h1 id="_idParaDest-125" class="heading-1">Running Java in Docker</h1>
    <p class="normal">Over the past few years, there have been a number of attempts to get Java working in Docker in a <a id="_idIndexMarker263"/>good way. Most importantly, Java<a id="_idIndexMarker264"/> hasn’t historically been very good at respecting limits set for Docker containers when it comes to the use of memory and CPU.</p>
    <p class="normal">Currently, the official Docker image for Java <a id="_idIndexMarker265"/>comes from the <strong class="keyWord">OpenJDK</strong> project: <a href="https://hub.docker.com/_/openjdk/"><span class="url">https://hub.docker.com/_/openjdk/</span></a>. We will use an alternative Docker image<a id="_idIndexMarker266"/> from the <strong class="keyWord">Eclipse</strong> <strong class="keyWord">Temurin </strong>project. It contains the same binaries from the OpenJDK project but provides variants of the Docker images that meet our needs better than the Docker images from the OpenJDK project.</p>
    <p class="normal">In this section, we will use a Docker<a id="_idIndexMarker267"/> image that contains the full <strong class="keyWord">JDK</strong> (<strong class="keyWord">Java Development Kit</strong>) with all its tools. When we start to package our microservices in Docker images in the <em class="italic">Using Docker with one microservice</em> section, we will use <a id="_idIndexMarker268"/>a more compact Docker image that is based on the <strong class="keyWord">JRE</strong> (<strong class="keyWord">Java Runtime Environment</strong>), only containing the Java tools required at runtime.</p>
    <p class="normal">As already mentioned, earlier versions of Java have not been very good at honoring the quotas specified for a Docker container using Linux cgroups; they simply ignored these settings. </p>
    <p class="normal">So, instead of allocating memory inside the JVM in relation to the memory available in the container, Java allocated memory as if it had access to all the memory in the Docker host. When trying to allocate more memory than allowed, the Java container was killed by the host with an “out of memory” error message. In the same way, Java allocated CPU-related resources such as thread pools in relation to the total number of available CPU cores in the Docker host, instead of the number of CPU cores that were made available for the container JVM was running in.</p>
    <p class="normal">In Java SE 9, initial support for container-based CPU and memory constraints was provided, much improved in Java SE 10.</p>
    <p class="normal">Let’s look at how Java SE 17 responds to limits we set on a container it runs in!</p>
    <p class="normal">In the following tests, we will run Docker Engine inside a virtual machine on a MacBook Pro, acting as the Docker host. The Docker host is configured to use <strong class="keyWord">8 CPU</strong> cores and <strong class="keyWord">16 GB of memory</strong>.</p>
    <p class="normal">We will start by seeing how we<a id="_idIndexMarker269"/> can limit the number of available CPUs to a container that<a id="_idIndexMarker270"/> runs Java. After that, we will do the same with limiting memory.</p>
    <h2 id="_idParaDest-126" class="heading-2">Limiting available CPUs</h2>
    <p class="normal">Let’s start by finding out how many available processors (that is, CPU cores) Java sees without applying any <a id="_idIndexMarker271"/>constraints. We can do this by sending the Java statement <code class="inlineCode">Runtime.getRuntime().availableprocessors()</code> to the Java CLI tool <code class="inlineCode">jshell</code>. We will run <code class="inlineCode">jshell</code> in a container using the Docker image that contains the full Java 17 JDK. The Docker tag for this image is <code class="inlineCode">eclipse-temurin:17</code>. The command looks like this:</p>
    <pre class="programlisting con"><code class="hljs-con">echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i eclipse-temurin:17 jshell -q
</code></pre>
    <p class="normal">This command will send the string <code class="inlineCode">Runtime.getRuntime().availableProcessors()</code> to the Docker container, which will process the string using <code class="inlineCode">jshell</code>. We will get the following response:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_05.png" alt="Text  Description automatically generated" width="746" height="147"/></figure>
    <p class="packt_figref">Figure 4.5: Response showing the number of CPU cores available</p>
    <p class="normal">The response of <code class="inlineCode">8</code> cores is as expected since the Docker host was configured to use 8 CPU cores. Let’s move on and restrict the Docker container to only be allowed to use three CPU cores using the <code class="inlineCode">--cpus 3</code> Docker option, then ask the JVM about how many available processors it sees:</p>
    <pre class="programlisting con"><code class="hljs-con">echo 'Runtime.getRuntime().availableProcessors()' | docker run --rm -i --cpus=3 eclipse-temurin:17 jshell -q
</code></pre>
    <p class="normal">The JVM now responds with <code class="inlineCode">Runtime.getRuntime().availableProcessors()$1 ==&gt; 3</code>; that is, Java SE 17 honors the settings in the container and will, therefore, be able to configure <a id="_idIndexMarker272"/>CPU-related resources such as thread pools correctly!</p>
    <h2 id="_idParaDest-127" class="heading-2">Limiting available memory</h2>
    <p class="normal">In terms of the amount of available memory, let’s ask the JVM for the maximum size that it thinks it can<a id="_idIndexMarker273"/> allocate for the heap. We can achieve this by asking the JVM for extra runtime information using the <code class="inlineCode">-XX:+PrintFlagsFinal</code> Java option and then using the <code class="inlineCode">grep</code> command to filter out the <code class="inlineCode">MaxHeapSize</code> parameter, like so:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run -it --rm eclipse-temurin:17 java -XX:+PrintFlagsFinal | grep "size_t MaxHeapSize"
</code></pre>
    <p class="normal">With 16 GB of memory allocated to the Docker host, we will get the following response:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_06.png" alt="Graphical user interface, text  Description automatically generated" width="755" height="127"/></figure>
    <p class="packt_figref">Figure 4.6: Response showing MaxHeapSize</p>
    <p class="normal">With no JVM memory constraints (that is, not using the JVM parameter <code class="inlineCode">-Xmx</code>), Java will allocate one-quarter of the memory available to the container for its heap. So, we expect it to allocate up to 4 GB to its heap. From the preceding screenshot, we can see that the response was 4,188,012,544 bytes. That equals <em class="italic">4,188,012,544 / 1024 / 1024</em> = 3,994 MB, which is close to the expected 4 GB.</p>
    <p class="normal">If we constrain the Docker container to only use up to 1 GB of memory using the Docker option <code class="inlineCode">-m=1024M</code>, we expect to see a lower max memory allocation. Running the command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run -it --rm -m=1024M eclipse-temurin:17 java -XX:+PrintFlagsFinal | grep "size_t MaxHeapSize"
</code></pre>
    <p class="normal">will result in the<a id="_idIndexMarker274"/> response 268,435,456 bytes, which equals <em class="italic">268,435,456 / 1024 / 1024</em><em class="chapterRef"> </em>= 256 MB. 256 MB is one-quarter of 1 GB, so again, this is as expected.</p>
    <p class="normal">We can, as usual, set the max heap size on the JVM ourselves. For example, if we want to allow the JVM to use 600 MB of the total 1 GB we have for its heap, we can specify that using the JVM option <code class="inlineCode">-Xmx600m</code> like so:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run -it --rm -m=1024M eclipse-temurin:17 java -Xmx600m -XX:+PrintFlagsFinal -version | grep "size_t MaxHeapSize"
</code></pre>
    <p class="normal">The JVM will respond with 629,145,600 bytes = <em class="italic">629,145,600 / 1024 / 1024</em><em class="chapterRef"> </em><em class="italic"> </em>= 600 MB, again as expected.</p>
    <p class="normal">Let’s conclude with an “out of memory” test to ensure that this really works!</p>
    <p class="normal">We’ll allocate some memory using <code class="inlineCode">jshell</code> in a JVM that runs in a container that has been given 1 GB of memory; that is, it has a max heap size of 256 MB.</p>
    <p class="normal">First, try to allocate a byte array of 100 MB:</p>
    <pre class="programlisting con"><code class="hljs-con">echo 'new byte[100_000_000]' | docker run -i --rm -m=1024M eclipse-temurin:17 jshell -q
</code></pre>
    <p class="normal">The command will respond with <code class="inlineCode">$1 ==&gt;</code>, meaning that it worked fine!</p>
    <div class="packt_tip">
      <p class="normal">Normally, <code class="inlineCode">jshell</code> will print out the value resulting from the command, but 100 MB of bytes all set to zero is a bit too much to print, so we get nothing.</p>
    </div>
    <p class="normal">Now, let’s try to allocate a byte array that is larger than the max heap size, for example, 500 MB:</p>
    <pre class="programlisting con"><code class="hljs-con">echo 'new byte[500_000_000]' | docker run -i --rm -m=1024M eclipse-temurin:17 jshell -q
</code></pre>
    <p class="normal">The JVM sees that it can’t perform the action since it honors the container settings of max<a id="_idIndexMarker275"/> memory and responds immediately with <code class="inlineCode">Exception java.lang.OutOfMemoryError: Java heap space</code>. Great!</p>
    <p class="normal">So, to summarize, we have now seen how Java honors the settings of available CPUs and the memory of its container. Let’s move on and build our first Docker images for one of the microservices!</p>
    <h1 id="_idParaDest-128" class="heading-1">Using Docker with one microservice</h1>
    <p class="normal">Now that we <a id="_idIndexMarker276"/>understand how Java works in a container, we can start using Docker with one of our microservices. Before we can run our microservice as a Docker container, we need to package it in a Docker image. To<a id="_idIndexMarker277"/> build a Docker image, we need a Dockerfile, so we will start with that. Next, we need a Docker-specific configuration for our microservice. Since a microservice that runs in a container is isolated from other microservices – it has its own IP address, hostname, and ports – it needs a different configuration compared to when it’s running on the same host with other microservices.</p>
    <p class="normal">For example, since the other microservices no longer run on the same host, no port conflicts will occur. When running in Docker, we can use the default port <code class="inlineCode">8080</code> for all our microservices without any risk of port conflicts. On the other hand, if we need to talk to the other microservices, we can no longer use <code class="inlineCode">localhost</code> like we could when we ran them on the same host.</p>
    <div class="packt_tip">
      <p class="normal">The source code in the microservices will not be affected by running the microservices in containers, only their configuration!</p>
    </div>
    <p class="normal">To handle the different configurations that are required when running locally without Docker and when running the microservices as Docker containers, we will use Spring profiles. Since <em class="chapterRef">Chapter 3</em>, <em class="italic">Creating a Set of Cooperating Microservices</em>, we have been using the default Spring profile for running locally without Docker. Now, we will create a new Spring profile named <code class="inlineCode">docker</code> to be used when we run<a id="_idIndexMarker278"/> our microservices as containers in Docker.</p>
    <h2 id="_idParaDest-129" class="heading-2">Changes in source code</h2>
    <p class="normal">We will start with the <code class="inlineCode">product</code> microservice, which <a id="_idIndexMarker279"/>can be found in the source code at <code class="inlineCode">$BOOK_HOME/Chapter04/microservices/product-service/</code>. In the next section, we will apply this to the other microservices as well.</p>
    <p class="normal">First, we add the Spring profile for Docker at the end of the property file <code class="inlineCode">application.yml</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">---</span>
<span class="hljs-attr">spring.config.activate.on-profile:</span> <span class="hljs-string">docker</span>
<span class="hljs-attr">server.port:</span> <span class="hljs-number">8080</span>
</code></pre>
    <div class="packt_tip">
      <p class="normal">Spring profiles can be used to specify the environment-specific configuration, which, in this case, is a configuration that is only to be used when running the microservice in a Docker container. Other examples are configurations that are specific to <code class="inlineCode">dev</code>, <code class="inlineCode">test</code>, and <code class="inlineCode">production</code> environments. Values in a profile override values from the default profile. By using YAML files, multiple Spring profiles can be placed in the same file, separated by <code class="inlineCode">---</code>.</p>
    </div>
    <p class="normal">The only parameter we change for now is the port that’s being used; we will use the default port <code class="inlineCode">8080</code> when running the microservice in a container.</p>
    <p class="normal">Next, we will create the Dockerfile that we will use to build the Docker image. As mentioned in <em class="chapterRef">Chapter 2</em>, <em class="italic">Introduction to Spring Boot</em>, a Dockerfile can be as straightforward as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> <span class="code-highlight"><strong class="hljs-slc">openjdk:</strong><strong class="hljs-number-slc">17</strong></span>
<span class="hljs-keyword">EXPOSE</span> <span class="code-highlight"><strong class="hljs-number-slc">8080</strong></span>
<span class="hljs-keyword">ADD</span> <span class="code-highlight"><strong class="hljs-slc">./build/libs/*.jar app.jar</strong></span>
<span class="hljs-keyword">ENTRYPOINT</span> <span class="code-highlight"><strong class="hljs-slc">[</strong><strong class="hljs-string-slc">"java"</strong><strong class="hljs-slc">,</strong><strong class="hljs-string-slc">"-jar"</strong><strong class="hljs-slc">,</strong><strong class="hljs-string-slc">"/app.jar"</strong><strong class="hljs-slc">]</strong></span>
</code></pre>
    <p class="normal">Some things to take note of are:</p>
    <ul>
      <li class="bulletList">The Docker images will be based on the official Docker image for OpenJDK and use version 17.</li>
      <li class="bulletList">Port <code class="inlineCode">8080</code> will be exposed to other Docker containers.</li>
      <li class="bulletList">The fat JAR file will be added to the Docker image from the Gradle build library, <code class="inlineCode">build/libs</code>.</li>
      <li class="bulletList">The command used by Docker to start a container based on this Docker image is <code class="inlineCode">java -jar /app.jar</code>.</li>
    </ul>
    <p class="normal">This simple approach has a few disadvantages:</p>
    <ul>
      <li class="bulletList">We are using the full JDK of Java SE 17, including compilers and other development tools. That makes the Docker images unnecessarily large and, from a security perspective, we don’t want to bring more tools into<a id="_idIndexMarker280"/> the image than necessary. <p class="bulletList">Therefore, we would prefer to use a base image for the Java SE 17 JRE that only contains programs and libraries required to run a Java program. Unfortunately, the OpenJDK project does not provide a Docker image for Java SE 17 JRE.</p>
      </li>
    </ul>
    <ul>
      <li class="bulletList">The fat JAR file takes time to unpackage when the Docker container starts up. A better approach is to instead unpackage the fat JAR when the Docker image is built.</li>
      <li class="bulletList">The fat JAR file is very big, as we will see below, some 20 MB. If we want to make repeatable changes to the application code in the Docker images during development, this will result in suboptimal usage of the Docker <code class="inlineCode">build</code> command. Since Docker images are built in layers, we will get one very big layer that needs to be replaced each time, even in the case where only a single Java class is changed in the application code.</li>
      <li class="bulletList">A better approach is to divide the content into different layers, where files that do not change so frequently are added in the first layer, and files that change the most are placed in the last layer. This will result in good use of Docker’s caching mechanism for layers. For the first stable layers that are not changed when some application code is changed, Docker will simply use the cache instead of rebuilding them. This will result in faster builds of the microservices’ Docker images.</li>
    </ul>
    <p class="normal">Regarding the lack of a Docker image for Java SE 17 JRE from the OpenJDK project, there are other open source projects that package the OpenJDK binaries into Docker images. One of the most widely used projects is <strong class="keyWord">Eclipse Temurin</strong> (<a href="https://adoptium.net/temurin/"><span class="url">https://adoptium.net/temurin/</span></a>). The<a id="_idIndexMarker281"/> Temurin project provides both full JDK editions and minimized JRE editions of their Docker images.</p>
    <p class="normal">When it comes to handling the suboptimal packaging of fat JAR files in Docker images, Spring Boot addressed this issue in v2.3.0, making it possible to extract the content of a fat <a id="_idIndexMarker282"/>JAR file into a number of folders. By default, Spring Boot creates the following folders after extracting a fat JAR file:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">dependencies</code>, containing all dependencies as JAR files</li>
      <li class="bulletList"><code class="inlineCode">spring-boot-loader</code>, containing Spring Boot classes that know how to start a Spring Boot application</li>
      <li class="bulletList"><code class="inlineCode">snapshot-dependencies</code>, containing snapshot dependencies, if any</li>
      <li class="bulletList"><code class="inlineCode">application</code>, containing application class files and resources</li>
    </ul>
    <p class="normal">The Spring Boot documentation recommends creating one Docker layer for each folder in the order listed above. After replacing the JDK-based Docker image with a JRE-based image and adding instructions for exploding the fat JAR file into proper layers in the Docker image, the Dockerfile looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="code-highlight"><strong class="hljs-keyword-slc">FROM</strong><strong class="hljs-slc"> eclipse-temurin:17.0.5_8-jre-focal as builder</strong></span>
<span class="hljs-keyword">WORKDIR</span> extracted
<span class="hljs-keyword">ADD</span> ./build/libs/*.jar app.jar
<span class="hljs-keyword">RUN</span> java -Djarmode=layertools -jar app.jar extract
<span class="code-highlight"><strong class="hljs-keyword-slc">FROM</strong><strong class="hljs-slc"> eclipse-temurin:17.0.5_8-jre-focal</strong></span>
<span class="hljs-keyword">WORKDIR</span> application
<span class="hljs-keyword">COPY</span> --from=builder extracted/dependencies/ ./
<span class="hljs-keyword">COPY</span> --from=builder extracted/spring-boot-loader/ ./
<span class="hljs-keyword">COPY</span> --from=builder extracted/snapshot-dependencies/ ./
<span class="hljs-keyword">COPY</span> --from=builder extracted/application/ ./
<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8080</span>
<span class="hljs-keyword">ENTRYPOINT</span> [<span class="hljs-string">"java"</span>, <span class="hljs-string">"org.springframework.boot.loader.JarLauncher"</span>]
</code></pre>
    <p class="normal">To handle the extraction of the fat JAR file in the Dockerfile we use a <strong class="keyWord">multi-stage build</strong>, meaning that there is a<a id="_idIndexMarker283"/> first step, named <code class="inlineCode">builder</code>, that handles the extraction. The second stage builds the actual Docker image that will be used at runtime, picking the files as required from the first stage. Using this technique, we can handle all packaging logic in the Dockerfile but, at the same time, keep the size of the final Docker image to a minimum:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The first stage starts with the line:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> eclipse-temurin:17.0.5_8-jre-focal as builder
</code></pre>
        <p class="normal">From this line, we can see that a Docker image from the Temurin project is used and that it contains Java SE JRE for v17.0.5_8. We can also see that the stage is named <code class="inlineCode">builder</code>.</p>
      </li>
    </ol>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="2">The <code class="inlineCode">builder</code> stage sets the working directory to <code class="inlineCode">extracted</code> and adds the fat JAR file from the Gradle build library, <code class="inlineCode">build/libs</code>, to that folder.</li>
      <li class="numberedList">The <code class="inlineCode">builder</code> stage then runs the command <code class="inlineCode">java -Djarmode=layertools -jar app.jar extract</code>, which will perform the extraction of the fat JAR file into its working <a id="_idIndexMarker284"/>directory, the <code class="inlineCode">extracted</code> folder.</li>
      <li class="numberedList">The next and final stage starts with the line:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> eclipse-temurin:17.0.5_8-jre-focal
</code></pre>
        <p class="normal">It uses the same base Docker image as in the first stage, and the <code class="inlineCode">application</code> folder as its working directory. It copies the exploded files from the <code class="inlineCode">builder</code> stage, folder by folder, into the <code class="inlineCode">application</code> folder. This creates one layer per folder, as described above. The parameter <code class="inlineCode">--from=builder</code> is used to instruct Docker to pick the files from the file system in the <code class="inlineCode">builder</code> stage.</p>
      </li>
    </ol>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="5">After exposing the proper ports, <code class="inlineCode">8080</code> in this case, the Dockerfile wraps up by telling Docker what Java class to run to start the microservice in the exploded format, that is, <code class="inlineCode">org.springframework.boot.loader.JarLauncher</code>.</li>
    </ol>
    <p class="normal">After learning about the required changes in the source code, we are ready to build our first Docker image.</p>
    <h2 id="_idParaDest-130" class="heading-2">Building a Docker image</h2>
    <p class="normal">To build the Docker image, we<a id="_idIndexMarker285"/> first need to build our deployment artifact (that is, the fat JAR file) for <code class="inlineCode">product-service</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">cd $BOOK_HOME/Chapter04
./gradlew :microservices:product-service:build
</code></pre>
    <div class="packt_tip">
      <p class="normal">Since we only want to build <code class="inlineCode">product-service</code> and the projects it depends on (the <code class="inlineCode">api</code> and <code class="inlineCode">util projects</code>), we don’t use the normal <code class="inlineCode">build</code> command, which builds all the microservices. Instead, we use a variant that tells Gradle to only build the <code class="inlineCode">product-service</code> project: <code class="inlineCode">:microservices:product-service:build</code>.</p>
    </div>
    <p class="normal">We can find the fat<a id="_idIndexMarker286"/> JAR file in the Gradle build library, <code class="inlineCode">build/libs</code>. The <code class="inlineCode">ls -l microservices/product-service/build/libs</code> command will report something like the following:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_07.png" alt="Graphical user interface, text  Description automatically generated" width="771" height="104"/></figure>
    <p class="packt_figref">Figure 4.7: Viewing the fat JAR file details</p>
    <div class="packt_tip">
      <p class="normal">As you can see, the JAR file is close to 20 MB in size – no wonder they are called fat JAR files!</p>
      <p class="normal">If you are curious about its actual content, you can view it by using the <code class="inlineCode">unzip -l microservices/product-service/build/libs/product-service-1.0.0-SNAPSHOT.jar</code> command.</p>
    </div>
    <p class="normal">Next, we will build the Docker image and name it <code class="inlineCode">product-service</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">cd microservices/product-service
docker build -t product-service .
</code></pre>
    <p class="normal">Docker will use the Dockerfile in the current directory to build Docker Engine. The image will be tagged with the name <code class="inlineCode">product-service</code> and stored locally inside the Docker engine.</p>
    <p class="normal">Verify that we got a Docker image, as expected, by using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker images | grep product-service
</code></pre>
    <p class="normal">The expected output is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_08.png" alt="Graphical user interface, text, application  Description automatically generated" width="631" height="92"/></figure>
    <p class="packt_figref">Figure 4.8: Verifying that we built our Docker image</p>
    <p class="normal">So now that we<a id="_idIndexMarker287"/> have built the image, let’s see how we can start the service.</p>
    <h2 id="_idParaDest-131" class="heading-2">Starting up the service</h2>
    <p class="normal">Let’s start up the <code class="inlineCode">product</code> microservice <a id="_idIndexMarker288"/>as a container by using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run --rm -p8080:8080 -e "SPRING_PROFILES_ACTIVE=docker" product-service
</code></pre>
    <p class="normal">This is what we can infer from the command:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><code class="inlineCode">docker run</code>: The <code class="inlineCode">docker run</code> command will start the container and display log output in the Terminal. The Terminal will be locked as long as the container runs.</li>
      <li class="numberedList">We have seen the <code class="inlineCode">--rm</code> option already; it will tell Docker to clean up the container once we stop the execution from the Terminal using <em class="keystroke">Ctrl</em> + <em class="keystroke">C</em>.</li>
      <li class="numberedList">The <code class="inlineCode">-p8080:8080</code> option maps port <code class="inlineCode">8080</code> in the container to port <code class="inlineCode">8080</code> in the Docker host, which makes it possible to call it from the outside. In the case of Docker Desktop for Mac, which runs Docker in a local Linux virtual machine, the port will also be port-forwarded to macOS, which is made available on <code class="inlineCode">localhost</code>. Remember that we can only have one container mapping to a specific port in the Docker host!</li>
      <li class="numberedList">With the <code class="inlineCode">-e</code> option, we can specify environment variables for the container, which, in this case, is <code class="inlineCode">SPRING_PROFILES_ACTIVE=docker</code>. The <code class="inlineCode">SPRING_PROFILES_ACTIVE</code> environment variable is used to tell Spring what profiles to use. In our case, we want Spring to use the <code class="inlineCode">docker</code> profile.</li>
      <li class="numberedList">Finally, we have <code class="inlineCode">product-service</code>, which is the name of the Docker image we built above and which Docker will use to start the container.</li>
    </ol>
    <p class="normal">The expected output is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_09.png" alt="Text  Description automatically generated" width="878" height="516"/></figure>
    <p class="packt_figref">Figure 4.9: Output after starting up the product microservice</p>
    <p class="normal">From the preceding <a id="_idIndexMarker289"/>screenshot we can see:</p>
    <ul>
      <li class="bulletList">The profile that’s used by Spring is <code class="inlineCode">docker</code>. Look for <code class="inlineCode">The following profiles are active: docker</code> in the output to verify this.</li>
      <li class="bulletList">The port that’s allocated by the container is <code class="inlineCode">8080</code>. Look for <code class="inlineCode">Netty started on port8080</code> in the output to verify this.</li>
      <li class="bulletList">The microservice is ready to accept requests once the log message <code class="inlineCode">Started ProductServiceApplication</code> has been written!</li>
    </ul>
    <p class="normal">We can use port <code class="inlineCode">8080</code> on <code class="inlineCode">localhost</code> to communicate with the microservice, as explained <a id="_idIndexMarker290"/>previously. Try out the following command in another Terminal window:</p>
    <pre class="programlisting con"><code class="hljs-con">curl localhost:8080/product/3
</code></pre>
    <p class="normal">The following is the expected output:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_10.png" alt="Graphical user interface, text  Description automatically generated" width="877" height="69"/></figure>
    <p class="packt_figref">Figure 4.10: Requesting information on product 3</p>
    <p class="normal">This is similar to the output we received from the previous chapter, but with one major difference: we now have the content of <code class="inlineCode">"service Address":"9dc086e4a88b/172.17.0.2:8080"</code>, the port is <code class="inlineCode">8080</code>, as expected, and the IP address, <code class="inlineCode">172.17.0.2</code>, is the IP address that’s been allocated to the container from an internal network in Docker – but where did the hostname, <code class="inlineCode">9dc086e4a88b</code>, come from?</p>
    <p class="normal">Ask Docker for all the running containers:</p>
    <pre class="programlisting con"><code class="hljs-con">docker ps
</code></pre>
    <p class="normal">We will see something like the following:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_11.png" alt="Graphical user interface, text  Description automatically generated" width="671" height="187"/></figure>
    <p class="packt_figref">Figure 4.11: All running containers</p>
    <p class="normal">As we can see from the preceding output, the hostname is equivalent to the ID of the container, which is good to know if you want to understand which container actually responded to your request!</p>
    <p class="normal">Wrap this up by <a id="_idIndexMarker291"/>stopping the container in the Terminal with the <em class="keystroke">Ctrl</em> + <em class="keystroke">C</em> command. With this done, we can now move on to running the container detached from the terminal.</p>
    <h2 id="_idParaDest-132" class="heading-2">Running the container in detached mode</h2>
    <p class="normal">Okay, that was great, but what if we don’t want to lock the Terminal from where we started the container? In most <a id="_idIndexMarker292"/>cases, it is inconvenient to have a Terminal session locked for each running container. It’s time to learn how to start the container as <strong class="keyWord">detached</strong> – running the container without locking the Terminal!</p>
    <p class="normal">We can do this by adding the <code class="inlineCode">-d</code> option and, at the same time, giving it a name using the <code class="inlineCode">--name</code> option. Giving it a name is optional, and Docker will generate a name if we don’t, but it makes it easier to send commands to the detached container using a name that we have decided. The <code class="inlineCode">--rm</code> option is no longer required since we will stop and remove the container explicitly when we are done with it:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run -d -p8080:8080 -e "SPRING_PROFILES_ACTIVE=docker" --name my-prd-srv product-service
</code></pre>
    <p class="normal">If we run the <code class="inlineCode">docker ps</code> command again, we will see our new container, called <code class="inlineCode">my-prd-srv</code>:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_12.png" alt="Graphical user interface, text  Description automatically generated" width="620" height="149"/></figure>
    <p class="packt_figref">Figure 4.12: Starting the container as detached</p>
    <p class="normal">But how do we get<a id="_idIndexMarker293"/> the log output from our container?</p>
    <p class="normal">Meet the <code class="inlineCode">docker</code> <code class="inlineCode">logs</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker logs my-prd-srv -f
</code></pre>
    <p class="normal">The <code class="inlineCode">-f</code> option tells the command to follow the log output, that is, not end the command when all the current log output has been written to the Terminal, but also wait for more output. If you expect a lot of old log messages that you don’t want to see, you can also add the <code class="inlineCode">--tail 0</code> option so that you only see new log messages. Alternatively, you can use the <code class="inlineCode">--since</code> option and specify either an absolute timestamp or a relative time, for example, <code class="inlineCode">--since 5m</code>, to see log messages that are, at most, five minutes old.</p>
    <p class="normal">Try this out with a new <code class="inlineCode">curl</code> request. You should see that a new log message has been written to the log output in the Terminal.</p>
    <p class="normal">Wrap this up by stopping and removing the container:</p>
    <pre class="programlisting con"><code class="hljs-con">docker rm -f my-prd-srv
</code></pre>
    <p class="normal">The <code class="inlineCode">-f</code> option forces Docker to remove the container, even if it is running. Docker will automatically stop the container before it removes it.</p>
    <p class="normal">Now that we know how to use Docker with a microservice, we can see how to manage a microservice<a id="_idIndexMarker294"/> landscape with the help of Docker Compose.</p>
    <h1 id="_idParaDest-133" class="heading-1">Managing a landscape of microservices using Docker Compose</h1>
    <p class="normal">We’ve already seen how we can run a single microservice as a Docker container, but what about managing<a id="_idIndexMarker295"/> a whole system landscape of microservices?</p>
    <p class="normal">As we mentioned earlier, this is the <a id="_idIndexMarker296"/>purpose of <code class="inlineCode">docker-compose</code>. By using single commands, we can build, start, log, and stop a group of cooperating microservices running as Docker containers.</p>
    <h2 id="_idParaDest-134" class="heading-2">Changes in the source code</h2>
    <p class="normal">To be able to use Docker Compose, we need to create a configuration file, <code class="inlineCode">docker-compose.yml</code>, that describes the microservices Docker Compose will manage for us. We also <a id="_idIndexMarker297"/>need to set up Dockerfiles for the remaining microservices and add a Docker-specific Spring profile to each of them. All four microservices have their own Dockerfile, but they all look the same as the preceding one.</p>
    <p class="normal">When it comes to the Spring profiles, the three core services, <code class="inlineCode">product-</code>, <code class="inlineCode">recommendation-</code>, and <code class="inlineCode">review-service</code>, have the same <code class="inlineCode">docker</code> profile, which only specifies that the default port <code class="inlineCode">8080</code> should be used when running as a container.</p>
    <p class="normal">For the <code class="inlineCode">product-composite-service</code>, things are a bit more complicated since it needs to know where to find the core services. When we ran all the services on localhost, it was configured to use localhost and individual port numbers, <code class="inlineCode">7001</code>-<code class="inlineCode">7003</code>, for each core service. When running in Docker, each service will have its own hostname but will be accessible on the same port number, <code class="inlineCode">8080</code>. Here, the <code class="inlineCode">docker</code> profile for <code class="inlineCode">product-composite-service</code> looks as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">---</span>
<span class="code-highlight"><strong class="hljs-attr-slc">spring.config.activate.on-profile:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">docker</strong></span>
<span class="hljs-attr">server.port:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">app:</span>
  <span class="hljs-attr">product-service:</span>
    <span class="hljs-attr">host:</span> <span class="hljs-string">product</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>
  <span class="hljs-attr">recommendation-service:</span>
    <span class="hljs-attr">host:</span> <span class="hljs-string">recommendation</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>
  <span class="hljs-attr">review-service:</span>
    <span class="hljs-attr">host:</span> <span class="hljs-string">review</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>
</code></pre>
    <p class="normal">This configuration is stored in the property file, <code class="inlineCode">application.yml</code>.</p>
    <p class="normal">Where did the<a id="_idIndexMarker298"/> hostnames <code class="inlineCode">product</code>, <code class="inlineCode">recommendation</code>, and <code class="inlineCode">review</code> come from?</p>
    <p class="normal">These are specified in the <code class="inlineCode">docker-compose.yml</code> file, which is located in the <code class="inlineCode">$BOOK_HOME/Chapter04</code> folder. It looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">version:</span> <span class="hljs-string">'2.1'</span>
<span class="hljs-attr">services:</span>
  <span class="hljs-attr">product:</span>
    <span class="hljs-attr">build:</span> <span class="hljs-string">microservices/product-service</span>
    <span class="hljs-attr">mem_limit:</span> <span class="hljs-string">512m</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">SPRING_PROFILES_ACTIVE=docker</span>
  <span class="hljs-attr">recommendation:</span>
    <span class="hljs-attr">build:</span> <span class="hljs-string">microservices/recommendation-service</span>
    <span class="hljs-attr">mem_limit:</span> <span class="hljs-string">512m</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">SPRING_PROFILES_ACTIVE=docker</span>
  <span class="hljs-attr">review:</span>
    <span class="hljs-attr">build:</span> <span class="hljs-string">microservices/review-service</span>
    <span class="hljs-attr">mem_limit:</span> <span class="hljs-string">512m</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">SPRING_PROFILES_ACTIVE=docker</span>
  <span class="hljs-attr">product-composite:</span>
    <span class="hljs-attr">build:</span> <span class="hljs-string">microservices/product-composite-service</span>
    <span class="hljs-attr">mem_limit:</span> <span class="hljs-string">512m</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">"</span><span class="hljs-string">8080:8080"</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">SPRING_PROFILES_ACTIVE=docker</span>
</code></pre>
    <p class="normal">For each microservice, we specify the following:</p>
    <ul>
      <li class="bulletList">The name of the microservice. This will also be the hostname of the container in the internal Docker network.</li>
      <li class="bulletList">A <code class="inlineCode">build</code> directive that specifies where to find the Dockerfile that was used to build the Docker image.</li>
      <li class="bulletList">A memory limit of 512 MB. 512 MB should be sufficient for all our microservices for the scope of this book. For this chapter, it could be set to a lower value, but as we add more capabilities in the microservices in the coming chapters, their memory requirements will increase.</li>
      <li class="bulletList">The environment variables that will be set up for the container. In our case, we used these to specify which Spring profile to use.</li>
    </ul>
    <p class="normal">For the <code class="inlineCode">product-composite</code> service, we will also specify port mappings – we will expose its port so it can<a id="_idIndexMarker299"/> be reached from outside Docker. The other microservices will not be accessible from the outside. Next, we will see how to start up a microservice landscape.</p>
    <div class="packt_tip">
      <p class="normal">In <em class="chapterRef">Chapter 10</em>, <em class="italic">Using Spring Cloud Gateway to Hide Microservices behind an Edge Server</em>, and <em class="chapterRef">Chapter 11</em>, <em class="italic">Securing Access to APIs</em>, we will learn more about how to lock down and secure external access to a system landscape of microservices.</p>
    </div>
    <h2 id="_idParaDest-135" class="heading-2">Starting up the microservice landscape</h2>
    <p class="normal">With all the necessary<a id="_idIndexMarker300"/> code changes in place, we can build our Docker images, start up the microservice landscape, and run some tests to verify that it works as expected. For this, we need to do the following:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">First, we build our deployment artifacts with Gradle and then the Docker images with Docker Compose:
        <pre class="programlisting con"><code class="hljs-con">cd $BOOK_HOME/Chapter04
./gradlew build
docker-compose build
</code></pre>
      </li>
      <li class="numberedList">Then, we need to verify that we can see our Docker images, as follows:
        <pre class="programlisting con"><code class="hljs-con">docker images | grep chapter04
</code></pre>
        <p class="normal">We should see the following output:</p>
      </li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B19825_04_13.png" alt="Text  Description automatically generated with low confidence" width="780" height="168"/></figure>
    <figure class="mediaobject">Figure 4.13: Verifying our Docker images</figure>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="3">Start up the <a id="_idIndexMarker301"/>microservices landscape with the following command:
        <pre class="programlisting con"><code class="hljs-con">docker-compose up -d
</code></pre>
      </li>
    </ol>
    <p class="normal">The <code class="inlineCode">-d</code> option will make Docker Compose run the containers in detached mode, the same as for Docker.</p>
    <p class="normal">We can follow the startup by monitoring the output that’s written to each container log with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker-compose logs -f
</code></pre>
    <div class="packt_tip">
      <p class="normal">The <code class="inlineCode">docker-compose logs</code> command supports the same <code class="inlineCode">-f</code> and <code class="inlineCode">--tail</code> options as <code class="inlineCode">docker logs</code>, as described earlier.</p>
      <p class="normal">The <code class="inlineCode">dockter-composelogs</code> command also supports restricting the log output to a group of containers. Simply add the names of the containers you want to see the log output of after the <code class="inlineCode">logs</code> command. For example, to only see log output from the <code class="inlineCode">product</code> and <code class="inlineCode">review</code> services, use <code class="inlineCode">docker-compose logs -f product review</code>.</p>
    </div>
    <p class="normal">When all four microservices have reported that they have started up, we are ready to try out the microservices landscape. Look for the following:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_14.png" alt="Text  Description automatically generated" width="878" height="216"/></figure>
    <p class="packt_figref">Figure 4.14: Starting up all four microservices</p>
    <div class="packt_tip">
      <p class="normal">Note that each log message is prefixed with the name of the container that produced the output!</p>
    </div>
    <p class="normal">Now, we are ready to run some tests to verify that this works as expected. The port number is the only <a id="_idIndexMarker302"/>change we need to make when calling the composite service in Docker compared to when we ran it directly on the localhost, as we did in the previous chapter. We now use port <code class="inlineCode">8080</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">curl localhost:8080/product-composite/123 -s | jq .
</code></pre>
    <p class="normal">We will get the same type of response:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_15.png" alt="Text  Description automatically generated" width="559" height="234"/></figure>
    <p class="packt_figref">Figure 4.15: Calling the composite service</p>
    <p class="normal">However, there’s <a id="_idIndexMarker303"/>one big difference – the hostnames and ports reported by <code class="inlineCode">serviceAddresses</code> in the response:</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_16.png" alt="A screenshot of a computer  Description automatically generated with medium confidence" width="397" height="140"/></figure>
    <p class="packt_figref">Figure 4.16: Viewing the serviceAddresses</p>
    <p class="normal">Here, we can see the hostnames and IP addresses that have been allocated to each of the Docker containers.</p>
    <p class="normal">We’re done; now only one step is left:</p>
    <pre class="programlisting con"><code class="hljs-con">docker-compose down
</code></pre>
    <p class="normal">The preceding command will shut down the microservices landscape. So far, we have seen how <a id="_idIndexMarker304"/>we can test the cooperating microservices running <strong class="keyWord">Bash</strong> commands by hand. In the next section, we will see how we can enhance our test script to automate these manual steps.</p>
    <h1 id="_idParaDest-136" class="heading-1">Automating tests of cooperating microservices</h1>
    <p class="normal">Docker Compose is really <a id="_idIndexMarker305"/>helpful when it comes to manually managing a group of microservices. In this section, we will take this one step further and integrate Docker Compose into our test script, <code class="inlineCode">test-em-all.bash</code>. The test script will automatically start up the microservice landscape, run all the required tests to verify that the microservice landscape works as expected, and finally, tear it down, leaving no traces behind.</p>
    <p class="normal">The test script can be found at <code class="inlineCode">$BOOK_HOME/Chapter04/test-em-all.bash</code>.</p>
    <p class="normal">Before the test script runs the test suite, it will check for the presence of a <code class="inlineCode">start</code> argument in the invocation of the test script. If found, it will restart the containers with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$@</span> == *<span class="hljs-string">"start"</span>* ]]
<span class="hljs-keyword">then</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Restarting the test environment..."</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"$ docker-compose down --remove-orphans"</span>
    docker-compose down --remove-orphans
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"$ docker-compose up -d"</span>
    docker-compose up -d
<span class="hljs-keyword">fi</span>
</code></pre>
    <p class="normal">After that, the test script will wait for the <code class="inlineCode">product-composite</code> service to respond with <code class="inlineCode">OK</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">waitForService http://<span class="hljs-variable">$HOST</span>:<span class="hljs-variable">${PORT}</span>/product-composite/1
</code></pre>
    <p class="normal">The <code class="inlineCode">waitForService</code> Bash function is implemented as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">function</span> <span class="hljs-title">testUrl</span>() {
    url=<span class="hljs-variable">$@</span>
    <span class="hljs-keyword">if</span> curl <span class="hljs-variable">$url</span> -ks -f -o /dev/null
    <span class="hljs-keyword">then</span>
          <span class="hljs-built_in">return</span> 0
    <span class="hljs-keyword">else</span>
          <span class="hljs-built_in">return</span> 1
    <span class="hljs-keyword">fi</span>;
}
<span class="code-highlight"><strong class="hljs-keyword-slc">function</strong><strong class="hljs-slc"> </strong><strong class="hljs-title-slc">waitForService</strong><strong class="hljs-slc">() {</strong></span>
    url=<span class="hljs-variable">$@</span>
    <span class="hljs-built_in">echo</span> -n <span class="hljs-string">"Wait for: </span><span class="hljs-variable">$url</span><span class="hljs-string">... "</span>
    n=0
    until testUrl <span class="hljs-variable">$url</span>
    <span class="hljs-keyword">do</span>
        n=$((n + <span class="hljs-number">1</span>))
        <span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$n</span> == 100 ]]
        <span class="hljs-keyword">then</span>
            <span class="hljs-built_in">echo</span> <span class="hljs-string">" Give up"</span>
            <span class="hljs-built_in">exit</span> 1
        <span class="hljs-keyword">else</span>
            <span class="hljs-built_in">sleep</span> 3
            <span class="hljs-built_in">echo</span> -n <span class="hljs-string">", retry #</span><span class="hljs-variable">$n</span><span class="hljs-string"> "</span>
        <span class="hljs-keyword">fi</span>
    <span class="hljs-keyword">done</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"DONE, continues..."</span>
}
</code></pre>
    <p class="normal">The <code class="inlineCode">waitForService</code> function sends HTTP requests to the supplied URL using <code class="inlineCode">curl</code>. Requests are sent repeatedly until <code class="inlineCode">curl</code> responds that it got a successful response back from <a id="_idIndexMarker306"/>the request. The function waits <code class="inlineCode">3</code> seconds between each attempt and gives up after <code class="inlineCode">100</code> attempts, stopping the script with a failure.</p>
    <p class="normal">Next, all the tests are executed as they were previously. Afterward, the script will tear down the landscape if it finds the <code class="inlineCode">stop</code> argument in the invocation parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$@</span> == *<span class="hljs-string">"stop"</span>* ]]
<span class="hljs-keyword">then</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"We are done, stopping the test environment..."</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"$ docker-compose down"</span>
    docker-compose down
<span class="hljs-keyword">fi</span>
</code></pre>
    <div class="packt_tip">
      <p class="normal">Note that the test script will not tear down the landscape if some tests fail; it will simply stop, leaving the landscape up for error analysis!</p>
    </div>
    <p class="normal">The test script has also changed the default port from <code class="inlineCode">7000</code>, which we used when we ran the <a id="_idIndexMarker307"/>microservices without Docker, to <code class="inlineCode">8080</code>, which is used by our Docker containers.</p>
    <p class="normal">Let’s try it out! To start the landscape, run the tests, and tear it down afterward, run the command:</p>
    <pre class="programlisting con"><code class="hljs-con">./test-em-all.bash start stop
</code></pre>
    <p class="normal">The following is some sample output from a test run focusing on the startup and shutdown phases. Output from the actual tests have been removed (they are the same as in the previous chapter):</p>
    <figure class="mediaobject"><img src="../Images/B19825_04_17.png" alt="Text  Description automatically generated" width="877" height="574"/></figure>
    <p class="packt_figref">Figure 4.17: Sample output from a test run</p>
    <p class="normal">After running these tests, we <a id="_idIndexMarker308"/>can move on to see how to troubleshoot tests that fail.</p>
    <h2 id="_idParaDest-137" class="heading-2">Troubleshooting a test run</h2>
    <p class="normal">If the tests that were running <code class="inlineCode">./test-em-all.bash start stop</code> fail, following these steps can help you<a id="_idIndexMarker309"/> identify the problem and resume the tests once the problem has been fixed:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">First, check the status of the running microservices with the following command:
        <pre class="programlisting con"><code class="hljs-con">docker-compose ps
</code></pre>
        <p class="normal">If all the microservices are up and running and healthy, you will receive the following output:</p>
      </li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B19825_04_18.png" alt="Graphical user interface, text  Description automatically generated" width="812" height="134"/></figure>
    <p class="packt_figref">Figure 4.18: Checking the status of running microservices</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="2">If any of the microservices do not have a status of <code class="inlineCode">Up</code>, check their log output for any errors by using the <code class="inlineCode">docker-compose logs</code> command. For example, you would use the following command if you wanted to check the log output for the <code class="inlineCode">product</code> service:
        <pre class="programlisting con"><code class="hljs-con">docker-compose logs product
</code></pre>
        <p class="normal">At this stage, it is not easy to cause an error to be logged, since the microservices are so simple. Instead, here is a sample error log from the <code class="inlineCode">product</code> microservice in <em class="chapterRef">Chapter 6</em>, <em class="italic">Adding Persistence</em>. Assume that the following is found in its log output:</p>
        <figure class="mediaobject"><img src="../Images/B19825_04_19.png" alt="Graphical user interface, text  Description automatically generated" width="812" height="110"/></figure>
        <figure class="mediaobject">Figure 4.19: Sample error information in the log output</figure>
        <p class="normal">From reading the above log output, it is quite clear that the <code class="inlineCode">product</code> microservice can’t reach its MongoDB database. Given that the database also runs as a Docker container managed by the same Docker Compose file, the <code class="inlineCode">docker-compose logs</code> command can be used to see what’s wrong with the database.</p>
        <p class="normal">If required, you <a id="_idIndexMarker310"/>can restart a failed container with the <code class="inlineCode">docker-compose restart</code> command. For example, you would use the following command if you wanted to restart the <code class="inlineCode">product</code> microservice:</p>
        <pre class="programlisting con"><code class="hljs-con">docker-compose restart product
</code></pre>
        <p class="normal">If a container is missing, for example, due to a crash, you start it up with the <code class="inlineCode">docker-compose up -d --scale</code> command. For example, you would use the following command for the <code class="inlineCode">product</code> microservice:</p>
        <pre class="programlisting con"><code class="hljs-con">docker-compose up -d --scale product=1
</code></pre>
        <p class="normal">If errors in the log output indicate that Docker is running out of disk space, parts of it can be reclaimed with the following command:</p>
        <pre class="programlisting con"><code class="hljs-con">docker system prune -f --volumes
</code></pre>
      </li>
    </ol>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="3">Once all the microservices are up and running and healthy, run the test script again, but without starting the microservices:
        <pre class="programlisting con"><code class="hljs-con">./test-em-all.bash
</code></pre>
        <p class="normal">The tests should now run fine!</p>
      </li>
    </ol>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="4">When you are done<a id="_idIndexMarker311"/> with the testing, remember to tear down the system landscape:
        <pre class="programlisting con"><code class="hljs-con">docker-compose down
</code></pre>
      </li>
    </ol>
    <div class="packt_tip">
      <p class="normal">Finally, a tip about a combined command that builds runtime artifacts and Docker images from the source and then executes all tests in Docker:</p>
      <pre class="programlisting con"><code class="hljs-con">./gradlew clean build &amp;&amp; docker-compose build &amp;&amp; ./test-em-all.bash start stop
</code></pre>
      <p class="normal">This is perfect if you want to check that everything works before you push new code to your Git repository or as part of a build pipeline in your build server!</p>
    </div>
    <h1 id="_idParaDest-138" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we have seen how Docker can be used to simplify testing a landscape of cooperating microservices.</p>
    <p class="normal">We learned how Java SE, since v10, honors constraints that we put on containers regarding how much CPU and memory they are allowed to use. We have also seen how little it takes to make it possible to run a Java-based microservice as a Docker container. Thanks to Spring profiles, we can run the microservice in Docker without having to make any code changes.</p>
    <p class="normal">Finally, we have seen how Docker Compose can help us manage a landscape of cooperating microservices with single commands, either manually or, even better, automatically, when integrated with a test script such as <code class="inlineCode">test-em-all.bash</code>.</p>
    <p class="normal">In the next chapter, we will study how we can add some documentation of the API using OpenAPI/Swagger descriptions.</p>
    <h1 id="_idParaDest-139" class="heading-1">Questions</h1>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">What are the major differences between a virtual machine and a Docker container?</li>
      <li class="numberedList">What is the purpose of namespaces and cgroups in Docker?</li>
      <li class="numberedList">What happens with a Java application that doesn’t honor the max memory settings in a container and allocates more memory than it is allowed to?</li>
      <li class="numberedList">How can we make a Spring-based application run as a Docker container without requiring modifications of its source code?</li>
      <li class="numberedList">Why will the following Docker Compose code snippet not work?
        <pre class="programlisting code"><code class="hljs-code">  <span class="hljs-attr">review:</span>
    <span class="hljs-attr">build:</span> <span class="hljs-string">microservices/review-service</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">"8080:8080"</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">SPRING_PROFILES_ACTIVE=docker</span>
  <span class="hljs-attr">product-composite:</span>
    <span class="hljs-attr">build:</span> <span class="hljs-string">microservices/product-composite-service</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">"8080:8080"</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">SPRING_PROFILES_ACTIVE=docker</span>
</code></pre>
      </li>
    </ol>
  </div>
</div>
</div>
</body></html>