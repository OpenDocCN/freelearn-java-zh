- en: The MapReduce Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce is a common data processing pattern made famous by Google and now
    implemented in various systems and frameworks, most notably Apache Hadoop. Nowadays,
    this pattern is familiar and easy to understand at its core, but running large-scale
    systems such as Hadoop comes with its own set of challenges and cost of ownership.
    In this chapter, we'll show how this pattern can be implemented on your own using
    serverless technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing big data applications in a serverless environment may seem counter-intuitive
    due to the computing limitations of FaaS. Certain types of problems fit very well
    into a serverless ecosystem, especially considering we practically have unlimited
    file storage with distributed filesystems such as AWS S3\. Additionally, MapReduce's
    magic is not so much in the application of an algorithm, but in the distribution
    of computing power such that computation is performed in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the application and development of a MapReduce
    pattern in a serverless environment. I'll cover the use cases for such a design
    and when it may or may not be a good fit. I'll also show how simple this pattern
    is within a serverless platform and what you should take into consideration before
    embarking on building your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of this chapter, you can expect to understand the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What problem MapReduce solves and when it may be appropriate to implement in
    a serverless environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and scaling considerations when applying this pattern on your own
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement your own MapReduce serverless system to count occurrences of
    from-to combinations from a corpus of email messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the Fanout pattern as a sub-component of the MapReduce pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MapReduce as a pattern and programming model has been around for many years,
    arising from parallel computing research and industry implementations. Most famously,
    MapReduce hit the mainstream with Google''s 2004 paper entitled *MapReduce—Simplified
    Data Processing on Large Clusters* ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)).
    Much of the benefit of Google''s initial MapReduce implementation was:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic parallelization and distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault-tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Status and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you take a step back and look at that list, it should look familiar. FaaS
    systems such as AWS Lambda give us most of these benefits. While status and monitoring
    aren't inherently baked into FaaS platforms, there are ways to ensure our functions
    are executing successfully. On that same topic, MapReduce systems were initially,
    and still are, very often, managed at the OS level, meaning operators are in charge
    of taking care of crashed or otherwise unhealthy nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding list of benefits is listed in the following slide from a presentation-like
    form of the research paper: [https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0002.html](https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0002.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Not too long after Google's 2004 MapReduce paper, the Apache Hadoop project
    was born. Hadoop's goal was an open source implementation of the MapReduce pattern
    for big data processing. Since then, Hadoop has arguably become the most popular
    MapReduce framework today. Additionally, the term *Hadoop* has evolved to include
    many other frameworks for big data processing and refers more to the ecosystem
    of tools rather than the single framework.
  prefs: []
  type: TYPE_NORMAL
- en: As powerful and popular as Hadoop is, it's a complicated beast in practice.
    In order to run a Hadoop cluster of any significance, one needs to run and master
    Zookeeper and **HDFS** (**Hadoop Distributed File System**), in addition to the
    Hadoop master and worker nodes themselves. For those unfamiliar with these tools
    and all of the DevOps ownership that comes with them, running a Hadoop cluster
    is not only daunting but impractical.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you've never worked with a MapReduce framework or system, the overall concepts
    are not incredibly complex. In fact, we can implement a single-process and single-threaded
    MapReduce system in a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, MapReduce is designed to extract a specific bit of information from
    a body of data and distill it down into some final result. That may sound very
    vague and arbitrary, and it is. The beauty of MapReduce is that one can apply
    it to so many different problems. A few examples should better demonstrate what
    MapReduce is and how you can use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *Hello World* MapReduce program counts the number of occurrences of a particular
    word in a body of text. The following code block does this with a few lines of
    Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First, this code performs a mapping phase that emits a two element tuple for
    every occurrence of a work. For example, a given word would emit ` ('amet', 1)`
    for the word `amet`. The result from this mapping phase is a list of `(word, 1)`
    pairs, where the `1` simply means we've encountered the word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The job of the reducer is to aggregate the mapper''s output into some final
    format. In our case, we''d like a final tally of the number of occurrences for
    each word. Reading through the preceding `reducer` function, it should be obvious
    how I''m doing that. A snippet from the final output is shown in the following
    code block. You can see that `amet` only shows up once in the `Lorem`, `ipsum`
    text blog, but `sit` shows up nine times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Role of the mapper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary purpose of the `mapper` is to emit data that the reducer will later
    aggregate into a final. In this trivial example, each occurrence of a word results
    in a `(word, 1)` pair since we're giving a single appearance of a word a score
    of `1`. We very well could have emitted the word by itself (that is, `'amet'`)
    and put the score of `1` in the reducer; however, this would have made the code
    less general. If we wanted to give a heavier weighting to certain words, we'd
    merely change our mapper to output a different number based on the word and would
    leave our `reducer` code as-is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows how we would give the word `amet` a score of
     `10` while all other words count as `1`. Of course, this is no longer counting
    word occurrences but instead scoring words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were computing something completely different, you should see now that
    we''d need to update the mapper function. Some examples of additional calculations
    we could make based on this Lorem ipsum text could be:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of uppercase letters in a word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of vowels in a word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average length of a word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these would require changes to the reducer step, which we'll cover in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Role of the reducer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the mapper's job is to extract and emit some form of data to be aggregated,
    the reducer's job is to perform that aggregation. In this example, the reducer
    receives the full list of `(word, 1)` pairs and just adds up the counts (`1`,
    in this case) for each word. If we were to perform a different aggregation, the
    `reducer` function would need to change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than counting the number of occurrences, let''s calculate the average
    length of a word. In this case, both our `mapper` and our `reducer` will need
    updating with the more significant changes happening within the `reducer`. The
    following code block changes our example to calculate the average word length
    for a body of text broken up into words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As in the prior example, the mapper is quite dumb and only returns the word
    length. Since this example doesn't rely on anything specific to the words, there
    is no need to return the word itself. The reducer code becomes even more straightforward.
    The input to the `reducer` is now a list of numbers. To calculate the average,
    it's a simple task of returning the total divided by the number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The real magic behind MapReduce implementations such as Hadoop is the distribution
    and parallelization of computation. Our trivial example would work well running
    on your laptop even when the input data was several megabytes. However, imagine
    a case where you would like to perform some analysis like this on data that is
    hundreds of gigabytes, terabytes, or even in the petabyte range.
  prefs: []
  type: TYPE_NORMAL
- en: Real MapReduce systems use two essential tricks to do this work efficiently.
    One is working in parallel as I've already mentioned. This means, for example,
    that multiple instances that do the computation comprise a Hadoop system or cluster.
    The other trick is co-locating data with the worker node that does the work. Data
    co-location reduces network traffic and speeds up overall processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mappers begin their work on a subset of the input data. You can imagine that
    when working on petabytes of data, there could be hundreds or thousands of nodes
    involved. Once the mappers have completed their job, they send their data to the
    reducers for final processing. The following diagram shows the details of this
    architecture from a conceptual standpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47a8dfe6-67d6-4fd4-822c-695345c7deef.png)'
  prefs: []
  type: TYPE_IMG
- en: Image adapted from *MapReduce:*
  prefs: []
  type: TYPE_NORMAL
- en: '*Simplified Data Processing on Large Clusters*, Jeff Dean, Sanjay Ghemawat'
  prefs: []
  type: TYPE_NORMAL
- en: Google, Inc. [https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html](https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html)
  prefs: []
  type: TYPE_NORMAL
- en: A key phase in Hadoop is the *shuffle* phase, labeled partitioning function in
    the previous diagram. The arrows coming out of the Map Tasks show that a subset
    of mapper data will be sent to various reducers. In Hadoop, all output for specific
    keys is sent to the same reducer node. For example, in our case of the word count,
    the key `('amet', 1)` would be sent to the same reducer machine/node regardless
    of which mapper emitted that key. The reason behind this is to reduce network
    latency and reduce complexity for the reducers. By guaranteeing that a reducer
    has all of the data needed to perform its final reduce task, reducers are both
    faster and simpler to implement. Without this guarantee, the framework would need
    to designate a master reducer and crawl horizontally to final all the necessary
    data. Not only is that complex, but it's also slow because of all of the network
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: There are many details that we cannot cover in a system as complex as Hadoop.
    If you have unanswered questions at this point, I'd encourage you to do some more
    investigation on your own. Hopefully, this discussion has been enough to set the
    stage for our serverless implementation of MapReduce in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce serverless architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce on a serverless platform is very different than in a system such as
    Hadoop. Most of the differences occur on the operational and system architecture
    side of things. Another huge difference is the limited processing power and memory
    we have with our FaaS. Because FaaS providers put in hard limits for both temporary
    storage space and memory, there are some problems that you cannot realistically
    solve with a serverless MapReduce implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that the foundational ideas in the MapReduce design still hold
    true. If you look back up at the start of the initial list of benefits provided
    by MapReduce, we naturally get many of these for free, albeit with a few caveats.
    MapReduce truly shines, due in large part to the parallelization of computation.
    We have that with serverless functions. Similarly, much work goes into ensuring
    Hadoop nodes are healthy and able to perform work. Again, we get that for free
    with serverless functions.
  prefs: []
  type: TYPE_NORMAL
- en: A significant feature we do *not* get is the co-location of our data and the
    processing of that data. Our distributed filesystem in this example will be AWS
    S3\. The only way to get data to our serverless functions is to either send that
    data via API or have our functions fetch the data across the network. Hadoop storage
    and computing co-location mean that each mapper node processes the data that it
    has stored locally, using the HDFS. This drastically cuts down on the amount of
    data being transferred over the network and is an implementation detail that makes
    the entire system possible.
  prefs: []
  type: TYPE_NORMAL
- en: Before you start your implementation of this pattern, ask yourself whether you
    can segment your data to a point where processing it with a Lambda function is
    possible. If your input data is 100 GB, it's feasible that you may have 100 functions
    handling 1 GB each, even with paying the penalty of network bandwidth. However,
    it won't be practical to expect a reducer to produce a 100 GB output file since
    it would need to hold that in memory to calculate the final result.
  prefs: []
  type: TYPE_NORMAL
- en: Either way, we need to consider the size of the data we're processing, both
    concerning reads and writes. Fortunately, it's easy to scale out Lambda functions,
    so executing 10s or 100s of Lambda functions are of little difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve drawn the overall architecture of our system in the following block diagram.
    We''ll walk through each of the five steps in detail. For this diagram, the actual
    problem at hand is less important than the real architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc0d6620-3097-4c26-8755-6c6946dfc3d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our implementation can be broken down into five significant steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We trigger a `driver` function that lists the content of a particular bucket
    on S3\. For each file in S3, the driver triggers an SNS event that ultimately
    triggers a `mapper` function. Each `mapper` function receives a different payload
    that corresponds to a file in S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mappers read data from S3 and perform a first level aggregation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mappers write the intermediate *keys* result to S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data writes to S3 trigger reducers. Every time a reducer is triggered, it
    checks whether all of the intermediate *keys* data is ready. If not, the reducer
    does nothing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all of the *keys* data is ready, a reducer will run a final aggregation
    across all intermediate *keys* files and write the final results to S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many things are going on here, each of which we will discuss in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Processing Enron emails with serverless MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I've based our example application on the Enron email corpus, which is publicly
    available on Kaggle. This data is made up of some 500,000 emails from the Enron
    corporation. In total, this dataset is approximately 1.5 GB. What we will be doing
    is counting the number of From-To emails. That is, for each person who *sent*
    an email, we will generate a count of the number of times they sent *to* a particular
    person.
  prefs: []
  type: TYPE_NORMAL
- en: Anyone may download and work with this dataset: [https://www.kaggle.com/wcukierski/enron-email-dataset.](https://www.kaggle.com/wcukierski/enron-email-dataset) The
    original data from Kaggle comes as a single file in CSV format. To make this data
    work with this example MapReduce program, I broke the single ~1.4 GB file into
    roughly 100 MB chunks. During this example, it's important to remember that we
    are starting from 14 separate files on S3.
  prefs: []
  type: TYPE_NORMAL
- en: The data format in our dataset is a CSV with two columns, the first being the
    email message location (on the mail server, presumably) and the second being the
    full email message. Since we're only concerned with the `From` and `To` fields,
    we'll just concern ourselves with the email message.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter may be found at: [https://github.com/brianz/serverless-design-patterns/tree/master/ch8](https://github.com/brianz/serverless-design-patterns/tree/master/ch8)
  prefs: []
  type: TYPE_NORMAL
- en: Driver function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To initiate the entire process, some event needs to be triggered. Here, we'll
    do this manually. The `driver` function is responsible for setting up the whole
    job and invoking the mappers in parallel. We'll accomplish this using some straightforward
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: By their nature, MapReduce jobs are batch-oriented, meaning they start up, do
    their work, write the results somewhere, and finally shut down. As such, doing
    this on some schedule (whether it be hourly, nightly, or weekly) makes sense.
    If we were doing this for real where the input data was changing, it would be
    trivial to set up this `driver` function to run on a schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, the entry point for all our functions is the `handler.py` file, which
    I have not shown. The `driver` function will invoke the `crawl` function located
    in `mapreduce/driver.py`. The crawl function contains all of the logic, so we''ll
    focus on this. I''ve shown the full listing of `mapreduce/driver.py` in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: One implementation detail we will use is uniquely identifying each MapReduce
    run using a UUID. In this way, it will be easy for a given run to find the necessary
    files to work within S3\. Without this, it would be much harder or impossible
    to know what files a given Lambda function should be looking at or processing.
  prefs: []
  type: TYPE_NORMAL
- en: As this crawler process starts, it lists the content of our input bucket on
    S3\. Each file or S3 `key` the crawler finds is wrapped up into a payload that
    it later uses to trigger the mappers. In the preceding code block, you can see
    the format of the payload objects. Downstream, the reducers will need to know
    how many total mappers were executed so that they know when to begin their work.
    The final `for` loop will amend each payload with the total number of mapper jobs
    being executed along with a unique `job_id`, which is merely an integer from `0`
    to `number_of_mappers - 1`.
  prefs: []
  type: TYPE_NORMAL
- en: To trigger the mappers in parallel, the crawler sends an SNS event. We could
    have accomplished this with mostly the same result by invoking the mappers directly.
    Personally, I prefer using SNS in these cases since the behavior is asynchronous
    by default. If you remember back to the chapter on the Fanout pattern, invoking
    an asynchronously Lambda function requires you to pass the correct argument to
    the Lambda `invoke` API. In this case, there isn't anything special to remember,
    and our code can trigger the event in the most basic fashion. In this particular
    case, there is otherwise little difference between the two methods and either
    would work.
  prefs: []
  type: TYPE_NORMAL
- en: What is important to recognize here is that an SNS event is triggered for each
    file the crawl function finds in S3\. In our example, there are 14 different files
    of approximately 100 MB each. Fourteen records mean that we will have 14 mapper
    functions running in parallel, each processing a specific S3 file. Mappers know
    which file to process because we've told them via the `bucket` and `key` arguments
    in the payload.
  prefs: []
  type: TYPE_NORMAL
- en: Astute readers may recognize this sub-pattern in the `crawl` function. A single
    function spawning multiple processes asynchronously is exactly what we discussed
    and implemented in the earlier chapters concerning the Fanout pattern. As noted
    in that chapter, you may use Fanout inside other more complex patterns such as
    MapReduce. As you move along with your serverless systems, look for opportunities
    to reuse patterns as they make sense when composing larger and more complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: Mapper implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a way to invoke mappers in parallel, let's look at the logic
    that they implement. Remember again that our task is to count the number of `(From,
    To)` email addresses from a large number of email messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The work involved here is relatively straightforward. With each mapper receiving
    a unique 100 MB file, each invocation will perform the same set of tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the file from S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse each message and extract the `From` and `To` fields, making sure to account
    for group sends (where the `From` user sends to multiple `To` addresses)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count the number of `(From, To)` occurrences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the results to S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I''ve shown the full listing of `mapreduce/mapper.py`  in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As with the crawler, there isn't much complexity to this mapper code. To count
    the number of `(From, To)` combinations I'm using a basic Python dictionary with
    the keys being a two-element tuple of `(From, To)` and the value being a number.
    The other bits of code around this deal with downloading the file from S3, parsing
    the email message, and calculating all of the `(From, To)` combinations, when
    an email contains multiple To recipients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the final result is ready, the mapper writes a new CSV file to S3\. Using
    the `Metadata` argument, we can communicate any extra information to our reducers
    without having to write to the file content. Here, we need to tell the reducers
    a few extra things such as:'
  prefs: []
  type: TYPE_NORMAL
- en: The `run_id`, which is used to limit the files scanned and processed since we're
    sharing an S3 bucket across MapReduce runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `job_id`, so we know which individual mapper job has finished
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of jobs, so the reducer will only start once all mappers have
    completed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducer implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point in our MapReduce run, the mappers have run and eventually write
    their intermediate output data to S3\. Mappers were triggered by an invocation
    of SNS events on a given SNS topic. We will set up the reducers to be triggered
    based on an `s3:ObjectCreated` event. Taking a look at the `serverless.yml` file,
    we can see how I''ve done this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `s3` section in the `events` block says: *Whenever a new object is uploaded
    to s3 with the *`-done.csv` *suffix, invoke the* `hander.reducer` *function*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as the mapper was reasonably straightforward, so too is the reducer. Much
    of the logic in the reducer is a matter of coordination, determining whether it''s
    time to do its work. Let''s enumerate the steps in the reducer to show precisely
    what it''s doing:'
  prefs: []
  type: TYPE_NORMAL
- en: Extra metadata from the S3 file that triggered the invocation. Key pieces of
    data in that `Metadata` attribute are necessary for coordination of the entire
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the contents of our S3 bucket and `run_id` prefix to determine whether
    all mappers have finished.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are still reducers running, there is nothing more to do. If all of
    the reducers *have* finished, start the final reduce step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write an empty file to S3, as a way to claim a lock on the final reduce step.
    Without this, it would be possible for two or more reducers to run concurrently
    if they were invoked at nearly the same time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the final reduce step, download all of the intermediate files from the mappers
    and perform the final aggregation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the final output to S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The full listing of `mapreduce/reducer.py` is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Stepping through this code is hopefully a simple exercise. As you can see, most
    of the work is that of coordination, reading data from S3, and determining whether
    it's time to perform the final reduce step. You can see that when the last mapper
    is finished, the total number of intermediate files will equal the number of mapper
    jobs initially invoked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at S3, we can see the final results after a successful run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d49365c-7db4-4d3f-8054-0c4d4e4ece2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, each mapper job created a unique `mapper-job_id-done.csv` file. Once all
    14 files arrived in S3, the final reducer step began, which ultimately read all
    14 files and produced the `FinalResults.csv` file. You can also see how individual
    MapReduce runs are segregated in S3 with the UUID embedded in each S3 key path.
    This is necessary so that each run can operate independently and know which files
    it should be scanning through in S3\. Again, a critical check in the final reducer
    step is to determine whether all of the mappers have finished their work and uploaded
    their results to S3\. The reducer will determine the mapper's state of completeness
    by counting the number of files in S3 using the `run_id` as a prefix during the
    S3 scan. If the number of these `-done.csv` files is less than the total number
    of mappers, they have not all completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a look at `FinalResults.csv`, we can see the count of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: What is neat about this is that the processing of all 1.5 GB of data happens
    quite quickly. In my testing, the system produced the final results after approximately
    50 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the limitations of serverless MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce on a serverless platform can work very well. However, there are limitations
    that you need to keep in mind. First and foremost, memory, storage, and time limits
    will ultimately determine whether this pattern is possible for your dataset. Additionally,
    systems such as Hadoop are frameworks that one may use for any analysis. When
    implementing MapReduce in a serverless context, you will likely be implementing
    a system that will solve a particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: I find that a serverless MapReduce implementation is viable when your final
    dataset is relatively small (a few hundred megabytes) such that your reducer can
    process all of the data without going over the memory limits for your FaaS provider.
    I will talk through some of the details behind that sentiment in the following.
  prefs: []
  type: TYPE_NORMAL
- en: Memory limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the reducer phase, all of the data produced from the mappers must, at some
    point, be read and stored in memory. In our example application, the reducer reads
    14 separate files sequentially and builds up a mapping of `(From, To)` addresses
    with corresponding numbers. The number of unique combinations for this dataset
    is 311,209\. That is, our final results file is a CSV with just over 311,000 lines
    for a total of 18.2 MB. As you can imagine, this is well within the boundaries
    of a single Lambda function; reading 14 files keeping approximately 18 MB of data
    in memory isn't beyond the abilities of an individual Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a case where we are counting IP addresses from a large number of large
    log files along with some other metric. IP addresses have the form `192.168.1.200`
    and can vary in their lengths when represented as a string. For this example,
    presume the format of the lines produced by the reducer will look like `176.100.206.13,0.6088772`,
    which is a single line of CSV with the IP address in the first column and a made-up
    metric in the second column. This string is 24 bytes long. Currently, the maximum
    memory for a single Lambda function is 3 GB, which is 3,221,225,472 bytes. With
    an average length of 24 bytes per IP address, we can hold less than 135 million
    unique IP addresses in memory—`3,221,225,472 / 24 = 134,217,728`. There are approximately
    3,706,452,992 unique IP4 addresses. It's clear that a serverless MapReduce implementation
    for working with IP addresses would break down if the number of unique IP addresses
    in the dataset was in the order of 100 million or more.
  prefs: []
  type: TYPE_NORMAL
- en: Storage limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FaaS systems have storage limits just like they have memory limits. If you have
    looked at the code I've implemented in the example application, you may have noticed
    that I download files and store them in `/tmp` before processing them. This strategy
    isn't necessary, as it's possible to read data from S3 and store it in memory.
    In my testing, I found performance gains when downloading the files to disk and
    then reading them with the standard filesystem `open` calls. Some of the CSV APIs
    I was using were also easier to use with a real file handler rather than a String
    in memory.
  prefs: []
  type: TYPE_NORMAL
- en: When downloading data and storing the files locally, you must keep in mind the
    storage limits enforced by your FaaS provider. For example, AWS Lambda currently
    gives you 512 MB of ephemeral storage in `/tmp`. If you have a need to download
    files larger than 512 MB, you would need to find another solution, such as reading
    data directly into memory and skipping disks entirely. Reading large data into
    memory will cut into your memory for the final result set, so the balance of getting
    this right when dealing with huge datasets can be tricky.
  prefs: []
  type: TYPE_NORMAL
- en: Time limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final limit to keep in mind is the execution limit. Even if your MapReduce
    implementation can stay within the storage and memory limits of your FaaS provider,
    you will still have to ensure your functions complete their work within a given
    time limit. As of this writing, AWS Lambda functions have an upper limit of 300
    seconds. If any part of your MapReduce system takes longer than 300 seconds, you're
    out of luck and will have to find a workaround. With mappers, it's relatively
    simple to break the work into smaller pieces and execute more concurrent mappers.
    However, when the reducer runs, it must load all of the mapper data to compile
    it down to the final result set. If this takes longer than 300 seconds, it will
    be impossible to produce the final results.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring alternate implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While you may find great success implementing your serverless MapReduce system,
    there are alternatives that still fall under the serverless umbrella or leverage
    managed services, which should give you a high degree of confidence. I'll talk
    through some of the other systems or techniques you should consider when working
    on your own data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Athena is a relatively new service from AWS. Of course, this is specific
    to AWS, but other cloud providers may offer comparable services. Athena gives
    you the ability to write SQL queries to analyze data stored on S3\. Before you
    can analyze your data with SQL, you must create a virtual *database* with associated
    *tables* across your structured or semi-structured S3 files. You may create these
    tables manually or with another AWS service called **Glue**.
  prefs: []
  type: TYPE_NORMAL
- en: I won't go into all of the details of setting up a new Athena database or tables
    but will show you the results and ease of use after you've set those up. In this
    example, I've created a database and table for web server logs from the **Big
    Data Benchmark** dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This data is publicly accessible on S3, and the details may be found at the
    Big Data Benchmark website: [https://amplab.cs.berkeley.edu/benchmark/](https://amplab.cs.berkeley.edu/benchmark/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a screenshot from the AWS Athena console. As you can see on the
    left, I''ve loaded up my `uservisits` table, which merely points to the public
    S3 bucket for the `uservisits` log data. I''ve already created a table so that
    Athena knows the structure and datatypes for the CSV data stored on S3\. Once
    this is done, I can use ANSI-SQL queries to analyze the data. In the following
    screenshot, you can see how I''ve selected the first 10 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79604fe2-00cc-4317-8d2d-72cbb745cd15.png)'
  prefs: []
  type: TYPE_IMG
- en: It's also possible to rename columns to something meaningful and change datatypes
    for each column. In my previous example table, several columns are named `colX`,
    where I've renamed other columns `ip`, `score`, and `agent`.
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, I''ll run a query that calculates the total score grouped by IP
    address. This query and the results can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6adb762a-eb47-483c-8545-78a2be0914ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final results of this query were quite impressive in my opinion. The query
    scanned just over 24 GB of data on S3 and took just under five minutes to execute.
    I can view this metadata in the History area of Athena:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24150ad7-b44c-4528-9db6-53bfbe198aab.png)'
  prefs: []
  type: TYPE_IMG
- en: Given the simplicity of the Athena system, it's one that I would strongly suggest
    you investigate. What is nice about this is that new data may arrive on S3 at
    regular intervals, and your queries would reflect the results whenever they're
    run. Additionally, the overhead and price for running these queries are quite
    low. Anything you can do with ANSI-SQL is possible with Athena. However, Athena
    has limitations in that your data needs to be well structured and the data prepared
    ahead of time. In our example MapReduce application, we had application logic
    that was extracting the `To` and `From` fields from email text. To do this with
    Athena would require a data preparation step to extract that data from the source
    data, and then store the extracted and structured information on S3.
  prefs: []
  type: TYPE_NORMAL
- en: Using a data store for results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example MapReduce system, we stored the state in S3\. That is, every
    mapper would work on a subset of the dataset, do some initial reduce step, and
    then save the intermediate results as a file on S3\. This technique is helpful
    since it's relatively simple and storing static data on S3 is painless. Since
    each mapper is writing a unique file to S3, we also didn't have to worry much
    about race conditions or other mappers overwriting our data.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of this technique is that our reducer needs to read in all of the
    intermediate results to do the final reduce step. As I explained earlier, this
    *could* be a limiting factor for your system depending on the size of the final
    result. One alternative implementation would be using a data store such as Redis
    to store the mapper keys and values.
  prefs: []
  type: TYPE_NORMAL
- en: The way this would work is that mappers, working in parallel, would process
    subsets of the initial dataset. Once the mappers have finished their initial aggregation,
    they would write the results to Redis, which would act as a central location for
    all of the reduced data. Mappers would either insert new records for a particular
    key if that key did not exist or update the data for a key. In some cases, such
    as counting items, we wouldn't even need a reducer as the mappers would merely
    increment the value stored in Redis if the key was already present.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where we would like to calculate an average or something else that
    depends on keeping track of all values for a particular key, the reduce step would
    consist of scanning through all keys and performing the final reduce step based
    on the values stored for each key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a case where we were calculating the average value per key. Mappers
    would perform work that looked something along these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Making sure to use the `pipeline` technique to ensure we don''t hit a race
    condition, our mappers push results into Redis as lists for each key. Reducers
    would then iterate around all of the keys and perform a count of the number of
    items in each list, as well as summing up the entire list. For example, the average
    value for a particular key named `height` would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: While Redis is incredibly performant, it would still be easy to overwhelm a
    single Redis server with enough concurrency from serverless functions. The Redis
    (or another data store) technique could also be a good workaround for cases when
    you reach memory limitation in your serverless MapReduce systems. There are other
    things to consider too, such as, how do you finally report the entire result set
    in aggregate, if needed. Also, how or when do you clear out the Redis DB?
  prefs: []
  type: TYPE_NORMAL
- en: Using Elastic MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic MapReduce (EMR) from AWS is another alternative if you need the full
    power of Hadoop. EMR is just what it sounds like, a managed Hadoop system that
    is easy to scale up or down as required. The advantage of EMR is that Hadoop developers
    should feel comfortable since it is a managed Hadoop infrastructure on demand.
    EMR can also run other frameworks, such as Spark and Hive.
  prefs: []
  type: TYPE_NORMAL
- en: EMR doesn't fit with the *serverless* theme really, since you pay for every
    minute that a cluster is up, regardless of whether it's running any of your jobs.
    Still, the fact that you can have a fully-managed Hadoop cluster is quite attractive
    if your use cases warrant it. Another beautiful thing about EMR, as with all things
    cloud, is that it's possible to create a cluster on-demand, run your jobs, and
    then shut it down. Creating and destroying an EMR cluster requires some form of
    automation with API calls, CloudFormation, or Terraform, but it's still possible
    and the more automation you can put in place, the better.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I gave an overview of what the MapReduce pattern looked like
    in a general sense and demonstrated how MapReduce works with some example code.
    From there, we reviewed the MapReduce pattern as applied to serverless architectures.
    We stepped through the details of implementing this pattern by parsing 1.5 GB
    of email data and counting the unique occurrences of `From` and `To` email addresses.
    I showed that a serverless system could be built using this pattern to perform
    our task in less than a minute, on average.
  prefs: []
  type: TYPE_NORMAL
- en: We covered some of the limitations of this pattern when implemented on a serverless
    platform. Finally, we discussed alternative solutions for general data analysis
    problems using serverless platforms such as AWS Athena and managed systems such
    as EMR, as well as ways to use a centralized data store such as Redis in a serverless
    MapReduce system.
  prefs: []
  type: TYPE_NORMAL
