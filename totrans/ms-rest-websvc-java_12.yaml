- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring and Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Monitoring** and **observability** are fundamental to ensuring that RESTful
    services operate reliably, efficiently, and securely. In distributed systems,
    where requests often pass through multiple services and components, having proper
    observability practices in place is critical. Without them, diagnosing performance
    bottlenecks, identifying the root causes of errors, and optimizing service behavior
    becomes incredibly challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will guide you through the essential practices and tools
    needed to achieve effective observability for RESTful services. We will begin
    by discussing **logging** best practices, explaining how structured logging and
    correlation IDs can simplify troubleshooting. We will then delve into distributed
    tracing, demonstrating how trace information such as `traceId` , `spanId` , and
    `parentSpanId` helps map the lifecycle of a request as it flows across multiple
    services.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce **Micrometer Tracing** , a powerful observability framework
    integrated into Spring Boot 3.x, which automatically instruments applications
    to capture trace data. Additionally, we will explore OpenTelemetry, a vendor-neutral
    framework that extends observability by collecting and correlating logs, metrics,
    and traces for a holistic view of distributed systems. By the end of this chapter,
    you will understand how to implement logging and tracing effectively, configure
    Spring Boot applications for observability, and visualize your data in tools like
    Zipkin and Jaeger.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides practical examples, step-by-step guidance, and best practices
    to ensure you can successfully monitor and debug your RESTful services.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Importance of logging in REST APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging best practices for API troubleshooting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging basics with SLF4J
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a central logging filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing service tracing in distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing tracing using Micrometer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics from tracing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenTelemetry for monitoring and observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for end-to-end observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will enhance our existing APIs—the Product API and the
    Order Management API—to be able to trace requests between them. To be able to
    follow along and use the code examples as they are printed in the book, you should
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate knowledge of the Java language and platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least a basic knowledge of Spring Boot or a similar framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java 21 and Maven 3.9.0 installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker 27.3.1 or higher installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can access the code for this chapter on GitHub at [https://github.com/PacktPublishing/Mastering-RESTful-Web-Services-with-Java/tree/main/chapter9](https://github.com/PacktPublishing/Mastering-RESTful-Web-Services-with-Java/tree/main/chapter9)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Importance of logging in REST AP Is
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging is the foundational element of observability in any system. Logs are
    essentially records of events that happen while your application runs. They are
    the first source of information when trying to troubleshoot an issue in any environment,
    especially in production.
  prefs: []
  type: TYPE_NORMAL
- en: Logs act as the system’s memory, providing insights into what went wrong or
    how certain processes behaved. For a REST API, logs can show the path of each
    request, which is crucial for understanding failures or slow performance.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, suppose your API returns a `500` `Internal Server Error` to a
    user. Without logs, you would have no way of knowing what caused the error. However,
    with logs, you could see that a database query failed because the server ran out
    of connections, helping you fix the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we are going to cover an effective logging design that
    will empower our applications significantly when troubleshooting is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Common logging pitfalls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the importance of logging, many developers struggle with using logs
    effectively. Common mistakes include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Over-logging** : Logging too much information can make it hard to find key
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Under-logging** : Insufficient logging might leave out key data needed for
    troubleshooting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging sensitive information** : Mistakenly logging things like user passwords
    or credit card numbers, which should never happen due to security and compliance
    concerns (e.g., GDPR or PCI DSS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A balance needs to be struck between logging enough to troubleshoot issues and
    not overwhelming the system with unnecessary data.
  prefs: []
  type: TYPE_NORMAL
- en: Effective log design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Designing logs properly is crucial for them to be useful. Each log entry should
    include relevant metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Timestamp** : When the log entry was created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log level** : Severity of the log ( `INFO` , `DEBUG` , `ERROR` ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service name** : The name of the service generating the log.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation ID** : A unique ID that allows you to track a request through
    multiple services (more on this below).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, a well-structured log entry in JSON might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![img](img/Information_Box_Icon.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation IDs
  prefs: []
  type: TYPE_NORMAL
- en: In microservices, a request might pass through multiple services. To track the
    entire request path, it is essential to attach a correlation ID to each request.
    The correlation ID is a unique identifier that stays with the request as it moves
    through the system. This allows you to correlate logs from different services
    to see how a single request was handled end to end. For example, a user request
    to retrieve profile information might go through an API gateway, then hit the
    authentication service, and finally query the user database. With a correlation
    ID, you can trace each step of the process across all services involved. Spring
    makes it easy to generate and propagate correlation IDs. You can generate a correlation
    ID at the start of a request and pass it along with HTTP headers between services.
    This will help you diagnose where issues occur in a chain of services.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about the best logging practices, understand the different
    log levels and their structure, and what should and should not be logged to avoid
    security issues.
  prefs: []
  type: TYPE_NORMAL
- en: Logging best practices for API troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective logging is essential for troubleshooting, especially when working
    with distributed systems or cloud-native architectures. Here are some best practices
    to ensure your logging strategy is robust.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right log level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logs should be written at the appropriate log level, which indicates the severity
    of the event:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TRACE` : Used for very fine-grained details, primarily for debugging low-level
    operations like recording the internal state of loops, method entry/exit points,
    or interactions between components in detail. Should be turned off in production
    due to the huge amount of logs that are generated at this level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DEBUG` : Used for low-level information that helps in debugging issues, such
    as details of an HTTP request. Logs provide detailed information that helps during
    development or debugging, focusing on application-specific logic or operations.
    Also, this level should be turned off in production due to the huge amount of
    logs that are generated at this level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INFO` : Used for general application flow information, such as when a service
    starts up or shuts down. These logs are less verbose than `DEBUG` or `TRACE` and
    are typically enabled in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARN` : Indicates something unusual but not necessarily an error. For example,
    a service might temporarily run out of resources but recover. These logs are a
    warning for potential future issues. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERROR` : Used when something has gone wrong, like an exception being thrown
    or a critical failure in a database connection. These logs often indicate that
    the system requires attention or intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FATAL` : Indicates a critical error that causes the application or service
    to crash or become unusable. These logs are extremely rare and signify the most
    severe issues that require immediate attention. Note that this level is not present
    universally in libraries like SLF4J or Logback and is often represented by the
    `ERROR` level; however, it is present in Log4J and Log4J2 logging libraries, which
    will not be covered by this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a REST API, a failed user login attempt due to incorrect credentials might
    be logged at the `WARN` level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Meanwhile, a database outage that causes the entire service to fail should
    be logged at the `ERROR` level or as `FATAL` if available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Choosing the right level ensures that you can quickly filter out non-critical
    logs when troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Structured logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Structured logging refers to logging in a consistent, machine-readable format,
    such as JSON. This allows logs to be easily parsed and queried by logging tools
    (like ELK Stack or Splunk), making it easier to filter, aggregate, and analyze
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than logging a simple message like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You should log the event in a structured format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can easily search for all login failures or group them by the `user`
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding sensitive data in logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sensitive information such as passwords, credit card numbers, or personal identifiers
    should never be logged. If this data is accidentally exposed in logs, it can lead
    to serious security breaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if a login attempt fails, it is okay to log the username, but
    never log the password. A log message like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'is dangerous. Instead, log something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can use Jackson’s `@JsonIgnore` or `@JsonProperty(access = Access.WRITE_ONLY)`
    annotations to prevent sensitive data from being serialized into logs.
  prefs: []
  type: TYPE_NORMAL
- en: Jackson is a widely used Java library for processing JSON data. It provides
    powerful capabilities for serializing Java objects into JSON and deserializing
    JSON into Java objects, making it a crucial tool in RESTful web services where
    data is often exchanged in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: In a Spring Boot application, Jackson is the default JSON processor and is commonly
    used to automatically transform request and response payloads, making API interactions
    seamless.
  prefs: []
  type: TYPE_NORMAL
- en: It allows developers to customize JSON output using annotations like `@JsonIgnore`
    , `@JsonProperty` , and `@JsonInclude` , ensuring that only the necessary fields
    are exposed while sensitive or unnecessary data is excluded.
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly useful when logging requests or response objects, as it
    ensures that sensitive information (like passwords or credit card details) does
    not get exposed in log entries.
  prefs: []
  type: TYPE_NORMAL
- en: When to use each annotation?
  prefs: []
  type: TYPE_NORMAL
- en: '`@JsonIgnore` : Use when you want to prevent a field from ever being included
    in serialized output, such as responses and logs, and deserialized input from
    requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@JsonProperty(access = Access.WRITE_ONLY)` : Use when you need to accept the
    field as input but want to exclude it from all serialized output, making it suitable
    for fields that should remain private (e.g., passwords) during logging or API
    responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us go through an example to demonstrate how to use these annotations in
    a RESTful service.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a `User` class with fields like `username` , `email` , and
    `password` . When logging this `User` object, we want to ensure that the password
    field is not included in the serialized output.
  prefs: []
  type: TYPE_NORMAL
- en: Completely exclude the field from serialization/deserialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `@JsonIgnore` annotation completely omits a field from deserialization and
    serialization, meaning it will not be included in the input or the output JSON
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: When you use `@JsonIgnore` on a field, it is completely ignored by Jackson both
    during serialization (when converting an object to JSON) and deserialization (when
    converting JSON to an object). This means that if you mark a field with `@JsonIgnore`
    , Jackson will neither include it in the output JSON nor allow it as an input
    in the JSON request body.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when you want to ensure that sensitive information (e.g., passwords
    or tokens) is never exposed in any serialized output, including logs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With this setup, if you log the User object using Jackson for serialization,
    the password field will be omitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Output log (password is excluded)** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `password` field is completely omitted from the log output
    because of the `@JsonIgnore` annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to exclude fields from serialization and deserialization,
    let us see how to allow a field to be deserialized from JSON input but excluded
    from serialization, such as in logs or API responses.
  prefs: []
  type: TYPE_NORMAL
- en: Allowing data input deserialization but excluding it from output serialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `@JsonProperty(access = Access.WRITE_ONLY)` annotation is useful when you
    want a field to be deserialized (e.g., when receiving input from a user) but not
    serialized (e.g., when logging or sending data as a response). This is common
    for fields like passwords in user registration forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With `@JsonProperty(access = Access.WRITE_ONLY)` , you can still accept the
    `password` field in incoming JSON requests, but it will be excluded from any serialized
    output, including logs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example usage:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When a new user registration request comes in, the `password` field will be
    available for processing, but it will not be logged.
  prefs: []
  type: TYPE_NORMAL
- en: '**Incoming request** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Output log (password is excluded from the log)** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With `@JsonProperty(access = Access.WRITE_ONLY)` , the password is received
    by the application but is excluded from logs or other serialized JSON output,
    ensuring that sensitive data is protected.
  prefs: []
  type: TYPE_NORMAL
- en: By using these annotations strategically, you can control sensitive data exposure
    in logs, which is an essential part of security best practices for RESTful APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get back to the last best practice for logging
  prefs: []
  type: TYPE_NORMAL
- en: Capturing contextual information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make logs more useful, include contextual information about each request,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP method ( `GET` , `POST` , etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endpoint (e.g., `/api/v1/users/123` )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response status ( `200` , `404` , `500` , etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request headers and payloads (but be careful to exclude sensitive data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This information will allow you to better understand what happened during an
    API request.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a request log might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By capturing such details, you can correlate issues with specific requests and
    quickly pinpoint where things went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into one of the most famous logging libraries in the market,
    SLF4J, and how to create logs from our application.
  prefs: []
  type: TYPE_NORMAL
- en: Logging basics with SLF4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces how to add logging points into your own Java code using
    SLF4J. This is crucial since you need to add loggers throughout your entire application
    to be able to generate logs from its flow that will help you troubleshoot it when
    it is deployed, especially in production.
  prefs: []
  type: TYPE_NORMAL
- en: In Spring Boot, SLF4J (Simple Logging Facade for Java) is commonly used as a
    logging API that can work with different logging frameworks (such as Logback,
    Log4j2, etc.). Spring Boot uses SLF4J by default and integrates it seamlessly,
    so all we need to do is inject the logger and start logging messages.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you are not using Spring Boot, you can just add the SLF4J dependency
    to your Maven POM dependencies file or Gradle dependencies file to use it if the
    framework that you are using does not already include it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start with a simple example of how to log a Spring Boot application
    using SLF4J. Here is a User Creation service that uses SLF4J to log messages at
    different log levels ( `INFO` , `WARN` , `ERROR` ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example:'
  prefs: []
  type: TYPE_NORMAL
- en: We log informational messages with `logger.info()` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We log an error with `logger.error()` , including the exception stack trace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LoggerFactory.getLogger(UserService.class)` creates a logger specifically
    for the `UserService` class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For successful user creation, we would have a log output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, for a failed user creation, for example, trying to create a null user,
    the output would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Using SLF4J with different log levels helps organize and filter log messages,
    making it easier to troubleshoot and debug.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let us look at how we can automate some of the logging
    in our application to reduce the burden of having tons of logging everywhere in
    your code, by implementing a central logging filter.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a central logging filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To improve observability in a RESTful application, we can implement a central
    logging component that logs all incoming HTTP requests and responses. A filter
    is an effective choice for this as it allows you to intercept requests before
    they reach the controller layer, enabling you to log key requests and response
    details in one place.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will implement a `RequestLoggingFilter` that logs incoming
    requests, following best practices such as structured logging, adding correlation
    IDs, and avoiding sensitive information. This filter will log essential request
    metadata, such as HTTP method, URL, status code, and processing time, in a structured
    JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: So, what is this filter doing?
  prefs: []
  type: TYPE_NORMAL
- en: '**Choosing the right log level** : The filter logs complete requests at the
    `INFO` level, which is appropriate for general application flow information. If
    a request encounters an error, it could be logged at `ERROR` in other components,
    such as exception handlers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured logging** : The filter uses structured logging to log information
    in JSON format, including fields like `correlationId` , `method` , `url` , `status`
    , and `duration` . Structured logging allows for easier parsing, searching, and
    aggregating in centralized logging tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding sensitive data** : The filter avoids logging the request body directly,
    which could contain sensitive information like passwords. If needed, further filtering
    can exclude or mask sensitive data in headers or query parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capturing contextual information** : The filter captures relevant metadata
    for each request, including the HTTP method, URL, status code, and duration. This
    provides valuable context for debugging and performance analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using correlation IDs** : The filter generates a correlation ID (if one is
    not already present) and stores it in the **Mapped Diagnostic Context (MDC)**
    . This ensures that the correlation ID is added to all subsequent logs within
    the request’s lifecycle, enabling end-to-end tracking across services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![img](img/lightbulb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Mapped Diagnostic Context (MDC)
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapped Diagnostic Context (MDC)** is a feature in logging frameworks like
    SLF4J (with Logback) and Log4j that allows developers to store and retrieve contextual
    information per thread. This context information is automatically included in
    log messages, making it easier to track related logs across different parts of
    an application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this logging filter in place, here is an example of what a log entry might
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'But to have the `correlationId` spread to other services and enable the power
    of tracking the request across multiple services, we need to update the header
    with the proper newly generated value from `correlationId` , adding it before
    the request is sent. We will do it using `BeanPostProcessor` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As an example, in the Order Management API, before calling the `productsApi`
    into the `ProductsQueryUseCaseImpl` implementation, we will make a call to the
    `beanPostProcessor` to have the `ApiClient` bean updated with the `correlationId`
    set in the header from the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ApiClient` is the REST client that was generated by the OpenAPI plugin
    using the Product API specification and will be used for every call to the Product
    API from the Order Management API. Here is the updated version of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This ensures that the `correlationId` is propagated with every service request.
    The filter in the called service will read the `correlationId` and include it
    in the logs, enabling you to uniquely track requests across the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional functionality can be added to the filter to capture even more detailed
    logging information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log-specific headers** : Capture headers like `User-Agent` or `Authorization`
    , but exclude or mask sensitive details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional logging for error responses** : Modify the filter to log `4xx`
    and `5xx` responses at `WARN` or `ERROR` levels for easier error tracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling** : Combine this filter with a global exception handler to
    capture and log unhandled exceptions, leveraging the correlation ID to tie error
    logs to their originating requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This logging filter implements best practices and creates consistent, structured
    logs across the application, making it easier to monitor, troubleshoot, and analyze
    incoming API requests.
  prefs: []
  type: TYPE_NORMAL
- en: And to effectively track requests across multiple services, implementing tracing
    is essential. It helps maintain a clear trace and simplifies troubleshooting in
    distributed systems. In the next section, we will explore how to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing service tracing in distributed systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In distributed systems, where a request might span multiple services, **distributed
    tracing** provides visibility into how a request moves through various components.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by understanding what we mean by distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: What is distributed tracing?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed tracing allows you to follow the lifecycle of a request as it flows
    from one service to another. This helps you see where delays or errors occur.
    In tracing terminology, each step in the request’s journey is called a span, and
    a trace is the entire set of spans associated with a request.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine a request comes into your system to create a new user.
    This request might touch on the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: API gateway
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Authentication service
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: User service (to create the user in the database)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notification service (to send a welcome email)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these steps is a span, and all the spans together form a trace.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing tools, such as Zipkin or Jaeger, can visualize the trace
    and highlight which services or steps are causing delays.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will understand how each trace is identified uniquely by adding a trace
    ID to each request.
  prefs: []
  type: TYPE_NORMAL
- en: Using trace IDs for end-to-end request tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like correlation IDs help in logs, trace IDs are unique identifiers attached
    to each request, allowing for end-to-end request tracking” you to track that request
    across multiple services. The difference is that trace IDs are automatically managed
    by tracing systems and include timing information.
  prefs: []
  type: TYPE_NORMAL
- en: In Spring, the Micrometer Tracing library automatically generates trace IDs
    for each request and propagates them across service boundaries. These IDs are
    included in the logs and tracing systems, allowing you to correlate logs and traces
    for detailed troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Spring Boot application, Micrometer Tracing generates the following log
    message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`traceId` helps you connect this event to other related events across different
    services.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us learn how to implement the tracing feature with Micrometer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing tracing using Micrometer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the release of Spring Boot 3.x, Spring Cloud Sleuth has been replaced by
    Micrometer Tracing for tracing support. Micrometer Tracing is fully compatible
    with Spring Boot 3.x and offers a more modern, flexible way to implement distributed
    tracing in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will walk through how to implement distributed tracing in
    a Spring Boot application using Micrometer Tracing, enabling you to track requests
    across services and get detailed insights into their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Micrometer Tracing in Spring Boot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement tracing in a Spring Boot application using Micrometer Tracing,
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Add Micrometer Tracing dependencies** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Micrometer Tracing is part of the Micrometer ecosystem, and it integrates easily
    with Spring Boot 3.x. To enable Micrometer Tracing in your project, add the necessary
    dependencies to your `pom.xml` (if using Maven):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what these dependencies are used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '`micrometer-observation` : Provides the core Observation API that serves as
    a facade for metrics, logging, and tracing. It allows you to instrument code once
    and get multiple observability benefits, focusing on what you want to observe
    rather than how to implement it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`micrometer-tracing-bridge-brave` : Bridges the Micrometer Observation API
    to Brave, enabling distributed tracing capabilities. This dependency is responsible
    for creating and propagating trace and span IDs across service boundaries, which
    is what adds the trace context to your logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spring-boot-starter-actuator` : Provides production-ready features to help
    monitor and manage your application. It includes endpoints for health checks,
    metrics, and other operational data. This starter automatically configures the
    observability infrastructure when combined with the Micrometer dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these dependencies enable comprehensive observability with metrics,
    tracing, and health monitoring in a Spring Boot microservices architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Configure Micrometer Tracing** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you’ve added the dependencies, Micrometer Tracing is automatically configured
    in Spring Boot. By default, Micrometer will instrument your HTTP requests, generating
    trace IDs and span IDs for each incoming request. These IDs are propagated across
    service boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable tracing fully, you may need to configure how traces are exported.
    For example, to export traces to Zipkin, add the following configuration to your
    `application.yml` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this configuration, we have the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`management.tracing.enabled=true` : Enables tracing for the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`management.tracing.sampling.probability=1.0` : Ensures that all requests are
    traced (for production, you might want to adjust this for performance reasons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`management.tracing.zipkin.enabled=true` : Enables exporting traces to Zipkin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`management.tracing.zipkin.endpoint` : Specifies the URL of your Zipkin server
    for trace collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logging.pattern.level` : Specifies a pattern on which each log will be presented.
    Here, we are setting it to log as follows: application name, traceId, spanId.
    Note that the format for getting the values in the logs for the trace ID and the
    span Id can differ from library to library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuring RestClient for tracing**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To guarantee that the generated trace is propagated through the called services,
    you need to configure your HTTP client to propagate the trace context. This is
    done by configuring `RestClient` to use the one that is created by Spring, using
    the dependencies and configurations done in steps 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using `OpenAPI` in this book to generate the client that queries
    the Products API from the Order Management API, you need to override the generated
    `RestClient` from OpenAPI with the one from instantiated in Spring.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, you need to properly set the beans in the configuration. In the
    Order Management API, these configurations are done on the `ProductsApiConfiguration`
    class, under the `adapter.outbound.rest` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This code configures the `RestClient` bean with a custom `ClientHttpRequestFactory`
    and an `ObservationRegistry` from the imported `micrometer-observation` dependency.
  prefs: []
  type: TYPE_NORMAL
- en: Then, it adds the configured `RestClient` bean into the `ApiClient` bean, followed
    by the configured `ApiClient` bean into the `ProductsApi` bean. We use the `ProductsApi`
    bean to call the external Products API from the Order Management API.
  prefs: []
  type: TYPE_NORMAL
- en: This is how we ensure that the `traceId` and `spanId` values are generated and
    that `traceId` is properly propagated over all the services that we call from
    the Order Management API.
  prefs: []
  type: TYPE_NORMAL
- en: But also, after that, we are configuring a custom `SpanHandler` with a `LogSpanHandler`
    . That is used to log useful information from the trace context into the application
    logs. Information like the duration, the request origin, its timestamp, and the
    `traceId` give us various data to monitor and troubleshoot the application in
    production. In the next section, you will see a real example logged by this `SpanHandler`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/Information_Box_Icon.png)'
  prefs: []
  type: TYPE_IMG
- en: What are beans?
  prefs: []
  type: TYPE_NORMAL
- en: In Spring, beans are simply Java objects that are created, managed, and configured
    by the Spring **IoC (Inversion of Control)** container. Think of them as the building
    blocks of your application. When you define a class as a bean, you’re telling
    Spring to take responsibility for instantiating it, handling its dependencies,
    and managing its lifecycle. You can define beans using annotations like `@Component`
    , `@Service` , or `@Bean` , or through XML configuration. Once registered, these
    beans can be automatically “wired” together, meaning Spring will inject dependencies
    between them without you having to manually create and connect objects. This approach
    makes your code more modular, easier to test, and less coupled.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing trace data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your application is instrumented with Micrometer Tracing, you can view
    the trace data in a distributed tracing tool like Zipkin or Jaeger. These tools
    allow you to visualize traces and spans, helping you diagnose performance bottlenecks
    or failures across services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to understand the difference between traces and spans:'
  prefs: []
  type: TYPE_NORMAL
- en: The trace would include multiple spans representing the various services and
    operations involved in a request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each span includes timing data, enabling you to identify slow services or problematic
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When Micrometer Tracing is implemented in a Spring Boot application, the logs
    will include additional fields such as `traceId` and `spanId` . These fields help
    you correlate logs across services in a distributed system. The `traceId` remains
    the same for the entire lifecycle of a request across different services, while
    each service or operation within a service gets its own `spanId` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how the logs will look with Micrometer Tracing enabled
    and properly configured, calling the Products API from the Order Management API.
    It follows the logging pattern that we defined in the `applications.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the elements contained in this log:'
  prefs: []
  type: TYPE_NORMAL
- en: '`2025-03-19T16:45:39.207-03:00` : **Timestamp** – When this log entry was created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INFO` : **Log level** – Indicates an informational message (not an error)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[order-management-api,67db1edfd85f42d21368a69936519fd1,1368a69936519fd1]`
    : Defined pattern logging, containing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service name** – Identifies which microservice generated the log'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trace Id** – Unique identifier tracking the request across all services'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Span Id** – Identifies this specific operation within the trace'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`24477` : **Process ID** – The operating system’s identifier for this application
    instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`---` : **Separator** – Visual divider in the log format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[order-management-api]` : **Application name** – Repeats the service name
    for readability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[nio-8090-exec-1]` : **Thread name** – The specific execution thread handling
    this request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[67db1edfd85f42d21368a69936519fd1-1368a69936519fd1]` : **Correlation ID**
    – Combined `traceId-spanId` for easy request tracking'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c.p.o.a.o.rest.ProductsQueryUseCaseImpl` : **Logger name** – The class that
    generated this log (shortened)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Getting product with id AA00001` : **Log message** – Simple text description
    of the operation being performed, showing the product ID being requested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is how Micrometer Tracing enables the logs in the application to make it
    possible to track requests between applications.
  prefs: []
  type: TYPE_NORMAL
- en: But if you want to have even more details in your logs, you will get them from
    the `LogSpanHandler` that we configured along with the beans in the configuration
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at a specific log generated by the `LogSpanHandler` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This specific log is the one generated by the `LogSpanHandler` configuration
    that adds all the information from the trace context.
  prefs: []
  type: TYPE_NORMAL
- en: The beginning of this log follows the same structure defined in the logging
    pattern shown previously, but what differs here is the information contained in
    its body, which is in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us understand each of the elements inside of this JSON body generated by
    the `LogSpanHandler` and how they can help us in the observability of our applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '`traceId` : **Distributed trace identifier** – Links all spans across services
    for this request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parentId` : **Parent span ID** – This field identifies the parent span from
    which the current span originated. If the current service or operation was triggered
    by another service, the parent span ID helps trace the hierarchy of calls between
    services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id` : **Span ID** – Unique identifier for this specific operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind` : **Span type** – `"CLIENT"` means the outbound request to another service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` : **Operation name** – Describes what action was performed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp` : **Start time** – When this operation began (in microseconds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration` : **Execution time** – How long the operation took (6.246ms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`localEndpoint` : **Service information** – Details about the originating service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` : Contextual metadata containing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application name** – Service identifier'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Client name** – Target server hostname'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exception** – Error status (none means successful)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP URL** – Full URL that was called'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method** – HTTP verb used ( `GET` )'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outcome** – Result category ( `SUCCESS` )'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Status** – HTTP response code ( `200 = OK` )'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URI** – Request path pattern with path variables'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you noticed, we talked about `parentSpanId` in this example. Let us understand
    better how this relates to `spanId` and how this can be useful in monitoring the
    performance of distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding parentSpanId and spanId
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In distributed tracing, every request generates a trace, which consists of multiple
    spans. A span represents a single unit of work, such as a service call, database
    query, or a specific business process within a service. Each span includes a unique
    identifier, called the `spanId` , and a `parentSpanId` that links it to the span
    from which it originated. This parent-child relationship helps to visualize how
    requests pro pagate through different services in a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: In sum, a `spanId` is a unique identifier for the current operation or service,
    and `parentSpanId` is the `spanId` of the calling operation or service. This field
    links spans together, showing which service called another.
  prefs: []
  type: TYPE_NORMAL
- en: Using these IDs, tracing tools like Zipkin or Jaeger can display a complete
    timeline of the trace, revealing the structure and timing of each request across
    services.
  prefs: []
  type: TYPE_NORMAL
- en: To visualize the relationship between `spanId` and `parentSpanId` , let’s walk
    through an example trace for a user registration request in an e-commerce application,
    where each service involved in the trace has its own `spanId` and, if applicable,
    a `parentSpanId` .
  prefs: []
  type: TYPE_NORMAL
- en: An API gateway receives the initial request and generates a `traceId` and `spanId`
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `User` service handles the user registration, with a span linked to the
    API gateway’s `spanId` as its `parentSpanId` .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Notification` service sends a welcome email to the user. This service’s span
    has the `spanId` of the `User` service as its `parentSpanId` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Below is a simplified visualization of what this trace might look like in a
    tool like Jaeger:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Span Name** | **Service** | **spanId** | **parentSpanId** | **Start Time**
    | **Duration** |'
  prefs: []
  type: TYPE_TB
- en: '| `register_user` | API Gateway | `span1` | - | 0ms | 15ms |'
  prefs: []
  type: TYPE_TB
- en: '| `create_user_record` | User | `span2` | `span1` | 5ms | 40ms |'
  prefs: []
  type: TYPE_TB
- en: '| `send_email` | Notification | `span3` | `span2` | 25ms | 30ms |'
  prefs: []
  type: TYPE_TB
- en: 'In this visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: The `register_user` span from the API gateway is the root of the trace. It has
    no `parentSpanId` because it starts the trace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `User Service` span ( `create_user_record` ) is a child of the `API Gateway`
    span, so it references `span1` as its `parentSpanId` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Notification` service span ( `send_email` ) is a child of the `User Service`
    span and has `span2` as its `parentSpanId` , indicating it was triggered by the
    user creation process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tracing tool displays the parent-child hierarchy as a timeline to visualize
    request propagation. Below is a diagram that matches this hierarchy, showing how
    each service relates in time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In a trace visualization tool:'
  prefs: []
  type: TYPE_NORMAL
- en: The `API Gateway` span ( `span1` ) initiates the request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`User Service` ( `span2` ) begins shortly after `API Gateway` , and it takes
    more time as it performs operations like database insertion, for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Notification Service` ( `span3` ) starts after `User Service` completes the
    user creation. The duration of each span indicates how long each operation took.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parent-child hierarchy insights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This trace hierarchy is useful for:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying bottlenecks** : If `User Service` took unusually long, it would
    appear as a longer bar in the timeline, prompting an investiga tion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracing errors** : If an error occurred in `Notification Service` , you could
    see it in the trace and quickly trace it back to the originating request from
    `User Service` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding dependencies** : By looking at the parent-child structure,
    you can see how each service depends on others and the sequence of operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This visualization and the `spanId` and `parentSpanId` relationship allow software
    engineers, architects, and system analysts to understand the flow and timeline
    of each request across multiple services, helping optimize performance, troubleshoot
    issues, and gain insights into the system’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Logs across multiple services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a request flows through multiple services in a distributed system, each
    service logs its part of the request independently. By correlating these logs
    through unique identifiers like `traceId` and `spanId` , we can connect individual
    logs across services to form a complete picture of the request’s journey. This
    end-to-end visibility is crucial for understanding how services interact, identifying
    bottlenecks, and troubleshooting errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, a User Registration request passes through three services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API Gateway** : Receives the initial request and routes it to the appropriate
    backend service.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**User Service** : Processes the registration by creating a user record in
    the database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Notification Service** : Sends a welcome email to the user upon successful
    registration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each service logs its part of the request using the same `traceId` to correlate
    logs. The `spanId` is unique within each service, while the `parentSpanId` links
    it back to the calling service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram below shows how the request moves through each service, with corresponding
    logs identified by numbers that correlate to the example logs below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Each log entry shown below is marked with a number corresponding to the steps
    in the diagram above. By following `traceId` , `spanId` , and `parentSpanId` ,
    we can see how each service is connected within the trace, enabling us to reconstruct
    the request’s journey.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us see examples of logs on each service this trace is going through.
  prefs: []
  type: TYPE_NORMAL
- en: API Gateway log
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the API gateway receives the request, it generates a new `traceId` (A)
    and its own `spanId` (1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: User Service log
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The User Service processes the registration, using the `traceId (A)` to connect
    it to the original request. The `User Service` log entry has a unique `spanId
    (2)` and references the API Gateway’s `spanId (1)` as its `parentSpanId` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notification Service log
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the User Service completes the user creation, it triggers the Notification
    Service to send a welcome email. The `Notification Service` log entry includes
    the `traceId` (A) to maintain continuity, generates its own `spanId` (3), and
    uses the User Service’s `spanId` (2) as its `parentSpanId` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the `traceId` (A) remains the same across all services, linking
    the logs together to represent the entire request flow. Each log’s `spanId` and
    `parentSpanId` establish a parent-child relationship, showing how each service
    is connected in the sequence. Here’s how these logs work together:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log 1 (API Gateway)** : Initiates the request with `traceId` (A) and `spanId`
    (1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log 2 (User Service)** : Continues the request, referencing the API Gateway’s
    `spanId` (1) with its `parentSpanId` and creating a new `spanId` (2) for itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log 3 (Notification Service)** : Completes the flow by linking back to the
    User Service’s `spanId` (2) and creating its own `spanId` (3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the combination of `traceId` , `spanId` , and `parentSpanId` , we can
    follow the lifecycle of the user registration request as it moves from service
    to service, providing a clear and structured view of the request’s journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logs are useful in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**End-to-end traceability** : By searching for logs with the same `traceId`
    , you can trace a request across different services (API Gateway, User Service,
    and Notification Service) and see how the request was handled at each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service dependencies** : The `parentSpanId` helps you understand how services
    are connected. In the example above, the Notification service was called by the
    User service, which was triggered by the API Gateway. The logs show the hierarchy
    of calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance insights** : Comparing the timestamps across spans can give you
    insights into performance bottlenecks. For instance, you can measure how much
    time each service took to handle the request by comparing the timestamps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micrometer Tracing enriches your logs with essential trace and span data that
    allows you to track requests across distributed systems. This traceability simplifies
    troubleshooting and helps you visualize the flow of requests, making it easier
    to detect performance issues or service failures. By integrating this data with
    tools like Zipkin or Jaeger, you can also visualize traces in real time, further
    enhancing your observability strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing traces with Zipkin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to run a local Zipkin instance on your machine, create the following
    `docker-compose.yml` file, which will automatically download and configure your
    local Zipkin instance with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This file defines from where we are getting the Docker image and the version,
    that is the latest available. Also, we will run this at memory, using the default
    port `9411` .
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this container, you need Docker installed on your system. Once it is
    installed, open a console in the same directory where this file is saved and run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Once the application is running, go to your local running Zipkin’s UI ( `http://localhost:9411`
    ) to view the traces. You should see a graphical representation of the trace,
    showing the `traceId` , `spanId` , and the parent-child relationships across services.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Complete trace journey in Zipkin dashboard with Micrometer](img/B21843_09_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Complete trace journey in Zipkin dashboard with Micrometer
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom spans
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the automatic tracing of HTTP requests, you may want to create
    custom spans to trace specific operations within your services. For example, you
    can trace important business logic or database queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create custom spans, inject the `Tracer` into your service and use it to
    manually create and manage spans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use `Tracer` to create a custom span called `createUser`
    , which tracks the execution of the user creation process. The span is manually
    started with `start()` and completed with `end()` . We also ensure that any exceptions
    are captured in the span by calling `newUserSpan.error(e)` .
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us understand how we can extract metrics from the tracing data and
    how this can help us monitor the whole application environment behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics from tracing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics for RESTful web services are essential for evaluating and optimizing
    the performance of these services. These metrics provide insights into how efficiently
    APIs handle requests, process data, and deliver responses.
  prefs: []
  type: TYPE_NORMAL
- en: This helps in ensuring that RESTful web services operate smoothly, providing
    a better user experience and meeting business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: With Micrometer Tracing in place, you can extract meaningful metrics from your
    trace data. Metrics give you quantitative insights into your API’s performance
    and health.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics like latency, throughput, and error rates are key to understanding how
    your REST API performs under load. These metrics help detect slow services, overloaded
    endpoints, or frequent errors that need to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Types of metrics to monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some common metrics to track in REST APIs include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Request duration** : How long does it take for the API to respond?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Request count** : The number of requests served over a period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Success/failure rate** : Percentage of successful vs. failed requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP error codes** : Count of `4xx` and `5xx` responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can configure Micrometer to track these metrics automatically. For example,
    to track request duration, add the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '**Metrics examples** :'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency** : Average time it takes for a request to complete (e.g., 200ms).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput** : The system processes an average of 150 requests per second
    during peak hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error Rate** : Percentage of requests that fail (e.g., 5% of requests return
    a 500 error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics can help identify performance bottlenecks. For instance, if one of your
    API endpoints consistently has a higher latency than others, it might indicate
    a need to optimize database queries, improve caching, or refactor code.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the `/api/v1/users` endpoint shows an average response time
    of 500ms, but other endpoints respond in under 100ms, you can use tracing data
    to find out where the delay occurs (e.g., in a database query or a third-party
    API call).
  prefs: []
  type: TYPE_NORMAL
- en: Viewing metrics with Micrometer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Micrometer integrates with Prometheus and Grafana to visualize your metrics
    in real-time dashboards. This allows you to create custom views and alerts based
    on the performance of your API.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in Grafana, you can create a dashboard that visualizes the latency
    of your API endpoints over time using time series graphs. These graphs help you
    spot trends and optimize performance by highlighting periods of increased latency,
    which might indicate bottlenecks or resource constraints. For instance, you can
    use a line chart to display how the average response time of a specific endpoint
    changes over time, making it easier to identify patterns or anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Grafana supports a variety of other visualizations that can be
    used to display metrics such as request counts, success/failure rates, and HTTP
    error codes. For example, you can use bar charts to compare the number of successful
    versus failed requests over a given period, or pie charts to show the distribution
    of different HTTP status codes returned by your API.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Metrics displayed in a Grafana dashboard](img/B21843_09_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Metrics displayed in a Grafana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have mastered Micrometer, let us look at another option for monitoring
    and observability, which is also open source and widely used in the market, OpenTelemetry.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTelemetry for monitoring and observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenTelemetry is an open-source, vendor-neutral observability framework that
    provides tools to collect telemetry data (logs, metrics, and traces) from your
    applications. It is a comprehensive standard designed to provide deep insights
    into distributed systems and is widely supported across languages and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTelemetry unifies logs, metrics, and traces into one framework, providing
    a standardized way to instrument your services. It works with a variety of backends
    (like Prometheus, Jaeger, Zipkin, and Grafana) and supports distributed tracing
    across microservices.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTelemetry helps you track the complete lifecycle of a request as it moves
    through multiple services, providing valuable insights into service performance,
    latency, and bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenTelemetry consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traces** : Monitor the journey of requests across multiple services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics** : Collect quantitative data on service performance, such as response
    times and error rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logs** : Record discrete events within the system, such as errors or warnings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using OpenTelemetry in Spring Boot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenTelemetry provides a standardized way to collect, process, and export telemetry
    data (logs, metrics, and traces) from your application. In a Spring Boot application,
    OpenTelemetry can be integrated to automatically capture tracing data across your
    services. Once integrated, this data can be exported to observability tools like
    Jaeger, Zipkin, or Grafana to visualize and monitor the flow of requests in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll go through the steps to set up OpenTelemetry in a Spring
    Boot application, validate that tracing is working, and review sample logs showing
    the output after implementing OpenTelemetry.
  prefs: []
  type: TYPE_NORMAL
- en: '**Add OpenTelemetry dependencies** : To integrate OpenTelemetry with Spring
    Boot, you’ll need the OpenTelemetry SDK along with specific instrumentation dependencies
    for Spring and HTTP clients. Add the following dependencies to your `pom.xml`
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under the dependency management tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, proceed to add the following dependencies under the dependencies
    tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us understand what each of these dependencies are used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '`opentelemetry-instrumentation-bom` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Bill of Materials (BOM) that ensures version alignment across all OpenTelemetry
    dependencies.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps manage compatible versions between OpenTelemetry components and their
    transitive dependencies.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be imported before other BOMs (like spring-boot-dependencies) when using
    Maven like we are doing here.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`micrometer-tracing-bridge-otel` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridges Micrometer’s Observation API to OpenTelemetry’s tracing system.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitates the propagation of trace context and spans between Micrometer and
    OpenTelemetry.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Essential component for enabling distributed tracing with OpenTelemetry in Spring
    Boot applications.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opentelemetry-spring-boot-starter` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides auto-configuration for OpenTelemetry in Spring Boot applications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Includes built-in instrumentation for many Spring Boot features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplifies the process of instrumenting a Spring Boot application with minimal
    configuration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Particularly useful for Spring Boot Native image applications or when seeking
    reduced startup overhead compared to the Java agent approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opentelemetry-exporter-otlp` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implements the OpenTelemetry Protocol (OTLP) exporter for sending telemetry
    data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows applications to export collected tracing data to OpenTelemetry Collectors
    or other backends
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports standardized telemetry data delivery between observability tools
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be configured to use either HTTP or gRPC transport protocols
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spring-boot-starter-actuator` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adds production-grade monitoring and management features to Spring Boot applications.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides dependency management and auto-configuration for Micrometer (metrics
    and tracing)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Required foundation for both metrics and tracing capabilities in Spring Boot.
    This is mandatory for any tracing, either only with Micrometer or along with OpenTelemetry
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposes endpoints for application health, metrics, and other operational data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuring OpenTelemetry into the application** : Let us have the following
    properties in the `application.yml` .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we are going to define the URL where OpenTelemetry will send the traces,
    and in this case, this is the path and exposed port from Jaeger that we are going
    to run from the `docker-compose` file that you will define next.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Under the `otel` tag, we are setting OpenTelemetry to not export logs or metrics,
    only traces. It will also work without these configurations but will throw multiple
    exceptions in the application log because Jaeger only reads traces, not logs or
    metrics.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are going to use another backend tool like Grafana that consumes the
    logs and the metrics, instead of setting it to none, you should add the proper
    configuration for the backend that you are going to use.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since OpenTelemetry is compatible with a wide variety of backends, you should
    refer to the documentation to see how to configure it for the logs and metrics
    backend that you will be using.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Next, under the management tag, you have the same configuration as shown in
    the Micrometer section. The configuration needs to contain tracing enabled and
    the sampling probability at 1.0 in order to generate as many traces as possible
    for our testing purposes. In production environments, you should configure that
    with a smaller value to avoid unneeded tracing.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Finally, be aware that the logging pattern with OpenTelemetry changes. Using
    pure Micrometer with Brave, as shown in the section Implementing tracing using
    Micrometer, registers the trace and the span as `traceId` and `spanId` in the
    MDC (Mapped Diagnostic Context), that is, from where the pattern gets its values.
    But OpenTelemetry registers them as `trace_id` and `span_id` . This is a slight
    change but if you do not take this into consideration, you will not see the tracing
    in your application logs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**Configuring OpenTelemetry exporter** : Create an OpenTelemetry configuration
    class in your Spring Boot application to export the traces to the defined tracing
    URL. This setup is generally handled automatically when using `opentelemetry-spring-boot-starter`
    , but you can add further customization to initialize it as a bean in Spring,
    like we are doing here.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will export the traces directly to be collected by the backend, which
    in our example will be Jaeger this time. To achieve that, we are going to use
    the OtlpHttpSpanExporter class from the OpenTelemetry library imported earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: With this configuration, the `OtlpHttpSpanExporter` will export the traces directly
    into the defined tracing URL from the `applications.yml` , so Jaeger can read
    our traces directly. By default, OpenTelemetry will automatically instrument HTTP
    and Spring MVC requests.
  prefs: []
  type: TYPE_NORMAL
- en: '**Validate tracing in the application** : To validate that OpenTelemetry tracing
    is working, we can:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check traces in the exported tool: Start your Spring Boot application and use
    Jaeger (or the chosen backend) to view the traces. Each incoming request should
    appear in the tracing tool as a new trace with a unique `traceId` .'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Observe logs with trace and span IDs: After setting up OpenTelemetry, logs
    should contain `traceId` and `spanId` , allowing you to correlate log entries
    across services.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Verify logs and trace visualization** : After implementing OpenTelemetry,
    you should see enhanced logs with traceId and spanId for each request. Additionally,
    the tracing backend (Zipkin, in this case) will provide a visual representation
    of the trace.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logs with OpenTelemetry Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below is an example log output from the `order-management-api` calling the
    `product-api` after implementing OpenTelemetry. Notice the `traceId` and `spanId`
    added to each log entry, following the defined logging pattern into the `application.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: It follows the same structure described in the section Viewing trace data, but
    here these traces and spans are being generated by OpenTelemetry.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the logs on the `product-api` side, which receives the `traceId`
    from the `order-management-api` and generates its own `spanId` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With these IDs, each log entry can be correlated to a specific request and its
    journey across multiple services.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn how to run a local Jaeger instance to see the whole tracing
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization example with Jaeger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to run a local Jaeger instance on your machine, create the following
    `docker-compose.yml` file, which will automatically download and configure your
    local Jaeger instance with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This file defines to get the latest Docker image for Jaeger. Also, here we are
    allocating port `4318` for tracing and `16686` for the Jaeger UI interface.
  prefs: []
  type: TYPE_NORMAL
- en: The property `COLLECTOR_OTLP_ENABLED=true` is optional on Jaeger v2 since its
    default is always true and mandatory on Jaeger v1. At the time of this writing,
    you should get above v2 while running this `docker-compose.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this container, you need Docker installed on your system. Once it is
    installed, open a console in the same directory as where this file is saved and
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Once the application is running, go to your local running Jaeger’s UI ( `http://localhost:16686`
    ) to view the traces. You should see a graphical representation of the trace,
    showing the `traceId` , `spanId` , and the parent-child relationships across services,
    and even showing deeper details like database `INSERT` and durations, giving a
    broad view of the trace.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Complete trace journey in Jaeger dashboard with OpenTelemetry](img/B21843_09_3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Complete trace journey in Jaeger dashboard with OpenTelemetry
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of any error, it will highlight what happened and where it happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Successful and error traces in Jaeger dashboard](img/B21843_09_4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Successful and error traces in Jaeger dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to refer to this chapter repository to get the working code version
    and replicate the same behavior on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: By examining both the logs in the application and the trace visualization in
    Jaeger, you can validate that OpenTelemetry is successfully capturing traces,
    correlating logs with tracing data, and providing a full view of the request’s
    journey through your distributed system. This setup not only helps troubleshoot
    issues but also provides insights into optimizing performance across services
    with the goal of monitoring distributed services.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a look at how to create custom spans with OpenTelemetry.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom spans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While OpenTelemetry automatically instruments HTTP requests, you can also create
    custom spans to monitor specific operations. Here’s an example in which a span
    is manually created for a user registration process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In this example, a custom span named “ `UserService.registerUser` " is created,
    and a username attribute is added. The span is started with `.startSpan()` and
    ended with `.end()` .
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have mastered the usage of OpenTelemetry as well, let us focus
    on some of the best practices for observability that you should be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for end-to-end observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability is an ongoing process that involves logs, metrics, and traces
    working together to provide full visibility into your REST API’s performance and
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing your environment to be completely covered by observability and monitoring
    is not an easy task, and you must consider the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining logs, metrics, and traces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logs, metrics, and traces complement each other to give you a holistic view
    of your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logs** : Provide detailed information about specific events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics** : Offer a quantitative summary of performance over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traces** : Show the lifecycle of individual requests across services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using these tools together, you can quickly diagnose issues and optimize
    performance. For example, you might use tracing to find a slow request, logs to
    determine why it’s slow, and metrics to track how often the issue occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Alarms and notifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With observability tools in place, you can set up alarms to notify your team
    when something goes wrong. For example, you can configure Prometheus to send alerts
    when the error rate exceeds a certain threshold or when response times spike.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of RESTful microservices, AWS CloudWatch offers comprehensive
    monitoring capabilities that transform raw operational data into readable, near-real-time
    metrics stored for up to 15 months.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when implementing API Gateway as the front door to your microservices
    architecture, CloudWatch can monitor key performance indicators such as IntegrationLatency
    to measure backend responsiveness, overall latency to assess API call efficiency,
    and cache performance metrics to optimize resource utilization. API Gateway logging,
    which feeds into CloudWatch Logs, provides valuable visibility into consumer access
    behaviors, allowing teams to understand common customer locations, analyze request
    patterns that might impact database partitioning, identify abnormal behavior that
    could indicate security concerns, and optimize configurations by tracking errors,
    latency, and cache performance. This monitoring framework creates a secure, easily
    maintainable environment that scales with growing business needs while providing
    actionable intelligence to continuously improve service delivery.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Observability is an ongoing process. As your system evolves, regularly review
    and refine your logging, tracing, and metrics collection to ensure you are capturing
    the most useful data. Use tools like Prometheus, Grafana, Zipkin, and OpenTelemetry
    to continually monitor and improve your system’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the core components and best practices for achieving
    effective monitoring and observability in RESTful services. Beginning with logging,
    we discussed the importance of structured logging for API troubleshooting, log
    levels to indicate severity, and correlation IDs to link requests across services.
    By implementing these logging practices centrally, such as with a filter in Spring
    Boot, we ensured consistent and secure logging across the application.
  prefs: []
  type: TYPE_NORMAL
- en: We then introduced distributed tracing, explaining how `traceId` , `spanId`
    , and `parentSpanId` create a parent-child relationship among services, allowing
    developers to track the journey of requests through a system.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer Tracing was covered as a key tool in Spring Boot 3.x for enabling
    and managing distributed tracing. It automatically instruments Spring Boot applications,
    capturing trace and span information for each request.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer Tracing integrates with multiple exporters, including Prometheus,
    Zipkin, and Jaeger, to send trace data to external observability platforms. With
    its configurable sampling and tagging, Micrometer Tracing provides granular visibility
    into each service, enabling efficient troubleshooting and performance optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Building on tracing, we explored OpenTelemetry as a vendor-neutral observability
    framework that collects and correlates traces, metrics, and logs in distributed
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTelemetry integrates smoothly with Spring Boot to provide out-of-the-box
    tracing for HTTP and Spring MVC requests, with added flexibility to create custom
    spans. We covered how to configure OpenTelemetry, validate its functionality through
    logging and visualization in tools like Zipkin, and observe end-to-end traces
    across services.
  prefs: []
  type: TYPE_NORMAL
- en: By combining logging, tracing, and OpenTelemetry with tools like Zipkin or Jaeger
    for visualization, we can gain a comprehensive view of each request across services.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provided foundational strategies for implementing robust observability,
    allowing for effective monitoring, faster troubleshooting, and insights to optimize
    the performance of RESTful APIs in complex, distributed environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about scaling and performance optimization
    techniques, to be able to make the best out of your applications.
  prefs: []
  type: TYPE_NORMAL
