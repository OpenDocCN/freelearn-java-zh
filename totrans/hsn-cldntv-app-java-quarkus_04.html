<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating a Container Image of Your Application</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we had a glimpse of the power of Quarkus applications by running a traditional JVM application and then turning it into a native build. There is much more to Quarkus than lean executables and low resource usage, though, so, in this chapter, we will keep learning how to create container images of our application that can then be deployed into a Kubernetes-native environment. For this purpose, our to-do list includes installing the Docker tool and the Community version of OpenShift, which is called <strong>Origin Community Distribution of Kubernetes</strong>, or simply <strong>OKD</strong>. Then, we will learn how to scale our application so that we can improve its response time even further.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li class="mce-root">Setting up Docker in your environment</li>
<li class="mce-root">Starting a Quarkus application in a container</li>
<li class="mce-root">Running a native executable in a container</li>
<li class="mce-root">Deploying your container image on OpenShift</li>
<li>Scaling our application to improve its throughput</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can find the source code for the project in this chapter on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter03">https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter03</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up Docker</h1>
                </header>
            
            <article>
                
<p>Docker is a tool that lets us simplify the creation and execution of containers in our environment. Each container, in turn, wraps up an application and its dependencies into a single standardized unit that includes everything it needs to run, that is, the system tools, code, and other required libraries. This guarantees that your application will always execute in the same way by sharing a simple container image. Docker is available in two versions:</p>
<ul>
<li><strong>Community Edition</strong> (<strong>CE</strong>): The Docker CE, which we will be using in this book, is ideal for developers and small teams looking for a quick start with Docker and container-based applications.</li>
<li><strong>Enterprise Edition</strong> (<strong>EE</strong>): The EE features additional capabilities such as a certified infrastructure, image management, and image security scanning.</li>
</ul>
<p>Although we will be using the Community version of Docker, this isn't going to reduce your application's full potential as we will be able to leverage advanced container capabilities through a native Kubernetes platform, which is an ideal solution for running business-critical applications in production at scale.</p>
<p>The installation of Docker is fully documented at <a href="https://docs.docker.com/install/">https://docs.docker.com/install/</a>. In a nutshell, you can follow several installation tactics, depending on your needs:</p>
<ul>
<li>From a midterm perspective, you may want to ease the upgrade of Docker. Most users choose to set up Docker's repositories and install and upgrade from there (<a href="https://docs.docker.com/install/linux/docker-ce/fedora/#install-using-the-repository">https://docs.docker.com/install/linux/docker-ce/fedora/#install-using-the-repository</a>).</li>
<li>Another option, which turns out to be pretty useful if you are installing Docker on a machine that is offline, requires manually installing the RPM package and manually handling upgrades as well (<a href="https://docs.docker.com/install/linux/docker-ce/fedora/#install-from-a-package">https://docs.docker.com/install/linux/docker-ce/fedora/#install-from-a-package</a>).</li>
<li>Finally, for quick and easy installation, you can use the automated script, which will detect your operating system and install Docker accordingly. For the sake of simplicity, we will choose this option.</li>
</ul>
<p>Let's proceed with installing Docker by following these steps:</p>
<ol>
<li>The automated script can be downloaded from <a href="https://get.docker.com/">https://get.docker.com/</a>, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ curl -fsSL https://get.docker.com -o get-docker.sh</strong></pre>
<ol start="2">
<li>Now, execute it with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sh get-docker.sh</strong></pre>
<div class="packt_tip">Important! Just like any other shell script, verify its content before executing it! Its content needs to match with the <kbd>install.sh</kbd> script located at <a href="https://github.com/docker/docker-install">https://github.com/docker/docker-install</a>. If the content doesn't match, verify whether the automated script is still being maintained by going to the Docker install page.</div>
<ol start="3">
<li>If you would like to run Docker as a non-privileged user, you should consider adding your user to the <kbd>docker</kbd> group by executing the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo usermod $(whoami) -G docker -a</strong></pre>
<ol start="4">
<li>For this to take effect, you will need to log out and log in again. We can check that our user is now in the Docker group by checking the output of the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ groups $(whoami)</strong></pre>
<ol start="5">
<li>The output should include <kbd>docker</kbd> in the list of groups. Now, you can verify that you can run Docker commands without a root user (or <kbd>sudo</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker run hello-world</strong></pre>
<ol start="6">
<li>The preceding command will pull the <kbd>hello-world</kbd> test image from the Docker repository, and run it in a container. When the test image starts, it prints an informative message and exits:</li>
</ol>
<pre style="padding-left: 60px"><strong>Status: Downloaded newer image for hello-world:latest</strong><br/><strong>Hello from Docker!</strong></pre>
<p>This message shows that your installation appears to be working correctly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Quarkus applications in a container</h1>
                </header>
            
            <article>
                
<p>Once you have installed Docker, you are just ready to build a Docker image out of your Java or native executable application. For this purpose, we will quickly build another simple application that inspects some environment variables to determine the container ID where the application is running.</p>
<p>The source code for this chapter is located in the <kbd>Chapter03/hello-okd</kbd> folder of this book's GitHub repository. We recommend importing the project into your IDE before you continue.</p>
<p>Let's dive into the code by starting with the REST endpoint class (<kbd>HelloOKD</kbd>), which returns some information from a <strong>Contexts and Dependency Injection</strong> (<strong>CDI</strong>) service:</p>
<pre> package com.packt.quarkus.chapter3;<br/> <br/> import javax.ws.rs.GET;<br/> import javax.ws.rs.Path;<br/> import javax.ws.rs.Produces;<br/> import javax.ws.rs.core.MediaType;<br/> import javax.inject.Inject;<br/> <br/> @Path("/getContainerId")<br/> public class HelloOKD {<br/>     <br/>     @Inject<br/>     ContainerService containerService;<br/> <br/>     @GET<br/>     @Produces(MediaType.TEXT_PLAIN)<br/>     public String hello() {<br/>         return "You are running on "  + <br/>           containerService.getContainerId();<br/>     }<br/> } </pre>
<p>The following code is for the <kbd>ContainerService</kbd> class, which is injected into the REST endpoint:</p>
<pre>package com.packt.quarkus.chapter3;<br/> <br/>import javax.enterprise.context.ApplicationScoped;<br/> <br/>@ApplicationScoped<br/>public class ContainerService {<br/> <br/>     public String getContainerId() {<br/>         return System.getenv().getOrDefault("HOSTNAME", "unknown");<br/>     }<br/>}<br/> </pre>
<p>This example shows the use of the CDI <kbd>@ApplicationScoped</kbd> annotation for injected objects. An object that is defined as <kbd>@ApplicationScoped</kbd> is created once for the duration of the lifetime of an application. In our case, it returns the <kbd>HOSTNAME</kbd> environment variable, which defaults to the Docker container ID.</p>
<p>In order to test our simple REST service, the following <kbd>HelloOKDTest</kbd> has been included in the project under the <kbd>src/test/java</kbd> path. Through its <kbd>testHelloEndpoint</kbd> method, we verify that the status code of the REST call was a success:</p>
<pre>package com.packt.quarkus.chapter3;<br/> <br/>import io.quarkus.test.junit.QuarkusTest;<br/>import org.junit.jupiter.api.Test;<br/> <br/>import static io.restassured.RestAssured.given;<br/>import static org.hamcrest.CoreMatchers.is;<br/> <br/>@QuarkusTest<br/>public class HelloOKDTest {<br/> <br/>    @Test<br/>    public void testHelloEndpoint() {<br/>        given()<br/>          .when().get("/getContainerId")<br/>          .then()<br/>             .statusCode(200);<br/>   }<br/> <br/>} </pre>
<p>Before we set off on our journey into Docker, let's check that the preceding test passes. The test phase will automatically kick in as we run the <kbd>install</kbd> goal of our project:</p>
<pre><strong>$ mvn install</strong></pre>
<p>A successful test should produce the following log:</p>
<pre><strong>[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.174 s - in com.packt.quarkus.chapter3.HelloOKDTest</strong><br/><strong>2019-11-17 19:15:16,227 INFO  [io.quarkus] (main) Quarkus stopped in 0.041s</strong><br/><strong>[INFO] </strong><br/><strong>[INFO] Results:</strong><br/><strong>[INFO] </strong><br/><strong>[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0</strong></pre>
<p>Now, let's move on to looking at Docker. If you take a look at the <kbd>src/main/docker</kbd> folder, you will notice that some of the files have been automatically added to your project:</p>
<pre><strong>$ tree src/main/docker</strong><br/><strong> src/main/docker</strong><br/><strong> ├── Dockerfile.jvm</strong><br/><strong> └── Dockerfile.native</strong></pre>
<p>The first file in the list, <kbd>Dockerfile.jvm</kbd>, is a Dockerfile that's been specifically written for a JVM environment. Its content is as follows:</p>
<pre>FROM fabric8/java-alpine-openjdk8-jre<br/>ENV JAVA_OPTIONS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager"<br/>ENV AB_ENABLED=jmx_exporter<br/>COPY target/lib/* /deployments/lib/<br/>COPY target/*-runner.jar /deployments/app.jar<br/>EXPOSE 8080<br/><br/># run with user 1001 and be prepared for be running in OpenShift too<br/>RUN adduser -G root --no-create-home --disabled-password 1001 \<br/>  &amp;&amp; chown -R 1001 /deployments \<br/>  &amp;&amp; chmod -R "g+rwX" /deployments \<br/>  &amp;&amp; chown -R 1001:root /deployments<br/>USER 1001<br/><br/>ENTRYPOINT [ "/deployments/run-java.sh" ]</pre>
<div class="packt_tip">A Dockerfile is a plain text file that contains a set of commands that we can use to assemble an image so that it can be executed by Docker. A Dockerfile needs to match with a specific format and set of instructions that have been documented in the Dockerfile reference (<a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>).</div>
<p>In our example, the Dockerfile contains instructions for building a Java environment using Fabric8 Java Base Image and enables the JMX exporter (<a href="https://github.com/prometheus/jmx_exporter">https://github.com/prometheus/jmx_exporter</a>) to expose process metrics. Now, we will build the image for our container, as follows:</p>
<pre><strong>$ docker build -f src/main/docker/Dockerfile.jvm -t quarkus/hello-okd .</strong></pre>
<p>In your console, you can verify that the Docker pull process will be triggered and that all the commands in the Dockerfile contribute to building the intermediate layers of your <kbd>quarkus/hello-okd</kbd> container image:</p>
<pre>Step 1/9 : FROM fabric8/java-alpine-openjdk8-jre<br/>Trying to pull repository docker.io/fabric8/java-alpine-openjdk8-jre ... <br/>sha256:b27090f384b30f0e3e29180438094011db1fa015bbf2e69decb921bc2486604f: Pulling from docker.io/fabric8/java-alpine-openjdk8-jre<br/>9d48c3bd43c5: Pull complete <br/>. . . . . . .<br/>Status: Downloaded newer image for docker.io/fabric8/java-alpine-openjdk8-jre:latest<br/> ---&gt; fe776eec30ad<br/>Step 2/9 : ENV JAVA_OPTIONS "-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager"<br/> ---&gt; Running in c5d31bae859e<br/> ---&gt; 01c99aac17db<br/>Removing intermediate container c5d31bae859e<br/>Step 3/9 : ENV AB_ENABLED jmx_exporter<br/> ---&gt; Running in c867300baaf0<br/> ---&gt; 52deadd505bc<br/>Removing intermediate container c867300baaf0<br/>Step 4/9 : COPY target/lib/* /deployments/lib/<br/> ---&gt; aa11b2b30f16<br/>Removing intermediate container dcbd13a3ae0f<br/>Step 5/9 : COPY target/*-runner.jar /deployments/app.jar<br/> ---&gt; 2f2e1218eff8<br/>Removing intermediate container 4b3861ba33d9<br/>Step 6/9 : EXPOSE 8080<br/> ---&gt; Running in 93eebaee5495<br/> ---&gt; 4008a4fdbb9c<br/>Removing intermediate container 93eebaee5495<br/>Step 7/9 : RUN adduser -G root --no-create-home --disabled-password 1001   &amp;&amp; chown -R 1001 /deployments   &amp;&amp; chmod -R "g+rwX" /deployments   &amp;&amp; chown -R 1001:root /deployments<br/> ---&gt; Running in 2a86b3aeaeae<br/> ---&gt; b21be209f09e<br/>Removing intermediate container 2a86b3aeaeae<br/>Step 8/9 : USER 1001<br/> ---&gt; Running in fac8d64b8793<br/> ---&gt; 94077bb5396a<br/>Removing intermediate container fac8d64b8793<br/>Step 9/9 : ENTRYPOINT /deployments/run-java.sh<br/> ---&gt; Running in 7bacd02dd631<br/> ---&gt; 9f269b2041d3<br/>Removing intermediate container 7bacd02dd631<br/>Successfully built 9f269b2041d3</pre>
<p>Now, let's check that the image is available in your local Docker repository by executing the <kbd>docker images</kbd> command:</p>
<pre><strong>$ docker images | grep hello-okd</strong></pre>
<p>You should see the following output:</p>
<pre><strong>quarkus/hello-okd                                        latest               9f269b2041d3        2 minutes ago       98.9 MB</strong></pre>
<p>As you can see, the locally cached image is now available in your local Docker repository. You can run it using the following command:</p>
<pre><strong>$ docker run -i --rm -p 8080:8080 quarkus/hello-okd</strong></pre>
<p>In the <kbd>run</kbd> command, we have included some additional flags, such as <kbd>--rm</kbd>, which removes the container automatically after it exits. The <kbd>-i</kbd> flag will connect the container to the Terminal. Finally, the <kbd>-p</kbd> flag exposes port <kbd>8080</kbd> externally, thus mapping to port <kbd>8080</kbd> on the host machine.</p>
<p>Since we will be exporting the service on the host port, that is, <kbd>8080</kbd>, check that no other service is engaging that port! You should be able to collect this output on the console, which is a log of the agent startup and, at the bottom, a log of our <kbd>hello-okd</kbd> service:</p>
<pre>exec java -Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -javaagent:/opt/agent-bond/agent-bond.jar=jmx_exporter{{9779:/opt/agent-bond/jmx_exporter_config.yml}} -XX:+UseParallelGC -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -XX:MinHeapFreeRatio=20 -XX:MaxHeapFreeRatio=40 -XX:+ExitOnOutOfMemoryError -cp . -jar /deployments/app.jar<br/>2019-11-11 10:29:12,505 INFO  [io.quarkus] (main) hello-okd 1.0-SNAPSHOT (running on Quarkus 1.0.0.Final) started in 0.666s. Listening on: http://0.0.0.0:8080<br/>2019-11-11 10:29:12,525 INFO  [io.quarkus] (main) Profile prod activated. <br/>2019-11-11 10:29:12,525 INFO  [io.quarkus] (main) Installed features: [cdi, resteasy]</pre>
<p>The Docker process is now running, which can be confirmed by the following command. This command will display the <kbd>Image</kbd> name for running containers:</p>
<pre><strong> $ docker ps --format '{{.Image}}'</strong></pre>
<p>The following is the output of running the preceding command:</p>
<pre><strong>quarkus/hello-okd</strong></pre>
<p>You can test that the application is running in the container with the following command:</p>
<pre><strong>$ curl http://localhost:8080/getContainerId</strong></pre>
<p>You should be able to see the same container ID that was printed by the <kbd>docker ps</kbd> command in the output:</p>
<pre><strong>You are running on a333f52881a1</strong></pre>
<p>Now, let's rebuild our container image so that we can use the native executable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the native executable process in a container</h1>
                </header>
            
            <article>
                
<p>As we have seen, the Quarkus Maven plugin has also produced <kbd>src/main/docker/Dockerfile.native</kbd>, which can be used as a template so that we can run our native executable in a container. Here's the content of this file:</p>
<pre>FROM registry.access.redhat.com/ubi8/ubi-minimal<br/>WORKDIR /work/<br/>COPY target/*-runner /work/application<br/>RUN chmod 775 /work<br/>EXPOSE 8080<br/>CMD ["./application", "-Dquarkus.http.host=0.0.0.0"]</pre>
<p>Since there's no need to use a JDK layer to start our application, the base layer for our container will be a stripped-down RHEL image known as <kbd>ubi-minimal</kbd>.</p>
<div class="packt_infobox"><span class="ILfuVd NA6bn"><span class="e24Kjd">Red Hat <strong>Universal Base Images</strong> (<strong>UBI</strong>) are OCI-compliant container OS images that include complimentary runtime languages and other packages that are freely redistributable.</span></span></div>
<p>Before building the Docker image, package your application by including the <kbd>-Dnative-image.docker-build</kbd> option:</p>
<pre><strong>$</strong><strong> mvn package -Pnative -Dnative-image.docker-build=true</strong> </pre>
<p>Check that the build was successful and then build the image with the following command:</p>
<pre><strong>$ docker build -f src/main/docker/Dockerfile.native -t quarkus/hello-okd-native .</strong></pre>
<p>From the console, you will see that the container will be created in much the same way that the Java application was, but using a different initial image (<kbd>ubi-minimal</kbd>):</p>
<pre>Sending build context to Docker daemon 32.57 MB<br/>Step 1/6 : FROM registry.access.redhat.com/ubi8/ubi-minimal<br/> ---&gt; 8c980b20fbaa<br/>Step 2/6 : WORKDIR /work/<br/> ---&gt; Using cache<br/> ---&gt; 0886c0b19e07<br/>Step 3/6 : COPY target/*-runner /work/application<br/> ---&gt; 7e66ae6447ce<br/>Removing intermediate container 2ddc91992af5<br/>Step 4/6 : RUN chmod 775 /work<br/> ---&gt; Running in e8d6ffbbc14e<br/> ---&gt; 780f6562417d<br/>Removing intermediate container e8d6ffbbc14e<br/>Step 5/6 : EXPOSE 8080<br/> ---&gt; Running in d0d48475565f<br/> ---&gt; 554f79b4cbb2<br/>Removing intermediate container d0d48475565f<br/>Step 6/6 : CMD ./application -Dquarkus.http.host=0.0.0.0<br/> ---&gt; Running in e0206ff3971f<br/> ---&gt; 33021bdaf4a4<br/>Removing intermediate container e0206ff3971f<br/>Successfully built 33021bdaf4a4</pre>
<p>Let's check that the image is available in the Docker repository:</p>
<pre><strong>$ docker images | grep hello-okd-native</strong></pre>
<p>You should see the following output:</p>
<pre><strong>quarkus/hello-okd-native                                        latest               33021bdaf4a4        59 seconds ago      113 MB</strong></pre>
<p>The <kbd>quarkus/hello-okd-native</kbd> image is now available. Now, run the container image using the following command:</p>
<pre><strong>$ docker run -i --rm -p 8080:8080 quarkus/hello-okd-native</strong></pre>
<p>No additional JVM layers will be displayed on the console. Here, we can see that our service was started up in just a few milliseconds:</p>
<pre>2019-11-11 11:59:46,817 INFO  [io.quarkus] (main) hello-okd 1.0-SNAPSHOT (running on Quarkus 1.0.0.CR1) started in 0.005s. Listening on: http://0.0.0.0:8080<br/>2019-11-11 11:59:46,817 INFO  [io.quarkus] (main) Profile prod activated. <br/>2019-11-11 11:59:46,817 INFO  [io.quarkus] (main) Installed features: [cdi, resteasy]y]</pre>
<p>Verify that the application returns the container ID when requesting the <kbd>getContainerId</kbd> URI:</p>
<pre><strong>curl http://localhost:8080/getContainerId</strong></pre>
<p>In our case, the output is as follows:</p>
<pre><strong>You are running on ff6574695d68</strong></pre>
<p>Great! We just managed to run a native application as a Docker image. Our next task will be deploying our image into a Kubernetes-native environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying Quarkus applications on a Kubernetes-native platform</h1>
                </header>
            
            <article>
                
<p>Now that we have verified how simple it is to run Quarkus applications in a container, we will deploy our application into a Kubernetes-native environment. Even if Kubernetes itself is sufficient to orchestrate your services, you can greatly extend its capabilities by installing OpenShift. Besides leveraging Kubernetes features, OpenShift also provides the following:</p>
<ul>
<li>Better management of container images through the use of <strong>image streams</strong>, which decouple the actual image from your application</li>
<li>Advanced CI/CD capabilities to make the whole CI/CD workflow a lot easier, also including a Jenkins certified image</li>
<li>A simpler build process as it's easier to build a docker image inside OpenShift through the <kbd>BuildConfig</kbd> component, which can perform automated image builds and push them to its internal registry</li>
<li>A wealth of certified plugins, such as storage/networking/monitoring plugins</li>
<li>Support for multitenancy through the <strong>Resource Scheduler</strong> component, which will determine where to run Pods</li>
<li>A large set of certified databases and middleware products</li>
<li>A simpler UI web application from where you can easily manage your cluster of services and create new applications</li>
</ul>
<p>OpenShift is available in several flavors:</p>
<ul>
<li><strong>Red Hat OpenShift Container Platform</strong> (requires a subscription): The supported Kubernetes platform that lets you build, deploy, and manage your container-based applications consistently across cloud and on-premises infrastructures.</li>
<li><strong>Red Hat OpenShift Dedicated</strong> (requires a subscription): This provides a supported, private, high-availability Red Hat OpenShift cluster hosted on Amazon Web Services or Google Cloud Platform.</li>
<li><strong>Red Hat OpenShift Online</strong> (several plans are available): It provides on-demand access to Red Hat OpenShift so that you can manage containerized applications.</li>
<li><strong>Origin Community Distribution of Kubernetes</strong> (<strong>OKD</strong>): This is the Community version of the Red Hat OpenShift Container Platform that you can freely use in any environment.</li>
</ul>
<p>For the purpose of this book, we will be installing <strong>Minishift</strong>, a simplified version of <strong>OKD</strong>, to launch a single-node cluster inside a virtual machine. This is the simplest approach to get started and try OpenShift on your local machine.</p>
<div class="packt_infobox">The current version of Minishift is based on the release 3.x of Openshift. It is highly recommended to move to an Openshift 4.x platform for most advanced examples, such as developing Cloud based reactive applications, which is discussed in the last chapter of this book.</div>
<p>The installation of Minishift is quite simple: all you'll need to do is download and unzip the latest distribution of it. Some prerequisites, however, do exist as you need to prepare your system by installing a hypervisor, which is required to start the virtual environment that OKD is provisioned on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Minishift</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to install Minishift on a machine running Fedora. If you don't run Fedora on your machine, you can check out the prerequisites for your OS at <a href="https://docs.okd.io/latest/minishift/getting-started/preparing-to-install.html">https://docs.okd.io/latest/minishift/getting-started/preparing-to-install.html</a>.</p>
<p>First, you will need to install two kernel modules (<kbd>libvirt</kbd> and <kbd>qemu<strong>-</strong>kvm</kbd>), which are needed to manage the various virtualization platforms. These are compliant with the <strong>Kernel-based Virtual Machine</strong> (<strong>KVM</strong>) technology. Follow these steps to do so:</p>
<ol>
<li>From the shell, execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo dnf install libvirt qemu-kvm</strong></pre>
<ol start="2">
<li>Then, to run the virtualization platform with your user, add it to the <kbd>libvirt</kbd> group:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo usermod -a -G libvirt $(whoami)</strong></pre>
<ol start="3">
<li>Next, configure the group membership with the user you are currently logged in as:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ newgrp libvirt</strong></pre>
<ol start="4">
<li>Finally, you will need to download and make the KVM driver for your Docker machine executable. As the root user, execute the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.10.0/docker-machine-driver-kvm-centos7 -o /usr/local/bin/docker-machine-driver-kvm</strong><br/><br/><strong>$ sudo chmod +x /usr/local/bin/docker-machine-driver-kvm</strong></pre>
<ol start="5">
<li>Once your user has been set up, download and unpack the latest Minishift release package from the official GitHub repository: <a href="https://github.com/minishift/minishift/releases">https://github.com/minishift/minishift/releases</a>. At the time of writing, this is the latest version of Minishift that can be downloaded:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget https://github.com/minishift/minishift/releases/download/v1.33.0/minishift-1.33.0-linux-amd64.tgz</strong> </pre>
<ol start="6">
<li>Once the download has completed, unpack the <kbd>.tar</kbd> file into a destination folder. For example, to unpack it into your home (<kbd>~</kbd>) directory, execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ tar xvf minishift-1.33.0-linux-amd64.tgz -C ~</strong></pre>
<p style="padding-left: 90px">Within this package, you will find the <kbd>minishift</kbd> executable, which can be used to start your Minishift environment.</p>
<ol start="7">
<li>Next, we will run the <kbd>minishift</kbd> command to start the installation process:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ ./minishift start</strong></pre>
<ol start="8">
<li>Once complete, you should see a message similar to the following in your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>-- Starting profile 'minishift'</strong><br/><strong>-- Check if deprecated options are used ... OK</strong><br/><strong>-- Checking if https://github.com is reachable ... OK</strong><br/><strong>-- Checking if requested OpenShift version 'v3.11.0' is valid ... OK</strong><br/><strong>-- Checking if requested OpenShift version 'v3.11.0' is supported ... OK</strong><br/><strong>-- Checking if requested hypervisor 'kvm' is supported on this platform ... OK</strong><br/><strong>-- Checking if KVM driver is installed ...</strong><br/><strong>. . . .</strong><br/><strong>OpenShift server started.</strong><br/><br/><strong>The server is accessible via web console at:</strong><br/><strong>https://192.168.42.103:8443/console</strong><br/><strong>The server is accessible at:</strong><br/><strong>https://192.168.42.190:8443</strong><br/><strong>You are logged in as: User: developer Password:</strong><br/><strong>To log in as administrator:</strong><br/><strong>oc login -u system:admin</strong></pre>
<p>And that's it! Minishift has been installed in your environment!</p>
<p>It's recommended that you include the following folders in the <kbd>$PATH</kbd> environment variable:</p>
<ul>
<li>The folder where you have unpacked the <kbd>minishift</kbd> tool.</li>
<li>The folder where the <kbd>oc</kbd> client tool is located. This tool is a command-line utility that you can use to manage your Minishift cluster. Once you start up the cluster, this tool is copied to <kbd>~/.minishift/cache/oc/&lt;oc-version&gt;/linux</kbd>.</li>
</ul>
<p>So, for example, if you unpacked Minishift in your home directory, replace <kbd>oc-version</kbd> with your tool version and execute the following command:</p>
<pre><strong>export PATH=$PATH:~/minishift-1.33.0-linux-amd64:~/.minishift/cache/oc/&lt;oc-version&gt;/linux</strong></pre>
<p>You can verify this by opening the OpenShift web console in your default browser (in our case, <kbd>https://192.168.42.190:8443</kbd>) or by passing the <kbd>console</kbd> argument to the <kbd>minishift</kbd> tool:</p>
<pre><strong>$ minishift console</strong></pre>
<p>Since the console runs on a secured connection, you will be warned that no signed certificates have been found in your browser. Add a security exception to your browser so that you land on the login page:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bd93639c-7c77-45e3-9837-1486b67607e1.png" style=""/></div>
<p class="CDPAlignLeft CDPAlign">Log in with <kbd>developer/developer</kbd> to enter the dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3cdc899c-2da8-45da-9f2b-1268cc2d91e2.png" style=""/></div>
<p>Congratulations! You have installed Minishift and verified it. The next step will be deploying our sample application on it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building and deploying a Quarkus application on OKD</h1>
                </header>
            
            <article>
                
<p>Minishift's dashboard contains a set of templates that can be used to build our applications quickly. At the time of writing, there's no Quarkus template; however, we can easily build and deploy our image as a <strong>binary build</strong> that conveys the Dockerfile that we have already tested.</p>
<div class="packt_infobox"><span>A <strong>binary build</strong> is a feature that allows developers to upload artifacts from a binary source instead of pulling the source from a Git repository URL.</span></div>
<p>For this purpose, we will be using the <kbd>oc</kbd> client tool, which is the Swiss Army knife that's used to configure OpenShift and its objects.</p>
<div class="packt_tip"><span>The following set of commands is contained in the <kbd>deploy-openshift.sh</kbd> file, which is located in the <kbd>Chapter03</kbd> directory of this book's GitHub repository. If you are impatient to see your application in the cloud, simply execute the script and check that the output matches what we've written in this paragraph. </span></div>
<p>The first thing we will need to do is create a namespace for our project, which will be created in our current OpenShift namespace. You can create the <kbd>quarkus-hello-okd</kbd> namespace with the following command:</p>
<pre><strong>$ oc new-project quarkus-hello-okd</strong></pre>
<p>The first thing we will need to do is define a binary build object using the <kbd>oc new-build</kbd> command:</p>
<pre><strong>$ oc new-build --binary --name=quarkus-hello-okd -l app=quarkus-hello-okd</strong></pre>
<p>The previous command will produce an image binary build that will be pushed into Minishift's internal registry. The following output describes the resources that were created for this purpose:</p>
<pre><strong>* A Docker build using binary input will be created</strong><br/><strong>* The resulting image will be pushed to image stream tag "quarkus-hello-okd:latest"</strong><br/><strong>* A binary build was created, use 'start-build --from-dir' to trigger a new build</strong><br/><br/><strong>--&gt; Creating resources with label app=quarkus-hello-okd ...</strong><br/><strong>    imagestream.image.openshift.io "quarkus-hello-okd" created</strong><br/><strong>    buildconfig.build.openshift.io "quarkus-hello-okd" created</strong><br/><strong>--&gt; Success</strong></pre>
<p>Now that the build configuration has been created, we can check its availability by querying the <kbd>bc</kbd> alias (which stands for build config):</p>
<pre><strong>$  oc get bc</strong></pre>
<p>You should see the following output:</p>
<pre><strong>NAME                TYPE      FROM      LATEST</strong><br/><strong>quarkus-hello-okd   Docker    Binary    0</strong></pre>
<p>As it is, the binary build doesn't contain any reference to our Dockerfile. We can add this information using the <kbd>oc patch</kbd> command, which is a useful shortcut that we can use to edit resources. In our case, we need to set the <kbd>dockerfilePath</kbd> attribute that refers to the <kbd>dockerStrategy</kbd> element to the location where our Dockerfile is. From the root of your Quarkus project, execute the following command:</p>
<pre><strong>$ oc patch bc/quarkus-hello-okd -p '{"spec":{"strategy":{"dockerStrategy":{"dockerfilePath":"src/main/docker/Dockerfile.native"}}}}'</strong></pre>
<p>The following output will be returned:</p>
<pre><strong>buildconfig.build.openshift.io/quarkus-hello-okd patched</strong></pre>
<p>If you check the binary build description, you will see that the Dockerfile path has been included:</p>
<pre><strong>$ oc describe bc/quarkus-hello-okd</strong></pre>
<p>The output is a bit verbose; however, it should contain the following information:</p>
<pre><strong>Strategy:           Docker</strong><br/><strong>Dockerfile Path:    src/main/docker/Dockerfile.native</strong></pre>
<p>Now, we are ready to start the build process, which will take the project's root folder (<kbd>.</kbd>) as input and will result in uploading <kbd>ImageStream</kbd> onto your Minishift environment. Execute the following command:</p>
<pre><strong>$ oc start-build quarkus-hello-okd --from-dir=. --follow</strong></pre>
<p>The output will notify you that the image has been built and pushed to the Minishift registry:</p>
<pre><strong>Uploading finished</strong><br/><strong>build.build.openshift.io/quarkus-hello-okd-1 started</strong><br/><strong>Receiving source from STDIN as archive ...</strong><br/><strong>Caching blobs under "/var/cache/blobs".</strong><br/><strong>Pulling image registry.access.redhat.com/ubi8/ubi-minimal ...</strong><br/><strong>. . . .</strong><br/><strong>Writing manifest to image destination</strong><br/><strong>Storing signatures</strong><br/><strong>STEP 1: FROM registry.access.redhat.com/ubi8/ubi-minimal</strong><br/><strong>STEP 2: WORKDIR /work/</strong><br/><strong>2d94c8983e7ec259aa0e0207c66b0e48fdd9544f66e3c17724a10f838aaa50ab</strong><br/><strong>STEP 3: COPY target/*-runner /work/application</strong><br/><strong>a6d9d0a228023d29203c20de7bcfd9019de00f62b4a4c3fdc544648f8f61988a</strong><br/><strong>STEP 4: RUN chmod 775 /work</strong><br/><strong>e7bf7467d2191478f0cdccd9b480d10ebd19eeae254a7fa0608646cba28c5a97</strong><br/><strong>STEP 5: EXPOSE 8080</strong><br/><strong>29e6198d99e27508aa75cd073290f66fbca605b8ff95cbb6287b4ecbfc1807e5</strong><br/><strong>STEP 6: CMD ["./application","-Dquarkus.http.host=0.0.0.0"]</strong><br/><strong>6c44a4a542ea96450ed578afd4c1859564926405036f61fbbf2e3660660e5f5e</strong><br/><strong>STEP 7: ENV "OPENSHIFT_BUILD_NAME"="quarkus-hello-okd-1" "OPENSHIFT_BUILD_NAMESPACE"="myproject"</strong><br/><strong>9aa01e4bc2585a05b37e09a10940e326eec6cc998010736318bbe5ed1962503b</strong><br/><strong>STEP 8: LABEL "io.openshift.build.name"="quarkus-hello-okd-1" "io.openshift.build.namespace"="myproject"</strong><br/><strong>STEP 9: COMMIT temp.builder.openshift.io/myproject/quarkus-hello-okd-1:d5dafe08</strong><br/><strong>5dafe14a20fffb50b151efbfc4871218a9b1a8516b618d5f4b3874a501d80bcd</strong><br/><br/><strong>Pushing image image-registry.openshift-image-registry.svc:5000/myproject/quarkus-hello-okd:latest ...</strong><br/><strong>. . .</strong><br/><strong>Successfully pushed image-registry.openshift-image-registry.svc:5000/myproject/quarkus-hello-okd@sha256:9fc48bb4b92081c415342407e8df41f38363a4bf82ad3a4319dddced19eff1b3</strong><br/><strong>Push successful</strong></pre>
<p>As a proof of concept, let's check the list of image streams that are available in the default project using its alias, <kbd>is</kbd>:</p>
<pre><strong>$ oc get is</strong></pre>
<p>You should see the following output:</p>
<pre><strong>NAME                IMAGE                                                             </strong><br/><strong>quarkus-hello-okd   image-registry.openshift-image-registry.svc:5000/myproject/quarkus-hello-okd</strong>   </pre>
<p>Your <kbd>ImageStream</kbd> is now available. All we have to do is create an application that uses <kbd>ImageStream quarkus-hello-okd</kbd> as input. This can be done using the following command:</p>
<pre><strong>$ oc new-app --image-stream=quarkus-hello-okd:latest</strong></pre>
<p>Now, the resources will be created. This will be confirmed by the resulting output:</p>
<pre><strong>--&gt; Found image 5dafe14 (2 minutes old) in image stream "myproject/quarkus-hello-okd" under tag "latest" for "quarkus-hello-okd:latest"</strong><br/><br/><strong>    Red Hat Universal Base Image 8 Minimal </strong><br/><strong>    -------------------------------------- </strong><br/><strong>    The Universal Base Image Minimal is a stripped down image that uses microdnf as a package manager. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.</strong><br/><br/><strong>    Tags: minimal rhel8</strong><br/><br/><strong>    * This image will be deployed in deployment config "quarkus<br/>    -hello-okd"</strong><br/><strong>    * Port 8080/tcp will be load balanced by service "quarkus<br/>    -hello-okd"</strong><br/><strong>      * Other containers can access this service through the hostname <br/>        "quarkus-hello-okd"</strong><br/><strong>    * WARNING: Image "myproject/quarkus-hello-okd:latest" runs as<br/>     the 'root' user which may not be permitted by your cluster  <br/>    administrator</strong><br/><br/><strong>--&gt; Creating resources ...</strong><br/><strong>    deploymentconfig.apps.openshift.io "quarkus-hello-okd" created</strong><br/><strong>    service "quarkus-hello-okd" created</strong><br/><strong>--&gt; Success</strong><br/><strong>    Application is not exposed. You can expose services to the<br/>    outside world by executing one or more of the commands below:</strong><br/><strong>     'oc expose svc/quarkus-hello-okd' </strong><br/><strong>    Run 'oc status' to view your app.</strong></pre>
<p>Now, our application is ready to be consumed. To allow external clients to access it, we need to expose it through a route object, as follows:</p>
<pre><strong>$ oc expose svc/quarkus-hello-okd</strong> </pre>
<p>The route will be exposed and the following log will be displayed:</p>
<pre><strong>route.route.openshift.io/quarkus-hello-okd exposed</strong></pre>
<p>We can verify the route address with the following command, which uses a JSON template to display the virtual host address of our <kbd>quarkus-hello-okd</kbd> route:</p>
<pre><strong>$ oc get route quarkus-hello-okd -o jsonpath --template="{.spec.host}"</strong></pre>
<p>In our case, the route is accessible at the following address:</p>
<pre><strong>quarkus-hello-okd-myproject.192.168.42.5.nip.io</strong> </pre>
<div class="packt_infobox">Please note that the actual IP address of the route is determined by the hypervisor according to your network configuration, so don't be surprised if it differs from the address that was exposed in this example.</div>
<p>You should be able to acknowledge this information from the web console, which shows that the application is up and running and that a single Pod<span> </span>has been started:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/62302b35-e85a-458c-8f32-efe6d45a5781.png" style=""/></div>
<p>If you go to the route host/port you have been assigned (in our case, <kbd>http://quarkus-hello-okd-myproject.192.168.42.5.nip.io</kbd>), you will see the following welcome screen:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fe8baf16-18e9-4e8a-b0f4-7eafd361f0a5.png" style=""/></div>
<p class="CDPAlignLeft CDPAlign">This is a simple static page that has been included in <kbd>src/main/resources/META-INF/resources/index.html</kbd> to show you that your application is available and contains some useful information about where you can place static assets and configuration. Your REST service, on the other hand, is still available through the REST URI:</p>
<pre><strong>$ curl quarkus-hello-okd-myproject.192.168.42.5.nip.io/getContainerId</strong></pre>
<p>Since the application is running on the <kbd>quarkus-hello-okd-1-84xwq</kbd> Pod, the expected output is as follows:</p>
<pre><strong>You are running on quarkus-hello-okd-1-7k2t8</strong></pre>
<p>Now, let's learn how to scale our Quarkus service by adding some replicas of our application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling our Quarkus service</h1>
                </header>
            
            <article>
                
<p>So far, you've learned how to deploy a Quarkus application on Minishift. The application is running in a Pod, which is allocated in its own internal IP address and is the equivalent of a machine running a container. In our case, the application is running on one Pod in an OpenShift node. This is sufficient to guarantee the availability of our applications since some liveness and readiness probes are periodically executed. If your Pods stop responding, the OpenShift platform will automatically restart them.</p>
<p>On the other hand, your application probably needs to satisfy a minimum throughput. This requirement usually can't be met with just one Pod unless the number of requests is pretty low. In this case, the simplest strategy is horizontal Pod scaling, which will improve the number of available resources that will be automatically balanced when a request for your application arrives on the router.</p>
<p>Before scaling up our application, we will need to define an upper memory limit for it, in order to reduce the impact it will have on the cluster in terms of system resources. Since our Quarkus application doesn't require a large amount of memory, we will set a limit of 50 MB as the upper limit, which is quite reasonable, and definitely thinner than an average Java application.</p>
<p>Execute the following command to set the memory limit to 50 MB. This will update the <strong>deployment configuration</strong> of your application:</p>
<pre><strong>$ oc set resources dc/quarkus-hello-okd --limits=memory=50M</strong></pre>
<div class="packt_tip">A deployment configuration (whose alias in the command line is simply <kbd>dc</kbd>) describes the state of a particular component of the application as a Pod template. When you update the deployment configuration, a deployment process occurs to scale down the application and scale it up with a new deployment configuration and a new replication controller for the application.</div>
<p>The following output should be returned:</p>
<pre><strong>deploymentconfig.apps.openshift.io/quarkus-hello-okd resource requirements updated</strong> </pre>
<p>As a proof of concept, you can verify the deployment configuration through the <kbd>describe</kbd> command:</p>
<pre><strong>$ oc describe dc/quarkus-hello-okd</strong></pre>
<p>The output of the <kbd>describe</kbd> command is a bit verbose; however, you should be able to see the following setting in the <kbd>Limits</kbd> section:</p>
<pre><strong>Limits:</strong><br/><strong>memory:        50M</strong></pre>
<p>Now, let's scale our application to 10 instances. This will be pretty fast since we have set a memory limit on the resources that are consumed by each Pod:</p>
<pre><strong>$ oc scale --replicas=10 dc/quarkus-hello-okd</strong></pre>
<p>The following is the expected output:</p>
<pre><strong>deploymentconfig.apps.openshift.io/quarkus-hello-okd scaled</strong></pre>
<p>Moving to the web console, in the <span class="packt_screen">Overview</span> panel, we will see that our application has scaled to 10 Pods:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/37d3f292-5448-4e98-b08c-793a23f269f9.png" style=""/></div>
<p>Now that we have a large number of available Pods, let's try to run a load test against our application:</p>
<pre>for i in {1..100}; do curl quarkus-hello-okd-myproject.192.168.42.5.nip.io/getContainerId ; echo ""; done;</pre>
<p>Now, you should be able to see the response that was produced by the REST application in your console. This displays the ID of the Pod that executed the request (the output has been truncated for brevity):</p>
<pre> You are running on quarkus-hello-okd-2-jzvp2\n<br/> You are running on quarkus-hello-okd-2-fc7h9\n<br/> You are running on quarkus-hello-okd-2-lj67f\n<br/> You are running on quarkus-hello-okd-2-qwm9j\n<br/> You are running on quarkus-hello-okd-2-n6kn6\n<br/> You are running on quarkus-hello-okd-2-bbk84\n<br/> You are running on quarkus-hello-okd-2-d5bj6\n<br/> You are running on quarkus-hello-okd-2-skc2h\n<br/> You are running on quarkus-hello-okd-2-bw5f9\n<br/> You are running on quarkus-hello-okd-2-p24jl\n<br/> You are running on quarkus-hello-okd-2-jzvp2\n<br/> ...  </pre>
<p>Although measuring the performance of our applications is beyond the scope of this book, you can go ahead and measure the time that's needed to run the equivalent Java application in the same cluster. You will notice a different response in terms of time and memory consumption!</p>
<p>That was our last task for this chapter. When you are done with this example, and you want to clean up the resources we created in our project, simply execute the following command, which will perform a bulk cleanup of resources:</p>
<pre><strong>oc delete all --all</strong></pre>
<p>The output may vary, depending on the number of Pods available. However, it should look similar to the following:</p>
<pre><strong>pod "quarkus-hello-okd-1-7k2t8" deleted</strong><br/><strong>pod "quarkus-hello-okd-1-build" deleted</strong><br/><strong>pod "quarkus-hello-okd-1-deploy" deleted</strong><br/><strong>replicationcontroller "quarkus-hello-okd-1" deleted</strong><br/><strong>service "quarkus-hello-okd" deleted</strong><br/><strong>deploymentconfig.apps.openshift.io "quarkus-hello-okd" deleted</strong><br/><strong>buildconfig.build.openshift.io "quarkus-hello-okd" deleted</strong><br/><strong>imagestream.image.openshift.io "quarkus-hello-okd" deleted</strong><br/><strong>route.route.openshift.io "quarkus-hello-okd" deleted</strong></pre>
<p>The preceding log confirms that all the deleted resources have been successfully evicted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we ran a simple REST application in a Docker container and then ran it in a Kubernetes-native environment, that is, Minishift. We saw how simple it is to make our applications highly available with a sound throughput by leveraging the features of a bundled distribution of OKD.</p>
<p>Now, it's time to add some more features to our application. In the next chapter, we will learn how to configure the Undertow extension, which can be added to provide web server capabilities to our application. It also includes some UI assets, which we will look at in brief.</p>


            </article>

            
        </section>
    </body></html>