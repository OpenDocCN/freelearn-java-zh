- en: Use Case - A Parallel Web Crawler
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 - 并行网络爬虫
- en: In the previous chapter, we discussed the actors model and a framework that
    you can use to program with actors. However, the actor model is a paradigm just
    like functional programming. In principle, if you know the actors model from one
    language, you can use it in another language even if it doesn't have a framework
    that supports the actor model. This is because an actor model is an approach to
    reasoning about parallel computations, not some language-specific set of tools.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了actor模型以及你可以用来用actor编程的框架。然而，actor模型是一种范式，就像函数式编程一样。原则上，如果你从一个语言中了解了actor模型，即使另一个语言没有支持actor模型的框架，你仍然可以使用它。这是因为actor模型是一种关于并行计算推理的方法，而不是一些特定语言的工具集。
- en: This state of things, just like functional programming, has its own benefits
    and drawbacks. The benefit is that you are not language-dependent if you depend
    on concepts. Once you know a concept, you can come to any programming language
    at all, and be capable of using them. However, the learning curve is steep. Precisely
    because it is all about paradigm and approach, it is not enough to install a library
    and start using it after skimming its documentation, as it is frequently the case
    with many other libraries or general-purpose languages that are similar one to
    another. Since it is all about shifting the paradigm, you need to apply some learning
    effort in order to understand the paradigm and how to use it in practice.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这种状态，就像函数式编程一样，有其自身的优点和缺点。优点是，如果你依赖于概念，那么你就不依赖于语言。一旦你了解了某个概念，你就可以在任何编程语言中使用它，并且能够使用它们。然而，学习曲线很陡峭。正是因为它全部关于范式和方法，仅仅安装一个库并浏览其文档后就开始使用，就像许多其他类似库或通用语言那样，是不够的。由于它全部关于范式转变，你需要付出一些学习努力来理解范式以及如何在实践中使用它。
- en: In the previous chapter, we built up a theoretical foundation on the toolset
    the actor model provides, as well as discussed the main concepts of the actor
    model and how to use it in practice. However, since it is a set of ideas and not
    just a library, it is necessary to develop an intuition for how it works. The
    best way to develop an intuition for a new paradigm is to have a look at some
    practical examples that will demonstrate how this paradigm is applicable in practice.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们建立了一套基于actor模型提供的工具集的理论基础，并讨论了actor模型的主要概念以及如何在实践中使用它。然而，由于它是一套理念而非仅仅是一个库，因此有必要培养对它工作原理的直觉。培养对新范式直觉的最好方法就是查看一些实际例子，这些例子将展示这个范式在实际中的应用。
- en: For this reason, in this chapter, we will be looking at a practical example
    of where an actors model can be applied. We will be looking at an example of web
    crawling.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将探讨actor模型可以应用的一个实际例子。我们将探讨网络爬虫的例子。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Problem statement
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Sequential solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序解决方案
- en: A parallel solution with Akka
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Akka的并行解决方案
- en: Problem statement
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题陈述
- en: In this chapter, we will be addressing the problem of building a web crawler.
    Web crawlers are important, for example, in the domain of indexing and searching
    the web.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将解决构建网络爬虫的问题。网络爬虫在索引和搜索网络领域非常重要。
- en: The graph structure of the web
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络的图结构
- en: All the websites can be imagined as a graph of pages. Every page contains some
    HTML markup and content. As part of this content, most web pages contain links
    to other pages. Since links are supposed to take you from one page to another,
    we can visualize the web as a graph. We can visualize links as edges that take
    you from one node to another node.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网站都可以想象成页面的图。每个页面都包含一些HTML标记和内容。作为内容的一部分，大多数网页都包含指向其他页面的链接。由于链接是用来从一个页面跳转到另一个页面的，因此我们可以将网络可视化为一个图。我们可以将链接可视化为从节点到节点的边。
- en: 'Given such a model for the entire internet, it''s possible to address the problem
    of searching for information over the web:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 给定整个互联网的这种模型，我们可以解决在网络上搜索信息的问题：
- en: '![](img/00be260a-4a75-4bdd-975a-388e35442e46.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00be260a-4a75-4bdd-975a-388e35442e46.png)'
- en: We are talking about a problem that is faced by the search engines. The objective
    of a search engine is to index the information stored online and devise algorithms
    that will find the required pages—nodes of the graph—efficiently based on certain
    queries by the end user. The algorithms required to index those nodes and to match
    the user requests to the information stored in the nodes are complex, and we will
    not be addressing them in this chapter. Often, these algorithms involve advanced
    machine learning and natural-language processing solutions to understand and evaluate
    the content stored in the pages. The brightest minds in machine learning and natural
    language processing work on search tasks for companies such as Google.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论搜索引擎面临的问题。搜索引擎的目标是索引存储在网上的信息，并设计算法，根据最终用户的查询高效地找到所需的页面——图中的节点。索引这些节点以及将用户请求与节点中存储的信息匹配所需的算法很复杂，我们不会在本章中讨论它们。通常，这些算法涉及高级机器学习和自然语言处理解决方案来理解和评估页面中存储的内容。机器学习和自然语言处理领域的最聪明的大脑在为像Google这样的公司处理搜索任务。
- en: Therefore, in this chapter, we will be addressing a task that is also faced
    by search engines but is easier to tackle. Before indexing the information stored
    online, this information needs to be collected. A collection of information is
    a task of traversing the graph of all the pages and storing the contents in a
    database. The task of web crawling is precisely the task of traversing the graph
    starting from one node and following to other nodes through links. In the example
    in this chapter, we will not be storing website information in a database but
    will focus on the web crawling task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将讨论一个搜索引擎也面临但更容易解决的问题。在索引存储在网上的信息之前，需要收集这些信息。收集信息是一项遍历所有页面图并存储内容到数据库中的任务。网络爬虫的任务正是从某个节点开始遍历图，通过链接跟随到其他节点。在本章的示例中，我们不会将网站信息存储到数据库中，而是将重点放在网络爬虫任务上。
- en: Collecting information from the graph
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从图中收集信息
- en: 'Collecting information from a graph can be depicted as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中收集信息可以表示如下：
- en: '![](img/58b28bfb-db22-430a-9b50-688c11ad0df5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/58b28bfb-db22-430a-9b50-688c11ad0df5.png)'
- en: 'In the preceding illustration, the blue nodes are the ones currently being
    processed, the green ones are the ones that have already been processed, and the
    white ones are the ones not yet processed. This preceding task can be implemented
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，蓝色节点是当前正在处理的节点，绿色节点是已经处理过的节点，白色节点是尚未处理的节点。这个前面的任务可以如下实现：
- en: Specify which URL you would like to start from, that is, the starting node from
    which the traversal will start.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定您希望从哪个URL开始，即遍历将开始的起始节点。
- en: 'The crawler will visit the URL by issuing a `GET` request to it, and receive
    some HTML:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爬虫将通过向其发出`GET`请求来访问URL，并接收一些HTML：
- en: '![](img/9cd5cfe4-0b52-4532-b1cb-7493a19da150.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9cd5cfe4-0b52-4532-b1cb-7493a19da150.png)'
- en: 'After receiving this HTML text, the crawler will extract the edges of the current
    node—the links to other nodes. The links are well defined in HTML markup. They
    are specified as follows:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接收到此HTML文本后，爬虫将提取当前节点的边缘——指向其他节点的链接。这些链接在HTML标记中定义良好。它们被指定如下：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So, once we have the HTML markup, we can look it up for the contents matching
    the preceding pattern. After that, we may want to collect all of the links presented
    on the page into one single collection structure, for example, a list. Once we
    have that, we may want to execute the same operation recursively on every node
    linked to the current one.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦我们有了HTML标记，我们就可以查找与先前模式匹配的内容。之后，我们可能想要将页面上展示的所有链接收集到一个单独的集合结构中，例如，一个列表。一旦我们有了这个，我们可能想要对当前节点链接的每个节点递归地执行相同的操作。
- en: Parallel nature of the task
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务并行性
- en: The preceding task was chosen as an example for this chapter because it is parallelizable
    by nature. Basically, whenever you have a bunch of tasks that are not dependent
    one on another, it may be a good idea to parallelize the tasks. In our example,
    the preceding task can be very well conceptualized as a set of independent operations.
    A single unit of operation is the task of visiting a URL, abstracting all of the
    other nodes that are linked by the page of this URL, and then executing the same
    task on these URLs recursively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 之前选择这个任务作为本章的例子，因为它本质上是可以并行化的。基本上，每当您有一系列相互之间不依赖的任务时，将任务并行化可能是一个好主意。在我们的例子中，前面的任务可以很好地被构想为一组独立的操作。单个操作单元是访问URL的任务，抽象出所有通过该URL页面链接的其他节点，然后对这些URL递归地执行相同的任务。
- en: This makes our crawling example a very good case for parallel actor application.
    Running all these tasks sequentially for every URL may be very time consuming,
    and therefore it is much more efficient to apply some strategy to the tasks. Even
    on a single-core CPU computer  processing power, even in the case when most of
    the tasks are processed in a simulated parallelism manner, the operation will
    still be dozens of times more efficient than in the sequential case. This is because
    a lot of waiting is involved in requesting HTML from the internet or a `GET` request.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们的爬虫示例非常适合并行actor应用。对于每个URL运行所有这些任务可能是非常耗时的，因此应用一些策略到任务上会更为高效。即使在单核CPU计算机上，即使大多数任务以模拟并行方式处理，操作仍然会比顺序情况高效数十倍。这是因为从互联网或`GET`请求请求HTML时涉及大量的等待。
- en: 'In the succeeding diagram, you can see an example of a request to have a process
    performed from a single thread:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的图中，您可以看到一个从单个线程执行过程的请求示例：
- en: '![](img/c90f1fef-650c-4480-9c22-3a781cfad746.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c90f1fef-650c-4480-9c22-3a781cfad746.png)'
- en: So, as you can see, the thread issues a request, and then it waits for the response
    to be derived. From your own browsing experience, you can easily say that sometimes
    a response from a website may take seconds to arrive. So essentially, these seconds
    will be wasted by the thread because it will block on this task, and the processor
    will do nothing useful at the time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如您所见，线程发出请求，然后等待响应的产生。根据您自己的浏览经验，您很容易地说，有时一个网站的响应可能需要几秒钟才能到达。所以本质上，这些秒数将会被线程浪费，因为它将在这个任务上阻塞，而此时处理器将无法做任何有用的工作。
- en: Even when you have a single core and even when you have only an option of simulated
    parallelism, the processor can still be utilized much more efficiently, if when
    one thread waits for the request to arrive at it, another thread issues a request
    or processes a response that has already arrived at it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您只有一个核心，即使您只有模拟并行化的选择，如果当一个线程等待请求到达时，另一个线程发出请求或处理已经到达的响应，处理器仍然可以更有效地被利用。
- en: 'As you can see from the diagram, requests are issued much more efficiently,
    and while one thread waits for a request and sleeps, the processor is busy with
    other threads that have a real job to do:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从图中可以看出，请求的发出效率要高得多，而且当一个线程等待请求并休眠时，处理器正忙于处理其他有实际工作要做的线程：
- en: '![](img/68bdca1f-90ae-44a2-84c5-23f3594327a5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68bdca1f-90ae-44a2-84c5-23f3594327a5.png)'
- en: In general, such kind of blocking can be very frequently the case. For example,
    you may have some operations that wait on a database to give a response, or on
    a message to arrive from the outer world. So even in the case of simulated parallelism,
    these kinds of situations can be greatly sped up. Needless to say, most modern
    machines work in a setting of several cores that will speed up the execution greatly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种阻塞情况可能非常常见。例如，您可能有一些操作需要等待数据库给出响应，或者等待外部世界传来的消息。所以即使在模拟并行的情况下，这些情况也可以大大加快速度。不用说，大多数现代机器都在多核的环境中工作，这将极大地加快执行速度。
- en: Now that we know the exact specification of the task, and we why exactly it
    makes sense to parallelize this task, let's get our hands dirty and see some implementation
    in action. However, before we dive into the details of the actor-based implementation
    of the task, let's first have a look at how this can be implemented sequentially,
    just so that we have a baseline to work against.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了任务的精确规格，并且我们知道了为什么并行化这个任务是有意义的，让我们动手看看一些实际的实现。然而，在我们深入到基于actor的任务实现的细节之前，让我们首先看看如何顺序地实现它，这样我们就有了一个基准来工作。
- en: Sequential solution
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序解决方案
- en: 'First of all, let''s have a look at how we would like to use the solution once
    it is built. Also, let''s have a look at exactly which output we expect from the
    solution:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看一旦构建了这个解决方案，我们希望如何使用它。同时，让我们看看我们期望从解决方案中得到的确切输出：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we perform the following things. First of all, we would
    like to be able to define our URL in terms of a Java-native URL object. In this
    example, we are using the Maven repository website as a crawling target. [mvnrepository.com](http://mvnrepository.com) is
    a website that provides an easy search through all of the Maven packages.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行以下操作。首先，我们希望能够用Java原生URL对象来定义我们的URL。在这个例子中，我们使用Maven仓库网站作为爬取目标。[mvnrepository.com](http://mvnrepository.com)是一个提供对所有Maven包进行简单搜索的网站。
- en: After that, we have a call to `fetchToDepth`. This is the method that is supposed
    to do the actual work for us and actually crawls the website for links. As a first
    argument, we supply it a URL we want to process. As the second argument, we supply
    it the depth to which you want to search the graph.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们调用`fetchToDepth`。这是应该为我们实际工作的方法，并实际爬取网站以获取链接。作为第一个参数，我们提供一个我们想要处理的URL。作为第二个参数，我们提供一个你想要搜索的图的深度。
- en: The idea behind having the depth is to avoid infinite recursion. The web is
    a very interconnected place. So, when we start from one node and begin to descend
    recursively into the nodes it is connected to, it may take a very long time to
    arrive at the terminal nodes, that is, a node that does not have any further links.
    It is quite possible that for the majority of the websites out there, such a lookup
    will be infinite or will take an unreasonably long time to accomplish. Therefore,
    we would like to limit our lookup depth. The semantics of how it will work is
    that for every edge the algorithm searches, the depth will be decreased by one.
    Once the depths reach zero, the algorithm will not try to follow the edges any
    further.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度的概念是为了避免无限递归。网络是一个非常互联的地方。因此，当我们从一个节点开始，并开始递归地进入与之相连的节点时，到达终端节点（即没有进一步链接的节点）可能需要非常长的时间。对于大多数网站来说，这样的查找可能是无限的，或者需要不合理的时间来完成。因此，我们希望限制我们的查找深度。其语义是，对于算法搜索的每个边，深度将减少一个。一旦深度达到零，算法将不会尝试跟随任何进一步的边。
- en: The result of this from function-execution will be a collection. Precisely,
    a set of links that were collected from this website. After that, we take the
    first 10 links and output them to the standard output, one link per line. Also,
    we output the number of links that the system managed to extract in total. We
    do not print all the links because there will be too many of them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 函数执行的结果将是一个集合。确切地说，是从该网站收集到的链接集合。然后，我们取出前10个链接并将它们输出到标准输出，每行一个链接。同时，我们输出系统成功提取的总链接数。我们不打印所有链接，因为会有太多。
- en: 'The output we are aiming to get is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望得到以下输出：
- en: '![](img/43cc0e31-f16f-4d3e-9980-5359a30fd57e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/43cc0e31-f16f-4d3e-9980-5359a30fd57e.png)'
- en: 'Next, let''s explore the sequential implementation of the preceding objectives.
    Let''s have a look at `fetchToDepth`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索前面目标的顺序实现。让我们看看`fetchToDepth`：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, the function accepts the three arguments. Two of them are the
    ones that we have already seen before and discussed in our example API usage.
    The third one is the set of URLs that the crawler has already visited. Why might
    we have such a set? Why does the crawler need it at all? How will it behave if
    we do not store the set of all visited links?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，该函数接受三个参数。其中两个是我们之前已经见过并在我们的示例API使用中讨论过的。第三个是爬虫已经访问过的URL集合。为什么我们会有这样一个集合？为什么爬虫需要它？如果我们不存储所有已访问链接的集合，它会如何表现？
- en: In fact, the same set must be stored for any similar graph-traversal situation.
    The problem here is the presence of cycles in a graph.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，任何类似的图遍历情况都必须存储相同的集合。这里的问题是图中存在循环。
- en: 'In the diagram below, you can see how a graph with cycles might emerge in a
    web crawler application:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，您可以看到在网页爬虫应用程序中如何出现具有循环的图：
- en: '![](img/8380c4c1-2f62-4079-8de7-7fb31dcfa4ae.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8380c4c1-2f62-4079-8de7-7fb31dcfa4ae.png)'
- en: 'It is enough for website **A** to reference website **B**, and then website
    **B** references website **A** for a cycle to emerge. If the graph has a cycle,
    there is a risk that our traversal algorithm will enter an infinite loop if it
    tries to traverse it without keeping track of the nodes that it has already visited.
    For this reason, we need to keep track of the visited nodes. The first thing to
    do in the `fetchToDepth` function is to fetch the given URL:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 仅当网站**A**引用网站**B**，然后网站**B**又引用网站**A**时，才会出现循环。如果图中存在循环，那么如果我们的遍历算法在不跟踪已访问节点的情况下尝试遍历它，那么它可能会进入无限循环。因此，我们需要跟踪已访问的节点。在`fetchToDepth`函数中要做的第一件事就是获取指定的URL：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are using the following function to do fetch the given URL:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下函数来获取指定的URL：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `fetch` function is a function that takes a URL and outputs `Option[Set[URL]]`,
    the data structure with the links that this page contains.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetch`函数是一个接收URL并输出`Option[Set[URL]]`的函数，这是包含此页面链接的数据结构。'
- en: Inside this function, nothing really remarkable happens except the side effects.
    We are using `Try` to capture the side effect of an error. Then, we convert `Try`
    to `Option` because we are not interested in the error type, as explained below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数内部，除了副作用之外，并没有真正值得注意的事情发生。我们使用`Try`来捕获错误的副作用。然后，我们将`Try`转换为`Option`，因为我们对错误类型不感兴趣，如下所述。
- en: The logic of this function is straightforward. We are using a popular Java library,
    called `Jsoap`, in order to connect to the given URL, issue a get request, retrieve
    all of the elements that have an `href` attribute that contains a link, and afterwards,
    map the resulting elements by extracting their links.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的逻辑很简单。我们使用一个流行的Java库，称为`Jsoap`，来连接给定的URL，发出GET请求，检索所有具有包含链接的`href`属性的元素，然后通过提取它们的链接来映射结果元素。
- en: The logic is wrapped into `Try`, which is later converted into `Option`. Why
    have such a system involving `Try` and `Option`? As we learned in the first part
    of this book, both `Try` and `Option` are effect types. The entire idea behind
    an effect type is to abstract away some side effects that might happen. Since
    our business logic is wrapped into the `Try` effect, some side effects that are
    abstracted by `Try` might happen in this logic.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑被封装在`Try`中，稍后将其转换为`Option`。为什么会有这样一个涉及`Try`和`Option`的系统？正如我们在本书的第一部分所学，`Try`和`Option`都是效果类型。效果类型背后的整个想法是抽象掉可能发生的某些副作用。由于我们的业务逻辑被封装在`Try`效果中，因此`Try`抽象掉的一些副作用可能会在这个逻辑中发生。
- en: '`Try` abstracts away the possibility of an error or an exception. In fact,
    a lot of things can go wrong when we are performing a request to a remote server.
    For example, the request can time out, or we may lose internet connection, or
    the server can return an erroneous status code.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Try`抽象掉了错误或异常的可能性。实际上，当我们向远程服务器发出请求时，可能会发生很多错误。例如，请求可能会超时，我们可能会失去互联网连接，或者服务器可能会返回错误的状态码。'
- en: In the context of massive data processing, things inevitably go around for some
    of the data nodes. This means that when performing the crawling operation discussed
    previously, we can be almost certain that sometimes it will fail. What happens
    if you design a function that does not account for such errors? Most likely, our
    algorithm will encounter an exception and will fail midway through the dataset.
    This is why fault tolerance is a crucial property of this kind of system.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模数据处理的情况下，一些数据节点不可避免地会循环。这意味着在执行之前讨论的爬取操作时，我们可以几乎肯定有时它会失败。如果你设计了一个没有考虑到这种错误的函数，会发生什么？很可能会遇到异常，并在数据集中途失败。这就是为什么容错性是这类系统的一个关键属性。
- en: In the preceding example, fault tolerance is achieved by wrapping the possibility
    of an error into the `Try` effect type. Another thing to notice here is not only
    the `Try` type but the fact that it is converted into `Option`. `Option` indicates
    that a function may or may not produce a result. In the context of converting
    from `Try` to `Option`, `Try` is mapped to `Some` if it is a successful attempt,
    and it is mapped to `None` if it is a failure. In the case of a failure, some
    information about the error that has happened gets lost when we convert `Try`
    to `Option`. This fact reflects the attitude that we don't care about the type
    of the error that might occur.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，容错是通过将错误的可能性封装在`Try`效果类型中实现的。这里要注意的另一件事不仅是`Try`类型，还有它被转换成了`Option`。`Option`表示一个函数可能或可能不产生结果。在从`Try`转换为`Option`的上下文中，如果`Try`是成功的尝试，它会被映射到`Some`；如果它是失败的，它会被映射到`None`。在失败的情况下，当我们把`Try`转换为`Option`时，一些关于已发生错误的详细信息会丢失。这一事实反映了我们不在乎可能发生的错误类型的态度。
- en: This kind of attitude demands further explanation, as it is usually the case
    everywhere where we intentionally drop data. We discussed that in this kind of
    massive data-processing, errors are inevitable. Of course, it is nice to have
    your algorithm catch and handle gracefully most of the errors that it will encounter,
    however, it is not always possible. You cannot remove the fact that sooner or
    later the algorithm will encounter an error since the algorithm works not on some
    dummy data but on real-world websites. Whenever you are working with real-world
    data, errors are a fact of life, and you cannot plan to handle every possible
    error that occurs gracefully.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种态度需要进一步解释，因为它通常在我们有意丢弃数据的地方都会出现。我们讨论过，在这种大量数据处理中，错误是不可避免的。当然，如果您的算法能够优雅地捕获和处理它将遇到的错误中的大多数，那将是件好事，然而，这并不总是可能的。算法最终会遇到错误这一事实是无法避免的，因为算法不是在处理一些虚拟数据，而是在处理现实世界的网站。无论何时您处理现实世界的数据，错误都是生活的事实，您无法计划处理发生的每一个可能的错误。
- en: Hence, a proper way to structure a program that is certain to encounter errors
    is fault tolerance. This means that we build the program as if it is certain to
    encounter errors and we plan for recovering from any error without actually specifying
    the type of the error. The fact that we are converting our `Try` to `Option` here
    reflects the attitude toward errors. The attitude is that we don't care about
    which error might happen during execution, we only care that an error can happen.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，构建一个肯定会遇到错误的程序的正确方式是容错。这意味着我们构建程序时，就像它肯定会遇到错误一样，并计划从任何错误中恢复，而不实际指定错误类型。我们在这里将`Try`转换为`Option`的事实反映了我们对错误的态度。这种态度是，我们不在乎执行过程中可能发生的哪种错误，我们只关心错误可能发生。
- en: Fault tolerance is one of the fundamental principles of the design of Akka.
    Akka is built with fault tolerance and resilience in mind, which means that actors
    are designed as if they are certain to encounter errors, and Akka provides you
    with a framework to specify how to recover from them.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 容错是Akka设计的基本原则之一。Akka是以容错和弹性为前提构建的，这意味着演员被设计成肯定会遇到错误，Akka为您提供了一个框架来指定如何从它们中恢复。
- en: 'Let''s now return to our `fetch` line from the `fetchToDepth` method  and see
    how exactly this attitude plays out in the logic of our example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到`fetchToDepth`方法中的`fetch`行，看看这种态度在我们的示例逻辑中是如何具体体现的：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s return to our `fetchToDepth` example. After we perform the affect statement
    on a given URL, we also perform a `getOrElse` call on it. `getOrElse` is a method
    defined on a Scala `Option` that has the following signature:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的`fetchToDepth`示例。在我们对一个给定的URL执行影响语句之后，我们也在它上面执行了一个`getOrElse`调用。`getOrElse`是Scala
    `Option`上定义的一个方法，其签名如下：
- en: '![](img/f482272d-d39f-40fc-ab68-f4b6fbea04c7.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f482272d-d39f-40fc-ab68-f4b6fbea04c7.png)'
- en: 'So basically, this method tries to extract a value from `Option`. If the value
    is not present in `Option`, it outputs the default value that is supplied to the
    method. `getOrElse` is equivalent to the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，这个方法试图从`Option`中提取一个值。如果值不在`Option`中，它将输出方法提供的默认值。`getOrElse`等同于以下代码：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So now we have a set of links extracted from a given link. Let''s now have
    a look at how it is traversed in our sequential example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经从给定的链接中提取了一组链接。让我们现在看看在我们的顺序示例中它是如何遍历的：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, you can see a chunk of the code in the `fetchToDepth`
    method. First of all, we check whether the depth is greater than `0`. Previously,
    we discussed how specifying depth constraints could save us from infinite execution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你可以看到`fetchToDepth`方法中的代码块。首先，我们检查深度是否大于`0`。之前我们讨论了如何通过指定深度约束来避免无限执行。
- en: The first statement after the `if` check is the current link accumulator, the
    `links` variable, being united with some larger right-hand statement. In the snippet
    above, only a fragment of this statement is presented, and we will discuss it
    step by step. The big picture is that this statement applies `fetchToDepth` recursively
    to all the links in the `links` set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在`if`检查之后的第一个语句是当前链接累加器，即`links`变量，它与一些更大的右侧语句合并。在上面的代码片段中，只展示了这个语句的一部分，我们将一步一步地讨论它。整体来看，这个语句递归地将`fetchToDepth`应用于`links`集合中的所有链接。
- en: 'This means that the right side of the `++` operator takes every link from the
    set that we have retrieved and extracts all the links present in its page. This
    must be done recursively. But first, we need to clean up the set of all abstracted
    URL from the current link so that it doesn''t contain the links that we have previously
    visited. This is to tackle the problem of cycles in the graph:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着`++`操作符的右侧从我们检索到的集合中获取每个链接，并提取其页面中存在的所有链接。这必须递归地完成。但首先，我们需要清理当前链接的所有抽象URL集合，以确保它不包含我们之前访问过的链接。这是为了解决图中循环的问题：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we have the resulting set further transformed by the call to the  `toList`
    and `zipWithIndex` methods. Basically, these two methods are required for logging
    purposes. Inside data-processing operations, you would like to have some reporting
    in order to keep track of the operations. In our case, we would like to assign
    a numeric ID to every link that we are going to visit, and we are going to log
    the fact that we are visiting a link with a given ID into the standard output.
    The signature of `zipWithIndex` is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用`toList`和`zipWithIndex`方法进一步转换得到的集合。基本上，这两个方法是为了日志记录目的而需要的。在数据处理操作中，你希望有一些报告来跟踪操作。在我们的案例中，我们希望给每个将要访问的链接分配一个数字ID，并将我们正在访问具有给定ID的链接的事实记录到标准输出中。`zipWithIndex`的签名如下：
- en: '![](img/6548e0a2-7673-4eb5-b667-92c1edb3e21a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6548e0a2-7673-4eb5-b667-92c1edb3e21a.png)'
- en: 'So basically, a list of elements, `A`, becomes a list of pairs of `A` and `Int`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，一个元素列表`A`变成了一个`A`和`Int`对的列表：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code, you can see the logic that actually performs the processing
    of every given URL. We are using a `foldLeft` method. Let''s have a look at its
    signature:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你可以看到实际执行每个给定URL处理的逻辑。我们使用了一个`foldLeft`方法。让我们看看它的签名：
- en: '![](img/d6848006-5a06-4b69-8003-96f70212be9a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6848006-5a06-4b69-8003-96f70212be9a.png)'
- en: 'Basically, this method folds your entire collection into a single value. For
    a simpler example of its use case, consider the following chunk of code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个方法将你的整个集合折叠成一个单一值。为了更简单地说明其用法，考虑以下代码块：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So basically, we start with some empty accumulator and specify what to do for
    each element given a value accumulated so far in scope. Every subsequent element
    of the list must be added to the accumulator.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，我们从一个空的累加器开始，并指定在作用域内给定一个累积值时对每个元素要做什么。列表的后续每个元素都必须添加到累加器中。
- en: Returning to our example of `fetchToDepth`, the justification for the usage
    of `foldLeft` is as follows. Ultimately, we have a set of links, and for every
    element of this set, we need to calculate the set of URLs it links to. However,
    we are interested in a combined set with all the links from all of the URLs we
    have just called. Hence, the example is similar to the example of the addition
    of integers. Except here, we are computing a union over a collection of sets of
    links.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的`fetchToDepth`示例，使用`foldLeft`的理由如下。最终，我们有一个链接集合，并且对于这个集合的每个元素，我们需要计算它链接到的URL集合。然而，我们感兴趣的是包含我们刚刚调用的所有URL的所有链接的合并集合。因此，这个例子类似于整数加法的例子。只不过在这里，我们是在一组链接集合上计算并集。
- en: Let's now have a look at the code block that is passed as the second argument
    to `foldLeft`. First, we perform a logging statement. Next, we perform a step
    of calculating the links that belong to the current URL. We are then adding these
    links into the combined accumulator.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看作为`foldLeft`第二个参数传递的代码块。首先，我们执行一个日志语句。然后，我们执行计算属于当前URL的链接的步骤。然后，我们将这些链接添加到组合累加器中。
- en: Notice also that before actually crawling a link, we perform a check to see
    whether it is contained already in the accumulator. It is crawled only if it is
    not contained in the current accumulator. This check is used to prevent redundant
    work. If the link is already contained in the accumulator, this means it was already
    processed by the algorithm since it is a depth-first traversal of the graph. So
    we do not need to process it again.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在实际上爬取链接之前，我们执行一个检查，看看它是否已经包含在累加器中。只有当它不包含在当前累加器中时，才会进行爬取。这个检查用于防止重复工作。如果链接已经包含在累加器中，这意味着它已经被算法处理过了，因为这是一个图的深度优先遍历。所以我们不需要再次处理它。
- en: 'Also, notice that we perform the processing using a recursive call to the `fetchToDepth`
    function with the depth reduced by one:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，我们使用对 `fetchToDepth` 函数的递归调用进行处理，深度减少一个：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, if the depth is `0`, we will return the set of links that we have extracted
    from the current page. Which means we will stop the algorithm at this point.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果深度为 `0`，我们将返回从当前页面提取的链接集合。这意味着我们将在此处停止算法。
- en: 'The entire code for the `fetchToDepth` function looks as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetchToDepth` 函数的完整代码如下：'
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, let's discuss what a parallel solution to the preceding problem might
    look like.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一下对前面问题的并行解决方案可能的样子。
- en: A parallel solution with Akka
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Akka 的并行解决方案
- en: Before approaching the problem of parallelizing our crawler, let's discuss our
    strategy of how we are going to approach it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决并行化我们的爬虫的问题之前，让我们讨论我们将如何处理它的策略。
- en: Strategy
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: We are going to model our problem of creating the actors' system with a tree
    of actors. This is because the task itself naturally forms a tree.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用演员树来建模创建演员系统的难题。这是因为任务本身自然形成了一个树。
- en: We have already discussed how all the links and pages on the internet comprise
    a graph. However, we have also discussed, in our example, two things that are
    not desirable when processing and traversing this graph—cycles and repetitions
    of work. So, if we have visited a certain node, we are not going to visit it anymore.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了所有互联网上的链接和页面如何构成一个图。然而，在我们的例子中，我们还讨论了在处理和遍历这个图时两个不希望出现的情况——循环和工作重复。所以，如果我们已经访问了一个特定的节点，我们就不会再访问它了。
- en: 'This means that our graph becomes a tree on processing time. This means you
    cannot get to a parent node from a child node when descending from a child node. This
    tree may look as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在处理时，我们的图变成了一个树。这意味着当你从子节点向下遍历时，你不能从子节点到达父节点。这个树可能看起来如下：
- en: '![](img/c37fdf35-76cc-4ae1-bee1-f217dfb18333.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c37fdf35-76cc-4ae1-bee1-f217dfb18333.png)'
- en: We are going to communicate with the root-level node. So basically, we will
    send this node a link that we are going to start crawling from, and afterward,
    this actor is going to crawl it. Once it extracts all the links from that link,
    it will for each such link spawn a child actor that will do the same operation
    on that link. This is a recursive strategy.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将与根级节点进行通信。所以基本上，我们将向此节点发送一个我们将从其开始爬取的链接，然后此演员将爬取它。一旦从这个链接中提取出所有链接，它将为每个这样的链接生成一个子演员，该子演员将对该链接执行相同的操作。这是一个递归策略。
- en: These worker actors will report the results to their parent actors. The results
    from the bottom-most actors are going to get propagated to the top-level actors
    all the way through the tree.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工作演员将向其父演员报告结果。最底层演员的结果将通过树形结构传播到顶层演员。
- en: In this implementation, notice how we do not restrict our usage of actors. Every
    link gets worked on by a dedicated actor, and we don't really care about how many
    actors will get spawned. This is because actors are very lightweight primitives,
    and you should not be afraid to be generous with spawning actors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，请注意我们并没有限制我们对演员的使用。每个链接都由一个专门的演员处理，我们并不真正关心将生成多少演员。这是因为演员是非常轻量级的原语，你不应该害怕慷慨地生成演员。
- en: Let's have a look at how to implement this strategy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现这个策略。
- en: Implementation
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'First of all, let''s have a look at the API we want and its usage:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们想要的 API 及其用法：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, you can see the main method of the parallelized actor
    application. As you can see, we create an actor system and a root-level worker
    actor. The idea is to represent all of the processing actors as a single class, `Worker`.
    The job of a single worker is to process a single URL and spawn additional workers
    to process the links extracted from it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你可以看到并行化演员应用程序的主要方法。正如你所看到的，我们创建了一个演员系统和根级别的工人演员。想法是将所有处理演员表示为一个单一的类，`Worker`。单个工人的任务是处理单个
    URL 并产生额外的工人来处理从中提取的链接。
- en: 'Next, you can see an example of querying the actor. First of all, we have the
    following line:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以看到一个查询演员的示例。首先，我们有以下这一行：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Ordinarily, you send messages to actors using the `!` operator. However, here
    we are using `?` because we are using an ask pattern. In essence, `?` is the act
    of sending a message to an actor and waiting for it to respond with another message.
    The ordinary `!` returns `Unit`, however, the `?` ask pattern returns a `Future`
    of a possible response:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你使用 `!` 操作符向演员发送消息。然而，这里我们使用 `?`，因为我们正在使用询问模式。本质上，`?` 是向演员发送消息并等待它以另一个消息响应的行为。普通的
    `!` 返回 `Unit`，然而，`?` 询问模式返回一个可能响应的 `Future`：
- en: '![](img/ca415ddb-fd39-4a22-9aa4-5026564482b7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca415ddb-fd39-4a22-9aa4-5026564482b7.png)'
- en: So essentially, we are sending a message `Job` to the root actor, and we are
    expecting some response in `Future`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本质上，我们向根演员发送一个 `Job` 消息，并且我们期望在 `Future` 中得到一些响应。
- en: '`Job` is an ordinary case class that specifies the URL we are going to scrape,
    as well as the depth to which we are going to descend. The semantics of these
    two parameters are the same as that in the sequential example.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`Job` 是一个普通的案例类，它指定了我们将要抓取的 URL 以及将要下探的深度。这两个参数的语义与顺序示例中的相同。'
- en: After that, we call the `onSuccess` method on `Future` to set the callbacks.
    So, once the result arrives at our location, we are going to report to the standard
    output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们在 `Future` 上调用 `onSuccess` 方法来设置回调。所以，一旦结果到达我们的位置，我们将向标准输出报告。
- en: 'Now, let''s have a look at how the worker actor is implemented and how it works:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看工作演员是如何实现以及它是如何工作的：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, the `receive` method is implemented in terms of another
    method called `awaitingForTasks`. This is because we are going to change the implementation
    of `receive` in the future based on the information that is sent to the actor
    in the messages. Basically, our actor is going to have several states in which
    it is going to accept different sets of messages and react to them differently,
    but the idea is to encapsulate every state in a separate callback method, and
    then switch between the implementations of these callbacks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`receive` 方法是用另一个称为 `awaitingForTasks` 的方法实现的。这是因为我们打算根据发送给演员的消息更改 `receive`
    的实现。基本上，我们的演员将处于几个状态，在这些状态下它将接受不同的消息集合并以不同的方式对它们做出反应，但想法是将每个状态封装在单独的回调方法中，然后在这些回调的实现之间切换。
- en: 'Let''s take a look at the default state of the actor, `awaitingForTasks`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看演员的默认状态，`awaitingForTasks`：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`awaitingForTasks` specifies how an actor should react when it receives the
    `Job` message. The message is supposed to tell the actor to start crawling a certain
    URL for a certain depth. Also, as you can see, we store all the visited nodes
    in a `visited` collection. Visited is a set of all visited URLs, and the semantics
    of it and some motivation for it are the same as in the sequential example, to
    avoid repeating unnecessary work.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`awaitingForTasks` 指定了当演员接收到 `Job` 消息时应该如何反应。消息应该告诉演员开始以特定深度爬取某个 URL。此外，正如你所看到的，我们将所有已访问的节点存储在
    `visited` 集合中。`Visited` 是所有已访问 URL 的集合，它的语义和动机与顺序示例中的相同，以避免重复不必要的操作。'
- en: After that, we set the `replyTo` variable. It specifies the sender of the task,
    which is interested in receiving the result of our crawling.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们设置了 `replyTo` 变量。它指定了任务的发送者，该发送者有兴趣接收我们爬取的结果。
- en: After setting this variable, we start the process of crawling. First of all,
    we are using the `fetch` method we are already familiar with from the sequential
    example to obtain the set of all the links present on a given URL's page, and
    filter out the links that we have already visited.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置这个变量之后，我们开始爬取过程。首先，我们使用从顺序示例中已经熟悉的 `fetch` 方法来获取给定 URL 页面上所有存在的链接集合，并过滤掉我们已访问的链接。
- en: 'After that, similar to our sequential example, we check whether the depth permits
    further descent, and if it does, we are going to perform the recursive processing
    of links as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，类似于我们的顺序示例，我们检查深度是否允许进一步下降，如果允许，我们将按照以下方式执行链接的递归处理：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: First, we are going to define an empty set of the child actors so that we can
    keep track of their processing and be able to control our own state depending
    on the state change of the child actors. For example, we must know when exactly
    to report to the requesting actor with the results of the job. That must be done
    only once all the child actors have finished their work.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个空的子演员集合，这样我们就可以跟踪它们的处理情况，并根据子演员的状态变化来控制我们自己的状态。例如，我们必须知道何时确切地向请求演员报告工作结果。这必须在所有子演员完成工作后只做一次。
- en: 'Also, we set the `answered` variable to `0`. This is a variable to keep track
    of the number of actors who have successfully replied to this actor with the result
    of their processing. The idea is that once this metric reaches the `children` size
    number, we are going to reply to the `replyTo` actor with the result of the processing.
    The most interesting method here is `dispatch`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将 `answered` 变量设置为 `0`。这是一个跟踪成功回复此演员处理结果的演员数量的变量。想法是，一旦这个指标达到 `children`
    大小，我们将用处理结果回复 `replyTo` 演员最有趣的方法是 `dispatch`：
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So, `dispatch` creates a new worker actor and adds it to the set of all the
    child actors, and finally, they are requested to perform a processing job on a
    given URL. A separate worker actor is initialized for a separate URL.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`dispatch` 创建一个新的工作演员并将其添加到所有子演员的集合中，最后，它们被要求对一个给定的 URL 执行处理工作。为每个单独的 URL
    初始化一个单独的工作演员。
- en: Finally, let's pay attention to `context become processing` line of the `Job`
    clause. Essentially, `context become` is a method that switches the implementation
    of this actor's `receive`. Previously, we had an implementation of `awaitingForTasks`.
    However, now we are switching it to `processing`, which we will be discussing
    further in this chapter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们关注一下 `Job` 子句中的 `context become processing` 行。本质上，`context become` 是一个切换此演员
    `receive` 实现的方法。之前，我们有一个 `awaitingForTasks` 的实现。然而，现在我们将它切换到 `processing`，我们将在本章中进一步讨论。
- en: 'But before discussing it, let''s have a look at their `else` branch of our
    `if` statement:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 但在讨论它之前，让我们看看我们的 `if` 语句的 `else` 分支：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So, as we can see, once you have reached a certain depth, we are going to return
    the links collected into a buffer to the requesting actor.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如我们所见，一旦达到一定的深度，我们将返回收集到的链接到请求演员的缓冲区中。
- en: 'Now let''s have a look at the `processing` state of the actor:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看演员的 `processing` 状态：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, once this actor becomes processing actor, it is going to react
    to the `Result` messages, and it will stop reacting to the `Job` messages. This
    means that once you have sent a `Job` message to the actor and it starts processing
    it, it will no longer accept any other job request.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一旦这个演员成为处理演员，它将响应 `Result` 消息，并且它将停止响应 `Job` 消息。这意味着一旦您向演员发送了 `Job` 消息并且它开始处理它，它将不再接受任何其他工作请求。
- en: In the body of `processing`, we make sure that the `replyTo` actor is set. In
    principle, it should be set at all times once we reach this point. However, `replyTo`
    is an `Option`, and a nice way to handle optionality would be to have a `match`
    statement that explicitly checks for this `Option` to be defined. You never know
    what bugs can occur in such a program, so it's better to be safe than sorry.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `processing` 的主体中，我们确保 `replyTo` 演员被设置。原则上，一旦我们达到这个点，它应该始终被设置。然而，`replyTo`
    是一个 `Option`，处理可选性的好方法是有一个 `match` 语句，该语句明确检查这个 `Option` 是否已定义。你永远不知道这样的程序中可能会出现什么错误，所以最好是安全第一。
- en: The logic of `processing` is as follows. The `Result` is a message that is supposed
    to arrive at this actor from its child actors. First, we are going to increment
    the number of actors that have answered this actor. We are doing that with `answered
    += 1`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`processing` 的逻辑如下。`Result` 是一个应该从其子演员到达此演员的消息。首先，我们将回答此演员的演员数量增加。我们通过 `answered
    += 1` 来做这件事。'
- en: After some debugging output, we add the payload that was sent to this actor
    by the child actor to the set of all the links collected by this actor—`buffer
    ++ = urls`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些调试输出之后，我们将子演员发送给此演员的有效负载添加到此演员收集的所有链接集合中——`buffer ++ = urls`。
- en: Finally, we check whether all of the child actors have replied. We do so by
    checking the `answered` counter to be equal to the size of all the children. If
    it is, then we respond to the requesting actor with the links that we have collected,
    `to ! Result(buffer)`, and finally we stop this actor because it has nothing else
    to do, `context stop self`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检查所有子演员是否都已回复。我们通过检查`answered`计数器是否等于所有子演员的大小来实现这一点。如果是，我们就向请求的演员发送我们收集的链接，`to
    ! Result(buffer)`，然后最终停止此演员，因为它没有其他事情可做，`context stop self`。
- en: 'The result of running this actor system is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此演员系统的结果如下：
- en: '![](img/57ec1db9-1c9d-4473-a6a5-b3fd289d3800.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57ec1db9-1c9d-4473-a6a5-b3fd289d3800.png)'
- en: Caveats
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意事项
- en: Actor applications, although much more convenient to write and reason about
    compared to synchronization-based applications, are still much trickier than your
    ordinary sequential applications. In this section, we will discuss some caveats
    of the application.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于同步的应用程序相比，虽然演员应用程序编写和推理更为方便，但它们仍然比普通的顺序应用程序复杂得多。在本节中，我们将讨论一些应用程序的注意事项。
- en: Visited links
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问过的链接
- en: The caveat that has the most impact here is the partiality of tracking of the
    actors that this actor has already visited. If you remember, in the sequential
    example, we are using the `foldLeft` function to accumulate the results of every
    URL processing, and at all times we have a complete, up-to-date list of all the
    URLs collected by the entire application. This means the recursive crawling calls
    always have a full picture of what the application has collected so far.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最具影响力的注意事项是跟踪已访问演员的局部性。如果你还记得，在顺序示例中，我们使用`foldLeft`函数来累积每个URL处理的结果，并且我们始终有一个完整、最新的所有URL列表，这是整个应用程序收集的。这意味着递归爬取调用总是对应用程序迄今为止收集的内容有一个全面的了解。
- en: 'In the diagram, we see a sequential example of processing with `foldLeft`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们看到一个使用`foldLeft`进行处理的顺序示例：
- en: '![](img/51aacb5f-28ed-45a8-8e98-195ebc6998cd.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51aacb5f-28ed-45a8-8e98-195ebc6998cd.png)'
- en: All the nodes that were already processed are highlighted in green, and the
    current URL is highlighted in blue. The current URL has the entire list of links
    that were collected previously. So, it is not going to process them. This situation
    is possible because the processing is done sequentially.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有已处理的节点都用绿色突出显示，当前URL用蓝色突出显示。当前URL包含之前收集的所有链接列表。因此，它不会处理它们。这种情况是可能的，因为处理是顺序进行的。
- en: 'However, the situation is different for the parallel example described in the
    following diagram:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以下图中描述的并行示例情况则不同：
- en: '![](img/87ba4746-11ba-42f2-8e09-1a0afa45c645.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87ba4746-11ba-42f2-8e09-1a0afa45c645.png)'
- en: In the preceding diagram, in blue, you can see the current node. The green nodes
    are all the nodes the current node is aware of. Notice that although it is aware
    of its sibling nodes, it is not aware of the result of processing these sibling
    nodes. This is because the processing is done in parallel. In the sequential example,
    we had a depth-first tree-traversal from left to right. However, in the parallel
    example, the children are processed in parallel with each other. This means that
    one node may have information about its siblings. However, it will not have the
    information of the results collected from its siblings. This is because these
    results are collected in parallel with this node computing its own results. And
    we do not know which node will finish first. This means the preceding application
    is not ideal in terms of eliminating redundant work.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，蓝色部分显示了当前节点。绿色节点是当前节点所了解的所有节点。请注意，尽管它了解其兄弟节点，但它并不了解处理这些兄弟节点的结果。这是因为处理是并行的。在顺序示例中，我们有一个从左到右的深度优先树遍历。然而，在并行示例中，子节点是相互并行处理的。这意味着一个节点可能了解其兄弟节点的信息。然而，它将不会了解从其兄弟节点收集的结果。这是因为这些结果是在此节点计算其自身结果的同时并行收集的。而且我们不知道哪个节点会先完成。这意味着前面的应用程序在消除冗余工作方面并不理想。
- en: The problem of storing all the links the actors have visited is a classic problem
    of a shared mutable resource. A solution to this problem within an actor model
    would be to create a single actor that has a list of all the links already visited
    and that do not require further visiting. And so, before descending through the
    tree, every actor should consult that actor on the subject of whether or not to
    process certain links.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 存储演员访问过的所有链接的问题是一个经典的共享可变资源问题。在演员模型中解决这个问题的一个方案是创建一个单独的演员，该演员有一个列表，列出了所有已经访问过且不需要进一步访问的链接。因此，在通过树下降之前，每个演员都应该就是否处理某些链接的问题咨询该演员。
- en: Another caveat that you should consider is fault tolerance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该考虑的另一个注意事项是容错性。
- en: Fault tolerance
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容错性
- en: 'To keep the example simple, in the parallel example we have used the `fetch`
    function from the sequential example to fetch the contents of a certain URL:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使示例简单，在并行示例中，我们使用了来自顺序示例的`fetch`函数来获取某个URL的内容：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The motivation for having this function return an `Option` was fault tolerance
    in the sequential example—if the result cannot be computed, we return a `None`.
    However, in an actor setting, Akka provides you with a framework to specify what
    to do if an actor has failed. So in principle, we could further refine our example
    with a dedicated `fetch` function that is perfectly capable of throwing exceptions.
    However, you might want to specify the actor-level logic on how to restart itself
    and how to keep its state through this kind of emergency.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数返回`Option`的动机是为了在顺序示例中的容错性——如果结果无法计算，我们返回`None`。然而，在演员设置中，Akka为你提供了一个框架来指定如果演员失败时应该做什么。所以原则上，我们可以进一步改进我们的示例，使用一个专门的`fetch`函数，这个函数能够完美地抛出异常。然而，你可能想要指定演员级别的逻辑，比如如何重启自身以及如何通过这种紧急情况保持其状态。
- en: Counting the responded actors
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数响应的演员
- en: 'In the example, we counted the child actors that responded to an actor in order
    to determine when an actor is ready to respond to its parent actor:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，我们计算了响应演员的子演员数量，以确定何时演员准备好对其父演员做出响应：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This scenario can give rise to certain undesirable outcomes. First of all, it
    means the time that it takes for the system to respond is equal to the time it
    takes for the deepest and slowest link to be resolved and processed.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景可能会产生某些不希望的结果。首先，这意味着系统响应所需的时间等于最深和最慢的链接被解决和处理所需的时间。
- en: 'The logic behind this reasoning is as follows. The `processing` callback determines
    when a  node of a processing tree is considered completed:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这种推理背后的逻辑如下。`processing`回调确定何时认为处理树中的一个节点已完成：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So a  node is completed whenever its slowest child is completed. However, we
    should remember that we are dealing with real-world processing, and we should
    not forget about all of the side effects that can happen in the real world. The
    first effect we've discussed is failures and errors. We address this side effect
    by engineering our application to be fault-tolerant. The other side effect is
    time. It can still be the case that some of the pages take unforgivably long to
    get fetched. So we must not forget that such a side effect can occur and might
    want to devise a strategy to tackle such side effects.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个节点完成，当且仅当其最慢的子节点完成。然而，我们应该记住，我们正在处理现实世界的处理，我们不应该忘记现实世界中可能发生的所有副作用。我们讨论的第一个副作用是失败和错误。我们通过设计我们的应用程序以具有容错性来处理这种副作用。另一个副作用是时间。仍然可能存在一些页面需要非常长的时间才能获取。因此，我们绝不能忘记这种副作用可能发生，并且可能需要制定策略来应对这种副作用。
- en: An intuitive strategy is timeouts. Just like in the case of fault tolerance,
    whenever a chunk of data takes too long to get processed, we can drop the chunk
    of data. The idea is that we still have enough data and for many applications,
    it is not crucial to have a 100% recall of all the target data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观的策略是超时。就像在容错性的情况下，每当一块数据处理时间过长时，我们可以丢弃这块数据。想法是，我们仍然有足够的数据，对于许多应用来说，并不需要100%地召回所有目标数据。
- en: Concretely, you might want to have a message scheduled to be sent to the current
    actor. Upon receiving this message, the actor will immediately send back all its
    results to the `replyTo` actor and terminate itself so that it doesn't react to
    any further messages. Also, the reaction to such a message may be to kill all
    the child actors recursively, because there is no longer a need for them to exist
    since they will not be able to report this error data through the parent actor.
    Another strategy may be to propagate this timeout message recursively to the child
    actors without killing them immediately. The child actors will then return whatever
    progress they have made at once and terminate.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你可能想安排一个消息发送到当前演员。在收到此消息后，演员将立即将其所有结果发送回`replyTo`演员并终止自己，以便它不会对任何后续消息做出反应。对此类消息的反应可能是递归地杀死所有子演员，因为它们不再需要存在，因为它们将无法通过父演员报告此错误数据。另一种策略可能是递归地将此超时消息传播到子演员，而不会立即杀死它们。然后，子演员将立即返回它们已经完成的任何进度并终止。
- en: Real-world side effects
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界的副作用
- en: In this subsection, we have already seen two instances of side effects—errors
    and time. The nature of the real world is such that often you don't know which
    side effects you will encounter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们已经看到了两种副作用的实例——错误和时间。现实世界的本质是，你往往不知道会遇到哪些副作用。
- en: Such side effects can include the necessity to limit the number of requests
    sent to a given domain because certain websites tend to block entities who send
    too many requests, or we might want to use proxy servers to access certain websites
    that are not accessible otherwise. Certain websites might have the data stored
    and fetched via Ajax, and ordinary techniques of scraping usually do not work.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些副作用可能包括限制发送到特定域的请求数量的必要性，因为某些网站倾向于阻止发送过多请求的实体，或者我们可能想使用代理服务器来访问其他方式无法访问的网站。某些网站可能通过Ajax存储和检索数据，常规的抓取技术通常不起作用。
- en: All these scenarios can be modeled as their own side effects. The idea when
    working with a real-world application is always to have a look at the side effects
    that might arise in your particular scenario.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些场景都可以建模为它们自己的副作用。当与实际应用一起工作时，始终要考虑你特定场景中可能出现的副作用。
- en: Once you have decided what you're going to encounter, you are able to decide
    how you are going to tackle and abstract the side effects. The tooling you can
    use includes the actor system's built-in capabilities or the capabilities of purely
    functional programming that we discussed in the first half of this book.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定了将要遇到什么，你就能决定如何处理和抽象副作用。你可以使用的工具包括演员系统的内置功能，或者我们在本书前半部分讨论的纯函数编程的能力。
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we looked at developing an application using Akka. First, we
    developed a sequential solution to the problem. Then, we determined the independent
    sub-tasks of this solution and discussed how to parallelize them. Finally, we
    devised a parallel solution using Akka.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用Akka开发应用程序。首先，我们开发了一个针对该问题的顺序解决方案。然后，我们确定了该解决方案的独立子任务，并讨论了如何并行化它们。最后，我们使用Akka设计了一个并行解决方案。
- en: Also, we discussed certain caveats that you might encounter when developing
    this kind of application. Most of them have to do with side effects that may occur,
    as well as actor-model-specific peculiarities when building parallel applications.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还讨论了在开发此类应用时可能会遇到的一些注意事项。其中大部分都与可能发生的副作用有关，以及构建并行应用时演员模型特有的特殊性。
