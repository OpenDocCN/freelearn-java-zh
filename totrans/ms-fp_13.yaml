- en: Use Case - A Parallel Web Crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the actors model and a framework that
    you can use to program with actors. However, the actor model is a paradigm just
    like functional programming. In principle, if you know the actors model from one
    language, you can use it in another language even if it doesn't have a framework
    that supports the actor model. This is because an actor model is an approach to
    reasoning about parallel computations, not some language-specific set of tools.
  prefs: []
  type: TYPE_NORMAL
- en: This state of things, just like functional programming, has its own benefits
    and drawbacks. The benefit is that you are not language-dependent if you depend
    on concepts. Once you know a concept, you can come to any programming language
    at all, and be capable of using them. However, the learning curve is steep. Precisely
    because it is all about paradigm and approach, it is not enough to install a library
    and start using it after skimming its documentation, as it is frequently the case
    with many other libraries or general-purpose languages that are similar one to
    another. Since it is all about shifting the paradigm, you need to apply some learning
    effort in order to understand the paradigm and how to use it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we built up a theoretical foundation on the toolset
    the actor model provides, as well as discussed the main concepts of the actor
    model and how to use it in practice. However, since it is a set of ideas and not
    just a library, it is necessary to develop an intuition for how it works. The
    best way to develop an intuition for a new paradigm is to have a look at some
    practical examples that will demonstrate how this paradigm is applicable in practice.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, in this chapter, we will be looking at a practical example
    of where an actors model can be applied. We will be looking at an example of web
    crawling.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parallel solution with Akka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be addressing the problem of building a web crawler.
    Web crawlers are important, for example, in the domain of indexing and searching
    the web.
  prefs: []
  type: TYPE_NORMAL
- en: The graph structure of the web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the websites can be imagined as a graph of pages. Every page contains some
    HTML markup and content. As part of this content, most web pages contain links
    to other pages. Since links are supposed to take you from one page to another,
    we can visualize the web as a graph. We can visualize links as edges that take
    you from one node to another node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given such a model for the entire internet, it''s possible to address the problem
    of searching for information over the web:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00be260a-4a75-4bdd-975a-388e35442e46.png)'
  prefs: []
  type: TYPE_IMG
- en: We are talking about a problem that is faced by the search engines. The objective
    of a search engine is to index the information stored online and devise algorithms
    that will find the required pages—nodes of the graph—efficiently based on certain
    queries by the end user. The algorithms required to index those nodes and to match
    the user requests to the information stored in the nodes are complex, and we will
    not be addressing them in this chapter. Often, these algorithms involve advanced
    machine learning and natural-language processing solutions to understand and evaluate
    the content stored in the pages. The brightest minds in machine learning and natural
    language processing work on search tasks for companies such as Google.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this chapter, we will be addressing a task that is also faced
    by search engines but is easier to tackle. Before indexing the information stored
    online, this information needs to be collected. A collection of information is
    a task of traversing the graph of all the pages and storing the contents in a
    database. The task of web crawling is precisely the task of traversing the graph
    starting from one node and following to other nodes through links. In the example
    in this chapter, we will not be storing website information in a database but
    will focus on the web crawling task.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting information from the graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Collecting information from a graph can be depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58b28bfb-db22-430a-9b50-688c11ad0df5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding illustration, the blue nodes are the ones currently being
    processed, the green ones are the ones that have already been processed, and the
    white ones are the ones not yet processed. This preceding task can be implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify which URL you would like to start from, that is, the starting node from
    which the traversal will start.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The crawler will visit the URL by issuing a `GET` request to it, and receive
    some HTML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9cd5cfe4-0b52-4532-b1cb-7493a19da150.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After receiving this HTML text, the crawler will extract the edges of the current
    node—the links to other nodes. The links are well defined in HTML markup. They
    are specified as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So, once we have the HTML markup, we can look it up for the contents matching
    the preceding pattern. After that, we may want to collect all of the links presented
    on the page into one single collection structure, for example, a list. Once we
    have that, we may want to execute the same operation recursively on every node
    linked to the current one.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel nature of the task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding task was chosen as an example for this chapter because it is parallelizable
    by nature. Basically, whenever you have a bunch of tasks that are not dependent
    one on another, it may be a good idea to parallelize the tasks. In our example,
    the preceding task can be very well conceptualized as a set of independent operations.
    A single unit of operation is the task of visiting a URL, abstracting all of the
    other nodes that are linked by the page of this URL, and then executing the same
    task on these URLs recursively.
  prefs: []
  type: TYPE_NORMAL
- en: This makes our crawling example a very good case for parallel actor application.
    Running all these tasks sequentially for every URL may be very time consuming,
    and therefore it is much more efficient to apply some strategy to the tasks. Even
    on a single-core CPU computer  processing power, even in the case when most of
    the tasks are processed in a simulated parallelism manner, the operation will
    still be dozens of times more efficient than in the sequential case. This is because
    a lot of waiting is involved in requesting HTML from the internet or a `GET` request.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the succeeding diagram, you can see an example of a request to have a process
    performed from a single thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c90f1fef-650c-4480-9c22-3a781cfad746.png)'
  prefs: []
  type: TYPE_IMG
- en: So, as you can see, the thread issues a request, and then it waits for the response
    to be derived. From your own browsing experience, you can easily say that sometimes
    a response from a website may take seconds to arrive. So essentially, these seconds
    will be wasted by the thread because it will block on this task, and the processor
    will do nothing useful at the time.
  prefs: []
  type: TYPE_NORMAL
- en: Even when you have a single core and even when you have only an option of simulated
    parallelism, the processor can still be utilized much more efficiently, if when
    one thread waits for the request to arrive at it, another thread issues a request
    or processes a response that has already arrived at it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the diagram, requests are issued much more efficiently,
    and while one thread waits for a request and sleeps, the processor is busy with
    other threads that have a real job to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68bdca1f-90ae-44a2-84c5-23f3594327a5.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, such kind of blocking can be very frequently the case. For example,
    you may have some operations that wait on a database to give a response, or on
    a message to arrive from the outer world. So even in the case of simulated parallelism,
    these kinds of situations can be greatly sped up. Needless to say, most modern
    machines work in a setting of several cores that will speed up the execution greatly.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the exact specification of the task, and we why exactly it
    makes sense to parallelize this task, let's get our hands dirty and see some implementation
    in action. However, before we dive into the details of the actor-based implementation
    of the task, let's first have a look at how this can be implemented sequentially,
    just so that we have a baseline to work against.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, let''s have a look at how we would like to use the solution once
    it is built. Also, let''s have a look at exactly which output we expect from the
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we perform the following things. First of all, we would
    like to be able to define our URL in terms of a Java-native URL object. In this
    example, we are using the Maven repository website as a crawling target. [mvnrepository.com](http://mvnrepository.com) is
    a website that provides an easy search through all of the Maven packages.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we have a call to `fetchToDepth`. This is the method that is supposed
    to do the actual work for us and actually crawls the website for links. As a first
    argument, we supply it a URL we want to process. As the second argument, we supply
    it the depth to which you want to search the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind having the depth is to avoid infinite recursion. The web is
    a very interconnected place. So, when we start from one node and begin to descend
    recursively into the nodes it is connected to, it may take a very long time to
    arrive at the terminal nodes, that is, a node that does not have any further links.
    It is quite possible that for the majority of the websites out there, such a lookup
    will be infinite or will take an unreasonably long time to accomplish. Therefore,
    we would like to limit our lookup depth. The semantics of how it will work is
    that for every edge the algorithm searches, the depth will be decreased by one.
    Once the depths reach zero, the algorithm will not try to follow the edges any
    further.
  prefs: []
  type: TYPE_NORMAL
- en: The result of this from function-execution will be a collection. Precisely,
    a set of links that were collected from this website. After that, we take the
    first 10 links and output them to the standard output, one link per line. Also,
    we output the number of links that the system managed to extract in total. We
    do not print all the links because there will be too many of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output we are aiming to get is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43cc0e31-f16f-4d3e-9980-5359a30fd57e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s explore the sequential implementation of the preceding objectives.
    Let''s have a look at `fetchToDepth`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the function accepts the three arguments. Two of them are the
    ones that we have already seen before and discussed in our example API usage.
    The third one is the set of URLs that the crawler has already visited. Why might
    we have such a set? Why does the crawler need it at all? How will it behave if
    we do not store the set of all visited links?
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the same set must be stored for any similar graph-traversal situation.
    The problem here is the presence of cycles in a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the diagram below, you can see how a graph with cycles might emerge in a
    web crawler application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8380c4c1-2f62-4079-8de7-7fb31dcfa4ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is enough for website **A** to reference website **B**, and then website
    **B** references website **A** for a cycle to emerge. If the graph has a cycle,
    there is a risk that our traversal algorithm will enter an infinite loop if it
    tries to traverse it without keeping track of the nodes that it has already visited.
    For this reason, we need to keep track of the visited nodes. The first thing to
    do in the `fetchToDepth` function is to fetch the given URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using the following function to do fetch the given URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `fetch` function is a function that takes a URL and outputs `Option[Set[URL]]`,
    the data structure with the links that this page contains.
  prefs: []
  type: TYPE_NORMAL
- en: Inside this function, nothing really remarkable happens except the side effects.
    We are using `Try` to capture the side effect of an error. Then, we convert `Try`
    to `Option` because we are not interested in the error type, as explained below.
  prefs: []
  type: TYPE_NORMAL
- en: The logic of this function is straightforward. We are using a popular Java library,
    called `Jsoap`, in order to connect to the given URL, issue a get request, retrieve
    all of the elements that have an `href` attribute that contains a link, and afterwards,
    map the resulting elements by extracting their links.
  prefs: []
  type: TYPE_NORMAL
- en: The logic is wrapped into `Try`, which is later converted into `Option`. Why
    have such a system involving `Try` and `Option`? As we learned in the first part
    of this book, both `Try` and `Option` are effect types. The entire idea behind
    an effect type is to abstract away some side effects that might happen. Since
    our business logic is wrapped into the `Try` effect, some side effects that are
    abstracted by `Try` might happen in this logic.
  prefs: []
  type: TYPE_NORMAL
- en: '`Try` abstracts away the possibility of an error or an exception. In fact,
    a lot of things can go wrong when we are performing a request to a remote server.
    For example, the request can time out, or we may lose internet connection, or
    the server can return an erroneous status code.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of massive data processing, things inevitably go around for some
    of the data nodes. This means that when performing the crawling operation discussed
    previously, we can be almost certain that sometimes it will fail. What happens
    if you design a function that does not account for such errors? Most likely, our
    algorithm will encounter an exception and will fail midway through the dataset.
    This is why fault tolerance is a crucial property of this kind of system.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, fault tolerance is achieved by wrapping the possibility
    of an error into the `Try` effect type. Another thing to notice here is not only
    the `Try` type but the fact that it is converted into `Option`. `Option` indicates
    that a function may or may not produce a result. In the context of converting
    from `Try` to `Option`, `Try` is mapped to `Some` if it is a successful attempt,
    and it is mapped to `None` if it is a failure. In the case of a failure, some
    information about the error that has happened gets lost when we convert `Try`
    to `Option`. This fact reflects the attitude that we don't care about the type
    of the error that might occur.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of attitude demands further explanation, as it is usually the case
    everywhere where we intentionally drop data. We discussed that in this kind of
    massive data-processing, errors are inevitable. Of course, it is nice to have
    your algorithm catch and handle gracefully most of the errors that it will encounter,
    however, it is not always possible. You cannot remove the fact that sooner or
    later the algorithm will encounter an error since the algorithm works not on some
    dummy data but on real-world websites. Whenever you are working with real-world
    data, errors are a fact of life, and you cannot plan to handle every possible
    error that occurs gracefully.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, a proper way to structure a program that is certain to encounter errors
    is fault tolerance. This means that we build the program as if it is certain to
    encounter errors and we plan for recovering from any error without actually specifying
    the type of the error. The fact that we are converting our `Try` to `Option` here
    reflects the attitude toward errors. The attitude is that we don't care about
    which error might happen during execution, we only care that an error can happen.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance is one of the fundamental principles of the design of Akka.
    Akka is built with fault tolerance and resilience in mind, which means that actors
    are designed as if they are certain to encounter errors, and Akka provides you
    with a framework to specify how to recover from them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now return to our `fetch` line from the `fetchToDepth` method  and see
    how exactly this attitude plays out in the logic of our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s return to our `fetchToDepth` example. After we perform the affect statement
    on a given URL, we also perform a `getOrElse` call on it. `getOrElse` is a method
    defined on a Scala `Option` that has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f482272d-d39f-40fc-ab68-f4b6fbea04c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So basically, this method tries to extract a value from `Option`. If the value
    is not present in `Option`, it outputs the default value that is supplied to the
    method. `getOrElse` is equivalent to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So now we have a set of links extracted from a given link. Let''s now have
    a look at how it is traversed in our sequential example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, you can see a chunk of the code in the `fetchToDepth`
    method. First of all, we check whether the depth is greater than `0`. Previously,
    we discussed how specifying depth constraints could save us from infinite execution.
  prefs: []
  type: TYPE_NORMAL
- en: The first statement after the `if` check is the current link accumulator, the
    `links` variable, being united with some larger right-hand statement. In the snippet
    above, only a fragment of this statement is presented, and we will discuss it
    step by step. The big picture is that this statement applies `fetchToDepth` recursively
    to all the links in the `links` set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the right side of the `++` operator takes every link from the
    set that we have retrieved and extracts all the links present in its page. This
    must be done recursively. But first, we need to clean up the set of all abstracted
    URL from the current link so that it doesn''t contain the links that we have previously
    visited. This is to tackle the problem of cycles in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have the resulting set further transformed by the call to the  `toList`
    and `zipWithIndex` methods. Basically, these two methods are required for logging
    purposes. Inside data-processing operations, you would like to have some reporting
    in order to keep track of the operations. In our case, we would like to assign
    a numeric ID to every link that we are going to visit, and we are going to log
    the fact that we are visiting a link with a given ID into the standard output.
    The signature of `zipWithIndex` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6548e0a2-7673-4eb5-b667-92c1edb3e21a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So basically, a list of elements, `A`, becomes a list of pairs of `A` and `Int`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, you can see the logic that actually performs the processing
    of every given URL. We are using a `foldLeft` method. Let''s have a look at its
    signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6848006-5a06-4b69-8003-96f70212be9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Basically, this method folds your entire collection into a single value. For
    a simpler example of its use case, consider the following chunk of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So basically, we start with some empty accumulator and specify what to do for
    each element given a value accumulated so far in scope. Every subsequent element
    of the list must be added to the accumulator.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our example of `fetchToDepth`, the justification for the usage
    of `foldLeft` is as follows. Ultimately, we have a set of links, and for every
    element of this set, we need to calculate the set of URLs it links to. However,
    we are interested in a combined set with all the links from all of the URLs we
    have just called. Hence, the example is similar to the example of the addition
    of integers. Except here, we are computing a union over a collection of sets of
    links.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now have a look at the code block that is passed as the second argument
    to `foldLeft`. First, we perform a logging statement. Next, we perform a step
    of calculating the links that belong to the current URL. We are then adding these
    links into the combined accumulator.
  prefs: []
  type: TYPE_NORMAL
- en: Notice also that before actually crawling a link, we perform a check to see
    whether it is contained already in the accumulator. It is crawled only if it is
    not contained in the current accumulator. This check is used to prevent redundant
    work. If the link is already contained in the accumulator, this means it was already
    processed by the algorithm since it is a depth-first traversal of the graph. So
    we do not need to process it again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, notice that we perform the processing using a recursive call to the `fetchToDepth`
    function with the depth reduced by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Finally, if the depth is `0`, we will return the set of links that we have extracted
    from the current page. Which means we will stop the algorithm at this point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire code for the `fetchToDepth` function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's discuss what a parallel solution to the preceding problem might
    look like.
  prefs: []
  type: TYPE_NORMAL
- en: A parallel solution with Akka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before approaching the problem of parallelizing our crawler, let's discuss our
    strategy of how we are going to approach it.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to model our problem of creating the actors' system with a tree
    of actors. This is because the task itself naturally forms a tree.
  prefs: []
  type: TYPE_NORMAL
- en: We have already discussed how all the links and pages on the internet comprise
    a graph. However, we have also discussed, in our example, two things that are
    not desirable when processing and traversing this graph—cycles and repetitions
    of work. So, if we have visited a certain node, we are not going to visit it anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that our graph becomes a tree on processing time. This means you
    cannot get to a parent node from a child node when descending from a child node. This
    tree may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c37fdf35-76cc-4ae1-bee1-f217dfb18333.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to communicate with the root-level node. So basically, we will
    send this node a link that we are going to start crawling from, and afterward,
    this actor is going to crawl it. Once it extracts all the links from that link,
    it will for each such link spawn a child actor that will do the same operation
    on that link. This is a recursive strategy.
  prefs: []
  type: TYPE_NORMAL
- en: These worker actors will report the results to their parent actors. The results
    from the bottom-most actors are going to get propagated to the top-level actors
    all the way through the tree.
  prefs: []
  type: TYPE_NORMAL
- en: In this implementation, notice how we do not restrict our usage of actors. Every
    link gets worked on by a dedicated actor, and we don't really care about how many
    actors will get spawned. This is because actors are very lightweight primitives,
    and you should not be afraid to be generous with spawning actors.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at how to implement this strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, let''s have a look at the API we want and its usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, you can see the main method of the parallelized actor
    application. As you can see, we create an actor system and a root-level worker
    actor. The idea is to represent all of the processing actors as a single class, `Worker`.
    The job of a single worker is to process a single URL and spawn additional workers
    to process the links extracted from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can see an example of querying the actor. First of all, we have the
    following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Ordinarily, you send messages to actors using the `!` operator. However, here
    we are using `?` because we are using an ask pattern. In essence, `?` is the act
    of sending a message to an actor and waiting for it to respond with another message.
    The ordinary `!` returns `Unit`, however, the `?` ask pattern returns a `Future`
    of a possible response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca415ddb-fd39-4a22-9aa4-5026564482b7.png)'
  prefs: []
  type: TYPE_IMG
- en: So essentially, we are sending a message `Job` to the root actor, and we are
    expecting some response in `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Job` is an ordinary case class that specifies the URL we are going to scrape,
    as well as the depth to which we are going to descend. The semantics of these
    two parameters are the same as that in the sequential example.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, we call the `onSuccess` method on `Future` to set the callbacks.
    So, once the result arrives at our location, we are going to report to the standard
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at how the worker actor is implemented and how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `receive` method is implemented in terms of another
    method called `awaitingForTasks`. This is because we are going to change the implementation
    of `receive` in the future based on the information that is sent to the actor
    in the messages. Basically, our actor is going to have several states in which
    it is going to accept different sets of messages and react to them differently,
    but the idea is to encapsulate every state in a separate callback method, and
    then switch between the implementations of these callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the default state of the actor, `awaitingForTasks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`awaitingForTasks` specifies how an actor should react when it receives the
    `Job` message. The message is supposed to tell the actor to start crawling a certain
    URL for a certain depth. Also, as you can see, we store all the visited nodes
    in a `visited` collection. Visited is a set of all visited URLs, and the semantics
    of it and some motivation for it are the same as in the sequential example, to
    avoid repeating unnecessary work.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, we set the `replyTo` variable. It specifies the sender of the task,
    which is interested in receiving the result of our crawling.
  prefs: []
  type: TYPE_NORMAL
- en: After setting this variable, we start the process of crawling. First of all,
    we are using the `fetch` method we are already familiar with from the sequential
    example to obtain the set of all the links present on a given URL's page, and
    filter out the links that we have already visited.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, similar to our sequential example, we check whether the depth permits
    further descent, and if it does, we are going to perform the recursive processing
    of links as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: First, we are going to define an empty set of the child actors so that we can
    keep track of their processing and be able to control our own state depending
    on the state change of the child actors. For example, we must know when exactly
    to report to the requesting actor with the results of the job. That must be done
    only once all the child actors have finished their work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we set the `answered` variable to `0`. This is a variable to keep track
    of the number of actors who have successfully replied to this actor with the result
    of their processing. The idea is that once this metric reaches the `children` size
    number, we are going to reply to the `replyTo` actor with the result of the processing.
    The most interesting method here is `dispatch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: So, `dispatch` creates a new worker actor and adds it to the set of all the
    child actors, and finally, they are requested to perform a processing job on a
    given URL. A separate worker actor is initialized for a separate URL.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's pay attention to `context become processing` line of the `Job`
    clause. Essentially, `context become` is a method that switches the implementation
    of this actor's `receive`. Previously, we had an implementation of `awaitingForTasks`.
    However, now we are switching it to `processing`, which we will be discussing
    further in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before discussing it, let''s have a look at their `else` branch of our
    `if` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: So, as we can see, once you have reached a certain depth, we are going to return
    the links collected into a buffer to the requesting actor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s have a look at the `processing` state of the actor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, once this actor becomes processing actor, it is going to react
    to the `Result` messages, and it will stop reacting to the `Job` messages. This
    means that once you have sent a `Job` message to the actor and it starts processing
    it, it will no longer accept any other job request.
  prefs: []
  type: TYPE_NORMAL
- en: In the body of `processing`, we make sure that the `replyTo` actor is set. In
    principle, it should be set at all times once we reach this point. However, `replyTo`
    is an `Option`, and a nice way to handle optionality would be to have a `match`
    statement that explicitly checks for this `Option` to be defined. You never know
    what bugs can occur in such a program, so it's better to be safe than sorry.
  prefs: []
  type: TYPE_NORMAL
- en: The logic of `processing` is as follows. The `Result` is a message that is supposed
    to arrive at this actor from its child actors. First, we are going to increment
    the number of actors that have answered this actor. We are doing that with `answered
    += 1`.
  prefs: []
  type: TYPE_NORMAL
- en: After some debugging output, we add the payload that was sent to this actor
    by the child actor to the set of all the links collected by this actor—`buffer
    ++ = urls`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we check whether all of the child actors have replied. We do so by
    checking the `answered` counter to be equal to the size of all the children. If
    it is, then we respond to the requesting actor with the links that we have collected,
    `to ! Result(buffer)`, and finally we stop this actor because it has nothing else
    to do, `context stop self`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of running this actor system is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57ec1db9-1c9d-4473-a6a5-b3fd289d3800.png)'
  prefs: []
  type: TYPE_IMG
- en: Caveats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actor applications, although much more convenient to write and reason about
    compared to synchronization-based applications, are still much trickier than your
    ordinary sequential applications. In this section, we will discuss some caveats
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Visited links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The caveat that has the most impact here is the partiality of tracking of the
    actors that this actor has already visited. If you remember, in the sequential
    example, we are using the `foldLeft` function to accumulate the results of every
    URL processing, and at all times we have a complete, up-to-date list of all the
    URLs collected by the entire application. This means the recursive crawling calls
    always have a full picture of what the application has collected so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the diagram, we see a sequential example of processing with `foldLeft`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51aacb5f-28ed-45a8-8e98-195ebc6998cd.png)'
  prefs: []
  type: TYPE_IMG
- en: All the nodes that were already processed are highlighted in green, and the
    current URL is highlighted in blue. The current URL has the entire list of links
    that were collected previously. So, it is not going to process them. This situation
    is possible because the processing is done sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the situation is different for the parallel example described in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87ba4746-11ba-42f2-8e09-1a0afa45c645.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, in blue, you can see the current node. The green nodes
    are all the nodes the current node is aware of. Notice that although it is aware
    of its sibling nodes, it is not aware of the result of processing these sibling
    nodes. This is because the processing is done in parallel. In the sequential example,
    we had a depth-first tree-traversal from left to right. However, in the parallel
    example, the children are processed in parallel with each other. This means that
    one node may have information about its siblings. However, it will not have the
    information of the results collected from its siblings. This is because these
    results are collected in parallel with this node computing its own results. And
    we do not know which node will finish first. This means the preceding application
    is not ideal in terms of eliminating redundant work.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of storing all the links the actors have visited is a classic problem
    of a shared mutable resource. A solution to this problem within an actor model
    would be to create a single actor that has a list of all the links already visited
    and that do not require further visiting. And so, before descending through the
    tree, every actor should consult that actor on the subject of whether or not to
    process certain links.
  prefs: []
  type: TYPE_NORMAL
- en: Another caveat that you should consider is fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To keep the example simple, in the parallel example we have used the `fetch`
    function from the sequential example to fetch the contents of a certain URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The motivation for having this function return an `Option` was fault tolerance
    in the sequential example—if the result cannot be computed, we return a `None`.
    However, in an actor setting, Akka provides you with a framework to specify what
    to do if an actor has failed. So in principle, we could further refine our example
    with a dedicated `fetch` function that is perfectly capable of throwing exceptions.
    However, you might want to specify the actor-level logic on how to restart itself
    and how to keep its state through this kind of emergency.
  prefs: []
  type: TYPE_NORMAL
- en: Counting the responded actors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the example, we counted the child actors that responded to an actor in order
    to determine when an actor is ready to respond to its parent actor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This scenario can give rise to certain undesirable outcomes. First of all, it
    means the time that it takes for the system to respond is equal to the time it
    takes for the deepest and slowest link to be resolved and processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic behind this reasoning is as follows. The `processing` callback determines
    when a  node of a processing tree is considered completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So a  node is completed whenever its slowest child is completed. However, we
    should remember that we are dealing with real-world processing, and we should
    not forget about all of the side effects that can happen in the real world. The
    first effect we've discussed is failures and errors. We address this side effect
    by engineering our application to be fault-tolerant. The other side effect is
    time. It can still be the case that some of the pages take unforgivably long to
    get fetched. So we must not forget that such a side effect can occur and might
    want to devise a strategy to tackle such side effects.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive strategy is timeouts. Just like in the case of fault tolerance,
    whenever a chunk of data takes too long to get processed, we can drop the chunk
    of data. The idea is that we still have enough data and for many applications,
    it is not crucial to have a 100% recall of all the target data.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, you might want to have a message scheduled to be sent to the current
    actor. Upon receiving this message, the actor will immediately send back all its
    results to the `replyTo` actor and terminate itself so that it doesn't react to
    any further messages. Also, the reaction to such a message may be to kill all
    the child actors recursively, because there is no longer a need for them to exist
    since they will not be able to report this error data through the parent actor.
    Another strategy may be to propagate this timeout message recursively to the child
    actors without killing them immediately. The child actors will then return whatever
    progress they have made at once and terminate.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world side effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we have already seen two instances of side effects—errors
    and time. The nature of the real world is such that often you don't know which
    side effects you will encounter.
  prefs: []
  type: TYPE_NORMAL
- en: Such side effects can include the necessity to limit the number of requests
    sent to a given domain because certain websites tend to block entities who send
    too many requests, or we might want to use proxy servers to access certain websites
    that are not accessible otherwise. Certain websites might have the data stored
    and fetched via Ajax, and ordinary techniques of scraping usually do not work.
  prefs: []
  type: TYPE_NORMAL
- en: All these scenarios can be modeled as their own side effects. The idea when
    working with a real-world application is always to have a look at the side effects
    that might arise in your particular scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have decided what you're going to encounter, you are able to decide
    how you are going to tackle and abstract the side effects. The tooling you can
    use includes the actor system's built-in capabilities or the capabilities of purely
    functional programming that we discussed in the first half of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at developing an application using Akka. First, we
    developed a sequential solution to the problem. Then, we determined the independent
    sub-tasks of this solution and discussed how to parallelize them. Finally, we
    devised a parallel solution using Akka.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we discussed certain caveats that you might encounter when developing
    this kind of application. Most of them have to do with side effects that may occur,
    as well as actor-model-specific peculiarities when building parallel applications.
  prefs: []
  type: TYPE_NORMAL
