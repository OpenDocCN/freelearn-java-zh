- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Centralized Logging with the EFK Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to collect and store log records from microservice
    instances, as well as how to search and analyze log records. As we mentioned in
    *Chapter 1*, *Introduction to Microservices*, it is difficult to get an overview
    of what is going on in a system landscape of microservices when each microservice
    instance writes log records to its local filesystem. We need a component that
    can collect the log records from the microservice’s local filesystem and store
    them in a central database for analysis, search, and visualization. A popular
    open-source-based solution for this is based on the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticsearch**, a distributed database with great capabilities for searching
    and analyzing large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fluentd**, a data collector that can be used to collect log records from
    various sources, filter and transform the collected information, and finally send
    it to various consumers, for example, Elasticsearch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**, a graphical frontend to Elasticsearch that can be used to visualize
    search results and run analyses of the collected log records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these tools are called the **EFK stack**, named after the initials
    of each tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Fluentd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the EFK stack on Kubernetes for development and test usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the collected log records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering log records from the microservices and finding related log records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing root cause analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For instructions on how to install the tools used in this book and how to access
    the source code for this book, see:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 21*, *Installation Instructions for macOS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 22*, *Installation Instructions for Microsoft Windows with WSL 2 and
    Ubuntu*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code examples in this chapter all come from the source code in `$BOOK_HOME/Chapter19`.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to view the changes applied to the source code in this chapter,
    that is, see the changes we made so that we can use the EFK stack for centralized
    log analysis, you can compare it with the source code for *Chapter 18*, *Using
    a Service Mesh to Improve Observability and Management*. You can use your favorite
    `diff` tool and compare the two folders, `$BOOK_HOME/Chapter18` and `$BOOK_HOME/Chapter19`.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Fluentd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn the basics of how to configure Fluentd. Before
    we do that, let’s learn a bit about the background of Fluentd and how it works
    at a high level.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Fluentd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Historically, one of the most popular open source stacks for handling log records
    has been the ELK stack from Elastic ([https://www.elastic.co](https://www.elastic.co)),
    based on Elasticsearch, Logstash (used for log collection and transformation),
    and Kibana. Since Logstash runs on a Java VM, it requires a relatively large amount
    of memory. Over the years, a number of open source alternatives have been developed
    that require significantly less memory than Logstash, one of them being Fluentd
    ([https://www.fluentd.org](https://www.fluentd.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd is managed by the **Cloud Native Computing Foundation** (**CNCF**) ([https://www.cncf.io](https://www.cncf.io)),
    the same organization that manages the Kubernetes project. Therefore, Fluentd
    has become a natural choice as an open-source-based log collector that runs in
    Kubernetes. Together with Elastic and Kibana, it forms the EFK stack.
  prefs: []
  type: TYPE_NORMAL
- en: CNCF maintains a list of alternative products for several categories, for example,
    for logging. For alternatives to Fluentd listed by CNCF, see [https://landscape.cncf.io/card-mode?category=logging&grouping=category](https://landscape.cncf.io/card-mode?category=logging&grouping=category).
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd is written in a mix of C and Ruby, using C for the performance-critical
    parts and Ruby where flexibility is of more importance, for example, allowing
    the simple installation of third-party plugins using Ruby’s `gem install` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'A log record is processed as an event in Fluentd and consists of the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: A `time` field describing when the log record was created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `tag` field that identifies what type of log record it is – the tag is used
    by Fluentd’s routing engine to determine how a log record will be processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `record` that contains the actual log information, which is stored as a JSON
    object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Fluentd configuration file is used to tell Fluentd how to collect, process,
    and finally send log records to various targets, such as Elasticsearch. A configuration
    file consists of the following types of core elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<source>`: Source elements describe where Fluentd will collect log records,
    for example, tailing log files that have been written to by Docker containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tailing a log file* means monitoring what is written to a log file. A frequently
    used Unix/Linux tool for monitoring what is appended to a file is named `tile`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Source elements typically tag the log records, describing the type of log record.
    They could, for example, be used to tag log records to state that they come from
    containers running in Kubernetes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`<filter>`: Filter elements are used to process the log records. For example,
    a filter element can parse log records that come from Spring Boot-based microservices
    and extract interesting parts of the log message into separate fields in the log
    record. Extracting information into separate fields in the log record makes the
    information searchable by Elasticsearch. A filter element selects the log records
    to process based on their tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<match>`: Match elements decide where to send log records, acting as output
    elements. They are used to perform two main tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending processed log records to targets such as Elasticsearch.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Routing to decide how to process log records. A routing rule can rewrite the
    tag and re-emit the log record into the Fluentd routing engine for further processing.
    A routing rule is expressed as an embedded `<rule>` element inside the `<match>`
    element. Output elements decide what log records to process, in the same way as
    a filter: based on the tag of the log records.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluentd comes with a number of built-in and external third-party plugins that
    are used by the source, filter, and output elements. We will see some of them
    in action when we walk through the configuration file in the next section. For
    more information on the available plugins, see Fluentd’s documentation, which
    is available at [https://docs.fluentd.org](https://docs.fluentd.org).
  prefs: []
  type: TYPE_NORMAL
- en: With this overview of Fluentd out of the way, we are ready to see how Fluentd
    can be configured to process the log records from our microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Fluentd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The configuration of Fluentd is based on the configuration files from a Fluentd
    project on GitHub, `fluentd-kubernetes-daemonset`. The project contains Fluentd
    configuration files for how to collect log records from containers that run in
    Kubernetes and how to send them to Elasticsearch once they have been processed.
  prefs: []
  type: TYPE_NORMAL
- en: We will reuse this configuration without changes, and it will simplify our own
    configuration to a great extent. The Fluentd configuration files can be found
    at [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/archived-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/archived-image/v1.4/debian-elasticsearch/conf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration files that provide this functionality are `kubernetes.conf`
    and `fluent.conf`. The `kubernetes.conf` configuration file contains the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: Source elements that tail container log files and log files from processes that
    run outside of Kubernetes, for example, `kubelet` and the Docker daemon. The source
    elements also tag the log records from Kubernetes with the full name of the log
    file with `/` replaced by `.` and prefixed with `kubernetes`. Since the tag is
    based on the full filename, the name contains the name of the namespace, pod,
    and container, among other things. So, the tag is very useful for finding log
    records of interest by matching the tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, the tag from the `product-composite` microservice could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`,
    while the tag for the corresponding `istio-proxy` in the same Pod could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A filter element that enriches the log records that come from containers running
    inside Kubernetes, along with Kubernetes-specific fields that contain information
    such as the names of the containers and the namespace they run in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main configuration file, `fluent.conf`, contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@include` statements for other configuration files, for example, the `kubernetes.conf`
    file we described previously. It also includes custom configuration files that
    are placed in a specific folder, making it very easy for us to reuse these configuration
    files without any changes and provide our own configuration file that only handles
    processing related to our own log records. We simply need to place our own configuration
    file in the folder specified by the `fluent.conf` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output element that sends log records to Elasticsearch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As described in the *Deploying Fluentd* section later on, these two configuration
    files will be packaged into the Docker image we will build for Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s left to cover in our own configuration file is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and parsing Spring Boot-formatted log records from our microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling multiline stack traces. Stack traces are written to log files using
    multiple lines. This makes it hard for Fluentd to handle a stack trace as a single
    log record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating log records from the `istio-proxy` sidecars from the log records
    that were created by the microservices running in the same Pod. The log records
    that are created by `istio-proxy` don’t follow the same pattern as the log patterns
    that are created by our Spring Boot-based microservices. Therefore, they must
    be handled separately so that Fluentd doesn’t try to parse them as Spring Boot-formatted
    log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve this, the configuration is, to a large extent, based on using the
    `rewrite_tag_filter` plugin. This plugin can be used for routing log records based
    on the concept of changing the name of a tag and then re-emitting the log record
    to the Fluentd routing engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This processing is summarized by the following UML activity diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, line  Description automatically
    generated](img/B19825_19_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.1: Fluentd processing of log records'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the design of the configuration file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The tags of all log records from Istio, including `istio-proxy`, are prefixed
    with `istio` so that they can be separated from the Spring Boot-based log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tags of all log records from the `hands-on` namespace (except for the log
    records from `istio-proxy`) are prefixed with `spring-boot`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log records from Spring Boot are checked for the presence of multiline stack
    traces. If the log record is part of a multiline stack trace, it is processed
    by the third-party `detect-exceptions` plugin to recreate the stack trace. Otherwise,
    it is parsed using a regular expression to extract information of interest. See
    the *Deploying Fluentd* section for details on this third-party plugin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `fluentd-hands-on.conf` configuration file implements this activity diagram.
    The configuration file is placed inside a Kubernetes ConfigMap (see `kubernetes/efk/fluentd-hands-on-configmap.yml`).
    Let’s go through this step by step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First comes the definition of the ConfigMap and the filename of the configuration
    file, `fluentd-hands-on.conf`. It looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the `data` element will contain the configuration of Fluentd.
    It starts with the filename and uses a vertical bar, `|`, to mark the beginning
    of the embedded configuration file for Fluentd.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first `<match>` element matches the log records from Istio, that is, tags
    that are prefixed with `Kubernetes` and contain `istio` as either part of their
    namespace or part of their container name. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s explain the preceding source code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `<match>` element matches any tags that follow the `kubernetes.**istio**`
    pattern, that is, tags that start with `Kubernetes` and then contain the word
    `istio` somewhere in the tag name. `istio` can come from the name of either the
    namespace or the container; both are part of the tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<match>` element contains only one `<rule>` element, which prefixes the
    tag with `istio`. The `${tag}` variable holds the current value of the tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this is the only `<rule>` element in the `<match>` element, it is configured
    to match all log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since all log records that come from Kubernetes have a `log` field, the `key`
    field is set to `log`, that is, the rule looks for a `log` field in the log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To match any string in the `log` field, the `pattern` field is set to the `^(.*)$`
    regular expression. `^` marks the beginning of a string, while `$` marks the end
    of a string. `(.*)` matches any number of characters, except for line breaks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log records are re-emitted to the Fluentd routing engine. Since no other
    elements in the configuration file match tags starting with `istio`, the log records
    will be sent directly to the output element for Elasticsearch, which is defined
    in the `fluent.conf` file we described previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second `<match>` element matches all log records from the `hands-on` namespace,
    that is, the log records that are emitted by our microservices. It looks like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the source code, we can see that:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The log records emitted by our microservices use formatting rules for the log
    message defined by Spring Boot, so their tags are prefixed with `spring-boot`.
    Then, they are re-emitted for further processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<match>` element is configured in the same way as the `<match kubernetes.**istio**>`
    element we looked at previously, to match all records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third `<match>` element matches `spring-boot` log records and determines
    whether they are ordinary Spring Boot log records or part of a multiline stack
    trace. Since Spring Boot 3, Project Reactor has added extra information to stack
    traces to clarify what caused an exception. (For details, see [https://projectreactor.io/docs/core/release/reference/#_reading_a_stack_trace_in_debug_mode](https://projectreactor.io/docs/core/release/reference/#_reading_a_stack_trace_in_debug_mode).)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To be able to parse the actual stack trace, we will filter out this information.
    The `<match>` element looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As seen in the source code, this is determined by using six `<rule>` elements:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The first uses a regular expression to check whether the `log` field in the
    log element starts with a timestamp or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the `log` field starts with a timestamp, the log record is treated as an
    ordinary Spring Boot log record and its tag is prefixed with `parse`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next follows four rule elements that are used to filter out the extra information
    added by Project Reactor; they all prefix the tag with `skip`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the last `<rule>` element will match, and the log record is handled
    as a multiline log record. Its tag is prefixed with `check.exception`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log record is re-emitted in either case and its tag will either start with
    `check.exception.spring-boot`, `skip.spring-boot,` or `parse.spring-boot` after
    this processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The fourth `<match>` element is used to get rid of the log output from Project
    Reactor, i.e. match tags starting with `skip.spring-boot`. The `<match>` element
    applies the `null` output plugin that throws away the events. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the fifth `<match>` element, the selected log records have a tag that starts
    with `check.exception.spring-boot`, that is, log records that are part of a multiline
    stack trace. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `detect_exceptions` plugin works like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `detect_exceptions` plugin is used to combine multiple one-line log records
    into a single log record that contains a complete stack trace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before a multiline log record is re-emitted into the routing engine, the `check`
    prefix is removed from the tag to prevent a never-ending processing loop of the
    log record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the configuration file consists of a `<filter>` element that parses
    Spring Boot log messages using a regular expression, extracting information of
    interest. It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that filter elements don’t re-emit log records; instead, they just pass
    them on to the next element in the configuration file that matches the log record’s
    tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following fields are extracted from the Spring Boot log message that’s
    stored in the `log` field in the log record:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<time>`: The timestamp for when the log record was created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.level>`: The log level of the log record: `FATAL`, `ERROR`, `WARN`,
    `INFO`, `DEBUG`, or `TRACE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.service>`: The name of the microservice'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.trace>`: The trace ID used to perform distributed tracing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.span>`: The span ID, the ID of the part of the distributed processing
    that this microservice executed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.pid>`: The process ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.thread>`: The thread ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.class>`: The name of the Java class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<log>`: The actual log message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The names of Spring Boot-based microservices are specified using the `spring.application.name`
    property. This property has been added to each microservice-specific property
    file in the config repository, in the `config-repo` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting regular expressions right can be challenging, to say the least. Thankfully,
    there are several websites that can help. When it comes to using regular expressions
    together with Fluentd, I recommend using the following site: [https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have been introduced to how Fluentd works and how the configuration
    file is constructed, we are ready to deploy the EFK stack.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the EFK stack on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deploying the EFK stack on Kubernetes will be done in the same way as we have
    deployed our own microservices: using Kubernetes manifest files for objects such
    as Deployments, Services, and ConfigMaps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment of the EFK stack is divided into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Elasticsearch and Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Fluentd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up access to Elasticsearch and Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But first, we need to build and deploy our own microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying our microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building, deploying, and verifying the deployment using the `test-em-all.bash`
    test script is done in the same way as it was done in *Chapter 18*, *Using a Service
    Mesh to Improve Observability and Management*, in the *Running commands to create
    the service mesh* section. These instructions assume that the cert-manager and
    Istio are installed as instructed in *Chapters 17* and *18*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, build the Docker images from the source with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `eval $(minikube docker-env -u)` command ensures that the `./gradlew build`
    command uses the host’s Docker engine and not the Docker engine in the Minikube
    instance. The `build` command uses Docker to run test containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recreate the namespace, `hands-on`, and set it as the default namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Resolve the Helm chart dependencies with the following commands.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we update the dependencies in the `components` folder:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we update the dependencies in the `environments` folder:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Deploy the system landscape using Helm and wait for all deployments to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Minikube tunnel in a separate terminal window, if it’s not already
    running (see *Chapter 18*, the *Setting up access to Istio services* section,
    for a recap, if required):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember that this command requires that your user has `sudo` privileges and
    that you enter your password during startup. It takes a couple of seconds before
    the command asks for the password, so it is easy to miss!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the normal tests to verify the deployment with the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect the output to be similar to what we saw in the previous chapters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_19_02.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.2: Tests running fine'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also try out the APIs manually by running the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Expect the requested product ID, `1`, in the response.
  prefs: []
  type: TYPE_NORMAL
- en: With the microservices deployed, we can move on and deploy Elasticsearch and
    Kibana!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Elasticsearch and Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will deploy Elasticsearch and Kibana to their own namespace, `logging`. Both
    Elasticsearch and Kibana will be deployed for development and test usage using
    a Kubernetes Deployment and Service object. The services will expose the standard
    ports for Elasticsearch and Kibana internally in the Kubernetes cluster, that
    is, port `9200` for Elasticsearch and port `5601` for Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: To provide external HTTP access to Elasticsearch and Kibana, we will create
    Istio objects as we did in *Chapter 18**, Using a Service Mesh to Improve Observability
    and Management*, for Kiali and Jaeger – see the *Setting up access to Istio services*
    section for a recap, if required. This will result in Elasticsearch and Kibana
    being available at [https://elasticsearch.minikube.me](https://elasticsearch.minikube.me)
    and [https://kibana.minikube.me](https://kibana.minikube.me).
  prefs: []
  type: TYPE_NORMAL
- en: The manifest files have been packaged in a Helm chart in the `kubernetes/helm/environments/logging`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: For recommended deployment options for Elasticsearch and Kibana in a production
    environment on Kubernetes, see [https://www.elastic.co/elastic-cloud-kubernetes](https://www.elastic.co/elastic-cloud-kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the latest versions that were available for version 7 when this
    chapter was written:'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch version 7.17.10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana version 7.17.10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch version 8 is not used, due to limited support in the Fluentd plugin
    for Elasticsearch; see [https://github.com/uken/fluent-plugin-elasticsearch/issues/1005](https://github.com/uken/fluent-plugin-elasticsearch/issues/1005).
    The base Docker image, `fluentd-kubernetes-daemonset`, that we will use in the
    following section to install Fluentd uses this plugin.
  prefs: []
  type: TYPE_NORMAL
- en: Before we deploy, let’s look at the most interesting parts of the manifest files
    in the Helm chart’s `template` folder.
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the manifest files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The manifest file for Elasticsearch, `elasticsearch.yml`, contains a standard
    Kubernetes Deployment and Service object that we have seen multiple times before,
    for example, in *Chapter 15*, *Introduction to Kubernetes*, in the *Trying out
    a sample deployment* section. The most interesting part of the manifest file is
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explain some of this manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: We use an official Docker image from Elastic that’s available at `docker.elastic.co`.
    The version is set to `7.17.10`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Elasticsearch container is allowed to allocate a relatively large amount
    of memory – 2 GB – to be able to run queries with good performance. The more memory,
    the better the performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The manifest file for Kibana, `kibana.yml`, also contains a standard Kubernetes
    Deployment and Service object. The most interesting parts in the manifest file
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explain some of the manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: For Kibana, we also use an official Docker image from Elastic that’s available
    at `docker.elastic.co`. The version is set to `7.17.10`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To connect Kibana with the Elasticsearch Pod, an environment variable, `ELASTICSEARCH_URL`,
    is defined to specify the address to the Elasticsearch service, `http://elasticsearch:9200`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the Istio manifests for setting up external access are found in the
    files `expose-elasticsearch.yml` and `expose-kibana.yml`. For a recap on how the
    `Gateway`, `VirtualService`, and `DestinationRule` objects are used, see the section
    *Creating the service mesh* in *Chapter 18*. They will provide the following forwarding
    of external requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://elasticsearch.minikube.me](https://elasticsearch.minikube.me) → `http://elasticsearch:9200`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://kibana.minikube.me](https://kibana.minikube.me) → `http://kibana:5601`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these insights, we are ready to perform the deployment of Elasticsearch
    and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Running the deploy commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deploy Elasticsearch and Kibana by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the deployment steps run faster, prefetch the Docker images for Elasticsearch
    and Kibana with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the Helm chart to create the `logging` namespace, deploy Elasticsearch
    and Kibana in it, and wait for the Pods to be ready:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that Elasticsearch is up and running with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Expect `You Know, for Search` as a response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Depending on your hardware, you might need to wait for a minute or two before
    Elasticsearch responds with this message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Kibana is up and running with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Expect `200` as the response.
  prefs: []
  type: TYPE_NORMAL
- en: Again, you might need to wait for a minute or two before Kibana is initialized
    and responds with `200`.
  prefs: []
  type: TYPE_NORMAL
- en: With Elasticsearch and Kibana deployed, we can start to deploy Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Fluentd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying Fluentd is a bit more complex compared to deploying Elasticsearch
    and Kibana. To deploy Fluentd, we will use a Docker image that’s been published
    by the Fluentd project on Docker Hub, `fluent/fluentd-kubernetes-daemonset`, and
    the sample Kubernetes manifest files from a Fluentd project on GitHub, `fluentd-kubernetes-daemonset`.
    It is located at [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
    As is implied by the name of the project, Fluentd will be deployed as a DaemonSet,
    running one Pod per Node in the Kubernetes cluster. Each Fluentd Pod is responsible
    for collecting log output from processes and containers that run on the same Node
    as the Pod. Since we are using Minikube with a single Node cluster, we will only
    have one Fluentd Pod.
  prefs: []
  type: TYPE_NORMAL
- en: To handle multiline log records that contain stack traces from exceptions, we
    will use a third-party Fluentd plugin provided by Google, `fluent-plugin-detect-exceptions`,
    which is available at [https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions).
    To be able to use this plugin, we will build our own Docker image where the `fluent-plugin-detect-exceptions`
    plugin will be installed.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd’s Docker image, `fluentd-kubernetes-daemonset`, will be used as the
    base image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following versions:'
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd version 1.4.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fluent-plugin-detect-exceptions` version 0.0.12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we deploy, let’s look at the most interesting parts of the manifest files.
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the manifest files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Dockerfile that’s used to build the Docker image, `kubernetes/efk/Dockerfile`,
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explain this in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The base image is Fluentd’s Docker image, `fluentd-kubernetes-daemonset`. The
    `v1.4.2-debian-elasticsearch-1.1` tag specifies that version 1.4.2 will be used
    with a package that contains built-in support for sending log records to Elasticsearch.
    The base Docker image contains the Fluentd configuration files that were mentioned
    in the *Configuring Fluentd* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Google plugin, `fluent-plugin-detect-exceptions`, is installed using Ruby’s
    package manager, `gem`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The manifest file of the DaemonSet, `kubernetes/efk/fluentd-ds.yml`, is based
    on a sample manifest file in the `fluentd-kubernetes-daemonset` project, which
    can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: 'This file is a bit complex, so let’s go through the most interesting parts
    separately:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, here’s the declaration of the DaemonSet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `kind` key specifies that this is a DaemonSet. The `namespace` key specifies
    that the DaemonSet will be created in the `kube-system` namespace and not in the
    `logging` namespace where Elasticsearch and Kibana are deployed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next part specifies the template for the Pods that are created by the DaemonSet.
    The most interesting parts are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Docker image that’s used for the Pods is `hands-on/fluentd:v1`. We will
    build this Docker image after walking through the manifest files using the Dockerfile
    we described previously.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A number of environment variables are supported by the Docker image and are
    used to customize it. The two most important ones are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`FLUENT_ELASTICSEARCH_HOST`, which specifies the hostname of the Elasticsearch
    service, `elasticsearch.logging`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLUENT_ELASTICSEARCH_PORT`, which specifies the port that’s used to communicate
    with Elasticsearch, `9200`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the Fluentd Pod runs in a different namespace to Elasticsearch, the hostname
    cannot be specified using its short name, that is, `elasticsearch`. Instead, the
    namespace part of the DNS name must also be specified, that is, `elasticsearch.logging`.
    As an alternative, the **fully qualified domain name** (**FQDN**), `elasticsearch.logging.svc.cluster.local`,
    can also be used. But since the last part of the DNS name, `svc.cluster.local`,
    is shared by all DNS names inside a Kubernetes cluster, it does not need to be
    specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a number of volumes, that is, filesystems, are mapped to the Pod,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a look at the source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Three folders on the host (that is, the Node) are mapped to the Fluentd Pod.
    These folders contain the log files that Fluentd will tail and collect log records
    from. The folders are `/var/log`, `/var/lib/docker/containers`, and `/run/log/journal`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our own configuration file, which specifies how Fluentd will process log records
    from our microservices, is mapped using a ConfigMap called `fluentd-hands-on-config`
    to the `/fluentd/etc/conf.d` folder. The base Docker image configures Fluentd
    to include any configuration file that’s found in the `/fluentd/etc/conf.d` folder.
    See the *Configuring Fluentd* section for details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code of the manifest file for the DaemonSet, see the `kubernetes/efk/fluentd-ds.yml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve walked through everything, we are ready to perform the deployment
    of Fluentd.
  prefs: []
  type: TYPE_NORMAL
- en: Running the deploy commands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy Fluentd, we have to build the Docker image, create the ConfigMap,
    and finally deploy the DaemonSet. Run the following commands to perform these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the Docker image and tag it with `hands-on/fluentd:v1` using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the ConfigMap, deploy Fluentd’s DaemonSet, and wait for the Pod to be
    ready with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the Fluentd Pod is healthy with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect a response of `2023-05-22 14:59:46 +0000 [info]: #0 fluentd worker is
    now running worker=0`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As for Elasticsearch and Kibana, you might need to wait for a minute or two
    before Fluentd responds with this message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fluentd will start to collect a considerable number of log records from the
    various containers in the Minikube instance. After a minute or so, you can ask
    Elasticsearch how many log records have been collected with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The command can be a bit slow the first time it is executed but should return
    a total count of several thousands of log records. In my case, it returned `55607`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This completes the deployment of the EFK stack. Now, it’s time to try it out
    and find out what all the collected log records are about!
  prefs: []
  type: TYPE_NORMAL
- en: Trying out the EFK stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we need to do before we can try out the EFK stack is to initialize
    Kibana so it knows what indices to use in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: An **index** in Elasticsearch corresponds to a **database** in SQL concepts.
    The SQL concepts **table**, **row**, and **column** correspond to **type**, **document**,
    and **property** in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that is done, we will try out the following common tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: We will start by analyzing what types of log records Fluentd has collected and
    stored in Elasticsearch. Kibana has a very useful visualization capability that
    can be used for this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will learn how to find all related log records created by the microservices
    while processing an external request. We will use the **trace ID** in the log
    records as a correlation ID to find related log records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we will learn how to use Kibana to perform **root cause analysis**,
    finding the actual reason for an error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start to use Kibana, we must specify what search indices to use in
    Elasticsearch and what field in the indices holds the timestamps for the log records.
  prefs: []
  type: TYPE_NORMAL
- en: Just a quick reminder that we are using a certificate created by our own CA,
    meaning that it is not trusted by web browsers! For a recap on how to make web
    browsers accept our certificate, see the *Observing the service mesh* section
    of *Chapter 18*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to initialize Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Kibana’s web UI using the [https://kibana.minikube.me](https://kibana.minikube.me)
    URL in a web browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Welcome home** page, click on the hamburger menu **≡** (three horizontal
    lines) in the upper-left corner, and click on **Stack Management** at the bottom
    of the menu to the left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Management** menu, go to the bottom and select **Index Patterns**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the button named **Create index pattern**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `logstash-*` as the index pattern name and click on the **Next Step**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indices are, by default, named `logstash` for historical reasons, even though
    Fluentd is used for log collection.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the drop-down list for the **Timestamp field** and select the only
    available field, `@timestamp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Create index pattern** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kibana will show a page that summarizes the fields that are available in the
    selected indices.
  prefs: []
  type: TYPE_NORMAL
- en: With Kibana initialized, we are ready to examine the collected log records.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the log records
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the deployment of Fluentd, we know that it immediately started to collect
    a significant number of log records. So, the first thing we need to do is get
    an understanding of what types of log records Fluentd has collected and stored
    in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Kibana’s visualization feature to divide the log records by the
    Kubernetes namespace and then ask Kibana to show us how the log records are divided
    by the type of container within each namespace. A pie chart is a suitable chart
    type for this type of analysis. Perform the following steps to create a pie chart:'
  prefs: []
  type: TYPE_NORMAL
- en: In Kibana’s web UI, click on the hamburger menu again and select **Visualize
    Library** under **Analytics** in the menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Create new visualization** button and select the **Lens** type
    on the next page. A web page like the following will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.3: Starting to analyze log records in Kibana'
  prefs: []
  type: TYPE_NORMAL
- en: Verify that **logstash-*** is the selected index pattern in the top-left drop-down
    menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Bar vertical stacked** drop-down menu next to the index pattern, select
    **Pie** as the visualization type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the time picker (a date interval selector) above the pie chart, set a date
    interval large enough to cover log records of interest (set to the **Last 15 minutes**
    in the following screenshot). Click on its calendar icon to adjust the time interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the field named **Search field names** below the index pattern, enter `kubernetes.namespace_name.keyword`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **Available fields** list, the field **kubernetes.namespace_name.keyword**
    is now present. Drag this field into the big box in the middle of the page, named
    **Drop some fields here to start**. Kibana will immediately start to analyze log
    records and render a pie chart divided into Kubernetes namespaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In my case, it looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_04.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.4: Kibana analysis of log records per Kubernetes namespace'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can see that the log records are divided into the Namespaces we have been
    working with in the previous chapters: `kube-system`, `istio-system`, `logging`,
    and our own `hands-on` Namespace. To see what containers have created the log
    records per Namespace, we need to add a second field.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the **Search field names** field, enter `kubernetes.container_name.keyword`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Available fields** list, the field `kubernetes.container_name.keyword`
    is now present. Drag this field into the big box in the middle of the page showing
    the pie chart. Kibana will immediately start to analyze log records and render
    a pie chart divided by Kubernetes namespace and container name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the result of *step 9*, we can see a lot of log records coming from `coredns`,
    67% in my case. Since we are not particularly interested in these log records,
    we can remove them by adding a filter with the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **+ Add filter** (in the top-left corner).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Field** `kubernetes.container_name.keyword` and the **is not -****Operator**.
    Finally, enter the **Value** `coredns` and click on the **Save** button.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In my case, the rendered pie chart now looks like this:![A screenshot of a computer  Description
    automatically generated with medium confidence](img/B19825_19_05.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.5: Kibana analysis of log records per namespace and container'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can find the log records from our microservices. Most of the log records
    come from the `review` and `recommendation` microservices. The `product` and `product-composite`
    microservices can be found under the **other** section of the pie chart.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Wrap up this introduction to how to analyze what types of log records we have
    collected by saving this pie chart in a dashboard. Click on the **Save** button
    in the top-right corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the page named **Save Lens visualization**, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give it a **Title**, for example, `hands-on-visualization`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a **Description**, for example, `This is my first visualization in Kibana`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Add to dashboard** box, select **New**. The page should look like
    this:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_19_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.6: Creating a dashboard in Kibana'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the button named **Save and go to Dashboard**. A dashboard like the
    following should be presented:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.7: The new dashboard in Kibana'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Save** button in the top-right corner, give the dashboard a name,
    for example, `hands-on-dashboard`, and click on the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can now always go back to this dashboard by selecting **Dashboard** from
    the hamburger menu.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana contains tons of features for analyzing log records – feel free to try
    them out on your own. For inspiration, see [https://www.elastic.co/guide/en/kibana/7.17/dashboard.html](https://www.elastic.co/guide/en/kibana/7.17/dashboard.html).
    We will now move on and start to locate the actual log records from our microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the log records from microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will learn how to utilize one of the main features of centralized
    logging, finding log records from our microservices. We will also learn how to
    use the trace ID in the log records to find log records from other microservices
    that belong to the same process, for example, processing an external request sent
    to the public API.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by creating some log records that we can look up with the help of
    Kibana. We will use the API to create a product with a unique product ID and then
    retrieve information about the product. After that, we can try to find the log
    records that were created when retrieving the product information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creation of log records in the microservices has been updated a bit from
    the previous chapter so that the `product-composite` and the three core microservices,
    `product`, `recommendation`, and `review`, all write a log record with the log
    level set to `INFO` when they begin processing a `get` request. Let’s go over
    the source code that’s been added to each microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Product composite microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Product microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Recommendation microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Review microservice log creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For more details, see the source code in the `microservices` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to use the API to create log records and, after
    that, use Kibana to look up the log records:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get an access token with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As mentioned in the introduction to this section, we will start by creating
    a product with a unique product ID. Create a minimalistic product (without recommendations
    and reviews) for `"productId" :1234` by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the product with the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect a response similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B19825_19_08.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.8: Look up the product with productId = 1234'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hopefully, we got some log records created by these API calls. Let’s jump over
    to Kibana and find out!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the Kibana web page, click **Discover** from the hamburger menu. You will
    see something like the following:![A screenshot of a computer  Description automatically
    generated with medium confidence](img/B19825_19_09.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.9: Kibana web UI with its major parts'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the top-left corner, we can see that Kibana has found **5,350** log records.
    The time picker shows that they are from the **Last 15 minutes**. In the histogram,
    we can see how the log records are spread out over time. Below the histogram is
    a table showing the most recent log events that were found by the query.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change the time interval, you can use the time picker. Click
    on its calendar icon to adjust the time interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get a better view of the content in the log records, add some fields from
    the log records as columns in the table under the histogram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to see all available fields, click on the down arrow to the right
    of the **Filter by type** label, and unselect **Hide empty fields**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the fields from the list of **Available fields** to the left. Scroll
    down until the field is found. To find the fields more easily, use the field named
    **Search field names** to filter the list of available fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hold the cursor over the field and a **+** button will appear (a white cross
    in a blue circle); click on it to add the field as a column in the table. Select
    the following fields, in order:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`spring.level`, the log level'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubernetes.namespace_name`, the Kubernetes namespace'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubernetes.container_name`, the name of the container'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spring.trace`, the trace ID used for distributed tracing'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`log`, the actual log message'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To save some space, you can hide the list of fields by clicking on the collapse
    icon next to the index pattern field (containing the text `logstash-*`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The web page should look something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_10.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.10: Kibana web UI showing log records'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The table now contains information that is of interest regarding the log records!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To find log records from the call to the `GET` API, we can ask Kibana to find
    log records where the log field contains the text `product.id=1234`. This matches
    the log output from the `product-composite` microservice that was shown previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be done by entering `log:"product.id=1234"` in the top-left **Search**
    field and clicking on the **Update** button (this button can also be named **Refresh**).
    Expect one log record to be found:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_11.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.11: Kibana web UI showing a log record for productId = 1234'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify that the timestamp is from when you called the `GET` API and verify that
    the name of the container that created the log record is `product-composite`,
    that is, verify that the log record was sent by the product composite microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we want to see the related log records from the other microservices that
    participated in the process of returning information about the product with product
    ID `1234`. In other words, we want to find log records with the same **trace ID**
    as that of the log record we found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To do this, place the cursor over the `spring.trace` field for the log record.
    Two small magnifying glasses will be shown to the right of the field, one with
    a **+** sign and one with a **-** sign. Click on the magnifying glass with the
    **+** sign to filter on the trace ID.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clear the **Search** field so that the only search criterion is the filter of
    the trace field. Then, click on the **Update** button to see the result. Expect
    a response like the following:![A screenshot of a computer  Description automatically
    generated with medium confidence](img/B19825_19_12.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.12: Kibana web UI showing log records for a trace ID'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see some detailed debug messages that clutter the view; let’s get rid
    of them!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Place the cursor over a **DEBUG** value and click on the magnifying glass with
    the **–** sign to filter out log records with the log level set to **DEBUG**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should now be able to see the four expected log records, one for each microservice
    involved in the lookup of product information for the product with product ID
    `1234`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.13: Kibana web UI showing log records for a trace ID with log level
    = INFO'
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the filters that were applied included the trace ID but excluded
    log records with the log level set to **DEBUG**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to find the expected log records, we are ready to take
    the next step. This will be to learn how to find unexpected log records, that
    is, error messages, and how to perform root cause analysis to find the reason
    for these error messages.
  prefs: []
  type: TYPE_NORMAL
- en: Performing root cause analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important features of centralized logging is that it makes it
    possible to analyze errors using log records from many sources and, based on that,
    perform root cause analysis, finding the actual reason for the error message.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will simulate an error and see how we can find information
    about it all the way down to the line of source code that caused the error in
    one of the microservices in the system landscape. To simulate an error, we will
    reuse the fault parameter we introduced in *Chapter 13*, *Improving Resilience
    Using Resilience4j*, in the *Adding programmable delays and random errors* section.
    We can use this to force the `product` microservice to throw an exception. Perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to generate a fault in the `product` microservice
    while searching for product information on the product with product ID `1234`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect the following error in response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, software, multimedia  Description
    automatically generated](img/B19825_19_14.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.14: A request that caused an error in the processing'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we must pretend that we have no clue about the reason for this error! Otherwise,
    the root cause analysis wouldn’t be very exciting, right?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s assume that we work in a support organization and have been asked to investigate
    a problem that just occurred when an end user tried to look up information regarding
    a product with product ID `1234` but got an error message saying “`500 Internal
    Server Error`" in response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before we start to analyze the problem, let’s delete the previous search filters
    in the Kibana web UI so that we can start from scratch. For each filter we defined
    in the previous section, click on its close icon (an **x**) to remove it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start by using the time picker to select a time interval that includes the point
    in time when the problem occurred. In my case, 15 minutes is sufficient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select log records belonging to our namespace, `hands-on`. This can be done
    by the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the list of fields to the left, by clicking on the hamburger icon (**≡**)
    in the top-left corner.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the field **kubernetes.namespace_name** in the list of **Selected fields**.
    A list of the top five namespaces is shown.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **+** sign after the **hands-on** namespace.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, search for log records with the log level set to **WARN** within this
    time frame where the log message mentions product ID `1234`. This can be done
    by clicking on the `spring.level` field in the list of selected fields. When you
    click on this field, its most used values will be displayed under it. Filter on
    the **WARN** value by clicking on its **+** sign. Kibana will now show log records
    within the selected time frame with their log level set to **WARN** from the **hands-on**
    namespace, like this:![A screenshot of a computer  Description automatically generated
    with medium confidence](img/B19825_19_15.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.15: Kiali web UI, showing log records that report ERRORs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see a number of error messages related to product ID `1234`. The top
    log entries have the same trace ID, so this seems like a trace ID of interest
    to use for further investigation. The first log entry also contains the text reported
    by the end user **500** and **Internal Server Error**, and the error message **Something
    went wrong…**, which probably has to do with the root cause of the error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Filter on the trace ID of the first log record as we did in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the filter of the `WARN` log level to be able to see all the records
    belonging to this trace ID. Expect Kibana to respond with a lot of log records
    looking something like this:![A screenshot of a computer  Description automatically
    generated with medium confidence](img/B19825_19_16.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.16: Kiali web UI, looking for the root cause'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unfortunately, we cannot find any stack trace identifying the root cause by
    using trace IDs. This is due to a limitation in the Fluentd plugin we use for
    collecting multiline exceptions, `fluent-plugin-detect-exceptions`. It cannot
    relate stack traces to the trace ID that was used. Instead, we can use a feature
    in Kibana to find surrounding log records that have occurred near in time to a
    specific log record.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Expand the log record that says **Error body: {… status”:500,”error”:”Internal
    Server Error”,”message”:”Something went wrong...”…}** using the arrow to the left
    of the log record. Detailed information about this specific log record will be
    revealed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.17: Kiali web UI, expanding the log record with the root cause log
    message'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a link named **View surrounding documents**; click on it to see
    nearby log records. Scroll down to the bottom of the page to find a **Load** field
    where the number of records can be specified. Increase the default value, from
    5 to 10\. Expect a web page like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.18: Kiali web UI, the root cause found'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third log record below the expanded log record contains the stack trace
    for the error message **Something went wrong...**. This error message looks interesting.
    It was logged by the `product` microservice just five milliseconds before the
    expanded log record. They seem to be related! The stack trace in that log record
    points to line 104 in `ProductServiceImpl.java`. Looking at the source code (see
    `microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java`),
    line 104 looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the root cause of the error. We did know this in advance, but now we
    have seen how we can navigate to it as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, the problem is quite simple to resolve; simply omit the `faultPercent`
    parameter in the request to the API. In other cases, resolving the root cause
    can be much harder to figure out!
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the root cause analysis. Click on the back button in the web
    browser to get back to the main page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to reuse the configuration of the search criteria and table layout,
    its definition can be saved by Kibana. Select, for example, to filter on log records
    from the `hands-on` namespace and click on the **Save** link in the top-right
    menu. Give the search definition a name and click on the **Save** button. The
    search definition can be restored when required using the **Open** link in the
    menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This concludes this chapter on using the EFK stack for centralized logging.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the importance of collecting log records from
    microservices in a system landscape into a common centralized database where analysis
    and searches of the stored log records can be performed. We used the EFK stack,
    Elasticsearch, Fluentd, and Kibana, to collect, process, store, analyze, and search
    for log records.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd was used to collect log records not only from our microservices but
    also from the various supporting containers in the Kubernetes cluster. Elasticsearch
    was used as a text search engine. Together with Kibana, we saw how easy it is
    to get an understanding of what types of log records we have collected.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to use Kibana to perform important tasks such as finding
    related log records from cooperating microservices and how to perform root cause
    analysis, finding the real problem for an error message.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to collect and analyze log records in this way is an important capability
    in a production environment, but these types of activities are always done afterward
    once the log record has been collected. Another important capability is to be
    able to monitor the current health of the microservices, collecting and visualizing
    runtime metrics in terms of the use of hardware resources, response times, and
    so on. We touched on this subject in the previous chapter, and in the next chapter,
    we will learn more about monitoring microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A user searched for `ERROR` log messages in the `hands-on` namespace for the
    last 30 days using the search criteria shown in the following screenshot, but
    none were found. Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.19: Kiali web UI, not showing expected log records'
  prefs: []
  type: TYPE_NORMAL
- en: A user has found a log record of interest (shown below). How can the user find
    related log records from this and other microservices, for example, that come
    from processing an external API request?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.20: Kiali web UI; how do we find related log records?'
  prefs: []
  type: TYPE_NORMAL
- en: A user has found a log record that seems to indicate the root cause of a problem
    that was reported by an end user. How can the user find the stack trace that shows
    where in the source code the error occurred?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screen shot of a computer  Description automatically generated with low
    confidence](img/B19825_19_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.21: Kiali web UI; how do we find the root cause?'
  prefs: []
  type: TYPE_NORMAL
- en: Why doesn’t the following Fluentd configuration element work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How can you determine whether Elasticsearch is up and running?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You suddenly lose connection to Kibana from your web browser. What could have
    caused this problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
