- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Centralized Logging with the EFK Stack
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EFK堆栈进行集中日志记录
- en: 'In this chapter, we will learn how to collect and store log records from microservice
    instances, as well as how to search and analyze log records. As we mentioned in
    *Chapter 1*, *Introduction to Microservices*, it is difficult to get an overview
    of what is going on in a system landscape of microservices when each microservice
    instance writes log records to its local filesystem. We need a component that
    can collect the log records from the microservice’s local filesystem and store
    them in a central database for analysis, search, and visualization. A popular
    open-source-based solution for this is based on the following tools:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何从微服务实例收集和存储日志记录，以及如何搜索和分析日志记录。正如我们在*第1章*，*微服务介绍*中提到的，当每个微服务实例将其日志记录写入其本地文件系统时，在微服务系统景观中很难获得整体概览。我们需要一个组件可以从微服务的本地文件系统中收集日志记录，并将它们存储在中央数据库中以供分析、搜索和可视化。针对此问题的流行开源解决方案基于以下工具：
- en: '**Elasticsearch**, a distributed database with great capabilities for searching
    and analyzing large datasets'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**，一个具有强大搜索和分析大数据集能力的分布式数据库'
- en: '**Fluentd**, a data collector that can be used to collect log records from
    various sources, filter and transform the collected information, and finally send
    it to various consumers, for example, Elasticsearch'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fluentd**，一种可以用于从各种来源收集日志记录、过滤和转换收集到的信息，并将其最终发送到各种消费者（例如，Elasticsearch）的数据收集器'
- en: '**Kibana**, a graphical frontend to Elasticsearch that can be used to visualize
    search results and run analyses of the collected log records'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kibana**，Elasticsearch的图形前端，可用于可视化搜索结果和运行收集到的日志记录的分析'
- en: Together, these tools are called the **EFK stack**, named after the initials
    of each tool.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具一起被称为**EFK堆栈**，以每个工具的首字母命名。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Configuring Fluentd
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Fluentd
- en: Deploying the EFK stack on Kubernetes for development and test usage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈以供开发和测试使用
- en: Analyzing the collected log records
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析收集到的日志记录
- en: Discovering log records from the microservices and finding related log records
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从微服务中发现日志记录并找到相关的日志记录
- en: Performing root cause analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行根本原因分析
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For instructions on how to install the tools used in this book and how to access
    the source code for this book, see:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何安装本书中使用的工具以及如何访问本书源代码的说明，请参阅：
- en: '*Chapter 21*, *Installation Instructions for macOS*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第21章*，*macOS安装说明*'
- en: '*Chapter 22*, *Installation Instructions for Microsoft Windows with WSL 2 and
    Ubuntu*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第22章*，*使用WSL 2和Ubuntu的Microsoft Windows安装说明*'
- en: The code examples in this chapter all come from the source code in `$BOOK_HOME/Chapter19`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码示例均来自`$BOOK_HOME/Chapter19`的源代码。
- en: If you want to view the changes applied to the source code in this chapter,
    that is, see the changes we made so that we can use the EFK stack for centralized
    log analysis, you can compare it with the source code for *Chapter 18*, *Using
    a Service Mesh to Improve Observability and Management*. You can use your favorite
    `diff` tool and compare the two folders, `$BOOK_HOME/Chapter18` and `$BOOK_HOME/Chapter19`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看本章源代码中应用的变化，即查看我们做出的更改，以便我们可以使用EFK堆栈进行集中日志分析，你可以将其与*第18章*，*使用服务网格提高可观察性和管理*的源代码进行比较。你可以使用你喜欢的`diff`工具比较两个文件夹，`$BOOK_HOME/Chapter18`和`$BOOK_HOME/Chapter19`。
- en: Introducing Fluentd
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Fluentd
- en: In this section, we will learn the basics of how to configure Fluentd. Before
    we do that, let’s learn a bit about the background of Fluentd and how it works
    at a high level.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何配置Fluentd的基础知识。在我们这样做之前，让我们了解一下Fluentd的背景以及它在高层次上是如何工作的。
- en: Overview of Fluentd
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fluentd概述
- en: Historically, one of the most popular open source stacks for handling log records
    has been the ELK stack from Elastic ([https://www.elastic.co](https://www.elastic.co)),
    based on Elasticsearch, Logstash (used for log collection and transformation),
    and Kibana. Since Logstash runs on a Java VM, it requires a relatively large amount
    of memory. Over the years, a number of open source alternatives have been developed
    that require significantly less memory than Logstash, one of them being Fluentd
    ([https://www.fluentd.org](https://www.fluentd.org)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，处理日志记录最受欢迎的开源堆栈之一是 Elastic 的 ELK 堆栈（[https://www.elastic.co](https://www.elastic.co)），它基于
    Elasticsearch、Logstash（用于日志收集和转换）和 Kibana。由于 Logstash 运行在 Java 虚拟机上，它需要相对较大的内存量。多年来，已经开发出许多开源替代方案，这些方案比
    Logstash 需要的内存量少得多，其中之一就是 Fluentd（[https://www.fluentd.org](https://www.fluentd.org)）。
- en: Fluentd is managed by the **Cloud Native Computing Foundation** (**CNCF**) ([https://www.cncf.io](https://www.cncf.io)),
    the same organization that manages the Kubernetes project. Therefore, Fluentd
    has become a natural choice as an open-source-based log collector that runs in
    Kubernetes. Together with Elastic and Kibana, it forms the EFK stack.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 由 **云原生计算基金会**（**CNCF**）（[https://www.cncf.io](https://www.cncf.io)）管理，该基金会还管理着
    Kubernetes 项目。因此，Fluentd 成为了一个基于开源的日志收集器，它自然地成为在 Kubernetes 中运行的日志收集器的首选。与 Elastic
    和 Kibana 一起，它形成了 EFK 堆栈。
- en: CNCF maintains a list of alternative products for several categories, for example,
    for logging. For alternatives to Fluentd listed by CNCF, see [https://landscape.cncf.io/card-mode?category=logging&grouping=category](https://landscape.cncf.io/card-mode?category=logging&grouping=category).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF 维护了多个类别的替代产品列表，例如，对于日志记录。有关 CNCF 列出的 Fluentd 替代方案，请参阅 [https://landscape.cncf.io/card-mode?category=logging&grouping=category](https://landscape.cncf.io/card-mode?category=logging&grouping=category)。
- en: Fluentd is written in a mix of C and Ruby, using C for the performance-critical
    parts and Ruby where flexibility is of more importance, for example, allowing
    the simple installation of third-party plugins using Ruby’s `gem install` command.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 是用 C 和 Ruby 混合编写的，使用 C 语言处理性能关键的部分，而在需要更多灵活性的地方使用 Ruby，例如，允许使用 Ruby
    的 `gem install` 命令简单地安装第三方插件。
- en: 'A log record is processed as an event in Fluentd and consists of the following
    information:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录在 Fluentd 中作为事件进行处理，并包含以下信息：
- en: A `time` field describing when the log record was created
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个描述日志记录创建时间的 `time` 字段
- en: A `tag` field that identifies what type of log record it is – the tag is used
    by Fluentd’s routing engine to determine how a log record will be processed
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个标识日志记录类型的 `tag` 字段——该标签由 Fluentd 的路由引擎用于确定日志记录的处理方式
- en: A `record` that contains the actual log information, which is stored as a JSON
    object
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含实际日志信息的 `record`，该信息存储为 JSON 对象
- en: 'A Fluentd configuration file is used to tell Fluentd how to collect, process,
    and finally send log records to various targets, such as Elasticsearch. A configuration
    file consists of the following types of core elements:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 配置文件用于告诉 Fluentd 如何收集、处理，并将日志记录最终发送到各种目标，例如 Elasticsearch。配置文件由以下类型的核心元素组成：
- en: '`<source>`: Source elements describe where Fluentd will collect log records,
    for example, tailing log files that have been written to by Docker containers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<source>`：源元素描述 Fluentd 将收集日志记录的位置，例如，收集由 Docker 容器写入的日志文件。'
- en: '*Tailing a log file* means monitoring what is written to a log file. A frequently
    used Unix/Linux tool for monitoring what is appended to a file is named `tile`.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*跟踪日志文件*意味着监控写入日志文件的内容。一个常用的 Unix/Linux 工具，用于监控文件附加的内容，名为 `tile`。'
- en: Source elements typically tag the log records, describing the type of log record.
    They could, for example, be used to tag log records to state that they come from
    containers running in Kubernetes.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 源元素通常会对日志记录进行标记，描述日志记录的类型。例如，它们可以用来标记日志记录，表明它们来自在 Kubernetes 中运行的容器。
- en: '`<filter>`: Filter elements are used to process the log records. For example,
    a filter element can parse log records that come from Spring Boot-based microservices
    and extract interesting parts of the log message into separate fields in the log
    record. Extracting information into separate fields in the log record makes the
    information searchable by Elasticsearch. A filter element selects the log records
    to process based on their tags.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<filter>`：过滤器元素用于处理日志记录。例如，一个过滤器元素可以解析来自基于 Spring Boot 的微服务的日志记录，并将日志消息的有趣部分提取到日志记录中的单独字段。将信息提取到日志记录中的单独字段使得信息可以通过
    Elasticsearch 进行搜索。过滤器元素根据日志记录的标签选择要处理的日志记录。'
- en: '`<match>`: Match elements decide where to send log records, acting as output
    elements. They are used to perform two main tasks:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`：匹配元素决定将日志记录发送到何处，充当输出元素。它们用于执行两个主要任务：'
- en: Sending processed log records to targets such as Elasticsearch.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将处理后的日志记录发送到目标，例如 Elasticsearch。
- en: 'Routing to decide how to process log records. A routing rule can rewrite the
    tag and re-emit the log record into the Fluentd routing engine for further processing.
    A routing rule is expressed as an embedded `<rule>` element inside the `<match>`
    element. Output elements decide what log records to process, in the same way as
    a filter: based on the tag of the log records.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由决定如何处理日志记录。路由规则可以重写标记并将日志记录重新发射到 Fluentd 路由引擎以进行进一步处理。路由规则以 `<match>` 元素内部的嵌入式
    `<rule>` 元素表示。输出元素决定要处理哪些日志记录，与过滤器类似：基于日志记录的标记。
- en: Fluentd comes with a number of built-in and external third-party plugins that
    are used by the source, filter, and output elements. We will see some of them
    in action when we walk through the configuration file in the next section. For
    more information on the available plugins, see Fluentd’s documentation, which
    is available at [https://docs.fluentd.org](https://docs.fluentd.org).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 随带一些内置和外部第三方插件，这些插件被源、过滤器和输出元素使用。在下一节中，我们将通过配置文件了解其中的一些插件。有关可用插件的信息，请参阅
    Fluentd 的文档，可在 [https://docs.fluentd.org](https://docs.fluentd.org) 找到。
- en: With this overview of Fluentd out of the way, we are ready to see how Fluentd
    can be configured to process the log records from our microservices.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成对 Fluentd 的概述之后，我们准备了解 Fluentd 如何配置以处理来自我们的微服务的日志记录。
- en: Configuring Fluentd
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Fluentd
- en: The configuration of Fluentd is based on the configuration files from a Fluentd
    project on GitHub, `fluentd-kubernetes-daemonset`. The project contains Fluentd
    configuration files for how to collect log records from containers that run in
    Kubernetes and how to send them to Elasticsearch once they have been processed.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 的配置基于 GitHub 上 Fluentd 项目的配置文件，即 `fluentd-kubernetes-daemonset`。该项目包含
    Fluentd 配置文件，说明如何从在 Kubernetes 中运行的容器收集日志记录，以及一旦处理完毕如何将它们发送到 Elasticsearch。
- en: We will reuse this configuration without changes, and it will simplify our own
    configuration to a great extent. The Fluentd configuration files can be found
    at [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/archived-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/archived-image/v1.4/debian-elasticsearch/conf).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不修改地重用此配置，这将极大地简化我们的配置。Fluentd 配置文件可以在 [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/archived-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/archived-image/v1.4/debian-elasticsearch/conf)
    找到。
- en: 'The configuration files that provide this functionality are `kubernetes.conf`
    and `fluent.conf`. The `kubernetes.conf` configuration file contains the following
    information:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提供此功能的配置文件是 `kubernetes.conf` 和 `fluent.conf`。`kubernetes.conf` 配置文件包含以下信息：
- en: Source elements that tail container log files and log files from processes that
    run outside of Kubernetes, for example, `kubelet` and the Docker daemon. The source
    elements also tag the log records from Kubernetes with the full name of the log
    file with `/` replaced by `.` and prefixed with `kubernetes`. Since the tag is
    based on the full filename, the name contains the name of the namespace, pod,
    and container, among other things. So, the tag is very useful for finding log
    records of interest by matching the tag.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源元素跟踪容器日志文件以及运行在 Kubernetes 外部的进程（例如，`kubelet` 和 Docker 守护进程）的日志文件。源元素还会将 Kubernetes
    的日志记录标记为具有完整文件名的全名，其中 `/` 被替换为 `.` 并以 `kubernetes` 为前缀。由于标记基于完整文件名，因此名称包含命名空间、Pod
    和容器等名称。因此，标记对于通过匹配标记查找感兴趣的日志记录非常有用。
- en: For example, the tag from the `product-composite` microservice could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`,
    while the tag for the corresponding `istio-proxy` in the same Pod could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，来自 `product-composite` 微服务的标记可能类似于 `kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`，而同一
    Pod 中相应的 `istio-proxy` 的标记可能类似于 `kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`。
- en: A filter element that enriches the log records that come from containers running
    inside Kubernetes, along with Kubernetes-specific fields that contain information
    such as the names of the containers and the namespace they run in.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个过滤器元素，丰富了来自运行在Kubernetes内部容器中的日志记录，以及包含容器名称和运行命名空间等信息的特定于Kubernetes的字段。
- en: 'The main configuration file, `fluent.conf`, contains the following information:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 主配置文件`fluent.conf`包含以下信息：
- en: '`@include` statements for other configuration files, for example, the `kubernetes.conf`
    file we described previously. It also includes custom configuration files that
    are placed in a specific folder, making it very easy for us to reuse these configuration
    files without any changes and provide our own configuration file that only handles
    processing related to our own log records. We simply need to place our own configuration
    file in the folder specified by the `fluent.conf` file.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@include`语句用于其他配置文件，例如我们之前描述的`kubernetes.conf`文件。它还包括放置在特定文件夹中的自定义配置文件，这使得我们能够非常容易地重用这些配置文件而无需任何更改，并提供我们自己的配置文件，该文件仅处理与我们的日志记录相关的处理。我们只需将我们的配置文件放置在`fluent.conf`文件指定的文件夹中即可。'
- en: An output element that sends log records to Elasticsearch.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出元素，将日志记录发送到Elasticsearch。
- en: As described in the *Deploying Fluentd* section later on, these two configuration
    files will be packaged into the Docker image we will build for Fluentd.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如后续的*部署Fluentd*部分所述，这两个配置文件将被打包到我们将为Fluentd构建的Docker镜像中。
- en: 'What’s left to cover in our own configuration file is the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自己的配置文件中需要覆盖的内容如下：
- en: Detecting and parsing Spring Boot-formatted log records from our microservices.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和解析来自我们的微服务的Spring Boot格式日志记录。
- en: Handling multiline stack traces. Stack traces are written to log files using
    multiple lines. This makes it hard for Fluentd to handle a stack trace as a single
    log record.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理多行堆栈跟踪。堆栈跟踪使用多行写入日志文件，这使得Fluentd难以将堆栈跟踪作为一个单独的日志记录处理。
- en: Separating log records from the `istio-proxy` sidecars from the log records
    that were created by the microservices running in the same Pod. The log records
    that are created by `istio-proxy` don’t follow the same pattern as the log patterns
    that are created by our Spring Boot-based microservices. Therefore, they must
    be handled separately so that Fluentd doesn’t try to parse them as Spring Boot-formatted
    log records.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将来自`istio-proxy`边车和同一Pod中运行的微服务创建的日志记录分开。由`istio-proxy`创建的日志记录不遵循我们基于Spring
    Boot的微服务创建的日志模式。因此，它们必须单独处理，以免Fluentd尝试将它们解析为Spring Boot格式的日志记录。
- en: To achieve this, the configuration is, to a large extent, based on using the
    `rewrite_tag_filter` plugin. This plugin can be used for routing log records based
    on the concept of changing the name of a tag and then re-emitting the log record
    to the Fluentd routing engine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，配置在很大程度上是基于使用`rewrite_tag_filter`插件。此插件可用于根据更改标签名称的概念路由日志记录，然后将日志记录重新发射到Fluentd路由引擎。
- en: 'This processing is summarized by the following UML activity diagram:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该处理由以下UML活动图总结：
- en: '![A picture containing text, screenshot, diagram, line  Description automatically
    generated](img/B19825_19_01.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表的图片，自动生成描述](img/B19825_19_01.png)'
- en: 'Figure 19.1: Fluentd processing of log records'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1：Fluentd处理日志记录
- en: 'At a high level, the design of the configuration file looks as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，配置文件的设计如下：
- en: The tags of all log records from Istio, including `istio-proxy`, are prefixed
    with `istio` so that they can be separated from the Spring Boot-based log records.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio的所有日志记录的标签，包括`istio-proxy`，都以前缀`istio`开头，以便它们可以与基于Spring Boot的日志记录分开。
- en: The tags of all log records from the `hands-on` namespace (except for the log
    records from `istio-proxy`) are prefixed with `spring-boot`.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自`hands-on`命名空间的所有日志记录的标签（`istio-proxy`的日志记录除外）都以前缀`spring-boot`开头。
- en: The log records from Spring Boot are checked for the presence of multiline stack
    traces. If the log record is part of a multiline stack trace, it is processed
    by the third-party `detect-exceptions` plugin to recreate the stack trace. Otherwise,
    it is parsed using a regular expression to extract information of interest. See
    the *Deploying Fluentd* section for details on this third-party plugin.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spring Boot 的日志记录会检查是否存在多行堆栈跟踪。如果日志记录是多行堆栈跟踪的一部分，它将由第三方 `detect-exceptions`
    插件处理以重新创建堆栈跟踪。否则，它将使用正则表达式解析以提取感兴趣的信息。有关此第三方插件的详细信息，请参阅 *Deploying Fluentd* 部分。
- en: 'The `fluentd-hands-on.conf` configuration file implements this activity diagram.
    The configuration file is placed inside a Kubernetes ConfigMap (see `kubernetes/efk/fluentd-hands-on-configmap.yml`).
    Let’s go through this step by step, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`fluentd-hands-on.conf` 配置文件实现了此活动图。配置文件放置在 Kubernetes ConfigMap 内（参见 `kubernetes/efk/fluentd-hands-on-configmap.yml`）。让我们一步一步地来了解这个过程，如下所示：'
- en: 'First comes the definition of the ConfigMap and the filename of the configuration
    file, `fluentd-hands-on.conf`. It looks as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是 ConfigMap 的定义和配置文件名，`fluentd-hands-on.conf`。它看起来如下：
- en: '[PRE0]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can see that the `data` element will contain the configuration of Fluentd.
    It starts with the filename and uses a vertical bar, `|`, to mark the beginning
    of the embedded configuration file for Fluentd.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，`data` 元素将包含 Fluentd 的配置。它以文件名开始，并使用垂直线 `|` 标记 Fluentd 嵌入式配置文件的开始。
- en: 'The first `<match>` element matches the log records from Istio, that is, tags
    that are prefixed with `Kubernetes` and contain `istio` as either part of their
    namespace or part of their container name. It looks like this:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个 `<match>` 元素匹配来自 Istio 的日志记录，即以 `Kubernetes` 开头并包含 `istio` 作为其命名空间或容器名称一部分的标签。它看起来像这样：
- en: '[PRE1]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s explain the preceding source code:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们解释前面的源代码：
- en: The `<match>` element matches any tags that follow the `kubernetes.**istio**`
    pattern, that is, tags that start with `Kubernetes` and then contain the word
    `istio` somewhere in the tag name. `istio` can come from the name of either the
    namespace or the container; both are part of the tag.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>` 元素匹配任何符合 `kubernetes.**istio**` 模式的标签，即以 `Kubernetes` 开头并在标签名称中包含
    `istio` 的标签。`istio` 可以来自命名空间或容器的名称；两者都是标签的一部分。'
- en: The `<match>` element contains only one `<rule>` element, which prefixes the
    tag with `istio`. The `${tag}` variable holds the current value of the tag.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>` 元素只包含一个 `<rule>` 元素，该元素将标签前缀设置为 `istio`。`${tag}` 变量持有当前标签的值。'
- en: Since this is the only `<rule>` element in the `<match>` element, it is configured
    to match all log records.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这是 `<match>` 元素中的唯一 `<rule>` 元素，因此它被配置为匹配所有日志记录。
- en: Since all log records that come from Kubernetes have a `log` field, the `key`
    field is set to `log`, that is, the rule looks for a `log` field in the log records.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于所有来自 Kubernetes 的日志记录都有一个 `log` 字段，因此 `key` 字段被设置为 `log`，即规则在日志记录中查找 `log`
    字段。
- en: To match any string in the `log` field, the `pattern` field is set to the `^(.*)$`
    regular expression. `^` marks the beginning of a string, while `$` marks the end
    of a string. `(.*)` matches any number of characters, except for line breaks.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了匹配 `log` 字段中的任何字符串，`pattern` 字段被设置为 `^(.*)$` 正则表达式。`^` 标记字符串的开始，而 `$` 标记字符串的结束。`(.*)`
    匹配任意数量的字符，除了换行符。
- en: The log records are re-emitted to the Fluentd routing engine. Since no other
    elements in the configuration file match tags starting with `istio`, the log records
    will be sent directly to the output element for Elasticsearch, which is defined
    in the `fluent.conf` file we described previously.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录被重新发送到 Fluentd 路由引擎。由于配置文件中没有其他元素匹配以 `istio` 开头的标签，因此日志记录将直接发送到之前描述的 `fluent.conf`
    文件中定义的 Elasticsearch 输出元素。
- en: 'The second `<match>` element matches all log records from the `hands-on` namespace,
    that is, the log records that are emitted by our microservices. It looks like
    this:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个 `<match>` 元素匹配来自 `hands-on` 命名空间的所有日志记录，即由我们的微服务发出的日志记录。它看起来像这样：
- en: '[PRE2]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the source code, we can see that:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从源代码中，我们可以看到：
- en: The log records emitted by our microservices use formatting rules for the log
    message defined by Spring Boot, so their tags are prefixed with `spring-boot`.
    Then, they are re-emitted for further processing.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们微服务发出的日志记录使用由 Spring Boot 定义的日志消息格式规则，因此它们的标签以 `spring-boot` 开头。然后，它们被重新发送以进行进一步处理。
- en: The `<match>` element is configured in the same way as the `<match kubernetes.**istio**>`
    element we looked at previously, to match all records.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>` 元素配置方式与之前查看的 `<match kubernetes.**istio**>` 元素相同，以匹配所有记录。'
- en: The third `<match>` element matches `spring-boot` log records and determines
    whether they are ordinary Spring Boot log records or part of a multiline stack
    trace. Since Spring Boot 3, Project Reactor has added extra information to stack
    traces to clarify what caused an exception. (For details, see [https://projectreactor.io/docs/core/release/reference/#_reading_a_stack_trace_in_debug_mode](https://projectreactor.io/docs/core/release/reference/#_reading_a_stack_trace_in_debug_mode).)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个 `<match>` 元素匹配 `spring-boot` 日志记录，并确定它们是普通 Spring Boot 日志记录还是多行堆栈跟踪的一部分。自
    Spring Boot 3 以来，Project Reactor 已向堆栈跟踪添加了额外信息，以澄清导致异常的原因。（有关详细信息，请参阅 [https://projectreactor.io/docs/core/release/reference/#_reading_a_stack_trace_in_debug_mode](https://projectreactor.io/docs/core/release/reference/#_reading_a_stack_trace_in_debug_mode)。）
- en: 'To be able to parse the actual stack trace, we will filter out this information.
    The `<match>` element looks like this:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了能够解析实际的堆栈跟踪，我们将过滤掉此信息。`<match>` 元素看起来像这样：
- en: '[PRE3]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As seen in the source code, this is determined by using six `<rule>` elements:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如源代码所示，这是通过使用六个 `<rule>` 元素来确定的：
- en: The first uses a regular expression to check whether the `log` field in the
    log element starts with a timestamp or not.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个使用正则表达式检查日志元素中的 `log` 字段是否以时间戳开头。
- en: If the `log` field starts with a timestamp, the log record is treated as an
    ordinary Spring Boot log record and its tag is prefixed with `parse`.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `log` 字段以时间戳开头，则该日志记录被视为普通的 Spring Boot 日志记录，并且其标签以 `parse` 为前缀。
- en: Next follows four rule elements that are used to filter out the extra information
    added by Project Reactor; they all prefix the tag with `skip`.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是四个规则元素，用于过滤掉 Project Reactor 添加的额外信息；它们都将标签前缀设置为 `skip`。
- en: Otherwise, the last `<rule>` element will match, and the log record is handled
    as a multiline log record. Its tag is prefixed with `check.exception`.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，最后一个 `<rule>` 元素将匹配，并将日志记录作为多行日志记录处理。其标签以 `check.exception` 为前缀。
- en: The log record is re-emitted in either case and its tag will either start with
    `check.exception.spring-boot`, `skip.spring-boot,` or `parse.spring-boot` after
    this processing.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此处理之后，日志记录将重新发射，并且其标签将开始于 `check.exception.spring-boot`、`skip.spring-boot`
    或 `parse.spring-boot`。
- en: 'The fourth `<match>` element is used to get rid of the log output from Project
    Reactor, i.e. match tags starting with `skip.spring-boot`. The `<match>` element
    applies the `null` output plugin that throws away the events. It looks like this:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四个 `<match>` 元素用于消除 Project Reactor 的日志输出，即以 `skip.spring-boot` 开头的匹配标签。该 `<match>`
    元素应用了 `null` 输出插件，丢弃事件。它看起来像这样：
- en: '[PRE4]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the fifth `<match>` element, the selected log records have a tag that starts
    with `check.exception.spring-boot`, that is, log records that are part of a multiline
    stack trace. It looks like this:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第五个 `<match>` 元素中，选定的日志记录具有以 `check.exception.spring-boot` 开头的标签，即作为多行堆栈跟踪一部分的日志记录。它看起来像这样：
- en: '[PRE5]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `detect_exceptions` plugin works like this:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`detect_exceptions` 插件的工作方式如下：'
- en: The `detect_exceptions` plugin is used to combine multiple one-line log records
    into a single log record that contains a complete stack trace.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detect_exceptions` 插件用于将多个单行日志记录合并成一个包含完整堆栈跟踪的单个日志记录。'
- en: Before a multiline log record is re-emitted into the routing engine, the `check`
    prefix is removed from the tag to prevent a never-ending processing loop of the
    log record.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多行日志记录重新发射到路由引擎之前，将 `check` 前缀从标签中移除，以防止日志记录的无限循环处理。
- en: 'Finally, the configuration file consists of a `<filter>` element that parses
    Spring Boot log messages using a regular expression, extracting information of
    interest. It looks like this:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，配置文件由一个 `<filter>` 元素组成，该元素使用正则表达式解析 Spring Boot 日志消息，提取感兴趣的信息。它看起来像这样：
- en: '[PRE6]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that filter elements don’t re-emit log records; instead, they just pass
    them on to the next element in the configuration file that matches the log record’s
    tag.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，过滤器元素不会重新发射日志记录；相反，它们只是将它们传递到配置文件中匹配日志记录标签的下一个元素。
- en: 'The following fields are extracted from the Spring Boot log message that’s
    stored in the `log` field in the log record:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从存储在日志记录中 `log` 字段的 Spring Boot 日志消息中提取以下字段：
- en: '`<time>`: The timestamp for when the log record was created'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<time>`：日志记录创建时的日期和时间戳'
- en: '`<spring.level>`: The log level of the log record: `FATAL`, `ERROR`, `WARN`,
    `INFO`, `DEBUG`, or `TRACE`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.service>`: The name of the microservice'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.trace>`: The trace ID used to perform distributed tracing'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.span>`: The span ID, the ID of the part of the distributed processing
    that this microservice executed'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.pid>`: The process ID'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.thread>`: The thread ID'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<spring.class>`: The name of the Java class'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<log>`: The actual log message'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The names of Spring Boot-based microservices are specified using the `spring.application.name`
    property. This property has been added to each microservice-specific property
    file in the config repository, in the `config-repo` folder.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting regular expressions right can be challenging, to say the least. Thankfully,
    there are several websites that can help. When it comes to using regular expressions
    together with Fluentd, I recommend using the following site: [https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have been introduced to how Fluentd works and how the configuration
    file is constructed, we are ready to deploy the EFK stack.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the EFK stack on Kubernetes
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deploying the EFK stack on Kubernetes will be done in the same way as we have
    deployed our own microservices: using Kubernetes manifest files for objects such
    as Deployments, Services, and ConfigMaps.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment of the EFK stack is divided into three parts:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Elasticsearch and Kibana
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Fluentd
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up access to Elasticsearch and Kibana
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But first, we need to build and deploy our own microservices.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying our microservices
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building, deploying, and verifying the deployment using the `test-em-all.bash`
    test script is done in the same way as it was done in *Chapter 18*, *Using a Service
    Mesh to Improve Observability and Management*, in the *Running commands to create
    the service mesh* section. These instructions assume that the cert-manager and
    Istio are installed as instructed in *Chapters 17* and *18*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to get started:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'First, build the Docker images from the source with the following commands:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `eval $(minikube docker-env -u)` command ensures that the `./gradlew build`
    command uses the host’s Docker engine and not the Docker engine in the Minikube
    instance. The `build` command uses Docker to run test containers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Recreate the namespace, `hands-on`, and set it as the default namespace:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Resolve the Helm chart dependencies with the following commands.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we update the dependencies in the `components` folder:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we update the dependencies in the `environments` folder:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Deploy the system landscape using Helm and wait for all deployments to complete:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Start the Minikube tunnel in a separate terminal window, if it’s not already
    running (see *Chapter 18*, the *Setting up access to Istio services* section,
    for a recap, if required):'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Remember that this command requires that your user has `sudo` privileges and
    that you enter your password during startup. It takes a couple of seconds before
    the command asks for the password, so it is easy to miss!
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住，这个命令要求你的用户具有`sudo`权限，并且在启动时输入你的密码。在命令请求密码之前需要几秒钟的时间，所以很容易错过！
- en: 'Run the normal tests to verify the deployment with the following command:'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行正常测试以验证部署：
- en: '[PRE13]'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Expect the output to be similar to what we saw in the previous chapters:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将与我们在前几章中看到的结果相似：
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_19_02.png)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B19825_19_02.png)'
- en: 'Figure 19.2: Tests running fine'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图19.2：测试运行良好
- en: 'You can also try out the APIs manually by running the following commands:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以通过运行以下命令手动尝试API：
- en: '[PRE14]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Expect the requested product ID, `1`, in the response.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 预期在响应中收到请求的产品ID，`1`。
- en: With the microservices deployed, we can move on and deploy Elasticsearch and
    Kibana!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了微服务后，我们可以继续部署Elasticsearch和Kibana！
- en: Deploying Elasticsearch and Kibana
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署Elasticsearch和Kibana
- en: We will deploy Elasticsearch and Kibana to their own namespace, `logging`. Both
    Elasticsearch and Kibana will be deployed for development and test usage using
    a Kubernetes Deployment and Service object. The services will expose the standard
    ports for Elasticsearch and Kibana internally in the Kubernetes cluster, that
    is, port `9200` for Elasticsearch and port `5601` for Kibana.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署Elasticsearch和Kibana到它们自己的命名空间`logging`。Elasticsearch和Kibana将使用Kubernetes
    Deployment和Service对象进行开发和测试部署。服务将在Kubernetes集群内部公开Elasticsearch和Kibana的标准端口，即Elasticsearch的端口`9200`和Kibana的端口`5601`。
- en: To provide external HTTP access to Elasticsearch and Kibana, we will create
    Istio objects as we did in *Chapter 18**, Using a Service Mesh to Improve Observability
    and Management*, for Kiali and Jaeger – see the *Setting up access to Istio services*
    section for a recap, if required. This will result in Elasticsearch and Kibana
    being available at [https://elasticsearch.minikube.me](https://elasticsearch.minikube.me)
    and [https://kibana.minikube.me](https://kibana.minikube.me).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要为Elasticsearch和Kibana提供外部HTTP访问，我们将创建与第18章中相同的Istio对象，即使用服务网格来提高可观察性和管理性，用于Kiali和Jaeger
    – 如果需要，请参阅“设置对Istio服务的访问”部分进行回顾。这将使Elasticsearch和Kibana在[https://elasticsearch.minikube.me](https://elasticsearch.minikube.me)和[https://kibana.minikube.me](https://kibana.minikube.me)可用。
- en: The manifest files have been packaged in a Helm chart in the `kubernetes/helm/environments/logging`
    folder.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 清单文件已打包在`kubernetes/helm/environments/logging`文件夹中的Helm图表中。
- en: For recommended deployment options for Elasticsearch and Kibana in a production
    environment on Kubernetes, see [https://www.elastic.co/elastic-cloud-kubernetes](https://www.elastic.co/elastic-cloud-kubernetes).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Kubernetes生产环境中部署Elasticsearch和Kibana的推荐选项，请参阅[https://www.elastic.co/elastic-cloud-kubernetes](https://www.elastic.co/elastic-cloud-kubernetes)。
- en: 'We will use the latest versions that were available for version 7 when this
    chapter was written:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在撰写本章时7.0版本可用的最新版本：
- en: Elasticsearch version 7.17.10
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch版本7.17.10
- en: Kibana version 7.17.10
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana版本7.17.10
- en: Elasticsearch version 8 is not used, due to limited support in the Fluentd plugin
    for Elasticsearch; see [https://github.com/uken/fluent-plugin-elasticsearch/issues/1005](https://github.com/uken/fluent-plugin-elasticsearch/issues/1005).
    The base Docker image, `fluentd-kubernetes-daemonset`, that we will use in the
    following section to install Fluentd uses this plugin.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Fluentd插件对Elasticsearch的支持有限，因此没有使用Elasticsearch版本8；请参阅[https://github.com/uken/fluent-plugin-elasticsearch/issues/1005](https://github.com/uken/fluent-plugin-elasticsearch/issues/1005)。我们将在下一节中使用此插件来安装Fluentd的基Docker镜像，即`fluentd-kubernetes-daemonset`。
- en: Before we deploy, let’s look at the most interesting parts of the manifest files
    in the Helm chart’s `template` folder.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署之前，让我们看看Helm图表的`template`文件夹中清单文件中最有趣的部分。
- en: A walkthrough of the manifest files
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清单文件的概述
- en: 'The manifest file for Elasticsearch, `elasticsearch.yml`, contains a standard
    Kubernetes Deployment and Service object that we have seen multiple times before,
    for example, in *Chapter 15*, *Introduction to Kubernetes*, in the *Trying out
    a sample deployment* section. The most interesting part of the manifest file is
    the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch的清单文件`elasticsearch.yml`包含了一个标准的Kubernetes Deployment和Service对象，我们之前已经多次见过，例如在*第15章*，*Kubernetes简介*中的*尝试示例部署*部分。清单文件中最有趣的部分如下：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s explain some of this manifest:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下这些清单文件中的某些内容：
- en: We use an official Docker image from Elastic that’s available at `docker.elastic.co`.
    The version is set to `7.17.10`.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Elasticsearch container is allowed to allocate a relatively large amount
    of memory – 2 GB – to be able to run queries with good performance. The more memory,
    the better the performance.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The manifest file for Kibana, `kibana.yml`, also contains a standard Kubernetes
    Deployment and Service object. The most interesting parts in the manifest file
    are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s explain some of the manifest:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: For Kibana, we also use an official Docker image from Elastic that’s available
    at `docker.elastic.co`. The version is set to `7.17.10`.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To connect Kibana with the Elasticsearch Pod, an environment variable, `ELASTICSEARCH_URL`,
    is defined to specify the address to the Elasticsearch service, `http://elasticsearch:9200`.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the Istio manifests for setting up external access are found in the
    files `expose-elasticsearch.yml` and `expose-kibana.yml`. For a recap on how the
    `Gateway`, `VirtualService`, and `DestinationRule` objects are used, see the section
    *Creating the service mesh* in *Chapter 18*. They will provide the following forwarding
    of external requests:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[https://elasticsearch.minikube.me](https://elasticsearch.minikube.me) → `http://elasticsearch:9200`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://kibana.minikube.me](https://kibana.minikube.me) → `http://kibana:5601`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these insights, we are ready to perform the deployment of Elasticsearch
    and Kibana.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Running the deploy commands
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deploy Elasticsearch and Kibana by performing the following steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the deployment steps run faster, prefetch the Docker images for Elasticsearch
    and Kibana with the following commands:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Use the Helm chart to create the `logging` namespace, deploy Elasticsearch
    and Kibana in it, and wait for the Pods to be ready:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Verify that Elasticsearch is up and running with the following command:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Expect `You Know, for Search` as a response.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Depending on your hardware, you might need to wait for a minute or two before
    Elasticsearch responds with this message.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that Kibana is up and running with the following command:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Expect `200` as the response.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Again, you might need to wait for a minute or two before Kibana is initialized
    and responds with `200`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: With Elasticsearch and Kibana deployed, we can start to deploy Fluentd.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Fluentd
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying Fluentd is a bit more complex compared to deploying Elasticsearch
    and Kibana. To deploy Fluentd, we will use a Docker image that’s been published
    by the Fluentd project on Docker Hub, `fluent/fluentd-kubernetes-daemonset`, and
    the sample Kubernetes manifest files from a Fluentd project on GitHub, `fluentd-kubernetes-daemonset`.
    It is located at [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
    As is implied by the name of the project, Fluentd will be deployed as a DaemonSet,
    running one Pod per Node in the Kubernetes cluster. Each Fluentd Pod is responsible
    for collecting log output from processes and containers that run on the same Node
    as the Pod. Since we are using Minikube with a single Node cluster, we will only
    have one Fluentd Pod.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: To handle multiline log records that contain stack traces from exceptions, we
    will use a third-party Fluentd plugin provided by Google, `fluent-plugin-detect-exceptions`,
    which is available at [https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions).
    To be able to use this plugin, we will build our own Docker image where the `fluent-plugin-detect-exceptions`
    plugin will be installed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd’s Docker image, `fluentd-kubernetes-daemonset`, will be used as the
    base image.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following versions:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd version 1.4.2
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fluent-plugin-detect-exceptions` version 0.0.12'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we deploy, let’s look at the most interesting parts of the manifest files.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the manifest files
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Dockerfile that’s used to build the Docker image, `kubernetes/efk/Dockerfile`,
    looks as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s explain this in detail:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The base image is Fluentd’s Docker image, `fluentd-kubernetes-daemonset`. The
    `v1.4.2-debian-elasticsearch-1.1` tag specifies that version 1.4.2 will be used
    with a package that contains built-in support for sending log records to Elasticsearch.
    The base Docker image contains the Fluentd configuration files that were mentioned
    in the *Configuring Fluentd* section.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Google plugin, `fluent-plugin-detect-exceptions`, is installed using Ruby’s
    package manager, `gem`.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The manifest file of the DaemonSet, `kubernetes/efk/fluentd-ds.yml`, is based
    on a sample manifest file in the `fluentd-kubernetes-daemonset` project, which
    can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'This file is a bit complex, so let’s go through the most interesting parts
    separately:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'First, here’s the declaration of the DaemonSet:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `kind` key specifies that this is a DaemonSet. The `namespace` key specifies
    that the DaemonSet will be created in the `kube-system` namespace and not in the
    `logging` namespace where Elasticsearch and Kibana are deployed.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next part specifies the template for the Pods that are created by the DaemonSet.
    The most interesting parts are as follows:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The Docker image that’s used for the Pods is `hands-on/fluentd:v1`. We will
    build this Docker image after walking through the manifest files using the Dockerfile
    we described previously.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A number of environment variables are supported by the Docker image and are
    used to customize it. The two most important ones are as follows:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`FLUENT_ELASTICSEARCH_HOST`, which specifies the hostname of the Elasticsearch
    service, `elasticsearch.logging`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FLUENT_ELASTICSEARCH_PORT`, which specifies the port that’s used to communicate
    with Elasticsearch, `9200`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the Fluentd Pod runs in a different namespace to Elasticsearch, the hostname
    cannot be specified using its short name, that is, `elasticsearch`. Instead, the
    namespace part of the DNS name must also be specified, that is, `elasticsearch.logging`.
    As an alternative, the **fully qualified domain name** (**FQDN**), `elasticsearch.logging.svc.cluster.local`,
    can also be used. But since the last part of the DNS name, `svc.cluster.local`,
    is shared by all DNS names inside a Kubernetes cluster, it does not need to be
    specified.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a number of volumes, that is, filesystems, are mapped to the Pod,
    as follows:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s take a look at the source code in detail:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Three folders on the host (that is, the Node) are mapped to the Fluentd Pod.
    These folders contain the log files that Fluentd will tail and collect log records
    from. The folders are `/var/log`, `/var/lib/docker/containers`, and `/run/log/journal`.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our own configuration file, which specifies how Fluentd will process log records
    from our microservices, is mapped using a ConfigMap called `fluentd-hands-on-config`
    to the `/fluentd/etc/conf.d` folder. The base Docker image configures Fluentd
    to include any configuration file that’s found in the `/fluentd/etc/conf.d` folder.
    See the *Configuring Fluentd* section for details.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code of the manifest file for the DaemonSet, see the `kubernetes/efk/fluentd-ds.yml`
    file.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve walked through everything, we are ready to perform the deployment
    of Fluentd.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Running the deploy commands
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy Fluentd, we have to build the Docker image, create the ConfigMap,
    and finally deploy the DaemonSet. Run the following commands to perform these
    steps:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the Docker image and tag it with `hands-on/fluentd:v1` using the following
    command:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create the ConfigMap, deploy Fluentd’s DaemonSet, and wait for the Pod to be
    ready with the following commands:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Verify that the Fluentd Pod is healthy with the following command:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Expect a response of `2023-05-22 14:59:46 +0000 [info]: #0 fluentd worker is
    now running worker=0`.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As for Elasticsearch and Kibana, you might need to wait for a minute or two
    before Fluentd responds with this message.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Fluentd will start to collect a considerable number of log records from the
    various containers in the Minikube instance. After a minute or so, you can ask
    Elasticsearch how many log records have been collected with the following command:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The command can be a bit slow the first time it is executed but should return
    a total count of several thousands of log records. In my case, it returned `55607`.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This completes the deployment of the EFK stack. Now, it’s time to try it out
    and find out what all the collected log records are about!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Trying out the EFK stack
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we need to do before we can try out the EFK stack is to initialize
    Kibana so it knows what indices to use in Elasticsearch.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: An **index** in Elasticsearch corresponds to a **database** in SQL concepts.
    The SQL concepts **table**, **row**, and **column** correspond to **type**, **document**,
    and **property** in Elasticsearch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that is done, we will try out the following common tasks:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: We will start by analyzing what types of log records Fluentd has collected and
    stored in Elasticsearch. Kibana has a very useful visualization capability that
    can be used for this.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will learn how to find all related log records created by the microservices
    while processing an external request. We will use the **trace ID** in the log
    records as a correlation ID to find related log records.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we will learn how to use Kibana to perform **root cause analysis**,
    finding the actual reason for an error.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing Kibana
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start to use Kibana, we must specify what search indices to use in
    Elasticsearch and what field in the indices holds the timestamps for the log records.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Just a quick reminder that we are using a certificate created by our own CA,
    meaning that it is not trusted by web browsers! For a recap on how to make web
    browsers accept our certificate, see the *Observing the service mesh* section
    of *Chapter 18*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to initialize Kibana:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Open Kibana’s web UI using the [https://kibana.minikube.me](https://kibana.minikube.me)
    URL in a web browser.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Welcome home** page, click on the hamburger menu **≡** (three horizontal
    lines) in the upper-left corner, and click on **Stack Management** at the bottom
    of the menu to the left.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Management** menu, go to the bottom and select **Index Patterns**.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the button named **Create index pattern**.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `logstash-*` as the index pattern name and click on the **Next Step**
    button.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indices are, by default, named `logstash` for historical reasons, even though
    Fluentd is used for log collection.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Click on the drop-down list for the **Timestamp field** and select the only
    available field, `@timestamp`.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Create index pattern** button.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kibana will show a page that summarizes the fields that are available in the
    selected indices.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: With Kibana initialized, we are ready to examine the collected log records.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the log records
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the deployment of Fluentd, we know that it immediately started to collect
    a significant number of log records. So, the first thing we need to do is get
    an understanding of what types of log records Fluentd has collected and stored
    in Elasticsearch.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Kibana’s visualization feature to divide the log records by the
    Kubernetes namespace and then ask Kibana to show us how the log records are divided
    by the type of container within each namespace. A pie chart is a suitable chart
    type for this type of analysis. Perform the following steps to create a pie chart:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In Kibana’s web UI, click on the hamburger menu again and select **Visualize
    Library** under **Analytics** in the menu.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Create new visualization** button and select the **Lens** type
    on the next page. A web page like the following will be displayed:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_03.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.3: Starting to analyze log records in Kibana'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Verify that **logstash-*** is the selected index pattern in the top-left drop-down
    menu.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Bar vertical stacked** drop-down menu next to the index pattern, select
    **Pie** as the visualization type.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the time picker (a date interval selector) above the pie chart, set a date
    interval large enough to cover log records of interest (set to the **Last 15 minutes**
    in the following screenshot). Click on its calendar icon to adjust the time interval.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the field named **Search field names** below the index pattern, enter `kubernetes.namespace_name.keyword`.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **Available fields** list, the field **kubernetes.namespace_name.keyword**
    is now present. Drag this field into the big box in the middle of the page, named
    **Drop some fields here to start**. Kibana will immediately start to analyze log
    records and render a pie chart divided into Kubernetes namespaces.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In my case, it looks like:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_04.png)'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.4: Kibana analysis of log records per Kubernetes namespace'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can see that the log records are divided into the Namespaces we have been
    working with in the previous chapters: `kube-system`, `istio-system`, `logging`,
    and our own `hands-on` Namespace. To see what containers have created the log
    records per Namespace, we need to add a second field.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the **Search field names** field, enter `kubernetes.container_name.keyword`.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Available fields** list, the field `kubernetes.container_name.keyword`
    is now present. Drag this field into the big box in the middle of the page showing
    the pie chart. Kibana will immediately start to analyze log records and render
    a pie chart divided by Kubernetes namespace and container name.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the result of *step 9*, we can see a lot of log records coming from `coredns`,
    67% in my case. Since we are not particularly interested in these log records,
    we can remove them by adding a filter with the following steps:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **+ Add filter** (in the top-left corner).
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Field** `kubernetes.container_name.keyword` and the **is not -****Operator**.
    Finally, enter the **Value** `coredns` and click on the **Save** button.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In my case, the rendered pie chart now looks like this:![A screenshot of a computer  Description
    automatically generated with medium confidence](img/B19825_19_05.png)
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.5: Kibana analysis of log records per namespace and container'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can find the log records from our microservices. Most of the log records
    come from the `review` and `recommendation` microservices. The `product` and `product-composite`
    microservices can be found under the **other** section of the pie chart.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Wrap up this introduction to how to analyze what types of log records we have
    collected by saving this pie chart in a dashboard. Click on the **Save** button
    in the top-right corner.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the page named **Save Lens visualization**, do the following:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give it a **Title**, for example, `hands-on-visualization`.
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a **Description**, for example, `This is my first visualization in Kibana`.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Add to dashboard** box, select **New**. The page should look like
    this:'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19825_19_06.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.6: Creating a dashboard in Kibana'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the button named **Save and go to Dashboard**. A dashboard like the
    following should be presented:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_07.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.7: The new dashboard in Kibana'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Save** button in the top-right corner, give the dashboard a name,
    for example, `hands-on-dashboard`, and click on the **Save** button.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can now always go back to this dashboard by selecting **Dashboard** from
    the hamburger menu.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Kibana contains tons of features for analyzing log records – feel free to try
    them out on your own. For inspiration, see [https://www.elastic.co/guide/en/kibana/7.17/dashboard.html](https://www.elastic.co/guide/en/kibana/7.17/dashboard.html).
    We will now move on and start to locate the actual log records from our microservice.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the log records from microservices
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will learn how to utilize one of the main features of centralized
    logging, finding log records from our microservices. We will also learn how to
    use the trace ID in the log records to find log records from other microservices
    that belong to the same process, for example, processing an external request sent
    to the public API.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by creating some log records that we can look up with the help of
    Kibana. We will use the API to create a product with a unique product ID and then
    retrieve information about the product. After that, we can try to find the log
    records that were created when retrieving the product information.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'The creation of log records in the microservices has been updated a bit from
    the previous chapter so that the `product-composite` and the three core microservices,
    `product`, `recommendation`, and `review`, all write a log record with the log
    level set to `INFO` when they begin processing a `get` request. Let’s go over
    the source code that’s been added to each microservice:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Product composite microservice log creation:'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Product microservice log creation:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Recommendation microservice log creation:'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Review microservice log creation:'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For more details, see the source code in the `microservices` folder.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to use the API to create log records and, after
    that, use Kibana to look up the log records:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Get an access token with the following command:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As mentioned in the introduction to this section, we will start by creating
    a product with a unique product ID. Create a minimalistic product (without recommendations
    and reviews) for `"productId" :1234` by executing the following command:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Read the product with the following command:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Expect a response similar to the following:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B19825_19_08.png)'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.8: Look up the product with productId = 1234'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hopefully, we got some log records created by these API calls. Let’s jump over
    to Kibana and find out!
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the Kibana web page, click **Discover** from the hamburger menu. You will
    see something like the following:![A screenshot of a computer  Description automatically
    generated with medium confidence](img/B19825_19_09.png)
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.9: Kibana web UI with its major parts'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the top-left corner, we can see that Kibana has found **5,350** log records.
    The time picker shows that they are from the **Last 15 minutes**. In the histogram,
    we can see how the log records are spread out over time. Below the histogram is
    a table showing the most recent log events that were found by the query.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to change the time interval, you can use the time picker. Click
    on its calendar icon to adjust the time interval.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get a better view of the content in the log records, add some fields from
    the log records as columns in the table under the histogram.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to see all available fields, click on the down arrow to the right
    of the **Filter by type** label, and unselect **Hide empty fields**.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the fields from the list of **Available fields** to the left. Scroll
    down until the field is found. To find the fields more easily, use the field named
    **Search field names** to filter the list of available fields.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hold the cursor over the field and a **+** button will appear (a white cross
    in a blue circle); click on it to add the field as a column in the table. Select
    the following fields, in order:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`spring.level`, the log level'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubernetes.namespace_name`, the Kubernetes namespace'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubernetes.container_name`, the name of the container'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spring.trace`, the trace ID used for distributed tracing'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`log`, the actual log message'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To save some space, you can hide the list of fields by clicking on the collapse
    icon next to the index pattern field (containing the text `logstash-*`).
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The web page should look something like the following:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_10.png)'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.10: Kibana web UI showing log records'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The table now contains information that is of interest regarding the log records!
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To find log records from the call to the `GET` API, we can ask Kibana to find
    log records where the log field contains the text `product.id=1234`. This matches
    the log output from the `product-composite` microservice that was shown previously.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be done by entering `log:"product.id=1234"` in the top-left **Search**
    field and clicking on the **Update** button (this button can also be named **Refresh**).
    Expect one log record to be found:'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_11.png)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.11: Kibana web UI showing a log record for productId = 1234'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify that the timestamp is from when you called the `GET` API and verify that
    the name of the container that created the log record is `product-composite`,
    that is, verify that the log record was sent by the product composite microservice.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we want to see the related log records from the other microservices that
    participated in the process of returning information about the product with product
    ID `1234`. In other words, we want to find log records with the same **trace ID**
    as that of the log record we found.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To do this, place the cursor over the `spring.trace` field for the log record.
    Two small magnifying glasses will be shown to the right of the field, one with
    a **+** sign and one with a **-** sign. Click on the magnifying glass with the
    **+** sign to filter on the trace ID.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clear the **Search** field so that the only search criterion is the filter of
    the trace field. Then, click on the **Update** button to see the result. Expect
    a response like the following:![A screenshot of a computer  Description automatically
    generated with medium confidence](img/B19825_19_12.png)
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.12: Kibana web UI showing log records for a trace ID'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see some detailed debug messages that clutter the view; let’s get rid
    of them!
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Place the cursor over a **DEBUG** value and click on the magnifying glass with
    the **–** sign to filter out log records with the log level set to **DEBUG**.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We should now be able to see the four expected log records, one for each microservice
    involved in the lookup of product information for the product with product ID
    `1234`:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_13.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.13: Kibana web UI showing log records for a trace ID with log level
    = INFO'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the filters that were applied included the trace ID but excluded
    log records with the log level set to **DEBUG**.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to find the expected log records, we are ready to take
    the next step. This will be to learn how to find unexpected log records, that
    is, error messages, and how to perform root cause analysis to find the reason
    for these error messages.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Performing root cause analyses
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important features of centralized logging is that it makes it
    possible to analyze errors using log records from many sources and, based on that,
    perform root cause analysis, finding the actual reason for the error message.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will simulate an error and see how we can find information
    about it all the way down to the line of source code that caused the error in
    one of the microservices in the system landscape. To simulate an error, we will
    reuse the fault parameter we introduced in *Chapter 13*, *Improving Resilience
    Using Resilience4j*, in the *Adding programmable delays and random errors* section.
    We can use this to force the `product` microservice to throw an exception. Perform
    the following steps:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to generate a fault in the `product` microservice
    while searching for product information on the product with product ID `1234`:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Expect the following error in response:'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, software, multimedia  Description
    automatically generated](img/B19825_19_14.png)'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 19.14: A request that caused an error in the processing'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we must pretend that we have no clue about the reason for this error! Otherwise,
    the root cause analysis wouldn’t be very exciting, right?
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s assume that we work in a support organization and have been asked to investigate
    a problem that just occurred when an end user tried to look up information regarding
    a product with product ID `1234` but got an error message saying “`500 Internal
    Server Error`" in response.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before we start to analyze the problem, let’s delete the previous search filters
    in the Kibana web UI so that we can start from scratch. For each filter we defined
    in the previous section, click on its close icon (an **x**) to remove it.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start by using the time picker to select a time interval that includes the point
    in time when the problem occurred. In my case, 15 minutes is sufficient.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select log records belonging to our namespace, `hands-on`. This can be done
    by the following steps:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the list of fields to the left, by clicking on the hamburger icon (**≡**)
    in the top-left corner.
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the field **kubernetes.namespace_name** in the list of **Selected fields**.
    A list of the top five namespaces is shown.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **+** sign after the **hands-on** namespace.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, search for log records with the log level set to **WARN** within this
    time frame where the log message mentions product ID `1234`. This can be done
    by clicking on the `spring.level` field in the list of selected fields. When you
    click on this field, its most used values will be displayed under it. Filter on
    the **WARN** value by clicking on its **+** sign. Kibana will now show log records
    within the selected time frame with their log level set to **WARN** from the **hands-on**
    namespace, like this:![A screenshot of a computer  Description automatically generated
    with medium confidence](img/B19825_19_15.png)
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.15: Kiali web UI, showing log records that report ERRORs'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see a number of error messages related to product ID `1234`. The top
    log entries have the same trace ID, so this seems like a trace ID of interest
    to use for further investigation. The first log entry also contains the text reported
    by the end user **500** and **Internal Server Error**, and the error message **Something
    went wrong…**, which probably has to do with the root cause of the error.
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Filter on the trace ID of the first log record as we did in the previous section.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the filter of the `WARN` log level to be able to see all the records
    belonging to this trace ID. Expect Kibana to respond with a lot of log records
    looking something like this:![A screenshot of a computer  Description automatically
    generated with medium confidence](img/B19825_19_16.png)
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 19.16: Kiali web UI, looking for the root cause'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unfortunately, we cannot find any stack trace identifying the root cause by
    using trace IDs. This is due to a limitation in the Fluentd plugin we use for
    collecting multiline exceptions, `fluent-plugin-detect-exceptions`. It cannot
    relate stack traces to the trace ID that was used. Instead, we can use a feature
    in Kibana to find surrounding log records that have occurred near in time to a
    specific log record.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Expand the log record that says **Error body: {… status”:500,”error”:”Internal
    Server Error”,”message”:”Something went wrong...”…}** using the arrow to the left
    of the log record. Detailed information about this specific log record will be
    revealed:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_17.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.17: Kiali web UI, expanding the log record with the root cause log
    message'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a link named **View surrounding documents**; click on it to see
    nearby log records. Scroll down to the bottom of the page to find a **Load** field
    where the number of records can be specified. Increase the default value, from
    5 to 10\. Expect a web page like the following:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_18.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.18: Kiali web UI, the root cause found'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'The third log record below the expanded log record contains the stack trace
    for the error message **Something went wrong...**. This error message looks interesting.
    It was logged by the `product` microservice just five milliseconds before the
    expanded log record. They seem to be related! The stack trace in that log record
    points to line 104 in `ProductServiceImpl.java`. Looking at the source code (see
    `microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java`),
    line 104 looks as follows:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is the root cause of the error. We did know this in advance, but now we
    have seen how we can navigate to it as well.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, the problem is quite simple to resolve; simply omit the `faultPercent`
    parameter in the request to the API. In other cases, resolving the root cause
    can be much harder to figure out!
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the root cause analysis. Click on the back button in the web
    browser to get back to the main page.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to reuse the configuration of the search criteria and table layout,
    its definition can be saved by Kibana. Select, for example, to filter on log records
    from the `hands-on` namespace and click on the **Save** link in the top-right
    menu. Give the search definition a name and click on the **Save** button. The
    search definition can be restored when required using the **Open** link in the
    menu.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This concludes this chapter on using the EFK stack for centralized logging.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the importance of collecting log records from
    microservices in a system landscape into a common centralized database where analysis
    and searches of the stored log records can be performed. We used the EFK stack,
    Elasticsearch, Fluentd, and Kibana, to collect, process, store, analyze, and search
    for log records.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd was used to collect log records not only from our microservices but
    also from the various supporting containers in the Kubernetes cluster. Elasticsearch
    was used as a text search engine. Together with Kibana, we saw how easy it is
    to get an understanding of what types of log records we have collected.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to use Kibana to perform important tasks such as finding
    related log records from cooperating microservices and how to perform root cause
    analysis, finding the real problem for an error message.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Being able to collect and analyze log records in this way is an important capability
    in a production environment, but these types of activities are always done afterward
    once the log record has been collected. Another important capability is to be
    able to monitor the current health of the microservices, collecting and visualizing
    runtime metrics in terms of the use of hardware resources, response times, and
    so on. We touched on this subject in the previous chapter, and in the next chapter,
    we will learn more about monitoring microservices.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A user searched for `ERROR` log messages in the `hands-on` namespace for the
    last 30 days using the search criteria shown in the following screenshot, but
    none were found. Why?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_19.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.19: Kiali web UI, not showing expected log records'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: A user has found a log record of interest (shown below). How can the user find
    related log records from this and other microservices, for example, that come
    from processing an external API request?
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19825_19_20.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.20: Kiali web UI; how do we find related log records?'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: A user has found a log record that seems to indicate the root cause of a problem
    that was reported by an end user. How can the user find the stack trace that shows
    where in the source code the error occurred?
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screen shot of a computer  Description automatically generated with low
    confidence](img/B19825_19_21.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.21: Kiali web UI; how do we find the root cause?'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Why doesn’t the following Fluentd configuration element work?
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How can you determine whether Elasticsearch is up and running?
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You suddenly lose connection to Kibana from your web browser. What could have
    caused this problem?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
