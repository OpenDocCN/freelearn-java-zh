<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Quarkus Core Concepts</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>Java was introduced to the open source community over 20 years ago. Since then, we cannot think of a single large IT company or organization that doesn't use Java. For this reason, Java is often regarded as a <strong>corporate</strong> language, which is not a bad thing <em>per se</em>: Java is the enterprise standard, and it's an extremely mature language with a huge ecosystem of tools and libraries around it and still the most used language by developers in the world.<br/></span></p>
<p class="selectionShareable"><span>20 years in the IT industry is, however, a considerable amount of time. Since the beginning, Java has gone through a long list of optimizations with the burden of keeping backward compatibility with earlier releases. Today, however, the IT landscape has significantly changed with the rise of new standards such as the cloud, containers, microservices, and Reactive Programming. Do we still need to use Java to address the latest application architectures and reach a higher level of productivity and efficiency? Yes! This book promises to do this while teaching you about <strong>Quarkus</strong>, a Kubernetes-native framework that will take supersonic, subatomic Java to new heights!</span></p>
<p>In the first part of this book, we will learn how to create Quarkus applications with simple tools while using a development environment to code, execute, and debug them. After completing all the green bars, we will concentrate on advanced topics to show you how to combine multiple Quarkus extensions to produce a serverless infrastructure.</p>
<p>As far as this chapter is concerned, we will have a quick tour of the Quarkus technology by covering these topics:</p>
<ul>
<li>An overview of the IT landscape, showing the benefits of cloud-native applications and microservices</li>
<li>The basics of the Quarkus architecture</li>
<li>Installing the required software (GraalVM to compile code natively and a development environment)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can find the source code for the project in this chapter on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter01">https://github.com/PacktPublishing/Hands-On-Cloud-Native-Applications-with-Java-and-Quarkus/tree/master/Chapter01</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From the big data hype to Kubernetes</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>About 10 years ago, the biggest buzz in the IT industry was the term <em>big data</em></span>. <span>Every major enterprise was racing to harness the mystical powers of massive, yet supposedly manageable, silos of data. Equipped with big data, no problem would prove insurmountable, and all forecasts would be met.</span></p>
<p class="selectionShareable"><span>But lately, these forecasts appear to have faded, and the worst-kept secret in the IT industry is that big data is dead – at least as we knew it. This doesn't mean that the volume or growth of data has broken down – or the opposite. It's just the underlying technology that has changed, which means that the architectures of applications that use big data have too.</span></p>
<p class="selectionShareable"><span>Take Hadoop as an example, which has been the icon of the big data hype. It was designed based on a set of assumptions that dramatically changed in a short time. One of these assumptions was that, in order to process a large batch of data, network latency was the evil and cloud-native storage simply wasn't an option. At that time, most of the IT industry data was on-premise, so the focus was on avoiding moving around big sets of information. This meant that data was to be co-located in order to compute it efficiently.</span></p>
<p>Today, this scenario has changed quite a bit: most applications still use large amounts of data, but data is now processed on the fly. That is to say, we now stream data instead of processing the whole dataset multiple times.</p>
<p class="selectionShareable"><span>Besides this, the network latency barrier has become less of an issue for cloud providers and there are even multiple cloud sources to choose from. Also, companies now have the option to deploy their own private cloud on-premise, leading to new scenarios such as <strong>hybrid clouds</strong>.</span></p>
<p class="selectionShareable"><span>Therefore, the focus is what really changed: today, big data does not merely mean a <strong>big</strong> quantity of datasets but flexible storage options for a big quantity of data.</span></p>
<p class="selectionShareable"><span>This is where containers and, specifically, Kubernetes fits in. In a nutshell, you can think of a container as a packaged application that contains just the libraries that are needed to run it, and Kubernetes is like an orchestrating system that makes sure all the containers have the appropriate resources while managing their life cycle.</span></p>
<p class="selectionShareable"><span>Kubernetes runs images and manages containers using <strong>Docker</strong>. However, Kubernetes can use other engines too (for example, <kbd>rkt</kbd>). Since we will be building our applications on top of Kubernetes, we will provide a short overview of its architecture in the next section.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Kubernetes architecture in a nutshell</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>The architecture of Kubernetes is focused around the concept of a loosely coupled, flexible mechanism for service discovery. Like most other distributed middleware platforms, a Kubernetes cluster is composed of one or more master nodes and multiple compute nodes.</span> The following diagram depicts a high-level view of a Kubernetes cluster:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5c429d97-786f-47cd-b488-d902f5897574.png" style=""/></div>
<p class="selectionShareable"><span><strong>Kubernetes Master nodes</strong> essentially make up the brain of the cluster. They are responsible for managing the overall cluster, exposing APIs, and scheduling deployments. <strong>Kubernetes nodes</strong> (right-hand side of the preceding diagram) contain the services that are needed to run applications in components called Pods.<br/></span></p>
<p>Each <strong>master node</strong> contains the following components:</p>
<ul>
<li><strong>API Server</strong>: This synchronizes and validates the information running in Pods and services.</li>
<li><strong>etcd</strong>: This provides consistent and highly available storage for the cluster data. <span>You can think of <kbd>etcd</kbd> as the brain's shared memory.</span></li>
<li><strong>Controller Manager server</strong>: This checks for changes in the <kbd>etcd</kbd> service and uses its API to enforce the desired state.</li>
<li><strong>HAProxy</strong>: This can be added when we're configuring HA masters to balance loads between several master endpoints.</li>
</ul>
<p><span><strong>Kubernetes nodes</strong> (simply called <strong>nodes</strong>) can be considered workhorses of a Kubernetes cluster. Each node exposes a set of resources (such as computing, networking, and storage) to your applications. The node also ships with additional components for service discovery, monitoring, logging, and optional add-ons. In terms of infrastructure, you can run a node as a <strong>virtual machine</strong> (<strong>VM</strong>) in your cloud environment or on top of bare-metal servers running in the data center.</span></p>
<p>Each node contains the following components:</p>
<ul>
<li><span><strong>Pod</strong>: This allows us to logically group containers and pieces of our application stacks together. A Pod acts as the logical boundary for such containers with shared resources and contexts. Pods can be scaled at runtime by creating Replica sets. This, in turn, ensures that the required number of Pods is always run by the deployment.</span></li>
<li><strong>Kubelet</strong>: This is an agent that runs on each node in the Kubernetes cluster. It makes sure that the containers are running in a Pod.</li>
<li><strong>Kube-Proxy</strong>: This maintains network rules on nodes to allow network communication between Pods.</li>
<li><strong>Container Runtime</strong>: This is the software that is responsible for running containers. Kubernetes supports multiple container runtimes (such as Docker, <kbd>containerd</kbd>, <kbd>cri-o</kbd>, and <kbd>rktlet</kbd>).</li>
</ul>
<p class="selectionShareable"><span>Now that we've covered the basics of the Kubernetes architecture, let's look at the top advantages that it can bring to your organization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits of using Kubernetes</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>The advantages that Kubernetes can bring to your organization are as follows:</span></p>
<ul>
<li class="selectionShareable"><span>Kubernetes greatly simplifies container management. As you have learned, when using Kubernetes, there's no need to manage containers directly. Instead, you just to have manage Pods. In order to have your applications available in your Pods, Kubernetes introduced an abstraction called a <strong>service</strong>. It defines a logical set of Pods with their IP address. This level of abstraction enhances fault tolerance and reduces downtime by launching containers on different machines.</span></li>
<li class="selectionShareable"><span>Kubernetes speeds up the process of building, testing, and releasing software by supporting a wide range of programming languages (Java, Go, Python, and so on) and offering advanced deployment features (automated rollouts and rollbacks, canary deployments, and more). This makes it a lot easier to configure effective <strong>Continuous Integration/Continuous Delivery</strong> (<strong>CI/CD</strong>) pipelines for your software.</span></li>
<li class="selectionShareable"><span>Kubernetes provides the fastest and least costly horizontal scalability for your pods, so when the number of users for your application increases, you can configure the replication service to fire new Pods and balance the load across them to avoid downtime.</span></li>
<li class="selectionShareable"><span>It's worth mentioning that Kubernetes is able to manage both stateless and stateful applications, because it allows ephemeral storage and persistent volumes. It also supports a number of storage types, such as NFS, GlusterFS, and cloud storage systems. Furthermore, a <strong>persistent volume</strong> (<strong>PV</strong>) life cycle doesn't depend on any pod using it, so you can keep the data as long as you need it.</span></li>
</ul>
<p>The benefits of using Kubernetes as a service orchestrator in your industry are clearly evident, but the next question is, how do we write our services to get the most out of this architecture? Can we still write our applications using the same standards we have learned about in the last few years? The next section will address this dilemma.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From Java EE to MicroProfile</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><strong>Java Enterprise Edition</strong> (<strong>EE</strong>) has reached an outstanding level of maturity and has a huge adoption in the IT Enterprise. A Java EE application is typically packaged as a monolithic application and deployed in an application server, which can host multiples of them.</p>
<div class="selectionShareable packt_infobox">A monolithic application can be thought of as a self-contained application that includes both the user interface and the business components that are required to run the applications.</div>
<p class="selectionShareable">This approach has been widely used for years. T<span>he reason is simple: monolithic applications are conceptually simple to develop and package because everything is contained in a bundle and can be edited with a single IDE. Also, scaling monolithic applications is simple: all you need to do is scale a single component.</span></p>
<p>As a result, the traditional way of coding enterprise applications produced an extremely large set of applications that were supposed to be available as long-running processes and needed an application server to manage their <strong>High Availability</strong> (<strong>HA</strong>). In turn, some other tooling was needed to manage server restarts in the case of failures and to check the overall health of the system.</p>
<p class="selectionShareable"><span>As server-based monolithic applications continued growing, several disadvantages became evident, as follows:</span></p>
<ul>
<li><strong>Difficult to maintain</strong>: This is due to the size of the applications, which makes it complex to create a patch for them.</li>
<li><strong>Limited scalability</strong>: You can scale the whole application, not the single services.</li>
<li><strong>Longer release cycles</strong>: Any changes that are made to the code require that we deploy the whole application, which complicates things when multiple teams are working on the same application.</li>
<li><strong>Less isolation</strong>: Deploying multiple applications in an application server can potentially lead to a whole system failure from a single application misbehaving.</li>
<li><strong>Slower startup</strong>: T<span>he startup time of a full monolithic stack is ill-famed to be slow, especially if multiple applications are deployed at the same time and potentially competing for the same resources.<br/></span></li>
<li><strong>Complex monitoring</strong>: It is harder to monitor and tune the activity of a single monolithic application which delivers a myriad of metrics.</li>
<li><strong>More complex CI/CD</strong>: Configuring a CI/CD pipeline for multiple monolithic applications is equally as hard.</li>
</ul>
<p class="selectionShareable"><span>In this scenario, a new paradigm called <strong>microservices</strong> emerged around a simple yet not new idea. The main theme behind microservices is that, for certain types of applications, once they are split into smaller and composable pieces, it's easier to build and maintain them. In a service-based architecture, we don't need to measure the uptime of our applications in weeks or months anymore since we can activate our services when they are needed. Therefore, the timing factor can be as little as minutes or seconds.</span></p>
<p class="selectionShareable"><span>In such an architecture, each component has its own life cycle spanning from development to testing, and the resulting application is simply the combination of all these single components. This approach marks a sensible departure from <strong>monolithic</strong> applications, where everything is built and tested as a single unit.</span></p>
<p class="selectionShareable"><span>Applications that are built as a set of smaller modular components are simpler to understand, easier to test/debug, and easier to maintain over the application life cycle. A microservice architecture leverages the agility of your company by reducing the time it takes to deploy improvements to production. This approach has been tried and tested and is superior for the following reasons:</span></p>
<ul>
<li class="selectionShareable"><span><strong>Increased resilience</strong>: The microservice architecture increases the system's overall capability to withstand any kind of unexpected failures or faults of components or networks by spinning up another component, even as the remaining application continues to function.</span></li>
<li class="selectionShareable"><span><strong>Developer independence</strong>: By working in smaller teams in parallel, you can speed up the work that's being done, especially for large enterprise applications that are composed of teams that are geographically and culturally diverse.</span></li>
<li class="selectionShareable"><span><strong>Scalability</strong>: Fewer resources are demanded by smaller components. This means we can easily scale them to meet the increasing demand of only that specific component.</span></li>
<li class="selectionShareable"><span><strong>CI/CD life cycle automation</strong>: Single components fit smoothly into CD pipelines and scenarios that have complex deployment.</span></li>
<li class="selectionShareable"><span><strong>Simpler mapping with the business</strong>: Microservice architectures are easier to map with the business domain logic since they have increasing independence and transparency across the organization.<br/></span></li>
</ul>
<p>To obtain the best results from our <strong>Software as a Service</strong> (<strong>SaaS</strong>), a methodology is required. In the next section, we will discuss the Twelve-Factor App methodology, which is recommended by developers for smoothly working and delivering applications with a focus on microservices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Twelve-Factor App methodology</h1>
                </header>
            
            <article>
                
<p>In 2011, the Heroku founder Adam Wiggins published <strong>the Twelve-Factor App</strong> methodology, which soon became a key reference for building <strong>Software as a Service</strong> (<strong>SaaS</strong>) based on their own experiences. This methodology is not exclusive to any programming language but is compatible with a microservices architecture and is based on containers and CI/CD pipelines. Let's take a look at the 12 factors:</p>
<ul>
<li><strong>Code base</strong>: You should build your application on top of one code base, tracked by a <strong>Version Control System</strong> (<strong>VCS</strong>). You should rely on a base repository for an individual application to ease CI/CD pipelines. It follows that deployments should be automatic so that everything can run in different environments without anything needing to be done.</li>
<li><strong>Dependencies</strong>: Don't push any dependencies to your project code base. Instead, use a package manager so that you will have all the dependencies synced across your environments to make sure you reproduce the same behavior.</li>
<li><strong>Config</strong>: Store your configuration in environment variables. The configuration should be well parted from the code so that the configuration varies in terms of where the application has been deployed.</li>
<li><strong>Backing services</strong>: Services should be easily interchangeable so that you can manage your backing services as attached resources. You must be able to easily exchange the backing services from one provider to another without changing your code. This maximizes portability and helps maintain your system.</li>
<li><strong>Build, run, release</strong>: There should be a clear and strict separation between the build, release, and run stages. You can achieve this by assigning a unique release ID and allowing releases to roll back. The automation between these stages should be as easy as possible.</li>
<li><strong>Stateless processes</strong>: This factor lies at the core of the microservices architecture. You should not be introducing state into your services. Any data that needs to be persisted must be stored in a backing service, typically a database or another storage.</li>
<li><strong>Port binding</strong>: By this factor, your application should be completely self-contained. It should not depend on the runtime startup of a web server into the execution environment to create a frontend service. The web app should make HTTP applications as a service available by binding the service to a port.</li>
<li><strong>Concurrency</strong>: You should break down your application into much smaller pieces. Smaller, well-defined apps allow you to scale out as needed to handle varying loads. You should be able to individually scale the single component.</li>
<li><strong>Disposability</strong>: You should aim to maximize the robustness of your systems by coding applications with a fast startup and graceful shutdown. This means you should be able to handle unexpected failures. A recommended approach consists of using a robust async backend that returns notifications when failures occur.</li>
<li><strong>Dev/prod Parity</strong>: You should aim to keep the development, staging, and production phases similar and homogeneous to limit deviation and errors. This also implicitly encourages a DevOps culture where software development and operations are unified.</li>
<li><strong>Logs</strong>: Logging is a key factor for debugging and monitoring your application's general health. The place where logs are stored shouldn't be a concern for developers. Instead, these logs should be treated as a continuous stream that's being separately captured and stored by a service.</li>
<li><strong>Admin Processes</strong>: In many cases, developers perform one-off administrative or maintenance tasks, such as database migrations, application patching, or one-time script execution for the app. It is essential to run one-off admin processes in an environment that is similar to the regular long-running processes of the app.</li>
</ul>
<p>Although some of the preceding patterns may seem trivial at first glance, they become essential building blocks as your services start to grow. Therefore, when designing your microservices applications, keep in mind that most challenges are not related just to coding, but rather to getting the basics wrong. As a matter of fact, even good teams fail at microservices when they don't have a culture that embraces DevOps and key building blocks such as the Twelve-Factor App methodology.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MicroProfile initiative</h1>
                </header>
            
            <article>
                
<p>Having discussed the methodology of microservices, we will now cover some aspects related to the specific API that can be used to develop microservices.</p>
<p>While, at first glance, it appears that Java and microservices don't really match, it would be a bad idea to discard the entire Java EE ecosystem (rebranded as <strong>Jakarta EE</strong> and transferred to the Eclipse Foundation) that has been created. A lot of effort has already gone into reusing Java EE for coding microservices.</p>
<p>As a matter of fact, many major vendors, including IBM, Red Hat, and Payara, have already provided a lightweight and extensible runtime to power m<span>icroservices</span> and cloud deployments. Their individual efforts were naturally followed by an open collaboration within the <strong>MicroProfile.io</strong> initiative.</p>
<p>MicroProfile components are built upon the model of Java EE, thereby making the transition to microservices development natural. This means you will be able to reuse the valuable knowledge of Java EE you have accumulated over the years to flexibly use multiple vendor specs to define application requirements.</p>
<p>In its initial release, the MicroProfile initiative included just a small cutdown of the Java EE API (JAX-RS 2.0, CDI 1.2, and JSON-P 1.0).</p>
<p>In a short time, however, new MicroProfile projects have been added. In 2018 alone, we saw the advent of MicroProfile 1.3, 1.4, 2.0, and 2.1, and the projects contained in them. The current release of the MicroProfile initiative extends the standards with functionality that isn't part of Java EE, such as configuration, resiliency, monitoring, health checking, and distributed tracing.</p>
<p>The following diagram depicts the building blocks of MicroProfile projects according to the latest specification (at the time of writing this book):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/66256e0f-6b46-4b99-8103-988b1f0ce0cf.png" style=""/></div>
<p>MicroProfile alone, however, is mostly insufficient when it comes to developing a complex enterprise application. For example, it does not include an API for persistence, transaction, or <strong>Security Socket Layer</strong> (<strong>SSL</strong>) management out of the box. For this reason, we need a framework that leverages the MicroProfile API with extensible functionalities and can be orchestrated by Kubernetes, which is going to be the new application server, from the management point of view.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quarkus – a Kubernetes-native Java framework</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>One of the main challenges of a microservices architecture is that the proliferation of services can increase the complexity of your systems unless you have a valid framework to orchestrate them from.</span> <span>Also, without a centralized function for authentication, data management, and an API gateway, the advantages of a microservices architecture are invalidated by these challenges.</span></p>
<p class="selectionShareable"><span>In this</span> sense<span>, the arrival of Kubernetes is a real revolution in IT patterns. With the help of Kubernetes-based orchestration, you can enhance efficiency and resource utilization by managing and scheduling your microservices in a dynamic manner.</span> <span>This also adds an advanced resiliency level. You can continue to operate as demand varies, without worrying about container failure. To close the circle and unify all the components, we need a framework that has been specifically thought of to work in this kind of architecture. Let's meet <strong>Quarkus</strong>.</span></p>
<p class="selectionShareable"><span>Quarkus emerges as a first-class citizen when it comes to managing cloud-native enterprise applications and has lots of amazing features that can enable scenarios that haven't been possible before.</span> <span>As you will see in the upcoming sections, Quarkus is able to build thin native code from Java classes and create container images out of it that you can run on top of Kubernetes or OpenShift. Quarkus also leverages the best of the breed of Java libraries you have been working with for years, such as </span><span>RESTEasy</span><span>, Hibernate, Apache Kafka, Vert.x, and much more. Let's look at the highlights of this framework in more detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Native code execution</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>Native code execution has been attempted several times during the long history of Java, but it never got much adoption by developers. First of all, it required some external tooling as this isn't provided out of the box by the platform's vendor. For monolithic applications, the advantage of native execution is minor because, in the long run, due to the advancement in the Hot Spot technology, the speed of Java can become closer to native execution (provided that you are willing to pay for a slower application bootstrap).</span></p>
<p class="selectionShareable"><span>Nevertheless, in a microservice scenario, spinning up a bunch of native services plays a crucial role, and even optimizing seconds or a fraction of a second can play a huge difference. In much the same way, if you aim to reach the highest</span> <span>memory density requirements, the maximum requests' throughput, along with a consistent CPU performance, Quarkus' native execution fits neatly in the picture.</span></p>
<p class="selectionShareable"><span>On the other hand, you can smoothly transition to Quarkus using plain Java bytecode, still delivering applications with high memory density requirements, excellent CPU raw performance, advanced garbage collections tactics, a large set of libraries or monitoring tools that require the standard JDK, and the ubiquitous <em>compile once and run everywhere</em>.  <br/></span></p>
<p>The following table summarizes some typical use cases for choosing between native applications and Java applications when developing with Quarkus:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Quarkus Native applications</strong></td>
<td><strong>Quarkus<span> </span>Java</strong> <strong>applications</strong></td>
</tr>
<tr>
<td><span>Highest memory density requirements</span></td>
<td><span>High memory density requirements</span></td>
</tr>
<tr>
<td>More consistent CPU performance</td>
<td><span>Best raw performance (CPU)</span></td>
</tr>
<tr>
<td>Fastest startup time</td>
<td><span>Fast startup time</span></td>
</tr>
<tr>
<td>Simpler garbage collection</td>
<td><span>Advanced garbage collection</span></td>
</tr>
<tr>
<td>Highest throughput</td>
<td><span>A large set of libraries and tools that only work with JDK</span></td>
</tr>
<tr>
<td>No JIT spikes</td>
<td><span>Compile once, run anywhere</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p class="selectionShareable"><span>As resulting from this picture, Quarkus is a breakthrough as it leverages native code execution while preserving the capability for you to run your services with OpenJDK and use Hot Spot's rich dynamic code execution capabilities when required.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container first</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>As anticipated, one of the most promising features of Quarkus is the capability to</span> <span>automatically</span> <span>generate container images out of your applications. The minimal footprint of native applications is optimized to be run inside a container.<br/></span></p>
<p class="selectionShareable"><span>Generating container images of your native applications also defeats one common pitfall related to native execution, which is potential conflicts or errors when the build was done with a different OS. Since the container wraps the OS of your choice, you can provide container-safe native builds of your applications without hitting the risk of crash dumps or the infamous blue screen scenario.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unifying the imperative and Reactive Programming models</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>Most Java developers are familiar with the imperative programming model, which translates into a sequence of instructions that are used to modify an object's state. On the other hand, asynchronous programming has always been a challenge for Java developers due to its inherent complexity and the lack of a solid pattern for propagating asynchronous changes. In this context, a paradigm called <strong>Reactive Programming</strong> has gained popularity due to its ability to conjugate the asynchronous programming pattern with data streams and the propagation of change.</span></p>
<p class="selectionShareable"><span>Quarkus has been designed from the ground up to unify the two models in the same platform so that you can take the benefits of both programming models and use them in your IT organization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding that sparks joy</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>Even the most powerful framework available wouldn't gain widespread adoption if it were overly complex to use and required lots of coding and configuration to accomplish even minor functions.</span></p>
<p class="selectionShareable"><span>As we have learned from Spring Boot's success, developers are more productive with a framework that doesn't require you to spend lots of time on its setup or configuration. Out of the box, Quarkus provides the following:</span></p>
<ul>
<li class="selectionShareable"><span>A unified configuration that can be easily maintained in a single property file</span></li>
<li class="selectionShareable"><span>A large set of defaults so that you can actually write applications, even with no configuration at all<br/></span></li>
</ul>
<p class="selectionShareable"><span>Besides this, you can have extraordinary features, such as the following:</span></p>
<ul>
<li class="selectionShareable"><span>Live reload of applications, without any third-party plugins</span></li>
<li class="selectionShareable"><span>Straight to container generation with native executables</span></li>
<li>Simplified testing by adding testing extensions built specifically for Quarkus</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Best-of-breed Java libraries and standards</h1>
                </header>
            
            <article>
                
<p>The criterion that makes a development stack a successful one is a combination of various things, such as an active number of contributors, a high degree of recognition and use by top industrial actors, compliance to well-known standards, and strong and active criteria validators.</p>
<p class="selectionShareable"><span>On this matter, Quarkus brings a cohesive, full-stack framework by leveraging the best-of-breed libraries you are already familiar with, which are automatically wired together to produce the final artifact. Quarkus extensions include the full Eclipse MicroProfile Stack, a persistence API (JPA), a transaction manager (Narayana), a reactive framework (Vert.x), <span class="st">an asynchronous event-driven network application framewor</span>k (Netty), and much more.</span></p>
<p class="selectionShareable"><span>Quarkus also includes an extension framework that third-party component authors can leverage to extend the framework. The Quarkus extension framework greatly reduces the complexity of making third-party frameworks run on Quarkus and compile to a native binary.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quarkus architecture</h1>
                </header>
            
            <article>
                
<p>Now that we know about some of the highlights of Quarkus, let's have a more in-depth look at the architecture of this framework.</p>
<p>At the heart of Quarkus, there is the <strong>core</strong> component that does the hard work of rewriting our application in the build phase so that super-optimized native executable and Java-runnable applications can be generated. To do that, Quarkus <strong>core</strong> requires the cooperation of a set of tools:</p>
<ul>
<li><strong>Jandex</strong>: This is a space-efficient Java annotation indexer and offline reflection library that's able to index all runtime visible Java annotations and class hierarchies for a set of classes into a memory-efficient representation.</li>
<li><strong>Gizmo</strong>: This is a bytecode generation library used by Quarkus to produce Java bytecode.</li>
<li><strong>GraalVM</strong>: This is a set of components. Every component has a specific function, such as a compiler, an SDK API for the integration of Graal languages and the configuration of native images, and a runtime environment for JVM-based languages.</li>
<li><strong>SubstrateVM</strong>: This is a subcomponent of GraalVM that allows for the <strong>ahead-of-time</strong> (<strong>AOT</strong>) compilation of Java applications of Java programs into self-contained executables.</li>
</ul>
<p>Moving on to the list of available Quarkus extensions, first and foremost, Quarkus fully implements the MicroProfile specifications. Quarkus also includes a set of extensions for Hibernate ORM for handling persistence, a transaction manager (Narayana), a connection pool manager (Agroal), plus several more, such as an API for Apache Kafka, Camel Routes, and the ability to run reactive applications (Vert.X).</p>
<p>The following diagram summarizes the core components of the Quarkus architecture, although the list of available extensions cannot be exhaustive for the sake of brevity:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4a630408-ce48-4a88-b1a5-108025088b33.png" style=""/></div>
<p>Having covered the basics of the Quarkus architecture, without further ado, we will now shift our attention to the installation of the tools that we'll need in order to build and run Quarkus applications. Our to-do list isn't that long and will be addressed shortly. In the next section, we will be installing GraalVM and a development environment for our applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with GraalVM</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>To compile Java code into native executables, you will need an extension of the virtual machine called <strong>GraalVM</strong>. To be precise, GraalVM is a universal virtual machine that facilitates the compilation of the bytecode of various languages (such as Python, JavaScript, Ruby, and so on). In addition to this, it allows for the integration of those languages in the same project. It has a few other features as well, among which is one that offers <strong>Substrate VM</strong>, a framework that allows AOT compilation for applications written in various languages. It also allows us to compile JVM bytecode into a native executable.</span></p>
<p>GraalVM is similar to any other JDK available from other vendors, except that it has <strong>Java-based JVM Compiler Interface</strong> (<strong>JVMCI</strong>) support, and it uses Graal as its default JIT compiler. Therefore, it can't <em>just</em> execute Java code but also languages such as JS, Python, and Ruby. This can be done through a language abstract syntax tree interpreter called <strong>Truffle</strong>, which was developed by Oracle in association with GraalVM.</p>
<p>The following diagram depicts a high-level view of the GraalVM stack:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9293c0d5-0ca0-4101-8f3d-1579a89bc838.png" style=""/></div>
<div class="paragraph">
<p>All of this sounds great, but GraalVM comes with a price as well. The dynamic nature of Java is severely constrained; for example, there's <span>the default reflection mechanism, which will not work unless a class/member has been explicitly registered for reflection. Also, class loading at runtime, dynamic proxies, and static initializers require, at best, some changes/workarounds in order to work.</span><span> </span></p>
</div>
<div class="paragraph packt_tip">How does Quarkus overcome these limitations? The trick is to move as much framework initialization at build time. During this phase, Quarkus is able to discover which classes need reflection at runtime through <span>metadata discovery (such as annotations).</span> Quarkus uses a set of tools such as Jandex to optimize annotation processing and bytecode generation. Also, to overcome other limitations of GraalVM, Quarkus uses a single-pass, single class loader and programmatically provides compiler hints to enable extensive dead code elimination, thereby substantially cutting down the size of the executable file.</div>
<p class="selectionShareable CDPAlignLeft CDPAlign"><span>To get started, we will install GraalVM from <a href="https://www.graalvm.org/">https://www.graalvm.org/</a>. As you will see from the</span> <span class="packt_screen">Downloads</span> <span>page, the Community and Enterprise GraalVM editions are available. In this book, we will be using the Community Edition, so proceed by downloading the community version that fits with your OS.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing GraalVM</h1>
                </header>
            
            <article>
                
<p>There are several ways to get started with GraalVM. You can either download the zipped binary for your operating system or build it from the source. For the purpose of this book, we will choose the former option. The installation steps are as follows:</p>
<ol>
<li class="selectionShareable"><span>Navigate to the</span> <span class="packt_screen">Downloads</span> <span>page and choose <strong><span class="packt_screen">Community Edition</span></strong>. You will be redirected to the GitHub project where the project is hosted. Download the archive that matches with operating system. <strong>Please note that the recommended version of GraalVM to use with Quarkus 1.0.0.final is the version 19.2.1</strong></span>.<span> As a matter of fact, the newer 19.3.0 version does not meet the requirements of Quarkus 1.0.0.final.</span></li>
<li class="selectionShareable"><span>Extract the archive to your filesystem.</span></li>
</ol>
<p>You should have the following top-level structure in the folder where GraalVM has been extracted:</p>
<pre style="padding-left: 60px"><strong>graalvm-ce-19.2.1/</strong><br/><strong> ├── 3rd_party_licenses.txt</strong><br/><strong> ├── ASSEMBLY_EXCEPTION</strong><br/><strong> ├── bin</strong><br/><strong> ├── GRAALVM-README.md</strong><br/><strong> ├── include</strong><br/><strong> ├── jre</strong><br/><strong> ├── lib</strong><br/><strong> ├── LICENSE</strong><br/><strong> ├── man</strong><br/><strong> ├── release</strong><br/><strong> ├── sample</strong><br/><strong> ├── src.zip</strong></pre>
<p>As you can see, the top-level structure of GraalVM is quite similar to the JDK. In the <span><kbd>bin</kbd> folder,</span> you will find many utilities and replacements for JDK tools. <span>Notably, when you use the <kbd>java</kbd> command in GraalVM, it runs the JVM and the default compiler, which is Graal. <kbd>javac</kbd> can be used to compile your code. Apart from this, the following commands are essentials when it comes to leveraging the native and polyglot functionalities of GraalVM:</span></p>
<ul>
<li class="selectionShareable"><span><kbd>js</kbd>: This command can be executed to run plain JavaScript code if we pass a set of options and the JavaScript filename as an argument.<br/></span></li>
<li class="selectionShareable"><span><kbd>node</kbd>: This command can be used to run Node.js-based applications. It relies on the <kbd>npm</kbd> command to install Node.js modules.</span></li>
<li><span><kbd>native-image</kbd><strong>:</strong> This command takes your Java class(es) and builds an AOT compiled executable or a shared library. It is not included by default in most recent GraalVM installations and you need the <kbd>gu</kbd> tools to install it.<br/></span></li>
<li><kbd>npm</kbd>: This is the package manager for Node.js. It puts modules in place so that <kbd>node</kbd> can find them and manages dependency conflicts intelligently.</li>
<li class="selectionShareable"><span><kbd>lli</kbd><strong>:</strong> This is an LLVM bitcode interpreter that can execute LLVM bitcode in a managed environment.</span></li>
<li class="selectionShareable"><span><kbd>gu</kbd><strong>:</strong> This tool can be used to install language packs for Python, R, and Ruby, as well as the native-image tool.<br/></span></li>
</ul>
<p>The first thing we are going to do is export the path where <span>GraalVM</span> has been installed into our environment:</p>
<pre class="selectionShareable"><span><strong>export </strong></span><span><strong>GRAALVM_HOME=</strong></span><strong><span>/path/to/graal</span></strong></pre>
<p><span>In addition, it is recommended to complete the installation adding the <kbd>bin</kbd> folder of GraalVM to your OS's <kbd>PATH</kbd>. For example, on Linux, we would do the following:</span></p>
<pre class="selectionShareable"><strong><span> export PATH=$PATH:$</span></strong><span><strong>GRAALVM_HOME</strong></span></pre>
<p class="selectionShareable"><span>Optionally, you can resolve to the GraalVM installation directory by setting the <kbd>JAVA_HOME</kbd> environment variable like so:</span></p>
<pre class="selectionShareable"><strong><span> export JAVA_HOME=</span></strong><strong><span>$</span></strong><span><strong>GRAALVM_HOME</strong></span></pre>
<div class="packt_tip">All the preceding environment settings should be added to the script that initializes your shell. For most Linux distributions, this means putting them in the <kbd>.bashrc</kbd> file.</div>
<p class="selectionShareable"><span>After you have set the <kbd>PATH</kbd> environment variable, it's pretty simple to check language versions with GraalVM's launchers:</span></p>
<pre class="selectionShareable"><span><strong>$ java -version</strong><br/><strong>openjdk version "1.8.0_232"</strong><br/><strong>OpenJDK Runtime Environment (build 1.8.0_232-20191008104205.buildslave.jdk8u-src-tar--b07)</strong><br/><strong>OpenJDK 64-Bit GraalVM CE 19.2.1 (build 25.232-b07-jvmci-19.2-b03, mixed mode)</strong><br/><br/><strong>$ node -v</strong><br/><strong>v10.16.3</strong><br/> <br/><strong>$ lli --version</strong><br/><strong>LLVM (GraalVM CE Native 19.2.1)</strong><br/></span></pre>
<p class="selectionShareable"><span>The executables belonging to all the language runtimes in GraalVM emulate the behavior of the languages' default runtimes. It should be enough to include GraalVM at the beginning of your <kbd>PATH</kbd> environment variable in order to run your applications with GraalVM.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running a Java application with GraalVM</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>To test whether your GraalVM environment works correctly, we will be adding a minimal Java class and running it. Open an editor and create the following <kbd>Main</kbd> class:</span></p>
<pre class="selectionShareable"><span>public class Main {<br/> public static void main(String[] args) {<br/> System.out.println("Hello GraalVM!");<br/> }<br/>}</span></pre>
<p class="selectionShareable"><span>Compile this class into bytecode and then run it on GraalVM using the following commands:<br/></span></p>
<pre class="selectionShareable"><span><strong>$ javac Main.java</strong><br/><strong>$ java Main</strong></span></pre>
<p>This will give us the following output:</p>
<pre class="selectionShareable"><strong><span> Hello GraalVM!</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building native images</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>Now that we have tested the compilation and execution of Java bytecode, we will convert the bytecode into native executables to achieve faster startup and a smaller footprint for our applications. In order to do that, we need the <kbd>native-image</kbd> tool, which allows us to AOT compile Java code into a standalone executable. We can install the <kbd>native-image</kbd> tool as follows:</span></p>
<pre class="selectionShareable"><span><strong>${GRAALVM_HOME}/bin/gu install native-image</strong><br/></span></pre>
<p>Upon successful installation, we can use the <kbd>native-image</kbd> tool against <span>the same <kbd>HelloWorld</kbd> Java class we have created. Run the following command to build a native image:</span></p>
<pre class="selectionShareable"><span><strong> $ native-image Main</strong></span></pre>
<p>You will see the following output:</p>
<pre class="selectionShareable"><span><strong>Build on Server(pid: 7874, port: 38225)*</strong><br/><strong>[main:7874]    classlist:   1,156.93 ms</strong><br/><strong>[main:7874]        (cap):     859.74 ms</strong><br/><strong>[main:7874]        setup:   1,940.59 ms</strong><br/><strong>[main:7874]   (typeflow):   2,415.87 ms</strong><br/><strong>[main:7874]    (objects):     680.43 ms</strong><br/><strong>[main:7874]   (features):     124.69 ms</strong><br/><strong>[main:7874]     analysis:   3,517.35 ms</strong><br/><strong>[main:7874]     universe:     179.14 ms</strong><br/><strong>[main:7874]      (parse):     413.36 ms</strong><br/><strong>[main:7874]     (inline):     728.98 ms</strong><br/><strong>[main:7874]    (compile):   3,642.72 ms</strong><br/><strong>[main:7874]      compile:   5,219.51 ms</strong><br/><strong>[main:7874]        image:     304.89 ms</strong><br/><strong>[main:7874]        write:      97.09 ms</strong><br/><strong>[main:7874]      [total]:  12,891.94 ms</strong><br/></span></pre>
<p class="selectionShareable"><span>This builds an executable file, just 2 MB in size, named <kbd>main</kbd> in the current working directory:</span></p>
<pre><strong>$ ls -al main</strong> <br/><strong>-rwxrwxr-x. 1 francesco francesco 2481352 Apr 16 10:06 main</strong></pre>
<p class="selectionShareable"><span>Invoking it executes the natively compiled code of the <kbd>Main</kbd> class, as follows:</span></p>
<pre class="selectionShareable"><span><strong>$ ./main</strong><br/><strong>Hello GraalVM!</strong></span></pre>
<p>Once that we have verified that our installation of GraalVM works, we can install a development environment, which will be needed to run the examples contained in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing a development environment</h1>
                </header>
            
            <article>
                
<p class="selectionShareable"><span>Choosing a development environment becomes less significant as we move away from monolithic development, where a large set of plugins is often required to build the complex interaction between the application layers. Therefore, we can choose any IDE that is capable of importing/exporting Maven or Gradle projects natively, as well as a decent set of features for speeding up our code or refactoring it. We will be using <strong>IntelliJ IDEA</strong>, which we can download from <a href="https://www.jetbrains.com/idea/">https://www.jetbrains.com/idea/</a></span><span>.</span></p>
<p class="selectionShareable"><span>As you will see from the</span> download <span>page (<a href="https://www.jetbrains.com/idea/download/">https://www.jetbrains.com/idea/download/</a>), both the Ultimate and Community versions are available. We will be using the latter, which can be freely downloaded.</span> <span>Choose to download the latest binary for your operating system.</span> <span>Then, unzip it into a folder of your preference (for example, in your <kbd>Home</kbd> folder):<br/></span></p>
<pre class="selectionShareable"><strong><span>tar xvzf ideaIC-2019.1.tar.gz -C $HOME</span></strong></pre>
<p class="selectionShareable"><span>Next, move into the <kbd>bin</kbd> folder of the installation and execute it with the following command:</span></p>
<pre class="selectionShareable"><strong><span> ./idea.sh</span></strong></pre>
<p>Let's have a minimal overview of the development environment's visual elements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief overview of IntelliJ IDEA</h1>
                </header>
            
            <article>
                
<p>Although we won't be focusing on a specific development environment to learn about Quarkus, we will provide a short overview of the visual elements that comprise IntelliJ IDEA to understand what actions you can do in a quicker and easier manner. As shown in the following screenshot, these are the main elements of the IntelliJ IDEA interface:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1f35f8a8-6947-42f4-a30f-0e5c7f406e9b.png" style=""/></div>
<p>Let's have a look at the various highlighted sections:</p>
<ol>
<li><strong>Menu bar</strong>: The menu bar includes options we can use to create or import projects and other key actions related to projects, such as code refactoring, builds, run, debug, version-control options, and so on.</li>
<li><strong>Toolbar</strong>: The toolbar contains some shortcuts for common execution actions, such as compile, debug, and run. You can also customize it according to your requirements.</li>
<li><strong>Navigation bar</strong>: The navigation bar enables navigation between sources within a project. This feature will come in handy as your code base grows.</li>
<li><strong>Tools tab</strong>: The tools tab, which can show up on either side of the main window, lets you access key tools such as Maven/Ant builds, databases, and so on.</li>
</ol>
<ol start="5">
<li><strong>Project perspective</strong>: The project perspective window contains all the elements of your project, such as packages, modules, classes, external libraries, and so on.</li>
<li><strong>Editor window</strong>: This is where you edit your code in IntelliJ IDEA using advanced features such as syntax highlighting, smart completion, quick-fix suggestions, and other useful features.</li>
</ol>
<p class="selectionShareable"><span>Great!</span> In the next chapter, we will be creating a simple application with Quarkus that we will import into IntelliJ IDEA.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing IntelliJ Plugin for Quarkus</h1>
                </header>
            
            <article>
                
<p>Before closing this chapter, it is worth mentioning that IntelliJ IDEA includes in its plugin marketplace a plugin to bootstrap Quarkus applications. You can install it through the <strong><span class="packt_screen">File | Settings | Plugin</span></strong> top menu option. Once you have selected the <span class="packt_screen">Plugin</span> option, search for "quarkus" in the marketplace text field.</p>
<p>Once found it, click on the <span class="packt_screen">Install</span> button as depicted by the following picture:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2eb04dfe-e1be-4074-9dd3-5d0863477320.png" style=""/></div>
<p>Restart the IDE for the changes to take effect. Once that the Plugin has been installed, you can add new Quarkus projects directly from the IDE. Here is a snapshot of the updated list of Projects:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/355a7971-dccb-441e-95d1-300b7edde452.png" style=""/></div>
<p>The project wizard will guide you through the selection of the Maven coordinates of your project and the extensions you want to have included in it:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d8c10044-3faa-40a5-b4d7-eddc0fe65a33.png" style=""/></div>
<p>Before sailing into unknown waters, we will pause for a while to briefly recap what we have learned in this chapter. Then, grab a cup of tea and get ready for departure!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we got an overview of the current landscape of the IT industry. As we have learned, Kubernetes adds a completely new dimension to traditional language-based building blocks by offering a new set of distributed services and a runtime environment for creating distributed systems that spread across multiple nodes. Although the core principles of creating containerized applications don't strictly require that you decompose your monolithic applications in single services, there are evident advantages in doing so in terms of isolation, scalability, team independence, monitoring, resilience, and life cycle automation.</p>
<p>Later, we discussed the actual applications that can run natively by introducing Quarkus, an amazing framework where we can create serverless, native applications without losing the skills we learned as Java developers.</p>
<p class="mce-root">Now that we've installed the required tools to get started with Quarkus, in the next chapter, we will code our first example application.</p>


            </article>

            
        </section>
    </body></html>