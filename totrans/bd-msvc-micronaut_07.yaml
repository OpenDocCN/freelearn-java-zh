- en: '*Chapter 5*: Integrating Microservices Using Event-Driven Architecture'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The essence of microservice architecture is breaking a monolith down into decoupled
    or loosely coupled microservices. As a result of such a decomposition into microservices,
    we separate the user and/or business concerns owned by each microservice. However,
    for an application as a whole, all the microservices need to work together by
    interacting with each other in executing and serving the user requests. Event-driven
    architecture has gained popularity in addressing these inter-microservices interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore how we can implement an event-driven architecture
    in the Micronaut framework. We will dive into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding event-driven architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event streaming with the Apache Kafka ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating microservices using event streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, readers will have a nifty knowledge of event-driven
    architecture and how to implement an event-streaming broker to integrate app microservices
    in the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the commands and technical instructions in this chapter are run on Windows
    10 and macOS. Code examples covered in this chapter are available in the book's
    GitHub repository at https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter05.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tools need to be installed and set up in the development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java SDK**: Java SDK version 13 or above (we used Java 14).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maven**: This is optional and only required if you would like to use Maven
    as the build system. However, we recommend having Maven set up on any development
    machine. Instructions regarding the downloading and installation of Maven can
    be found at [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development IDE**: Based on your preferences, any Java-based IDE can be used,
    but for purposes of this chapter, IntelliJ was used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git**: Instructions regarding the downloading and installation of Git can
    be found at [https://git-scm.com/downloads](https://git-scm.com/downloads).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PostgreSQL**: Instructions regarding downloading and installation can be
    found at [https://www.postgresql.org/download/](https://www.postgresql.org/download/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDB**: MongoDB Atlas provides a free online database-as-a-service up
    to 512 MB storage. However, if a local database is preferred, then instructions
    regarding downloading and installation can be found at [https://docs.mongodb.com/manual/administration/install-community/](https://docs.mongodb.com/manual/administration/install-community/).
    We used a local installation while writing this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**REST client**: Any HTTP REST client can be used. We used the **Advanced REST
    Client** (**ARC**) Chrome plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker**: Instructions regarding the downloading and installation of Docker
    can be found at https://docs.docker.com/get-docker/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding event-driven architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event-driven architecture is pivotal in connecting different microservices.
    Before we dive into how to implement an event-driven interaction system, let's
    understand its fundamentals.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key components at the core of any event-driven architecture
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event**: An event is simply a change in the state of the system that needs
    to be traced. In a microservice architecture, a microservice may make or detect
    a change in the data''s state that might be worth noticing by other services.
    This state change is communicated as an event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event producer**: An event producer is any microservice or component that
    is making or detecting a state change and generating an event for other components/services
    in the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event consumer**: An event consumer is any microservice or component that
    is consuming an event. Interestingly, this event consumption might trigger this
    component to produce another event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event broker**: The event broker acts as a go-between between all the producer
    and consumer parties. It maintains a metadata quorum to keep track of events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These key components come together to realize an event-driven architecture.
    Broadly speaking, there are two implementation strategies – **pub/sub** (also
    called event messaging) and **event streaming**. To learn more about these strategies,
    let's dive into the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Event messaging or a pub/sub model in an event-driven architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **pub/sub** model is a *push-based* model. In a push-based model, event publishing
    is owned by the event producer, and events are pushed from the producer and sent
    to consumers. The following are the key components in a pub/sub implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event producer**: Any component that is making or detecting a state change
    will generate an event and publish it to the event broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event broker**: The event broker will receive the generated event and push
    the event to all the required event queues. These event queues are subscribed
    to by event consumers. Therefore, events are pushed down to the event consumers
    by the broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event consumer**: The event consumer will receive the event and do what is
    required. It may also generate a new event(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is depicted in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Pub/sub model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.1_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Pub/sub model
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, when **Event Producer 1** generates the **Event
    Foo**, it is pushed to the **Event Broker**. The **Event Broker** further pushes
    this event to **Event Consumer 1** and **Event Consumer 2**.
  prefs: []
  type: TYPE_NORMAL
- en: So, overall, for **Event Bar** (which is generated by **Event Producer n**),
    **Event Broker** pushes it to **Event Consumer k**.
  prefs: []
  type: TYPE_NORMAL
- en: In a pub/sub model, once the event is produced and communicated to the consumer
    via the event broker, the event consumer must do the necessary immediately, as
    once an event is consumed, it perishes. The event consumer can never go back to
    historic events. This model is also sometimes referred to as the **event messaging
    model**.
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming in event-driven architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An **event streaming** model is a *pull-based* model. In a pull-based model,
    the onus lies on the event consumer to fetch the event. In an event streaming
    implementation, the key components will act as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event producer**: Any component that is making or detecting a state change
    will generate the event and send it to the event broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event broker**: The event broker will receive the generated event and broadcast
    the event by putting the event in an event stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event consumer**: The event consumer continuously monitors one or more event
    streams on the event broker. When a new event is pushed to the event stream, the
    consumer fetches the event and does what is required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Event streaming model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.2_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Event streaming model
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, when **Event Producer 1** generates the **Event
    Foo**, it pushes the event to **Event Broker** and **Event Broker** puts it in
    the **Foo Stream**. **Event Consumer 1** and **Event Consumer 2** fetch the event
    from the **Foo Stream**. **Event Bar** is fetched from **Bar Stream** by **Event
    Consumer k**.
  prefs: []
  type: TYPE_NORMAL
- en: In an event-streaming model, as event consumers fetch the data from an event
    stream, they can fetch any offset of the event stream. This even enables event
    consumers to access historic events. It especially comes in handy if you have
    a new consumer added to the system that might not be in touch with the recent
    state of the system and may start processing historic events first. For these
    reasons, event streaming is usually preferred over event messaging.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will get started with hands-on event streaming using
    a popular event-streaming stack.
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming with the Apache Kafka ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Kafka is an industry-leading event streaming system. In the Apache Kafka
    ecosystem, the following are some of the key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event topic**: An event topic consists of a stream of immutable, ordered
    messages belonging to a particular category. Each event topic may have one or
    more partitions. A partition is indexed storage that supports multi-concurrency
    in Apache. Apache Kafka keeps at least one partition per topic and may add more
    partitions as specified (at the time of topic creation) or required. When a new
    message is published to the topic, Apache Kafka decides which topic partition
    will be used to append the message. Each topic appends the most recent message
    at the end. This is shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Apache Kafka topic anatomy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.3_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Apache Kafka topic anatomy
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, when a new message is published to the steam,
    it is appended at the end. Event consumers can freely choose which topic offset
    to read. While **Consumer 1** reads at the first offset, **Consumer 2** reads
    from the sixth offset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Event broker**: An event broker is a façade that provides an interface to
    write or read from event topic(s). Apache Kafka usually has the leader and follower
    brokers. The leader broker (for a topic) will serve all the write requests. If
    a leader broker fails, then the follower broker chimes in as leader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka cluster**: If Apache Kafka has a quorum consisting of more than one
    event broker, then it''s called a cluster. In a cluster, each broker will usually
    lead a distinct topic and may substitute as a follower for other topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event producer**: Producers publish an event message to the topic stream.
    A producer interacts with the Apache Kafka ecosystem to know which broker should
    be used for writing an event message to a topic stream. If a broker fails or a
    new broker is added, Apache Kafka notifies the required producers of this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event consumer**: A consumer reads the event messages from the topic stream.
    An event consumer will interact with the Apache Kafka ecosystem to know which
    broker should be used to read from a topic stream. Furthermore, Apache Kafka keeps
    track of the topic offset for each event consumer to resume event consumption
    properly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zookeeper**: Zookeeper maintains the metadata quorum for the Apache Kafka
    ecosystem. It essentially maintains information about all brokers for event producers
    and consumers. It also keeps track of the topic offset for each event consumer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the various components in the Apache Kafka
    ecosystem and how they interact with each other in event streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Apache Kafka ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.4_B16585.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Apache Kafka ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the Apache Kafka ecosystem is shown at a glance. For
    each event topic, there will be at least one leader event broker and one or more
    follower event brokers. Information about leaders and followers is maintained
    in Zookeeper. When an event producer pushes a message, a broker will write the
    message to the required topic stream. Similarly, when an event consumer pulls
    a message, an event broker will fetch the message from the required topic stream.
    Zookeeper maintains the offset information for each topic for all the event consumers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into how to use the Apache Kafka ecosystem
    for event streaming in the pet clinic application.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating microservices using event streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn and perform hands-on exercises, we will implement a simple scenario
    of event streaming in the pet clinic application. Consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Event streaming in the pet clinic application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.5_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Event streaming in the pet clinic application
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned diagram, whenever there is a new vet review, the **pet-clinic-reviews**
    microservice will send the review to **Apache Kafka Streaming**. Apache Kafka
    appends the review to the **vet-reviews** topic stream. And, as the **pet-clinic**
    microservice is continuously monitoring the **vet-reviews** topic stream, it will
    fetch any new reviews appended to the topic and update the average rating accordingly.
    This is a simpleton diagram but will help to focus on the key learning objectives.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will begin by setting up the Apache Kafka ecosystem
    locally inside Docker to learn more about Apache Kafka streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Apache Kafka ecosystem locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To set up the Apache Kafka ecosystem locally, we will use Docker. The `docker-compose`
    file for all the required components and configurations can be found in the chapter''s
    GitHub workspace under `resources`: [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/blob/master/Chapter05/micronaut-petclinic/pet-clinic-reviews/src/main/resources/kafka-zookeeper-kafdrop-docker/docker-compose.yml](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/blob/master/Chapter05/micronaut-petclinic/pet-clinic-reviews/src/main/resources/kafka-zookeeper-kafdrop-docker/docker-compose.yml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to install and set up Apache Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `docker-compose` file from the aforementioned URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the GitBash terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the directory to where you have placed the `docker-compose` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `docker-compose up` command in the GitBash terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a result of following these instructions, Docker will install Zookeeper,
    Apache Kafka, and Kafdrop. Kafdrop is an intuitive admin GUI for managing Apache
    Kafka. In the following section, we will verify their installation.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Apache Kafka ecosystem setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test whether the Apache Kafka ecosystem is installed successfully, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the GitBash terminal and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Change the directory to **opt/bitnami/kafka/bin/**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add a topic stream by running the following in the GitBash terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To add a message to the topic, run the following in the GitBash terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A terminal prompt will appear, type `hello-world!`, and then hit *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Ctrl* + *D*, which should successfully add the event to the topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By following these instructions, we added a `foo-stream` topic and added a
    message to this topic. To see this topic stream, we can open Kafdrop by opening
    `http://localhost:9100/` in a browser window. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Kafdrop showing foo-stream topic messages'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.6_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Kafdrop showing foo-stream topic messages
  prefs: []
  type: TYPE_NORMAL
- en: Kafdrop provides an intuitive GUI for viewing and managing all the Apache Kafka
    streams. In the previous screenshot, we can see the messages inside the just created
    **foo-stream**.
  prefs: []
  type: TYPE_NORMAL
- en: Hitherto, we set up the Apache Kafka ecosystem locally in a Dockerized environment,
    and in the next section, we will use this setup for hands-on event streaming in
    the `pet-clinic-reviews` and `pet-clinic` microservices. We will begin by making
    the required changes in the `pet-clinic-reviews` microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an event-producer client in the pet-clinic-reviews microservice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will begin making the required changes to the `pet-clinic-reviews` microservices
    so that it can stream out the vet reviews to Apache Kafka. For this hands-on exercise,
    we will keep things simple. Therefore, we will skip the security setup and resume
    the code base from [*Chapter 3*](B16585_03_Final_VK_ePub.xhtml#_idTextAnchor065),
    *Working on the RESTful Web Services*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to see how this goes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we will need to add a Kafka dependency to the `pom.xml` project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By importing the `micronaut-kafka` dependency, we can leverage the Kafka toolkit
    in the `pet-clinic-reviews` microservice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the dependency has been imported, we will then need to configure `application.properties`
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As mentioned in preceding `application.properties`, we will fix port `8083`
    for the `pet-clinic-reviews` microservice and configure the Kafka connection by
    providing Bootstrap server details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will create a Kafka client in the `pet-clinic-reviews` microservice,
    which can send messages to the `vet-reviews` topic. Begin by creating a package,
    `com.packtpub.micronaut.integration.client`. This package will contain the required
    client and, in the future, may contain more artifacts related to service integration.
    We now add `VetReviewClient` to this package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`VetReviewClient` is annotated with `@KafkaClient`. Using the `@KafkaClient`
    annotation, we can inject `VetReviewClient` as a Kafka client. Furthermore, just
    by simply using `@Topic("vet-reviews")`, we can send the messages (no need to
    even create the topic) to the `vet-reviews` topic stream.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Hitherto, we have configured application properties and created a simple Kafka
    client. In the following code, we will make changes to `createVetReview()` in
    `VetReviewResource` so it can send messages to the topic stream when a new vet
    review is posted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that we can simply inject `VetReviewClient`
    into `VetReviewResource`. In `createVetReview()`, when a vet review is successfully
    inserted, we can send the message to the `vet-reviews` stream using `VetReviewClient`.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduced the event producer in the `pet-clinic-reviews`
    microservice. In the following section, we will verify this event producer by
    invoking the HTTP `POST` endpoint to create a new vet review.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the event producer in the pet-clinic-reviews microservice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test the event producer that has just been created, boot up the `pet-clinic-reviews`
    microservice locally and access the HTTP `POST` endpoint. In the following screenshot,
    we are using a REST client to invoke the `vet-reviews` HTTP `POST` endpoint to
    create a vet review:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Creating a vet review for testing the event producer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.7_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Creating a vet review for testing the event producer
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding screenshot, when we submit a request to create a
    new vet review, it will persist the vet review and also stream out the review
    to Apache Kafka. This event message can be verified by accessing Kafdrop at `http://localhost:9100/`.
    This is what the screen outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – A newly added review to the vet-reviews stream'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.8_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – A newly added review to the vet-reviews stream
  prefs: []
  type: TYPE_NORMAL
- en: As viewed in Kafdrop, we can verify that the event from the `pet-clinic-reviews`
    microservice is streamed out and added to the `vet-reviews` topic.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we verified the event producer in the `pet-clinic-reviews`
    microservice. In the following section, we will explore how to implement an event
    consumer in the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an event consumer client in the pet-clinic microservice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will implement an event consumer in the `pet-clinic` microservice
    so that it can consume messages streamed in the `vet-reviews` topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we will need to add a Kafka dependency to the `pom.xml` project.
    This is shown with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Importing `micronaut-kafka` will enable us to leverage the Kafka consumer toolkit.
    Once the dependency has been imported, we will then need to configure `application.properties`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned in the preceding code, we will fix port `8082` for the `pet-clinic`
    microservice and configure the Kafka connection by providing Bootstrap server
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Next, to contain all the Kafka integration artifacts, we will create a `com.packtpub.micronaut.integration`
    package. Since we will be consuming from the `vet-reviews` topic stream, we will
    add `VetReviewDTO` to the `com.packtpub.micronaut.integration.domain` package.
  prefs: []
  type: TYPE_NORMAL
- en: Some developers advocate keeping the DTOs in a shared repository that can be
    re-used in all microservices. However, keeping all the DTOs under an owning microservice
    is good for better encapsulation. Furthermore, there could be cases where a DTO
    such as `VetReviewDTO` could assume the desired object definition in one microservice
    and a different one in another microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a Kafka listener in the `com.packtpub.micronaut.integration.client`
    package to leverage the `micronaut-kafka` toolkit. Refer to the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we see that we created the `VetReviewListener` using
    the `@KafkaListener` annotation. In the `@KafkaListener` annotation, we passed
    `groupId`. Assigning a group ID to a Kafka listener adds the listener to a consumer
    group. This may be required when there are multiple consumer services for a topic
    stream so that the Kafka ecosystem can maintain an isolated offset for each consumer.
    Using `@Topic("vet-reviews")` allows `VetReviewListener` to receive any streamed
    out messages from the `vet-reviews` stream. When `VetReviewListener` receives
    any message, it invokes `updateVetAverageRating()` in `VetService`. In the following
    code snippet, we added this method in `VetService` to update the average rating
    for a vet when a new review is added to the `pet-clinic-reviews` microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we see that the `updateVetAverageRating()` method retrieves
    the last stored rating. If the last stored rating is `null`, it assumes it to
    be `0`. In any case, it will add on the new rating and determine a new average
    rating. Once the average rating has been determined, rating information is persisted
    in the database by making a call to the repository.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored how we can implement an event consumer in the `pet-clinic`
    microservice. In the following section, we will verify this event consumer by
    creating a new vet review.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the event consumer in the pet-clinic microservice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test the event consumer that has just been created, we can boot up the `pet-clinic`
    (event consumer) and `pet-clinic-reviews` (event producer) microservices. Once
    the `pet-clinic-reviews` microservice is running, add a new vet review. In the
    following screenshot, you can see that we are using an HTTP REST client to post
    a vet review:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Adding a new vet review to test the event consumer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.9_B16585_Fixed.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Adding a new vet review to test the event consumer
  prefs: []
  type: TYPE_NORMAL
- en: In the `POST` request to the `vet-reviews` resource, we are adding an abysmal
    rating. The `pet-clinic-reviews` microservice successfully executed the request
    and responded with an `HTTP 201` response, assigning a review ID to the review
    submitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, in the `pet-clinic` microservice, if
    we put a debug point in `VetReviewListener`, we can verify that the Kafka topic
    stream is sending out the message for a new vet review:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Event message received by the event consumer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.10_B16585_Fixed_edited.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – Event message received by the event consumer
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, when the `pet-clinic-reviews` microservice
    produces an event message, it is received by the `pet-clinic` microservice. This
    is the magic that integrates these two microservices using event-driven architecture.
    And this pattern can be extended to integrate microservices in a variety of different
    scenarios, such as a service sending out a message to multiple microservices or
    chained event messages, or choreographing complex microservice integrations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we verified the event consumer in the `pet-clinic` microservice
    such that when a new vet review is added to `pet-clinic-reviews`, `pet-clinic`
    receives the review information from the `vet-reviews` topic stream.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started things off with some fundamentals of event-driven
    architecture, discussing two different kinds of event publishing models, which
    are pub/sub and event streaming. We discussed the core components of each model,
    as well as the pros/cons of using each model.
  prefs: []
  type: TYPE_NORMAL
- en: Since event streaming was better suited for the pet-clinic application, we dived
    into event streaming using the Apache Kafka ecosystem. For hands-on exercises,
    we integrated the `pet-clinic-reviews` and the `pet-clinic` microservices using
    an Apache Kafka topic stream. We verified the integration by creating a new vet
    review and received the rating in the `pet-clinic` microservice to update the
    average rating for a vet.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has provided you with a solid understanding of event-driven architecture
    and a practical skillset in implementing an event-streaming system in the Micronaut
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how we can automate quality testing using
    built-in as well as third-party tools in the Micronaut framework.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is event-driven architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the pub/sub model in event-driven architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is event streaming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the various components that make up the Apache Kafka ecosystem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is the Apache ecosystem set up in Docker?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How are microservices integrated in the Micronaut framework using event streaming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is an event consumer implemented in the Micronaut framework?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is an event producer implemented in the Micronaut framework?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
