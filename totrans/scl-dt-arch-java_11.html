<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer262">
<h1 class="chapter-number" id="_idParaDest-167"><a id="_idTextAnchor168"/>11</h1>
<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Measuring Performance and Benchmarking Your Applications</h1>
<p>In the preceding chapters, we learned how to architect a solution for data ingestion and data publishing problems. We also discussed how we can choose the correct technology stack and platform to implement a cost-effective and scalable solution. Apart from these, we learned about various architectural patterns for data ingestion. We also discussed data governance and data security. However, as an architect, our job is not only to create a scalable solution but a high-performing one. This is where the role of performance engineering comes into a data architect’s toolkit.</p>
<p>In this chapter, we will discuss the meaning of performance engineering and why is it so important. We will also learn how is it different from performance testing. Then, we will learn how to plan our performance tests and other performance engineering activities. Then, we will briefly discuss performance benchmarking techniques. Finally, we will learn about the common methodologies to fine-tune the performance of our solution to mitigate or avoid various kinds of performance bottlenecks during data ingestion or data publishing.</p>
<p>By the end of this chapter, you will know what performance engineering is and how to plan for it. You will know how to benchmark and publish performance results. You will know what the available performance tools are and when to use them. Finally, you will know how to fine-tune performance to create an optimized, highly performant solution for the data problem, as well as how to perform performance benchmarking.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Performance engineering and planning</li>
<li>Tools for performance engineering</li>
<li>Publishing performance benchmarks</li>
<li>Optimizing performance</li>
</ul>
<h1 id="_idParaDest-169"><a id="_idTextAnchor170"/>Performance engineering and planning </h1>
<p><strong class="bold">Software performance engineering</strong> (<strong class="bold">SPE</strong>) is a systematic and quantitative software-based <a id="_idIndexMarker1150"/>approach to designing, architecting, and implementing solutions optimally to <a id="_idIndexMarker1151"/>meet various <strong class="bold">non-functional requirements</strong> (<strong class="bold">NFRs</strong>) such as performance, capacity, scalability, availability, and reliability. Earlier in this book, we dealt with scalability, availability, and reliability. In this chapter, we will focus on performance and capacity. Alternatively, SPE is defined as a proactive and continuous process of performance testing and monitoring. It involves different stakeholders such as testers, developers, performance engineers, business analysts, and architects. As we will discuss later in this chapter, performance engineering is a seamless process that runs in parallel with development activities, providing a continuous feedback loop to the developers and architects so that performance requirements are imbibed while the software is developed.</p>
<p>Now that we’ve defined performance engineering, let’s discuss the phases of a performance engineering life cycle and how it <a id="_idIndexMarker1152"/>progresses alongside the <strong class="bold">software development life cycle</strong> (<strong class="bold">SDLC</strong>) activities. The following diagram shows this:</p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="Figure 11.1 – Performance engineering life cycle " height="1170" src="image/B17084_11_001.jpg" width="1043"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Performance engineering life cycle</p>
<p>As we can see, the following are the various stages of the performance engineering life cycle:</p>
<ul>
<li><strong class="bold">NFR gathering</strong>: To develop a high-performant data pipeline, it is important to understand the NFRs of the solution. For<a id="_idIndexMarker1153"/> example, while<a id="_idIndexMarker1154"/> designing a DaaS, it makes more sense to know what the required <strong class="bold">transactions per second</strong> (<strong class="bold">TPS</strong>) are and what the average parallel load to the system would be. There may be another requirement while designing the pipeline, which is that it should be able to be integrated with Datadog for monitoring purposes. In such a scenario, a technology stack should be chosen so that Datadog integration is supported. To gather all this information, we need a close connection between product<a id="_idIndexMarker1155"/> owners, system analysts, <strong class="bold">subject matter experts</strong> (<strong class="bold">SMEs</strong>), architects, the SPE team, and DevOps.</li>
<li><strong class="bold">Design for performance and performance modeling</strong>: In the waterfall model, performance testing and optimization are done after the functional and integrational test cycles are over. The <a id="_idIndexMarker1156"/>problem with this approach is that sometimes, architectures that work beautifully with small datasets may not work at all in terms of load testing. Due to this, we have to re-engineer the solution again. This causes a lot of waste in terms of effort, time, and money. As modern data engineering teams have increasingly adopted agile methodologies, the opportunity has increased for simultaneously adopting performance engineering. After the nonfunctional requirements have been gathered, designing for performance requires that the following criteria be fulfilled:<ul><li>Satisfy the NFRs with optimal speed</li>
<li>The solution must be scalable enough to have similar performance, even with increased load </li>
</ul></li>
</ul>
<p>Our architecture should be designed to scale as business data increases exponentially. This brings to life ideas such as design to fail, scaling out rather than scaling up the application, and auto-scaling resources in the cloud. Another method that’s commonly applied <a id="_idIndexMarker1157"/>during performance-oriented design is <strong class="bold">performance modeling</strong>.</p>
<p>Performance modeling is the process of modeling application performance based on features involved in the growth rate of data to find out probable breaches of SLA. It also helps validate design decisions and infrastructure decisions. Let’s look at an example – suppose input messages are coming in an application at a rate of <em class="italic">x</em>, and the service rate of the application is <em class="italic">y</em>. What happens if the arrival rate of messages quadruples itself? How do we ensure the same response time? Do we need to quadruple the service rate or double the service rate? This kind of decision can be made by doing performance modeling. In modern data architectures where applications tend to run on containerized or virtualized platforms, this can determine how to allocate resources to scale in the future.</p>
<ul>
<li><strong class="bold">Modular performance tests and optimization</strong>: In this phase, using NFRs, <strong class="bold">non-functional tests</strong> (<strong class="bold">NFTs</strong>) are classified as <a id="_idIndexMarker1158"/>stress tests, load tests, or soak tests. Once the test cases<a id="_idIndexMarker1159"/> have been classified, a test case document that maps NFTs with detailed steps to run the test scenario is prepared. Optionally, an NFT to NFR performance matrix is created. Then, using these documents, NFRs can be tested as a module gets developed and functionally tested. Running these test cases along with functional testing ensures early detection of any performance glitches. Optimization and performance tuning can be done as required by the development or DevOps teams.</li>
<li><strong class="bold">Fully integrated performance test</strong>: Once the modular performance test is done, end-to-end performance tests can be<a id="_idIndexMarker1160"/> run. As the <strong class="bold">quality assurance</strong> (<strong class="bold">QA</strong>) of a scenario finishes, that <a id="_idIndexMarker1161"/>scenario moves to be tested for integration performance. Usually, in this layer, not much tuning or optimization is required. However, optimization of the overall end-to-end flow may be required during this activity.</li>
<li><strong class="bold">Monitoring and capacity management</strong>: Once the pipeline is in production, we need to continuously monitor <a id="_idIndexMarker1162"/>and evaluate any unusual activities. Based on the future and current state of the workloads, capacity can be predicted and managed properly.</li>
</ul>
<p>In this section, we learned the various stages of the performance engineering life cycle. Next, let’s understand the differences between performance engineering and performance testing.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/>Performance engineering versus performance testing</h2>
<p>The key differences between performance engineering and performance testing are as follows:</p>
<ul>
<li>Performance testing is a QA <a id="_idIndexMarker1163"/>activity that runs test cases to check the quality of NFRs and find any issues. It is performed to <a id="_idIndexMarker1164"/>check how the system will behave in terms of production load and anticipates any issues that could come up during heavy loads. On the other hand, performance engineering is a holistic process that runs hand-in-hand with SDLC. Unlike performance testing, performance engineering starts as early as the analysis phase. It also facilitates the discovery of performance issues early in the development life cycle.</li>
<li>Performance testing follows the waterfall model of the software development process. It is only done when software development and functional testing have been completed. The problem with such an approach is that if the application fails to perform with production loads, we may need to redesign and re-implement, causing unnecessary time and financial loss. However, performance engineering is a continuous process that goes hand-in-hand with all phases of SDLC and is usually implemented by agile teams with a continuous feedback loop to the development and design team. By providing early analysis of performance needs and early discovery of issues, performance engineering helps us save time and money.</li>
<li>Performance testing is conducted by the QA team, whereas performance engineering involves architects, developers, SMEs, performance engineers, and QA.</li>
</ul>
<p>In this section, we learned about performance engineering, why is it needed, and its life cycle. We also discussed the difference between performance testing and performance engineering. In the next section, we will briefly discuss the performance engineering tools available in the market.</p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor172"/>Tools for performance engineering</h1>
<p>In this section, we will briefly discuss <a id="_idIndexMarker1165"/>various performance engineering tools.</p>
<p>The following are the different categories<a id="_idIndexMarker1166"/> of performance engineering tools available:</p>
<ul>
<li><strong class="bold">Observability tools</strong>: These tools monitor <a id="_idIndexMarker1167"/>and gather information <a id="_idIndexMarker1168"/>about the application. These tools potentially help to identify bottlenecks, track throughput and latency, memory usage, and so on. In data engineering, each system is different, and the throughput and latency requirements are also different. Observability tools help identify if our application is lagging in terms of throughput or latency and by how much. They also help identify hidden issues that may <a id="_idIndexMarker1169"/>only show up in the long run, in production. For example, a small memory leak in the application may not be noticeable within a few days of deployment. When such an application keeps on running, the<a id="_idIndexMarker1170"/> tenured region of JVM heap space keeps slowly increasing until it overruns the heap space. The following are a few examples of observability tools:<ul><li><strong class="bold">Datadog</strong>: This is a very popular<a id="_idIndexMarker1171"/> monitoring tool that can do application monitoring, network monitoring, database monitoring, container<a id="_idIndexMarker1172"/> monitoring, serverless monitoring, and more. It has an inbuilt dashboard and capabilities to customize your dashboard according to your needs. It has alerting, log integration, and other cool features. It is a paid product, which provides enterprise support. For more information, please visit <a href="https://www.datadoghq.com/">https://www.datadoghq.com/</a>. </li>
<li><strong class="bold">Grafana with Graphite/Prometheus</strong>: This is an open source monitoring and dashboarding tool. It is either <a id="_idIndexMarker1173"/>used with Prometheus or Graphite. Both Prometheus and Graphite are open source monitoring toolkits that help generate and publish various metrics. Prometheus has a data collector module that can pull data to generate metrics. On the other hand, Graphite can only passively listen for data but can’t collect it. Some other tool, such as Collectd, needs to collect and push data to Graphite. To query Graphite metrics, functions are used, whereas PromQL is used to query Prometheus metrics. These generated <a id="_idIndexMarker1174"/>metrics are integrated with Grafana to create different kinds of dashboards, such as stats dashboards, time series monitoring, status timeline and history, alerting dashboards, and so on. For more information, please visit <a href="https://grafana.com/">https://grafana.com/</a>. </li>
<li><strong class="bold">Dynatrace</strong>: Dynatrace is <a id="_idIndexMarker1175"/>another commercial monitoring and dashboarding tool that has very similar features to Datadog. It also provides an AI assistant to help answer your<a id="_idIndexMarker1176"/> queries dynamically. It supports DevOps and CloudOps integrations such as CI/CD pipelines and so on. For more information, please visit <a href="https://www.dynatrace.com/">https://www.dynatrace.com/</a>. </li>
<li><strong class="bold">Confluent Control Center</strong>: This is an inbuilt confluent Kafka monitoring tool that is shipped with an Enterprise (Licensed) version of Confluent Kafka. It helps monitor various Kafka <a id="_idIndexMarker1177"/>components such as topics, producers, consumers, Kafka Connect clusters, KSQL queries, as well as<a id="_idIndexMarker1178"/> overall Kafka cluster health. For more information, please visit <a href="https://docs.confluent.io/platform/current/control-center/index.xhtml">https://docs.confluent.io/platform/current/control-center/index.xhtml</a>. </li>
<li><strong class="bold">Lenses</strong>: This is a tool that provides observability of Kafka topics, clusters, and streams. Lenses not only <a id="_idIndexMarker1179"/>supports observability but also DataOps for Kafka clusters. For more information, please visit <a href="https://docs.lenses.io/">https://docs.lenses.io/</a>.</li>
</ul></li>
<li><strong class="bold">Performance testing and benchmarking tools</strong>: These tools are used to do all kinds of performance tests, such as smoke<a id="_idIndexMarker1180"/> tests, load tests, and stress tests. Some of them also provide<a id="_idIndexMarker1181"/> benchmarking features. The following are a few of the tools that can be used for performance testing and benchmarking:<ul><li><strong class="bold">JMeter</strong>: This is a free <a id="_idIndexMarker1182"/>open source tool written in Java that does performance tests. It is especially helpful for big data performance testing and any performance testing of APIs, such as REST and GraphQL. JMeter Hadoop plugins are available to do big data performance testing. We can run the load tests and export the result to a file in multiple<a id="_idIndexMarker1183"/> formats. For more information, please visit <a href="https://jmeter.apache.org/">https://jmeter.apache.org/</a>.</li>
<li><strong class="bold">SoapUI</strong>: This is another open source performance test tool that is used for functional testing as well. It supports load testing with multiple users, threads, and parallelism for web<a id="_idIndexMarker1184"/> services such as REST, SOAP, and GraphQL. It has a professional commercial edition as<a id="_idIndexMarker1185"/> well called ReadyAPI. The commercial edition supports more advanced features and specific plugins for testing <a id="_idIndexMarker1186"/>GraphQL and Kafka streaming applications. For more information, please visit <a href="https://www.soapui.org/">https://www.soapui.org/</a>. </li>
<li><strong class="bold">Blazemeter</strong>: This is another open <a id="_idIndexMarker1187"/>source performance test tool that’s used to run scalable tests for microservices such <a id="_idIndexMarker1188"/>as REST or GraphQL APIs. It supports a few monitoring functionalities as well. For more information, please visit <a href="https://www.blazemeter.com/">https://www.blazemeter.com/</a>.</li>
<li><strong class="bold">LoadRunner</strong>: LoadRunner is a commercial product from Microfocus that enables load testing for various<a id="_idIndexMarker1189"/> workloads and various kinds of applications. It supports testing of over 50 types of <a id="_idIndexMarker1190"/>applications such as microservices, HTML, MQTT, Oracle, and so on. For more information, please visit <a href="https://www.microfocus.com/en-us/products/loadrunner-professional/overview">https://www.microfocus.com/en-us/products/loadrunner-professional/overview</a>.</li>
<li><strong class="bold">SandStorm</strong>: This is a commercial <a id="_idIndexMarker1191"/>benchmarking and enterprise-level performance testing tool. It has huge support for various applications and tools, from JDBC connections to big data testing. It supports NoSQL databases such as Cassandra, HBase, and MongoDB, as well as other big data components such as Hadoop, Elasticsearch, and Solar. It also provides support for messaging platforms such as Kafka and RabbitMQ. For more information, please visit <a href="http://www.sandstormsolution.com/">http://www.sandstormsolution.com/</a>.</li>
<li><strong class="bold">Kafka perf test</strong>: This is an inbuilt <a id="_idIndexMarker1192"/>performance test tool that gets shipped along with Apache Kafka. Two scripts get shipped along with Apache Kafka: <strong class="source-inline">kafka-producer-perf-test.sh</strong> and <strong class="source-inline">kafka-consumer-perf-test.sh</strong>. While the former <a id="_idIndexMarker1193"/>script is used to test producer performance, the latter is <a id="_idIndexMarker1194"/>used to test consumer performance. To learn more about this feature, go to <a href="https://docs.cloudera.com/runtime/7.2.10/kafka-managing/topics/kafka-manage-cli-perf-test.xhtml">https://docs.cloudera.com/runtime/7.2.10/kafka-managing/topics/kafka-manage-cli-perf-test.xhtml</a>.</li>
<li><strong class="bold">OpenMessaging Benchmark Framework</strong>: This is a set of tools that allows you to <a id="_idIndexMarker1195"/>benchmark distributed <a id="_idIndexMarker1196"/>messaging systems over the cloud easily. It supports multiple message platforms such as Apache Kafka, Apache Pulsar, Apache RocketMQ, and so on. For more information, please visit <a href="https://openmessaging.cloud/docs/benchmarks/">https://openmessaging.cloud/docs/benchmarks/</a>.</li>
</ul></li>
</ul>
<p>In this section, we briefly discussed multiple tools that can be used for performance engineering. Now that we have a fair idea of what performance engineering is, how to do it, and the tools we can use for it, let’s look at how we can create performance benchmarks using the knowledge that we have gained.</p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>Publishing performance benchmarks</h1>
<p>In this section, we will learn about<a id="_idIndexMarker1197"/> performance benchmarking and how to develop and publish them. We will start by defining what a performance benchmark is. A benchmark in software performance testing is defined as a point of reference against which the quality measures of a software solution can be assessed. It can be used to do a comparative study of different solutions to the same problem or compare software products.</p>
<p>Benchmarks are like stats or <a id="_idIndexMarker1198"/>metrics to determine the quality of software. Just like in sports such as soccer, each player’s worth or quality is determined by various stats such as their overall number of goals scored, number of goals scored per match, number of goals scored tournament-wise, and so on. These stats help compare different players under different specifications. Similarly, benchmarks in the software world help determine the worth of a software product or solution under specific conditions. </p>
<p>Now, let’s practically run some performance tests and create a performance benchmark. We will use the REST API that we developed in <a href="B17084_09.xhtml#_idTextAnchor144"><em class="italic">Chapter 9</em></a>, <em class="italic">Exposing MongoDB as a Service</em>, to do the performance testing. We will use JMeter to test and record the performance of the application. We chose JMeter since it is easy to use and is an open source product based on Java.</p>
<p>Follow these steps to do a performance test and benchmark:</p>
<ol>
<li><strong class="bold">Add a thread group</strong>:  First, we must add a thread group. To add a thread group, we need to do the<a id="_idIndexMarker1199"/> following:<ol><li>Start the JMeter tool.</li>
<li>From the tree in the left pane, select <strong class="bold">Test Plan</strong>.</li>
<li>Right-click on <strong class="bold">Test Plan</strong>. Then, click <strong class="bold">Add</strong> | <strong class="bold">Threads (Users)</strong> | <strong class="bold">Thread Group</strong> to add a thread group, as shown in the following screenshot:</li>
</ol></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer234">
<img alt="Figure-11.2 – Adding a thread group " height="827" src="image/B17084_11_002.jpg" width="1464"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure-11.2 – Adding a thread group</p>
<p>In the <strong class="bold">Thread Group</strong> creation wizard, fill in the thread group’s name and enter the thread’s properties, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer235">
<img alt="Figure 11.3 – The Thread Group creation wizard " height="958" src="image/B17084_11_003.jpg" width="892"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – The Thread Group creation wizard</p>
<p>As you can see, <strong class="bold">Number of Threads (users)</strong>, <strong class="bold">Loop Count</strong>, and <strong class="bold">Ramp-up period (seconds)</strong> have been <a id="_idIndexMarker1200"/>configured. Let’s try to understand these terms:</p>
<ul>
<li><strong class="bold">Number of Threads (users)</strong> corresponds to the number of users making the request simultaneously, as shown in the following diagram:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer236">
<img alt="Figure 11.4 – Number of threads versus loop count " height="331" src="image/B17084_11_004.jpg" width="759"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Number of threads versus loop count</p>
<ul>
<li><strong class="bold">Loop Count</strong>, on the other hand, corresponds to the number of requests a single user makes to the server. In our example, <strong class="bold">Loop Count</strong> is <strong class="source-inline">5</strong>. So, a user makes 5 requests to the server.</li>
<li><strong class="bold">Ramp-up period (seconds)</strong> tells JMeter how much time to wait before starting the next user. In our <a id="_idIndexMarker1201"/>example, <strong class="bold">Number of Threads (users)</strong> is <strong class="source-inline">50</strong> and <strong class="bold">Ramp-up period (seconds)</strong> is also <strong class="source-inline">50</strong>. Hence, there is a delay of 1 second before it starts the next user.</li>
</ul>
<p>Now that we have configured our thread group, let’s try adding the JMeter elements.</p>
<ol>
<li value="2"><strong class="bold">Configure the JMeter elements</strong>: Now, we will add a JMeter config element known as an HTTP Request. This helps<a id="_idIndexMarker1202"/> make REST or web service calls for load tests. Right-click <strong class="bold">REST Thread group</strong> and select <strong class="bold">Add</strong> | <strong class="bold">Sampler</strong> | <strong class="bold">HTTP Request</strong>, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer237">
<img alt="Figure 11.5 – Adding a sampler " height="772" src="image/B17084_11_005.jpg" width="1312"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Adding a sampler</p>
<p>Set up <strong class="bold">Protocol [http]</strong>, <strong class="bold">Server Name or IP</strong>, <strong class="bold">Port Number</strong>, <strong class="bold">HTTP Request</strong>, and <strong class="bold">Path</strong> in the control panel that opens to configure the HTTP Request sampler. We are testing a REST DaaS application, so we will set HTTP <strong class="bold">Protocol [http]</strong> to <strong class="source-inline">http</strong>, <strong class="bold">Server Name or IP</strong> to the IP address of the machine where REST DaaS is <a id="_idIndexMarker1203"/>running, <strong class="bold">Port Number</strong> to <strong class="source-inline">8080</strong> (if running on a local machine), <strong class="bold">HTTP Request</strong> to <strong class="source-inline">GET</strong>, and our <strong class="bold">Path</strong>, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer238">
<img alt="Figure 11.6 – Configuring the HTTP Request sampler " height="570" src="image/B17084_11_006.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Configuring the HTTP Request sampler</p>
<p>Now that we have configured the HTTP Request sampler, we need to add a few JMeter elements that can <a id="_idIndexMarker1204"/>monitor and publish performance reports. For this, we must configure one or more listeners.</p>
<ol>
<li value="3"><strong class="bold">Add a listener</strong>: To create performance benchmarks, we must add three listeners, as follows:<ol><li><strong class="bold">Summary Report</strong></li>
<li><strong class="bold">Aggregate Report</strong></li>
<li><strong class="bold">Response Time Graph</strong></li>
</ol></li>
</ol>
<p>The steps to add a listener are similar to<a id="_idIndexMarker1205"/> configuring any new listener. Here, we will demonstrate how to add an aggregated performance benchmark, as follows:</p>
<ol>
<li value="4">Right-click <strong class="bold">REST Thread Group</strong> and then select <strong class="bold">Add</strong> | <strong class="bold">Listener</strong> | <strong class="bold">Aggregate Report</strong>, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer239">
<img alt="Figure 11.7 – Adding a listener " height="744" src="image/B17084_11_007.jpg" width="1440"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Adding a listener</p>
<ol>
<li value="5">Rename the report to <strong class="source-inline">Aggregate Performance Benchmark</strong> in the configuration wizard of <strong class="bold">Aggregate Report</strong>, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer240">
<img alt="Figure 11.8 – Configuring the Aggregate Report listener " height="744" src="image/B17084_11_008.jpg" width="1440"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Configuring the Aggregate Report listener</p>
<p>Follow similar steps to <a id="_idIndexMarker1206"/>set up the <strong class="bold">Summary Report</strong> and <strong class="bold">Response Time Graph</strong> listeners. By doing so, we will be set to run the test and generate the report.</p>
<ol>
<li value="4"><strong class="bold">Run load tests and create a benchmark</strong>: Run the tests by<a id="_idIndexMarker1207"/> clicking the start symbol that’s encircled in red in the<a id="_idIndexMarker1208"/> following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="Figure 11.9 – Running performance tests by clicking the Run button " height="738" src="image/B17084_11_009.jpg" width="1440"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – Running performance tests by clicking the Run button</p>
<p>On successfully executing the performance tests, the following reports are generated:</p>
<ul>
<li><strong class="bold">Summary Report</strong>: This report provides a <a id="_idIndexMarker1209"/>summary of the performance benchmark and shows the average, minimum, and maximum response time of the request. You can also see the average throughput of the application. The following screenshot shows a summary of the benchmark results:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer242">
<img alt="Figure 11.10 – Generated summary benchmark report " height="423" src="image/B17084_11_010.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Generated summary benchmark report</p>
<p>Notice the <strong class="bold"># Samples</strong> column in the preceding screenshot; its value is <strong class="bold">250</strong>. The value of the samples is derived using the following formula:</p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<img alt="" height="62" src="image/Formula_11.1.jpg" width="540"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="" height="66" src="image/Formula_11.2.jpg" width="900"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer245">
<img alt="" height="55" src="image/Formula_11.3.jpg" width="600"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer246">
<img alt="" height="46" src="image/Formula_11.4.jpg" width="360"/>
</div>
</div>
<ul>
<li><strong class="bold">Aggregate Report</strong>: This denotes the <a id="_idIndexMarker1210"/>aggregated benchmark report. Apart from showing the average, median, and maximum response time, it has columns such as <strong class="bold">90% Line</strong> and <strong class="bold">95% Line</strong>. A <strong class="bold">90% Line</strong> column denotes the average response time of 90% of the request. It assumes that 10% of the request contains outliers. Similarly, a <strong class="bold">95% Line</strong> assumes that 5% of requests are outliers. The following screenshot shows the aggregated performance benchmark:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer247">
<img alt="Figure 11.11 – Generated aggregated benchmark report " height="423" src="image/B17084_11_011.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Generated aggregated benchmark report</p>
<ul>
<li><strong class="bold">Response Time Graph</strong>: A performance<a id="_idIndexMarker1211"/> benchmark can contain multiple charts or tables to benchmark the application performance. A Response Time Graph depicts the recorded response time at different timelines:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer248">
<img alt="Figure 11.12 – Generated Response Time Graph " height="1012" src="image/B17084_11_012.jpg" width="1420"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Generated Response Time Graph</p>
<p>In performance benchmarking activities, a lot of the time, we do comparative studies where these reports <a id="_idIndexMarker1212"/>are used to create a combined report or graph visualization comparing the performance benchmarks of two different solutions.</p>
<p>In this section, we learned why benchmarking is needed and what needs to be considered while benchmarking our solutions. Benchmarking provides us with a way to categorize or classify our application performance as good, bad, or just fine. In the next section, we will find out how can we improve our performance and optimize our data engineering solutions.</p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/>Optimizing performance</h1>
<p>One of the main reasons that <a id="_idIndexMarker1213"/>benchmarking, performance testing, and monitoring of applications and systems are done is because of a goal – to optimize the<a id="_idIndexMarker1214"/> performance so that the system can work to its best potential. The difference between extraordinary software and ordinary software is determined by how well the system is tuned for better performance. In this section, we will learn about various techniques you can use to fine-tune your data engineering pipeline. Although performance tuning is a vast topic, when it comes to various data engineering solutions, we will try to cover the basics of optimizing Java-based data <a id="_idIndexMarker1215"/>engineering solutions. In the following subsection, we will briefly look at various performance tuning techniques.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor175"/>Java Virtual Machine and garbage collection optimizations</h2>
<p><strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>) performance tuning is the process of adjusting the various JVM arguments or parameters to <a id="_idIndexMarker1216"/>suit the need of our application so that it performs the best it can.</p>
<p>JVM tuning involves two kinds of optimization, as follows:</p>
<ul>
<li>Heap space optimization</li>
<li><strong class="bold">Garbage collection</strong> (<strong class="bold">GC</strong>) optimization</li>
</ul>
<p>But before we talk about these <a id="_idIndexMarker1217"/>optimizations, it is worth noting that JVM tuning needs to be the last resort to tune the performance of an application. We should start by tuning application code bases, databases, and resource availability.</p>
<h3>Overview of the JVM heap space</h3>
<p>Before we deep<a id="_idIndexMarker1218"/> dive into JVM and GC tuning, let’s spend some time understanding the JVM heap space.</p>
<p>The following diagram shows what a JVM heap space looks like:</p>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="Figure 11.13 – JVM heap space " height="295" src="image/B17084_11_013.jpg" width="671"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – JVM heap space</p>
<p>As we can see, the JVM heap space is divided into four compartments, as follows:</p>
<ol>
<li value="1">Meta or Perm Space</li>
<li>Eden Space</li>
<li>Survivor Space</li>
<li>Tenured Space</li>
</ol>
<p>Meta Space (known as Perm Space for older JDK versions) stores the <a id="_idIndexMarker1219"/>metadata of the heap.</p>
<p>Java objects get promoted<a id="_idIndexMarker1220"/> from Eden Space to Tenured Space, based on the tenure for which they are alive. The following steps show how the Java objects get promoted: </p>
<ol>
<li value="1">The newly created objects from the Java application get stored in <em class="italic">Eden Space</em>. </li>
<li>When <em class="italic">Eden Space</em> is full, a minor GC event occurs and the objects that are still referenced by the Java application are promoted to <em class="italic">Survivor Space 0</em>. </li>
<li>Again, in the next cycle, when <em class="italic">Eden Space</em> is full, a second minor GC event gets triggered. At first, this moves all the objects that are still referenced by the application from <em class="italic">Survivor Space 0</em> to <em class="italic">Survivor Space 1</em>, and then it promotes referenced objects from <em class="italic">Eden Space</em> to <em class="italic">Survivor Space 0</em>. </li>
<li>A major GC event occurs when referenced objects are promoted from <em class="italic">Survivor Space 1</em> to <em class="italic">Tenured Space</em>. Objects that get promoted to tenured space are called old-generation objects. <em class="italic">Eden Space</em>, <em class="italic">Survivor Space 0</em>, and <em class="italic">Survivor Space 1</em> objects are <a id="_idIndexMarker1221"/>young-generation objects.</li>
</ol>
<p>Earlier, we discussed how minor and major GC happens to free up the heap space. But is there a single way or multiple ways to do GC? If so, what should we choose and when? We’ll explore this in the next section.</p>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="bold">Garbage collection</strong> is a process that <a id="_idIndexMarker1222"/>automatically determines what memory in the JVM is no longer being used by a Java application and recycles that memory for other usages.</p>
<h3>Types of garbage collector</h3>
<p>The following are the different types of GCs:</p>
<ul>
<li><strong class="bold">Serial garbage collector</strong>: A single GC is suitable for single-threaded applications. It freezes all application<a id="_idIndexMarker1223"/> threads while doing garbage collection and does so using a single thread.</li>
<li><strong class="bold">Parallel garbage collector</strong>: A parallel GC also freezes all threads from the application but uses multiple threads to<a id="_idIndexMarker1224"/> do garbage collection. Hence, the pause interval of the application threads reduces considerably. It is designed to work for multi-processor environments or multi-threaded environments with medium and large data sizes.</li>
<li><strong class="bold">Concurrent Mark Sweep (CMS) garbage collector</strong>: As evident from the name, the garbage collection job is<a id="_idIndexMarker1225"/> performed concurrent to the application. So, it doesn’t require application threads to pause. Instead, it shares threads with the application thread for concurrent sweep execution. However, it needs to pause application threads <a id="_idIndexMarker1226"/>shortly for an <strong class="bold">initial mark pause</strong>, where it marks the live objects initially. Then, a second pause, called a <strong class="bold">remark pause</strong>, suspends application<a id="_idIndexMarker1227"/> threads and is used to find any Java objects that need to be collected. These Java objects are created during the concurrent tracing phase. The following diagram explains the difference between serial, parallel, and CMS GCs:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer250">
<img alt="Figure 11.14 – Difference between the single, parallel, and CMS garbage collectors " height="218" src="image/B17084_11_014.jpg" width="496"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Difference between the single, parallel, and CMS garbage collectors</p>
<p>As we can see, the serial collector uses a single thread for garbage collection, while pausing all application threads. The parallel collector pauses the application threads but since it <a id="_idIndexMarker1228"/>uses multiple threads to do its job, the pause time is less. CMS, on the other hand, runs concurrently along with the application threads after the initial mark phase.</p>
<ul>
<li><strong class="bold">G1 garbage collector</strong>: This is a relatively new GC introduced in Java 7 and later. It depends on a new algorithm for concurrent garbage collection. It runs its longer job alongside the application threads and quicker jobs by pausing the threads. It works using the<a id="_idIndexMarker1229"/> evacuation style of memory cleaning. For the evacuation style of memory cleaning, the G1 collector divides the heap into regions. Each region is a small, independent heap that can be dynamically assigned to Eden, Survivor, or Tenured Space. The following diagram shows how the G1 collector sees the data:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer251">
<img alt="Figure 11.15 – G1 garbage collector divides heap space into regions " height="421" src="image/B17084_11_015.jpg" width="301"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – G1 garbage collector divides heap space into regions</p>
<p>The GC simply copies data from one region to another. This needs to be retained and marks the older region as blank.</p>
<ul>
<li><strong class="bold">Z garbage collector</strong>: This is an <a id="_idIndexMarker1230"/>experimental GC for very scalable low latency implementations.</li>
</ul>
<p>The following points should be kept in mind regarding GC:</p>
<ul>
<li>Minor GC events should <a id="_idIndexMarker1231"/>collect as many dead objects as possible to reduce the frequency of full GC.</li>
<li>More efficient object cleanup is possible when more memory is available for a GC event. More efficient object cleanup ensures a lower frequency of full GC events.</li>
<li>In the case of performance tuning using GC, you can only tune two parameters out of the three – that is, throughput, latency, and memory usage.</li>
</ul>
<p>Next, let’s see how to tune the performance using a GC.</p>
<h3>Tuning performance using a GC</h3>
<p>Performance<a id="_idIndexMarker1232"/> tuning using GC settings is also known as GC tuning. We must follow these steps to perform GC tuning:</p>
<ol>
<li value="1"><strong class="bold">Investigate the memory footprint</strong>: One of the most commonly used methods to look for any performance issue caused due to GC can be found in the memory footprint<a id="_idIndexMarker1233"/> and is present in the GC logs. GC logs can be enabled and generated from a Java application without affecting performance. So, it is a popular tool to investigate performance issues in production. You can enable GC logs using the following command:<p class="source-code"><strong class="bold">-XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:&lt;filename&gt;</strong></p></li>
</ol>
<p>The following screenshot shows what a typical GC log looks like:</p>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="Figure – 11.16 – Sample GC log " height="416" src="image/B17084_11_016.jpg" width="1220"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure – 11.16 – Sample GC log</p>
<p>In the preceding<a id="_idIndexMarker1234"/> screenshot, each line <a id="_idIndexMarker1235"/>shows various GC information. Let’s focus on the third line of the log, which shows a full GC event:</p>
<div>
<div class="IMG---Figure" id="_idContainer253">
<img alt="Figure – 11.17 – Anatomy of a GC log statement " height="593" src="image/B17084_11_017.jpg" width="823"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure – 11.17 – Anatomy of a GC log statement</p>
<p>As shown in the preceding diagram, let’s pick apart the GC log statement and understand the various parts of it:</p>
<ul>
<li><strong class="source-inline">2022-07-01T16:46:0.434</strong>: The timestamp<a id="_idIndexMarker1236"/> when the GC event occurred.</li>
<li><strong class="source-inline">Full GC</strong>: This field describes the type of GC. It can either be a full GC or GC.</li>
<li><strong class="source-inline">[PSYoungGen: 10752K-&gt;0K(141824K)]</strong>: After the GC event occurred, all the space used in the young generation was recycled. Also, the value inside brackets (<strong class="source-inline">141824K</strong>) denotes the total allocated space in the young generation.</li>
<li><strong class="source-inline">[ParOldGen: 213644K-&gt;215361K(459264K)]</strong>: After the GC ran, the old generation’s used space increased from <strong class="source-inline">213644K</strong> to <strong class="source-inline">215361K</strong>. Total allocated memory for the old generation is <strong class="source-inline">459264K</strong>.</li>
<li><strong class="source-inline">224396K-&gt;215361K(601088K)</strong>: After the GC ran, the total memory of used space was reduced from <strong class="source-inline">224396K</strong> to <strong class="source-inline">215361K</strong>. The total allocated memory is <strong class="source-inline">601088K</strong>.</li>
<li><strong class="source-inline">[Metaspace: 2649K-&gt;2649K(1056768K)]</strong>: No memory was reclaimed from the Meta space as a result of the GC. The total allocated Meta space is <strong class="source-inline">1056768K</strong>.</li>
<li><strong class="source-inline">3.4609247 secs</strong>: Total time taken by GC.</li>
<li><strong class="source-inline">[Times: user=3.40 sys=0.02, real=3.46 secs]</strong>: This part of the log statement tells us about the time taken to do the garbage collection. The <strong class="source-inline">user</strong> time tells the <a id="_idIndexMarker1237"/>processor time that the GC took to execute. The <strong class="source-inline">sys</strong> time denotes the time taken by I/O and other system activities. Finally, the <strong class="source-inline">real</strong> time denotes the total time taken to finish the GC event.</li>
</ul>
<p>Based on these footprints, we can determine whether we need to increase the heap space or increase the<a id="_idIndexMarker1238"/> meta space and specifies any memory leaks that are happening in the application.</p>
<ol>
<li value="2"><strong class="bold">Memory tuning</strong>: A memory leak <a id="_idIndexMarker1239"/>may occur if the following observations are listed:<ul><li>The JVM heap size is being filled frequently</li>
<li>The young generation space is being completely recycled, but the old generation used space is increasing with every GC run</li>
</ul></li>
</ol>
<p>Before deciding whether it is a genuine memory leak problem or not, you should increase the heap space using the following commands:</p>
<p class="source-code"><strong class="bold">-Xms&lt;heap size&gt;[unit] // for min heap size</strong></p>
<p class="source-code"><strong class="bold">-Xmx&lt;heap size&gt;[unit] //for max heap size</strong></p>
<p class="source-code"><strong class="bold">//unit can be g(GB),m(MB) or k(KB)</strong></p>
<p>If this doesn’t help, then the root cause is most likely a memory leak.</p>
<p>If we see that the young <a id="_idIndexMarker1240"/>generation space is getting filled frequently or that the Meta space is being heavily used, we can plan to change the total allocated space in all these regions using the following commands:</p>
<ul>
<li><strong class="source-inline">-XX:MaxMetaspaceSize</strong>: This sets the maximum amount of memory that can be allocated for the class metadata. The default value is <strong class="source-inline">infinite</strong> (or the same as the heap space).</li>
<li><strong class="source-inline">-XX:MetaspaceSize</strong>: Sets the threshold size of the allocated class metadata above which the GC will be triggered. </li>
<li><strong class="source-inline">-XX:MinMetaspaceFreeRatio</strong>: The minimum percentage of the Meta space memory region that needs to be available after garbage collection. If the amount of memory left is below the threshold, the Meta space region will be resized.</li>
<li><strong class="source-inline">-XX:MaxMetaspaceFreeRatio</strong>: The maximum percentage of the Meta space memory region that needs to be available after garbage collection. If the amount of memory left is above the threshold, the Meta space region will be resized.</li>
<li><strong class="source-inline">-XX:NewSize</strong>: This sets the<a id="_idIndexMarker1241"/> initial size of the young generation space.</li>
<li><strong class="source-inline">-XXMaxNewSize</strong>: This specifies the maximum size of young generation space.</li>
<li><strong class="source-inline">-Xmn</strong>: This specifies the size of the entire young generation space, meaning Eden and the two survivor spaces.</li>
</ul>
<ol>
<li value="3"><strong class="bold">Latency tuning</strong>: If you come across a<a id="_idIndexMarker1242"/> scenario where you are seeing that the application is being frozen a lot although CPU utilization or memory utilization is not being used much by the <a id="_idIndexMarker1243"/>application, then you must tune the GC for latency. Follow these steps to perform latency tuning:<ol><li>Check the type of GC that the application is using. Change it to G1, if it’s not already set. You can enable G1 using the <strong class="source-inline">-XX:+G1GC</strong> command.</li>
<li>Set <strong class="source-inline">-Xmx</strong> and <strong class="source-inline">-Xms</strong> to the same value to reduce application pause intervals.</li>
<li>Set the <strong class="source-inline">-XX:+AlwaysPreTouch</strong> flag to <strong class="source-inline">true</strong> so that the memory pages are loaded when the application is started.</li>
<li>If you are using G1, check whether the minor GC or full GC is taking more time. If the minor GC is taking more time, we can reduce the values of <strong class="source-inline">-XX:G1NewSizePercent</strong> and <strong class="source-inline">-XX:G1MaxNewSizePercent</strong> . If the major GC is taking more time, we can increase the value of the <strong class="source-inline">-XX:G1MixedGCCountTarget</strong> flag, which will help spread the tenured GC into multiple runs and reduce the frequency of full GC events.</li>
</ol></li>
<li><strong class="bold">Throughput tuning</strong>: To increase<a id="_idIndexMarker1244"/> the throughput of an application, you can do either of the following or both:<ul><li>Increase the value of the <strong class="source-inline">--XX:MaxGCPauseMillis</strong> property to clean more garbage in a single GC run. However, this may affect your latency.</li>
<li>Load memory pages into memory at the start of the application by setting the <strong class="source-inline">-XX:+AlwaysPreTouch</strong> and <strong class="source-inline">-XX:+UseLargePages</strong> flags.</li>
</ul></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="bold">Latency</strong> is the total time<a id="_idIndexMarker1245"/> elapsed to process and send an event, message, or data to its destination. On the other <a id="_idIndexMarker1246"/>hand, <strong class="bold">throughput</strong> is the number of records, events, or messages processed within a specified period.</p>
<p>Although JVM and GC<a id="_idIndexMarker1247"/> tuning is a vast topic, we briefly tried to cover a few important JVM and GC tuning techniques to improve throughput and latency. In the next section, we will discuss how can we optimize big data loads.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor176"/>Big data performance tuning</h2>
<p>Performance tuning for big data is a <a id="_idIndexMarker1248"/>huge topic. For brevity, we will limit ourselves to a few performance tuning tips and tricks that are usually applied to the most popular big data processing technologies, namely Spark and Hive.</p>
<h3>Hive performance tuning</h3>
<p><strong class="bold">Hive performance tuning</strong> is the<a id="_idIndexMarker1249"/> collective process and technique to<a id="_idIndexMarker1250"/> improve and accelerate the performance of your Hive environment. The following are some commonly faced Hive performance issues:</p>
<ul>
<li><strong class="bold">Slow-running queries</strong>: Often, you will notice that your Hive query is taking a huge amount of time to finish. There can be several reasons for slow-running queries. The following are a few <a id="_idIndexMarker1251"/>commonly encountered scenarios of slow-running queries and their solution:<ul><li>Poorly written queries result in a cross-join or full outer join. An unintended cross-join can happen when the join columns in either of the tables have duplicates or a self-join is happening in the query. Fine-tune your Hive queries to avoid cross-joins as much as possible.</li>
<li>The speed of a slow-running query can be improved by applying a map-side join if one of the join tables contains a small amount of data. In a map-side join, the smaller dataset is broadcast to all mapper nodes so that the join happens locally without much shuffling. The downside of a map-side join is that the data in the smaller table needs to be small enough to fit into memory.</li>
<li>For a scenario suitable for<a id="_idIndexMarker1252"/> a map-side join, we can further improve the speed of the tables needed for the join to be bucketed. <strong class="bold">Bucketing</strong> is the technique by which data in a Hive table is broken into a fixed number of ranges or clusters, based on the join column(s). A bucketed table can be used for a <strong class="bold">bucket map-join</strong> or <strong class="bold">sort-merge-bucket (SMB) map-join</strong>, both of which perform better than normal map-joins. However, bucketed tables can be joined with each other, only if the total buckets of one table are multiples of the number of buckets in the other table. For example, Table1 has 2 buckets and Table2 has 4 buckets. Since 4 is a multiple of 2, these tables can be joined.</li>
<li>Sometimes, data grows rapidly, which makes Hive jobs slow. In such cases, map-side operations take a huge amount of time. To overcome such issues, use partitioning.</li>
<li>If you notice that data read is slow or data shuffle is quite slow, check for the <em class="italic">data format</em> and <em class="italic">compression</em> that were used. For example, columnar structures such as Parquet and ORC have 5% to 10% higher performance than JSON. The columnar format also is known to have around a 90% higher compression ratio than JSON. More compressed data can reduce a good amount of network latency and hence improve overall performance.</li>
<li>The Hive execution speed <a id="_idIndexMarker1253"/>can be considerably improved by changing the <em class="italic">execution engine</em> from map-reduce to Tez or Spark. You can do this by setting the following parameter in the Hive configuration file:<p class="source-code"><strong class="bold">&lt;property&gt;</strong></p><p class="source-code"><strong class="bold">  &lt;name&gt;hive.execution.engine&lt;/name&gt;</strong></p><p class="source-code"><strong class="bold">  &lt;value&gt;spark&lt;/value&gt;</strong></p><p class="source-code"><strong class="bold">&lt;/property&gt;</strong></p></li>
</ul></li>
<li><strong class="bold">Job failures</strong>: Job failures are comparatively rare in Hive. Most job failures don’t occur due to performance<a id="_idIndexMarker1254"/> bottlenecks. However, if you observe that the job is running for a long time and hanging and then it fails due to a <strong class="source-inline">TimeoutException</strong>, chances<a id="_idIndexMarker1255"/> are that you have encountered a <em class="italic">small file issue</em>. This occurs when millions of small files (whose size is less than the block size of 128 MB) are written into the Hive table’s external path. Each HDFS file has metadata stored in Hadoop’s name-node. Too many small files in a Hive table cause too much metadata to be read by the job. Hence, the job fails either due to memory overrun or timeout exceptions. In such cases, either run a compaction job (refer to the <em class="italic">Core batch processing patterns</em> section of <a href="B17084_07.xhtml#_idTextAnchor110"><em class="italic">Chapter 7</em></a>, <em class="italic">Core Architectural Design Patterns</em>) or store the data in sequential files.</li>
</ul>
<p>With that, we’ve discussed various Hive optimization techniques that are used to fine-tune Hive query speeds and performance. Now, let’s look at Spark performance tuning.</p>
<h3>Spark performance tuning</h3>
<p>Spark is the most popular big data<a id="_idIndexMarker1256"/> processing engine. When Spark applications are tuned properly, it lowers the resource cost while maintaining the <a id="_idIndexMarker1257"/>SLA for critical processes. This is important for both cloud and on-premise environments.  </p>
<p>A Spark tuning job starts by debugging and observing the problems that occur during a Spark job’s execution. You can observe various metrics using the Spark UI or any profiling application such as Datadog.</p>
<p>The following are a few best practices for Spark optimization that are commonly applied while handling Spark applications:</p>
<ul>
<li><strong class="bold">Serialization</strong>: <ul><li><em class="italic">Problem</em>: Slow data read <a id="_idIndexMarker1258"/>from a Hive table or HDFS</li>
<li><em class="italic">Cause</em>: The default Java serializer slows down the read speed of the data from Hive or HDFS</li>
<li><em class="italic">Solution</em>: To overcome this problem, we should set the default serializer to Kyro Serializer, which performs a much faster Serde operation</li>
</ul></li>
<li><strong class="bold">Partition sizes</strong>: <ul><li><em class="italic">Problem</em>: The Spark job runs slowly since <a id="_idIndexMarker1259"/>one or two executors among many executors take much more time to finish the task. Sometimes, the job hangs due to a slow-performing executor.</li>
<li><em class="italic">Cause</em>: A possible cause can be data skew. In such a scenario, you will find that the slow-performing executor is processing the bulk of the data. Hence, the data is not well-partitioned.</li>
<li><em class="italic">Solution</em>: Repartition the data loaded into Spark DataFrames before processing the data further.</li>
</ul></li>
</ul>
<p>Let’s look at another common problem that’s encountered related to partitioning:</p>
<ul>
<li> <em class="italic">Problem</em>: The Spark jobs are slowing down as all executors are heavily loaded and all executors are taking a long time to process the job.</li>
<li><em class="italic">Cause</em>: The likely cause is that the data has outgrown and you are running the Spark job with far fewer partitions.</li>
<li><em class="italic">Solution</em>: Repartition the data loaded in the Spark DataFrame and increase the number of partitions. Adjust the<a id="_idIndexMarker1260"/> number of partitions until optimum performance is achieved. </li>
</ul>
<p>Another common problem that’s encountered in Spark is as follows:</p>
<ul>
<li><em class="italic">Problem</em>: A Spark job needs to write a single file as an output but the job gets stuck in the last step.</li>
<li><em class="italic">Cause</em>: Since you have used <strong class="source-inline">repartition()</strong> to create a single output file, a full data shuffle takes place, choking the performance. </li>
<li><em class="italic">Solution</em>: Use <strong class="source-inline">coalesce()</strong> instead of <strong class="source-inline">repartition()</strong>. <strong class="source-inline">coalesce()</strong> avoids doing a full shuffle as it collects data from all other partitions and copies data to a selected partition that contains the maximum amount of data.</li>
</ul>
<ul>
<li><strong class="bold">Executor and driver sizing</strong>: Now, let’s look at a few performance issues that you can encounter due to ineffective settings<a id="_idIndexMarker1261"/> for the executor and driver resources:<ul><li><em class="italic">Problem</em>: We are encountering multiple memory issues such as <strong class="source-inline">OutofMemoryException</strong>, GC overhead memory exceeded, or JVM heap space overrun.</li>
<li><em class="italic">Cause</em>: Inefficient configuration of the driver and executor memory and CPU cores.</li>
<li><em class="italic">Solution</em>: There is no one solution. Here, we will discuss various best practices that can be used to avoid inefficient driver and executor resource configuration. Before we begin, we will assume that you have basic familiarity with Apache Spark and know its basic concepts. If you are new to Apache Spark, you can read this concise<a id="_idIndexMarker1262"/> blog about the basics of the Spark architecture: <a href="https://www.edureka.co/blog/spark-architecture/">https://www.edureka.co/blog/spark-architecture/</a>.</li>
<li>For both executor and driver<a id="_idIndexMarker1263"/> sizing, there is a total of five properties that we need to set for Spark job optimization. They are as follows:<ul><li><strong class="bold">Executor cores or executor-cores</strong>: According to the official documentation, an optimal <a id="_idIndexMarker1264"/>executor core ranges from 2 to 5. For an optimized solution with maximum parallelism, we can set this value to <strong class="source-inline">5</strong> by using the following property:</li>
</ul><p class="source-code"><strong class="bold">--executor-cores 5</strong></p><ul><li><strong class="bold">The number of executors or num-executors</strong>: Setting the number of executors is often tricky. If we are using a shared cluster, then we have to note the number of total <a id="_idIndexMarker1265"/>minimum cores/vCPUs available. If we are using a standalone cluster, then we must note the core size of each node. Let’s assume that this number is 16 per node. Here, the total vCPUs will be 48, if the number of nodes is 3. In such a scenario, the aim should be to utilize all 16 vCPUs for the driver and executor. As discussed previously, maximum parallelism can be achieved by setting the executor per core to <strong class="source-inline">5</strong>. So, in a node, the maximum number of executors will be 3 (16 cores / 5 executor-cores per executor). So, for three nodes, we will have a total of 9 executors (3 nodes x 3 executors/nodes). However, according to the best practice documentation of Spark, the executor cores per executor should be the same as the driver core. Hence, we would require 1 worker to work out of the 9 workers as the application driver. Hence, the number of executors will become 8 (9 workers – 1 driver worker). As a general formula, you can use the following: </li>
</ul></li>
</ul></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer254">
<img alt="" height="55" src="image/Formula_11.5.jpg" width="900"/>
</div>
</div>
<p>Here, Nc denotes the number of cores per node, Ec denotes the executor cores per executor, and Tn denotes the total number of nodes.</p>
<p>In this use case, <em class="italic">Nc</em> is 16, <em class="italic">Ec</em> is 5, and <em class="italic">Tn</em> is 3. Hence, we reach the same result of 8 (<strong class="source-inline">FLOOR</strong>(16*3/5) -1).</p>
<p>With that, we have discussed how to optimally set the driver and executor properties. However, for Spark 3.0 and above running on a YARN cluster, it makes more sense to enable dynamic allocation. Although you can set a minimum and a maximum number <a id="_idIndexMarker1266"/>of executor cores, you must let the environment itself determine the number of executors needed. Optionally, you may want to set a cap on the maximum number of executors (by using the <strong class="source-inline">spark.dynamicAllocation.maxExecutors</strong> property) after discussing this with your Hadoop admin.</p>
<ul>
<li><strong class="bold">Executor memory or executor-memory</strong>: To calculate the optimal executor memory, we <a id="_idIndexMarker1267"/>need to understand the total amount of memory that’s used by an executor. The total memory that’s used by an executor is the total of the executor memory and the memory overhead:</li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><img alt="Figure 11.18 – Total memory used by executor = memory overhead + executor-memory " height="141" src="image/B17084_11_018.png" width="401"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Total memory used by executor = memory overhead + executor-memory</p>
<p>The memory overhead of the executor defaults to a greater amount between 10% of the executor’s memory size or 384 MB. Now, to find the correct executor memory size, we <a id="_idIndexMarker1268"/>need to look at the YARN resource manager’s <strong class="bold">Nodes</strong> tab. Each record in the <strong class="bold">Nodes</strong> tab will have a <strong class="bold">Total Memory Column</strong>, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="Figure 11.19 – Total memory available for the executors in a node " height="356" src="image/B17084_11_019.jpg" width="1148"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – Total memory available for the executors in a node</p>
<p>In the preceding screenshot, a total of <strong class="bold">112 GB</strong> of memory is available from the node for executors and drivers, after memory has been set aside for the cluster manager. Now, we must calculate the memory overhead for this executor by using the following formula:</p>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="" height="53" src="image/Formula_11.6.jpg" width="935"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer258">
<img alt="" height="51" src="image/Formula_11.7.jpg" width="750"/>
</div>
</div>
<p>Let’s try to use the example described earlier for calculating the executor memory. We will divide this available node memory by the total number of executors per node. Then, we will divide this by 1.1. The formula to calculate executor memory is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer259">
<img alt="" height="106" src="image/Formula_11.8.jpg" width="985"/>
</div>
</div>
<p>Here, <em class="italic">Nm</em> is the total node memory and <em class="italic">Ne</em> is the number of executors per node. </p>
<p>Using the preceding formula in our scenario, the executor memory should be 36.94 GB (112/3 – .384). So, we can set the executor memory to 36 GB.</p>
<ul>
<li><strong class="bold">Driver cores or driver-cores</strong>: By default, this will be set to <strong class="source-inline">1</strong>. </li>
<li><strong class="bold">Driver memory or driver-memory</strong>: Driver memory is either less than or equal to the executor memory. Based on a specific scenario, this value can be set to less than or equal to the executor’s memory. One of the optimizations that is advisable in the case of driver memory issues is to make the driver memory the same as the executor memory. This can speed up performance.</li>
</ul>
<ul>
<li><strong class="bold">Directed acyclic graph (DAG) optimization</strong>: Let’s look at<a id="_idIndexMarker1269"/> some issues that you can resolve using DAG optimization:<ul><li><em class="italic">Problem</em>: In the DAG, we can see that more than one stage is identical in terms of all its tasks or operations.</li>
<li><em class="italic">Cause</em>: The DataFrame, denoted as d1, is being used to derive more than one DataFrame (for example – d2, d3, and d4). Since Spark is lazily computed, when creating<a id="_idIndexMarker1270"/> each DataFrame (d2, d3, and d4), d1 is recalculated every time.</li>
<li><em class="italic">Solution</em>: In such a scenario, we must persist the dependent DataFrame (d1) using the <strong class="source-inline">Dataset&lt;T&gt; persist(StorageLevel newLevel)</strong> method.</li>
</ul></li>
</ul>
<p>Now that we’ve discussed performance tuning for big data, let’s learn how to tune real-time applications.</p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor177"/>Optimizing streaming applications</h2>
<p>Now, let’s learn how to optimize <a id="_idIndexMarker1271"/>streaming applications. Here, our discussion will mainly focus on Kafka since we discussed this earlier in this book. When it comes to streaming applications, we can tune their latency and throughput. In this section, we will learn how to observe a performance bottleneck in Kafka. Then, we will learn how to optimize producers and consumers. Finally, we will discuss a few tips and tricks that help tune overall Kafka cluster performance.</p>
<h3>Observing performance bottlenecks in streaming applications</h3>
<p>The first thing about any tuning is<a id="_idIndexMarker1272"/> monitoring and finding out what the problem is and where it’s occurring. For real-time stream processing applications, this is of utmost importance. There are quite a few Kafka observability tools available such as Lenses and <strong class="bold">Confluent Control Center</strong> (<strong class="bold">C3</strong>). Here, we will see how C3 helps us observe anomalies.</p>
<p>When you navigate to any topic in C3, you will see three tabs – <strong class="bold">Producer</strong>, <strong class="bold">Consumer</strong>, and <strong class="bold">Consumer lag</strong>. The <strong class="bold">Consumer lag</strong> tab can tell if a consumer is slow or not. The following screenshot shows that the consumer (group) is lagging by <strong class="bold">1,654</strong> records while reading data from the topic:</p>
<div>
<div class="IMG---Figure" id="_idContainer260">
<img alt="Figure 11.20 – Consumer lag " height="567" src="image/B17084_11_020.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Consumer lag</p>
<p>The preceding screenshot also shows the number of active consumers in this consumer group. This can indicate whether the consumer is set to utilize the full performance potential of the topic or not. To<a id="_idIndexMarker1273"/> learn more, click on the <strong class="bold">Consumer group ID</strong> property (in the preceding screenshot, it is <strong class="bold">analyticsTeam</strong>). You will see the following screen:</p>
<div>
<div class="IMG---Figure" id="_idContainer261">
<img alt="Figure 11.21 – Partition-wise consumer lag  " height="807" src="image/B17084_11_021.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – Partition-wise consumer lag </p>
<p>The preceding screenshot shows the lag of the consumer group partition-wise. From <em class="italic">Figure 11.19</em> and <em class="italic">Figure 11.20</em>, it is evident that only one consumer is running but that the topic has three partitions. As we can see, there is scope for tuning the consumer application by increasing the number of consumers in the consumer group. Similarly, we can look at the <a id="_idIndexMarker1274"/>producer speed versus the consumer speed of a topic and find any slowness in the producer.</p>
<h3>Producer tuning</h3>
<p>The following are a few<a id="_idIndexMarker1275"/> common tuning techniques for Kafka producers:</p>
<ul>
<li>When the producer sends a message to the Kafka broker, it receives an acknowledgment. The <strong class="source-inline">acks=all</strong> property, along with the value of <strong class="source-inline">min.insync.replicas</strong>, determine the throughput of a producer. If <strong class="source-inline">acks=all</strong> is set and the acknowledgment takes more time to come (because it must write all its replicas before sending the acknowledgment), then the producer cannot produce any further messages. This reduces the throughput considerably. We must make a choice here between durability and throughput.</li>
<li>Idempotent producers guarantee exactly-once delivery of messages. However, this comes with a cost: it reduces the throughput of a producer. So, a system must choose between deduplication and throughput. While using idempotent producers, you can improve throughput slightly by increasing the value of <strong class="source-inline">max.in.flight.requests.per.connection</strong>.</li>
<li>One way to improve throughput is to increase the property value of <strong class="source-inline">batch.size</strong>. Although a producer is used to send the message, it does so asynchronously (for most applications). Producers usually have a buffer where producing records are batched before they’re sent to the Kafka broker. Sending the records to the broker in batches improves the throughput of the producer. However, latency increases as the value of <strong class="source-inline">batch.size</strong> increases. Set <strong class="source-inline">batch.size</strong> to a balanced value so that we get optimum throughput without affecting the latency considerably.</li>
<li><strong class="source-inline">Linger.ms</strong> is another property that is affected when the messages in the Kafka producer buffer are sent to the Kafka broker. The higher the value of <strong class="source-inline">linger.ms</strong>, the higher the throughput and latency will be. Again, it must be set in a balanced fashion to have optimum throughput as well as latency. For extremely huge loads of data, a higher value of <strong class="source-inline">linger.ms</strong> can give a considerable boost to performance.</li>
</ul>
<p>With that, we have briefly <a id="_idIndexMarker1276"/>discussed the techniques for producer optimization. Now, let’s find out how we can optimize the performance of Kafka consumers. </p>
<h3>Consumer tuning</h3>
<p>The following are a few tips and tricks <a id="_idIndexMarker1277"/>you can utilize to optimize the consumer to make full use of the potential improvements the consumer can have:</p>
<ul>
<li>For a consumer to consume in the most optimized fashion, the total number of consumers should be equal to the number of partitions. In a multithreaded Kafka consumer, the total number of threads across all the consumers in the consumer group should be equal to the number of topic partitions.</li>
<li>Another typical scenario that makes the consumer slow is too much rebalancing of the consumer. This can happen if the time that’s taken to poll and process <strong class="source-inline">max.poll.records</strong> is more than the value of <strong class="source-inline">max.poll.interval.ms</strong>. In such cases, you may want to either increase the value of <strong class="source-inline">max.poll.interval.ms</strong> or decrease the value of <strong class="source-inline">max.poll.records</strong>.</li>
<li>Rebalancing can happen in scenarios where a consumer can’t send the heartbeat or the heartbeat packages reach slowly due to network latency. If we are okay with static consumers (the consumer statically maps to a partition), we can configure a unique <strong class="source-inline">group.instance.id</strong> value for each consumer instance in the consumer group. This will increase latency for the consumer that goes down but will ensure great latency and throughput for other partitions as it will avoid unnecessary consumer rebalancing.</li>
<li>Since <strong class="source-inline">consumer.commitSync()</strong> blocks your thread unless a commit is done successfully, it may be slower in most cases compared to <strong class="source-inline">consumer.commitAsync()</strong>. However, if there is a fatal error, it makes sense to use <strong class="source-inline">commitSync()</strong> to ensure the messages are committed before the consumer application goes down.</li>
</ul>
<p>Although we mainly<a id="_idIndexMarker1278"/> focused this discussion on Apache Kafka, other alternative products that enable stream processing such as Apache Pulsar and AWS Kinesis have similar performance tuning techniques. </p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>Database tuning</h2>
<p>Database tuning is an important <a id="_idIndexMarker1279"/>activity when it comes to performance tuning. It includes SQL tuning, data read and write tuning from database tables, database-level tuning, and making optimizations while creating data models. Since these all vary considerably from one database to another (including SQL and NoSQL databases), database tuning is outside the scope of this book.</p>
<p>Now, let’s summarize what we learned in this chapter.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor179"/>Summary</h1>
<p>We started this chapter by understanding what performance engineering is and learning about the performance engineering life cycle. We also pointed out the differences between performance engineering and performance testing. Then, we briefly discussed the various tools that are available to help with performance engineering. We learned about the basics of performance benchmarking and what to consider while creating a benchmark. Then, we learned about various performance optimization techniques and how to apply them to Java applications, big data applications, streaming applications, and databases.</p>
<p>With that, we have learned how to do performance engineering for both batch-based and real-time data engineering problems. In the next chapter, we will learn how to evaluate multiple architectural solutions and how to present the recommendations.</p>
</div>
</div>
</body></html>