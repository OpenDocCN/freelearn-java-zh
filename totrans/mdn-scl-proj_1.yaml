- en: Predict the Class of a Flower from the Iris Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从鸢尾花数据集中预测花的类别
- en: This chapter kicks off a **machine learning** (**ML**) initiative in Scala and
    Spark. Speaking of Spark, its **Machine Learning Library** (**MLlib**) living
    under the `spark.ml` package and accessible via its MLlib `DataFrame`-based API
    will help us develop scalable data analysis applications. The MLlib `DataFrame`-based
    API, also known as Spark ML, provides powerful learning algorithms and pipeline
    building tools for data analysis. Needless to say, we will, starting this chapter,
    leverage MLlib's classification algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章在Scala和Spark中启动了一个**机器学习**（**ML**）倡议。谈到Spark，其**机器学习库**（**MLlib**）位于`spark.ml`包下，可以通过其基于MLlib
    `DataFrame`的API访问。Spark ML，也称为基于MLlib的`DataFrame` API，提供了强大的学习算法和管道构建工具，用于数据分析。不用说，从本章开始，我们将利用MLlib的分类算法。
- en: The Spark ecosystem, also boasting of APIs to R, Python, and Java in addition
    to Scala, empowers our readers, be they beginner, or seasoned data professionals,
    to make sense of and extract analytics from various datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark生态系统，除了Scala之外，还提供了R、Python和Java的API，使我们的读者，无论是初学者还是经验丰富的数据专业人士，都能够理解并从各种数据集中提取分析。
- en: Speaking of datasets, the Iris dataset is the simplest, yet the most famous
    data analysis task in the ML space. This chapter builds a solution to the data
    analysis classification task that the Iris dataset represents.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 谈及数据集，鸢尾花数据集是机器学习空间中最简单而又最著名的数据分析任务。本章构建了一个解决方案，以解决鸢尾花数据集所代表的数据分析分类任务。
- en: 'Here is the dataset we will refer to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们将要参考的数据集：
- en: 'UCI Machine Learning Repository: Iris Data Set'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCI机器学习仓库：鸢尾花数据集
- en: Accessed July 13, 2018
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问日期：2018年7月13日
- en: 'Website URL: [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站URL：[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)
- en: The overarching learning objective of this chapter is to implement a Scala solution
    to the so-called **multivariate** classification task represented by the Iris
    dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要学习目标是实现一个Scala解决方案，以解决由鸢尾花数据集所代表的所谓**多元**分类任务。
- en: 'The following list is a section-wise breakdown of individual learning outcomes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表是按章节划分的各个学习成果的概述：
- en: A multivariate classification problem
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多元分类问题
- en: Project overview—problem formulation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目概述—问题表述
- en: Getting started with Spark
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用Spark
- en: Implementing a multiclass classification pipeline
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现多类分类管道
- en: The following section offers the reader an in-depth perspective on the Iris
    dataset classification problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分为读者提供了对鸢尾花数据集分类问题的深入视角。
- en: A multivariate classification problem
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元分类问题
- en: The most famous dataset in data science history is Sir Ronald Aylmer Fisher's classical
    Iris flower dataset, also known as Anderson's dataset. It was introduced in 1936,
    as a study in understanding multivariate (or multiclass) classification. What
    then is multivariate?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学历史上最著名的数据库是罗纳德·艾尔默·费舍尔爵士的经典鸢尾花数据集，也称为安德森数据集。它在1936年作为理解多元（或多类）分类的研究被引入。那么什么是多元变量呢？
- en: Understanding multivariate
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解多元变量
- en: 'The term multivariate can bear two meanings:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 术语多元可以有两个含义：
- en: In terms of an adjective, multivariate means having or involving one or more
    variables.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从形容词的角度来看，多元变量意味着包含一个或多个变量。
- en: In terms of a noun, multivariate may represent a mathematical vector whose individual
    elements are variate. Each individual element in this vector is a measurable quantity
    or variable.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从名词的角度来看，多元可能代表一个数学向量，其个别元素是可变的。这个向量中的每个个别元素都是一个可测量的数量或变量。
- en: Both meanings mentioned have a common denominator variable. Conducting a multivariate
    analysis of an experimental unit involves at least one measurable quantity or
    variable. A classic example of such an analysis is the Iris dataset, having one
    or more (outcome) variables per observation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 提到的两种含义都有一个共同的分母变量。对一个实验单位进行多元分析至少涉及一个可测量的数量或变量。此类分析的典型例子是鸢尾花数据集，每个观测值都有一个或多个（结果）变量。
- en: In this subsection, we understood multivariate in terms of variables. In the
    next subsection, we briefly touch upon different kinds of variables, one of them
    being categorical variables.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们通过变量来理解多元变量。在下一小节中，我们将简要介绍不同种类的变量，其中之一就是分类变量。
- en: Different kinds of variables
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同种类的变量
- en: 'In general, variables are of two types:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，变量有两种类型：
- en: '**Quantitative variable**: It is a variable representing a measurement that
    is quantified by a numeric value. Some examples of quantitative variables are:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定量变量**：它是一个表示通过数值量化的测量的变量。定量变量的例子包括：'
- en: A variable representing the age of a girl called `Huan` (`Age_Huan`). In September
    of 2017, the variable representing her age contained the value `24`. Next year,
    one year later, that variable would be the number 1 (arithmetically) added to
    her current age.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示名叫“Huan”的女孩年龄的变量（`Age_Huan`）。在2017年9月，代表她年龄的变量包含的值是`24`。明年，一年后，这个变量将是她当前年龄的1（算术上）。
- en: The variable representing the number of planets in the solar system (`Planet_Number`). Currently,
    pending the discovery of any new planets in the future, this variable contains
    the number `12`. If scientists found a new celestial body tomorrow that they think
    qualifies to be a planet, the `Planet_Number` variable's new value would be bumped
    up from its current value of `12` to `13`.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示太阳系中行星数量的变量（`Planet_Number`）。目前，在未来的任何新行星被发现之前，这个变量包含的数字是`12`。如果科学家明天发现他们认为有资格成为行星的新天体，`Planet_Number`变量的新值将从当前的`12`增加到`13`。
- en: '**Categorical variable**: A variable that cannot be assigned a numerical measure
    in the natural order of things. For example, the status of an individual in the
    United States. It could be one of the following values: a citizen, permanent resident,
    or a non-resident.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类变量**：在自然顺序中不能赋予数值测量的变量。例如，美国个人的状态。它可以是以下值之一：公民、永久居民或非居民。'
- en: In the next subsection, we will describe categorical variables in some detail.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个子节中，我们将详细描述分类变量。
- en: Categorical variables
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类变量
- en: 'We will draw upon the definition of a categorical variable from the previous
    subsection. Categorical variables distinguish themselves from quantitative variables
    in a fundamental way. As opposed to a quantitative variable that represents a
    measure of a something in numerical terms, a categorical variable represents a
    grouping name or a category name, which can take one of the finite numbers of
    possible categories. For example, the species of an Iris flower is a categorical
    variable and the value it takes could be one value from a finite set of categorical
    values: Iris-setosa, Iris-virginica, and Iris-versicolor.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将借鉴前一个子节中分类变量的定义。分类变量在本质上与定量变量区分开来。与表示某种东西的数值测量的定量变量相反，分类变量表示一个分组名称或类别名称，它可以取有限数量的可能类别中的一个。例如，鸢尾花的种类是一个分类变量，它所取的值可以是有限集合中的任何一个值：Iris-setosa、Iris-virginica和Iris-versicolor。
- en: 'It may be useful to draw on other examples of categorical variables; these
    are listed here as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有助于引用其他分类变量的例子；以下列出了这些例子：
- en: The blood group of an individual as in A+, A-, B+, B-, AB+, AB-, O+, or O-
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人的血型，如A+、A-、B+、B-、AB+、AB-、O+或O-
- en: The county that an individual is a resident of given a finite list of counties
    in the state of Missouri
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人居住的县，给定密苏里州有限数量的县列表
- en: The political affiliation of a United States citizen could take up categorical
    values in the form of Democrat, Republican, or Green Party
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国公民的政治归属可以采取民主党、共和党或绿党的分类值
- en: In global warming studies, the type of a forest is a categorical variable that
    could take one of three values in the form of tropical, temperate, or taiga
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在全球变暖研究中，森林的类型是一个分类变量，它可以取三个值，即热带、温带或泰加。
- en: The first item in the preceding list, the blood group of a person, is a categorical
    variable whose corresponding data (values) are categorized (classified) into eight
    groups (A, B, AB, or O with their positives or negatives). In a similar vein,
    the species of an Iris flower is a categorical variable whose data (values) are
    categorized (classified) into three species groups—Iris-setosa, Iris-versicolor,
    and Iris-virginica.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前列出的第一个项目，即人的血型，是一个分类变量，其对应的数据（值）被分类（归类）为八个组（A、B、AB或O及其正负）。类似地，鸢尾花的种类是一个分类变量，其数据（值）被分类（归类）为三个物种组—Iris-setosa、Iris-versicolor和Iris-virginica。
- en: That said, a common data analysis task in ML is to index, or encode, current
    string representations of categorical values into a numeric form; doubles for
    example. Such indexing is a prelude to a prediction on the target or label, which
    we shall talk more about shortly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，机器学习中一个常见的数据分析任务是索引或编码当前字符串表示的分类值到数值形式；例如，双倍。这种索引是预测目标或标签的前奏，我们将在稍后详细讨论。
- en: In respect to the Iris flower dataset, its species variable data is subject
    to a classification (or categorization) task with the express purpose of being
    able to make a prediction on the species of an Iris flower. At this point, we
    want to examine the Iris dataset, its rows, row characteristics, and much more,
    which is the focus of the upcoming topic.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于鸢尾花数据集，其物种变量数据受到分类（或分类）任务的约束，其明确目的是能够预测鸢尾花的物种。在此阶段，我们想要检查鸢尾花数据集，其行，行特征以及更多内容，这是即将到来的主题的焦点。
- en: Fischer's Iris dataset
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 费舍尔的鸢尾花数据集
- en: 'The Iris flower dataset comprises of a total of 150 rows, where each row represents
    one flower. Each row is also known as an **observation**. This 150 observation
    Iris dataset is made up of three kinds of observations related to three different
    Iris flower species. The following table is an illustration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集总共包含150行，其中每行代表一朵花。每一行也被称为**观测**。这个由150个观测值组成的鸢尾花数据集由与三种不同的鸢尾花物种相关的三种观测值组成。以下表格是说明：
- en: '![](img/fefaf096-d868-407a-9302-a18314b30487.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fefaf096-d868-407a-9302-a18314b30487.png)'
- en: Iris dataset observation breakup table
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集观测值分解表
- en: 'Referring to the preceding table, it is clear that three flower species are
    represented in the Iris dataset. Each flower species in this dataset contributes
    equally to 50 observations apiece. Each observation holds four measurements. One
    measurement corresponds to one flower feature, where each flower feature corresponds
    to one of the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参考前面的表格，很明显，鸢尾花数据集中表示了三种花种。这个数据集中的每个花种都平均贡献了50个观测值。每个观测值包含四个测量值。一个测量值对应一朵花的一个特征，其中每个花特征对应以下之一：
- en: '**Sepal Length**'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**萼片长度**'
- en: '**Sepal Width**'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**萼片宽度**'
- en: '**Petal Length**'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花瓣长度**'
- en: '**Petal Width**'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花瓣宽度**'
- en: 'The features listed earlier are illustrated in the following table for clarity:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，以下表格展示了之前列出的特征：
- en: '![](img/f45af5d4-2ac7-4f79-8aef-5bad685bc9d2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f45af5d4-2ac7-4f79-8aef-5bad685bc9d2.png)'
- en: Iris features
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花特征
- en: 'Okay, so three flower species are represented in the Iris dataset. Speaking
    of species, we will henceforth replace the term *species* with the term *classes*
    whenever there is the need to stick to an ML terminology context. That means **#1**-**Iris-setosa** from
    earlier refers to **Class # 1**, **#2**-**Iris-virginica** to **Class # 2**, and
    **#3**-**Iris-versicolor** to **Class # 3**.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '好吧，所以鸢尾花数据集中表示了三种花种。说到物种，我们将从现在开始，每当需要坚持机器学习术语背景时，将术语*物种*替换为*类别*。这意味着**#1**-**Iris-setosa**从早先指的是**类别
    # 1**，**#2**-**Iris-virginica**指的是**类别 # 2**，**#3**-**Iris-versicolor**指的是**类别
    # 3**。'
- en: 'We just listed three different Iris flower species that are represented in
    the Iris dataset. What do they look like? What do their features look like? These
    questions are answered in the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚列出了在鸢尾花数据集中表示的三个不同的鸢尾花物种。它们看起来是什么样子？它们的特点是什么？以下截图回答了这些问题：
- en: '![](img/3d0b8100-a058-4788-98fd-34b3ed8bdc60.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d0b8100-a058-4788-98fd-34b3ed8bdc60.jpg)'
- en: Representations of three species of Iris flower
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花三种物种的表示
- en: That said, let's look at the **Sepal** and **Petal** portions of each class
    of Iris flower. The **Sepal** (the larger lower part) and **Petal** (the lower
    smaller part) dimensions are how each class of Iris flower bears a relationship
    to the other two classes of Iris flowers. In the next section, we will summarize
    our discussion and expand the scope of the discussion of the Iris dataset to a multiclass,
    multidimensional classification task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们来看看鸢尾花每个类别的**萼片**和**花瓣**部分。**萼片**（较大的下部部分）和**花瓣**（较小的下部部分）的尺寸是每个类别的鸢尾花与其他两个类别的鸢尾花之间关系的体现。在下一节中，我们将总结我们的讨论，并将鸢尾花数据集的讨论范围扩展到多类、多维分类任务。
- en: The Iris dataset represents a multiclass, multidimensional classification task
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鸢尾花数据集代表一个多类、多维分类任务
- en: 'In this section, we will restate the facts about the Iris dataset and describe
    it in the context of an ML classification task:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重申关于鸢尾花数据集的事实，并描述其在机器学习分类任务中的背景：
- en: The Iris dataset classification task is multiclass because a prediction of the
    class of a new incoming Iris flower from the wild can belong to any of three classes.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iris 数据集分类任务是多类的，因为从野外新到达的 Iris 花的类别预测可以属于三个类别中的任何一个。
- en: Indeed, this chapter is all about attempting a species classification (inferring
    the target class of a new Iris flower) using sepal and petal dimensions as feature
    parameters.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 的确，本章全部内容都是关于尝试进行物种分类（推断新 Iris 花的目标类别）使用萼片和花瓣尺寸作为特征参数。
- en: The Iris dataset classification is multidimensional because there are four features.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iris 数据集分类是多维的，因为有四个特征。
- en: 'There are 150 observations, where each observation is comprised of measurements
    on four features. These measurements are also known by the following terms:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有 150 个观测值，每个观测值由四个特征的测量组成。这些测量也可以用以下术语来表示：
- en: Input attributes or instances
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入属性或实例
- en: Predictor variables (`X`)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测变量 (`X`)
- en: Input variables (`X`)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入变量 (`X`)
- en: Classification of an Iris flower picked in the wild is carried out by a model
    (the computed mapping function) that is given four flower feature measurements.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在野外采集的 Iris 花的分类是通过一个模型（计算出的映射函数）来完成的，该模型提供了四个花特征测量值。
- en: 'The outcome of the Iris flower classification task is the identification of
    a (computed) predicted value for the response from the predictors by a process
    of learning (or fitting) a discrete number of targets or category labels (`Y`).
    The outcome or predicted value may mean the same as the following:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iris 花分类任务的输出是通过学习（或拟合）离散数量的目标或类别标签（`Y`）的过程，从预测变量识别一个（计算出的）预测值。输出或预测值可能意味着以下内容：
- en: 'Categorical response variable: In a later section, we shall see that an indexer
    algorithm will transform all categorical values to numbers'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类别响应变量：在后面的章节中，我们将看到索引算法会将所有类别值转换为数字
- en: Response or outcome variable (`Y`)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应或输出变量 (`Y`)
- en: So far, we have claimed that the outcome (`Y`) of our multiclass classification
    task is dependent on inputs (`X`). Where will these inputs come from? This is
    answered in the next section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们声称我们的多类分类任务的输出（`Y`）取决于输入（`X`）。这些输入将从哪里来？这个问题将在下一节中回答。
- en: The training dataset
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据集
- en: An integral aspect of our data analysis or classification task we did not hitherto
    mention is the training dataset. A training dataset is our classification task's
    source of input data (`X`). We take advantage of this dataset to obtain a prediction
    on each target class, simply by deriving optimal perimeters or boundary conditions.
    We just redefined our classification process by adding in the extra detail of
    the training dataset. For a classification task, then we have `X` on one side
    and `Y` on the other, with an inferred mapping function in the middle. That brings
    us to the mapping or predictor function, which is the focus of the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据分析或分类任务中未提及的一个重要方面是训练数据集。训练数据集是我们分类任务的输入数据源（`X`）。我们利用这个数据集通过推导最优边界或条件来获得每个目标类别的预测。我们只是通过添加训练数据集的额外细节来重新定义我们的分类过程。对于分类任务，我们有一边的
    `X` 和另一边的 `Y`，中间有一个推断的映射函数。这带我们来到了映射或预测函数，这是下一节的重点。
- en: The mapping function
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射函数
- en: 'We have so far talked about an input variable (`X`) and an output variable
    (`Y`). The goal of any classification task, therefore, is to discover patterns
    and find a mapping (predictor) function that will take feature measurements (`X`)
    and map input over to the output (`Y`). That function is mathematically formulated
    as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了输入变量（`X`）和输出变量（`Y`）。因此，任何分类任务的目标是发现模式并找到一个映射（预测）函数，该函数将特征测量（`X`）映射到输出（`Y`）。这个函数在数学上被表示为：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This mapping is how supervised learning works. A supervised learning algorithm
    is said to learn or discover this function. This will be the goal of the next
    section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种映射就是监督学习的工作方式。一个监督学习算法被说成是学习或发现这个函数。这将是下一节的目标。
- en: An algorithm and its mapping function
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法和其映射函数
- en: 'This section starts with a schematic depicting the components of the mapping
    function and an algorithm that learns the mapping function. The algorithm is learning
    the mapping function, as shown in the following diagram:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本节从展示映射函数组件和学习的映射函数的算法的示意图开始。算法正在学习映射函数，如下面的图所示：
- en: '![](img/313714d2-55c9-4136-91d6-336e3d311575.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/313714d2-55c9-4136-91d6-336e3d311575.png)'
- en: An input to output mapping function and an algorithm learning the mapping function
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到输出的映射函数以及学习映射函数的算法
- en: The goal of our classification process is to let the algorithm derive the best
    possible approximation of a mapping function by a learning (or fitting) process.
    When we find an Iris flower out in the wild and want to classify it, we use its
    input measurements as new input data that our algorithm's mapping function will
    accept in order to give us a predictor value (**Y**). In other words, given feature
    measurements of an Iris flower (the new data), the mapping function produced by
    a supervised learning algorithm (this will be a random forest) will classify the
    flower.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类过程的目标是通过学习（或拟合）过程让算法推导出映射函数的最佳可能近似。当我们发现野外的 Iris 花并想要对其进行分类时，我们使用其输入测量值作为新的输入数据，我们的算法的映射函数将接受这些数据以给出预测值（**Y**）。换句话说，给定
    Iris 花的特征测量值（新数据），由监督学习算法（这将是一个随机森林）产生的映射函数将对该花进行分类。
- en: 'Two kinds of ML problems exist that supervised learning classification algorithms
    can solve. These are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种机器学习问题，监督学习分类算法可以解决。以下是这些问题的描述：
- en: Classification tasks
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类任务
- en: Regression tasks
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归任务
- en: In the following paragraph, we will talk about a mapping function with an example. 
    We explain the role played by a "supervised learning classification task" in deducing
    the mapping function. The concept of a model is introduced.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我们将通过一个例子来讨论映射函数。我们解释了“监督学习分类任务”在推导映射函数中所起的作用。引入了模型的概念。
- en: Let's say we already knew that the (mapping) function `f(x)` for the Iris dataset
    classification task is exactly of the form `x + 1`,   then there is there no need
    for us to find a new mapping function.  If we recall, a mapping function is one
    that maps the relationship between flower features, such as sepal length and sepal
    width, on the species the flower belongs to? No.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已知 Iris 数据集分类任务的映射函数 `f(x)` 精确地是 `x + 1` 的形式，那么我们就没有必要寻找新的映射函数。如果我们回想一下，映射函数是一种将花特征（如花瓣长度和花瓣宽度）与花所属物种之间的关系映射的函数？不是。
- en: Therefore, there is no preexisting function `x + 1` that clearly maps the relationship
    between flower features and the flower's species. What we need is a model that
    will model the aforementioned relationship as closely as possible. Data and its
    classification seldom tend to be straightforward. A supervised learning classification
    task starts life with no knowledge of what function `f(x)` is. A supervised learning
    classification process applies ML techniques and strategies in an iterative process
    of deduction to ultimately learn what `f(x)` is.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，不存在预先存在的函数 `x + 1` 可以明确映射花特征与花种之间的关系。我们需要的是一个尽可能精确地模拟上述关系的模型。数据和其分类很少是直截了当的。一个监督学习分类任务从对函数
    `f(x)` 一无所知开始。监督学习分类过程通过迭代推理过程应用机器学习技术和策略，最终学习出 `f(x)` 是什么。
- en: In our case, such an ML endeavor is a classification task, a task where the
    function or mapping function is referred to in statistical or ML terminology as
    a **model**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，这种机器学习努力是一个分类任务，在统计学或机器学习术语中，这种函数或映射函数被称为**模型**。
- en: In the next section, we will describe what supervised learning is and how it
    relates to the Iris dataset classification.  Indeed, this apparently simplest
    of ML techniques finds wide applications in data analysis, especially in the business
    domain.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述什么是监督学习以及它与 Iris 数据集分类的关系。实际上，这种看似最简单的机器学习技术广泛应用于数据分析，尤其是在商业领域。
- en: Supervised learning – how it relates to the Iris classification task
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习 – 它与 Iris 分类任务的关系
- en: 'At the outset, the following is a list of salient aspects of supervised learning:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，以下是一个监督学习的显著方面的列表：
- en: The term **supervised** in supervised learning stems from the fact that the
    algorithm is learning or inferring what the mapping function is.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习中的“监督”一词源于算法正在学习或推断映射函数是什么。
- en: A data analysis task, either classification or regression.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析任务，无论是分类还是回归。
- en: It contains a process of learning or inferring a mapping function from a labeled
    training dataset.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含从标记的训练数据集中学习或推断映射函数的过程。
- en: Our Iris training dataset has training examples or samples, where each example
    may be represented by an input feature vector consisting of four measurements.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的 Iris 训练数据集包含训练示例或样本，其中每个示例可能由一个包含四个测量的输入特征向量表示。
- en: A supervised learning algorithm learns or infers or derives the best possible
    approximation of a mapping function by carrying out a data analysis on the training
    data. The mapping function is also known as a model in statistical or ML terminology.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习算法通过对训练数据进行数据分析，学习或推断或推导出映射函数的最佳可能近似。在统计或机器学习术语中，映射函数也被称为模型。
- en: 'The algorithm provides our model with parameters that it learns from the training
    example set or training dataset in an iterative process, as follows:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法通过迭代过程从训练示例集或训练数据集中学习参数，如下所示：
- en: Each iteration produces predicted class labels for new input instances from
    the wild
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次迭代都会为新输入实例生成预测的类别标签
- en: Each iteration of the learning process produces progressively better generalizations
    of what the output class label should be, and as in anything that has an end,
    the learning process for the algorithm also ends with a high degree of reasonable
    correctness on the prediction
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程的每次迭代都会逐步产生对输出类别标签应该是什么的更好泛化，并且正如任何有终点的事物一样，算法的学习过程也以预测的高度合理性结束。
- en: An ML classification process employing supervised learning has algorithm samples
    with correctly predetermined labels.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用监督学习的机器学习分类过程具有正确预定的标签的算法样本。
- en: The Iris dataset is a typical example of a supervised learning classification
    process. The term supervised arises from the fact that the algorithm at each step
    of an iterative learning process applies an appropriate correction on its previously
    generated model building process to generate its next best model.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红花数据集是监督学习分类过程的典型例子。术语“监督”源于算法在迭代学习过程的每一步都对其先前生成的模型构建过程进行适当的校正，以生成其下一个最佳模型。
- en: In the next section, we will define a training dataset. In the next section,
    and in the remaining sections, we will use the Random Forest classification algorithm
    to run data analysis transformation tasks. One such task worth noting here is
    a process of transformation of string labels to an indexed label column represented
    by doubles.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将定义一个训练数据集。在下一节以及剩余的章节中，我们将使用随机森林分类算法来运行数据分析转换任务。这里值得注意的一个任务是将字符串标签转换为表示为双精度数的索引标签列的过程。
- en: Random Forest classification algorithm
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类算法
- en: In a preceding section, we noted the crucial role played by the input or training
    dataset. In this section, we reiterate the importance of this dataset. That said,
    the training dataset from an ML algorithm standpoint is one that the Random Forest
    algorithm takes advantage of to train or fit the model by generating the parameters
    it needs. These are parameters the model needs to come up with the next best-predicted
    value. In this chapter, we will put the Random Forest algorithm to work on training
    (and testing) Iris datasets. Indeed, the next paragraph starts with a discussion
    on Random Forest algorithms or simply Random Forests.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个部分，我们提到了输入或训练数据集所起的关键作用。在本节中，我们再次强调这个数据集的重要性。也就是说，从机器学习算法的角度来看，训练数据集是随机森林算法利用它来训练或拟合模型，通过生成它需要的参数来训练或拟合模型。这些参数是模型需要用来得出下一个最佳预测值的参数。在本章中，我们将把随机森林算法应用于训练（和测试）红花数据集。实际上，下一段将开始讨论随机森林算法或简称为随机森林。
- en: A Random Forest algorithm encompasses decision tree-based supervised learning
    methods. It can be viewed as a composite whole comprising a large number of decision
    trees. In ML terminology, a Random Forest is an ensemble resulting from a profusion
    of decision trees.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法包含基于决策树的监督学习方法。它可以被视为由大量决策树组成的复合整体。在机器学习术语中，随机森林是由众多决策树组成的集成。
- en: A decision tree, as the name implies, is a progressive decision-making process,
    made up of a root node followed by successive subtrees. The decision tree algorithm
    snakes its way up the tree, stopping at every node, starting with the root node,
    to pose a do-you-belong-to-a-certain-category question. Depending on whether the
    answer is a yes or a no, a decision is made to travel up a certain branch until
    the next node is encountered, where the algorithm repeats its interrogation. Of
    course, at each node, the answer received by the algorithm determines the next
    branch to be on. The final outcome is a predicted outcome on a leaf that terminates.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，正如其名所暗示的，是一个渐进的决策过程，由一个根节点和随后的子树组成。决策树算法沿着树爬行，在每个节点停止，从根节点开始，提出一个“你是否属于某个类别”的问题。根据答案是否为是或否，做出决定沿着某个分支向上移动，直到遇到下一个节点，算法重复其询问。当然，在每个节点，算法收到的答案决定了下一个分支。最终的结果是在一个终止的叶节点上的预测结果。
- en: Speaking of trees, branches, and nodes, the dataset can be viewed as a tree
    made up of multiple subtrees. Each decision at a node of the dataset and the decision
    tree algorithm's choice of a certain branch is the result of an optimal composite
    of feature variables. Using a Random Forest algorithm, multiple decision trees
    are created. Each decision tree in this ensemble is the outcome of a randomized
    ordering of variables. That brings us to what random forests are—an ensemble of
    a multitude of decision trees.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 说到树、分支和节点，数据集可以看作是由多个子树组成的树。数据集节点上的每个决策以及决策树算法选择某个分支的决策，是特征变量最优组合的结果。使用随机森林算法，创建了多个决策树。这个集成中的每个决策树都是变量随机排序的结果。这带我们来到了随机森林是什么——它是众多决策树的集成。
- en: It is to be noted that one decision tree by itself cannot work well for a smaller
    sample like the Iris dataset. This is where the Random Forest algorithm steps
    in. It brings together or aggregates all of the predictions from its forest of
    decision trees. All of the aggregated results from individual decision trees in
    this forest would form one ensemble, better known as a Random Forest.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，单个决策树本身对于像鸢尾花数据集这样的较小样本可能工作得不好。这就是随机森林算法介入的地方。它将决策树森林中的所有预测汇集在一起。这个森林中所有单个决策树的汇总结果将形成一个集成，更广为人知的是随机森林。
- en: We chose the Random Forest method to make our predictions for a good reason.
    The net prediction formed out of an ensemble of predictions is significantly more
    accurate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择随机森林方法来做出预测，这有一个很好的理由。由预测集成的网络预测显著更准确。
- en: In the next section, we will formulate our classification problem, and in the
    *Getting started with Spark* section that follows, implementation details for
    the project are given.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将制定我们的分类问题，在随后的*Spark入门*部分，将给出项目的实现细节。
- en: Project overview – problem formulation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目概述 – 问题表述
- en: The intent of this project is to develop an ML workflow or more accurately a pipeline.
    The goal is to solve the classification problem on the most famous dataset in
    data science history.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目的是开发一个机器学习工作流程，或者更准确地说，是一个管道。目标是解决数据科学历史上最著名的分类问题。
- en: If we saw a flower out in the wild that we know belongs to one of three Iris
    species, we have a classification problem on our hands. If we made measurements
    (`X`) on the unknown flower, the task is to learn to recognize the species to
    which the flower (and its plant) belongs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在野外看到一朵我们知道属于三种鸢尾花物种之一的花，我们就面临一个分类问题。如果我们对未知的花进行了测量（`X`），任务就是学会识别这朵花（及其植物）所属的物种。
- en: Categorical variables represent types of data which may be divided into groups.
    Examples of categorical variables are race, sex, age group, and educational level.
    While the latter two variables may also be considered in a numerical manner by
    using exact values for age and highest grade completed, it is often more informative
    to categorize such variables into a relatively small number of groups.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 分类别变量代表可以分成组的数据类型。类别变量的例子有种族、性别、年龄组和教育水平。尽管后两个变量也可以通过使用年龄和最高完成等级的确切值以数值方式考虑，但通常将此类变量分类到相对较少的组中更有信息量。
- en: Analysis of categorical data generally involves the use of data tables. A two-way
    table presents categorical data by counting the number of observations that fall
    into each group for two variables, one divided into rows and the other divided
    into columns.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 常规的类别数据分析通常涉及使用数据表。一个双向表通过计数两个变量中每个组中观察值的数量来呈现类别数据，其中一个变量分为行，另一个变量分为列。
- en: 'In a nutshell, the high-level formulation of the classification problem is
    given as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，分类问题的概述如下：
- en: '![](img/5ba34af0-8b84-4465-b1e7-d4b7478c0f0e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ba34af0-8b84-4465-b1e7-d4b7478c0f0e.png)'
- en: High-level formulation of the Iris supervised learning classification problem
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 红 Iris 监督学习分类问题的概述
- en: In the Iris dataset, each row contains categorical data (values) in the fifth
    column. Each such value is associated with a label (**Y**).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Iris 数据集中，每一行包含第五列中的类别数据（值）。每个这样的值都与一个标签（**Y**）相关联。
- en: 'The formulation consists of the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 公式包括以下内容：
- en: Observed features
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察到的特征
- en: Category labels
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别标签
- en: Observed features are also known as **predictor variables**. Such variables
    have predetermined measured values. These are the inputs X. On the other hand,
    category labels denote possible output values that predicted variables can take.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到的特征也被称为**预测变量**。这些变量具有预定的测量值。这些是输入 X。另一方面，类别标签表示预测变量可以采取的可能输出值。
- en: 'The predictor variables are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 预测变量如下：
- en: '`sepal_length`: It represents sepal length, in centimeters, used as input'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sepal_length`：它代表花萼长度，以厘米为单位，用作输入'
- en: '`sepal_width`: It represents sepal width, in centimeters, used as input'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sepal_width`：它代表花萼宽度，以厘米为单位，用作输入'
- en: '`petal_length`: It represents petal length, in centimeters, used as input'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`petal_length`：它代表花瓣长度，以厘米为单位，用作输入'
- en: '`petal_width`: It represents petal width, in centimeters, used as input'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`petal_width`：它代表花瓣宽度，以厘米为单位，用作输入'
- en: '`setosa`: It represents Iris-setosa, true or false, used as target'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setosa`：它代表 Iris-setosa，真或假，用作目标'
- en: '`versicolour`: It represents Iris-versicolour, true or false, used as target'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`versicolour`：它代表 Iris-versicolour，真或假，用作目标'
- en: '`virginica`: It represents Iris-virginica, true or false, used as target'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`virginica`：它代表 Iris-virginica，真或假，用作目标'
- en: Four outcome variables were measured from each sample; the length and the width
    of the sepals and petals.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个样本测量了四个结果变量；花萼和花瓣的长度和宽度。
- en: The total build time of the project should be no more than a day in order to
    get everything working. For those new to the data science area, understanding
    the background theory, setting up the software, and getting to build the pipeline
    could take an extra day or two.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让一切正常工作，项目的总构建时间不应超过一天。对于数据科学领域的新手来说，理解背景理论、设置软件以及构建管道可能需要额外的一天或两天。
- en: Getting started with Spark
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Spark
- en: The instructions are for Windows users. Note that to run Spark Version 2 and
    above, Java Version 8 and above, Scala Version 2.11, **Simple Build Tool** (**SBT**)
    version that is at least 0.13.8 is a prerequisite. The code for the Iris project
    depends on Spark 2.3.1, the latest distribution at the time of writing this chapter.
    This version was released on December 1, 2017\. Implementations in subsequent
    chapters would likely be based on Spark 2.3.0, released February 28, 2017\. Spark
    2.3.0 is a major update version that comes with fixes to over 1,400 tickets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 指令适用于 Windows 用户。请注意，为了运行 Spark 版本 2 及以上版本，Java 版本 8 及以上版本，Scala 版本 2.11，**简单构建工具**（**SBT**）版本至少为
    0.13.8 是先决条件。Iris 项目的代码依赖于 Spark 2.3.1，这是撰写本章时的最新发行版。后续章节的实现可能基于 2017 年 2 月 28
    日发布的 Spark 2.3.0。Spark 2.3.0 是一个主要更新版本，其中包括对 1400 多个票证的修复。
- en: The Spark 2.0 brought with it a raft of improvements. The introduction of the
    dataframe as the fundamental abstraction of data is one such improvement. Readers
    will find that the dataframe abstraction and its supporting APIs enhance their
    data science and analysis tasks, not to mention this powerful feature's improved
    performance over **Resilient Distributed Datasets** (**RDDs**). Support for RDDs
    is very much available in the latest Spark release as well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 带来了一系列改进。将 dataframe 作为数据的基本抽象的引入就是其中之一。读者会发现 dataframe 抽象及其支持 API
    促进了他们的数据科学和分析任务，更不用说这个强大功能在 **弹性分布式数据集**（**RDDs**）上的性能改进了。在最新的 Spark 版本中，对 RDD
    的支持仍然非常丰富。
- en: Setting up prerequisite software
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置先决软件
- en: A note on hardware before jumping to prerequisites. The hardware infrastructure
    I use throughout in this chapter comprises of a 64-bit Windows Dell 8700 machine
    running Windows 10 with Intel(R) Core(TM) i7-4770 CPU @ 3.40 GHz and an installed
    memory of 32 GB.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳到先决条件之前，关于硬件的一些说明。我在本章中使用的硬件基础设施包括一台64位Windows Dell 8700机器，运行Windows 10，配备Intel(R)
    Core(TM) i7-4770 CPU @ 3.40 GHz和32GB的内存。
- en: In this subsection, we document three software prerequisites that must be in
    place before installing Spark.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们记录了在安装Spark之前必须准备的三种软件先决条件。
- en: At the time of this writing, my prerequisite software setup consisted of JDK
    8, Scala 2.11.12, and SBT 0.13.8, respectively. The following list is a minimal,
    recommended setup (note that you are free to try a higher JDK 8 version and Scala
    2.12.x).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我的先决软件设置包括JDK 8、Scala 2.11.12和SBT 0.13.8。以下列表是一个最小、推荐的设置（请注意，你可以尝试更高版本的JDK
    8和Scala 2.12.x）。
- en: 'Here is the required prerequisite list for this chapter:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本章所需的先决条件列表：
- en: Java SE Development Kit 8
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java SE开发工具包8
- en: Scala 2.11.12
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.12
- en: SBT 0.13.8 or above
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBT 0.13.8或更高版本
- en: If you are like me, dedicating an entire box with the sole ambition of evolving
    your own Spark big data ecosystem is not a bad idea. With that in mind, start
    with an appropriate machine (with ample space and at least 8 GB of memory), running
    your preferred OS, and install the preceding mentioned prerequisites listed in
    order. What about lower versions of the JDK, you may ask? Indeed, lower versions
    of the JDK are not compatible with Spark 2.3.1.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，只为了发展自己的Spark大数据生态系统而专门准备一个完整的箱子，这并不是一个坏主意。考虑到这一点，从一台合适的机器（有足够的空间和至少8GB的内存）开始，运行你偏好的操作系统，并按照顺序安装前面提到的先决条件。你可能会问，JDK的较低版本如何？确实，JDK的较低版本与Spark
    2.3.1不兼容。
- en: While I will not go into the JDK installation process here, here are a couple
    of notes. Download Java 8 ([http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html))
    and once the installer is done installing the `Java` folder, do not forget to
    set up two new system environment variables—the `JAVA_HOME` environment variable
    pointing to the root folder of your Java installation, and the `JAVA_HOME/bin`
    in your system path environment variable.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我不会在这里详细介绍JDK的安装过程，但这里有一些注意事项。下载Java 8（[http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)），一旦安装程序完成安装`Java`文件夹，不要忘记设置两个新的系统环境变量——`JAVA_HOME`环境变量指向你的Java安装根目录，以及`JAVA_HOME/bin`在你的系统路径环境变量中。
- en: 'After setting the system `JAVA_HOME` environment, here is how to do a quick
    sanity check by listing the value of `JAVA_HOME` on the command line:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置系统`JAVA_HOME`环境后，以下是如何通过在命令行上列出`JAVA_HOME`的值来快速进行sanity check的方法：
- en: '[PRE1]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now what remains is to do another quick check to be certain you installed the
    JDK flawlessly. Issue the following commands on your command line or Terminal.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是再进行一次快速检查，以确保你完美地安装了JDK。在你的命令行或终端中运行以下命令：
- en: 'Note that this screen only represents the Windows command line:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个屏幕只代表Windows命令行：
- en: '[PRE2]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point, if your sanity checks passed, the next step is to install Scala.
    The following brief steps outline that process. The Scala download page at [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris) documents
    many ways to install Scala (for different OS environments). However, we only list
    three methods to install Scala.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，如果你的 sanity checks通过了，下一步就是安装Scala。以下简要步骤概述了该过程。Scala下载页面在[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)上记录了许多安装Scala的方法（针对不同的操作系统环境）。然而，我们只列出了三种安装Scala的方法。
- en: Before diving into the Scala installation, a quick note here. While the latest
    stable version of Scala is 2.12.4, I prefer a slightly older version, version
    2.11.12, which is the version I will use in this chapter. You may download it
    at [http://scala-lang.org/download/2.11.12.html](http://scala-lang.org/download/2.11.12.html). Whether
    you prefer version 2.12 or 2.11, the choice is yours to make, as long as the version
    is not anything below 2.11.x. The following installation methods listed will get
    you started down that path.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入Scala安装之前，这里做一个简要说明。虽然Scala的最新稳定版本是2.12.4，但我更喜欢一个稍微旧一点的版本，即2.11.12，这是我在本章中将使用的版本。您可以从[http://scala-lang.org/download/2.11.12.html](http://scala-lang.org/download/2.11.12.html)下载它。无论您更喜欢2.12还是2.11版本，选择权在您手中，只要版本不是低于2.11.x的任何版本。以下列出的安装方法将帮助您开始这一过程。
- en: 'Scala can be installed through the following methods:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Scala可以通过以下方法进行安装：
- en: '**Install Scala**: Locate the section titled Other ways to install Scala at [http://scala-lang.org/download/](http://scala-lang.org/download/) and download
    the Scala binaries from there. Then you can install Scala by following the instructions
    at [http://scala-lang.org/download/install.html.](http://scala-lang.org/download/install.html) Install
    SBT from [https://www.scala-sbt.org/download.html](https://www.scala-sbt.org/download.html)
    and follow the setup instructions at [https://www.scala-sbt.org/1.0/docs/Setup.html.](https://www.scala-sbt.org/1.0/docs/Setup.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装Scala**：在[http://scala-lang.org/download/](http://scala-lang.org/download/)中找到标题为“其他安装Scala方法”的部分，并从那里下载Scala二进制文件。然后您可以根据[http://scala-lang.org/download/install.html](http://scala-lang.org/download/install.html)中的说明安装Scala。从[https://www.scala-sbt.org/download.html](https://www.scala-sbt.org/download.html)安装SBT，并按照[https://www.scala-sbt.org/1.0/docs/Setup.html](https://www.scala-sbt.org/1.0/docs/Setup.html)中的设置说明进行操作。'
- en: '**Scala in the IntelliJ IDE**: Instructions are given at [https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html.](https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在IntelliJ IDE中使用Scala**：有关说明请参阅[https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html](https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html)。'
- en: '**Scala in the IntelliJ IDE with SBT**: This is another handy way to play with
    Scala. Instructions are given at [https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html.](https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在IntelliJ IDE中使用SBT的Scala**：这是另一种方便地使用Scala的方法。有关说明请参阅[https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html](https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html)。'
- en: The acronym **SBT** that just appeared in the preceding list is short for **Simple
    Build Tool**. Indeed, you will run into references to SBT fairly often throughout
    this book.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面列表中刚刚出现的缩写**SBT**代表**Simple Build Tool**。确实，您会在本书的很多地方遇到对SBT的引用。
- en: Take up the item from the first method of the preceding list and work through
    the (mostly self-explanatory) instructions. Finally, if you forgot to set environment
    variables, do set up a brand new `SCALA_HOME` system environment variable (like
    `JAVA_HOME`), or simply update an existing `SCALA_HOME`. Naturally, the `SCALA_HOME/bin`
    entry is added to the path environment variable.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面列表的第一种方法中选取一个项目，并按照（主要自解释的）说明进行操作。最后，如果您忘记设置环境变量，请设置一个新的`SCALA_HOME`系统环境变量（类似于`JAVA_HOME`），或者简单地更新现有的`SCALA_HOME`。当然，`SCALA_HOME/bin`条目被添加到路径环境变量中。
- en: You do not necessarily need Scala installed system-wide. The SBT environment
    gives us access to its own Scala environment anyway. However, having a system-wide
    Scala installation allows you to quickly implement Scala code rather than spinning
    up an entire SBT project.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您不一定需要在系统范围内安装Scala。无论如何，SBT环境都为我们提供了访问其自己的Scala环境。然而，拥有系统范围内的Scala安装可以让您快速实现Scala代码，而不是启动整个SBT项目。
- en: Let us review what we have accomplished so far. We installed Scala by working
    through the first method Scala installation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止我们已经完成的事情。我们通过使用Scala安装的第一种方法安装了Scala。
- en: 'To confirm that we did install Scala, let''s run a basic test:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认我们已经安装了Scala，让我们运行一个基本的测试：
- en: '[PRE3]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code listing confirms that our most basic Scala installation
    went off without a hitch. This paves the way for a system-wide SBT installation.
    Once again, it comes down to setting up the `SBT_HOME` system environment variable
    and setting `$SBT_HOME/bin` in the path. This is the most fundamental bridge to
    cross. Next, let''s run a sanity check to verify that SBT is all set up. Open
    up a command-line window or Terminal. We installed SBT 0.13.17, as shown in the
    following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码列表确认我们的最基本Scala安装没有问题。这为系统范围内的SBT安装铺平了道路。再次强调，这归结为设置`SBT_HOME`系统环境变量，并将`$SBT_HOME/bin`设置在路径中。这是最基本的桥梁。接下来，让我们运行一个检查来验证SBT是否已正确设置。打开一个命令行窗口或终端。我们安装了SBT
    0.13.17，如下所示：
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are left with method two and method three. These are left as an exercise
    for the reader. Method three will let us take advantage of all the nice features
    that an IDE like IntelliJ has.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下的是第二种和第三种方法。这些方法留给读者作为练习。第三种方法将使我们能够利用IDE如IntelliJ的所有优秀功能。
- en: Shortly, the approach we will take in developing our pipeline involves taking
    an existing SBT project and importing it into IntelliJ, or we just create the
    SBT project in IntelliJ.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们在开发管道时采取的方法是，将现有的SBT项目导入IntelliJ，或者我们直接在IntelliJ中创建SBT项目。
- en: What's next? The Spark installation of course. Read all about it in the upcoming
    section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是什么？当然是Spark的安装。在即将到来的章节中详细了解。
- en: Installing Spark in standalone deploy mode
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以独立部署模式安装Spark
- en: In this section, we set up a Spark development environment in standalone deploy
    mode. To get started with Spark and start developing quickly, Spark's shell is
    the way to go.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们以独立部署模式设置Spark开发环境。要快速开始使用Spark并进行开发，Spark的shell是最佳选择。
- en: Spark supports Scala, Python, R, and Java with appropriate APIs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持Scala、Python、R和Java，并提供相应的API。
- en: 'The Spark binary download offers developers two components:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Spark二进制文件下载为开发者提供了两个组件：
- en: The Spark's shell
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的shell
- en: A standalone cluster
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个独立集群
- en: Once the binary is downloaded and extracted (instructions will follow), the
    Spark shell and standalone Scala application will let you spin up a standalone
    cluster in standalone cluster mode.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦下载并解压二进制文件（后续将提供说明），Spark shell和独立Scala应用程序将允许您在独立集群模式下启动一个独立集群。
- en: This cluster is self-contained and private because it is local to one machine.
    The Spark shell allows you to easily configure this standalone cluster. Not only
    does it give you quick access to an interactive Scala shell, but also lets you
    develop a Spark application that you can deploy into the cluster (lending it the
    name standalone deploy mode), right in the Scala shell.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个集群是自包含且私有的，因为它仅位于一台机器上。Spark shell允许您轻松配置这个独立集群。它不仅为您提供快速访问交互式Scala shell，还允许您在Scala
    shell中开发可以部署到集群中的应用程序（给它命名为独立部署模式），直接在Scala shell中进行。
- en: In this mode, the cluster's driver node and worker nodes reside on the same
    machine, not to mention the fact that our Spark application will take up all the
    cores available on that machine by default. The important feature of this mode
    that makes all this possible is the interactive (Spark) Scala shell.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，集群的驱动节点和工作节点位于同一台机器上，更不用说我们的Spark应用程序默认会占用该机器上所有可用的核心。使所有这一切成为可能的重要特性是交互式（Spark）Scala
    shell。
- en: Spark 2.3 is the latest version. It comes with over 1,400 fixes. A Spark 2.3
    installation on Java 8 might be the first thing to do before we get started on
    our next project in [Chapter 2](a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml), *Build
    a Breast Cancer Prognosis Pipeline with the Power of Spark and Scala.*
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.3是最新版本。它包含1400多个修复。在Java 8上安装Spark 2.3可能是我们在[第2章](a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml)“利用Spark和Scala的力量构建乳腺癌预后管道”开始下一个项目之前要做的第一件事。
- en: 'Without further ado, let''s get started setting up Spark in standalone deploy
    mode. The following sequence of instructions are helpful:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们开始设置以独立部署模式运行Spark。以下步骤很有帮助：
- en: 'System checks: First make sure you have at least 8 GB of memory, leaving at
    least 75% of this memory for Spark. Mine has 32 GB. Once the system checks pass,
    download the Spark 2.3.1 binary from here: [http://spark.apache.org/downloads.html.](http://spark.apache.org/downloads.html)'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统检查：首先确保您至少有8 GB的内存，并至少保留75%的内存供Spark使用。我的系统有32 GB。一旦系统检查通过，请从这里下载Spark 2.3.1的二进制文件：[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)。
- en: You will need a decompression utility capable of extracting the `.tar.gz` and `.gz` archives
    because Windows does not have native support for these archives. 7-Zip is a suitable
    program for this. You can obtain it from [http://7-zip.org/download.html](http://7-zip.org/download.html).
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要一个能够提取`.tar.gz`和`.gz`存档的解压缩工具，因为Windows对这些存档没有原生支持。7-Zip是适合这个任务的程序。你可以从[http://7-zip.org/download.html](http://7-zip.org/download.html)获取它。
- en: Choose the package type prebuilt for Apache Hadoop 2.7 and later and download
    `spark--2.2.1-bin-hadoop2.7.tgz`.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择为Apache Hadoop 2.7及以后版本预先构建的包类型，并下载`spark--2.2.1-bin-hadoop2.7.tgz`。
- en: 'Extract the package to someplace convenient, which will become your Spark root
    folder. For example, my Spark root folder is: `C:\spark-2.2.1-bin-hadoop2.7`.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将软件包解压到方便的地方，这将成为你的Spark根文件夹。例如，我的Spark根文件夹是：`C:\spark-2.2.1-bin-hadoop2.7`。
- en: Now, set up the environment variable, `SPARK_HOME` pointing to the Spark root
    folder. We would also need a path entry in the `PATH` variable to point to `SPARK_HOME/bin`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，设置环境变量`SPARK_HOME`指向Spark根文件夹。我们还需要在`PATH`变量中添加一个路径条目，指向`SPARK_HOME/bin`。
- en: 'Next, set up the environment variable, `HADOOP_HOME`, to, say, `C:\Hadoop`,
    and create a new path entry for Spark by pointing it to the `bin` folder of the
    Spark home directory. Now, launch `spark-shell` like this:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，设置环境变量`HADOOP_HOME`，例如设置为`C:\Hadoop`，并为Spark创建一个新的路径条目，指向Spark主目录的`bin`文件夹。现在，像这样启动`spark-shell`：
- en: '[PRE5]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'What happens next might frustrate Windows users. If you are one of those users,
    you will run into the following error. The following screenshot is a representation
    of this problem:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来发生的事情可能会让Windows用户感到沮丧。如果你是这些用户之一，你将遇到以下错误。以下截图是此问题的表示：
- en: '![](img/3efbec97-5ec2-46dc-b401-c0436cd0fbec.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3efbec97-5ec2-46dc-b401-c0436cd0fbec.png)'
- en: Error message on Windows
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Windows上的错误信息
- en: 'To get around this issue, you may proceed with the following steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可以按照以下步骤进行：
- en: Create a new folder as `C\tmp\hive`.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的文件夹，命名为`C\tmp\hive`。
- en: Then get the missing `WINUTILS.exe` binary from here: [https://github.com/steveloughran/winutils](https://github.com/steveloughran/winutils).
    Drop this into `C\Hadoop\bin`.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从这里获取缺失的`WINUTILS.exe`二进制文件：[https://github.com/steveloughran/winutils](https://github.com/steveloughran/winutils)。将其放入`C\Hadoop\bin`。
- en: The preceding step 2 is necessary because the Spark download does not contain
    the `WINUTILS.exe` that is required to run Hadoop. That, then, is the source of
    the `java.io.IOException`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤2是必要的，因为Spark下载不包含运行Hadoop所需的`WINUTILS.exe`。那么，这就是`java.io.IOException`的来源。
- en: 'With that knowledge, open up the Command Prompt window in administrator mode and
    execute the newly downloaded `WINUTILS.EXE` like this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理员模式下打开命令提示符窗口，并像这样执行新下载的`WINUTILS.EXE`：
- en: '[PRE6]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, issue the `spark-shell` command. This time around, Spark's interactive
    development environment launches normally, spinning up its own `SparkContext`
    instance `sc` and a `SparkSession` `spark` session, respectively. While the `sc` feature
    is a powerful entry point to the underlying local standalone cluster, `spark`
    is the main entry point to Spark's data processing APIs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，发出`spark-shell`命令。这一次，Spark的交互式开发环境正常启动，分别启动自己的`SparkContext`实例`sc`和`SparkSession`会话。虽然`sc`功能是访问底层本地独立集群的强大入口点，但`spark`是Spark数据处理API的主要入口点。
- en: 'The following is the output from the `spark-shell` command. `SparkContext` is
    made available to you as `sc` and the Spark session is available to you as `spark`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从`spark-shell`命令输出的内容。`SparkContext`作为`sc`提供给你，Spark会话作为`spark`提供给你：
- en: '[PRE7]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `local[2]` option in the `spark-shell` launch shown earlier lets us run
    Spark locally with `2` threads.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面显示的`spark-shell`启动中，`local[2]`选项让我们能够使用`2`个线程在本地运行Spark。
- en: 'Before diving into the next topic in this section, it is a good idea to understand
    the following Spark shell development environment features that make development
    and data analysis possible:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入本节下一个主题之前，了解以下Spark shell开发环境特性是很有好处的，这些特性使得开发和数据分析成为可能：
- en: '`SparkSession`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkSession`'
- en: '`SparkBuilder`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkBuilder`'
- en: '`SparkContext`'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkContext`'
- en: '`SparkConf`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkConf`'
- en: The `SparkSession` API ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession))
    describes `SparkSession` as a programmatic access entry point to Spark's dataset
    and dataframe APIs, respectively.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` API ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession))
    将 `SparkSession` 描述为程序访问入口点，分别用于 Spark 的数据集和 dataframe API。'
- en: 'What is `SparkBuilder`? The `SparkBuilder` companion object contains a `builder`
    method, which, when invoked, allows us to retrieve an existing `SparkSession`
    or even create one. We will now obtain our `SparkSession` instance in a two-step
    process, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 `SparkBuilder`？`SparkBuilder` 伴生对象包含一个 `builder` 方法，当调用它时，允许我们检索现有的 `SparkSession`
    或甚至创建一个。我们现在将按照以下两步过程获取我们的 `SparkSession` 实例：
- en: Import the `SparkSession` class.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `SparkSession` 类。
- en: 'Invoke the `builder` method with `getOrCreate` on the resulting `builder`:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成的 `builder` 上调用 `getOrCreate` 方法来调用 `builder` 方法：
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `SparkContext` API ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext))
    describes `SparkContext` as a first-line entry point for setting or configuring
    Spark cluster properties (RDDs, accumulators, broadcast variables, and much more)
    affecting the cluster's functionality. One way this configuration happens is by
    passing in a `SparkConf` instance as a `SparkContext` constructor parameter. One
    `SparkContext` exists per JVM instance.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext` API ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext))
    将 `SparkContext` 描述为设置或配置 Spark 集群属性（RDD、累加器、广播变量等等）的第一线入口点，这些属性影响集群的功能。这种配置发生的一种方式是通过将
    `SparkConf` 实例作为 `SparkContext` 构造函数参数传递。每个 JVM 实例存在一个 `SparkContext`。'
- en: In a sense, `SparkContext` is also how a Spark driver application connects to
    a cluster through, for example, Hadoop's Yarn **ResourceManager** (**RM**).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，`SparkContext` 也是 Spark 驱动应用程序通过例如 Hadoop 的 Yarn **ResourceManager**
    (**RM**) 连接到集群的方式。
- en: 'Let''s inspect our Spark environment now. We will start by launching the Spark
    shell. That said, a typical Spark shell interactive environment screen has its
    own SparkSession available as  `spark`,   whose value we try to read off in the
    code block as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查我们的 Spark 环境。我们将首先启动 Spark shell。也就是说，典型的 Spark shell 交互式环境屏幕有一个可用的 SparkSession
    作为 `spark`，我们试图在以下代码块中读取其值：
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The Spark shell also boasts of its own `SparkContext` instance `sc`, which
    is associated with `SparkSession` `spark`. In the following code, `sc` returns `SparkContext`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell 也拥有自己的 `SparkContext` 实例 `sc`，它与 `SparkSession` `spark` 相关联。在下面的代码中，`sc`
    返回 `SparkContext`：
- en: '[PRE10]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`sc` can do more. In the following code, invoking the `version` method on `sc` gives
    us the version of Spark running in our cluster:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`sc` 可以做更多。在下面的代码中，在 `sc` 上调用 `version` 方法会给我们显示我们集群中运行的 Spark 的版本：'
- en: '[PRE11]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since `sc` represents a connection to the Spark cluster, it holds a special
    object called `SparkConf`, holding cluster configuration properties in an `Array`. Invoking
    the `getConf` method on the `SparkContext` yields `SparkConf`, whose `getAll`
    method (shown as follows) yields an `Array` of cluster (or connection) properties,
    as shown in the following code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `sc` 代表与 Spark 集群的连接，它包含一个称为 `SparkConf` 的特殊对象，该对象以 `Array` 形式持有集群配置属性。在
    `SparkContext` 上调用 `getConf` 方法会得到 `SparkConf`，其 `getAll` 方法（如下所示）会得到一个包含集群（或连接）属性的
    `Array`，如下面的代码所示：
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There may be references to `sqlContext` and `sqlContext.implicits._` in the
    Spark shell. What is `sqlContext`? As of Spark 2 and the preceding versions, `sqlContext`
    is deprecated and `SparkSession.builder` is used instead to return a `SparkSession`
    instance, which we reiterate is the entry point to programming Spark with the
    dataset and dataframe API. Hence, we are going to ignore those `sqlContext` instances
    and focus on `SparkSession` instead.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark shell 中可能有对 `sqlContext` 和 `sqlContext.implicits._` 的引用。`sqlContext`
    是什么？截至 Spark 2 及其之前的版本，`sqlContext` 已被弃用，`SparkSession.builder` 被用来返回一个 `SparkSession`
    实例，我们重申这是使用数据集和 dataframe API 编程 Spark 的入口点。因此，我们将忽略那些 `sqlContext` 实例，而专注于 `SparkSession`。
- en: 'Note that `spark.app.name` bears the default name `spark-shell`. Let''s assign
    a different name to the `app-name` property as `Iris-Pipeline`. We do this by
    invoking the `setAppName` method and passing to it the new app name, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`spark.app.name`默认名称为`spark-shell`。让我们将`app-name`属性赋予一个不同的名称，即`Iris-Pipeline`。我们通过调用`setAppName`方法并传递新的应用程序名称来实现这一点，如下所示：
- en: '[PRE13]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To check if the configuration change took effect, let''s invoke the `getAll`
    method again. The following output should reflect that change. It simply illustrates
    how `SparkContext` can be used to modify our cluster environment:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查配置更改是否生效，让我们再次调用`getAll`方法。以下输出应反映这一更改。它简单地说明了如何使用`SparkContext`来修改我们的集群环境：
- en: '[PRE14]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `spark.app.name` property just had its value updated to the new name. Our
    goal in the next section is to use `spark-shell` to analyze data in an interactive
    fashion.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.app.name`属性刚刚更新了其值。在下一节中，我们的目标是使用`spark-shell`以交互式方式分析数据。'
- en: Developing a simple interactive data analysis utility
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个简单的交互式数据分析工具
- en: We will develop a simple Scala program in the Spark shell's interactive Scala
    shell. We will restate our goal, which is that we want to be able to analyze data
    interactively. That dataset—an external **comma-separated values** (**CSV**) file
    called `iris.csv`—resides in the same folder where `spark-shell` is launched from.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Spark壳的交互式Scala壳中开发一个简单的Scala程序。我们将重申我们的目标，即我们希望能够交互式地分析数据。这个数据集——一个名为`iris.csv`的外部**逗号分隔值(CSV**)文件——位于从`spark-shell`启动的同一文件夹中。
- en: 'This program, which could just as well be written in a regular Scala **Read
    Eval Print Loop **(**REPL**) shell, reads a file, and prints out its contents,
    getting a data analysis task done. However, what is important here is that the
    Spark shell is flexible in that it also allows you to write Scala code that will
    allow you to easily connect your data with various Spark APIs and derive abstractions,
    such as dataframes or RDDs, in some useful way. More about `DataFrame` and `Dataset`
    to follow:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序，也可以在常规Scala **Read Eval Print Loop (REPL**)壳中编写，读取文件并打印其内容，完成数据分析任务。然而，这里重要的是Spark壳的灵活性，它还允许你编写Scala代码，以便你能够轻松地将数据与各种Spark
    API连接，并以某种有用的方式推导出抽象，如dataframes或RDDs。关于`DataFrame`和`Dataset`的更多内容将在后面介绍：
- en: '![](img/b8fa8a32-975c-4def-9bca-edae287ad5f0.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8fa8a32-975c-4def-9bca-edae287ad5f0.jpg)'
- en: Reading iris.csv with source
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用source读取iris.csv
- en: In the preceding program, nothing fancy is happening. We are trying to read
    a file called `iris.csv` using the `Source` class. We import the `Source.scala`
    file from the `scala.io` package and from there on, we create an object called
    `DataReader` and a `main` method inside it. Inside the `main` method, we invoke
    the `fromFile` method of the companion object `Source`. The `fromFile` method
    takes in a string representation of the dataset file path as an argument and returns
    a `BufferedSource` instance, which we assign to a `val` that we name `datasrc`.
    By the way, the API for `Source` can be found at [https://www.scala-lang.org/api/current/scala/io/Source.html.](https://www.scala-lang.org/api/current/scala/io/Source.html)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的程序中，没有发生什么特别的事情。我们正在尝试使用`Source`类读取名为`iris.csv`的文件。我们从`scala.io`包中导入`Source.scala`文件，然后创建一个名为`DataReader`的对象和其内部的`main`方法。在`main`方法内部，我们调用伴随对象`Source`的`fromFile`方法。`fromFile`方法接受数据集文件路径的字符串表示作为参数，并返回一个`BufferedSource`实例，我们将它分配给一个名为`datasrc`的`val`。顺便说一下，`Source`的API可以在[https://www.scala-lang.org/api/current/scala/io/Source.html](https://www.scala-lang.org/api/current/scala/io/Source.html)找到。
- en: On the `BufferedSource` handle, we then invoke the `getLines` method that returns
    an iterator, which in turn invokes `foreach` that will print out all the lines
    in `iris.csv` minus the newline characters. We wrap all of this code in a `try`
    and a `catch` and a `finally`. The `finally` construct exists for a reason and
    that has to do with the fact that we need to close the `BufferedSource` instance
    `datasrc` after it is done working on the file.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在`BufferedSource`处理句柄上，我们随后调用了`getLines`方法，该方法返回一个迭代器，然后调用`foreach`，它会打印出`iris.csv`中的所有行，但不包括换行符。我们将所有这些代码包裹在`try`、`catch`和`finally`中。`finally`构造存在的原因与我们需要在文件处理完毕后关闭`BufferedSource`实例`datasrc`有关。
- en: Initially, we ran into a `FileNotFoundException` because the dataset file `iris.csv`
    was not found. The CSV file is then dropped in, the program is run, and the output
    is what we expect.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，我们遇到了`FileNotFoundException`异常，因为数据集文件`iris.csv`没有找到。然后将CSV文件放入，运行程序，输出结果就是我们预期的。
- en: That wasn't so hard. In the next subsection, the goal is to read our `iris.csv`
    file and derive `Dataset` or `DataFrame` out of it.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不难。在下一个子节中，目标是读取我们的`iris.csv`文件，并从中派生出`Dataset`或`DataFrame`。
- en: Reading a data file and deriving DataFrame out of it
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取数据文件并从中派生出DataFrame
- en: The Spark API for [https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset) has
    it that a `DataFrame` is `Dataset[Row]` and that `Dataset` contains a view called
    `DataFrame`. Falling back to the description of `Dataset` in the Spark documentation,
    we can redefine `Dataset` as a Spark abstraction of distributed collections holding
    data items. That said, `Dataset[Row]` contains rows. `Row` could be an abstraction
    representing a row from the raw file dataset.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API对于[https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset)的说明是，`DataFrame`是`Dataset[Row]`，而`Dataset`包含一个名为`DataFrame`的视图。根据Spark文档中对`Dataset`的描述，我们可以将`Dataset`重新定义为Spark对分布式集合的抽象，这些集合包含数据项。也就是说，`Dataset[Row]`包含行。`Row`可能是一个抽象，表示来自原始文件数据集的行。
- en: We need to read the `iris.csv` file and transform it into `DataFrame`. That
    is the stated goal of this subsection and that is exactly what we shall accomplish
    very soon.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要读取`iris.csv`文件并将其转换为`DataFrame`。这就是本小节的目标，我们很快就会实现这一点。
- en: 'With all this in mind, lets get down to building `DataFrame`. We start by invoking
    the `read` method on `spark`, our `SparkSession`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，让我们开始构建`DataFrame`。我们首先在`spark`，我们的`SparkSession`上调用`read`方法：
- en: '[PRE15]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `read()` invoke produced `DataFrameReader` `dfReader1`, which according
    to [https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader) is
    an interface to load a dataset from external storage systems.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`read()`调用生成了`DataFrameReader` `dfReader1`，根据[https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader)的说明，这是一个从外部存储系统中加载数据集的接口。'
- en: 'Next, we will inform Spark that our data is in CSV format. This is done by
    invoking the `format` method with a `com.databricks.spark.csv` argument that Spark
    recognizes:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通知Spark我们的数据是CSV格式。这是通过调用带有`com.databricks.spark.csv`参数的`format`方法来完成的，Spark可以识别这个参数：
- en: '[PRE16]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `format` method simply returned `DataFrameReader` again. The `iris.csv`
    file contains `header`. We could specify this as an input `option`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`format`方法只是再次返回了`DataFrameReader`。`iris.csv`文件包含`header`。我们可以将其指定为输入`option`：'
- en: '[PRE17]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: That returned our same old `DataFrameReader`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了我们熟悉的`DataFrameReader`。
- en: 'What we need next is a way to identify the schema for us. Invoking the `option`
    method again with a key `inferSchema` and a value of `true` lets Spark infer the
    schema automatically for us:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种方法来识别为我们识别的模式。再次调用`option`方法，使用键`inferSchema`和值为`true`，让Spark自动为我们推断模式：
- en: '[PRE18]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s `load` our input now:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来`加载`我们的输入：
- en: '[PRE19]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`DataFrameReader` transformed our input CSV into `DataFrame`! This was exactly
    what we set out to do.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrameReader`将我们的输入CSV转换成了`DataFrame`！这正是我们一开始设定的目标。'
- en: '`DataFrame` is simply an untyped view of `Dataset` as `type DataFrame = Dataset[Row]`.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`简单地说就是`Dataset`的无类型视图，表示为`type DataFrame = Dataset[Row]`。'
- en: With our `DataFrame` being a view on `Dataset[Row]`, all the methods on `Dataset`
    are available.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的`DataFrame`是`Dataset[Row]`的视图，所以`Dataset`上的所有方法都是可用的。
- en: 'For now, we want to see what this dataset has in it. The raw file had 150 columns
    in it. Therefore, we want Spark to:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们想看看这个数据集中有什么。原始文件中有150列。因此，我们希望Spark：
- en: Return the row count in our dataset
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回数据集中的行数
- en: Display the top 20 rows of our dataset
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示数据集的前20行
- en: 'Next, we will invoke the `count` method. We want to reaffirm the number of
    rows contained in the dataset:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调用`count`方法。我们想要再次确认数据集中包含的行数：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We just invoked the `count` method on our `DataFrame`. That returned the number
    `150`, which is right.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在我们的`DataFrame`上调用了`count`方法。它返回了数字`150`，这是正确的。
- en: 'Next, we will bring together all of the code developed in this section into
    one line of code:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将本节中开发的全部代码合并成一行代码：
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We just created `DataFrame` `irisDataFrame` . If you want to view the DataFrame, 
    just invoke the `show` method on it. This will return the first 20 rows of the
    irisDataFrame  `DataFrame`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建了`DataFrame` `irisDataFrame`。如果您想查看DataFrame，只需在它上面调用`show`方法。这将返回irisDataFrame的前20行`DataFrame`：
- en: '![](img/2667d94e-9b3a-41bd-802c-c9719b96c6df.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2667d94e-9b3a-41bd-802c-c9719b96c6df.jpg)'
- en: First 20 rows of the Iris dataset
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集的前 20 行
- en: At this point, type `:quit` or *Ctrl* + *D* to exit the Spark shell. This wraps
    up this section, but opens a segue to the next, where we take things to the next
    level. Instead of relying on `spark-shell` to develop a larger program, we will
    create our Iris prediction pipeline program in an SBT project. This is the focus
    of the next section.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，输入 `:quit` 或 *Ctrl* + *D* 以退出 Spark shell。这总结了本节内容，但为下一节打开了过渡，我们将把事情提升到下一个层次。我们不会依赖于
    `spark-shell` 来开发更大的程序，而是将在 SBT 项目中创建我们的 Iris 预测管道程序。这是下一节的重点。
- en: Implementing the Iris pipeline
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Iris 管道
- en: In this section, we will set forth what our pipeline implementation objectives
    are. We will document tangible results as we step through individual implementation
    steps.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将阐述我们的管道实现目标。我们将随着每个实现步骤的进行记录可衡量的结果。
- en: Before we implement the Iris pipeline, we want to understand what a pipeline
    is from a conceptual and practical perspective. Therefore, we define a pipeline
    as a `DataFrame` processing workflow with multiple pipeline stages operating in
    a certain sequence.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现 Iris 管道之前，我们想要从概念和实践的角度理解管道是什么。因此，我们将管道定义为具有多个管道阶段以一定顺序运行的 `DataFrame`
    处理工作流程。
- en: A DataFrame is a Spark abstraction that provides an API. This API lets us work
    with collections of objects.  At a high-level it represents a distributed collection
    holding rows of data, much like a relational database table. Each member of a
    row (for example, a Sepal-Width measurement) in this DataFrame falls under a named
    column called Sepal-Width.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是 Spark 的一种抽象，它提供了一个 API。这个 API 允许我们处理对象集合。从高层次来看，它代表了一个分布式集合，包含数据行，类似于关系数据库表。在这个
    DataFrame 中，每一行成员（例如，花瓣宽度测量值）都隶属于一个名为花瓣宽度的命名列。
- en: 'Each stage in a pipeline is an algorithm that is either a  `Transformer` or
    an  `Estimator`.   As a `DataFrame` or DataFrame(s) flow through the pipeline, 
    two types of  stages (algorithms) exist:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 管道中的每个阶段都是一个算法，它要么是 `Transformer`，要么是 `Estimator`。当 `DataFrame` 或 DataFrame(s)
    流经管道时，存在两种类型的阶段（算法）：
- en: '`Transformer` stage:  This involves a transformation action that transforms
    one `DataFrame` into another `DataFrame`'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transformer` 阶段：这涉及一个转换动作，将一个 `DataFrame` 转换为另一个 `DataFrame`'
- en: '`Estimator` stage: This involves a training action on a `DataFrame` that produces
    another `DataFrame`.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Estimator` 阶段：这涉及在 `DataFrame` 上执行训练动作，产生另一个 `DataFrame`。'
- en: 'In summary, a pipeline is a single unit, requiring stages, but inclusive of
    parameters and DataFrame(s). The entire pipeline structure is listed as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，管道是一个单一单元，需要阶段，但包括参数和 DataFrame(s)。整个管道结构如下所示：
- en: '`Transformer`'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transformer`'
- en: '`Estimator`'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Estimator`'
- en: '`Parameters` (hyper or otherwise)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Parameters`（超参数或其他）'
- en: '`DataFrame`'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataFrame`'
- en: This is where Spark comes in. Its MLlib library provides a set of pipeline APIs
    allowing developers to access multiple algorithms and facilitates their combining
    into a single pipeline of ordered stages, much like a sequence of choreographed
    motions in a ballet. In this chapter, we will use the random forest classifier.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 Spark 发挥作用的地方。它的 MLlib 库提供了一套管道 API，允许开发者访问多个算法，并促进它们组合成一个有序阶段的单一管道，就像芭蕾舞中的编排动作序列。在本章中，我们将使用随机森林分类器。
- en: We covered essential pipeline concepts. These are practicalities that will help
    us move into the section, where we will list implementation objectives.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了管道的基本概念。这些实用性将帮助我们进入下一部分，我们将列出实现目标。
- en: Iris pipeline implementation objectives
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Iris 管道实现目标
- en: Before listing the implementation objectives, we will lay out an architecture
    for our pipeline. Shown here under are two diagrams representing an ML workflow,
    a pipeline.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在列出实现目标之前，我们将为我们的管道制定一个架构。下面展示的是两个表示机器学习工作流程（管道）的图表。
- en: 'The following diagrams together help in understanding the different components
    of this project. That said, this pipeline involves training (fitting), transformation,
    and validation operations. More than one model is trained and the best model (or
    mapping function) is selected to give us an accurate approximation predicting the
    species of an Iris flower (based on measurements of those flowers):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表共同帮助理解这个项目的不同组件。话虽如此，这个管道涉及训练（拟合）、转换和验证操作。训练了多个模型，并选择最佳模型（或映射函数）以提供准确预测
    Iris 花种类的近似值（基于这些花的测量值）：
- en: '![](img/0ea4c627-29bd-46c6-b668-41beeb8136f5.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ea4c627-29bd-46c6-b668-41beeb8136f5.png)'
- en: Project block diagram
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 项目框图
- en: 'A breakdown of the project block diagram is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 项目框图分解如下：
- en: '**Spark**, which represents the Spark cluster and its ecosystem'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark**，代表Spark集群及其生态系统'
- en: '**Training dataset**'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集**'
- en: '**Model**'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**'
- en: '**Dataset attributes** or feature measurements'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集属性**或特征测量'
- en: An **inference** process, that produces a prediction column
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**推理**过程，生成预测列
- en: The following diagram represents a more detailed description of the different
    phases in terms of the functions performed in each phase. Later we will come to
    visualize pipeline in terms of its constituent stages.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 下图详细描述了不同阶段的功能，我们将稍后通过其构成阶段来可视化管道：
- en: 'For now, the diagram depicts four stages, starting with a **data pre-processing**
    phase, which is considered separate from the numbered phases deliberately. Think
    of the pipeline as a two-step process:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，该图描绘了四个阶段，从**数据预处理**阶段开始，这个阶段被故意视为与编号阶段分开。将管道视为两步过程：
- en: A **data cleansing** phase, or **pre-processing** phase. An important phase
    that could include a subphase of **Exploratory Data Analysis** (**EDA**) (not
    explicitly depicted in the latter diagram).
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据清洗**阶段或**预处理**阶段。一个重要的阶段，可能包括**探索性数据分析**（EDA）的子阶段（在后面的图中没有明确表示）。'
- en: 'A data analysis phase that begins with **Feature Extraction**, followed by
    **Model Fitting**, and **Model validation**, all the way to deployment of an Uber
    pipeline JAR into Spark:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个从**特征提取**开始的数据分析阶段，接着是**模型拟合**，然后是**模型验证**，最后将Uber管道JAR部署到Spark中：
- en: '![](img/521c0e06-1391-4a31-9d5b-e5d9de1e7f9a.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/521c0e06-1391-4a31-9d5b-e5d9de1e7f9a.png)'
- en: Pipeline diagram
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 管道图
- en: Referring to the preceding diagram, the first implementation objective is to
    set up Spark inside an SBT project. An SBT project is a self-contained application,
    which we can run on the command line to predict Iris labels. In the SBT project, 
    dependencies are specified in a `build.sbt` file and our application code will
    create its  own  `SparkSession` and `SparkContext`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 参考前面的图，第一个实现目标是设置Spark在SBT项目中。SBT项目是一个自包含的应用程序，我们可以在命令行上运行它来预测Iris标签。在SBT项目中，依赖关系在`build.sbt`文件中指定，并且我们的应用程序代码将创建自己的`SparkSession`和`SparkContext`。
- en: 'So that brings us to a listing of implementation objectives and these are as
    follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们列出了以下实现目标：
- en: Get the Iris dataset from the UCI Machine Learning Repository
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从UCI机器学习仓库获取Iris数据集
- en: Conduct preliminary EDA in the Spark shell
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark shell中进行初步EDA
- en: Create a new Scala project in IntelliJ, and carry out all implementation steps,
    until the evaluation of the Random Forest classifier
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在IntelliJ中创建一个新的Scala项目，并执行所有实现步骤，直到评估随机森林分类器
- en: Deploy the application to your local Spark cluster
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将应用程序部署到您的本地Spark集群
- en: Step 1 – getting the Iris dataset from the UCI Machine Learning Repository
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1步 – 从UCI机器学习仓库获取Iris数据集
- en: Head over to the UCI Machine Learning Repository website at [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)
    and click on Download: Data Folder. Extract this folder someplace convenient and
    copy over `iris.csv` into the root of your project folder.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 访问UCI机器学习仓库网站[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)，点击下载：数据文件夹。将此文件夹提取到方便的位置，并将`iris.csv`复制到项目文件夹的根目录。
- en: 'You may refer back to the project overview for an in-depth description of the
    Iris dataset. We depict the contents of the `iris.csv` file here, as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考项目概述以深入了解Iris数据集的描述。我们在此展示`iris.csv`文件的内容，如下所示：
- en: '![](img/71fbb790-c523-461f-b89a-951043a47d3b.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/71fbb790-c523-461f-b89a-951043a47d3b.jpg)'
- en: A snapshot of the Iris dataset with 150 sets
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Iris数据集的快照，包含150个数据集
- en: You may recall that the `iris.csv` file is a 150-row file, with comma-separated
    values.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得`iris.csv`文件是一个150行的文件，包含逗号分隔的值。
- en: Now that we have the dataset, the first step will be performing EDA on it. The
    Iris dataset is multivariate, meaning there is more than one (independent) variable,
    so we will carry out a basic multivariate EDA on it. But we need `DataFrame` to
    let us do that. How we create a dataframe as a prelude to EDA is the goal of the
    next section.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集，第一步将是对其进行EDA。Iris数据集是多变量的，这意味着有多个（独立）变量，因此我们将对其进行基本的多元EDA。但我们需要`DataFrame`来实现这一点。如何在EDA之前创建DataFrame是下一节的目标。
- en: Step 2 – preliminary EDA
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步 – 初步EDA
- en: Before we get down to building the SBT pipeline project, we will conduct a preliminary
    EDA in `spark-shell`. The plan is to derive a dataframe out of the dataset and
    then calculate basic statistics on it.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们着手构建SBT管道项目之前，我们将在`spark-shell`中进行初步的EDA。计划是从数据集中导出一个dataframe，然后对其计算基本统计数据。
- en: 'We have three tasks at hand for `spark-shell`:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手头有三个`spark-shell`任务：
- en: Fire up `spark-shell`
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`spark-shell`
- en: Load the `iris.csv` file and build `DataFrame`
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`iris.csv`文件并构建`DataFrame`
- en: Calculate the statistics
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算统计数据
- en: We will then port that code over to a Scala file inside our SBT project.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这段代码移植到我们的SBT项目中的一个Scala文件中。
- en: That said, let's get down to loading the `iris.csv` file (inputting the data
    source) before eventually building `DataFrame`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们开始加载`iris.csv`文件（输入数据源），在最终构建`DataFrame`之前。
- en: Firing up Spark shell
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动Spark shell
- en: Fire up the Spark Shell by issuing the following command on the command line.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在命令行中输入以下命令来启动Spark Shell。
- en: '[PRE22]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the next step, we start with the available Spark session 'spark'.  'spark'
    will be our entry point to programming with Spark. It also holds properties required
    to connect to our Spark (local) cluster. With this information, our next goal
    is to load the iris.csv file and produce a DataFrame
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们从可用的Spark会话`spark`开始。`spark`将是我们的Spark编程的入口点。它还包含连接到我们的Spark（本地）集群所需的属性。有了这些信息，我们的下一个目标是加载iris.csv文件并生成一个DataFrame。
- en: Loading the iris.csv file and building a DataFrame
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载iris.csv文件并构建DataFrame
- en: 'The first step to loading the iris csv file is to invoke the `read` method
    on `spark`. The `read` method returns `DataFrameReader`, which can be used to
    read our dataset:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 加载iris csv文件的第一步是在`spark`上调用`read`方法。`read`方法返回`DataFrameReader`，可以用来读取我们的数据集：
- en: '[PRE23]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`dfReader1` is of type `org.apache.spark.sql.DataFrameReader`. Calling the
    `format` method on `dfReader1` with Spark''s `com.databricks.spark.csv` CSV format-specifier
    string returns `DataFrameReader` again:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`dfReader1`是`org.apache.spark.sql.DataFrameReader`类型。在`dfReader1`上调用Spark的`com.databricks.spark.csv`
    CSV格式指定字符串的`format`方法会再次返回`DataFrameReader`：'
- en: '[PRE24]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After all, `iris.csv` is a CSV file.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，`iris.csv`是一个CSV文件。
- en: Needless to say, `dfReader1` and `dfReader2` are the same `DataFrameReader`
    instance.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 不言而喻，`dfReader1`和`dfReader2`是同一个`DataFrameReader`实例。
- en: 'At this point, `DataFrameReader` needs an input data source `option` in the
    form of a key-value pair. Invoke the `option` method with two arguments, a key
    `"header"` of type string and its value `true` of type Boolean:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，`DataFrameReader`需要一个以键值对形式存在的输入数据源`option`。使用两个参数调用`option`方法，一个字符串类型的键`"header"`和其布尔类型的值`true`：
- en: '[PRE25]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the next step, we invoke the `option` method again with an argument `inferSchema`
    and a `true` value:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们再次使用参数`inferSchema`和`true`值调用`option`方法：
- en: '[PRE26]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: What is `inferSchema` doing here? We are simply telling Spark to guess the schema
    of our input data source for us.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`inferSchema`在这里做什么？我们只是简单地告诉Spark为我们猜测输入数据源的架构。'
- en: Up until now, we have been preparing `DataFrameReader` to load `iris.csv`. External
    data sources require a path for Spark to load the data for `DataFrameReader` to
    process and spit out `DataFrame`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在准备`DataFrameReader`以加载`iris.csv`。外部数据源需要为Spark提供一个路径，以便`DataFrameReader`可以加载数据并输出`DataFrame`。
- en: 'The time is now right to invoke the `load` method on `DataFrameReader` `dfReader4`.
    Pass into the `load` method the path to the Iris dataset file. In this case, the
    file is right under the root of the project folder:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候在`DataFrameReader` `dfReader4`上调用`load`方法了。将Iris数据集文件的路径传递给`load`方法。在这种情况下，文件位于项目文件夹的根目录下：
- en: '[PRE27]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: That's it. We now have `DataFrame`!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们现在有了`DataFrame`！
- en: Calculating statistics
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算统计数据
- en: 'Invoking the `describe` method on this `DataFrame` should cause Spark to perform
    a basic statistical analysis on each column of `DataFrame`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`DataFrame`上调用`describe`方法应该会导致Spark对`DataFrame`的每一列执行基本统计分析：
- en: '[PRE28]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Lets fix the `WARN.Utils` issue described in the preceding code block. The fix
    is to locate the file `spark-defaults-template.sh` under `SPARK_HOME/conf` and
    save it as `spark-defaults.sh`.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修复前面代码块中描述的`WARN.Utils`问题。修复方法是找到位于`SPARK_HOME/conf`下的文件`spark-defaults-template.sh`并将其保存为`spark-defaults.sh`。
- en: 'At the bottom of this file, add an entry for `spark.debug.maxToStringFields`.
    The following screenshot illustrates this:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在此文件的底部添加一个条目`spark.debug.maxToStringFields`。以下截图说明了这一点：
- en: '![](img/46d332b7-6858-429d-a493-bbb6b451cb28.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/46d332b7-6858-429d-a493-bbb6b451cb28.jpg)'
- en: Fixing the WARN Utils problem in spark-defaults.sh
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在`spark-defaults.sh`中修复WARN Utils问题
- en: Save the file and restart `spark-shell`.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 保存文件并重新启动`spark-shell`。
- en: Now, inspect the updated Spark configuration again. We updated the value of
    `spark.debug.maxToStringFields` in the `spark-defaults.sh` file. This change is
    supposed to fix the truncation problem reported by Spark. We will confirm imminently
    that the change we made caused Spark to update its configuration also. That is
    easily done by inspecting `SparkConf`.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，再次检查更新的 Spark 配置。我们在 `spark-defaults.sh` 文件中更新了 `spark.debug.maxToStringFields`
    的值。这个更改旨在解决 Spark 报告的截断问题。我们将立即确认我们所做的更改导致 Spark 也更新了其配置。这很容易通过检查 `SparkConf`
    来完成。
- en: Inspecting your SparkConf again
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 再次检查 SparkConf
- en: 'As before, invoking the `getConf` returns the `SparkContext` instance that
    stores configuration values. Invoking `getAll` on that instance returns an `Array`
    of configuration values. One of those values is an updated value of `spark.debug.maxToStringFields`:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，调用 `getConf` 返回存储配置值的 `SparkContext` 实例。在该实例上调用 `getAll` 返回一个配置值的 `Array`。其中之一是
    `spark.debug.maxToStringFields` 的更新值：
- en: '[PRE29]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: That updated value for `spark.debug.maxToStringFields` is now `150`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.debug.maxToStringFields` 的更新值现在是 `150`。'
- en: '`spark.debug.maxToStringFields` had a default value of `25` inside a private
    object called `Utils`.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在名为 `Utils` 的私有对象内部，`spark.debug.maxToStringFields` 有一个默认值 `25`。
- en: Calculating statistics again
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 再次计算统计数据
- en: 'Run the invoke on the dataframe `describe` method and pass to it column names:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在 dataframe 的 `describe` 方法上运行 invoke 并传递列名：
- en: '[PRE30]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The invoke on the `describe` method of `DataFrame` `dfReader` results in a
    transformed `DataFrame` that we call dFrame2.  On dFrame2, we invoke the `show`
    method to return a table of statistical results. This completes the first phase
    of a basic yet important EDA:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `DataFrame` `dfReader` 的 `describe` 方法上调用 invoke 会得到一个我们称之为 dFrame2 的转换后的
    `DataFrame`。在 dFrame2 上，我们调用 `show` 方法以返回一个统计结果表。这完成了基本但重要的 EDA 的第一阶段：
- en: '[PRE31]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The results of the statistical analysis are shown in the following screenshot:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 统计分析的结果显示在下述截图：
- en: '![](img/44bb7dee-e4c6-4dde-8476-ccc52bbd88ca.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/44bb7dee-e4c6-4dde-8476-ccc52bbd88ca.jpg)'
- en: Results of statistical analysis
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 统计分析结果
- en: 'We did all that extra work simply to demonstrate the individual data reading,
    loading, and transformation stages. Next, we will wrap all of our previous work
    in one line of code:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做了所有这些额外的工作只是为了展示单独的数据读取、加载和转换阶段。接下来，我们将所有之前的工作封装在一行代码中：
- en: '[PRE32]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That completes the EDA on `spark-shell`. In the next section, we undertake steps
    to implement, build (using SBT), deploy (using `spark-submit`), and execute our
    Spark pipeline application. We start by creating a skeletal SBT project.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了在 `spark-shell` 上的 EDA。在下一节中，我们将采取实施、构建（使用 SBT）、部署（使用 `spark-submit`）和执行我们的
    Spark 管道应用程序的步骤。我们首先创建一个 SBT 项目的框架。
- en: Step 3 – creating an SBT project
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 步 – 创建 SBT 项目
- en: Lay out your SBT project in a folder of your choice and name it `IrisPipeline`
    or any name that makes sense to you. This will hold all of our files needed to
    implement and run the pipeline on the Iris dataset.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在你选择的文件夹中布局你的 SBT 项目，并命名为 `IrisPipeline` 或任何对你有意义的名称。这将包含我们实现和运行 Iris 数据集上管道所需的所有文件。
- en: 'The structure of our SBT project looks like the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 SBT 项目的结构如下所示：
- en: '![](img/ff1f985f-e989-4cd6-944f-9b8fd4a473be.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/ff1f985f-e989-4cd6-944f-9b8fd4a473be.jpg)'
- en: Project structure
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 项目结构
- en: 'We will list dependencies in the `build.sbt` file. This is going to be an SBT
    project. Hence, we will bring in the following key libraries:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `build.sbt` 文件中列出依赖项。这将是一个 SBT 项目。因此，我们将引入以下关键库：
- en: Spark Core
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 核心组件
- en: Spark MLlib
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: Spark SQL
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: 'The following screenshot illustrates the `build.sbt` file:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 下述截图展示了 `build.sbt` 文件：
- en: '![](img/92bebe43-485c-4ddb-98b2-5cff6f2efa44.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/92bebe43-485c-4ddb-98b2-5cff6f2efa44.jpg)'
- en: The build.sbt file with Spark dependencies
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 包含 Spark 依赖项的 build.sbt 文件
- en: The `build.sbt` file referenced in the preceding snapshot is readily available
    for you in the book's download bundle. Drill down to the folder `Chapter01` code
    under `ModernScalaProjects_Code` and copy the folder over to a convenient location
    on your computer.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 前一快照中引用的 `build.sbt` 文件在书的下载包中 readily 可用。在 `ModernScalaProjects_Code` 下的 `Chapter01`
    代码文件夹中深入挖掘，并将文件夹复制到你的电脑上的一个方便位置。
- en: Drop the `iris.csv` file that you downloaded in *Step 1 – getting the Iris dataset
    from the UCI Machine Learning Repository* into the root folder of our new SBT
    project. Refer to the earlier screenshot that depicts the updated project structure
    with the `iris.csv` file inside of it.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 将在 *第 1 步 - 从 UCI 机器学习仓库获取 Iris 数据集* 中下载的 `iris.csv` 文件删除到我们新 SBT 项目的根目录中。请参考之前的截图，其中显示了包含
    `iris.csv` 文件的更新后的项目结构。
- en: Step 4 – creating Scala files in SBT project
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 步 - 在 SBT 项目中创建 Scala 文件
- en: 'Step 4 is broken down into the following steps:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步分解为以下步骤：
- en: Create the Scala file `iris.scala` in the `com.packt.modern.chapter1` package.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `com.packt.modern.chapter1` 包中创建一个名为 `iris.scala` 的 Scala 文件。
- en: Up until now, we relied on `SparkSession` and `SparkContext`, which `spark-shell`
    gave us. This time around, we need to create `SparkSession`, which will, in turn,
    give us `SparkContext`.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们依赖于 `SparkSession` 和 `SparkContext`，这是 `spark-shell` 给我们的。这次，我们需要创建
    `SparkSession`，它反过来会给我们 `SparkContext`。
- en: What follows is how the code is laid out in the `iris.scala` file.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是如何在 `iris.scala` 文件中布局代码。
- en: 'In `iris.scala`, after the package statement, place the following `import`
    statements:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `iris.scala` 中，在包声明之后，放置以下 `import` 语句：
- en: '[PRE33]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create `SparkSession` inside a trait, which we shall call `IrisWrapper`:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个名为 `IrisWrapper` 的特质内部创建 `SparkSession`：
- en: '[PRE34]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Just one `SparkSession` is made available to all classes extending from `IrisWrapper`.
    Create `val` to hold the `iris.csv` file path:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个 `SparkSession` 被提供给所有继承自 `IrisWrapper` 的类。创建一个 `val` 来保存 `iris.csv` 文件路径：
- en: '[PRE35]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create a method to build `DataFrame`. This method takes in the complete path
    to the Iris dataset path as `String` and returns `DataFrame`:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个构建 `DataFrame` 的方法。此方法接受 Iris 数据集完整路径作为 `String` 并返回 `DataFrame`：
- en: '[PRE36]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Import the `DataFrame` class by updating the previous `import` statement for
    `SparkSession`:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更新 `SparkSession` 的先前 `import` 语句来导入 `DataFrame` 类：
- en: '[PRE37]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create a nested function inside `buildDataFrame` to process the raw dataset.
    Name this function `getRows`. `getRows` which takes no parameters but returns
    `Array[(Vector, String)]`. The `textFile` method on the `SparkContext` variable
    processes the `iris.csv` into `RDD[String]`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `buildDataFrame` 函数内部创建一个嵌套函数来处理原始数据集。将此函数命名为 `getRows`。`getRows` 函数不接受任何参数，但返回
    `Array[(Vector, String)]`。`SparkContext` 变量的 `textFile` 方法将 `iris.csv` 处理成 `RDD[String]`：
- en: '[PRE38]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The resulting RDD contains two partitions. Each partition, in turn, contains
    rows of strings separated by a newline character, `'\n'`. Each row in the RDD
    represents its original counterpart in the raw data.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 RDD 包含两个分区。每个分区反过来包含由换行符 `'\n'` 分隔的字符串行。RDD 中的每一行代表原始数据中的对应行。
- en: In the next step, we will attempt several data transformation steps. We start
    by applying a `flatMap` operation over the RDD, culminating in the `DataFrame`
    creation. `DataFrame` is a view over `Dataset`, which happens to the fundamental
    data abstraction unit in the Spark 2.0 line.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将尝试几个数据转换步骤。我们首先在 RDD 上应用 `flatMap` 操作，最终创建 `DataFrame`。`DataFrame`
    是 `Dataset` 的一个视图，而 `Dataset` 正好是 Spark 2.0 线中的基本数据抽象单元。
- en: Step 5 – preprocessing, data transformation, and DataFrame creation
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 步 - 预处理、数据转换和 DataFrame 创建
- en: We will get started by invoking `flatMap`, by passing a function block to it,
    and successive transformations listed as follows, eventually resulting in `Array[(org.apache.spark.ml.linalg.Vector,
    String)]`. A vector represents a row of feature measurements.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过传递一个函数块给它并按以下顺序进行连续转换来开始，最终得到 `Array[(org.apache.spark.ml.linalg.Vector,
    String)]`。一个向量代表特征测量的行。
- en: 'The Scala code to give us `Array[(org.apache.spark.ml.linalg.Vector, String)]`
    is as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 给出 `Array[(org.apache.spark.ml.linalg.Vector, String)]` 的 Scala 代码如下：
- en: '[PRE39]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, drop the `header` column, but not before doing a collection that returns
    an `Array[Array[String]]`:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，删除 `header` 列，但在删除之前先进行一个返回 `Array[Array[String]]` 的收集：
- en: '[PRE40]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The header column is gone; now import the `Vectors` class:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 标题列已消失；现在导入 `Vectors` 类：
- en: '[PRE41]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, transform `Array[Array[String]]` into `Array[(Vector, String)]`:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将 `Array[Array[String]]` 转换为 `Array[(Vector, String)]`：
- en: '[PRE42]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The last step remaining is to create a final DataFrame
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的最后一步是创建一个最终的 DataFrame
- en: DataFrame Creation
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame 创建
- en: 'Now, we invoke the `createDataFrame` method with a parameter, `getRows`. This
    returns `DataFrame` with `featureVector` and `speciesLabel` (for example, Iris-setosa):'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用参数 `getRows` 调用 `createDataFrame` 方法。这将返回包含 `featureVector` 和 `speciesLabel`（例如，Iris-setosa）的
    `DataFrame`：
- en: '[PRE43]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Display the top 20 rows in the new dataframe:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新 DataFrame 的前 20 行：
- en: '[PRE44]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We need to index the species label column by converting the strings Iris-setosa,
    Iris-virginica, and Iris-versicolor into doubles. We will use a `StringIndexer`
    to do that.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过将 Iris-setosa、Iris-virginica 和 Iris-versicolor 这些字符串转换为双精度值来索引物种标签列。我们将使用
    `StringIndexer` 来完成这个任务。
- en: Now create a file called `IrisPipeline.scala`.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个名为 `IrisPipeline.scala` 的文件。
- en: 'Create an object `IrisPipeline` that extends our `IrisWrapper` trait:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 `IrisPipeline` 的对象，它扩展了我们的 `IrisWrapper` 特性：
- en: '[PRE45]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Import the `StringIndexer` algorithm class:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `StringIndexer` 算法类：
- en: '[PRE46]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now create a `StringIndexer` algorithm instance. The `StringIndexer` will map
    our species label column to an indexed learned column:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个 `StringIndexer` 算法实例。`StringIndexer` 将我们的物种标签列映射到一个索引学习列：
- en: '[PRE47]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Step 6 – creating, training, and testing data
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 步 - 创建、训练和测试数据
- en: 'Now, let''s split our dataset in two by providing a random seed:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过提供一个随机种子来将我们的数据集分成两部分：
- en: '[PRE48]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now our new `splitDataset` contains two datasets:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的新 `splitDataset` 包含两个数据集：
- en: '**Train dataset:** A dataset containing `Array[(Vector, iris-species-label-column:
    String)]`'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集**：包含 `Array[(Vector, iris-species-label-column: String)]` 的数据集'
- en: '**Test dataset:** A dataset containing `Array[(Vector, iris-species-label-column:
    String)]`'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据集**：包含 `Array[(Vector, iris-species-label-column: String)]` 的数据集'
- en: 'Confirm that the new dataset is of size `2`:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 确认新数据集的大小为 `2`：
- en: '[PRE49]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Assign the training dataset to a variable, `trainSet`:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练数据集分配给一个变量，`trainSet`：
- en: '[PRE50]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Assign the testing dataset to a variable, `testSet`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 将测试数据集分配给一个变量，`testSet`：
- en: '[PRE51]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Count the number of rows in the training dataset:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 计算训练数据集的行数：
- en: '[PRE52]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Count the number of rows in the testing dataset:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试数据集的行数：
- en: '[PRE53]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: There are 150 rows in all.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有 150 行。
- en: Step 7 – creating a Random Forest classifier
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 步 - 创建随机森林分类器
- en: In reference to Step 5 - DataFrame Creation. This DataFrame 'dataFrame' contains
    column names that corresponds to the columns present in the DataFrame produced
    in that step
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 参考第 5 步 - DataFrame 创建。这个 DataFrame 'dataFrame' 包含的列名与该步骤生成的 DataFrame 中的列相对应
- en: 'The first step to create a classifier is to  pass into it (hyper) parameters.
    A fairly comprehensive list of parameters look like this:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 创建分类器的第一步是将（超）参数传递给它。一个相当全面的参数列表看起来像这样：
- en: From 'dataFrame' we need the Features column name - **iris-features-column**
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 'dataFrame' 我们需要特征列的名称 - **iris-features-column**
- en: From 'dataFrame' we also need the Indexed label column name - **iris-species-label-column**
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 'dataFrame' 我们还需要索引标签列的名称 - **iris-species-label-column**
- en: The `sqrt` setting for `featureSubsetStrategy`
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featureSubsetStrategy` 的 `sqrt` 设置'
- en: Number of features to be considered per split (we have 150 observations and
    four features that will make our `max_features` value `2`)
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次分割要考虑的特征数（我们有 150 个观察值和四个特征，这将使我们的 `max_features` 值为 `2`）
- en: Impurity settings—values can be gini and entropy
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杂质设置——值可以是 gini 和熵
- en: Number of trees to train (since the number of trees is greater than one, we
    set a tree maximum depth), which is a number equal to the number of nodes
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要训练的树的数量（因为树的数量大于一个，我们设置树的最大深度），这是一个等于节点数量的数字
- en: The required minimum number of feature measurements (sampled observations),
    also known as the minimum instances per node
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的最小特征测量数（样本观察值），也称为每个节点的最小实例数
- en: Look at the `IrisPipeline.scala` file for values of each of these parameters.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 `IrisPipeline.scala` 文件以获取这些参数的值。
- en: But this time, we will employ an exhaustive grid search-based model selection
    process based on combinations of parameters, where parameter value ranges are
    specified.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次，我们将采用基于参数组合的穷举网格搜索模型选择过程，其中参数值范围被指定。
- en: 'Create a `randomForestClassifier` instance. Set the features and `featureSubsetStrategy`:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 `randomForestClassifier` 实例。设置特征和 `featureSubsetStrategy`：
- en: '[PRE54]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Start building `Pipeline`, which has two stages, `Indexer` and `Classifier`:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 开始构建 `Pipeline`，它有两个阶段，`Indexer` 和 `Classifier`：
- en: '[PRE55]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Next, set the hyperparameter `num_trees` (number of trees) on the classifier
    to `15`, a `Max_Depth` parameter, and an impurity with two possible values of
    gini and entropy.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将分类器上的超参数 `num_trees`（树的数量）设置为 `15`，一个 `Max_Depth` 参数，以及具有两个可能值 gini 和熵的杂质。
- en: 'Build a parameter grid with all three hyperparameters:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个包含所有三个超参数的参数网格：
- en: '[PRE56]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Step 8 – training the Random Forest classifier
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 步 - 训练随机森林分类器
- en: 'Next, we want to split our training set into a validation set and a training
    set:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要将我们的训练集分成一个验证集和一个训练集：
- en: '[PRE57]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'On this variable, set `Seed`, set `EstimatorParamMaps`, set `Estimator` with
    `irisPipeline`, and set a training ratio to `0.8`:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个变量上，设置`Seed`，设置`EstimatorParamMaps`，设置`Estimator`为`irisPipeline`，并将训练比例设置为`0.8`：
- en: '[PRE58]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Finally, do a fit and a transform with our training dataset and testing dataset.
    Great! Now the classifier is trained. In the next step, we will apply this classifier
    to testing the data.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用我们的训练数据集和测试数据集进行拟合和转换。太好了！现在分类器已经训练好了。在下一步中，我们将应用这个分类器来测试数据。
- en: Step 9 – applying the Random Forest classifier to test data
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9步 – 将随机森林分类器应用于测试数据
- en: 'The purpose of our validation set is to be able to make a choice between models.
    We want an evaluation metric and hyperparameter tuning. We will now create an
    instance of a validation estimator called `TrainValidationSplit`, which will split
    the training set into a validation set and a training set:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的验证集的目的是能够在模型之间做出选择。我们想要一个评估指标和超参数调整。现在我们将创建一个名为`TrainValidationSplit`的验证估计器的实例，它将训练集分成一个验证集和一个训练集：
- en: '[PRE59]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Next, we fit this estimator over the training dataset to produce a model and
    a transformer that we will use to transform our testing dataset. Finally, we perform
    a validation for hyperparameter tuning by applying an evaluator for a metric.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这个估计量拟合到训练数据集上，以生成一个模型和一个转换器，我们将使用它们来转换我们的测试数据集。最后，我们通过应用一个用于度量的评估器来进行超参数调整的验证。
- en: 'The new `ValidatedTestResults` `DataFrame` should look something like this:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 新的`ValidatedTestResults` `DataFrame`应该看起来像这样：
- en: '[PRE60]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s return a new dataset by passing in column expressions for `prediction`
    and `label`:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过传递`prediction`和`label`列的表达式来返回一个新的数据集：
- en: '[PRE61]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In the line of code, we produced a new `DataFrame` with two columns:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码行中，我们生成了一个包含两列的新`DataFrame`：
- en: An input label
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入标签
- en: A predicted label, which is compared with its corresponding value in the input
    label column
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测标签，它与输入标签列中的对应值进行比较
- en: That brings us to the next step, an evaluation step. We want to know how well
    our model performed. That is the goal of the next step.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这就带我们到了下一步，一个评估步骤。我们想知道我们的模型表现如何。这就是下一步的目标。
- en: Step 10 – evaluate Random Forest classifier
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10步 – 评估随机森林分类器
- en: In this section, we will test the accuracy of the model. We want to know how
    well our model performed. Any ML process is incomplete without an evaluation of
    the classifier.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将测试模型的准确性。我们想知道我们的模型表现如何。任何机器学习过程如果没有对分类器的评估都是不完整的。
- en: 'That said, we perform an evaluation as a two-step process:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们以两步过程进行评估：
- en: Evaluate the model output
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型输出
- en: 'Pass in three hyperparameters:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传入三个超参数：
- en: '[PRE62]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Set the label column, a metric name, the prediction column `label`, and invoke
    evaluation with the `validatedTestResults` dataset.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 设置标签列，一个度量名称，预测列`label`，并使用`validatedTestResults`数据集调用评估。
- en: Note the accuracy of the model output results on the testing dataset from the
    `modelOutputAccuracy` variable.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 注意从`modelOutputAccuracy`变量中测试数据集上的模型输出结果的准确性。
- en: The other metrics to evaluate are how close the predicted label value in the
    `'predicted'` column is to the actual label value in the (indexed) label column.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 需要评估的其他指标是预测标签值在`'predicted'`列中与（索引的）标签列中的实际标签值有多接近。
- en: 'Next, we want to extract the metrics:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要提取指标：
- en: '[PRE63]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Our pipeline produced predictions. As with any prediction, we need to have
    a healthy degree of skepticism. Naturally, we want a sense of how our engineered
    prediction process performed. The algorithm did all the heavy lifting for us in
    this regard. That said, everything we did in this step was done for the purpose
    of evaluation. Who is being evaluated here or what evaluation is worth reiterating? That
    said, we wanted to know how close the predicted values were compared to the actual
    label value. To obtain that knowledge, we decided to use the `MulticlassMetrics`
    class to evaluate metrics that will give us a measure of the performance of the
    model via two methods:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的管道产生了预测。与任何预测一样，我们需要保持一定的怀疑态度。自然地，我们想要了解我们的预测过程表现如何。在这方面，算法为我们做了大量的工作。话虽如此，我们在这一步所做的一切都是为了评估。这里谁在受到评估，或者哪种评估值得重复？话虽如此，我们想知道预测值与实际标签值有多接近。为了获得这种知识，我们决定使用`MulticlassMetrics`类来评估将给我们提供模型性能度量的指标，通过两种方法：
- en: Accuracy
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率
- en: Weighted precision
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权精度
- en: The following lines of code will give us value of Accuracy and Weighted Precision.
    First we will create an accuracyMetrics tuple, which should contain the values
    of both accuracy and weighted precision
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行将给出准确率和加权精度的值。首先，我们将创建一个包含准确率和加权精度值的accuracyMetrics元组
- en: '[PRE64]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Obtain the value of accuracy.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 获取准确率的值。
- en: '[PRE65]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Next, obtain the value of weighted precision.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，获取加权精度的值。
- en: '[PRE66]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: These metrics represent evaluation results for our classifier or classification
    model. In the next step, we will run the application as a packaged SBT application.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标代表了我们分类器或分类模型的评估结果。在下一步中，我们将以打包的SBT应用程序运行应用程序。
- en: Step 11 – running the pipeline as an SBT application
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11步 – 以SBT应用程序运行管道
- en: 'At the root of your project folder, issue the `sbt console` command, and in
    the Scala shell, import the `IrisPipeline` object and then invoke the `main` method
    of `IrisPipeline` with the argument `iris`:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的项目文件夹根目录下，执行`sbt console`命令，然后在Scala shell中导入`IrisPipeline`对象，然后使用`iris`参数调用`IrisPipeline`的`main`方法：
- en: '[PRE67]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In the next section, we will show you how to package the application so that
    it is ready to be deployed into Spark as an Uber JAR.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将向您展示如何打包应用程序，使其准备好作为Uber JAR部署到Spark。
- en: Step 12 – packaging the application
  id: totrans-498
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12步 – 打包应用程序
- en: 'In the root folder of your SBT application, run:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的SBT应用程序的根目录下运行：
- en: '[PRE68]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'When SBT is done packaging, the Uber JAR can be deployed into our cluster,
    using `spark-submit`, but since we are in standalone deploy mode, it will be deployed
    into `[local]`:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 当SBT完成打包后，可以使用`spark-submit`将Uber JAR部署到我们的集群中，但由于我们处于独立部署模式，它将被部署到 `[local]`：
- en: '![](img/bfb1ec64-d3dd-4ec2-aa50-49d358ac5a6f.jpg)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bfb1ec64-d3dd-4ec2-aa50-49d358ac5a6f.jpg)'
- en: The application JAR file
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序JAR文件
- en: The package command created a JAR file that is available under the target folder.
    In the next section, we will deploy the application into Spark.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 打包命令创建了一个位于目标文件夹下的JAR文件。在下一节中，我们将部署应用程序到Spark。
- en: Step 13 – submitting the pipeline application to Spark local
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13步 – 将管道应用程序提交到Spark本地
- en: At the root of the application folder, issue the `spark-submit` command with
    the class and JAR file path arguments, respectively.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序文件夹根目录下，使用类和JAR文件路径参数分别执行`spark-submit`命令。
- en: 'If everything went well, the application does the following:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，应用程序将执行以下操作：
- en: Loads up the data.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Performs EDA.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行EDA。
- en: Creates training, testing, and validation datasets.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练、测试和验证数据集。
- en: Creates a Random Forest classifier model.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机森林分类器模型。
- en: Trains the model.
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Tests the accuracy of the model. This is the most important part—the ML classification
    task.
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型的准确性。这是最重要的部分——机器学习分类任务。
- en: To accomplish this, we apply our trained Random Forest classifier model to the
    test dataset. This dataset consists of Iris flower data of so far not seen by
    the model. Unseen data is nothing but Iris flowers picked in the wild.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实现这一点，我们将训练好的随机森林分类器模型应用于测试数据集。这个数据集包含了模型尚未见过的鸢尾花数据。未见数据不过是野外的鸢尾花。
- en: Applying the model to the test dataset results in a prediction about the species
    of an unseen (new) flower.
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型应用于测试数据集，结果是对未见（新）花朵物种的预测。
- en: The last part is where the pipeline runs an evaluation process, which essentially
    is about checking if the model reports the correct species.
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的部分是管道运行评估过程，这本质上就是检查模型是否报告了正确的物种。
- en: Lastly, pipeline reports back on how important a certain feature of the Iris
    flower turned out to be. As a matter of fact, the petal width turns out to be
    more important than the sepal width in carrying out the classification task.
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，管道会报告某个特定特征在鸢尾花中的重要性。事实上，花瓣宽度在执行分类任务时比萼片宽度更重要。
- en: That brings us to the last section of this chapter. We will summarize what we
    have learned. Not only that, we will give readers a glimpse into what they will
    learn in the next chapter.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带我们来到本章的最后一节。我们将总结我们已经学到的内容。不仅如此，我们还将向读者展示他们将在下一章中学到的东西。
- en: Summary
  id: totrans-519
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we implemented an ML workflow or an ML pipeline. The pipeline
    combined several stages of data analysis into one workflow. We started by loading
    the data and from there on, we created training and test data, preprocessed the
    dataset, trained the `RandomForestClassifier` model, applied the Random Forest
    classifier to test data, evaluated the classifier, and computed a process that
    demonstrated the importance of each feature in the classification. We fulfilled
    the goal that we laid out early on in the *Project overview – problem formulation*
    section.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了一个机器学习工作流程或机器学习管道。该管道将数据分析的几个阶段结合成一个工作流程。我们首先加载数据，然后创建了训练数据和测试数据，对数据集进行了预处理，训练了
    `RandomForestClassifier` 模型，将随机森林分类器应用于测试数据，评估了分类器，并计算了一个过程，展示了每个特征在分类中的重要性。我们实现了在
    *项目概述 – 问题定义* 部分早期设定的目标。
- en: In the next chapter, we will analyze the **Wisconsin Breast Cancer Data Set**.
    This dataset has only categorical data. We will build another pipeline, but this
    time, we will set up the Hortonworks Development Platform Sandbox to develop and
    deploy a breast cancer prediction pipeline. Given a set of categorical feature
    variables, this pipeline will predict whether a given sample is benign or malignant.
    In the next and the last section of the current chapter, we will list a set of
    questions that will test your knowledge of what you have learned so far.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将分析 **威斯康星州乳腺癌数据集**。这个数据集只有分类数据。我们将构建另一个管道，但这次，我们将设置 Hortonworks 开发平台沙盒来开发和部署乳腺癌预测管道。给定一组分类特征变量，这个管道将预测一个给定的样本是良性还是恶性。在当前章节的下一部分和最后一部分，我们将列出一系列问题，以测试您到目前为止所学知识的掌握程度。
- en: Questions
  id: totrans-522
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Here are a list of questions for your reference:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一份供您参考的问题列表：
- en: What do you understand by EDA? Why is it important?
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何理解 EDA？为什么它很重要？
- en: Why do we create training and test data?
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要创建训练数据和测试数据？
- en: Why did we index the data that we pulled from the UCI Machine Learning Repository?
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要索引从 UCI 机器学习仓库中提取的数据？
- en: Why is the Iris dataset so famous?
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 Iris 数据集如此著名？
- en: Name one powerful feature of the random forest classifier.
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林分类器的一个强大功能是什么？
- en: What is supervisory as opposed to unsupervised learning?
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是监督学习与无监督学习相对？
- en: Explain briefly the process of creating our model with training data.
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简要解释使用训练数据创建我们模型的过程。
- en: What are feature variables in relation to the Iris dataset?
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征变量与 Iris 数据集有何关系？
- en: What is the entry point to programming with Spark?
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Spark 编程的入口点是什么？
- en: 'Task: The Iris dataset problem was a statistical classification problem. Create
    a confusion or error matrix with the rows being predicted setosa, predicted versicolor,
    and predicted virginica, and the columns being actual species, such as setosa,
    versicolor, and virginica. Having done that, interpret this matrix.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 任务：Iris 数据集问题是一个统计分类问题。创建一个混淆矩阵或错误矩阵，其中行是预测的 setosa、预测的 versicolor 和预测的 virginica，列是实际物种，如
    setosa、versicolor 和 virginica。完成这个任务后，解释这个矩阵。
