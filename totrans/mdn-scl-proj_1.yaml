- en: Predict the Class of a Flower from the Iris Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter kicks off a **machine learning** (**ML**) initiative in Scala and
    Spark. Speaking of Spark, its **Machine Learning Library** (**MLlib**) living
    under the `spark.ml` package and accessible via its MLlib `DataFrame`-based API
    will help us develop scalable data analysis applications. The MLlib `DataFrame`-based
    API, also known as Spark ML, provides powerful learning algorithms and pipeline
    building tools for data analysis. Needless to say, we will, starting this chapter,
    leverage MLlib's classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark ecosystem, also boasting of APIs to R, Python, and Java in addition
    to Scala, empowers our readers, be they beginner, or seasoned data professionals,
    to make sense of and extract analytics from various datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of datasets, the Iris dataset is the simplest, yet the most famous
    data analysis task in the ML space. This chapter builds a solution to the data
    analysis classification task that the Iris dataset represents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the dataset we will refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository: Iris Data Set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessed July 13, 2018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Website URL: [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overarching learning objective of this chapter is to implement a Scala solution
    to the so-called **multivariate** classification task represented by the Iris
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list is a section-wise breakdown of individual learning outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: A multivariate classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project overview—problem formulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a multiclass classification pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section offers the reader an in-depth perspective on the Iris
    dataset classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: A multivariate classification problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most famous dataset in data science history is Sir Ronald Aylmer Fisher's classical
    Iris flower dataset, also known as Anderson's dataset. It was introduced in 1936,
    as a study in understanding multivariate (or multiclass) classification. What
    then is multivariate?
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multivariate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term multivariate can bear two meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of an adjective, multivariate means having or involving one or more
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of a noun, multivariate may represent a mathematical vector whose individual
    elements are variate. Each individual element in this vector is a measurable quantity
    or variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both meanings mentioned have a common denominator variable. Conducting a multivariate
    analysis of an experimental unit involves at least one measurable quantity or
    variable. A classic example of such an analysis is the Iris dataset, having one
    or more (outcome) variables per observation.
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we understood multivariate in terms of variables. In the
    next subsection, we briefly touch upon different kinds of variables, one of them
    being categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Different kinds of variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, variables are of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantitative variable**: It is a variable representing a measurement that
    is quantified by a numeric value. Some examples of quantitative variables are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A variable representing the age of a girl called `Huan` (`Age_Huan`). In September
    of 2017, the variable representing her age contained the value `24`. Next year,
    one year later, that variable would be the number 1 (arithmetically) added to
    her current age.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variable representing the number of planets in the solar system (`Planet_Number`). Currently,
    pending the discovery of any new planets in the future, this variable contains
    the number `12`. If scientists found a new celestial body tomorrow that they think
    qualifies to be a planet, the `Planet_Number` variable's new value would be bumped
    up from its current value of `12` to `13`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical variable**: A variable that cannot be assigned a numerical measure
    in the natural order of things. For example, the status of an individual in the
    United States. It could be one of the following values: a citizen, permanent resident,
    or a non-resident.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next subsection, we will describe categorical variables in some detail.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will draw upon the definition of a categorical variable from the previous
    subsection. Categorical variables distinguish themselves from quantitative variables
    in a fundamental way. As opposed to a quantitative variable that represents a
    measure of a something in numerical terms, a categorical variable represents a
    grouping name or a category name, which can take one of the finite numbers of
    possible categories. For example, the species of an Iris flower is a categorical
    variable and the value it takes could be one value from a finite set of categorical
    values: Iris-setosa, Iris-virginica, and Iris-versicolor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It may be useful to draw on other examples of categorical variables; these
    are listed here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The blood group of an individual as in A+, A-, B+, B-, AB+, AB-, O+, or O-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The county that an individual is a resident of given a finite list of counties
    in the state of Missouri
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The political affiliation of a United States citizen could take up categorical
    values in the form of Democrat, Republican, or Green Party
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In global warming studies, the type of a forest is a categorical variable that
    could take one of three values in the form of tropical, temperate, or taiga
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first item in the preceding list, the blood group of a person, is a categorical
    variable whose corresponding data (values) are categorized (classified) into eight
    groups (A, B, AB, or O with their positives or negatives). In a similar vein,
    the species of an Iris flower is a categorical variable whose data (values) are
    categorized (classified) into three species groups—Iris-setosa, Iris-versicolor,
    and Iris-virginica.
  prefs: []
  type: TYPE_NORMAL
- en: That said, a common data analysis task in ML is to index, or encode, current
    string representations of categorical values into a numeric form; doubles for
    example. Such indexing is a prelude to a prediction on the target or label, which
    we shall talk more about shortly.
  prefs: []
  type: TYPE_NORMAL
- en: In respect to the Iris flower dataset, its species variable data is subject
    to a classification (or categorization) task with the express purpose of being
    able to make a prediction on the species of an Iris flower. At this point, we
    want to examine the Iris dataset, its rows, row characteristics, and much more,
    which is the focus of the upcoming topic.
  prefs: []
  type: TYPE_NORMAL
- en: Fischer's Iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Iris flower dataset comprises of a total of 150 rows, where each row represents
    one flower. Each row is also known as an **observation**. This 150 observation
    Iris dataset is made up of three kinds of observations related to three different
    Iris flower species. The following table is an illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fefaf096-d868-407a-9302-a18314b30487.png)'
  prefs: []
  type: TYPE_IMG
- en: Iris dataset observation breakup table
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring to the preceding table, it is clear that three flower species are
    represented in the Iris dataset. Each flower species in this dataset contributes
    equally to 50 observations apiece. Each observation holds four measurements. One
    measurement corresponds to one flower feature, where each flower feature corresponds
    to one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sepal Length**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sepal Width**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Petal Length**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Petal Width**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The features listed earlier are illustrated in the following table for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f45af5d4-2ac7-4f79-8aef-5bad685bc9d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Iris features
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so three flower species are represented in the Iris dataset. Speaking
    of species, we will henceforth replace the term *species* with the term *classes*
    whenever there is the need to stick to an ML terminology context. That means **#1**-**Iris-setosa** from
    earlier refers to **Class # 1**, **#2**-**Iris-virginica** to **Class # 2**, and
    **#3**-**Iris-versicolor** to **Class # 3**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We just listed three different Iris flower species that are represented in
    the Iris dataset. What do they look like? What do their features look like? These
    questions are answered in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d0b8100-a058-4788-98fd-34b3ed8bdc60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Representations of three species of Iris flower
  prefs: []
  type: TYPE_NORMAL
- en: That said, let's look at the **Sepal** and **Petal** portions of each class
    of Iris flower. The **Sepal** (the larger lower part) and **Petal** (the lower
    smaller part) dimensions are how each class of Iris flower bears a relationship
    to the other two classes of Iris flowers. In the next section, we will summarize
    our discussion and expand the scope of the discussion of the Iris dataset to a multiclass,
    multidimensional classification task.
  prefs: []
  type: TYPE_NORMAL
- en: The Iris dataset represents a multiclass, multidimensional classification task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will restate the facts about the Iris dataset and describe
    it in the context of an ML classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: The Iris dataset classification task is multiclass because a prediction of the
    class of a new incoming Iris flower from the wild can belong to any of three classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, this chapter is all about attempting a species classification (inferring
    the target class of a new Iris flower) using sepal and petal dimensions as feature
    parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Iris dataset classification is multidimensional because there are four features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are 150 observations, where each observation is comprised of measurements
    on four features. These measurements are also known by the following terms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input attributes or instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictor variables (`X`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input variables (`X`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification of an Iris flower picked in the wild is carried out by a model
    (the computed mapping function) that is given four flower feature measurements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The outcome of the Iris flower classification task is the identification of
    a (computed) predicted value for the response from the predictors by a process
    of learning (or fitting) a discrete number of targets or category labels (`Y`).
    The outcome or predicted value may mean the same as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Categorical response variable: In a later section, we shall see that an indexer
    algorithm will transform all categorical values to numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response or outcome variable (`Y`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have claimed that the outcome (`Y`) of our multiclass classification
    task is dependent on inputs (`X`). Where will these inputs come from? This is
    answered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An integral aspect of our data analysis or classification task we did not hitherto
    mention is the training dataset. A training dataset is our classification task's
    source of input data (`X`). We take advantage of this dataset to obtain a prediction
    on each target class, simply by deriving optimal perimeters or boundary conditions.
    We just redefined our classification process by adding in the extra detail of
    the training dataset. For a classification task, then we have `X` on one side
    and `Y` on the other, with an inferred mapping function in the middle. That brings
    us to the mapping or predictor function, which is the focus of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The mapping function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have so far talked about an input variable (`X`) and an output variable
    (`Y`). The goal of any classification task, therefore, is to discover patterns
    and find a mapping (predictor) function that will take feature measurements (`X`)
    and map input over to the output (`Y`). That function is mathematically formulated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This mapping is how supervised learning works. A supervised learning algorithm
    is said to learn or discover this function. This will be the goal of the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm and its mapping function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section starts with a schematic depicting the components of the mapping
    function and an algorithm that learns the mapping function. The algorithm is learning
    the mapping function, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/313714d2-55c9-4136-91d6-336e3d311575.png)'
  prefs: []
  type: TYPE_IMG
- en: An input to output mapping function and an algorithm learning the mapping function
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our classification process is to let the algorithm derive the best
    possible approximation of a mapping function by a learning (or fitting) process.
    When we find an Iris flower out in the wild and want to classify it, we use its
    input measurements as new input data that our algorithm's mapping function will
    accept in order to give us a predictor value (**Y**). In other words, given feature
    measurements of an Iris flower (the new data), the mapping function produced by
    a supervised learning algorithm (this will be a random forest) will classify the
    flower.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two kinds of ML problems exist that supervised learning classification algorithms
    can solve. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following paragraph, we will talk about a mapping function with an example. 
    We explain the role played by a "supervised learning classification task" in deducing
    the mapping function. The concept of a model is introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we already knew that the (mapping) function `f(x)` for the Iris dataset
    classification task is exactly of the form `x + 1`,   then there is there no need
    for us to find a new mapping function.  If we recall, a mapping function is one
    that maps the relationship between flower features, such as sepal length and sepal
    width, on the species the flower belongs to? No.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, there is no preexisting function `x + 1` that clearly maps the relationship
    between flower features and the flower's species. What we need is a model that
    will model the aforementioned relationship as closely as possible. Data and its
    classification seldom tend to be straightforward. A supervised learning classification
    task starts life with no knowledge of what function `f(x)` is. A supervised learning
    classification process applies ML techniques and strategies in an iterative process
    of deduction to ultimately learn what `f(x)` is.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, such an ML endeavor is a classification task, a task where the
    function or mapping function is referred to in statistical or ML terminology as
    a **model**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe what supervised learning is and how it
    relates to the Iris dataset classification.  Indeed, this apparently simplest
    of ML techniques finds wide applications in data analysis, especially in the business
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning – how it relates to the Iris classification task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the outset, the following is a list of salient aspects of supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: The term **supervised** in supervised learning stems from the fact that the
    algorithm is learning or inferring what the mapping function is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data analysis task, either classification or regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains a process of learning or inferring a mapping function from a labeled
    training dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our Iris training dataset has training examples or samples, where each example
    may be represented by an input feature vector consisting of four measurements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A supervised learning algorithm learns or infers or derives the best possible
    approximation of a mapping function by carrying out a data analysis on the training
    data. The mapping function is also known as a model in statistical or ML terminology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The algorithm provides our model with parameters that it learns from the training
    example set or training dataset in an iterative process, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each iteration produces predicted class labels for new input instances from
    the wild
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each iteration of the learning process produces progressively better generalizations
    of what the output class label should be, and as in anything that has an end,
    the learning process for the algorithm also ends with a high degree of reasonable
    correctness on the prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ML classification process employing supervised learning has algorithm samples
    with correctly predetermined labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Iris dataset is a typical example of a supervised learning classification
    process. The term supervised arises from the fact that the algorithm at each step
    of an iterative learning process applies an appropriate correction on its previously
    generated model building process to generate its next best model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will define a training dataset. In the next section,
    and in the remaining sections, we will use the Random Forest classification algorithm
    to run data analysis transformation tasks. One such task worth noting here is
    a process of transformation of string labels to an indexed label column represented
    by doubles.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest classification algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a preceding section, we noted the crucial role played by the input or training
    dataset. In this section, we reiterate the importance of this dataset. That said,
    the training dataset from an ML algorithm standpoint is one that the Random Forest
    algorithm takes advantage of to train or fit the model by generating the parameters
    it needs. These are parameters the model needs to come up with the next best-predicted
    value. In this chapter, we will put the Random Forest algorithm to work on training
    (and testing) Iris datasets. Indeed, the next paragraph starts with a discussion
    on Random Forest algorithms or simply Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: A Random Forest algorithm encompasses decision tree-based supervised learning
    methods. It can be viewed as a composite whole comprising a large number of decision
    trees. In ML terminology, a Random Forest is an ensemble resulting from a profusion
    of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree, as the name implies, is a progressive decision-making process,
    made up of a root node followed by successive subtrees. The decision tree algorithm
    snakes its way up the tree, stopping at every node, starting with the root node,
    to pose a do-you-belong-to-a-certain-category question. Depending on whether the
    answer is a yes or a no, a decision is made to travel up a certain branch until
    the next node is encountered, where the algorithm repeats its interrogation. Of
    course, at each node, the answer received by the algorithm determines the next
    branch to be on. The final outcome is a predicted outcome on a leaf that terminates.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of trees, branches, and nodes, the dataset can be viewed as a tree
    made up of multiple subtrees. Each decision at a node of the dataset and the decision
    tree algorithm's choice of a certain branch is the result of an optimal composite
    of feature variables. Using a Random Forest algorithm, multiple decision trees
    are created. Each decision tree in this ensemble is the outcome of a randomized
    ordering of variables. That brings us to what random forests are—an ensemble of
    a multitude of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that one decision tree by itself cannot work well for a smaller
    sample like the Iris dataset. This is where the Random Forest algorithm steps
    in. It brings together or aggregates all of the predictions from its forest of
    decision trees. All of the aggregated results from individual decision trees in
    this forest would form one ensemble, better known as a Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: We chose the Random Forest method to make our predictions for a good reason.
    The net prediction formed out of an ensemble of predictions is significantly more
    accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will formulate our classification problem, and in the
    *Getting started with Spark* section that follows, implementation details for
    the project are given.
  prefs: []
  type: TYPE_NORMAL
- en: Project overview – problem formulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The intent of this project is to develop an ML workflow or more accurately a pipeline.
    The goal is to solve the classification problem on the most famous dataset in
    data science history.
  prefs: []
  type: TYPE_NORMAL
- en: If we saw a flower out in the wild that we know belongs to one of three Iris
    species, we have a classification problem on our hands. If we made measurements
    (`X`) on the unknown flower, the task is to learn to recognize the species to
    which the flower (and its plant) belongs.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables represent types of data which may be divided into groups.
    Examples of categorical variables are race, sex, age group, and educational level.
    While the latter two variables may also be considered in a numerical manner by
    using exact values for age and highest grade completed, it is often more informative
    to categorize such variables into a relatively small number of groups.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of categorical data generally involves the use of data tables. A two-way
    table presents categorical data by counting the number of observations that fall
    into each group for two variables, one divided into rows and the other divided
    into columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the high-level formulation of the classification problem is
    given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ba34af0-8b84-4465-b1e7-d4b7478c0f0e.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level formulation of the Iris supervised learning classification problem
  prefs: []
  type: TYPE_NORMAL
- en: In the Iris dataset, each row contains categorical data (values) in the fifth
    column. Each such value is associated with a label (**Y**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The formulation consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Observed features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Category labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observed features are also known as **predictor variables**. Such variables
    have predetermined measured values. These are the inputs X. On the other hand,
    category labels denote possible output values that predicted variables can take.
  prefs: []
  type: TYPE_NORMAL
- en: 'The predictor variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sepal_length`: It represents sepal length, in centimeters, used as input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sepal_width`: It represents sepal width, in centimeters, used as input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`petal_length`: It represents petal length, in centimeters, used as input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`petal_width`: It represents petal width, in centimeters, used as input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setosa`: It represents Iris-setosa, true or false, used as target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`versicolour`: It represents Iris-versicolour, true or false, used as target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virginica`: It represents Iris-virginica, true or false, used as target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four outcome variables were measured from each sample; the length and the width
    of the sepals and petals.
  prefs: []
  type: TYPE_NORMAL
- en: The total build time of the project should be no more than a day in order to
    get everything working. For those new to the data science area, understanding
    the background theory, setting up the software, and getting to build the pipeline
    could take an extra day or two.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The instructions are for Windows users. Note that to run Spark Version 2 and
    above, Java Version 8 and above, Scala Version 2.11, **Simple Build Tool** (**SBT**)
    version that is at least 0.13.8 is a prerequisite. The code for the Iris project
    depends on Spark 2.3.1, the latest distribution at the time of writing this chapter.
    This version was released on December 1, 2017\. Implementations in subsequent
    chapters would likely be based on Spark 2.3.0, released February 28, 2017\. Spark
    2.3.0 is a major update version that comes with fixes to over 1,400 tickets.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark 2.0 brought with it a raft of improvements. The introduction of the
    dataframe as the fundamental abstraction of data is one such improvement. Readers
    will find that the dataframe abstraction and its supporting APIs enhance their
    data science and analysis tasks, not to mention this powerful feature's improved
    performance over **Resilient Distributed Datasets** (**RDDs**). Support for RDDs
    is very much available in the latest Spark release as well.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up prerequisite software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A note on hardware before jumping to prerequisites. The hardware infrastructure
    I use throughout in this chapter comprises of a 64-bit Windows Dell 8700 machine
    running Windows 10 with Intel(R) Core(TM) i7-4770 CPU @ 3.40 GHz and an installed
    memory of 32 GB.
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we document three software prerequisites that must be in
    place before installing Spark.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of this writing, my prerequisite software setup consisted of JDK
    8, Scala 2.11.12, and SBT 0.13.8, respectively. The following list is a minimal,
    recommended setup (note that you are free to try a higher JDK 8 version and Scala
    2.12.x).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the required prerequisite list for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Java SE Development Kit 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala 2.11.12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBT 0.13.8 or above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are like me, dedicating an entire box with the sole ambition of evolving
    your own Spark big data ecosystem is not a bad idea. With that in mind, start
    with an appropriate machine (with ample space and at least 8 GB of memory), running
    your preferred OS, and install the preceding mentioned prerequisites listed in
    order. What about lower versions of the JDK, you may ask? Indeed, lower versions
    of the JDK are not compatible with Spark 2.3.1.
  prefs: []
  type: TYPE_NORMAL
- en: While I will not go into the JDK installation process here, here are a couple
    of notes. Download Java 8 ([http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html))
    and once the installer is done installing the `Java` folder, do not forget to
    set up two new system environment variables—the `JAVA_HOME` environment variable
    pointing to the root folder of your Java installation, and the `JAVA_HOME/bin`
    in your system path environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the system `JAVA_HOME` environment, here is how to do a quick
    sanity check by listing the value of `JAVA_HOME` on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now what remains is to do another quick check to be certain you installed the
    JDK flawlessly. Issue the following commands on your command line or Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this screen only represents the Windows command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: At this point, if your sanity checks passed, the next step is to install Scala.
    The following brief steps outline that process. The Scala download page at [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris) documents
    many ways to install Scala (for different OS environments). However, we only list
    three methods to install Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the Scala installation, a quick note here. While the latest
    stable version of Scala is 2.12.4, I prefer a slightly older version, version
    2.11.12, which is the version I will use in this chapter. You may download it
    at [http://scala-lang.org/download/2.11.12.html](http://scala-lang.org/download/2.11.12.html). Whether
    you prefer version 2.12 or 2.11, the choice is yours to make, as long as the version
    is not anything below 2.11.x. The following installation methods listed will get
    you started down that path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scala can be installed through the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Install Scala**: Locate the section titled Other ways to install Scala at [http://scala-lang.org/download/](http://scala-lang.org/download/) and download
    the Scala binaries from there. Then you can install Scala by following the instructions
    at [http://scala-lang.org/download/install.html.](http://scala-lang.org/download/install.html) Install
    SBT from [https://www.scala-sbt.org/download.html](https://www.scala-sbt.org/download.html)
    and follow the setup instructions at [https://www.scala-sbt.org/1.0/docs/Setup.html.](https://www.scala-sbt.org/1.0/docs/Setup.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scala in the IntelliJ IDE**: Instructions are given at [https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html.](https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scala in the IntelliJ IDE with SBT**: This is another handy way to play with
    Scala. Instructions are given at [https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html.](https://docs.scala-lang.org/getting-started-intellij-track/getting-started-with-scala-in-intellij.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The acronym **SBT** that just appeared in the preceding list is short for **Simple
    Build Tool**. Indeed, you will run into references to SBT fairly often throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Take up the item from the first method of the preceding list and work through
    the (mostly self-explanatory) instructions. Finally, if you forgot to set environment
    variables, do set up a brand new `SCALA_HOME` system environment variable (like
    `JAVA_HOME`), or simply update an existing `SCALA_HOME`. Naturally, the `SCALA_HOME/bin`
    entry is added to the path environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: You do not necessarily need Scala installed system-wide. The SBT environment
    gives us access to its own Scala environment anyway. However, having a system-wide
    Scala installation allows you to quickly implement Scala code rather than spinning
    up an entire SBT project.
  prefs: []
  type: TYPE_NORMAL
- en: Let us review what we have accomplished so far. We installed Scala by working
    through the first method Scala installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm that we did install Scala, let''s run a basic test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code listing confirms that our most basic Scala installation
    went off without a hitch. This paves the way for a system-wide SBT installation.
    Once again, it comes down to setting up the `SBT_HOME` system environment variable
    and setting `$SBT_HOME/bin` in the path. This is the most fundamental bridge to
    cross. Next, let''s run a sanity check to verify that SBT is all set up. Open
    up a command-line window or Terminal. We installed SBT 0.13.17, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We are left with method two and method three. These are left as an exercise
    for the reader. Method three will let us take advantage of all the nice features
    that an IDE like IntelliJ has.
  prefs: []
  type: TYPE_NORMAL
- en: Shortly, the approach we will take in developing our pipeline involves taking
    an existing SBT project and importing it into IntelliJ, or we just create the
    SBT project in IntelliJ.
  prefs: []
  type: TYPE_NORMAL
- en: What's next? The Spark installation of course. Read all about it in the upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark in standalone deploy mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we set up a Spark development environment in standalone deploy
    mode. To get started with Spark and start developing quickly, Spark's shell is
    the way to go.
  prefs: []
  type: TYPE_NORMAL
- en: Spark supports Scala, Python, R, and Java with appropriate APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark binary download offers developers two components:'
  prefs: []
  type: TYPE_NORMAL
- en: The Spark's shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A standalone cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the binary is downloaded and extracted (instructions will follow), the
    Spark shell and standalone Scala application will let you spin up a standalone
    cluster in standalone cluster mode.
  prefs: []
  type: TYPE_NORMAL
- en: This cluster is self-contained and private because it is local to one machine.
    The Spark shell allows you to easily configure this standalone cluster. Not only
    does it give you quick access to an interactive Scala shell, but also lets you
    develop a Spark application that you can deploy into the cluster (lending it the
    name standalone deploy mode), right in the Scala shell.
  prefs: []
  type: TYPE_NORMAL
- en: In this mode, the cluster's driver node and worker nodes reside on the same
    machine, not to mention the fact that our Spark application will take up all the
    cores available on that machine by default. The important feature of this mode
    that makes all this possible is the interactive (Spark) Scala shell.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.3 is the latest version. It comes with over 1,400 fixes. A Spark 2.3
    installation on Java 8 might be the first thing to do before we get started on
    our next project in [Chapter 2](a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml), *Build
    a Breast Cancer Prognosis Pipeline with the Power of Spark and Scala.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Without further ado, let''s get started setting up Spark in standalone deploy
    mode. The following sequence of instructions are helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: 'System checks: First make sure you have at least 8 GB of memory, leaving at
    least 75% of this memory for Spark. Mine has 32 GB. Once the system checks pass,
    download the Spark 2.3.1 binary from here: [http://spark.apache.org/downloads.html.](http://spark.apache.org/downloads.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will need a decompression utility capable of extracting the `.tar.gz` and `.gz` archives
    because Windows does not have native support for these archives. 7-Zip is a suitable
    program for this. You can obtain it from [http://7-zip.org/download.html](http://7-zip.org/download.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the package type prebuilt for Apache Hadoop 2.7 and later and download
    `spark--2.2.1-bin-hadoop2.7.tgz`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the package to someplace convenient, which will become your Spark root
    folder. For example, my Spark root folder is: `C:\spark-2.2.1-bin-hadoop2.7`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, set up the environment variable, `SPARK_HOME` pointing to the Spark root
    folder. We would also need a path entry in the `PATH` variable to point to `SPARK_HOME/bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, set up the environment variable, `HADOOP_HOME`, to, say, `C:\Hadoop`,
    and create a new path entry for Spark by pointing it to the `bin` folder of the
    Spark home directory. Now, launch `spark-shell` like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'What happens next might frustrate Windows users. If you are one of those users,
    you will run into the following error. The following screenshot is a representation
    of this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3efbec97-5ec2-46dc-b401-c0436cd0fbec.png)'
  prefs: []
  type: TYPE_IMG
- en: Error message on Windows
  prefs: []
  type: TYPE_NORMAL
- en: 'To get around this issue, you may proceed with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new folder as `C\tmp\hive`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then get the missing `WINUTILS.exe` binary from here: [https://github.com/steveloughran/winutils](https://github.com/steveloughran/winutils).
    Drop this into `C\Hadoop\bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding step 2 is necessary because the Spark download does not contain
    the `WINUTILS.exe` that is required to run Hadoop. That, then, is the source of
    the `java.io.IOException`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that knowledge, open up the Command Prompt window in administrator mode and
    execute the newly downloaded `WINUTILS.EXE` like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, issue the `spark-shell` command. This time around, Spark's interactive
    development environment launches normally, spinning up its own `SparkContext`
    instance `sc` and a `SparkSession` `spark` session, respectively. While the `sc` feature
    is a powerful entry point to the underlying local standalone cluster, `spark`
    is the main entry point to Spark's data processing APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output from the `spark-shell` command. `SparkContext` is
    made available to you as `sc` and the Spark session is available to you as `spark`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `local[2]` option in the `spark-shell` launch shown earlier lets us run
    Spark locally with `2` threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into the next topic in this section, it is a good idea to understand
    the following Spark shell development environment features that make development
    and data analysis possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparkBuilder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparkContext`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SparkConf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SparkSession` API ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.SparkSession))
    describes `SparkSession` as a programmatic access entry point to Spark's dataset
    and dataframe APIs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is `SparkBuilder`? The `SparkBuilder` companion object contains a `builder`
    method, which, when invoked, allows us to retrieve an existing `SparkSession`
    or even create one. We will now obtain our `SparkSession` instance in a two-step
    process, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `SparkSession` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `builder` method with `getOrCreate` on the resulting `builder`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `SparkContext` API ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.SparkContext))
    describes `SparkContext` as a first-line entry point for setting or configuring
    Spark cluster properties (RDDs, accumulators, broadcast variables, and much more)
    affecting the cluster's functionality. One way this configuration happens is by
    passing in a `SparkConf` instance as a `SparkContext` constructor parameter. One
    `SparkContext` exists per JVM instance.
  prefs: []
  type: TYPE_NORMAL
- en: In a sense, `SparkContext` is also how a Spark driver application connects to
    a cluster through, for example, Hadoop's Yarn **ResourceManager** (**RM**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect our Spark environment now. We will start by launching the Spark
    shell. That said, a typical Spark shell interactive environment screen has its
    own SparkSession available as  `spark`,   whose value we try to read off in the
    code block as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark shell also boasts of its own `SparkContext` instance `sc`, which
    is associated with `SparkSession` `spark`. In the following code, `sc` returns `SparkContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`sc` can do more. In the following code, invoking the `version` method on `sc` gives
    us the version of Spark running in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `sc` represents a connection to the Spark cluster, it holds a special
    object called `SparkConf`, holding cluster configuration properties in an `Array`. Invoking
    the `getConf` method on the `SparkContext` yields `SparkConf`, whose `getAll`
    method (shown as follows) yields an `Array` of cluster (or connection) properties,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There may be references to `sqlContext` and `sqlContext.implicits._` in the
    Spark shell. What is `sqlContext`? As of Spark 2 and the preceding versions, `sqlContext`
    is deprecated and `SparkSession.builder` is used instead to return a `SparkSession`
    instance, which we reiterate is the entry point to programming Spark with the
    dataset and dataframe API. Hence, we are going to ignore those `sqlContext` instances
    and focus on `SparkSession` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that `spark.app.name` bears the default name `spark-shell`. Let''s assign
    a different name to the `app-name` property as `Iris-Pipeline`. We do this by
    invoking the `setAppName` method and passing to it the new app name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To check if the configuration change took effect, let''s invoke the `getAll`
    method again. The following output should reflect that change. It simply illustrates
    how `SparkContext` can be used to modify our cluster environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `spark.app.name` property just had its value updated to the new name. Our
    goal in the next section is to use `spark-shell` to analyze data in an interactive
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a simple interactive data analysis utility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will develop a simple Scala program in the Spark shell's interactive Scala
    shell. We will restate our goal, which is that we want to be able to analyze data
    interactively. That dataset—an external **comma-separated values** (**CSV**) file
    called `iris.csv`—resides in the same folder where `spark-shell` is launched from.
  prefs: []
  type: TYPE_NORMAL
- en: 'This program, which could just as well be written in a regular Scala **Read
    Eval Print Loop **(**REPL**) shell, reads a file, and prints out its contents,
    getting a data analysis task done. However, what is important here is that the
    Spark shell is flexible in that it also allows you to write Scala code that will
    allow you to easily connect your data with various Spark APIs and derive abstractions,
    such as dataframes or RDDs, in some useful way. More about `DataFrame` and `Dataset`
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8fa8a32-975c-4def-9bca-edae287ad5f0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Reading iris.csv with source
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding program, nothing fancy is happening. We are trying to read
    a file called `iris.csv` using the `Source` class. We import the `Source.scala`
    file from the `scala.io` package and from there on, we create an object called
    `DataReader` and a `main` method inside it. Inside the `main` method, we invoke
    the `fromFile` method of the companion object `Source`. The `fromFile` method
    takes in a string representation of the dataset file path as an argument and returns
    a `BufferedSource` instance, which we assign to a `val` that we name `datasrc`.
    By the way, the API for `Source` can be found at [https://www.scala-lang.org/api/current/scala/io/Source.html.](https://www.scala-lang.org/api/current/scala/io/Source.html)
  prefs: []
  type: TYPE_NORMAL
- en: On the `BufferedSource` handle, we then invoke the `getLines` method that returns
    an iterator, which in turn invokes `foreach` that will print out all the lines
    in `iris.csv` minus the newline characters. We wrap all of this code in a `try`
    and a `catch` and a `finally`. The `finally` construct exists for a reason and
    that has to do with the fact that we need to close the `BufferedSource` instance
    `datasrc` after it is done working on the file.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we ran into a `FileNotFoundException` because the dataset file `iris.csv`
    was not found. The CSV file is then dropped in, the program is run, and the output
    is what we expect.
  prefs: []
  type: TYPE_NORMAL
- en: That wasn't so hard. In the next subsection, the goal is to read our `iris.csv`
    file and derive `Dataset` or `DataFrame` out of it.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a data file and deriving DataFrame out of it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark API for [https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.Dataset) has
    it that a `DataFrame` is `Dataset[Row]` and that `Dataset` contains a view called
    `DataFrame`. Falling back to the description of `Dataset` in the Spark documentation,
    we can redefine `Dataset` as a Spark abstraction of distributed collections holding
    data items. That said, `Dataset[Row]` contains rows. `Row` could be an abstraction
    representing a row from the raw file dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We need to read the `iris.csv` file and transform it into `DataFrame`. That
    is the stated goal of this subsection and that is exactly what we shall accomplish
    very soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this in mind, lets get down to building `DataFrame`. We start by invoking
    the `read` method on `spark`, our `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `read()` invoke produced `DataFrameReader` `dfReader1`, which according
    to [https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader) is
    an interface to load a dataset from external storage systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will inform Spark that our data is in CSV format. This is done by
    invoking the `format` method with a `com.databricks.spark.csv` argument that Spark
    recognizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `format` method simply returned `DataFrameReader` again. The `iris.csv`
    file contains `header`. We could specify this as an input `option`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That returned our same old `DataFrameReader`.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need next is a way to identify the schema for us. Invoking the `option`
    method again with a key `inferSchema` and a value of `true` lets Spark infer the
    schema automatically for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s `load` our input now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`DataFrameReader` transformed our input CSV into `DataFrame`! This was exactly
    what we set out to do.'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` is simply an untyped view of `Dataset` as `type DataFrame = Dataset[Row]`.'
  prefs: []
  type: TYPE_NORMAL
- en: With our `DataFrame` being a view on `Dataset[Row]`, all the methods on `Dataset`
    are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we want to see what this dataset has in it. The raw file had 150 columns
    in it. Therefore, we want Spark to:'
  prefs: []
  type: TYPE_NORMAL
- en: Return the row count in our dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the top 20 rows of our dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will invoke the `count` method. We want to reaffirm the number of
    rows contained in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We just invoked the `count` method on our `DataFrame`. That returned the number
    `150`, which is right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will bring together all of the code developed in this section into
    one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We just created `DataFrame` `irisDataFrame` . If you want to view the DataFrame, 
    just invoke the `show` method on it. This will return the first 20 rows of the
    irisDataFrame  `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2667d94e-9b3a-41bd-802c-c9719b96c6df.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First 20 rows of the Iris dataset
  prefs: []
  type: TYPE_NORMAL
- en: At this point, type `:quit` or *Ctrl* + *D* to exit the Spark shell. This wraps
    up this section, but opens a segue to the next, where we take things to the next
    level. Instead of relying on `spark-shell` to develop a larger program, we will
    create our Iris prediction pipeline program in an SBT project. This is the focus
    of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Iris pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will set forth what our pipeline implementation objectives
    are. We will document tangible results as we step through individual implementation
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Before we implement the Iris pipeline, we want to understand what a pipeline
    is from a conceptual and practical perspective. Therefore, we define a pipeline
    as a `DataFrame` processing workflow with multiple pipeline stages operating in
    a certain sequence.
  prefs: []
  type: TYPE_NORMAL
- en: A DataFrame is a Spark abstraction that provides an API. This API lets us work
    with collections of objects.  At a high-level it represents a distributed collection
    holding rows of data, much like a relational database table. Each member of a
    row (for example, a Sepal-Width measurement) in this DataFrame falls under a named
    column called Sepal-Width.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each stage in a pipeline is an algorithm that is either a  `Transformer` or
    an  `Estimator`.   As a `DataFrame` or DataFrame(s) flow through the pipeline, 
    two types of  stages (algorithms) exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Transformer` stage:  This involves a transformation action that transforms
    one `DataFrame` into another `DataFrame`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Estimator` stage: This involves a training action on a `DataFrame` that produces
    another `DataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, a pipeline is a single unit, requiring stages, but inclusive of
    parameters and DataFrame(s). The entire pipeline structure is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Transformer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Estimator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Parameters` (hyper or otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where Spark comes in. Its MLlib library provides a set of pipeline APIs
    allowing developers to access multiple algorithms and facilitates their combining
    into a single pipeline of ordered stages, much like a sequence of choreographed
    motions in a ballet. In this chapter, we will use the random forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: We covered essential pipeline concepts. These are practicalities that will help
    us move into the section, where we will list implementation objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Iris pipeline implementation objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before listing the implementation objectives, we will lay out an architecture
    for our pipeline. Shown here under are two diagrams representing an ML workflow,
    a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagrams together help in understanding the different components
    of this project. That said, this pipeline involves training (fitting), transformation,
    and validation operations. More than one model is trained and the best model (or
    mapping function) is selected to give us an accurate approximation predicting the
    species of an Iris flower (based on measurements of those flowers):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ea4c627-29bd-46c6-b668-41beeb8136f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Project block diagram
  prefs: []
  type: TYPE_NORMAL
- en: 'A breakdown of the project block diagram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark**, which represents the Spark cluster and its ecosystem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training dataset**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset attributes** or feature measurements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **inference** process, that produces a prediction column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram represents a more detailed description of the different
    phases in terms of the functions performed in each phase. Later we will come to
    visualize pipeline in terms of its constituent stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, the diagram depicts four stages, starting with a **data pre-processing**
    phase, which is considered separate from the numbered phases deliberately. Think
    of the pipeline as a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: A **data cleansing** phase, or **pre-processing** phase. An important phase
    that could include a subphase of **Exploratory Data Analysis** (**EDA**) (not
    explicitly depicted in the latter diagram).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A data analysis phase that begins with **Feature Extraction**, followed by
    **Model Fitting**, and **Model validation**, all the way to deployment of an Uber
    pipeline JAR into Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/521c0e06-1391-4a31-9d5b-e5d9de1e7f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Pipeline diagram
  prefs: []
  type: TYPE_NORMAL
- en: Referring to the preceding diagram, the first implementation objective is to
    set up Spark inside an SBT project. An SBT project is a self-contained application,
    which we can run on the command line to predict Iris labels. In the SBT project, 
    dependencies are specified in a `build.sbt` file and our application code will
    create its  own  `SparkSession` and `SparkContext`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So that brings us to a listing of implementation objectives and these are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the Iris dataset from the UCI Machine Learning Repository
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conduct preliminary EDA in the Spark shell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new Scala project in IntelliJ, and carry out all implementation steps,
    until the evaluation of the Random Forest classifier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the application to your local Spark cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 1 – getting the Iris dataset from the UCI Machine Learning Repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Head over to the UCI Machine Learning Repository website at [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)
    and click on Download: Data Folder. Extract this folder someplace convenient and
    copy over `iris.csv` into the root of your project folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may refer back to the project overview for an in-depth description of the
    Iris dataset. We depict the contents of the `iris.csv` file here, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71fbb790-c523-461f-b89a-951043a47d3b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A snapshot of the Iris dataset with 150 sets
  prefs: []
  type: TYPE_NORMAL
- en: You may recall that the `iris.csv` file is a 150-row file, with comma-separated
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the dataset, the first step will be performing EDA on it. The
    Iris dataset is multivariate, meaning there is more than one (independent) variable,
    so we will carry out a basic multivariate EDA on it. But we need `DataFrame` to
    let us do that. How we create a dataframe as a prelude to EDA is the goal of the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – preliminary EDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get down to building the SBT pipeline project, we will conduct a preliminary
    EDA in `spark-shell`. The plan is to derive a dataframe out of the dataset and
    then calculate basic statistics on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have three tasks at hand for `spark-shell`:'
  prefs: []
  type: TYPE_NORMAL
- en: Fire up `spark-shell`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `iris.csv` file and build `DataFrame`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the statistics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will then port that code over to a Scala file inside our SBT project.
  prefs: []
  type: TYPE_NORMAL
- en: That said, let's get down to loading the `iris.csv` file (inputting the data
    source) before eventually building `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Firing up Spark shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fire up the Spark Shell by issuing the following command on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, we start with the available Spark session 'spark'.  'spark'
    will be our entry point to programming with Spark. It also holds properties required
    to connect to our Spark (local) cluster. With this information, our next goal
    is to load the iris.csv file and produce a DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Loading the iris.csv file and building a DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step to loading the iris csv file is to invoke the `read` method
    on `spark`. The `read` method returns `DataFrameReader`, which can be used to
    read our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`dfReader1` is of type `org.apache.spark.sql.DataFrameReader`. Calling the
    `format` method on `dfReader1` with Spark''s `com.databricks.spark.csv` CSV format-specifier
    string returns `DataFrameReader` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After all, `iris.csv` is a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, `dfReader1` and `dfReader2` are the same `DataFrameReader`
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, `DataFrameReader` needs an input data source `option` in the
    form of a key-value pair. Invoke the `option` method with two arguments, a key
    `"header"` of type string and its value `true` of type Boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we invoke the `option` method again with an argument `inferSchema`
    and a `true` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: What is `inferSchema` doing here? We are simply telling Spark to guess the schema
    of our input data source for us.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have been preparing `DataFrameReader` to load `iris.csv`. External
    data sources require a path for Spark to load the data for `DataFrameReader` to
    process and spit out `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time is now right to invoke the `load` method on `DataFrameReader` `dfReader4`.
    Pass into the `load` method the path to the Iris dataset file. In this case, the
    file is right under the root of the project folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: That's it. We now have `DataFrame`!
  prefs: []
  type: TYPE_NORMAL
- en: Calculating statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Invoking the `describe` method on this `DataFrame` should cause Spark to perform
    a basic statistical analysis on each column of `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Lets fix the `WARN.Utils` issue described in the preceding code block. The fix
    is to locate the file `spark-defaults-template.sh` under `SPARK_HOME/conf` and
    save it as `spark-defaults.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the bottom of this file, add an entry for `spark.debug.maxToStringFields`.
    The following screenshot illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46d332b7-6858-429d-a493-bbb6b451cb28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fixing the WARN Utils problem in spark-defaults.sh
  prefs: []
  type: TYPE_NORMAL
- en: Save the file and restart `spark-shell`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, inspect the updated Spark configuration again. We updated the value of
    `spark.debug.maxToStringFields` in the `spark-defaults.sh` file. This change is
    supposed to fix the truncation problem reported by Spark. We will confirm imminently
    that the change we made caused Spark to update its configuration also. That is
    easily done by inspecting `SparkConf`.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting your SparkConf again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As before, invoking the `getConf` returns the `SparkContext` instance that
    stores configuration values. Invoking `getAll` on that instance returns an `Array`
    of configuration values. One of those values is an updated value of `spark.debug.maxToStringFields`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: That updated value for `spark.debug.maxToStringFields` is now `150`.
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.debug.maxToStringFields` had a default value of `25` inside a private
    object called `Utils`.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating statistics again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the invoke on the dataframe `describe` method and pass to it column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The invoke on the `describe` method of `DataFrame` `dfReader` results in a
    transformed `DataFrame` that we call dFrame2.  On dFrame2, we invoke the `show`
    method to return a table of statistical results. This completes the first phase
    of a basic yet important EDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the statistical analysis are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44bb7dee-e4c6-4dde-8476-ccc52bbd88ca.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Results of statistical analysis
  prefs: []
  type: TYPE_NORMAL
- en: 'We did all that extra work simply to demonstrate the individual data reading,
    loading, and transformation stages. Next, we will wrap all of our previous work
    in one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: That completes the EDA on `spark-shell`. In the next section, we undertake steps
    to implement, build (using SBT), deploy (using `spark-submit`), and execute our
    Spark pipeline application. We start by creating a skeletal SBT project.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – creating an SBT project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lay out your SBT project in a folder of your choice and name it `IrisPipeline`
    or any name that makes sense to you. This will hold all of our files needed to
    implement and run the pipeline on the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of our SBT project looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff1f985f-e989-4cd6-944f-9b8fd4a473be.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Project structure
  prefs: []
  type: TYPE_NORMAL
- en: 'We will list dependencies in the `build.sbt` file. This is going to be an SBT
    project. Hence, we will bring in the following key libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark Core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates the `build.sbt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92bebe43-485c-4ddb-98b2-5cff6f2efa44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The build.sbt file with Spark dependencies
  prefs: []
  type: TYPE_NORMAL
- en: The `build.sbt` file referenced in the preceding snapshot is readily available
    for you in the book's download bundle. Drill down to the folder `Chapter01` code
    under `ModernScalaProjects_Code` and copy the folder over to a convenient location
    on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Drop the `iris.csv` file that you downloaded in *Step 1 – getting the Iris dataset
    from the UCI Machine Learning Repository* into the root folder of our new SBT
    project. Refer to the earlier screenshot that depicts the updated project structure
    with the `iris.csv` file inside of it.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – creating Scala files in SBT project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 4 is broken down into the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the Scala file `iris.scala` in the `com.packt.modern.chapter1` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Up until now, we relied on `SparkSession` and `SparkContext`, which `spark-shell`
    gave us. This time around, we need to create `SparkSession`, which will, in turn,
    give us `SparkContext`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What follows is how the code is laid out in the `iris.scala` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `iris.scala`, after the package statement, place the following `import`
    statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `SparkSession` inside a trait, which we shall call `IrisWrapper`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Just one `SparkSession` is made available to all classes extending from `IrisWrapper`.
    Create `val` to hold the `iris.csv` file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method to build `DataFrame`. This method takes in the complete path
    to the Iris dataset path as `String` and returns `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `DataFrame` class by updating the previous `import` statement for
    `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a nested function inside `buildDataFrame` to process the raw dataset.
    Name this function `getRows`. `getRows` which takes no parameters but returns
    `Array[(Vector, String)]`. The `textFile` method on the `SparkContext` variable
    processes the `iris.csv` into `RDD[String]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The resulting RDD contains two partitions. Each partition, in turn, contains
    rows of strings separated by a newline character, `'\n'`. Each row in the RDD
    represents its original counterpart in the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will attempt several data transformation steps. We start
    by applying a `flatMap` operation over the RDD, culminating in the `DataFrame`
    creation. `DataFrame` is a view over `Dataset`, which happens to the fundamental
    data abstraction unit in the Spark 2.0 line.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – preprocessing, data transformation, and DataFrame creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will get started by invoking `flatMap`, by passing a function block to it,
    and successive transformations listed as follows, eventually resulting in `Array[(org.apache.spark.ml.linalg.Vector,
    String)]`. A vector represents a row of feature measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scala code to give us `Array[(org.apache.spark.ml.linalg.Vector, String)]`
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, drop the `header` column, but not before doing a collection that returns
    an `Array[Array[String]]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The header column is gone; now import the `Vectors` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, transform `Array[Array[String]]` into `Array[(Vector, String)]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The last step remaining is to create a final DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we invoke the `createDataFrame` method with a parameter, `getRows`. This
    returns `DataFrame` with `featureVector` and `speciesLabel` (for example, Iris-setosa):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the top 20 rows in the new dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We need to index the species label column by converting the strings Iris-setosa,
    Iris-virginica, and Iris-versicolor into doubles. We will use a `StringIndexer`
    to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Now create a file called `IrisPipeline.scala`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an object `IrisPipeline` that extends our `IrisWrapper` trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `StringIndexer` algorithm class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a `StringIndexer` algorithm instance. The `StringIndexer` will map
    our species label column to an indexed learned column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Step 6 – creating, training, and testing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s split our dataset in two by providing a random seed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our new `splitDataset` contains two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train dataset:** A dataset containing `Array[(Vector, iris-species-label-column:
    String)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test dataset:** A dataset containing `Array[(Vector, iris-species-label-column:
    String)]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confirm that the new dataset is of size `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the training dataset to a variable, `trainSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the testing dataset to a variable, `testSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of rows in the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of rows in the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: There are 150 rows in all.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – creating a Random Forest classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reference to Step 5 - DataFrame Creation. This DataFrame 'dataFrame' contains
    column names that corresponds to the columns present in the DataFrame produced
    in that step
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to create a classifier is to  pass into it (hyper) parameters.
    A fairly comprehensive list of parameters look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: From 'dataFrame' we need the Features column name - **iris-features-column**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From 'dataFrame' we also need the Indexed label column name - **iris-species-label-column**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sqrt` setting for `featureSubsetStrategy`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of features to be considered per split (we have 150 observations and
    four features that will make our `max_features` value `2`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impurity settings—values can be gini and entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of trees to train (since the number of trees is greater than one, we
    set a tree maximum depth), which is a number equal to the number of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required minimum number of feature measurements (sampled observations),
    also known as the minimum instances per node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at the `IrisPipeline.scala` file for values of each of these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: But this time, we will employ an exhaustive grid search-based model selection
    process based on combinations of parameters, where parameter value ranges are
    specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `randomForestClassifier` instance. Set the features and `featureSubsetStrategy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Start building `Pipeline`, which has two stages, `Indexer` and `Classifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Next, set the hyperparameter `num_trees` (number of trees) on the classifier
    to `15`, a `Max_Depth` parameter, and an impurity with two possible values of
    gini and entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a parameter grid with all three hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Step 8 – training the Random Forest classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we want to split our training set into a validation set and a training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'On this variable, set `Seed`, set `EstimatorParamMaps`, set `Estimator` with
    `irisPipeline`, and set a training ratio to `0.8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Finally, do a fit and a transform with our training dataset and testing dataset.
    Great! Now the classifier is trained. In the next step, we will apply this classifier
    to testing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Step 9 – applying the Random Forest classifier to test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of our validation set is to be able to make a choice between models.
    We want an evaluation metric and hyperparameter tuning. We will now create an
    instance of a validation estimator called `TrainValidationSplit`, which will split
    the training set into a validation set and a training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Next, we fit this estimator over the training dataset to produce a model and
    a transformer that we will use to transform our testing dataset. Finally, we perform
    a validation for hyperparameter tuning by applying an evaluator for a metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new `ValidatedTestResults` `DataFrame` should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s return a new dataset by passing in column expressions for `prediction`
    and `label`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'In the line of code, we produced a new `DataFrame` with two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: An input label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A predicted label, which is compared with its corresponding value in the input
    label column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That brings us to the next step, an evaluation step. We want to know how well
    our model performed. That is the goal of the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Step 10 – evaluate Random Forest classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will test the accuracy of the model. We want to know how
    well our model performed. Any ML process is incomplete without an evaluation of
    the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, we perform an evaluation as a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the model output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass in three hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Set the label column, a metric name, the prediction column `label`, and invoke
    evaluation with the `validatedTestResults` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note the accuracy of the model output results on the testing dataset from the
    `modelOutputAccuracy` variable.
  prefs: []
  type: TYPE_NORMAL
- en: The other metrics to evaluate are how close the predicted label value in the
    `'predicted'` column is to the actual label value in the (indexed) label column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we want to extract the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Our pipeline produced predictions. As with any prediction, we need to have
    a healthy degree of skepticism. Naturally, we want a sense of how our engineered
    prediction process performed. The algorithm did all the heavy lifting for us in
    this regard. That said, everything we did in this step was done for the purpose
    of evaluation. Who is being evaluated here or what evaluation is worth reiterating? That
    said, we wanted to know how close the predicted values were compared to the actual
    label value. To obtain that knowledge, we decided to use the `MulticlassMetrics`
    class to evaluate metrics that will give us a measure of the performance of the
    model via two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following lines of code will give us value of Accuracy and Weighted Precision.
    First we will create an accuracyMetrics tuple, which should contain the values
    of both accuracy and weighted precision
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Obtain the value of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Next, obtain the value of weighted precision.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: These metrics represent evaluation results for our classifier or classification
    model. In the next step, we will run the application as a packaged SBT application.
  prefs: []
  type: TYPE_NORMAL
- en: Step 11 – running the pipeline as an SBT application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the root of your project folder, issue the `sbt console` command, and in
    the Scala shell, import the `IrisPipeline` object and then invoke the `main` method
    of `IrisPipeline` with the argument `iris`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will show you how to package the application so that
    it is ready to be deployed into Spark as an Uber JAR.
  prefs: []
  type: TYPE_NORMAL
- en: Step 12 – packaging the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the root folder of your SBT application, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'When SBT is done packaging, the Uber JAR can be deployed into our cluster,
    using `spark-submit`, but since we are in standalone deploy mode, it will be deployed
    into `[local]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfb1ec64-d3dd-4ec2-aa50-49d358ac5a6f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The application JAR file
  prefs: []
  type: TYPE_NORMAL
- en: The package command created a JAR file that is available under the target folder.
    In the next section, we will deploy the application into Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Step 13 – submitting the pipeline application to Spark local
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the root of the application folder, issue the `spark-submit` command with
    the class and JAR file path arguments, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything went well, the application does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads up the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performs EDA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates training, testing, and validation datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates a Random Forest classifier model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trains the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tests the accuracy of the model. This is the most important part—the ML classification
    task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To accomplish this, we apply our trained Random Forest classifier model to the
    test dataset. This dataset consists of Iris flower data of so far not seen by
    the model. Unseen data is nothing but Iris flowers picked in the wild.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying the model to the test dataset results in a prediction about the species
    of an unseen (new) flower.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last part is where the pipeline runs an evaluation process, which essentially
    is about checking if the model reports the correct species.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, pipeline reports back on how important a certain feature of the Iris
    flower turned out to be. As a matter of fact, the petal width turns out to be
    more important than the sepal width in carrying out the classification task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That brings us to the last section of this chapter. We will summarize what we
    have learned. Not only that, we will give readers a glimpse into what they will
    learn in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we implemented an ML workflow or an ML pipeline. The pipeline
    combined several stages of data analysis into one workflow. We started by loading
    the data and from there on, we created training and test data, preprocessed the
    dataset, trained the `RandomForestClassifier` model, applied the Random Forest
    classifier to test data, evaluated the classifier, and computed a process that
    demonstrated the importance of each feature in the classification. We fulfilled
    the goal that we laid out early on in the *Project overview – problem formulation*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will analyze the **Wisconsin Breast Cancer Data Set**.
    This dataset has only categorical data. We will build another pipeline, but this
    time, we will set up the Hortonworks Development Platform Sandbox to develop and
    deploy a breast cancer prediction pipeline. Given a set of categorical feature
    variables, this pipeline will predict whether a given sample is benign or malignant.
    In the next and the last section of the current chapter, we will list a set of
    questions that will test your knowledge of what you have learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a list of questions for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: What do you understand by EDA? Why is it important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we create training and test data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why did we index the data that we pulled from the UCI Machine Learning Repository?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the Iris dataset so famous?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name one powerful feature of the random forest classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is supervisory as opposed to unsupervised learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain briefly the process of creating our model with training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are feature variables in relation to the Iris dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the entry point to programming with Spark?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Task: The Iris dataset problem was a statistical classification problem. Create
    a confusion or error matrix with the rows being predicted setosa, predicted versicolor,
    and predicted virginica, and the columns being actual species, such as setosa,
    versicolor, and virginica. Having done that, interpret this matrix.'
  prefs: []
  type: TYPE_NORMAL
