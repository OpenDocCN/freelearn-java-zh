- en: Scaling Out with the Fan-Out Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next turn in our serverless journey takes us away from web-centric patterns
    and towards those suitable for a variety of problems, web and otherwise. In this
    chapter, we'll discuss the fan-out pattern, which may be used in many different
    contexts, either by itself as a standalone system or within a larger project as
    a sub-unit. Conceptually, the fan-out pattern is precisely what it sounds like—one
    serverless entry point results in multiple invocations of downstream systems.
    Big data platforms and computer science algorithms have been using this trick
    for a very long time; by taking a sizable computational problem and breaking it
    into smaller pieces, a system can get to the result faster by working on those
    smaller pieces concurrently. Conceptually, this is precisely how MapReduce works
    in the mapping step.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to split a single unit of work into multiple
    smaller groups of work using the fan-out pattern. We will go through use cases
    for this pattern and the various problems for which it's well suited.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you can expect to know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to set up a fan-out architecture for resizing images in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the fan-out pattern to split a single large input file into smaller
    files and process those pieces in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What types of problem is the fan-out pattern is suitable for?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many ways, this is the most straightforward pattern covered in this book.
    A single entry point, whether it be an HTTP request, some event notification,
    or anything else supported on your cloud provider of choice, triggers multiple invocations of
    some other serverless function in parallel. What one gains in this architecture
    is parallelism and hence speed. Our first example is one which is easy to understand
    and which you can view as the `Hello World` of serverless architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a system which takes an image and creates multiple versions of the
    original image with different sizes smaller than the original. How can this be
    solved at its simplest? Once a user uploads an image, our system notices the new
    image upload and, using a `for` loop, iterates and creates the various thumbnails.
    Some fictitious code to do this may look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This can work just fine but runs the risk of slowing down drastically as a
    single process is in charge of the entire pipeline for one image. Logically speaking,
    each resize event is completely independent, only depending on the original image
    to perform its task. As such, this is a perfect task to run in parallel. The following
    diagram shows the general design for the fan-out pattern, where a single entry
    point triggers multiple downstream processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94ee5054-cece-42f2-b8f4-547b50747179.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some event or trigger will result in a call to an entry point function. In the
    image resize example, this event occurs when an image is uploaded to an AWS S3
    bucket. Admittedly, setting this up is very simple, and AWS makes the invocation
    of Lambda functions quite easy due to all their cross-service integrations. However,
    one may apply this pattern to any cloud provider, and the trigger could very well
    be the uploading of an image over an `HTTP POST` rather than to an S3 bucket.
    Once the entry function is invoked, it will be responsible for triggering multiple
    `worker` functions, passing them the needed data to do their jobs. The key to
    this entire architecture is that the triggering of the worker processes occurs
    in such a way that they all run in parallel, or as close to parallel as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous versus asynchronous invocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sole job of the entry point function is to initiate the fan-out and distribute
    work to multiple sub-functions. With this design, it's important to remember our
    goal, to parallelize the workload such that all of the worker functions are running
    in parallel. Just as our naive implementation with a `for` loop works synchronously,
    it's entirely possible to attempt to build an architecture as pictured above,
    but wind up with one which is synchronous. How can this happen?
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, the entry point must make multiple calls to kick off the sub-tasks.
    For that, we''ll use some form of looping. Using AWS as an example, the entry
    point can use the AWS APIs to invoke Lambda functions within that loop. The following
    code block demonstrates invoking a `lambda` function via the JavaScript AWS SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s safe to ignore much of the detail in the preceding code block. At a high
    level, our Node.js code iterates around an array of three items and invokes a
    named Lambda function each time. You may think that this code implements the architecture
    shown in the diagram above. However, running this, you''d quickly learn that this
    code operates entirely synchronously. That is, each iteration of the loop waits
    for a response to be returned from the `lambda.invoke` call. The reason for this
    is that, by default, Lambda invocation APIs assume a request type of request/response.
    More plainly, the default mode of operation is synchronous, where the client is
    expecting a return value from the invoked function. The good news is that this
    is trivial to fix by calling the `invoke` function with the correct parameter,
    which instructs it that we don''t care about a return value. Merely add `InvocationType:
    "Event"` to the `params` and you''re all done.'
  prefs: []
  type: TYPE_NORMAL
- en: Resizing images in parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example will be implemented in Node.js for no other reason than to change
    things from the Python code in previous chapters. There is a single dependency
    in this example, which we use for the image resizing, called `jimp`. I'll touch
    on some of the steps to get going with a new Node project using the Serverless
    Framework.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting up a new Node.js project isn''t any different from doing so with any
    other supported language. We''ll tell serverless to use the `aws-nodejs` template
    and name our project `fanout`. The `-p` argument simply tells Serverless to place
    all of the generated code in the `serverless` directory, which is relative to
    the location where we execute this command. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll add our single dependency for `jimp`. Here, I''m using `yarn`,
    but `npm` works fine as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all that is done, our code layout looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Setting up trigger and worker functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, let''s wire up the `trigger` function and define the `worker` function.
    As noted earlier, this entire process will begin upon uploading an image to S3\.
    The Serverless Framework makes this type of wiring straightforward with the `events`
    section in `serverless.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What this says is that the `uploadImage` function will be called whenever an
    object is created in the named S3 bucket. That's all there is to it. Again, this
    event could have been anything else supported in AWS, provided the trigger gives
    access to some image which needs resizing. If you're using a cloud provider other
    than AWS, you'll need to figure out which trigger makes sense for your platform.
  prefs: []
  type: TYPE_NORMAL
- en: You'll also notice the definition of the `ResizeImage` function. What's curious
    is that there are no `events` listed. That is because, in our case, the `uploadImage`
    function will act as the trigger, calling this Lambda function manually using `aws-sdk`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up permissions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with all things AWS, we'll need to ensure IAM permissions are set up correctly.
    The `UploadImage` function will interact with a single AWS resource other than
    itself, and that is the `ResizeImage` function. For `UploadImage` to invoke `ResizeImage`,
    we need to grant it explicit permission.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, `ResizeImage` needs access to write data to the final resting
    place of the resized photos. We'll place these images in a different S3 bucket
    and again grant access via the `iamRoleStatements` section.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see both of these statements in the following code, along with other
    configurations in the full `serverless.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From a security perspective, there is a slight imperfection in our IAM roles
    above since both functions are granted the same permissions. That is, `ResizeImage`
    is allowed to call itself, and `uploadImage` is allowed access to the results
    S3 bucket. Ideally, only the functions which need the permissions would be granted
    those permissions. It is possible to set up per-function IAM access using the
    Serverless Framework but it's a bit verbose and outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the application code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the setup done, we can now focus on the application code. Let's take a
    look at the `uploadImage` function, as that is the gateway to the entire process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to initialize our two dependencies at the top of this `handler.js`
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `aws-sdk`, which is automatically available in the Lambda runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `jimp` library for doing the image manipulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going from the top down, our `uploadImage` function defines a few things. First,
    our `params` object contains the base of the Lamda invocation parameters. Note
    here that we're using an `InvocationType` of `"Event"`, which is extremely important
    in order to get the asynchronous fan-out described earlier. Next, we'll hardcode
    a few image widths to which we'll resize the original image. `jimp` is capable
    of taking a single dimension (height or width) and automatically calculating the
    other dimension to retain the original aspect ratio.
  prefs: []
  type: TYPE_NORMAL
- en: The `uploadImage` function, when invoked, receives quite a bit of metadata about
    the invocation in the `event` parameter. In our case, the information about the
    uploaded images will be contained in this `event` object. All of that data ends
    up in an array of `Records`. In reality, there should only be a single record
    to deal with. Just to be safe, we'll continue working as if there are a variable
    number of items in here and grab them all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this function will iterate around the array of different sizes and
    invoke the appropriate callback as many times, with a slightly different payload.
    The list of `S3Objects` is the same for each iteration, but the size field for
    each `resizeImage` invocation will be different. The following code block shows
    the full implementation of the `uploadImage` function, which invokes the `ResizeImage`
    Lambda function asynchronously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With that, we can turn our attention to the work on actually resizing the images.
    Just as in `uploadImage`, `resizeImage` receives a payload of data in the `event`
    parameter, which is an object type. Remember that the `S3Objects` attribute passed
    over to this function is an array of S3 images. It's safe to say that this will
    be an array of length one for this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As this function iterates around the list of `S3Objects`, it will extract the
    pertinent data needed for it to perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the original image from S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resize the in-memory image contents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the resized image to a local buffer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the resized image to the destination bucket with the updated name
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code block shows the full implementation of the `resizeImage`
    function, which is responsible for downsizing an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Testing our code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test this, all that''s needed is to upload an image to our target directory.
    I''ll use one of my photos from the High Sierra in California, along with the
    AWS command-line interface. The original photo is 2,816 × 2,112 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeba7e7c-feff-48f6-830b-1488988bb923.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s inspect the logs from the `ResizeImage` function. What we expect to
    see is three invocations right around the same time. Bear in mind that these may
    finish at entirely different times since the workload between them may vary; however,
    the starting times should be very close together. Looking at the CloudWatch log
    results, we can see what we''re hoping for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0d66f3c-56cd-4a4d-84fc-1dc1ca12ca32.png)'
  prefs: []
  type: TYPE_IMG
- en: Success! Each one of these log streams corresponds to a unique invocation of
    `ResizeImage`. Additionally, the Last Event Time is precisely the same across
    all three invocations.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, each Log Streams corresponds to a single invocation, but that
    isn't always necessarily true. As more and more requests come in, CloudWatch will
    group Log statements into existing streams. Here, I started with no Log Streams
    at all for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s possible to view the logs in the AWS console or use the `sls logs` command
    to see them all together. Unfortunately, the start times are not automatically
    added to the `CloudWatch`, `Log` statement when using the AWS API (which is what
    `sls` commands ultimately use). However, we can see the results from any of our
    `console.log` statements along with the ending times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These results also make sense. The smallest resizing job uses the least amount
    of memory and completes first. The largest resize job uses the most memory and
    finishes last. We need to acknowledge that the 128 px job starts early and gets
    a tiny head start. However, looking at the duration, it's also clear that the
    total execution time is higher when the resized file is bigger. I suspect this
    is due to the uploading to S3 and not the resizing process itself. Regardless,
    for this example, it's unimportant which takes longer and why. What is important
    is that we now have a system which receives a single input and invokes multiple
    worker processes in parallel. Had this work been done synchronously, the execution
    would have taken approximately 20 seconds, which is the sum of all three resize
    durations. Using the fan-out pattern, this is cut down to 7 seconds, which is
    the time it takes for the longest running task to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking into the S3 bucket of the results, we can view the three new images
    with the new widths embedded in the name. Additionally, you can see the image
    sizes vary, where the smallest image has the smallest file size and the largest
    image the largest file size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Alternate Implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding example is merely one, albeit common, example of how you can implement
    a fan-out pattern. There are many ways to turn one event or trigger into multiple
    parallel processes. Some options for this are specific to the cloud service you're
    using. Since the vast majority of my cloud experience is with AWS, I'll cover
    some alternative architectures. These may be portable to other cloud providers
    with corollary service offerings under different names.
  prefs: []
  type: TYPE_NORMAL
- en: Using notifications with subscriptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My preceding example was controlled by a master receiver function, which was
    invoked on some trigger and then performed the work of calling the worker processes
    manually. One alternative is to replace the entry point function with a **Simple
    Notification Service** (**SNS**) topic. If you're unfamiliar with SNS, it's just
    what it sounds like—a system which, when triggered, notifies subscribers that
    something has happened.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our example was focused on transforming an image in multiple ways,
    it makes sense to set up a trigger when a new file is added. However, what happens
    when we want to start doing some processing when another type of event occurs?
    For example, a new user signs up on our website via their Facebook account, and
    we want to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Send them a welcome email
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up their account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull their Facebook social graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This workflow is all made up, but the main idea is the same—a single event results
    in multiple jobs, which may operate in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: In cases like this, an event of interest would trigger an SNS notification on
    a particular topic. SNS payloads can contain whatever your application code decides
    to send. Downstream, zero or more subscribers may be listening to that topic and
    choose to do some work when a new notification arrives. On AWS, Lambda functions
    may be triggered by SNS notifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our fan-out architecture looks slightly different if using SNS to trigger one
    or more Lambda workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4354b836-08d6-420d-8ffb-33d1ee9d847c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we're freed from the burden of keeping track of what worker processes need
    to be called when an interesting event occurs. With the SNS design, individual
    workers subscribe to the SNS topic and trigger upon receiving an SNS notification.
    It's important to note that each one of these workers is a unique Lambda function
    with separate application code. In case, where the parallel jobs are all performing
    disparate tasks, there is no problem, since these would always need to be separated
    out and managed as unique functions. This design fits very nicely when the work
    to be parallelized can occur based on a singular event, but where each job is
    unique and can stand alone. Crawling a user's Facebook social graph and inserting
    some records in your database are entirely separable. For that reason, this architecture
    is an excellent choice. When a new job needs to run based on the same event, the
    work involves implementing the application code and subscribing to the existing
    SNS topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model doesn''t work very well if all of the workers are performing the
    same task. The reason for this is that an SNS trigger occurs on a single topic
    and delivers the same payload to all subscribers. In the image resize example,
    the `UploadImage` function invoked `ResizeImage` three times with three different
    payloads. If we had built the image resize example with the SNS design, each resizing
    worker would need to be its own independently managed Lambda function with the
    knowledge of what size to use when resizing images. To be more clear, there would
    be three different Lambda functions which corresponded to the three different
    image sizes we wanted to resize:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ResizeImage256`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResizeImage512`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResizeImage1024`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we wanted to add a new image size, it would mean implementing a new function.
    That rapidly becomes unwieldy, and problems such as this aren't a good fit for
    this design.
  prefs: []
  type: TYPE_NORMAL
- en: Using notifications with queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another take on this is to use an SNS notification to deliver to multiple queues.
    When an SNS event is triggered, that notification takes place fairly quickly (a
    few seconds, at most). If there are 100 subscribers attached to that topic, all
    100 subscribers will wake up and start working in parallel. That behavior may
    be exactly what you need in certain scenarios. However, in those cases where you
    may not want your system to operate at full capacity, it''s possible to deliver
    SNS data to one or more **Simple Queuing Service** (**SQS**) queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf58db97-ceba-4de6-9811-4836491136a7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the subscribers to an SNS topic are SQS queues rather than Lamba functions.
    This sort of design may work well when you don't necessarily want, or need to
    keep up with, a high volume of events. By throwing data on queues, it's easier
    to control the consumption rate of the data.
  prefs: []
  type: TYPE_NORMAL
- en: SQS behaves as one would expect a queue to act; that is, data placed into the
    queue remains in the queue until some process comes along and consumes it, finally
    marking it as consumed. This pattern would be a great design to protect some backing
    service such as a relational database. Take the case where a high number of transactions
    arrives all at once and they ultimately need to be written to a database. Using
    the previous example, this could result in an equally high number of database
    writes since there is nothing to slow down the workers being invoked once the
    SNS event is triggered. To buffer that work, the SNS notifications trigger writes
    to the SQS queues, which result in all of the data queuing up for future processing.
    Workers process then poll the queues at some acceptable and known rate so as not
    to overwhelm the database either saturating the number of open connections or
    putting too much load on it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the fan-out pattern and discussed its overall merits
    and basic architecture when using a serverless platform. We discussed, in detail,
    the implementation of an example serverless application, which created multiple
    resized images in parallel using this pattern. In this example, we also learned
    the basics of deploying a Node.js application using the Serverless Framework on
    top of AWS. We also discussed different implementations of the fan-out pattern
    using different AWS services and when those alternative designs may be suitable.
  prefs: []
  type: TYPE_NORMAL
- en: Readers should understand the fan-out pattern well and be ready to use this
    pattern in future chapters in this book as part of more complex patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll work on processing data using queues and the messaging pattern.
  prefs: []
  type: TYPE_NORMAL
