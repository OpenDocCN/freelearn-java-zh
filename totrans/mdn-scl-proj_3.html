<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Stock Price Predictions</h1>
                </header>
            
            <article>
                
<p>The goal of this chapter is to predict the values of near-or long-term equity prices by using <strong>machine learning</strong> (<strong>ML</strong>). From an investor's perspective, investments (in equity) across several companies are stocks, while such investments in an individual company are shares. Most investors lean on a <span>long-term investment strategy for the best returns. Investment</span> analysts employ mathematical stock analysis models to <span>help predict future stock prices or price movements in the long term. Such models factor </span>in past <span>equity prices and other indicators to perform a company's financial health evaluation.</span></p>
<p>The overarching learning objective of this chapter is to implement a Scala solution that will predict stock market prices. Starting from the stock price prediction dataset, we will use the Spark ML library's ML APIs to build a stock price prediction pipeline.</p>
<p>Here is the dataset we will refer to:</p>
<ul>
<li>Daily News for Stock Market Prediction | Kaggle.</li>
<li>In-text: (<a href="http://Kaggle.com">Kaggle.com</a>, 2018) Daily News for Stock Market Prediction.</li>
<li>Kaggle. [online] Available at: <a href="https://www.kaggle.com/aaron7sun/stocknews">https://www.kaggle.com/aaron7sun/stocknews</a> [Accessed 27 Jul. 2018].</li>
</ul>
<p>The following list is a section-wise breakdown of the individual learning outcomes in this chapter:</p>
<ul>
<li>Stock price binary classification problem</li>
<li>Getting started</li>
<li>Implementation objective</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stock price binary classification problem</h1>
                </header>
            
            <article>
                
<p>Stock prices have a tendency to go up and down. We want to Spark ML and a Spark time-series library to explore historical stock price data going back a couple years and come up numbers like the average closing price. We also want our stock price prediction model to forecast what the stock price will be over the timeframe of a few days.</p>
<p><span>T</span><span>his chapter presents an ML methodology to reduce the complexity associated with stock price prediction. We will obtain a smaller set of optimal financial indicators by feature selection and employ a </span><span>Random Forest algorithm</span><span> to build a price prediction p</span><span>ipeline.</span></p>
<p>We must first download the dataset from the <kbd>ModernScalaProjects_Code</kbd> folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stock price prediction dataset at a glance</h1>
                </header>
            
            <article>
                
<p>We will use data from two sources:</p>
<ul>
<li><strong>Reddit worldnews</strong></li>
<li><strong>Dow Jones Industrial Average</strong> (<strong>DJIA</strong>) </li>
</ul>
<p>The <em>Getting started</em> section that follows has two clear goals:</p>
<ul>
<li>Moving our development environment into a virtual appliance from a previous local <span>Spark shell-centered </span>development environment. This naturally implies setting up prerequisite resources.</li>
<li>Attaining the preceding goal also implies being able to spin up a brand new Spark cluster, running inside the virtual appliance.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p>In order to address the goals of this section, we will compile a resource list—a list of prerequisite software to be set up<span>—</span><span>b</span>efore taking a shot at the first stated goal of setting up the <strong>Hortonworks Development Platform</strong> (<strong>HDP</strong>) Sandbox, a so-called virtual appliance from the Hortonworks organization. The virtual appliance overview section is helpful regarding this.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At its core, the HDP Sandbox is a robust data pipeline development environment. This appliance and its supporting ecosystem, like the underlying OS and the virtual machine configuration settings, make up the core of the development infrastructure.</p>
<p>The following is the resource list—the <span>prerequisite software—that must be set or verified before proceeding further:</span></p>
<ul>
<li>A 64-bit host machine with support for hardware virtualization. To check for processor and motherboard support for virtualization, download and run a small utility called SecurAble. <span>BIOS should be enabled or set to support virtualization.</span></li>
<li>Host OS Windows 7, 8, or 10, macOS.</li>
<li>Compatible browsers such as Internet Explorer 9, stable versions of Mozilla Firefox, Google Chrome, or Opera.</li>
<li>At least 16 GB of RAM on the host machine.</li>
<li><span>S</span>upported virtualization applications need to be installed, such as Oracle VirtualBox Version 5.1 or above (this is our preferred virtualization application) or VMWare Fusion.</li>
<li>The HDP Sandbox download file. This file is delivered as a virtual appliance with the <strong>Open Virtualization Format Archive</strong> (<strong>OVA</strong>) file.</li>
</ul>
<p>In the next section, we will review the prerequisites on our resources list.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support for hardware virtualization</h1>
                </header>
            
            <article>
                
<p>Grab a copy of SecurAble from the <kbd>ModernScalaProjects_Code</kbd> folder. SecurAble is a small program that is able to tell you the following about your machine's processor:</p>
<ul>
<li>Confirm the presence or absence of 64-bit instructions on your host machine processor</li>
<li>Whether there is hardware support for virtualization</li>
</ul>
<p>To determine the preceding prerequisites, SecurAble will not make any changes to your machine. On running the SecurAble application file, it will present a screen that should like the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c6352373-6f3a-44a7-b6f9-be17bb709f73.jpg" style="width:26.58em;height:20.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of screen on running the SecurAble application file</div>
<p>Clicking on <span class="packt_screen">64 Maximum Bit Length</span> makes SecurAble return the presence or absence of 64-bit processing, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/98ad62c6-8210-48e6-b6f0-6c27a12e2241.jpg" style="width:25.92em;height:19.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot of window displaying the presence or absence of 64-bit processing </div>
<p>The chipset on my Windows 64-bit machine is confirmed as offering a 64-bit mode of operation. Next, clicking on <span class="packt_screen">Yes Hardware Virtualization</span> makes SecurAble report back that my processor does offer hardware support for virtualization, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4646632-5d0b-4225-ba81-fd91ffe0464a.jpg" style="width:28.25em;height:21.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of window displaying the hardware support for virtualization on SecurAble</div>
<p>If SecurAble reports back with the exact same results on your machine, it is likely that you have a host machine that can support Oracle VirtualBox. Note that at the BIOS level, support for virtualization is likely already set. If that is not the case, enable it. Note that SecurAble won't be able to report on BIOS support for the virtualization feature.</p>
<p>Ensure that the preceding prerequisites are satisfied before moving further. Next, take up prerequisite which is regarding installing a supported virtualization application that is able to host a virtual appliance. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the supported virtualization application </h1>
                </header>
            
            <article>
                
<p>The following are the steps to install the virtualization application:</p>
<ol>
<li>Download the latest VirtualBox binaries from the Oracle VirtualBox website:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/38462476-3e66-4f26-a1db-fbb92e065eef.jpg" style="width:44.58em;height:25.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of latest VirtualBox binaries</div>
<ol start="2">
<li>Double-click on the Oracle VirtualBox binary. The setup welcome screen presents itself as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/78887793-15ca-4780-9259-a579404a7e84.jpg" style="width:28.67em;height:22.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of the setup window</div>
<ol start="3">
<li>Click <span class="packt_screen">Next</span> on the welcome screen. In the new screen that presents itself, select where you want VirtualBox to be installed:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/44c5ddb2-99d9-4963-8866-43869afebbc8.jpg" style="width:30.33em;height:23.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of setup steps to place the folder</div>
<div class="packt_figref"/>
<ol start="4">
<li>Click <span class="packt_screen">OK</span> to move on to the <span class="packt_screen">Ready to Install</span> screen:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/14ca56f5-5bd2-4ea7-b6de-0b37c81ea584.jpg" style="width:36.67em;height:29.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot of Ready for installation screen</div>
<ol start="5">
<li>Click <span class="packt_screen">Install</span> and complete any self-explanatory steps needed to complete the installation. Once this process is complete, place a shortcut on your taskbar or on the desktop. Now, launch the VirtualBox application, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6b765c50-c700-491f-82c0-b17aa422ac08.jpg" style="width:51.67em;height:38.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of Ready for launch of VirtualBox application</div>
<p style="padding-left: 60px">You may optionally remove the inaccessible machines <kbd>vm</kbd>, <kbd>vm_1</kbd>, and <kbd>CentOS</kbd> as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1a4b82b1-ceac-4aff-a859-8e3de5932f1f.jpg" style="width:49.25em;height:37.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot displaying how to remove the inaccessible machines</span></div>
<div class="packt_figref"/>
<ol start="6">
<li>Next, deselect the <span class="packt_screen">Auto Capture Keyboard</span> option under <span class="packt_screen">File</span> | <span class="packt_screen">Preferences...</span> | <span class="packt_screen">Input</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4015e2d1-672c-4b18-b531-f9a6bdf3f5bd.jpg" style="width:46.25em;height:28.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of final steps to set the virtual machine</div>
<p>The virtual machine is now all set. In the next step, we will download and import the Sandbox into it.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the HDP Sandbox and importing it</h1>
                </header>
            
            <article>
                
<p>The following are the steps to download the HDP Sandbox:</p>
<ol>
<li>Head over to <a href="https://hortonworks.com/downloads/#sandbox">https://hortonworks.com/downloads/#sandbox</a> and download the Hortonworks Sandbox virtual appliance file.</li>
<li>Move the Sandbox virtual appliance file into a convenient location on your host machine. Perform the following click actions in sequence: <span class="packt_screen">File</span> | <span class="packt_screen">Import Appliance....</span> Then, choose a virtual appliance file to import, which thereby imports the respective disk image<span>:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/36c85b5d-81cc-40e5-85cc-c524d8bec275.jpg"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Steps to be performed to import the disk image</span></div>
<div class="packt_figref"/>
<ol start="3">
<li>Next, let's tweak <span class="packt_screen">Appliance settings</span> in the <span class="packt_screen">Appliance settings</span> screen. <span>Make sure you increase the available <span class="packt_screen">RAM</span> to at least</span> <kbd>10000 MB</kbd><span>. Leave the other default settings intact </span>and click <span class="packt_screen">Import</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3f0a0b95-dcee-483d-8ad0-b610adadb9a2.jpg" style="width:42.75em;height:22.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot to import the virtual appliance into Oracle VirtualBox</div>
<p>The virtual appliance has now been imported into Oracle VirtualBox. The following section offers a brief overview of the Hortonworks Sandbox virtual appliance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hortonworks Sandbox virtual appliance overview</h1>
                </header>
            
            <article>
                
<p>The Hortonworks Sandbox is a virtual machine, or virtual appliance, which is delivered as a file with an <kbd>.ova</kbd> or <kbd>.ovf</kbd> extension. It appears as a bare machine to the host OS, and has the following components:</p>
<ul>
<li>A guest operating system that is treated as an application by the underlying host operating system</li>
<li>The v<span>irtual appliance file we want is an <kbd>.ova</kbd> file, which is available in the <kbd>ModernScalaProjects_Code</kbd> folder under the virtual machines folder</span></li>
<li>Applications that run in the guest OS</li>
</ul>
<p>All that being said, we are done setting up prerequisites. Let's run the virtual machine for the first time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Turning on the virtual machine and powering up the Sandbox</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the following steps:</p>
<ol>
<li>Run the Oracle VirtualBox startup icon. The startup screen with a powered-off Sandbox appears as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3841319e-0437-4b64-bc01-4c0944734c92.jpg" style="width:46.25em;height:34.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of the startup screen with a powered-off Sandbox</div>
<p style="padding-left: 90px">The startup screen displays the updated Hortonworks Sandbox virtual appliance with its updated configuration. For example, our <span class="packt_screen">Base Memory</span> is now <span class="packt_screen">10000</span> MB.</p>
<ol start="2">
<li>Next, right-click on the Sandbox and click <span class="packt_screen">Start</span> | <span class="packt_screen">Normal Start</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6bc43418-3871-4240-828a-3a3bada41dd9.jpg" style="width:49.42em;height:37.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of steps to be performed to get started</div>
<p style="padding-left: 90px">If everything went well, you should see the following <span class="packt_screen">Hortonworks Docker Sandbox HDP [Running]</span> screen:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dffcce19-8e3c-449c-a4b9-16806bec2a68.jpg" style="width:34.67em;height:22.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of Hortonworks Docker Sandbox HDP [Running] </div>
<ol start="3">
<li class="mce-root">We want to log in to the Sandbox. <em>Alt</em> + <em>F5</em> takes you to the <kbd>sandbox-host login</kbd> screen as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a81c4253-3350-4e5e-9900-922a7065e3b1.jpg" style="width:35.25em;height:23.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot displaying how to log in to the Sandbox</div>
<p style="padding-left: 90px"><span>Sign in as <kbd>root</kbd> with a </span><kbd>Password</kbd> of <kbd>hadoop</kbd>. </p>
<ol start="4">
<li>Edit the <kbd>hosts</kbd> file, and map <kbd>127.0.0.1</kbd> to <kbd>sandbox-hdp.hortonworks.com</kbd>. On a Windows (host) machine, this is located under <kbd>C:\Windows\System32\drivers\etc</kbd><span>:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4fb175e9-84a1-487d-a01d-ee543d6c2878.jpg" style="width:55.00em;height:30.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot displaying editing of host files</div>
<ol start="5">
<li>Save the updated <kbd>hosts</kbd> file and verify that these changes took effect before loading the URL as <kbd>sandbox-hdp.hortonworks.com:8888</kbd> in the browser.</li>
</ol>
<ol start="6">
<li>Next, load the URL as <span><kbd>sandbox-hdp.hortonworks.com:4200</kbd> in your browser to launch the </span>Sandbox web client. Change the default password from <kbd>hadoop</kbd> to something else. Note that the virtual appliance runs a CentOS Linux guest OS:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/d9b477bd-83dd-414d-8a1b-2ff2d272b8f2.jpg" style="width:38.92em;height:21.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot to launch the Sandbox web client</div>
<p class="mce-root CDPAlignLeft CDPAlign">In the next section, we will set up an SSH client for transferring files between the Sandbox and your local (host) machine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up SSH access for data transfer between Sandbox and the host machine</h1>
                </header>
            
            <article>
                
<p><strong>SSH</strong> stands for <strong>Secure Shell</strong>. We want to set up the SSH network protocol to establish a remote login and secure file transfers between our host machine and a virtual machine that's running the virtual appliance. </p>
<p>Two steps need to be followed: </p>
<ul>
<li>Set up PuTTY, a third-party SSH and Telnet client</li>
<li>Set up WinSCP, a <span><strong>Secure File Transfer Protocol</strong> (<strong>SFTP</strong>) client for Windows</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up PuTTY, a third-party SSH and Telnet client</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the following installation steps of PuTTY:</p>
<ol>
<li>The PuTTY installer, <kbd>putty-64bit-0.70-installer.exe</kbd>, is available in the <kbd>ModernScalaProjects_Code</kbd> folder. You can run it by double-clicking on the installer icon as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b9cd6719-e2a4-409c-b1a5-89b814ba7a69.jpg" style="width:7.83em;height:10.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">PuTTY installer icon</div>
<ol start="2">
<li>Choose a destination folder to install PuTTY into and click <span class="packt_screen">Next</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb23ddf9-66f4-47e4-b8a3-c419bccffd8c.jpg" style="width:33.33em;height:25.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot to Install PuTTY</div>
<div class="packt_figref"/>
<ol start="3">
<li>Select or deselect any product features you want to be installed:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/aaa00e57-da33-40c6-9f9a-20466d7be783.jpg" style="width:28.50em;height:22.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of list of product features </div>
<ol start="4">
<li><span>Next, click<span> </span><span class="packt_screen">Install</span>. PuTTY and other supporting utilities will be installed:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/124219fe-a14f-4978-9fd3-a68692da595c.jpg" style="width:32.17em;height:22.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot of PuTTY and supporting utilities being installed</span></div>
<div class="packt_figref"/>
<ol start="5">
<li>Run PuTTYgen. On the <span class="packt_screen">PuTTY Key Generator</span> screen, press the <span class="packt_screen">Generate</span> button and follow the onscreen instructions. Click on the <span class="packt_screen">Save public key</span> button and save the generated public key into a file called <kbd>authorized_keys</kbd> in a convenient location, but not before typing in a passphrase:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1610be45-722e-4f37-965b-8e362cf93585.jpg" style="width:30.83em;height:30.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of steps to be performed after running PuTTY Key Generator</div>
<div class="packt_figref"/>
<ol start="6">
<li>Click on <span class="packt_screen">Save private key</span>, which is marked by <span class="packt_screen">3</span> in the preceding screenshot. This will let you save your private key in a convenient location. This could be the same as the public key location, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/302e7c1f-0e73-423f-9a23-4b86e78e68ea.jpg" style="width:29.17em;height:17.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of private key being saved in a convenient location</div>
<ol start="7">
<li>At this point, we want to upload the public key to our Sandbox. Start up the Sandbox, and then load the Sandbox web client like we did earlier. Carry out steps <span class="packt_screen">1</span>, <span class="packt_screen">2</span>, <span class="packt_screen">3</span>, and <span class="packt_screen">4</span>, as shown in the following screenshot. The public key is saved as <kbd>authorized_key</kbd>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e984d752-f20c-4754-bc3b-c428dd7b3e8b.jpg" style="width:45.58em;height:21.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot displaying the upload of public key to our Sandbox</span></div>
<div class="packt_figref"/>
<ol start="8">
<li>Dismiss PuTTYgen with <span class="packt_screen">File</span> | <span class="packt_screen">Exit</span>.</li>
<li>Open PuTTY and click on <span class="packt_screen">Session</span>. We want to create and save a session. Follow the numbers in the following screenshot to set it up:
<ol>
<li>Click on <span class="packt_screen">Session</span> and select <span class="packt_screen">Logging</span></li>
<li>Enter <span class="packt_screen">Host Name</span> as our Sandbox</li>
<li>Enter <span class="packt_screen">Port</span> as <kbd>2222</kbd></li>
<li>Then click on button <span class="packt_screen">Save</span></li>
</ol>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fd63a21b-3f05-47a8-809d-31d13c5b1be1.jpg" style="width:37.92em;height:26.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot displaying the steps to be performed for creating and saving the session</span></div>
<div class="packt_figref"/>
<ol start="10">
<li><span>Save the session as <kbd>sandbox-hdp.hortonworks.com</kbd> (saved sessions) by clicking <span class="packt_screen">Save</span>. Next, click on <span class="packt_screen">Data</span> under <span class="packt_screen">Connection</span> and enter the login name of the Sandbox. Do not click <span class="packt_screen">Open</span> yet:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/601f33bc-0c9d-4d5f-ad13-ff6013d7fc9a.jpg" style="width:32.50em;height:31.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot showing the steps to be performed after saving the session</div>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="11">
<li>After entering the <span class="packt_screen">Auto-login username</span>, click on <span class="packt_screen">Connection |</span> <span class="packt_screen">SSH</span> | <span class="packt_screen">Auth </span><span class="packt_screen"><span class="packt_screen"><span>|</span></span>Load the private key</span> after clicking on <span class="packt_screen">Browse...</span>. Load the private key and click <span class="packt_screen">Open</span>. This should establish an SSH connection with the Sandbox:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f0ec68f9-32e4-40d2-9688-d48bd05adeea.jpg" style="width:30.00em;height:28.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot of steps to be performed after entering the Auto-login username</div>
<p>Let's recap and summarize the steps we took so far to set up PuTTY, a third-party SSH and Telnet client, and hence SSH access for data transfer between the Sandbox and the host machine:</p>
<ol>
<li>Click on <span class="packt_screen">Session</span>. Enter the hostname under the <span class="packt_screen">Host Name (or IP address)</span> field of the Sandbox virtual appliance and then select the appropriate SSH protocol. Moving on, navigate to <span class="packt_screen">Connection</span> | <span class="packt_screen">Data</span> and enter your login name for the Sandbox in the auto-login box. </li>
<li>Then, navigate to <span class="packt_screen">Connection</span> | <span class="packt_screen">SSH</span> | <span class="packt_screen">Auth</span> | <span class="packt_screen">Load the private key</span>.</li>
<li>Finally, click on <span class="packt_screen">Session</span>. Load the saved session and click <span class="packt_screen">Save</span>; this updates the session.</li>
</ol>
<p>WinSCP is a popular graphical SSH client for Windows that makes it easy to transfer files between the local (host) machine and the Sandbox. Let's set up WinSCP now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up WinSCP, an SFTP client for Windows</h1>
                </header>
            
            <article>
                
<p>The following steps explain how to set up WinSCP:</p>
<ol>
<li>The WinSCP binary file is available under the <kbd>ModernScalaProjects_Code</kbd> folder. Download it and run it. Once installed, launch WinSCP for the very first time. The <span class="packt_screen">Login</span> screen presents itself as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9006aee6-6271-4f80-8ae2-f8b154a3cc9a.jpg" style="width:46.00em;height:28.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of the login screen</div>
<p style="padding-left: 90px">Click on <span class="packt_screen">New Site</span>, making sure that the <span class="packt_screen">File protocol</span> is <span class="packt_screen">SFTP</span>, and under <span class="packt_screen">Host name</span>, you may enter the Sandbox hostname. Change the port from <kbd>22</kbd> to <kbd>2222</kbd>. You might want to enter <kbd>root</kbd> under <span class="packt_screen">User name</span> and the <span class="packt_screen">Password</span> for the Sandbox WebClient. Next, click <span class="packt_screen">6</span>, which takes us to the <span class="packt_screen">Advanced Site Settings</span> screen:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6943e276-a0a3-42b6-a922-45d1930b350a.jpg" style="width:36.67em;height:28.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot of Advanced Site Settings screen</div>
<ol start="2">
<li>In the preceding <span class="packt_screen">Advanced Site Settings</span> screen, drill down to <span class="packt_screen">Authentication</span> under <span class="packt_screen">SSH</span> and load the private key file. Click <span class="packt_screen">OK</span>.</li>
</ol>
<ol start="3">
<li>Now, launch WinSCP again. Clicking on <span class="packt_screen">Login</span> should establish a connection with the Sandbox and you should be able to transfer files back and forth as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4e3f6445-5320-454a-9c0b-fd6dce9770c2.jpg" style="width:49.42em;height:31.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot showing Sandbox being able to transfer files back and forth </div>
<ol start="4">
<li> After the connection is established, the resulting screen should look as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ec691a8b-bdd3-48c8-9246-3e51969c1016.jpg" style="width:51.83em;height:33.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot of screen after the connection is establiashed</div>
<p>In the next step, we will move on to Sandbox configuration updates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the default Python required by Zeppelin</h1>
                </header>
            
            <article>
                
<p>The Sandbox boasts a fully-fledged Spark development environment with one notable difference from our previous local Spark development environment: Zeppelin Notebook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is Zeppelin?</h1>
                </header>
            
            <article>
                
<p>Zeppelin is a web-based notebook that has the following features:</p>
<ul>
<li>Data exploration</li>
<li>Interactive data analysis</li>
<li>Data visualization and interactive dashboards</li>
<li><span>Collaborative d</span>ocument sharing</li>
</ul>
<p>Zeppelin depends on Python 2.7 or above, but the Sandbox itself only supports Version 2.6. Therefore, we are going to have to replace 2.6 with 2.7. Before we go any further, let's check our Python version:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5db67030-9dda-4741-9af2-57ba019d1831.jpg" style="width:39.67em;height:8.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot displaying the python version</div>
<p>That's right! We need to get rid of Python 2.6 with Python 2.7 and bring the Notebook up to date.</p>
<p>The steps to accomplish this are summarized as follows:</p>
<ul>
<li>Set up the Anaconda data science environment. You can simply set up a lighter version of Anaconda, which would be Miniconda with Python 2.7. Miniconda brings with it many popular data science packages.</li>
<li>Set up any packages that do not come packaged with Miniconda. Make sure that you have SciPy, NumPy, Matplotlib, and Pandas. Sometimes, we can simply pass a Spark/Scala <kbd>DataFrame</kbd> into Python on Pyspark to produce visualizations quickly.</li>
</ul>
<p>Work through the following steps:</p>
<ol>
<li><span>The first step is to download the installer for Miniconda</span><span> and run it as follows:</span></li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/75c0d8e1-6dcc-48a5-9d37-70b63f5c5601.jpg" style="width:38.00em;height:9.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot displaying the installer for Miniconda </div>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px">Go through the installation process. It is straightforward. Once the installation is complete, restart the web client to allow changes to take effect. Now, log back in to the Sandbox with <kbd>root</kbd> and your secret password. </p>
<ol start="2">
<li>To check whether we do have a new, upgraded Python, issue the <kbd>python</kbd> command as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ade5b58a-f703-45a8-91cc-d4555dd7ee05.jpg" style="width:39.67em;height:11.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot displaying how to issue the Python command</div>
<p>Voila! We have a new Python version: 2.7.14.</p>
<p>In the next section, we will update the Zeppelin instance using curl.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating our Zeppelin instance</h1>
                </header>
            
            <article>
                
<p>Following steps need t<span>o be performed to update the Zeppelin instance</span>:</p>
<ul>
<li>Install curl, if not installed already</li>
<li>Update your Zeppelin instance with the latest and greatest notebooks from Hortonworks</li>
</ul>
<p>Please follow the steps to install curl:</p>
<ol>
<li>Run the <kbd>curl --help</kbd> command:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/39a6a4f1-ddd3-4c87-81f6-b271905e999d.jpg" style="width:40.00em;height:10.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot displaying how to run the curl --help command</span></div>
<ol start="2">
<li>The <kbd>curl --help</kbd> command confirms that we have curl already installed. Let's try updating Zeppelin now:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/edd0d588-9d88-4e27-9ff2-0eb9e9b52f1c.jpg" style="width:60.58em;height:24.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot displaying updating of Zeppelin </div>
<ol start="3">
<li class="CDPAlignLeft CDPAlign"> Run the <kbd>curl</kbd> command to update the Zeppelin instance with the latest notebooks. The following screenshot shows the updated <span>Zeppelin instance:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/43cee218-3c22-4bbb-a681-84cdac45c925.jpg" style="width:54.25em;height:24.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot displaying updated Zeppelin instance</div>
<p><span>Now, let's go back to</span> <kbd>hdp.hortonworks.com:8888</kbd>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the Ambari Dashboard and Zeppelin UI</h1>
                </header>
            
            <article>
                
<p>The following are the steps required to launch Ambari and Zeppelin:</p>
<ol>
<li><span>Click on the <span class="packt_screen">Launch Dashboard</span> button and log in as <kbd>maria_dev</kbd> to navigate to the Ambari dashboard:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/30ad3895-24be-4834-a82a-ece2df5e729f.jpg" style="width:45.83em;height:37.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot displaying navigation through the Ambari dashboard</div>
<div class="packt_figref"/>
<ol start="2">
<li>Clicking on <span class="packt_screen">Zeppelin UI</span> under <span class="packt_screen">Quick Links</span> takes us to the Zeppelin UI, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/da934061-8dce-4d70-8ccd-95f115900051.jpg" style="width:47.75em;height:31.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screendhot of Zeppelin UI page</div>
<p style="padding-left: 90px"><span>The Zeppelin UI runs on port <kbd>9995</kbd>. For our Zeppelin Notebook to work with Spark 2 and Python 2.7, the Spark and Python interpreters need updating.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating Zeppelin Notebook configuration by adding or updating interpreters</h1>
                </header>
            
            <article>
                
<p>The following steps need to be performed to update the Zeppelin notebook:</p>
<ol>
<li>We need to update the interpreters, including a Spark 2 interpreter and adding a Python interpreter.</li>
</ol>
<ol start="2">
<li>On the Zeppelin UI page, click on <span class="packt_screen">anonymous</span> | <span class="packt_screen">Interpreter</span> as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7b9e55d1-2aae-4352-91f5-ce4e9360639a.jpg" style="width:35.42em;height:18.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot to perform steps on the Zeppelin UI page</div>
<p style="padding-left: 90px">Clicking on the <span class="packt_screen">Interpreter</span> link takes us to the interpreter's page. First, we will update the Spark 2 interpreter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating a Spark 2 interpreter</h1>
                </header>
            
            <article>
                
<p>The following are the steps involved in updating the Spark 2 interpreter:</p>
<ol>
<li>We will update the <kbd>SPARK_HOME</kbd> property as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/afb6b94a-5835-40ab-bb4d-9097afd44429.jpg" style="width:51.67em;height:16.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot displaying how to update the SPARK_HOME property</span></div>
<div class="packt_figref"/>
<ol start="2">
<li><span>Next, we will update the <kbd>zeppelin.pyspark.python</kbd> property to point to the new Python interpreter:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3ed9b87c-c188-43e2-874b-2b711f73adfb.jpg"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot displaying how to </span><span>update the </span><span>zeppelin.pyspark.python property  </span></div>
<div class="packt_figref"/>
<ol start="3">
<li>Next, let's create a new Python interpreter as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e0e488db-8aeb-4375-aba7-742efc2f70f2.jpg" style="width:35.83em;height:17.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot to create a new Python interpreter</div>
<p style="padding-left: 90px">Update <span><kbd>zeppelin.pyspark.python</kbd> to <kbd>/usr/local/bin/bin/python</kbd>.</span></p>
<ol start="4">
<li>For all of these interpreter changes to take effect, we will need to restart the services. Head over to the Ambari dashboard page. Locate <span class="packt_screen">Service Actions</span> at the top-right corner and in the dropdown, select <span class="packt_screen">Restart All</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7eb274e3-b546-46dd-97ed-b9bff1e82f58.jpg" style="width:56.75em;height:15.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Screenshot displaying final step for Zeppelin Notebook to be ready for development</span></div>
<p>At this point, the Zeppelin Notebook is ready for development.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation objectives</h1>
                </header>
            
            <article>
                
<p>The goal of this section will be to get started with developing a data pipeline using the Random Forests algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">List of implementation goals</h1>
                </header>
            
            <article>
                
<p>The following implementation objectives are the same and cover both the Random Forests pipeline and linear regression. We will perform preliminary steps such as <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>) once and develop specific implementation code that pertains to a particular pipeline. Therefore, the implementation objectives are listed here as follows:</p>
<ul>
<li>Get the stock price dataset.</li>
<li>Carry out preliminary EDA in the Sandbox Zeppelin Notebook environment (or Spark shell), and run a statistical analysis.</li>
<li>Develop the pipeline incrementally in Zeppelin, and port the code into IntelliJ. This means doing the following:
<ol>
<li>Create a new Scala project in IntelliJ, or import an existing empty project into IntelliJ, and create Scala artifacts from code that was incrementally developed in the Notebook.</li>
<li>Do not forget to wire up all the necessary dependencies in the <kbd>build.sbt</kbd> file.</li>
</ol>
<ol start="3">
<li>Interpret the results of the pipeline, such as how well the classifier performed. How close are the predicted values to those in the original dataset?</li>
</ol>
</li>
</ul>
<p>In the next subsection, we will download the stock price dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – creating a Scala representation of the path to the dataset file</h1>
                </header>
            
            <article>
                
<p>The stock price dataset is available in the <kbd>ModernScalaProjects_Code</kbd> folder. Grab a copy and upload it to the Sandbox, then place it in a convenient location as follows:</p>
<pre class="mce-root"><strong>scala&gt; val dataSetPath = "\\&lt;&lt;Path to the folder containing the Data File&gt;&gt;"</strong><br/><strong><span>scala&gt; val dataFile = dataSetPath + "\\News.csv"</span></strong></pre>
<p>In the next step, let's create a <strong>resilient distributed dataset</strong> (<strong>RDD</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – creating an RDD[String]</h1>
                </header>
            
            <article>
                
<p>Invoke the <kbd>textFile</kbd> method of <kbd>sparkContext</kbd>, which is supplied by Spark in the Sandbox:</p>
<pre class="mce-root"><strong>scala&gt; val result1 = spark.sparkContext.textFile(dataSetPath + "News.csv")</strong><br/><strong>result1: org.apache.spark.rdd.RDD[String] = C:\&lt;&lt;Path to your own Data File&gt;&gt;\News.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</strong></pre>
<p>The resultant RDD, <kbd>result1</kbd>, is a partitioned structure. In the next step, we will iterate over these partitions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – splitting the RDD around the newline character in the dataset</h1>
                </header>
            
            <article>
                
<p>Invoke a <kbd>flatMap</kbd> operation over the <kbd>result1</kbd> RDD, and split each partition around its <kbd>"\n"</kbd> (end of the line) character as follows:</p>
<pre class="mce-root"><strong>scala&gt; val result2 = result1.flatMap{ partition =&gt; partition.split("\n").toList }</strong><br/><strong>result2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:27</strong></pre>
<p class="mce-root">Each <kbd>partition</kbd> is after a string. In the next step, we will transform the <kbd>result2</kbd> RDD.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – transforming the RDD[String] </h1>
                </header>
            
            <article>
                
<p class="mce-root">Invoke a <kbd>map</kbd> operation over the <kbd>result2</kbd> RDD. Each line in this RDD (a row of data) consists of stock price headline data separated by a comma, as follows:</p>
<pre class="mce-root"><strong>scala&gt; val result2A = result2.map(_.split(","))</strong><br/><strong>result2A: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at &lt;console&gt;:29</strong></pre>
<p class="mce-root">The resultant RDD, <kbd>result2A</kbd>, is an <kbd>RDD[Array[String]]</kbd>. The RDD consists of an array of strings, where each string represents a row.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – carrying out preliminary data analysis</h1>
                </header>
            
            <article>
                
<p>This step is broken down into a set of smaller steps. The process starts with the creation of <kbd>DataFrame</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating DataFrame from the original dataset</h1>
                </header>
            
            <article>
                
<p>Load the stock price dataset file again by specifying an appropriate <kbd>option</kbd> for Spark to automatically infer the schema of the dataset before it can create the <kbd>DataFrame</kbd>, as follows: </p>
<pre class="mce-root"><strong>scala&gt; val spDataFrame = spark.read.format("com.databricks.spark.csv").option("delimiter", ",").option("header", "true").option("inferSchema", "true").load("News.csv")</strong><br/><strong>spDataFrame: org.apache.spark.sql.DataFrame = [Date: string, Label: int ... 5 more fields]</strong></pre>
<p class="mce-root">The resultant structure, <kbd>spDataFrame</kbd>, is <kbd>DataFrame</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropping the Date and Label columns from the DataFrame</h1>
                </header>
            
            <article>
                
<p class="mce-root"><kbd>Date</kbd> and <kbd>Label</kbd> are columns we may exclude for now as follows:</p>
<pre class="mce-root"><strong>scala&gt; val newsColsOnly = spDataFrame.drop("Date", "Label")</strong><br/><strong>newsColsOnly: org.apache.spark.sql.DataFrame = [Top1: string, Top2: string ... 23 more fields]</strong></pre>
<p>The resultant structure is a new <kbd>DataFrame</kbd> that consists of all 25 top headlines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Having Spark describe the DataFrame</h1>
                </header>
            
            <article>
                
<p>Invoke the <kbd>describe</kbd> and <kbd>show</kbd> methods in that order to give us a visual of the first 20 rows as follows:</p>
<pre class="mce-root"><strong>scala&gt; val expAnalysisTestFrame = spDataFrame.describe("TopHeadline1", "TopHeadline2", "TopHeadline3","TopHeadline4","TopHeadline5", TopHead.....)</strong><br/><strong>expAnalysisTestFrame: org.apache.spark.sql.DataFrame = [summary: string, TopHeadline: string ... 25 more fields]</strong><br/><br/><strong><span>scala&gt; newsColsOnly.show</span></strong></pre>
<p class="mce-root">In the next step, let's add a new column to the <kbd>DataFrame</kbd>, an <kbd>expAnalysisTestFrame</kbd>, called <kbd>AllMergedNews</kbd>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding a new column to the DataFrame and deriving Vector out of it</h1>
                </header>
            
            <article>
                
<p>Let's perform the following steps to add a new column and derive <kbd>Vector</kbd>:</p>
<ol>
<li class="mce-root">Create a new <kbd>DataFrame</kbd> by creating a new <kbd>AllMergedNews</kbd> column in place of the <kbd>Top1</kbd> column, which is done by invoking the <kbd>withColumn</kbd> method as follows:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>scala&gt; val mergedNewsColumnsFrame = newsColsOnly.withColumn("AllMergedNews", newsColsOnly("TopHeadline1"))</strong><br/><strong>mergedNewsColumnsFrame: org.apache.spark.sql.DataFrame = [TopHeadline1: string, TopHeadline2: string ... 23 more fields]</strong></pre>
<ol start="2">
<li>Next, transform the <kbd>mergedNewsColumns</kbd> <kbd>DataFrame</kbd> into <kbd>Vector</kbd> as follows:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>scala&gt; import org.apache.spark.sql.functions</strong><br/><strong>import org.apache.spark.sql.functions</strong><br/><br/><strong>scala&gt; val mergedFrameList = for (i &lt;- 2L to newsColsOnly.count() + 1L) yield mergedNewsColumnsFrame.withColumn("AllMergedNews", functions.concat(mergedNewsColumnsFrame("AllMergedNews"), functions.lit(" "), mergedNewsColumnsFrame("Top" + i.toString)))</strong><br/><strong>mergedFrameList: scala.collection.immutable.IndexedSeq[org.apache.spark.sql.DataFrame] = Vector([Top1: string, Top2: string ... 4 more fields], [Top1: string, Top2: string ... 4 more fields])</strong></pre>
<ol start="3">
<li>In the next step, we simply derive the <kbd>mergedFinalFrame</kbd> <kbd>DataFrame</kbd> from <kbd>mergedFrameList</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>scala&gt; val mergedFinalFrame = mergedFrameList(0)</strong><br/><strong>mergedFinalFrame: org.apache.spark.sql.DataFrame = [Top1: string, Top2: string ... 4 more fields]</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 90px">At this point, we have <kbd>DataFrame</kbd> that needs some preprocessing. Let's start by getting rid of stop words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing stop words – a preprocessing step </h1>
                </header>
            
            <article>
                
<p>The stop words we want to be eliminated are words such as <em>a</em>, <em>an</em>, <em>the</em>, and <em>in</em>. <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>) comes to the rescue as follows:</p>
<pre>import org.apache.spark.ml.feature.StopWordsRemover<br/>import org.apache.spark.ml.param.StringArrayParam<br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre>val stopwordEliminator = StopWordsRemover(new StringArrayParam("words","..), new StringArrayParam("stopEliminated", "..)</pre>
<p>The next step is going to be using the <kbd>transform</kbd> operation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming the merged DataFrame</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <kbd>mergedFinalFrame</kbd> <kbd>DataFrame</kbd> is passed into the <kbd>transform</kbd> method. This NLTK step gets rid of all stop words that were not necessary for our analysis:</p>
<pre class="mce-root">val cleanedDataFrame = stopwordEliminator.transform(mergedFinalFrame)<br/> cleanedDataFrame.show()<br/><strong>+-----------+-----+--------------------+--------------------+</strong><br/><strong>| Date|label| words| stopEliminated|</strong><br/><strong>+-----------+-----+--------------------+--------------------+</strong><br/><strong>| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|</strong><br/><strong>|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|</strong><br/><strong>+-----------+-----+--------------------+--------------------+</strong></pre>
<p>In the next step, we will use a feature transformer called <kbd>NGram</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming a DataFrame into an array of NGrams</h1>
                </header>
            
            <article>
                
<p>What is an n-gram? It is simply a sequence of items such as letters, words, and so on (as in our dataset). Our dataset resembles a corpus of text. It is an ideal candidate for processing into an array of n-grams, which is an array consisting of words from the latest version of our dataset, devoid of stop words:</p>
<pre class="mce-root">//Import the feature Transformer NGram<br/>import org.apache.spark.ml.feature.NGram<br/> <br/>//Create an N-gram instance; create an N-Gram of size 2<br/>val aNGram = new NGram(new StringArrayParam("stopRemoved"..), new StringArrayParam("ngrams", n=2)<br/> <br/>// transform the cleanedDataFrame (the one devoid of stop words)<br/>val cleanedDataFrame2 = aNGram.transform(cleanedDataFrame)<br/> <br/>//display the first 20 rows<br/> cleanedDataFrame2.show()<br/><strong> +-----------+-----+--------------------+--------------------+--------------------+</strong><br/><strong> | Date|label| words| stopEliminated| Ngrams|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+</strong><br/><strong>  | 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|</strong><br/><strong> |11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+</strong></pre>
<p>In the next step, we will create a new dataset by adding a column called <kbd>ndashgrams</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding a new column to the DataFrame, devoid of stop words</h1>
                </header>
            
            <article>
                
<p>Derive a new <kbd>DataFrame</kbd> by adding a new column called <kbd>ndashgrams</kbd> to <kbd>DataFrame</kbd> <kbd>cleanedDataFrame2</kbd> as follows:</p>
<pre class="mce-root">cleanedDataFrame3 = cleanedDataFrame2.withColumn('ndashgrams', ....)<br/><br/>cleanedDataFrame3.show()<br/><strong>+-----------+-----+--------------------+--------------------+--------------------+</strong><br/><strong> | Date|label| words| stopEliminated| Ngrams|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+</strong><br/><strong> | 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|</strong><br/><strong> |11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+</strong></pre>
<p>The next step gets more interesting. We will apply what is known as a count vectorizer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing a vocabulary from our dataset corpus</h1>
                </header>
            
            <article>
                
<p>Why <kbd>CountVectorizer</kbd>? We need one to construct a vocabulary of certain terms regarding our stock price corpus:</p>
<pre class="mce-root">import org.apache.spark.ml.feature.CountVectorizer <br/><br/>//We need a so-called count vectorizer to give us a CountVectorizerModel that will convert our 'corpus' //into a sparse vector of n-gram counts<br/> <br/> val countVectorizer = new CountVectorizer<br/>//Set Hyper-parameters that the CountVectorizer algorithm can take<br/>countVectorizer.inputCol(new StringArrayParam("NGrams")<br/>countVectorizer.outputCol(new StringArrayParam("SparseVectorCounts")<br/>//set a filter to ignore rare words<br/>countVectorizer.minTF(new DoubleParam(1.0))</pre>
<p><kbd>CountVectorizer</kbd> generates a <kbd>CountVectorizerModel</kbd>, which can convert our corpus into a sparse vector of n-gram token counts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training CountVectorizer</h1>
                </header>
            
            <article>
                
<p>We want to train our <kbd>CountVectorizer</kbd> by passing into it our latest version of the dataset as shown in the following code snippet: </p>
<pre class="mce-root">cleanedDataFrame3 = countVectorizer.fit(cleanedDataFrame2)<br/> <br/>cleanedDataFrame3.show()<br/><strong>| Date|label| words| stopEliminated| NGrams| SparseVectorCounts|</strong><br/><strong>| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|</strong><br/><strong>|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+--------------------</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using StringIndexer to transform our input label column</h1>
                </header>
            
            <article>
                
<p>Now, let's index our <kbd>label</kbd> input in the dataset using <kbd>StringIndexer</kbd> as follows:</p>
<pre class="mce-root">import org.apache.spark.ml.feature.StringIndexer<br/><br/>val indexedLabel = new StringIndexer(new StringArrayParam("label"), new StringArrayParam("label2"), ...)<br/> <br/>cleanedDataFrame4 = indexedLabel.fit(cleanedDataFrame3).transform(cleanedDataFrame3)</pre>
<p>Next, let's drop the input label column <kbd>label</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropping the input label column</h1>
                </header>
            
            <article>
                
<p>Invoke the <kbd>drop</kbd> method on <kbd>DataFrame</kbd> <kbd>cleanedDataFrame4</kbd> as follows:</p>
<pre class="mce-root">val <span>cleanedDataFrame5 = cleanedDataFrame4</span>.drop('label')<br/>DataFrame[Date: string, words: array&lt;string&gt;, stopRemoved: array&lt;string&gt;, ngrams: array&lt;string&gt;, countVect: vector, label2: double]<br/> <br/><span>cleanedDataFrame5</span>.show()<br/><br/><strong>| Date|label| words| stopEliminated| NGrams| SparseVectorCount|label2|</strong><br/><strong>| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|</strong></pre>
<p class="mce-root"/>
<pre class="mce-root"><strong>|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+--------------------+------+</strong></pre>
<p>Next, let's add a new column called <kbd>label2</kbd> in place of the dropped <kbd>label</kbd> column.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding a new column to our DataFrame </h1>
                </header>
            
            <article>
                
<p>This time, invoke the <kbd>withColumn</kbd> method to add in the <kbd>label2</kbd> column as a replacement for <kbd>label1</kbd>:</p>
<pre class="mce-root">val  cleanedDataFrame6 = cleanedDataFrame5.withColumn('label', cleanedDataFrame.label2)<br/>cleanedDataFrame6.show()<br/><br/><strong>| Date|label| words| stopRemoved| ngrams| countVect|label2|</strong><br/><strong>| 09 09 09| 0|[Latvia, downs, ...|[Latvia, downs, ...|[Latvia downs, d...|</strong><br/><strong>|11 11 09 09| 1|[Why, wont, Aust...|[wont, Australia, N...|[wont Australia, Au...|</strong><br/><strong> +-----------+-----+--------------------+--------------------+--------------------+--------------------+------+</strong></pre>
<p>Now, it is time to divide our dataset into a training set and a test set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dividing the DataSet into training and test sets</h1>
                </header>
            
            <article>
                
<p><span>Let's split the dataset into two datasets. 85% of the dataset will be the training dataset and the remaining 15% will be the testing dataset, as follows:</span></p>
<pre><span>//Split the dataset in two. 85% of the dataset becomes the Training (data)set and 15% becomes the testing (data) set<br/></span><span>val finalDataSet1</span>: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = cleanedDataFrame6.randomSplit(<span>Array</span>(<span>0.85</span>, <span>0.15</span>), <span>98765L</span>)<br/><span>println</span>(<span>"Size of the new split dataset " </span>+ finalDataSet.size)<br/><br/>//the testDataSet<br/><span>val </span>testDataSet = finalDataSet1(<span>1</span>)<br/><br/>//the Training Dataset<br/><span>val </span>trainDataSet = finalDataSet1(<span>0</span>)</pre>
<p>Let's create a <kbd>StringIndexer</kbd> to index the <kbd>label2</kbd> column.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating labelIndexer to index the indexedLabel column</h1>
                </header>
            
            <article>
                
<p>Let's create <kbd>labelIndexer</kbd> now. This will create a new indexed input and output the columns <kbd>label</kbd> and <kbd>indexedLabel</kbd>, as follows:</p>
<pre>val labelIndexer = new IndexToString().setInputCol("label").setOutputCol("indexedLabel").fit(input)</pre>
<p>Next, let's <kbd>transform</kbd> our indexed labels back to the original labels that were not indexed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating StringIndexer to index a column label</h1>
                </header>
            
            <article>
                
<p>The following will help us create a <kbd>StringIndexer</kbd> to index the <kbd>label</kbd> column:</p>
<pre><span>val stringI</span>ndexer = <span>new </span>StringIndexer().setInputCol("prediction").setOutputCol("predictionLabel")</pre>
<p>In the next step, we will create <kbd>RandomForestClassifier</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating RandomForestClassifier</h1>
                </header>
            
            <article>
                
<p>Let's create <span><kbd>randomForestClassifier</kbd> now and pass in the appropriate hyperparameters as follows:</span></p>
<pre><span>val </span>randomForestClassifier = <span>new </span>RandomForestClassifier().setFeaturesCol(sp<span>FeaturesIndexedLabel</span>._1)<br/>.setFeatureSubsetStrategy(<span>"sqrt"</span>)</pre>
<p>We have a classifier now. Now, we will create a new pipeline and create stages, with each stage holding the indexers that we have just created.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new data pipeline with three stages</h1>
                </header>
            
            <article>
                
<p>Let's create the appropriate imports first, as follows:</p>
<pre><span>import </span>org.apache.spark.ml.classification.RandomForestClassifier<br/><span>import </span>org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator<br/><span>import </span>org.apache.spark.ml.param._<br/><span>import </span>org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}<br/><span>import </span>org.apache.spark.ml.{Pipeline, PipelineStage}<br/><br/></pre>
<p><span>Let's start building a pipeline now. This is a pipeline that has three stages, which are <kbd>StringIndexer</kbd>, <kbd>LabelIndexer</kbd>, and <kbd>randomForestClassifier</kbd>.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new data pipeline with hyperparameters</h1>
                </header>
            
            <article>
                
<p>The following steps need to be performed to create a new data pipeline:</p>
<ol>
<li>Create a new pipeline with the following three stages:</li>
</ol>
<pre style="padding-left: 60px"><span>val so</span>Pipeline = <span>new </span>Pipeline().<br/>setStages(<span>Array</span>[PipelineStage](labelIndexer) ++ Array[PipelineStage](randomForestClassifier)) ++ Array[PipelineStage](stringIndexer)]</pre>
<ol start="2">
<li>Create a hyperparameter called <kbd>NumTrees</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>//Lets set the hyper parameter NumTrees<br/></span><span>val </span>rfNum_Trees = randomForestClassifier.setNumTrees(<span>15</span>)<br/><span>println</span>(<span>"Hyper Parameter num_trees is: " </span>+ rfNum_Trees.<span>numTrees</span>)</pre>
<ol start="3">
<li>Create a hyperparameter tree called <kbd>MaxDepth</kbd> and set it to <kbd>2</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>//set this default parameter in the classifier's embedded param map<br/></span><span>val </span>rfMax_Depth = rfNum_Trees.setMaxDepth(<span>2</span>)<br/><span>println</span>(<span>"Hyper Parameter max_depth is: " </span>+ rfMax_Depth.<span>maxDepth</span>)</pre>
<p>It is time to train the pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training our new data pipeline</h1>
                </header>
            
            <article>
                
<p>We have a pipeline that is ready to be trained on the training dataset. Fitting (training) also runs the indexers, as follows:</p>
<pre class="mce-root">val stockPriceModel = pipeline.fit(trainingData)</pre>
<p>Next, run a <kbd>transformation</kbd> operation on our <kbd>stockPriceModel</kbd> and generate stock price predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating stock price predictions</h1>
                </header>
            
            <article>
                
<p>The following steps need to be performed to generate stock price predictions:</p>
<ol>
<li>Run the <kbd>stockPriceModel</kbd> transformation operation on our test dataset as follows:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">// Generate predictions.<br/>val predictions = stockPriceModel.transform(testData)</pre>
<ol start="2">
<li>Let's display the relevant columns of our <kbd>predictions</kbd> <kbd>DataFrame</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px">predictions.select("predictedLabel", "label", "features").show(5)</pre>
<ol start="3">
<li>Finally, we want to evaluate the accuracy of our model, its ability to generate predictions, or in other words, find out how close the generated output was to the predictor labels, as follows:</li>
</ol>
<pre style="padding-left: 60px">modelOutputAccuracyEvaluator: Double = new MulticlassClassificationEvaluator()<br/>.setLabelCol("indexedLabel")<br/>.setPredictionCol("prediction")<br/>.setMetricName("precision")<br/><br/>val accuracy = modelOutputAccuracyEvaluator.evaluate(predictions)<br/><br/></pre>
<p><span>Before we wind up this chapter, there are other metrics that we can evaluate. We leave this as an exercise for the reader. </span></p>
<p><span>Using the <kbd>MulticlassMetrics</kbd> class in the Spark ML API, it is possible to generate metrics that can tell us how close the predicted label value in the predicted column is to the actual label value in the <kbd>label</kbd> column.</span></p>
<p>Readers are invited to come up with two more metrics:</p>
<ul>
<li>Accuracy</li>
<li>Weighted precision</li>
</ul>
<p>There are many other ways to build ML models to predict stock prices and help investors build their long-term investment strategy. For example, linear regression is another commonly used but fairly popular method for predicting stock prices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to leverage the Random Forests algorithm to predict stock prices based on historical trends. </p>
<p><span>In next chapter, we will create a spam classifier. We will start with two datasets, one representing ham and the other, spam dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>Here are a list of few questions:</p>
<ol>
<li>What do you understand by linear regression? Why is it important?</li>
<li>How does linear regression differ from logistic regression?</li>
<li>Name one powerful feature of the binary classifier.</li>
<li>What are the feature variables in relation to the stock price dataset?</li>
</ol>


            </article>

            
        </section>
    </body></html>