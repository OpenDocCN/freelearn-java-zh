<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;System Monitoring"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. System Monitoring</h1></div></div></div><p>The previous two chapters focused on Elasticsearch monitoring tools, including Elasticsearch-head, Bigdesk, and Marvel. This chapter will introduce another monitoring tool, <span class="strong"><strong>Kopf</strong></span>. We<a id="id172" class="indexterm"/> will also discuss <span class="strong"><strong>Elasticsearch, Logstash, and Kibana</strong></span> (<span class="strong"><strong>ELK</strong></span>), Nagios, and various<a id="id173" class="indexterm"/> GNU/Linux command line tools in terms of general purpose system monitoring.</p><p>This chapter covers these topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Monitoring Elasticsearch with Kopf</li><li class="listitem" style="list-style-type: disc">Configuring an Elasticsearch, Logstash, and Kibana (ELK) stack for system log file aggregation and analysis</li><li class="listitem" style="list-style-type: disc">System-level monitoring of a cluster using Nagios</li><li class="listitem" style="list-style-type: disc">GNU/Linux command line tools for system and process management</li></ul></div><div class="section" title="Working with Kopf"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Working with Kopf</h1></div></div></div><p>Kopf is a web-based cluster <a id="id174" class="indexterm"/>management tool like Elasticsearch-head, but has a more modern look and a few different features. With Kopf, users can check the state of nodes and indices, run REST queries, and perform basic management tasks.</p><div class="section" title="Installing Kopf"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Installing Kopf</h2></div></div></div><p>Kopf works on <a id="id175" class="indexterm"/>Elasticsearch 0.90.x and up. Use the following table to determine which <a id="id176" class="indexterm"/>Kopf version is best suited to your cluster:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Elasticsearch Version</p>
</th><th style="text-align: left" valign="bottom">
<p>Kopf Branch</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>0.90.x</p>
</td><td style="text-align: left" valign="top">
<p>0.90</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1.x</p>
</td><td style="text-align: left" valign="top">
<p>1.0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2.x</p>
</td><td style="text-align: left" valign="top">
<p>2.0</p>
</td></tr></tbody></table></div><p>To install <a id="id177" class="indexterm"/>Kopf, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install Kopf on at <a id="id178" class="indexterm"/>least one node in your cluster as an Elasticsearch plugin with the following command, replacing <code class="literal">{branch}</code> with the value from the <code class="literal">branch</code> column in the preceding table:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ sudo /usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf/{branch}</strong></span>
</pre></div><p>This example will install Kopf on <code class="literal">elasticsearch-node-01</code>. Since this node is running Elasticsearch 2.3.2, the command will look like this:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ sudo /usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf/2.0</strong></span>
</pre></div></li><li class="listitem">To open Kopf, browse to:<div class="informalexample"><pre class="programlisting">
<code class="literal">http://elasticsearch-node-01:9200/_plugin/kopf/</code>
</pre></div><p>You should see something like this:</p><div class="mediaobject"><img src="graphics/B03798_05_01.jpg" alt="Installing Kopf"/><div class="caption"><p>Kopf cluster page</p></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>No.</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Titlebar and cluster state</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Cluster summary</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>3</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Display filters</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>4</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Node and index actions</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>5</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Indices</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>6</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Nodes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>7</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Shard allocations</p>
</td></tr></tbody></table></div></li></ol></div><p>The green title bar on<a id="id179" class="indexterm"/> this page indicates that the cluster is in a <code class="literal">green</code> state. Likewise, the title bar changes to yellow or red if it enters either of those states.</p><p>All Kopf dashboard pages also <a id="id180" class="indexterm"/>show the indicators listed on the top of this screenshot, including number of nodes, indices, shards, documents, and total index size.</p><div class="section" title="The cluster page"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec04"/>The cluster page</h3></div></div></div><p>The screenshot in the previous<a id="id181" class="indexterm"/> section shows the Kopf cluster page. The Elasticsearch cluster's nodes, indices, and shard allocations are listed on this page. This page also provides the following administrative capabilities:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Closing and opening indices</li><li class="listitem" style="list-style-type: disc">Optimizing indices</li><li class="listitem" style="list-style-type: disc">Refreshing indices</li><li class="listitem" style="list-style-type: disc">Clearing index caches</li><li class="listitem" style="list-style-type: disc">Deleting indices</li><li class="listitem" style="list-style-type: disc">Disabling/enabling shard allocation</li><li class="listitem" style="list-style-type: disc">Viewing index settings</li><li class="listitem" style="list-style-type: disc">Viewing index mappings</li></ul></div><p>Like the <span class="strong"><strong>Cluster Overview</strong></span> tab in Elasticsearch-head, the Kopf <span class="strong"><strong>cluster</strong></span> page is a great first stop when diagnosing Elasticsearch issues. It will inform you of the cluster state, whether a node is down, and if the node has a high heap/disk/CPU/load.</p></div><div class="section" title="The nodes page"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec05"/>The nodes page</h3></div></div></div><p>The <span class="strong"><strong>nodes</strong></span> page, shown<a id="id182" class="indexterm"/> in the following screenshot, provides figures for load, CPU usage, JVM heap usage, disk usage, and uptime for all nodes in the cluster:</p><div class="mediaobject"><img src="graphics/B03798_05_02.jpg" alt="The nodes page"/><div class="caption"><p>Kopf nodes page</p></div></div><p>This page, like the <span class="strong"><strong>cluster</strong></span> page, is a good starting point when diagnosing Elasticsearch issues.</p></div><div class="section" title="The rest page"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec06"/>The rest page</h3></div></div></div><p>The Kopf <span class="strong"><strong>rest</strong></span> page is a <a id="id183" class="indexterm"/>general-purpose tool for running arbitrary queries against Elasticsearch. You can run any query in the Elasticsearch API using<a id="id184" class="indexterm"/> this page. The following screenshot is running a simple <span class="strong"><strong>Search</strong></span>
<span class="strong"><strong> API</strong></span> query against the Elasticsearch cluster:</p><div class="mediaobject"><img src="graphics/B03798_05_03.jpg" alt="The rest page"/><div class="caption"><p>Kopf rest page</p></div></div><p>The <span class="strong"><strong>rest</strong></span> page is useful for everything from testing query syntax to retrieving cluster metrics, and can help in gauging and optimizing query performance. For example, if a particular query is<a id="id185" class="indexterm"/> running slowly, use the <span class="strong"><strong>rest</strong></span> page to test different variations of the query and determine which query components have the highest performance impact.</p></div><div class="section" title="The more dropdown"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec07"/>The more dropdown</h3></div></div></div><p>The <span class="strong"><strong>more</strong></span> dropdown has a<a id="id186" class="indexterm"/> variety of other cluster management tools, including:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Tool Name</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Create Index</p>
</td><td style="text-align: left" valign="top">
<p>Create an index and <a id="id187" class="indexterm"/>assign a number of shards, replicas, mapping, and other settings</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Cluster Settings</p>
</td><td style="text-align: left" valign="top">
<p>Configure cluster, routing, and<a id="id188" class="indexterm"/> recovery settings</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Aliases</p>
</td><td style="text-align: left" valign="top">
<p>View existing and create<a id="id189" class="indexterm"/> new index aliases</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Analysis</p>
</td><td style="text-align: left" valign="top">
<p>Test and verify<a id="id190" class="indexterm"/> index analyzers</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Percolator</p>
</td><td style="text-align: left" valign="top">
<p>View <a id="id191" class="indexterm"/>existing and create new percolator <a id="id192" class="indexterm"/>queries</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Warmers</p>
</td><td style="text-align: left" valign="top">
<p>View<a id="id193" class="indexterm"/> existing and create new index warmer queries</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Snapshot</p>
</td><td style="text-align: left" valign="top">
<p>Create <a id="id194" class="indexterm"/>new index snapshots on the local filesystem, URL, S3, HDFS, or Azure</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Index Templates</p>
</td><td style="text-align: left" valign="top">
<p>View existing and<a id="id195" class="indexterm"/> create new index templates</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Cat APIs</p>
</td><td style="text-align: left" valign="top">
<p>Run a<a id="id196" class="indexterm"/> subset of all possible Elasticsearch API "Cat" methods</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Hot Threads</p>
</td><td style="text-align: left" valign="top">
<p>Query for <a id="id197" class="indexterm"/>Elasticsearch "Hot" threads</p>
</td></tr></tbody></table></div><p>The following screenshot shows the <span class="strong"><strong>HOT THREADS</strong></span> page. This page is helpful when diagnosing slow search and indexing performance:</p><div class="mediaobject"><img src="graphics/B03798_05_04.jpg" alt="The more dropdown"/><div class="caption"><p>Hot Threads</p></div></div></div></div></div></div>
<div class="section" title="Working with Logstash and Kibana"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Working with Logstash and Kibana</h1></div></div></div><p>Logstash is a <a id="id198" class="indexterm"/>utility for aggregating and normalizing log files from disparate sources and<a id="id199" class="indexterm"/> storing them in an Elasticsearch cluster. Once logs are stored in Elasticsearch, we will use Kibana, the same tool Marvel's user interface is built on, to view and explore our aggregated logs.</p><div class="section" title="ELK"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec48"/>ELK</h2></div></div></div><p>The Elasticsearch community<a id="id200" class="indexterm"/> refers to the Elasticsearch, Logstash, and Kibana tool combination as the ELK stack. This section shows how to load NGINX server logs into ELK, but there are many other potential use cases for these technologies.</p><p>ELK can help us explore NGINX server logs by:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Visualizing server traffic over time</li><li class="listitem" style="list-style-type: disc">Plotting server visits by location on a map</li><li class="listitem" style="list-style-type: disc">Searching logs by resource extension (HTML, JS, CSS, and so on), IP address, byte count, or user-agent strings</li><li class="listitem" style="list-style-type: disc">Discovering web requests that result in internal server errors</li><li class="listitem" style="list-style-type: disc">Finding attackers in a distributed denial of service attack</li></ul></div><p>Other uses for <a id="id201" class="indexterm"/>ELK include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Logging all Elasticsearch queries in a web application for future performance analysis</li><li class="listitem" style="list-style-type: disc">Aggregating server system logs into one location for analysis and visualization</li><li class="listitem" style="list-style-type: disc">Logging operations from a data processing or ingestion pipeline for future analysis and auditing</li></ul></div></div><div class="section" title="Installation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec49"/>Installation</h2></div></div></div><p>Although this example will store aggregate log data from Logstash directly into Elasticsearch, it's important to<a id="id202" class="indexterm"/> ensure that these aggregated logs do not affect the performance of the production cluster. To avoid this potential performance problem, we'll configure Logstash to route logs to a secondary monitoring cluster; in our case, this is the <code class="literal">elasticsearch-marvel-01</code> node.</p><div class="section" title="Installing Logstash"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec08"/>Installing Logstash</h3></div></div></div><p>It doesn't matter what<a id="id203" class="indexterm"/> host Logstash lives on, since it can redirect logs to any Elasticsearch<a id="id204" class="indexterm"/> instance. Since Kibana will be installed on <code class="literal">elasticsearch-marvel-01</code>, we'll put Logstash there as well:</p><p>From <code class="literal">elasticsearch-marvel-01</code>, run the following:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo mkdir /opt/log</strong></span>
<span class="strong"><strong>stash</strong></span>
<span class="strong"><strong>sudo chown -R `whoami` /opt/log</strong></span>
<span class="strong"><strong>stash</strong></span>
<span class="strong"><strong>cd /opt/log</strong></span>
<span class="strong"><strong>stash</strong></span>
<span class="strong"><strong>wget https://download.elastic.co/logstash/logstash/logstash-2.1.1.t</strong></span>
<span class="strong"><strong>ar.gz</strong></span>
<span class="strong"><strong>tar xzvf logstash-2.1.1.t</strong></span>
<span class="strong"><strong>ar.gz</strong></span>
<span class="strong"><strong>cd logstash-2</strong></span>
<span class="strong"><strong>.1.1/</strong></span>
</pre></div></div><div class="section" title="Loading NGINX logs"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec09"/>Loading NGINX logs</h3></div></div></div><p>Now let's load some <a id="id205" class="indexterm"/>sample NGINX logs into Elasticsearch using Logstash. While Logstash has built-in parsers for many different log types (Apache, Linux syslogs, and so on), it doesn't natively support NGINX logs. This means that users have to explicitly tell Logstash how to deal with these files. To address this, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Put some sample<a id="id206" class="indexterm"/> NGINX log files in <code class="literal">/opt/logstash/</code><code class="literal">logs:</code><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>$ ls -1 /opt/logstash/logs | head –n20</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03798_05_05.jpg" alt="Loading NGINX logs"/><div class="caption"><p>NGINX log files for Logstash</p></div></div></li><li class="listitem">Create a new file on <code class="literal">/opt/logstash/patterns/nginx.grok</code> on <code class="literal">elasticsearch-marvel-01</code> with the following content:<div class="informalexample"><pre class="programlisting">NGINXACCESS %{IPORHOST:remote_addr} - %{USERNAME:remote_user} \[%{HTTPDATE:timestamp}\] %{QS:request} %{INT:status} %{INT:body_bytes_sent} %{QS:http_referer} %{QS:http_user_agent}</pre></div><p>Then create a Logstash configuration file at <code class="literal">/opt/logstash/logstash.conf</code> with the following content:</p><div class="informalexample"><pre class="programlisting">input {
  file {
    type =&gt; "nginx"
    path =&gt; "/opt/logstash/logs/access.log*"
    start_position =&gt; "beginning"
    sincedb_path =&gt; "/dev/null"
  }
}

filter {
  if [type] == "nginx" {
  grok {
    patterns_dir =&gt; "./patterns"
    match =&gt; {
        "message" =&gt; "%{NGINXACCESS}"
    }
  }
  date {
    match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
  geoip {
    source =&gt; "remote_addr"
  }

  }
}

output {
  elasticsearch { hosts =&gt; ["elasticsearch-marvel-01:9200"] }
}</pre></div><p>This configuration<a id="id207" class="indexterm"/> tells Logstash to read all <code class="literal">access.log*</code> files from the filesystem using our newly defined <code class="literal">nginx</code> format, identifies the timestamp column used by our NGINX format, tells Logstash to use a Geo IP lookup on the visitor's IP address, and finally tells Logstash to save logs to the Elasticsearch host instance at <code class="literal">elasticsearch-marvel-01:9200</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>Read more about the<a id="id208" class="indexterm"/> Logstash configuration file format at: <a class="ulink" href="https://www.elastic.co/guide/en/logstash/current/configuration.html">https://www.elastic.co/guide/en/logstash/current/configuration.html</a>.</p></div></div></li><li class="listitem">Now run Logstash, specifying<a id="id209" class="indexterm"/> the configuration file:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>cd /opt/logstash</strong></span>
<span class="strong"><strong>./logstash-2.1.1/bin/logstash agent -f logstash.conf </strong></span>
</pre></div><p>After a few minutes, Logstash will load all of the data into Elasticsearch. The new index created in Kopf should now be viewable.</p><p>The next section will focus on exploring the data in Kibana.</p><div class="mediaobject"><img src="graphics/B03798_05_06.jpg" alt="Loading NGINX logs"/><div class="caption"><p>Viewing Logstash index from Kopf</p></div></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>This process will go much faster if the <code class="literal">geoip</code> configuration setting from the <code class="literal">logstash.conf</code> configuration file is removed.</p></div></div></div><div class="section" title="Installing Kibana"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec10"/>Installing Kibana</h3></div></div></div><p>Follow these steps to<a id="id210" class="indexterm"/> install Kibana on your system:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Determine the<a id="id211" class="indexterm"/> appropriate version of Kibana to download from <a class="ulink" href="https://www.elastic.co/downloads/kibana">https://www.elastic.co/downloads/kibana</a>. Since this example is <a id="id212" class="indexterm"/>using Elasticsearch 2.3.2, we'll install Kibana 4.5.4.</li><li class="listitem">Download and unpackage Kibana on <code class="literal">elasticsearch-marvel-01</code> in the <code class="literal">/opt/kibana/</code> directory:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo mkdir /opt/kibana</strong></span>
<span class="strong"><strong>sudo chown -R `whoami` /opt/kibana/</strong></span>
<span class="strong"><strong>cd /opt/kibana/</strong></span>
<span class="strong"><strong>wget https://download.elastic.co/kibana/kibana/kibana-4.5.0-linux-x64.tar.gz</strong></span>
<span class="strong"><strong>tar xzvf kibana-4.5.0-linux-x64.tar.gz</strong></span>
<span class="strong"><strong>cd kibana-4.5.0-linux-x64/</strong></span>
</pre></div></li><li class="listitem">Edit Kibana's <code class="literal">conf/kibana.yml</code> file to point to the correct Elasticsearch host. In this case, change this:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong># The Elasticsearch instance to use for all your queries.</strong></span>
<span class="strong"><strong>elasticsearch_url: "http://localhost:9200"</strong></span>
</pre></div><p>To:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong># The Elasticsearch instance to use for all your queries.</strong></span>
<span class="strong"><strong>elasticsearch_url: "http://elasticsearch-marvel-01:9200"</strong></span>
</pre></div></li><li class="listitem">Now start Kibana:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>./bin/kibana</strong></span>
</pre></div></li><li class="listitem">Visit <code class="literal">http://elasticsearch-marvel-01:5601/</code> to view the Kibana landing page. It should look like the following screenshot:<div class="mediaobject"><img src="graphics/B03798_05_07.jpg" alt="Installing Kibana"/><div class="caption"><p>Configuring Kibana</p></div></div></li><li class="listitem">Notice<a id="id213" class="indexterm"/> <code class="literal">logstash-*</code> is already selected by default, so just click the <span class="strong"><strong>Create</strong></span> button to continue.</li><li class="listitem">Navigate to the <span class="strong"><strong>Discover</strong></span> tab to start exploring your log data:<div class="mediaobject"><img src="graphics/B03798_05_08.jpg" alt="Installing Kibana"/><div class="caption"><p>Search for data in Kibana</p></div></div><p>You may not see any data here at first. This is because all of the data loaded is more than 15 minutes old.</p></li><li class="listitem">Click the date<a id="id214" class="indexterm"/> range filter in the upper-right of the page, by default set to <span class="strong"><strong>Last 15 Minutes</strong></span>. Now select a more appropriate range, such as <span class="strong"><strong>This Month</strong></span>. You should now start to see some results:<div class="mediaobject"><img src="graphics/B03798_05_09.jpg" alt="Installing Kibana"/><div class="caption"><p>Viewing log data from this month</p></div></div></li></ol></div><p>The <code class="literal">default _source</code> column is a little hard to read, so specify some columns from the left-hand side of the <a id="id215" class="indexterm"/>page: <code class="literal">http_user_agent</code>, <code class="literal">remote_addr</code>, and <code class="literal">status</code>. Clicking on any of these selected columns will run an aggregation query displaying the most commonly occurring values for each field:</p><div class="mediaobject"><img src="graphics/B03798_05_10.jpg" alt="Installing Kibana"/><div class="caption"><p>Applying search filters to Kibana results</p></div></div><p>The <span class="strong"><strong>Visualize</strong></span> page lets us create arbitrary data visualizations. As an example, we'll create two sample<a id="id216" class="indexterm"/> visualizations: a Tile map to plot the geolocated IP addresses in our dataset, and a Vertical bar chart for displaying counts of different HTTP status codes in the log files. Follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, configure the Tile map visualization as shown here:<div class="mediaobject"><img src="graphics/B03798_05_11.jpg" alt="Installing Kibana"/><div class="caption"><p>Geospatial visualization of Kibana results</p></div></div></li><li class="listitem">Click <span class="strong"><strong>Save</strong></span> to <a id="id217" class="indexterm"/>save your changes, and create the Vertical bar chart:<div class="mediaobject"><img src="graphics/B03798_05_12.jpg" alt="Installing Kibana"/><div class="caption"><p>Breakdown by HTTP status code</p></div></div></li><li class="listitem">After saving<a id="id218" class="indexterm"/> this chart, go to the <span class="strong"><strong>Dashboard</strong></span> page in order to display both components on the same page.</li><li class="listitem">Select the two components by clicking the <span class="strong"><strong>Add Visualization</strong></span> button. Move them around the dashboard to resize and reorder them until you get something like this:<div class="mediaobject"><img src="graphics/B03798_05_13.jpg" alt="Installing Kibana"/><div class="caption"><p>Kibana dashboard view</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>Learn more<a id="id219" class="indexterm"/> about Kibana and Logstash by visiting the official Elasticsearch documentation at:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.elastic.co/videos/kibana-logstash">https://www.elastic.co/videos/kibana-logstash</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.elastic.co/products/kibana">https://www.elastic.co/products/kibana</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.elastic.co/products/logstash">https://www.elastic.co/products/logstash</a></li></ul></div></div></div></li></ol></div></div></div></div>
<div class="section" title="Working with Nagios"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Working with Nagios</h1></div></div></div><p>Nagios is a system<a id="id220" class="indexterm"/> monitoring and alerting tool. This section will focus on configuring a simple Nagios installation that monitors the nodes in our Elasticsearch cluster, as well as the Elasticsearch process on those. If a node or process shuts down, Nagios will send us an alert.</p><p>It's a good idea to install Nagios on a host outside of the Elasticsearch clusters in order to avoid affecting the monitoring process due to other things going on in the system, such as high Elasticsearch load. Create a new host for Nagios and call it <code class="literal">elasticsearc</code>
<code class="literal">h-nagios-01.</code>
</p><div class="section" title="Installing Nagios"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec50"/>Installing Nagios</h2></div></div></div><p>In addition to the dedicated Nagios<a id="id221" class="indexterm"/> host, <code class="literal">elasticsearch-nagios-01</code>, install the <a id="id222" class="indexterm"/>
<span class="strong"><strong>Nagios Remote Plugin Executor</strong></span> (<span class="strong"><strong>NRPE</strong></span>) server on all of the Elasticsearch cluster nodes in order to monitor the Elasticsearch process. Follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Run the following command on each of the Elasticsearch nodes: <code class="literal">elasticsearch-node-01</code>, <code class="literal">elasticsearch-node-02</code>, <code class="literal">elasticsearch-node-03</code>, and <code class="literal">elasticsea</code><code class="literal">rch-marvel-01</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo apt-get install nagios-nrpe-server</strong></span>
</pre></div></li><li class="listitem">Then install Nagios on the new host <code class="literal">elasticsearch-nagios-01</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo apt-get install nagios3 nagios-nrpe-plugin</strong></span>
</pre></div></li><li class="listitem">This process will ask you to enter a password. Make sure you remember it.<p>Now a Nagios plugin is needed to ensure that Elasticsearch is running. There are <a id="id223" class="indexterm"/>several plugins available, but this book uses a simple script available on GitHub: <a class="ulink" href="https://github.com/orthecreedence/check_elasticsearch">https://github.com/orthecreedence/check_elasticsearch</a>.</p></li><li class="listitem">To download and install this script on <code class="literal">elasticsearch-nagios-01</code>, run:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>wget https://raw.githubusercontent.com/orthecreedence/check_elasticsearch/master/check_elasticsearch</strong></span>
<span class="strong"><strong>chmod +x check_elasticsearch</strong></span>
<span class="strong"><strong>sudo cp check_elasticsearch /usr/lib/nagios/plugins/</strong></span>
</pre></div></li><li class="listitem">Next, add a Nagios command to run this plugin. On <code class="literal">elasticsearch-nagios-01</code>, create a new file, <code class="literal">/etc/nagios-plugins/config/elasticsearch.cfg</code>, with this content:<div class="informalexample"><pre class="programlisting"># Check to ensure elasticsearch is running
define command{
        command_name    check_elasticsearch
        command_line    /usr/lib/nagios/plugins/check_elasticsearch -H $HOSTNAME$ -P 9200
        }</pre></div></li><li class="listitem">Lastly, specify which hosts to monitor for the Nagios server. Be sure to have it monitor the Elasticsearch process on those hosts, using the <code class="literal">check_elasticsearch</code> utility, by editing<a id="id224" class="indexterm"/> the configuration file <code class="literal">/etc/nagios3/conf.d/localhost_nagios2.cfg</code>:<div class="informalexample"><pre class="programlisting">define host{
        use                     generic-host            
        host_name               elasticsearch-node-01
        alias                   elasticsearch-node-01
        address                 192.168.56.111
        }

define host{
        use                     generic-host            
        host_name               elasticsearch-node-02
        alias                   elasticsearch-node-02
        address                 192.168.56.112
        }

define host{
        use                     generic-host            
        host_name               elasticsearch-node-03
        alias                   elasticsearch-node-03
        address                 192.168.56.113
        }

define host{
        use                     generic-host            
        host_name               elasticsearch-marvel-01
        alias                   elasticsearch-marvel-01
        address                 192.168.56.120
        }

define hostgroup {
        hostgroup_name  elasticsearch-servers
                alias           Elasticsearch servers
                members         elasticsearch-node-01, elasticsearch-node-02, elasticsearch-node-03, elasticsearch-marvel-01
        }

define contact{
        contact_name Elasticsearch Admin
        service_notification_period 24x7
        host_notification_period 24x7
        service_notification_options w,u,c,r,f
        host_notification_options d,u,r,f
        service_notification_commands notify-service-by-email
        host_notification_commands notify-host-by-email
        email admin@your-domain.com
        }

define service{
        use                             generic-service         ; Name of service template to use
        hostgroup_name                  elasticsearch-servers
        service_description             Elasticsearch
        check_command                   check_elasticsearch
        }</pre></div></li><li class="listitem">Next, configure <code class="literal">elasticsearch-node-01</code>, <code class="literal">elasticsearch-node-02</code>, <code class="literal">elasticsearch-node-03</code>, and <code class="literal">elasticsearch-marvel-01</code> to allow our Nagios host <code class="literal">elasticsearch-nagios-01</code> to collect metrics:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo vim /etc/nagios/nrpe.cfg</strong></span>
</pre></div></li><li class="listitem">Edit the <code class="literal">allowed_hosts</code> setting to include the <code class="literal">elasticsearch-nagios-01</code> IP address; in our<a id="id225" class="indexterm"/> case, this is <code class="literal">192.168.56.130</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>allowed_hosts=127.0.0.1,192.168.56.130</strong></span>
</pre></div></li><li class="listitem">Now restart the NRPE server on all nodes in our Elasticsearch clusters:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo service nagios-nrpe-server restart</strong></span>
</pre></div></li><li class="listitem">And finally, restart <code class="literal">nagios3</code> on <code class="literal">elasticsearch-nagios-01</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo service nagios3 restart</strong></span>
</pre></div></li><li class="listitem">Open a web browser to see the Nagios web administration portal, with the username <code class="literal">nagiosadmin</code> and the password you entered earlier:<div class="informalexample"><pre class="programlisting">
<code class="literal">http://elasticsearch-nagios-01/nagios3</code>
</pre></div></li><li class="listitem">After giving Nagios a few minutes to collect metrics on all nodes, click on the <span class="strong"><strong>Hosts</strong></span> sidebar link to see the state of all nodes in the clusters:<div class="mediaobject"><img src="graphics/B03798_05_14.jpg" alt="Installing Nagios"/><div class="caption"><p>Viewing Elasticsearch hosts in Nagios</p></div></div></li><li class="listitem">Click <span class="strong"><strong>Services</strong></span> in the<a id="id226" class="indexterm"/> left-hand menu to see the state of the Elasticsearch process on each node:<div class="mediaobject"><img src="graphics/B03798_05_15.jpg" alt="Installing Nagios"/><div class="caption"><p>Viewing Elasticsearch status in Nagios</p></div></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>Notice that in <code class="literal">elasticsearch-marvel-01</code>, the Elasticsearch process is in a yellow <span class="strong"><strong>WARNING</strong></span> state. This means that the cluster is in a <code class="literal">yellow</code> state, because there is only one node in the Marvel cluster and not all shards are replicated.</p></div></div><p>Now we'll demonstrate <a id="id227" class="indexterm"/>what Nagios does when one node shuts down and we stop the Elasticsearch process on a different node.</p><p>Shut down <code class="literal">elasticsearch-node-01</code> and disable Elasticsearch on <code class="literal">elasticsearch-node-02</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>ssh root@elasticsearch-node-01</strong></span>
<span class="strong"><strong>shutdown -h now</strong></span>
<span class="strong"><strong>ssh root@elasticsearch-node-02</strong></span>
<span class="strong"><strong>service elasticsearch stop</strong></span>
</pre></div><p>The next time Nagios polls for the cluster state (this will take a few minutes), the following will display on the Nagios web dashboard's services page:</p><div class="mediaobject"><img src="graphics/B03798_05_16.jpg" alt="Installing Nagios"/><div class="caption"><p>Error reporting in Nagios</p></div></div><p>Nagios now indicates that <code class="literal">elasticsearch-node-01</code> is down and that it can't connect to the Elasticsearch process on <code class="literal">elasticsearch-node-02</code>. Nagios also indicates Elasticsearch has entered<a id="id228" class="indexterm"/> a <code class="literal">red</code> state on <code class="literal">elasticsearch-node-03</code> because not all shards are available. Nagios will send <code class="literal">admin@your-domain.com</code> an email about the warnings and errors, based on our previous configuration. Things will return to normal after starting <code class="literal">elasticsearch-node-01</code> and restarting Elasticsearch on <code class="literal">elast</code>
<code class="literal">icsearch-node-02</code>.</p></div></div>
<div class="section" title="Command line tools for system and process management"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Command line tools for system and process management</h1></div></div></div><p>The command line is<a id="id229" class="indexterm"/> an invaluable tool for system monitoring. In this section, we'll go over a few basic GNU/Linux command line utilities for system and process management. Knowing these tools is essential for anyone managing an Elasticsearch cluster on GNU/Linux.</p><div class="section" title="top"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec51"/>top</h2></div></div></div><p>The <code class="literal">top</code> command lists <a id="id230" class="indexterm"/>processes with the highest CPU and memory. This tool is useful to determine whether a process other than Elasticsearch is hogging resources, or to check whether Elasticsearch is using an abnormal amount of CPU or memory.</p><p>The<code class="literal"> top</code> command refreshes automatically, so you only have to run it once and watch.</p><p>When running the command, you should see the following result:</p><div class="mediaobject"><img src="graphics/B03798_05_17.jpg" alt="top"/><div class="caption"><p>The top command</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>Press <span class="emphasis"><em>Shift</em></span>+<span class="emphasis"><em>M</em></span> while <code class="literal">top</code> is running to sort processes by those using the most memory instead of CPU.</p></div></div></div><div class="section" title="tail"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec52"/>tail</h2></div></div></div><p>The <code class="literal">tail -f</code> command is <a id="id231" class="indexterm"/>useful for viewing log files in real time. Use it to view Elasticsearch log files as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>tail -f /var/log/elasticsearch/*</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03798_05_18.jpg" alt="tail"/><div class="caption"><p>"tailing" Elasticsearch log files</p></div></div></div><div class="section" title="grep"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec53"/>grep</h2></div></div></div><p>The <code class="literal">grep</code> command is a general <a id="id232" class="indexterm"/>purpose text search tool. One useful application of <code class="literal">grep</code> is to search through a directory of log files for a specific string. To <span class="emphasis"><em>grep</em></span> or search <code class="literal">/var/log/elasticsearch</code> for all logged exceptions, run the following command with the <code class="literal">-r</code> (recursive) and <code class="literal">-i</code> (case insensitive search) options:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>grep -ir exception /var/log/elasticsearch/*.log</strong></span>
</pre></div><p>Assuming there are some exceptions logged in your Elasticsearch log files, you should see something like this:</p><div class="mediaobject"><img src="graphics/B03798_05_19.jpg" alt="grep"/><div class="caption"><p>"grepping" log files for exceptions</p></div></div></div><div class="section" title="ps"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/>ps</h2></div></div></div><p>Use the <code class="literal">ps</code> command<a id="id233" class="indexterm"/> along with the <code class="literal">grep</code> command to see whether a particular process is running. This is a useful sanity check if you are running into issues stopping or starting Elasticsearch (or another process).</p><p>To check whether Elasticsearch is running, use the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>ps -ef | grep -i elasticsearch</strong></span>
</pre></div><p>This command will output nothing if Elasticsearch is not running. If it's running, you should see something like this:</p><div class="mediaobject"><img src="graphics/B03798_05_20.jpg" alt="ps"/><div class="caption"><p>Using ps to view the Elasticsearch process</p></div></div></div><div class="section" title="kill"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec55"/>kill</h2></div></div></div><p>Use the <code class="literal">kill</code> command stop a<a id="id234" class="indexterm"/> process that won't shut down gracefully. For example, to shut down the Elasticsearch process listed previously, run the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>sudo kill 2501</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03798_05_21.jpg" alt="kill"/><div class="caption"><p>Kill the Elasticsearch process and verify it with the ps command</p></div></div></div><div class="section" title="free"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/>free</h2></div></div></div><p>The <code class="literal">free</code> command tells<a id="id235" class="indexterm"/> us how much memory is in use on a system. Its usage is:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>free -m</strong></span>
</pre></div><p>Running this command will yield something similar to:</p><div class="mediaobject"><img src="graphics/B03798_05_22.jpg" alt="free"/><div class="caption"><p>The free command shows the amount of RAM on the system</p></div></div><p>This output means that <a id="id236" class="indexterm"/>we are using 333 MB of our available 490 MB memory store.</p></div><div class="section" title="du and df"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/>du and df</h2></div></div></div><p>The <code class="literal">du</code> and <code class="literal">df</code> commands<a id="id237" class="indexterm"/> tell us how much disk space is available on the host. Use <code class="literal">du</code> to see how much data is stored in the<a id="id238" class="indexterm"/> current directory, as shown here:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>cd /var/log/elasticsearch</strong></span>
<span class="strong"><strong>du -h</strong></span>
</pre></div><p>You should see a result similar to this:</p><div class="mediaobject"><img src="graphics/B03798_05_23.jpg" alt="du and df"/><div class="caption"><p>The du command calculates the size of a directory</p></div></div><p>In this case, there<a id="id239" class="indexterm"/> are 15 MB of log files in <code class="literal">/var/log/elasticsearch/</code>.</p><p>Use <code class="literal">df</code> to see how much<a id="id240" class="indexterm"/> disk space is available across the system, as shown here:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>df -h</strong></span>
</pre></div><p>You should see a result similar to this:</p><div class="mediaobject"><img src="graphics/B03798_05_24.jpg" alt="du and df"/><div class="caption"><p>Disk usage on elasticsearch-node-01</p></div></div><p>The output here <a id="id241" class="indexterm"/>says there is <code class="literal">1.3G</code> of available storage left on the <code class="literal">/</code> mount point.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>Note that in <a id="id242" class="indexterm"/>both of these commands, the <code class="literal">-h</code> flag stands<a id="id243" class="indexterm"/> for <span class="strong"><strong>human readable</strong></span>, meaning they will output values in terms of KB, MB, or GB, as opposed to just bytes.</p></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Summary</h1></div></div></div><p>This chapter examined the Elasticsearch monitoring tool Kopf, the Elasticsearch, Logstash, and Kibana (ELK) log aggregation stack, the system monitoring tool Nagios, and various GNU/Linux command line utilities.</p><p>Some takeaways are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Kopf is an Elasticsearch monitoring tool similar to Elasticsearch-head, but provides a few different metrics.</li><li class="listitem" style="list-style-type: disc">The Elasticsearch, Logstash, and Kibana (ELK) stack is a tool for searching, analyzing, enriching, and visualizing log files.</li><li class="listitem" style="list-style-type: disc">Consider using a tool such as Nagios to monitor an Elasticsearch cluster. Nagios can be configured to send out email notifications when a process goes down or if the node itself goes down.</li><li class="listitem" style="list-style-type: disc">Using a few GNU/Linux command tools, we can gather many of the same metrics provided by the various Elasticsearch monitoring tools.</li></ul></div><p>The next chapter will discuss troubleshooting Elasticsearch performance and reliability issues. The monitoring tools discussed in this chapter will be useful when tackling the real-world problems outlined in upcoming chapters.</p></div></body></html>