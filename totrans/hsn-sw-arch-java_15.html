<html><head></head><body>
		<div id="_idContainer117">
			<h1 id="_idParaDest-290"><a id="_idTextAnchor292"/>Chapter 12: Cross-Cutting Concerns</h1>
			<p>Throughout the previous chapters, we have explored many different aspects of Java application development. Starting from the beginning of the development life cycle (including requirements collection and architecture design), we've focused on many different technological aspects, including frameworks and middleware.</p>
			<p>At this point, several cross-cutting concerns need to be examined, regardless of the kind of application we are building and the architectural style we choose. In this chapter, we are going to look at a few of these aspects, as follows:</p>
			<ul>
				<li>Identity management</li>
				<li>Security</li>
				<li>Resiliency</li>
			</ul>
			<p>The cross-cutting concerns <a id="_idIndexMarker1555"/>discussed in this chapter provide some very useful information about topics that are crucial for a project's success. Indeed, implementing identity management, security, and resiliency in the right way can be beneficial to the success of our application, both from an architectural point of view (by providing elegant, scalable, and reusable solutions) and a functional point of view (by avoiding reinventing the wheel and approaching these issues in a standardized way).</p>
			<p>With that said, let's get started with a classic issue in application development: identity management.</p>
			<h1 id="_idParaDest-291"><a id="_idTextAnchor293"/>Identity management</h1>
			<p><strong class="bold">Identity management</strong> is a <a id="_idIndexMarker1556"/>broad concept that deals with many different aspects and involves interaction with many different systems.</p>
			<p>This concept is indeed related to identifying a user (that is, who is asking for a particular resource or functionality) and checking the associated permissions (whether they are allowed to do so and so, or not). So, it's easy to see how this is a core concept, common in many applications and many components inside the application. If we have different functionalities provided by different components (as in a microservices application), then obviously each of them will need to perform the same kind of checks, to be sure about the user's identity and act accordingly.</p>
			<p>However, having<a id="_idIndexMarker1557"/> an ad hoc identity management infrastructure for each application can be considered an <em class="italic">antipattern</em>, especially in a complex enterprise environment, since each application (or component) has the same goal of identifying the user and its permissions.</p>
			<p>For this reason, a common approach is to define a company-wide identity management strategy and adopt it in all of the applications, including the off-premises and microservices architectures.</p>
			<p>Now, to come back to the fundamentals, identity management is basically about two main concepts:</p>
			<ul>
				<li><strong class="bold">Authentication</strong>: This<a id="_idIndexMarker1558"/> is a way of ensuring, with the maximum possible degree of certainty, that the person asking for access to a resource (or to perform an action) is the person that they claim to be. Here is a diagram of the username and password authentication method:</li>
			</ul>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/Figure_12.1_B16354.jpg" alt="Figure 12.1 – Authentication&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Authentication</p>
			<ul>
				<li><strong class="bold">Authorization</strong>: This <a id="_idIndexMarker1559"/>is a way of declaring who can access each resource and perform a specific action, as shown in the following diagram. This may involve authenticated and non-authenticated entities (sometimes referred to as anonymous access).</li>
			</ul>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/Figure_12.2_B16354.jpg" alt="Figure 12.2 – Authorization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Authorization</p>
			<p>Both authentication and authorization include two main scenarios:</p>
			<ul>
				<li><strong class="bold">Machine to machine</strong>: This <a id="_idIndexMarker1560"/>is when the entity requesting access is an application, for example, in batch calculations or other processes that do not directly involve the interaction of a human user. This is<a id="_idIndexMarker1561"/> also called <strong class="bold">server to server</strong>.</li>
				<li><strong class="bold">Interactive</strong> or <strong class="bold">use</strong>: This is <a id="_idIndexMarker1562"/>the other scenario, with a human operator interacting directly with the resource, hence requesting authentication and authorization.</li>
			</ul>
			<p>Now that we have the hang of some basic concepts, let's learn a bit more about authentication and authorization.</p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor294"/>Authentication</h2>
			<p>As stated, <strong class="bold">authentication</strong> is about <a id="_idIndexMarker1563"/>verifying that the <a id="_idIndexMarker1564"/>entity performing a request (be it a human or a machine) is who they claim to be. There are many different ways to perform this verification. The main differentiator is what the user presents (and needs to be checked). It falls into one of the following three categories:</p>
			<ul>
				<li><strong class="bold">Something that the user knows</strong>: This <a id="_idIndexMarker1565"/>refers to secrets, such as passwords, pins, or similar things, like the sequence to unlock a mobile phone.</li>
				<li><strong class="bold">Something that the user has</strong>: This refers to physical devices (such as badges or hardware tokens) or software artifacts (such as certificates and software tokens).</li>
				<li><strong class="bold">Something that the user is</strong>: In this <a id="_idIndexMarker1566"/>case, authentication is linked to biometric factors (such as a fingerprint or face identification), or similar things like a signature.</li>
			</ul>
			<p>There are <a id="_idIndexMarker1567"/>several things to consider here, as follows:</p>
			<ul>
				<li>The first is that a piece of public information, such as a username, may be associated with the authentication factor. In this case, multiple users can share the same factor (such as a password or a badge) and we can tell them apart by using the username. The unintentional occurrence of this pattern (such as two users choosing the same password by accident) may be harmless, whereas intentional implementations (multiple users using the same badge) can be a security issue.</li>
				<li>You also have to consider that a combination of more than one authentication factor is considered a best practice and is encouraged for stronger security implementations. This is<a id="_idIndexMarker1568"/> called <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>). Moreover, in some specific environments (such as banking) this may be mandated by specific regulations. Strong authentication is often one of those specifics and refers to an authentication process leveraging at least two different factors, belonging to different groups (for example, <em class="italic">something that a user knows</em>, plus <em class="italic">something that a user has</em>).</li>
				<li>Some authentication factors may be subject to policies. The most common examples are password rules (length, complexity) or expiration policies (forcing a user to change a factor after a certain time where possible).</li>
			</ul>
			<p>Of course, an immediate concern that comes to mind is how and where to store the information relevant<a id="_idIndexMarker1569"/> for implementing authentication – in other words, where to save our usernames and passwords (and/or the other kinds of secrets used for authentication). </p>
			<p>The most common technology <a id="_idIndexMarker1570"/>used for this goal is <strong class="bold">LDAP</strong>, which is short for <strong class="bold">Lightweight Directory Access Protocol</strong>. LDAP<a id="_idIndexMarker1571"/> is a protocol for storing user information. An LDAP server can be seen as a standard way to store information about users, including things such as usernames, emails, phone numbers, and, of course, passwords. Being quite an old standard, around since the 1990s, it's widely adopted and compatible with a lot of other technology. </p>
			<p>Without going into too much detail, we can look at it as just another datastore, which we can connect to using a connection URL. Then, we can query the datastore by passing specific attributes to search for specific entries. </p>
			<p>The authentication operation against an LDAP server is <a id="_idIndexMarker1572"/>called <strong class="bold">Bind</strong>. LDAP can typically encrypt the passwords in various ways. One very famous implementation of an LDAP server (technically, an extension of it, providing<a id="_idIndexMarker1573"/> more services than just the standard) is <strong class="bold">Microsoft Active Directory</strong>.</p>
			<p>LDAP is not the only way to store user information (including passwords) but is likely the only widely adopted standard. Indeed, it is common to store user information in relational databases, but this is almost exclusively done in a custom way, meaning that there is no standard naming nor formats for tables and columns storing usernames, passwords, and so on.</p>
			<p>One other <a id="_idIndexMarker1574"/>way to store user information is to use files, but this is an approach that's not scalable nor efficient. It works mostly for a small set of users or testing purposes. A common file format used to store user<a id="_idIndexMarker1575"/> information is <strong class="source-inline">.htpasswd</strong>, which is simply a flat file storing a username and password, in a definition originally used by the Apache httpd server for authentication purposes.</p>
			<p>It is a commonly accepted best practice to store passwords in an encrypted form whenever possible. This is a crucial point. Whatever the user store technology (such as LDAP or a database), it is crucial that the passwords are not stored in cleartext. The reason is simple and quite obvious: if our server gets compromised in some way, the attacker should not be able to access the stored passwords.</p>
			<p>I have used<a id="_idIndexMarker1576"/> the word <em class="italic">encryption</em> generically. A solution, indeed, can be to encrypt the passwords with a symmetrical algorithm, such as AES. Symmetrical encryption implies that by using a specific secret key, I can make the password unusable. Then, I can again decrypt the password using the same key.</p>
			<p>This <a id="_idIndexMarker1577"/>approach is useful, but we will still need to store the key securely since an attacker with the encrypted password and the key can access the original password as cleartext. Hence, a more secure way is to store the hashed password.</p>
			<p>By hashing a password, you transform it into an encrypted string. The great thing, compared to the previous approach, is that we are implementing asymmetrical encryption. There is no way (if we are using a proper algorithm) to reverse the encrypted string to the original one in a reasonable amount of time. In this way, we can store the encrypted passwords without requiring any key. To validate the passwords provided by the clients, we simply apply the same hashing algorithm used for saving it initially and compare the results. Even if an attacker gains access to our user information store, the stolen encrypted passwords will be more or less useless.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">It's certainly better to encrypt a password rather than store it in cleartext; even the encrypted ones are not 100% secure. Indeed, even if it is impossible, in theory, to reconstruct the original password from a hashed value, some techniques attempt to do so. In particular, it is possible to try to run a brute-force attack, which basically tries a lot of passwords (from a dictionary, or simply random strings), hashes them, and compares the output with a known actual hash. A more efficient alternative is to use <strong class="bold">rainbow tables</strong>, which<a id="_idIndexMarker1578"/> are basically tables of passwords and their pre-computed hashes. Defenses against these kinds of techniques are possible, however, by using longer and more complex passwords and using salting, which is a way to add some more randomness to hashed passwords.</p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor295"/>Authorization</h2>
			<p>User <strong class="bold">authorization</strong> is <a id="_idIndexMarker1579"/>complementary to authentication. Once<a id="_idIndexMarker1580"/> we are sure that a user is who they claim to be (using authentication), we have to understand what they are allowed to do. This means which resources and which operations they are permitted to use.</p>
			<p>The most basic form of authorization is no authorization. In simple systems, you can allow an authenticated user to do everything.</p>
			<p>A <a id="_idIndexMarker1581"/>better approach, in real-world applications, is to grant granular permissions, differentiated for different kinds of users. This is basically the concept of roles. </p>
			<p>A <strong class="bold">role</strong> can be considered a link between a set of users and a set of permissions. It is usually mapped to a job function or a department and is defined by a list of permissions, in terms of resources that can be accessed and functionalities that can be used. Each user can be associated with a role, and with this, they inherit the permissions associated with that role. </p>
			<p>This kind of authorization methodology is called <strong class="bold">Role-Based Access Control</strong> (<strong class="bold">RBAC</strong>). Based <a id="_idIndexMarker1582"/>on the kind of RBAC implementation, each user can be assigned to more than one role, with different kinds of compositions. Normally, policies are additive, meaning<a id="_idIndexMarker1583"/> that a user belonging to more than one role gets all the permissions from both roles. However, this may be subject to slight changes, especially if the permissions conflict, up to the point that there may be implementations denying the possibility of having more than one role associated with each user.</p>
			<p>Another aspect of RBAC implementations concerns role inheritance. Some RBAC implementations employ the concept of a hierarchy of roles, meaning that a role can inherit the set of permissions associated with its parent role. This allows for a modular system. In the Java Enterprise world, <strong class="bold">JAAS</strong> (short for <strong class="bold">Java Authentication and Authorization Service</strong>) is the <a id="_idIndexMarker1584"/>implementation standard for authentication and authorization. It can be regarded as a reference implementation of an RBAC-based security system.</p>
			<p>An alternative to <a id="_idIndexMarker1585"/>RBAC is <strong class="bold">Policy-Based Access Control</strong> (<strong class="bold">PBAC</strong>). In this approach, the permission is calculated against a set of attributes, using Boolean logic, in the form of an <strong class="source-inline">if then</strong> statement, where more than one attribute can be combined with <strong class="source-inline">AND</strong>, <strong class="source-inline">OR</strong>, and other logic operators. The attributes can be simply related to the user (such as checking whether a user belongs to a particular group), or to other conditions (such as the time of the day, the source IP, and the geographical location).</p>
			<p><strong class="source-inline">SELinux</strong>, which<a id="_idIndexMarker1586"/> is a security module underlying some <strong class="bold">Linux</strong> OS variants (including <strong class="bold">Android</strong>) is a common implementation of PBAC.</p>
			<h2 id="_idParaDest-294"><a id="_idTextAnchor296"/>Identity and Access Management</h2>
			<p><strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) is a term usually associated with systems that <a id="_idIndexMarker1587"/>provide authentication, authorization, and other identity security services to client applications. The function of an IAM system is to implement such features in a unified way, so each application can directly use it and benefit from an adequate level of security. Other than what we have seen here in terms<a id="_idIndexMarker1588"/> of authentication and authorization, an IAM system also provides the following:</p>
			<ul>
				<li><strong class="bold">Decoupling the user store</strong>: This means that usernames, passwords, and other information can be stored in the technology of choice (such as LDAP or a database), and the client application does not need to know the implementation details. An IAM can also usually unify multiple storage systems in a unique view. And of course, if the user storage system needs to change (such as being moved from LDAP to a database), or we have to add a new one, we don't need to make any changes to the client applications.</li>
				<li><strong class="bold">Federating other authentication systems (such as more IAM systems)</strong>: This can be particularly useful in shared systems where access is required from more than one organization. Most of us have experienced something like this when accessing a service through a third-party login using <strong class="bold">Google</strong> or <strong class="bold">Facebook</strong>.</li>
				<li><strong class="bold">Single sign-on</strong> (<strong class="bold">SSO</strong>): This means<a id="_idIndexMarker1589"/> that we only need to log in (and log out) once, and then we can directly access the set of applications configured in the IAM.</li>
			</ul>
			<p>There are many different ways (and standards) to implement such features, depending on each specific IAM <a id="_idIndexMarker1590"/>product used. Such standards often boil down to some key concepts:</p>
			<ul>
				<li><strong class="bold">Provisioning and connecting each application managed by the IAM</strong>: This usually means configuring each application to point to the IAM. In the Java world, a common way to achieve this is to configure a servlet filter to intercept all requests. Other alternatives are agent software or reverse proxies that implement the same functionality of intercepting all the requests coming to our application.</li>
				<li><strong class="bold">Checking each request coming to each application</strong>: In case a request needs to be authenticated (because it is trying to access a protected resource or perform a limited action), check whether the client is already authenticated. If not, redirect to an authentication system (such as a login form).</li>
				<li><strong class="bold">Identifying the user</strong>: Once the client provides a valid authentication credential (such as a username and password), it must be provided with a unique<a id="_idIndexMarker1591"/> identifier, which is regarded as the <em class="italic">ID card</em> of the user, used to recognize it across different requests (and potentially log in to other applications in an SSO scenario). To do so, the client is often provided with a session token, which may then be stored by the client application (as in a cookie) and usually has a limited lifespan.</li>
			</ul>
			<p>A standard way to implement this kind of <a id="_idIndexMarker1592"/>scenario is the <strong class="bold">OAuth protocol</strong>. </p>
			<p>However, IAM is not the only security aspect that we need to take care of in a cloud-native architecture. Indeed, the topic of security in an application (especially in a cloud-native one) includes many more considerations. We are going to discuss some of them in the next section.</p>
			<h1 id="_idParaDest-295"><a id="_idTextAnchor297"/>Security</h1>
			<p><strong class="bold">Security</strong> is a <a id="_idIndexMarker1593"/>very complex aspect, as well as a foundational and crucial one. Unless security is your main focus (which is unlikely if you are in charge of defining the whole architecture of a cloud-native application), chances are that you will have some experts to work with. Nevertheless, it's important to take care of some simple security implications right from the outset of software implementation (including requirement collection, design, and development), to avoid going through a security check after you have completed architecture and development, only to realize that you have to make a lot of changes to implement security (thereby incurring costs and delays).</p>
			<p>This approach is often referred to<a id="_idIndexMarker1594"/> as <strong class="bold">shift-left security</strong>, and it's a common practice in <strong class="bold">DevOps</strong> teams.</p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor298"/>Intrinsic software security</h2>
			<p>The first<a id="_idIndexMarker1595"/> aspect to take care of is <strong class="bold">intrinsic software security</strong>. Indeed, software code can be subject to security vulnerabilities, often due to bugs or poor software testing.</p>
			<p>The<a id="_idIndexMarker1596"/> main scenario is software behaving unexpectedly as a result of a malformed or maliciously crafted input. Some common security issues<a id="_idIndexMarker1597"/> of this kind are the following:</p>
			<ul>
				<li><strong class="bold">SQL injection</strong>: A<a id="_idIndexMarker1598"/> malicious parameter is passed to the application and is attached to a SQL string. The application then performs a special SQL operation that is different from the expected operation and can allow the attacker access to unauthorized data (or even to damage existing data).</li>
				<li><strong class="bold">Unsafe memory handling</strong>: A <a id="_idIndexMarker1599"/>purposely wrong parameter is passed to the application and is copied to a special portion of memory, which the server interprets as executable code. Hence, unauthorized instructions can be executed. A well-known instance of this kind of bug <a id="_idIndexMarker1600"/>is the <em class="italic">buffer overflow</em>.</li>
				<li><strong class="bold">Cross-site scripting</strong>: This<a id="_idIndexMarker1601"/> is a specific security issue in web applications where an attacker can inject client-server code (such as JavaScript) that is then executed and the attacker can use it to steal data or perform unauthorized operations.</li>
			</ul>
			<p>There are several<a id="_idIndexMarker1602"/> techniques for avoiding or mitigating these issues:</p>
			<ul>
				<li><strong class="bold">Input sanitizing</strong>: Every <a id="_idIndexMarker1603"/>input should be checked for special characters and anything unnecessary. Checking the format and the length is also important.</li>
				<li><strong class="bold">Running as a user with limited permissions on the local machine (the fewer permissions, the better)</strong>: If there's an unexpected security exception, the impact may be limited.</li>
				<li><strong class="bold">Sandboxing</strong>: In<a id="_idIndexMarker1604"/> this case, the application will run within a limited and constrained environment. It is kind of an extension of the previous approach. There are various techniques for doing this, depending on the specific application technology. The JVM itself is kind of a sandbox. Containers are another way to implement sandboxing.</li>
			</ul>
			<p>The preceding <a id="_idIndexMarker1605"/>topics are a quick list of common issues (and advice to mitigate them) with regard to software development. However, these approaches, while crucial, are not exhaustive, and it's important to take a look at the overall security of our applications and systems, which will involve some other considerations.</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor299"/>Overall application security</h2>
			<p>Good<a id="_idIndexMarker1606"/> overall security starts with the way we write our application but doesn't end there. There are several other security techniques that may<a id="_idIndexMarker1607"/> involve different IT departments, such as network administrators. Let's look at some of them here:</p>
			<ul>
				<li><strong class="bold">Network firewalls</strong>: They <a id="_idIndexMarker1608"/>are an integral piece of the enterprise security strategy and are very often completely transparent to developers and architects (at least until you find that some of the connections you want to make are failing due to a missing network rule). The primary duty of firewalls is to block all the network connections unless they are explicitly allowed. This includes rules on ports, protocols, IP addresses, and so on.</li>
			</ul>
			<p>Nowadays, however, firewalls are way more sophisticated than they used to be. They are now capable of inspecting the application-level protocols and are often not only deployed at the forefront of the infrastructure but also between each component, to monitor and limit unauthorized accesses.</p>
			<p>For the same reason, some orchestrator <a id="_idIndexMarker1609"/>tools (such as <strong class="bold">Kubernetes</strong>, but also the public cloud providers) offer the possibility to implement the<a id="_idIndexMarker1610"/> so-called <em class="italic">network policies</em>, which are<a id="_idIndexMarker1611"/> essentially <strong class="bold">Access Control Lists</strong> (<strong class="bold">ACLs</strong>) acting as a network firewall, hence not allowing (or dropping) unwanted network connections. Firewalls can be hardware<a id="_idIndexMarker1612"/> appliances (with major vendors <a id="_idIndexMarker1613"/>including <strong class="bold">Cisco</strong> and <strong class="bold">Check Point</strong>, among others), or <a id="_idIndexMarker1614"/>even software <a id="_idIndexMarker1615"/>distributions (such as <strong class="bold">PFSense</strong> and <strong class="bold">Zeroshell</strong>).</p>
			<ul>
				<li><strong class="bold">Intrusion Protection Systems</strong> (<strong class="bold">IPSes</strong>) (similar to <strong class="bold">Intrusion Detection Systems</strong>, with a<a id="_idIndexMarker1616"/> slight difference in the implementation): These<a id="_idIndexMarker1617"/> are an extension to firewalls. An IPS, like a firewall, is capable of inspecting network connections. But instead of just identifying authorized and unauthorized routes, an IPS is also capable of inspecting the packages to identify signatures (recurrent patterns) of well-known attacks (such as SQL injections or similar behaviors).</li>
			</ul>
			<p>Moreover, an IPS can inspect other aspects of an application beyond just its network connections. Typically, an IPS can access application logs or even inspect the application behavior at runtime, with the same goal of identifying and blocking malevolent behavior. In this context, IPSes are similar to antivirus software <a id="_idIndexMarker1618"/>running on <a id="_idIndexMarker1619"/>workstations. Two common IPS implementations are <strong class="bold">Snort</strong> and <strong class="bold">Suricata</strong>.</p>
			<ul>
				<li><strong class="bold">Source code inspection</strong>: This <a id="_idIndexMarker1620"/>is focused <a id="_idIndexMarker1621"/>on analyzing the code for well-known bugs. While this is a general-purpose technique, it can be focused on security issues. In most cases, this kind of analysis is integrated into the software delivery cycle as a standard step for each release. This kind of test is also<a id="_idIndexMarker1622"/> named <strong class="bold">static software analysis</strong> because it refers to inspecting the software when it is not being executed (hence, looking at the source code).</li>
			</ul>
			<p>A <a id="_idIndexMarker1623"/>technique similar to the previous point is checking the versions of dependencies in an application. This may refer to libraries, such as Maven dependencies. Such modules are checked against databases for known vulnerabilities linked to the specific version. This is part of following the general recommendation <a id="_idIndexMarker1624"/>of keeping the software constantly patched and upgraded.</p>
			<p>All of the aspects seen so far are relevant best practices that can be partially or completely adopted in your project. However, there are contexts where security checks and considerations must be applied in a standardized and well-defined way, which we will see next.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor300"/>Security standards and regulations</h2>
			<p><strong class="bold">Security</strong> is <a id="_idIndexMarker1625"/>a core concept in applications, especially in some specific industries, such as financial services, defense, healthcare, and the public sector. But it's really a cross-concept that cannot be ignored in any context. For this reason, there are sets of regulations, sometimes mandated by law or industry standards (for example, banking associations), that mandate and standardize some security practices. These include the following:</p>
			<ul>
				<li><strong class="bold">Payment Card Industry Data Security Standard </strong>(<strong class="bold">PCI DSS</strong>): This is a very <a id="_idIndexMarker1626"/>widespread<a id="_idIndexMarker1627"/> standard for implementing and maintaining IT systems that provide credit card payments. The goal is to reduce fraud and establish the maximum level of trust and safety for credit card users. PCI DSS mandates a set of rules not only on the system itself (such as access control and network security) but also in the way IT staff should handle such systems (by defining roles and responsibilities).</li>
				<li><strong class="bold">Common Criteria</strong> (<strong class="bold">CC</strong>): This <a id="_idIndexMarker1628"/>is an international <a id="_idIndexMarker1629"/>standard (under the denomination ISO/IEC 15408) that certifies a set of tests for checking the security of an IT system. Such certification is conducted by authorized entities, and the <a id="_idIndexMarker1630"/>certified systems are registered on an official list.</li>
				<li><strong class="bold">Open Web Application Security Project</strong> (<strong class="bold">OWASP</strong>): This approach is a bit different <a id="_idIndexMarker1631"/>from<a id="_idIndexMarker1632"/> what we have seen so far. Instead of being a centralized testing institution providing a certification, OWASP is an open and distributed initiative that provides a set of tools, tests, and best practices for application security (especially focused on web application security). OWASP also shares and maintains a list of <a id="_idIndexMarker1633"/>well-known security issues. The association distributes the <strong class="bold">Dependency-Check</strong> tool (<a href="https://owasp.org/www-project-dependency-check">https://owasp.org/www-project-dependency-check</a>), which helps in identifying vulnerable software dependencies, and the Dependency-Track tool monitors and checks dependency usage.</li>
			</ul>
			<p>As we explained, security is a crucial topic that must be considered important in all project phases (from design to implementation to testing) and across all different teams (from developers to testers to sysadmins). This is the reason why we decided to consider it a cross-cutting concern (and why we discussed it in this chapter). To establish and maintain security in our applications, best practices must be taken into account at every step of a development project, including coding. But to maintain a safe system, we should also consider other potential sources of disruption and data loss, and ways to avoid or mitigate them, which we will look at in the next section.</p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor301"/>Resiliency</h1>
			<p>Security is<a id="_idIndexMarker1634"/> about preventing fraudulent activities, the theft of data, and other improper behavior that could lead to service disruptions. However, our application can go down or provide degraded service for several other reasons. This could be due to a traffic spike causing an overload, a software bug, or a hardware failure.</p>
			<p>The core concept (sometimes underestimated) behind<a id="_idIndexMarker1635"/> the resiliency of a system is the <strong class="bold">Service Level Agreement</strong> (<strong class="bold">SLA</strong>).</p>
			<p>An SLA is an attempt to quantify (and usually enforce with a contract) some core metrics that our service should respect.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor302"/>Uptime</h2>
			<p>The most<a id="_idIndexMarker1636"/> widely used SLA is <strong class="bold">uptime</strong>, measuring the availability of the system. It is a <a id="_idIndexMarker1637"/>basic metric, and it's commonly very meaningful for services providing essential components, such as connectivity or access to storage. However, if we consider more complex systems (such as an entire application, or a set of different applications, as in microservices architectures), it becomes more complex to define. Indeed, our application may still be available, but responding with the wrong content, or simply showing static pages (such as a so-called <em class="italic">courtesy page</em>, explaining that the system is currently unavailable). </p>
			<p>So, the uptime should be defined carefully in complex systems, by restricting it to specific features and defining their expected behaviors (such as the data that these features should provide).</p>
			<p>Uptime is usually measured as a percentage over a defined period, such as 99.9% per year.</p>
			<p>When considering the uptime, it's useful to define the two possible types of outages:</p>
			<ul>
				<li><strong class="bold">Planned downtime</strong>: This <a id="_idIndexMarker1638"/>refers to service disruption <a id="_idIndexMarker1639"/>occurring due to maintenance or other predictable operations, such as deployments. To reduce planned downtime, one technique is to reduce the number of releases. However, this kind of technique may be impractical for modern systems because it will reduce agility and increase time to market. So, an alternative approach is to implement rolling releases or similar techniques to continue to provide services (eventually in a degraded mode) while performing releases or other maintenance activities.</li>
				<li><strong class="bold">Unplanned downtime</strong>: This is, of <a id="_idIndexMarker1640"/>course, linked to unpredictable events, such as system <a id="_idIndexMarker1641"/>crashes or hardware failures. As we will see in this section, there are several techniques available for increasing uptime, especially in cloud-native architectures.</li>
			</ul>
			<p>With regard to unplanned downtime, there are several further metrics (I would say <em class="italic">sub-metrics</em>) that measure certain specific aspects that are useful for further monitoring of the service levels of a system:</p>
			<ul>
				<li><strong class="bold">Mean time between failures</strong>: This<a id="_idIndexMarker1642"/> measures the average time between two services outages (as said before, an outage can be defined in many ways, ranging from being completely down to services answering incorrectly). A system with a short mean time between failures, even if still respecting the overall uptime SLA, should be considered unstable and probably fixed or strengthened.</li>
				<li><strong class="bold">Mean time to recovery</strong>: This measures the average time to restore a system to operation following a period of downtime. This includes any kind of workaround or manual fix to resolve an issue. These kinds of fixes are considered temporary. A system with a high mean time to recovery might need some supporting tools or better training for the team operating it.</li>
				<li><strong class="bold">Mean time to repair</strong>: This is similar to the previous metric but measures the complete resolution of an issue definitively. The difference between this metric and the previous one is subtle and subjective. A high mean time to repair can signify a poorly designed system or the lack of good troubleshooting tools.</li>
			</ul>
			<p>Uptime is not the only SLA to consider when monitoring a system with regard to resiliency. Several other metrics can be measured, such as the response time of an API (which can be measured with something such as <em class="italic">90% of the calls should respond in under 1 millisecond</em>), the error rate (the percentage of successful calls per day), or other related metrics. </p>
			<p>But as we said, there are several techniques to achieve these SLAs and increase system reliability.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor303"/>Increasing system resiliency</h2>
			<p>The<a id="_idIndexMarker1643"/> most commonly used (sometimes overused) technique for increasing system <a id="_idIndexMarker1644"/>resiliency is <strong class="bold">clustering</strong>. A <strong class="bold">cluster</strong> is a set of <a id="_idIndexMarker1645"/>components working concurrently in a mirrored way. In a cluster, there is a way to constantly share the system status between two or more instances. In this way, we can keep the services running in case downtime (planned or unplanned) occurs in one of the systems belonging to the cluster itself. </p>
			<p>Moreover, a <a id="_idIndexMarker1646"/>cluster may involve a redundant implementation of every subsystem (including network, storage, and so on) to further improve resiliency in the event of the failure of supporting infrastructure.</p>
			<p>Clusters are usually complex to set up and there is a price to pay for the increase in reliability, usually a performance impact due to the replication of the state. Moreover, depending on the specific application, there are several restrictions for implementing a cluster, such as network latency and the number of servers (nodes).</p>
			<p>We <a id="_idIndexMarker1647"/> discussed a related topic in <a href="B16354_11_Final_JM_ePUB.xhtml#_idTextAnchor271"><em class="italic">Chapter 11</em></a><em class="italic">,</em> <em class="italic">Dealing with Data</em>, when talking about <strong class="bold">NoSQL</strong> repositories <a id="_idIndexMarker1648"/>and the <strong class="bold">CAP</strong> theorem. Since the data inside a cluster can be considered distributed storage, it must obey the CAP theorem.</p>
			<p>A cluster often relies on a networking device, such as a network load balancer, that points to every node of the cluster and re-establishes full system operativity in case of a failure, by redirecting all the requests to a node that is still alive.</p>
			<p>To communicate with each other, the cluster nodes use specific protocols, commonly known<a id="_idIndexMarker1649"/> as a <strong class="bold">heartbeat</strong> protocol, which usually involves the exchange of special messages over the network or filesystem. A widely used library for implementing heartbeat and leader election<a id="_idIndexMarker1650"/> in Java is <strong class="bold">JGroups</strong>.</p>
			<p>One common issue with clusters is<a id="_idIndexMarker1651"/> the <strong class="bold">split-brain</strong> problem. In a split-brain situation, the cluster is divided into two or more subsets, which are unable to reach each other via the heartbeat. This usually occurs because of a network interruption between the two system subsets, caused by a physical disconnection, a misconfiguration, or something else. In this situation, one part of the cluster is unaware if the other part is still up and running (but cannot be seen using the heartbeat) or is down. To maintain data consistency (as seen in the CAP theorem in <a href="B16354_11_Final_JM_ePUB.xhtml#_idTextAnchor271"><em class="italic">Chapter 11</em></a>, <em class="italic">Dealing with Data</em>) in the case of split-brain, the cluster may decide to stop operating (or at least stop writing functionalities) to avoid processing conflicting operations in two parts of the cluster that are not communicating with each other.</p>
			<p>To address these scenarios, clusters may invoke the concept of a quorum. A <strong class="bold">quorum</strong> is the number of <a id="_idIndexMarker1652"/>nodes in a cluster required for the cluster to operate properly. A quorum is commonly fixed to <em class="italic">half of the cluster nodes + 1</em>.</p>
			<p>While the details may vary with the type of specific cluster implementation, a quorum is usually necessary to elect a cluster leader. The leader may be the only member of the cluster running, or, more commonly, having other duties related to cluster coordination (such as declaring a cluster fully functional or not). To properly handle split-brain situations, a cluster is usually composed of an odd number of nodes, so if there's a split between two subsets, one of the two will be in the majority and continue to operate, while the other will be the minority and will shut down (or at least deny write operations).</p>
			<p>An alternative<a id="_idIndexMarker1653"/> to the use of a quorum is the technique<a id="_idIndexMarker1654"/> of <strong class="bold">witnesses</strong>. A cluster may be implemented with an even number of nodes, and then have a special node (usually dislocated in the cloud or a remote location) that acts as a witness.</p>
			<p>If there's a cluster split, the witness node can reach every subset of the cluster and decide which one should continue to operate.</p>
			<p>As we have said, clustering<a id="_idIndexMarker1655"/> can be expensive and has lots of requirements. Moreover, in modern architectures (such as <strong class="bold">microservices</strong>), there are alternative approaches for operating in the case of a failure in distributed setups. One common consideration is about the eventual consistency, discussed in the previous chapter, under the <em class="italic">Exploring NoSQL repositories</em> section. </p>
			<p>For all these reasons, there are other approaches to improving system availability, which can be used as an alternative or a complement to clustering.</p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor304"/>Further techniques for improving reliability</h2>
			<p>An<a id="_idIndexMarker1656"/> alternative approach to clustering used to improve reliability is <strong class="bold">High Availability</strong> (<strong class="bold">HA</strong>). An HA system is similar to a clustered system. The<a id="_idIndexMarker1657"/> main difference is that in normal conditions, not<a id="_idIndexMarker1658"/> all nodes are serving requests. Conversely, in this kind of setup, there is usually one (or a limited number of) primary nodes running and serving requests, and one or more failover nodes, which are ready to take over in the case of a failure of the primary node.</p>
			<p>The time for restoring the system may vary depending on the implementation of the systems and the amount of data to restore. The system taking over can already be up and running (and more or less aligned with the primary). In this scenario, it's called a <strong class="bold">Hot Standby</strong>. An <a id="_idIndexMarker1659"/>alternative scenario is when the failover servers are usually shut down and lack data. In this case, the system is called <strong class="bold">Cold Standby</strong> and may take <a id="_idIndexMarker1660"/>some time to become fully operational.</p>
			<p>An extreme <a id="_idIndexMarker1661"/>case of Cold Standby<a id="_idIndexMarker1662"/> is <strong class="bold">Disaster Recovery</strong> (<strong class="bold">DR</strong>). This kind of system, often mandated by law, is dislocated in a remote geographical location, and aligned periodically. How remote it should be and how often it is aligned are parameters that will vary depending on how critical the system is and how much budget is available. A DR system, as the name implies, is useful when recovering from the complete disruption of a data center (due to things such as a fire, an earthquake, or flooding). Those events, even if unlikely, are crucial to consider because being unprepared means losing a lot of money or being unable to re-establish a system.</p>
			<p>DR is<a id="_idIndexMarker1663"/> also linked to the concept of <strong class="bold">Backup and Restore</strong>. Constantly <a id="_idIndexMarker1664"/>backing up data (and configurations) is crucial to re-establishing system operation in the case of a disaster or unforeseen data loss (think about a human error or a bug). Backed-up data should also be periodically tested for restore to check the completeness of data, especially if (as is advised) the data is encrypted.</p>
			<p>Whether you are planning to use clustering, HA, or DR, two special metrics are commonly used to measure the effectiveness and the goals of this kind of configuration:</p>
			<ul>
				<li><strong class="bold">Recovery time objective</strong> (<strong class="bold">RTO</strong>): This is the time needed for a failover node to take over<a id="_idIndexMarker1665"/> after the primary node fails. This time can be 0 (or very limited) in the case of clustering, as every node is already up and running, or can be very high in the case of DR (which may be acceptable as the occurrence of a disaster is usually very unlikely).</li>
				<li><strong class="bold">Recovery point objective</strong> (<strong class="bold">RPO</strong>): This is the amount of data that it is acceptable<a id="_idIndexMarker1666"/> to lose. This may be measured in terms of time (such as the number of minutes, hours, or days since the last sync), the number of records, or something similar. An RPO can be 0 (or very limited) in a clustered system, while it can be reasonably high (such as 24 hours) in the case of DR.</li>
			</ul>
			<p>A last important topic is<a id="_idIndexMarker1667"/> the <strong class="bold">physical location</strong> of the application. Indeed, all of the approaches that we have seen so far (clustering, HA, and DR, with all the related metrics and measurements, such as RPO and RTO) can be implemented in various physical setups, greatly varying the final effect (and the implementation costs).</p>
			<p>One<a id="_idIndexMarker1668"/> core concept is<a id="_idIndexMarker1669"/> the <strong class="bold">data center</strong>. Indeed, each node (or portion) of a cluster (or of an HA or DR setup) can be running on a physically different data center in a specific geographical location.</p>
			<p>Running<a id="_idIndexMarker1670"/> servers in different data centers usually provides the maximum level of resiliency, especially if the data centers are far away from each other. On the other hand, the connection between applications running in different data centers can be expensive and subject to high latency. Cloud providers often call the different data <a id="_idIndexMarker1671"/>centers <strong class="bold">availability zones</strong>, further grouping them by geographical area, to provide information about the distance between them and the users.</p>
			<p>However, even if an application is running in just one data center, there are techniques to improve resilience to failures and disasters. A good option is running the application copies in different rooms in a data center. The rooms of a data center, even if belonging to the same building, can be greatly independent of each other. These data centers may apply specific techniques to enforce such independence (such as dedicated power lines, different networking equipment, and specific air conditioning systems). However, it's easy to understand that major disasters such as earthquakes, floods, and fires will be disruptive for all the rooms in the same way. However, hosting in separate rooms is usually cheaper than in separate data centers, and rooms have quite good connectivity with each other.</p>
			<p>A lower degree of isolation can be obtained by running different copies of our application (different nodes of a cluster) on <a id="_idIndexMarker1672"/>different racks. A <strong class="bold">rack</strong> is a grouping of servers, often all running in the same room (or close to each other, at least). In this sense, two applications running on different racks may be unaffected by minor issues impacting just one rack, such as a local network hardware failure or power adapter disruption, as these physical devices are commonly specific to each rack.</p>
			<p>Of course, a blackout or a defect in the air conditioning system will almost certainly impact all the instances of our cluster, even if running on different racks. For all of these reasons, different racks are cheaper than the other implementations seen so far, but can be pretty poor in offering resilience to major disasters, and are completely unsuitable for implementing proper DR.</p>
			<p>A closer alternative to<a id="_idIndexMarker1673"/> different racks is running our application in the same rack but on different machines. Here, the only redundancy available is against local hardware failures, such as a disk, memory, or CPU malfunctioning. Every other physical issue, including minor ones (such as a power adapter failing), will almost certainly impact the cluster availability.</p>
			<p>Last but not least, it is possible to have the instances of a cluster running on the same physical machine thanks to containers or server virtualization.</p>
			<p>Needless <a id="_idIndexMarker1674"/>to say, this technique, while very easy and cheap to implement, will not provide any protection against hardware failures. The only available reliability improvement is against software crashes, as the different nodes will be isolated to some degree.</p>
			<p>All of the approaches that we have seen so far offer ways to improve the overall application availability and were available long before cloud-native applications. However, some <a id="_idIndexMarker1675"/>modern evolutions of such techniques (such as the <strong class="bold">saga pattern</strong>, seen in <a href="B16354_09_Final_JM_ePUB.xhtml#_idTextAnchor230"><em class="italic">Chapter 9</em></a>, <em class="italic">Designing Cloud-Native Architectures</em>) happen to better suit modern applications (such as microservices-based ones). </p>
			<p>A topic that is worth<a id="_idIndexMarker1676"/> highlighting is <strong class="bold">reliability</strong>. In the past, reliability was treated exclusively at the infrastructure level, with highly available hardware and redundant network connections. However, nowadays, it is more common to design application architectures that are aware of where they run, or of how many instances are running concurrently. In other words, applications that take reliability and high availability into consideration have become common. In this way, it is possible to implement mixed approaches that provide degraded functionalities if failure is detected in other nodes of the cluster. So, our application will still be partially available (for example in read-only mode), thereby reducing the total outage.</p>
			<p>Another technique is<a id="_idIndexMarker1677"/> to apply <strong class="bold">tiering</strong> to functionalities (for example, to different microservices). To do so, it's possible to label each specific functionality according to the severity and the SLA needed. Hence, some functionalities can be deployed on highly resilient, expensive, and geographically distributed systems, while other functionalities can be considered disposable (or less important) and then deployed on cheaper infrastructure, taking into account that they will be impacted by outages in some situations (but this is accepted, as the functionalities provided are not considered core).</p>
			<p>All of these last considerations are to say that even if you will never have the job of completely designing the availability options of a full data center (or of more than one data center) in your role as a software architect, you will still benefit from knowing the basics of application availability so that you can design applications properly (especially the cloud-native, microservices-based ones), thereby greatly improving the overall availability of the system.</p>
			<p>With this section, we have completed our overview of cross-cutting concerns in software architectures.</p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor305"/>Summary</h1>
			<p>In this chapter, we have seen an overview of the different cross-cutting concerns that affect software architecture. This also included some solutions and supporting systems and tools. </p>
			<p>We have learned the different ways of managing identity inside our application (especially when it involves several different components, such as in a microservice architecture).</p>
			<p>We had an overview of the security considerations to be made when designing and implementing an application (such as intrinsic software security and overall software security), which are crucial in a shift-left approach, which is the way security is managed in DevOps scenarios.</p>
			<p>Last but not least, we had a complete overview of application resiliency, discussing what a cluster is, what the implications of using clustering are, and what other alternatives (such as HA and DR) can be implemented. </p>
			<p>In the next chapter, we are going to explore the tooling supporting the software life cycle, with a particular focus on continuous integration and continuous delivery.</p>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor306"/>Further reading</h1>
			<ul>
				<li>Neil Daswani, Christoph Kern, and Anita Kesavan: <em class="italic">Foundations of Security: What Every Programmer Needs to Know</em></li>
				<li>The Keycloak community: <em class="italic">The Keycloak OpenSource IDM </em>(<a href="https://www.keycloak.org">https://www.keycloak.org</a>)</li>
				<li>Evan Marcus and Hal Stern: <em class="italic">Blueprints for High Availability: Timely, Practical, Reliable</em></li>
			</ul>
		</div>
	</body></html>