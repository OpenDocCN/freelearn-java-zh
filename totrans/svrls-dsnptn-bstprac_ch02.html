<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>A Three-Tier Web Application Using REST</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A Three-Tier Web Application Using REST</h1>
                </header>
            
            <article>
                
<p class="mce-root">It should <span>be safe to say that the vast majority of developers know what REST is. A three-tier web application consists of the following:</span><br/></p>
<ul>
<li>Presentation layer (HTML and CSS)</li>
<li>Business logic <span>layer</span> (application code)</li>
<li>Data <span>layer</span> (<span>Relational Database Management System</span> or another type of data store)</li>
</ul>
<p><span>The three-tier web application</span> is extremely well known and one of the most common designs on the web today. <span>Readers are likely familiar with this design when thinking about a web application's static content (that is, HTML, JavaScript, and CSS) which are served from a <strong>content delivery network</strong>&#160;(<strong>CDN</strong>), which talks to a RESTful API hosted on a web server, which, in turn, talks to a database.</span></p>
<p><span>In this chapter, we will go through the process of building a three-tier web application using HTML, JavaScript, and CSS for our presentation layer, a REST API for our business logic, and a Postgres database for our data tier. Most importantly, and keeping in line with this book, this will all be accomplished using serverless technologies or services where you do not need to manage servers yourself.</span></p>
<p>By the end of this chapter, you can expect to know the following:</p>
<ul>
<li>How to author a REST API using <strong>Amazon Web Services</strong><span>&#160;(</span><strong>AWS</strong><span>)&#160;</span>Lambda and Python</li>
<li>How to build, manage, and deploy static assets to a CDN</li>
<li>How to create and maintain an RDS Postgres database</li>
<li>Options for designing RESTful APIs, including different languages, frameworks, and layouts of functions</li>
<li>Static asset life cycle management and caching for optimal performance</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Serverless tooling</h1>
                </header>
            
            <article>
                
<p>Since this is the very first chapter that has application code and working examples, it's important to talk through some of the tooling and systems to set the stage for this and subsequent chapters. In this and the following chapters on web applications, our toolchain will consist of services from&#160;<span>AWS</span>:</p>
<ul>
<li>AWS API Gateway as the HTTP proxy</li>
<li>AWS Lambda for computing</li>
<li>AWS S3 for static file serving</li>
<li>AWS CloudFront for the&#160;CDN</li>
<li>AWS RDS for RDBMS management</li>
<li><strong>AWS Certificate Manager</strong> (<strong>ACM</strong>) for free certificate management</li>
</ul>
<p>While AWS is the dominant player in the Platform as a service<span>&#160;(</span>PaaS<span>)</span>&#160;ecosystem, it is by no means the only choice. While reading this chapter and others in this book, remember that the patterns presented should apply to any cloud provider, albeit sometimes with a certain degree of adaptation.</p>
<p>You may be questioning the reasoning behind discussing other services such as S3 and RDS. Very often, perhaps usually, when people say <em>serverless</em> they are talking about functions as a service with AWS Lambda, or equivalent services from different cloud providers. This question is a valid one, but it's also critical to remember that our definition for serverless architectures in this book is complete systems where you don't need to manage any operating systems yourself. <span>Your goal is not to maintain a single real or virtual server and push the hard work to your favorite cloud provider, allowing you to focus on your application. Admittedly, not all cloud providers have the vast number of services at our disposal as in the AWS ecosystem. Take this into consideration when choosing a PaaS upon which to build your serverless applications.</span></p>
<p>Building a system with a <span>Function as a service&#160;</span>(<span>FaaS</span>) backbone for the business logic is a step in the right direction; however, if you are still managing a database, are you serverless? Managing your own RDBMS or web server for the serving of static assets puts you outside of the serverless architecture box, even if your compute layer is serverless.</p>
<p><span>In <a href="svrls-dsnptn-bstprac_ch01.html">Chapter 1</a>, <em>Introduction</em>, I listed a few of the popular frameworks that allow us to manage, deploy, and maintain serverless applications. All of our sample applications in this book will use the Serverless Framework (<a href="https://serverless.com/">https://serverless.com/</a>), which will help us glue various services and systems together, allow us to deploy code, and provide much more functionality to make the development process faster and more straightforward.&#160;</span>Just as with cloud providers, you have multiple choices for frameworks and utilities. Covering all of the options is outside the scope of this book and, for simplicity, we will stick with the Serverless Framework, which is mature, well used, and frequently updated.</p>
<div class="packt_infobox">From this point on, I will refer to the Serverless Framework when talking about the framework itself to differentiate it from the general serverless topic. Due to the name <em>Serverless</em>, the Serverless Framework can be a bit confusing in the context of a book about serverless design patterns. From here on, simply be on the lookout for the Serverless Framework if we're discussing details of how to deploy, manage, or otherwise control resources on our cloud provider, AWS.</div>
<p>Our example project will be a REST API for professional coffee evaluation, called cupping in the coffee realm. Coffee cupping at its core is nothing more than a fancy spreadsheet of scores for individual coffees, where scores are applied to one or more criteria, such as acidity, body, uniformity, and so on. If you enjoy coffee and APIs as I do, you should enjoy this and subsequent chapters.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">System architecture</h1>
                </header>
            
            <article>
                
<p><span>At first glance, a three-tier web application using a REST API can be an easy topic and pattern. After all, there are only three layers, which are responsible for very discrete tasks, and the final result is just a web application after all. However, there are many nuances and areas for tweaking with any web application. Serverless web applications are no different. This chapter will attempt to cover as many areas as possible, but it's impossible to include every possible configuration or design option.</span></p>
<p>Seeing as we are responsible software designers, let's sketch out our architecture at a high level and drill down into more detail as we work through the different layers:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/d8fc4399-0f77-4b6a-b771-4090fdf6daa5.png" style="width:30.75em;height:21.75em;"/></div>
<p>This diagram should look familiar as it's the backbone of many client-server web applications out there today. Let's walk through the different layers, going from the top down. After discussing these layers at a high level, we'll get into the implementation details with a real-world example.</p>
<p>You can find all of the code in this chapter in the following repository:</p>
<p><a href="https://github.com/brianz/serverless-design-patterns/tree/master/ch2">https://github.com/brianz/serverless-design-patterns/tree/master/ch2</a></p>
<div class="packt_infobox">Even though a common and arguably simple pattern, there are many possible complexities when deploying a stack like this on AWS with a serverless architecture. While AWS is the PaaS of choice for this and subsequent chapters, there are many topics that cannot be covered in great depth due to the size of AWS as a topic by itself. If you get stuck or are confused by any missing content, feel free to open a GitHub issue at the preceding repository to begin a dialog and I'll do my best to help.</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Presentation layer</h1>
                </header>
            
            <article>
                
<p>Static files may be served from numerous places. In our case, we will use two different AWS services which will provide respectable performance shipping assets down to the client, as well as fault tolerance and caching, among other things. Our HTML, CSS, and JavaScript files will all live in an S3 bucket. We will then create a CDN using CloudFront, AWS's CDN offering. CloudFront will not only give us better performance than S3 by itself; we will gain the ability to globally distribute and cache our content, in addition to serving files from our very own custom domain using a free TLS certificate from AWS Certificate Manager.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logic layer</h1>
                </header>
            
            <article>
                
<p>The logical layer is the guts of our application, our code. In this and other examples, we'll use Python as our programming language and deploy our functions as isolated compute units in AWS Lambda. The Serverless Framework will make this quite painless, and this will be the foundation for moving fast and iterating on our code.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data layer</h1>
                </header>
            
            <article>
                
<p>While not the core focus of this book, running databases is an integral part of modern-day web applications. In this pattern, we'll use a hosted PostgreSQL, which the AWS <strong>Relational Data Store</strong> (<strong>RDS</strong>) service will manage for us.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logic layer</h1>
                </header>
            
            <article>
                
<p>Application code is likely the area of most interest and the layer that has the most changes from a traditional web application hosted on a managed server, so let's start with that.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application code and function layout</h1>
                </header>
            
            <article>
                
<p>Let's differentiate two classifications of our organization for the logical layer:</p>
<ul>
<li><span>Organization</span>&#160;of the Lambda functions themselves, within AWS</li>
<li>Organization of the application code</li>
</ul>
<p>Lambda functions are the unit of work in Lambda and other FaaS providers (for simplicity and clarity, I will refer to these as Lambda functions from here on out). A single Lambda function may be updated or deployed in isolation without affecting other Lambda functions.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Organization of the Lambda functions</h1>
                </header>
            
            <article>
                
<p>With a REST API, there are a few options you have as to how each API endpoint maps to a function. The primary options in this design are whether to have a single Lambda function handle a single HTTP verb/resource combination, or whether to have a single lambda function handle all HTTP verbs for a particular resource. It should become more evident as we work through this chapter that <span>Lambda function organization and application code organization are related, but not the same:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="images/b20d300f-52c8-4424-984b-d03a3cda4ab6.jpg" style="width:47.75em;height:24.67em;"/></div>
<p><span>In the preceding diagram, we see a possible Lambda function layout for a REST API. On the left, unique functions handle a unique CRUD event and resource combination. On the right, Lambda functions perform actions on a single resource but with different actions (create, read, update, or delete).</span>With the first model (left side of the diagram), each REST endpoint is mapped to a single Lambda function. This design provides fine-grained control for updating functions, allowing for the deployment of a single API endpoint without the danger of inadvertently affecting other APIs.</p>
<p>The major downside is that this may quickly become unwieldy as the API grows. Imagine the case of an API with 20 resources (<kbd>session</kbd>, <kbd>user</kbd>, and so on), each with three to four actions <kbd>/</kbd> HTTP verbs. If you follow this scenario through with some basic multiplication, the quick growth of the Lambda functions that you'll need to manage and navigate will become obvious.</p>
<p>With the next design, logical groups of REST endpoints are grouped and triggered, in effect the main function that routes the request to the appropriate handler. If you imagine the simple case of listing <kbd>sessions</kbd> from this API, an HTTP <kbd>GET</kbd> would come into the <kbd>/session</kbd> endpoint, which would trigger the <kbd>handle_sessions()</kbd> function. As a part of this payload, our application code would know that a <kbd>GET</kbd> method was invoked and would then invoke a <kbd>get_sessions()</kbd> function, perhaps the same as in the previous design. The significant benefit of this architecture is that the number of Lambda functions is drastically reduced over the previous design. The downside is that deploying updates affects all REST endpoints, which are handled by a single function. However, this may also be a benefit. If there were a bug in some shared code that affected all <kbd>/session/{id}</kbd> endpoints (<kbd>GET</kbd>, <kbd>PUT</kbd>, and <kbd>DELETE</kbd>), we'd only need to update a single function to fix them all. With the previous design, we would need to update three functions individually.</p>
<p>For this chapter, we will use the grouped design so that we have a single Lambda function for groups of REST endpoints. Each group will share a common URL pattern, and the HTTP verb will be used to trigger different functions within the application code.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Organization of the application code</h1>
                </header>
            
            <article>
                
<p>Organization of our application code is entirely different than our prior discussion, although there is a bit of overlap.</p>
<p>Laying out application code in a serverless project is slightly different than in a traditional web application. While the differences aren't that drastic, I find serverless projects a bit more susceptible and intolerant of designs or layouts that are not thought through in detail. Because it's so easy to get started, it's also easy to start moving in the wrong direction before thinking through and answering essential design decisions.</p>
<p>Over the years, these are a few of the big lessons I've learned when writing serverless application code:</p>
<ul>
<li>Configuration should be done with environment variables, rather than different configuration files</li>
<li>Application code should be well structured, highly modular, and namespaced</li>
<li>Think through how many functions you need before coding begins</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Configuration with environment variables</h1>
                </header>
            
            <article>
                
<p>If you're familiar with the Twelve-Factor App or have worked with Docker much, you'll know that configuration may be done using environment variables rather than managing multiple disparate configuration files. According to The Twelve-Factor App (<a href="https://12factor.net/config">https://12factor.net/config</a>):<a href="https://12factor.net/config)"></a></p>
<div class="packt_quote">"Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard."</div>
<p>Using environment variables for FaaS enables code deployments to different systems (dev, QA, production, and so on). Changing configuration can be as simple as updating a variable in your function's config. However, for safety and repeatability, environment variable changes should go through some process such as CI/CD to minimize the chance of errors.</p>
<p>On the flip side, if using file-based configuration, updating the application <span>typically</span> requires updating a file, possibly checking into source control and redeploying the entire application.</p>
<p>In my opinion, there is an enormous increase in productivity using environment variables when creating new systems or deploying between different systems. To perform a new stack deployment or update of an existing stack, you merely load up new environment variables and executes a standard set of steps that don't change between stacks. Due to the ease and speed with which you can do this, it encourages separation of stacks for different purposes (development, testing, production, and much more).</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Code structure</h1>
                </header>
            
            <article>
                
<p>With any new application, there are many ways to organize code on disk. The following structure has worked very well for my colleagues and me across multiple projects. I encourage readers to use this as a starting point and adapt as needed. If using Node.js or another supported language for your FaaS provider, this may look slightly different. Throughout this chapter, we will fill in our example coffee <kbd>cupping</kbd> API and will add more files as we build the application:</p>
<pre style="padding-left: 60px">├── Makefile <br/>├── README.md <br/>├── envs/<br/>│   ├── dev <br/>│   ├── qa <br/>│   └── production <br/>├── requirements.txt <br/>├── serverless/ <br/>│   ├── cupping/<br/>│   ├── handler.py <br/>│   ├── lib/<br/>│   └── serverless.yml <br/>└── tests/<br/>    ├── conftest.py <br/>    ├── factories.py <br/>    ├── helpers.py <br/>    ├── test_session_model.py <br/>    └── test_session_persistence.py </pre>
<p>A <kbd>Makefile</kbd> may be something you skip. I use Docker as a host for application development since it's a reasonably easy way to manage environment variables during deployment and testing. A simple <kbd>Makefile</kbd> can make tedious tasks much less verbose by hiding the complexity behind a make target.</p>
<div class="packt_tip">For details on this Docker methodology, I'll point you to a detailed blog post at <a href="http://blog.brianz.bz/post/structuring-serverless-applications-with-python/">http://blog.brianz.bz/post/structuring-serverless-applications-with-python/</a>. It'd be perfectly acceptable to run your serverless application development on your primary computer without any extra virtualization (Docker, VirtualBox, and so on). This Docker/Makefile pattern has worked quite well for me recently across multiple projects. I still edit files using my host system, but all runtime work (testing, deployment, building packages, and so on) is done from within a Docker container.</div>
<p>The <kbd>envs</kbd> directory holds environment variable files, which are simple <kbd>key=value</kbd> pairs. Each environment has a corresponding file of the same name. Looking at preceding the example, it should be clear where the configuration resides for each environment and what you'd need to do when creating a new environment.</p>
<p>We place all the code into the <kbd>serverless</kbd> directory, including application code we write, as well as its dependencies. Our application code is namespaced into the <kbd>cupping</kbd> directory. We will install third-party libraries into <kbd>lib</kbd>. Of course, as you write your application, your application code will be namespaced to something that is appropriate for your project. I recommend using a meaningful name rather than something generic such as&#160;<kbd>code</kbd> or <kbd>app</kbd>, just to aid new developers who come after you in navigating the source tree and for general clarity and explicitness.</p>
<p>Alongside the application code live two files—<kbd>serverless.yml</kbd>, which defines and controls the Serverless Framework, and <kbd>handler.py</kbd>, which is the main entry point for any API calls. In the preceding diagram, we discussed how logical groupings of API endpoints would be handled by a common function within a given file,&#160;<kbd>handler.py</kbd>, which will be the entry point for these API calls and delegate the hard to work to other functions. In some ways, <kbd>handler.py</kbd> has a straightforward job, which will become apparent.</p>
<p>As responsible developers, we will make sure our code is well tested. With <kbd>pytest</kbd> as our testing framework of choice in this project, all unit test files live in a single <kbd>test</kbd> folder with some additional helpers and configuration utilities. In the preceding example, there are only two test files; more will be added to the final project. Your exact testing strategy isn't as important as the simple fact of having well-written tests. Serverless projects are incredibly fast to deploy, and there may be an inclination to forego tests. Why write unit tests when you can just deploy it for real and test it manually? One cannot overstate the benefit of having robust unit tests. Regardless of your tooling or language, all serverless projects of any decent size or complexity will benefit from tests, which you may run locally or on a continuous integration platform or system. Tests give us the confidence to deploy quickly and often and also set us up for continuous delivery moving forward, allowing a CD system to deploy our code automatically only after our tests have passed.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Function layout</h1>
                </header>
            
            <article>
                
<p>In our example, we will implement a pattern where a single Lambda function will handle a single grouping of URL endpoints. The initial set of endpoints that we will implement will be the following:</p>
<ul>
<li>List coffee <kbd>cupping</kbd> sessions: <kbd>GET /session</kbd></li>
<li>Create a coffee <kbd>cupping</kbd> session: <kbd>POST /session</kbd></li>
<li>Get coffee <kbd>cupping</kbd> session details: <kbd>GET /session/{id}</kbd></li>
<li>Delete a coffee <kbd>cupping</kbd> session: <kbd>DELETE /session/{id}</kbd></li>
</ul>
<p>Two unique URLs will map to two Lambda functions. These lambda functions will be responsible for inspecting the HTTP request passed in from API Gateway, determining what HTTP method is being called, and invoking the appropriate application code to fulfill the request:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/8fb9edd0-ed99-4fa7-abdb-36d005f70c56.png" style="width:34.25em;height:19.75em;"/></div>
<p><span>Request routing for a</span> <kbd>/session</kbd> <span>endpoint. The application code will inspect the HTTP method and route to the appropriate application code for execution.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Presentation layer</h1>
                </header>
            
            <article>
                
<p>Presentation layers are not necessarily the most exciting area but, in reality, they are the entry point for your entire web application, and you should think through the details carefully. Naive deployments of HTML, CSS, and JavaScript files may result in slow load times, which has a noticeable impact on user experience.</p>
<p>When building serverless systems on top of AWS, there are a few different services that enable us to host static assets quite easily. Other PaaS systems have similar offerings, although there may not be a one-to-one comparison with all of the AWS services.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">File storage with S3</h1>
                </header>
            
            <article>
                
<p>Any frontend assets need a filesystem as a home. In this case, the natural choice is AWS <strong>Simple Storage Service</strong> (<strong>S3</strong>), which is Amazon's high durability object storage service. S3 advertises 99.999999999% durability, so it's safe to say our files will be available when we need them. While it's possible to serve content from S3 as a website on a custom domain, it's not the best choice for this scenario. AWS CloudFront will aid us in distributing files to end users quickly and efficiently.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CDN with CloudFront</h1>
                </header>
            
            <article>
                
<p>CloudFront is Amazon's CDN service. A CDN's primary focus is to improve the delivery of static assets to the end user. This task is typically accomplished by running multiple <strong>points of presence</strong> (<strong>POPs</strong>) around the globe and distributing your contents in those various geographic locations. When a user somewhere on the planet requests one or more of your files, the CDN can fetch the content that is closest to the user to minimize latency. Of course, this is only one small and dumbed-down explanation of a single CDN feature. The bottom line is that CDNs help us to speed up delivery of our content and should be used in any web application.</p>
<p>CloudFront has some very nice features that will allow us to integrate with other AWS services. We will create a CloudFront distribution that will pull our content from S3. In this way, CloudFront is a layer that aids in the acceleration of content delivery but does not own any content itself. We'll be able to configure caching controls to suit our needs and will also be able to serve our content over a custom domain with a free TLS certificate from AWS Certificate Manager. All of this is possible thanks to CloudFront.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data layer</h1>
                </header>
            
            <article>
                
<p>It's safe to say that most web applications today have some data store, whether it's a relational database (PostgreSQL, MySQL, SQLServer, and so on), a non-relational database (MongoDB, Redis, Cassandra, and so on), or even static file storage (S3, OS filesystem, and so on).</p>
<p><span>AWS RDS service will manage a</span> <span>PostgreSQL database for our</span> coffee <kbd>cupping</kbd> application. RDS offers different RDBMS choices, most notably PostgreSQL, MySQL, Oracle, and SQLServer. There are other choices, and I encourage you to take a look at the various offerings. For this exercise, we'll be using a standard PostgreSQL database hosted on RDS. Many configuration options come with RDS, which we won't cover. Just know that it's possible and quite simple to run, configure, and manage a high-availability RDBMS instance using RDS. Other PaaS providers offer similar services for relational databases.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Writing our logic layer</h1>
                </header>
            
            <article>
                
<p>Since we've covered the overall design and different layers, let's get down to the implementation of our application code.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application entrypoint</h1>
                </header>
            
            <article>
                
<p>Every application, web or otherwise, needs a primary entry point. In our case, we'll use <kbd>handler.py</kbd> to begin application execution when a Lambda function is invoked. Serverless Framework applications will generate a <kbd>handler.py</kbd> file when you bootstrap a new project, so this pattern should be familiar to anyone who has used Serverless before. If you've never worked with the Serverless Framework, what follows will be a thorough introduction:</p>
<pre style="padding-left: 30px">import sys

from pathlib import Path

# Munge our sys path so libs can be found
CWD = Path(__file__).resolve().cwd() / 'lib'
sys.path.insert(0, str(CWD))

import simplejson as json

from cupping.handlers.session import (
        handle_session,
        handle_session_detail,
)

from cupping.exceptions import Http404

CORS_HEADERS = {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Credentials': True
}

def session(event, context):
    """/session endpoint for POST or GET"""
    http_method = event['httpMethod']

    status_code = 200
    response = {}

    try:
        response = handle_session(http_method, event)
    except Exception as e:
        status_code = 500
        # TODO - log error
        response = {'errors': ['Unexpected server error']}

    response = {
        'statusCode': status_code,
        'body': json.dumps(response),
        'headers': CORS_HEADERS,
    }

    return response


def session_detail(event, context):
    http_method = event['httpMethod']

    status_code = 200
    response = {}

    try:
        response = handle_session_detail(http_method, event)
    except Http404 as e:
        status_code = 404
        response = {'errors': [str(e)]}
    except Exception as e:
        status_code = 500
        # TODO - log error
        response = {'errors': ['Unexpected server error']}

    response = {
        'statusCode': status_code,
        'body': json.dumps(response),
        'headers': CORS_HEADERS,
    }

    return response</pre>
<p>Our <kbd>handler.py</kbd> code isn't very complicated and delegates most application logic to different parts of our application code (namespaced into the <kbd>cupping</kbd> package). This pattern of having a single entry point for all Lambda functions is advantageous for a few reasons.</p>
<p><span>When Lambda functions execute, they only know what they know. That is, we as application developers are used to installing extra packages in some known location (which is the default system package location) or perhaps creating a Python</span> <kbd>virtualenv</kbd> <span>and configuring our server to look there during the import cycle. During a Lambda, we are responsible for managing this ourselves. Application code has no idea where to look for packages beyond the built-in libraries without being told where to look. The code block below shows how to manipulate Python's <kbd>path</kbd> so that it can find any extra packages we wish to use.</span></p>
<pre>    import sys<br/><br/>    from pathlib import Path<br/><br/>    # Munge our sys path so libs can be found<br/>    CWD = Path(__file__).resolve().cwd() / 'lib'<br/>    sys.path.insert(0, str(CWD))</pre>
<p>These four lines of code accomplish the task of resolving the current directory of our <kbd>handler.py</kbd> file and appending a <kbd>/lib</kbd> onto it. The result is the absolute path of the <kbd>lib</kbd> directory where we've installed all of our system packages. During the deployment step, the Serverless Framework will package all directories and files that reside at or below the same directory level as the <kbd>serverless.yml</kbd> file, resulting in our <kbd>lib</kbd> being available to our application code during runtime. Any import statement for a third-party library will work as expected, only after the addition of the full path to <kbd>lib</kbd> being manually added to the system path. In the preceding example, there is an import for the third-party <kbd>simplejson</kbd> module. Had this import been placed above the&#160;<kbd>sys.path.insert</kbd>&#160;call, it would have failed.</p>
<p>When this path manipulation occurs as soon as possible (that is, as soon as <kbd>handler.py</kbd> is invoked), other parts of our application code can import packages without the danger of the import failing. If this path manipulation is done across different files only when a particular package is needed, errors will be inevitable as you will at some point forget to include this logic. Additionally, doing this work in a single place means there is no duplication of logic.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application logic</h1>
                </header>
            
            <article>
                
<p>Since our handler function is so simple, let's take a look at the application code to see what exactly is going on. Our coffee cupping API is fairly simple and only handles a single resource type at this point, a coffee <kbd>cupping</kbd> session object. Before moving forward, it's helpful to take a look at the shape of this resource type:</p>
<pre>    {<br/>     "name": "Cupping session for Serverless Patterns",<br/>     "formName": "Modified SCAA",<br/>     "cuppings": [<br/>     {<br/>       "name": "<span>Guatemala</span> Huehuetenango",<br/>       "overallScore": "85.5",<br/>       "scores": {<br/>         "Aroma": 8.6,<br/>         "Body": 8,<br/>         "Flavor": 10<br/>       },<br/>       "notes": "This was pretty good",<br/>       "descriptors": ["woody", "berries"]<br/>     },<br/>     {<br/>       "name": "Ethiopia Yirgacheffe",<br/>       "overallScore": "90",<br/>       "scores": {<br/>         "Aroma": 8.6,<br/>         "Body": 8,<br/>         "Flavor": 10<br/>       },<br/>       "notes": "Nice",<br/>       "descriptors": ["blueberry"]<br/>     }<br/>     ]<br/>    }</pre>
<p>Much of the logic of this application is simply the transformation back and forth between JSON and database records. The actual application code isn't that important in the context of this book. If you'd like to learn more about the actual implementation, I encourage you to view the source code at <a href="https://github.com/brianz/serverless-design-patterns">https://github.com/brianz/serverless-design-patterns</a>.</p>
<p>The logic in <kbd>handler.py</kbd> will delegate the API requests to the <kbd>cupping/handlers/session.py</kbd> file, which you can see in the following code. The purpose here is to service requests for a particular URL pattern (which is <kbd>/session</kbd>, <kbd>/session/{id}</kbd>) and particular HTTP verb (that is,&#160;<kbd>GET</kbd>, <kbd>POST</kbd>, <kbd>DELETE</kbd>) and execute the appropriate application code:</p>
<pre style="padding-left: 30px">from schematics.exceptions import DataError<br/><br/>from .decorators import decode_json<br/>from .helpers import (<br/>        create_session_from_json_payload,<br/>        prettify_schematics_errors,<br/>)<br/><br/>from ..models import (<br/>        CuppingModel,<br/>        SessionModel,<br/>)<br/>from ..persistence import Session, queries<br/>from ..exceptions import Http404, InvalidInputData<br/><br/><br/>def get_sessions(data):<br/>    sessions = queries.get_sessions()<br/>    models = [SessionModel.from_row(s) for s in queries.get_sessions()]<br/>    return {'sessions': [m.to_native() for m in models]}<br/><br/><br/>@decode_json<br/>def create_session(json_payload):<br/>    if not json_payload or not hasattr(json_payload, 'get'):<br/>        return {'errors': ['Invalid input data']}<br/><br/>    print('Creating session', json_payload)<br/><br/>    try:<br/>        session = create_session_from_json_payload(json_payload)<br/>        print('Created session: %s' % (session.id, ))<br/>        response = {<br/>                'session': {<br/>                    'id': session.id,<br/>                    'name': session.name,<br/>                }<br/>        }<br/>    except InvalidInputData as e:<br/>        response = {'errors': e.errors}<br/><br/>    return response<br/><br/><br/>def _get_session_from_path_parameters(data):<br/>    try:<br/>        session_id = int(data.get('pathParameters', {}).get('id'))<br/>    except (AttributeError, TypeError, ValueError):<br/>        raise Http404('Invalid session id')<br/><br/>    session = queries.get_session_by_id(session_id)<br/>    if session is None:<br/>        raise Http404('Invalid session id')<br/><br/>    return session<br/><br/><br/>def get_session(data):<br/>    print('Reading session', data)<br/>    session = _get_session_from_path_parameters(data)<br/>    model = SessionModel.from_row(session)<br/>    return {'session': model.to_native()}<br/><br/><br/>def handle_session(http_method, payload):<br/>    method_map = {<br/>            'GET': get_sessions,<br/>            'POST': create_session,<br/>    }<br/>    method = http_method.upper()<br/>    return method_map[method](payload)<br/><br/><br/>def handle_session_detail(http_method, payload):<br/>    method_map = {<br/>            'GET': get_session,<br/>            'DELETE': delete_session,<br/>    }<br/>    method = http_method.upper()<br/>    return method_map[method](payload)</pre>
<p>The final two functions are the gateway into this part of our application, where HTTP verbs are mapped to different functions.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Wiring handler.py to Lambda via API Gateway</h1>
                </header>
            
            <article>
                
<p>Next, we need to wire up our API endpoints to Lambda and our <kbd>handler.py</kbd> entry point. This wiring looks like this in a <kbd>serverless.yml</kbd> configuration file:</p>
<pre style="padding-left: 30px">functions:<br/>  HandleSession:<br/>    handler: handler.session<br/>    events:<br/>      - http:<br/>          path: session<br/>          method: get<br/>          cors: true<br/>      - http:<br/>          path: session<br/>          method: post<br/>          cors: true<br/>  HandleSessionDetail:<br/>    handler: handler.session_detail<br/>    events:<br/>      - http:<br/>          path: session/{id}<br/>          method: get<br/>          cors: true<br/>          request:<br/>            parameters:<br/>              paths:<br/>                id: true<br/>      - http:<br/>          path: session/{id}<br/>          method: delete<br/>          cors: true<br/>          request:<br/>            parameters:<br/>              paths:<br/>                id: true</pre>
<p>We define two Lambda functions that have different configuration options, <kbd>HandleSession</kbd> and <kbd>HandleSessionDetail</kbd>.</p>
<p>Under each function's name, there are multiple statements that control configuration. Look at both sections and you'll notice the <kbd>handler:</kbd> statement, which instructs Lambda what code to call when the Lambda function is executed. For both, we'll be running one of the Python functions in <kbd>handler.py</kbd> that we covered in the preceding code snippet.</p>
<p>But what calls these Lambda functions in the first place? The <kbd>events:</kbd> section is responsible for setting up invocation points and making the connection between a particular event and our Lambda function. Across the FaaS landscape, functions are invoked in response to an event. In the AWS landscape, the number of events that can trigger a Lambda function is quite large. In this scenario, we are configuring events to be HTTP endpoints with a particular path and HTTP method. API Gateway is the proxy that will provide us with unique HTTPS URLs, which get wired up to our Lambda functions according to our configuration. As you read through the configuration, our design and intent should be apparent. Again, there are a seemingly infinite number of ways to set up an API with these technologies and this example just scratches the surface to discuss the overall pattern.</p>
<div class="packt_tip">Because the frontend JavaScript code will be making HTTP requests to the serverless backend, which is hosted on a different domain, CORS will need to be set up for each API endpoint. <span>Controlling CORS</span> is simple to do by adding <kbd>cors: true</kbd> for each endpoint in <kbd>serverless.yml</kbd>. In addition to this setting, the application code will explicitly need to return the proper headers in the responses.</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying the REST API</h1>
                </header>
            
            <article>
                
<p><span>Now the fun part, we'll deploy our REST API using the Serverless Framework. At this point, we have not discussed the various configuration options when implementing serverless architectures on AWS. I'll cover different possibilities, and our particular configuration, later on in this chapter.</span></p>
<p>My pattern of using Docker as a build and deployment tool makes this process a bit easier. You are not required to do this, and there are likely other ways to make the process even simpler.</p>
<p>We will do all package building and deployment from inside a running Docker container, which I start and enter with the following <kbd>Makefile</kbd> target:</p>
<pre><strong>brianz@gold(master=)$ ENV=dev make shell</strong></pre>
<p>This equates to the following Docker command:</p>
<pre style="padding-left: 30px"><strong>docker run --rm -it \</strong><br/><strong>        -v `pwd`:/code \</strong><br/><strong>        --env ENV=$(ENV) \</strong><br/><strong>        --env-file envs/$2 \</strong><br/><strong>        --name=coffee-cupping-$(ENV) \</strong><br/><strong>        verypossible/serverless:1.20.0-python3 bash</strong></pre>
<p>There is nothing magical here. We're starting up a Docker container from an image that contains the Serverless Framework as well as some other Python packages for a Python 3 runtime. The main trick is that, based on the <kbd>ENV</kbd> setting upon creation of the container, we pull environment variables from the desired <kbd>envs</kbd> files and load them into the running container. Those environment variables can then be referenced from within <kbd>serverless.yml</kbd> and injected into the Lambda functions, hence controlling configuration of the final application by starting from files on our local system. Full details are out of scope, but can be reviewed at <a href="http://blog.brianz.bz/post/structuring-serverless-applications-with-python/">http://blog.brianz.bz/post/structuring-serverless-applications-with-python/</a>.<a href="http://blog.brianz.bz/post/structuring-serverless-applications-with-python/"></a></p>
<div class="packt_tip">The <kbd>Makefile</kbd> and commands I'm running here are not very sophisticated; however, they may appear to be so if you are unfamiliar with Docker or make. I encourage those unfamiliar with them to read through the <kbd>Makefile</kbd> targets and do a bit of exploration on their own at <a href="https://github.com/brianz/serverless-design-patterns/blob/master/ch2/Makefile">https://github.com/brianz/serverless-design-patterns/blob/master/ch2/Makefile</a>. Feel free to open a GitHub issue if you get stuck or need more clarity.</div>
<p>Now that we're inside a container with all of our configuration set from environment variables, we can deploy the entire stack. Our first step is to ensure we have our libraries built and installed into the <kbd>lib</kbd> directory. In the Python world, the <kbd>pip</kbd> command can help us. Take a look at the <kbd>Makefile</kbd> in the repository for details. Our steps for doing the initial deployment are, therefore, as follows:</p>
<pre style="padding-left: 30px"><strong><span>root@091655eda5d0:/code# make libs<br/>......<br/># packages now installed in libs<br/>....<br/>root@091655eda5d0:/code# make deploy <br/>cd serverless &amp;&amp; sls deploy -s dev <br/>Serverless: Packaging service... <br/>Serverless: Excluding development dependencies... <br/>Serverless: Uploading CloudFormation file to S3... <br/>Serverless: Uploading artifacts... <br/>Serverless: Uploading service .zip file to S3 (5.27 MB)... <br/>Serverless: Validating template... <br/>Serverless: Updating Stack... <br/>Serverless: Checking Stack update progress... <br/>.............. <br/>Serverless: Stack update finished... <br/>Service Information <br/>service: coffee-cupping <br/>stage: dev <br/>region: us-west-2 <br/>api keys: <br/> None <br/>endpoints: <br/> GET - https://2treukfv8j.execute-api.us-west-2.amazonaws.com/dev/session <br/> POST - https://2treukfv8j.execute-api.us-west-2.amazonaws.com/dev/session <br/> GET - https://2treukfv8j.execute-api.us-west-2.amazonaws.com/dev/session/{id} <br/> DELETE - https://2treukfv8j.execute-api.us-west-2.amazonaws.com/dev/session/{id} <br/>functions: <br/> HandleSession: coffee-cupping-dev-HandleSession</span></strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying the Postgres database</h1>
                </header>
            
            <article>
                
<p>Many frameworks for working with AWS serverless architectures expose access to <kbd>CloudFormation</kbd>, AWS's tool for managing multiple related resources as a single entity. The Serverless Framework is no different and, in fact, the <kbd>CloudFormation</kbd> interface is verbatim <kbd>CloudFormation</kbd> templating with a few nice add-ons specifically for variables, environment variables included. A common theme here is that this is a huge topic and the details are out of the scope of this book.</p>
<p><span><kbd>CloudFormation</kbd> creates t</span>he RDS instance on our behalf with several lines of setup in <kbd>serverless.yml</kbd>. Details aside, note how there are multiple references to <kbd>${env:VPC_ID}</kbd> and other calls to <kbd>${env:}</kbd>. The <kbd>${env}</kbd> syntax is a method for pulling variables from the environment that exists in the Docker container from our process of starting up the container. You may accomplish the same thing on your host system provided you have a way of managing environment variables.</p>
<p>Much of the complexity of this setup comes from the fact that Lambda functions by default will not have network access to AWS resources inside a <strong>virtual private cloud</strong> (<strong>VPC</strong>). Since RDS instances need to run inside a VPC, the Lambda functions need to be configured to run inside the same VPC and permissions set up accordingly:</p>
<pre style="padding-left: 30px">resources:<br/>  Resources:<br/>    ServerlessSecurityGroup:<br/>      Type: AWS::EC2::SecurityGroup<br/>      Properties:<br/>        GroupDescription: SecurityGroup for Serverless Functions<br/>        VpcId: ${env:VPC_ID}<br/>    RDSSecurityGroup:<br/>      Type: AWS::EC2::SecurityGroup<br/>      Properties:<br/>        GroupDescription: Ingress for RDS Instance<br/>        VpcId: ${env:VPC_ID}<br/>        SecurityGroupIngress:<br/>        - IpProtocol: tcp<br/>          FromPort: '5432'<br/>          ToPort: '5432'<br/>          SourceSecurityGroupId:<br/>            Ref: ServerlessSecurityGroup<br/>    RDSSubnetGroup:<br/>      Type: AWS::RDS::DBSubnetGroup<br/>      Properties:<br/>        DBSubnetGroupDescription: RDS Subnet Group<br/>        SubnetIds:<br/>          - ${env:SUBNET_ID_A}<br/>          - ${env:SUBNET_ID_B}<br/>          - ${env:SUBNET_ID_C}<br/>    RDSPostgresInstance:<br/>      Type: AWS::RDS::DBInstance<br/>      Properties:<br/>        AllocatedStorage: 100<br/>        AutoMinorVersionUpgrade: true<br/>        AvailabilityZone: ${self:provider.region}a<br/>        DBInstanceClass: db.t2.micro<br/>        DBName: ${env:CUPPING_DB_NAME}<br/>        DBSubnetGroupName:<br/>          Ref: RDSSubnetGroup<br/>        Engine: postgres<br/>        EngineVersion: 9.6.2<br/>        MasterUsername: ${env:CUPPING_DB_USERNAME}<br/>        MasterUserPassword: ${env:CUPPING_DB_PASSWORD}<br/>        PubliclyAccessible: false<br/>        VPCSecurityGroups:<br/>          - Fn::GetAtt: RDSSecurityGroup.GroupId</pre>
<p>During deployment, the Serverless Framework will add any defined <kbd>Resources</kbd> into the default <kbd>CloudFormation</kbd> template and deploy them together. Having our database described, we can perform a <kbd>make deploy</kbd> and see our dedicated PostgreSQL resource.</p>
<div class="packt_infobox">RDS and other hosted data stores are not silver bullets. These systems can still go down, and there are real constraints concerning computing power. However, a significant benefit of using a hosted data store is the hard work of managing, monitoring, and configuring is delegated to someone else. Serverless is not accurate in this case for a variety of reasons. I will assert that a hosted database eases much of the burden of managing your system and is an excellent fit in a truly serverless architecture.</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Setting up static assets</h1>
                </header>
            
            <article>
                
<p class="mce-root">Setting up an S3 bucket and CloudFront distribution to host static media isn't complicated and, in theory, <span>we could add this</span> to the <kbd>Resources</kbd> section of our <kbd>serverless.yml</kbd> file. The ability of Serverless to manage so many resources via <kbd>CloudFormation</kbd> is a slippery slope, since setting up systems can quickly become an exercise in learning and debugging <kbd>CloudFormation</kbd>. Another downside of a growing <kbd>Resources</kbd> section in the <kbd>serverless.yml</kbd> file is that deployments will take longer and longer. It's possible to only deploy application code during development, which results in single-digit second deployments; but when some system resource is updated, including environment variables, the entire <kbd>CloudFormation</kbd> stack needs to updated.</p>
<p>Rather than creating the S3 bucket and CloudFront distribution via <kbd>serverless.yml</kbd>, we can use a separate <kbd>CloudFormation</kbd> template designed just for this purpose. Another reason for splitting this out into a separate step is that this layer rarely changes. Once the CloudFront distribution is set up, there is a good chance you won't need to change anything for a very long time, if ever.</p>
<p>The following repository contains a <kbd>CloudFormation</kbd> template, a helper script, and documentation to set up a single - page web application on AWS:</p>
<p><a href="https://github.com/verypossible/cloudfront-single-page-app">https://github.com/verypossible/cloudfront-single-page-app</a></p>
<p>Again, you may read the details of this stack creation in the GitHub repository. After we choose one&#160;this stack with the necessary variables, we will end up with the following:</p>
<ul>
<li>An S3 bucket, which will host all of our static content</li>
<li>A CloudFront distribution, which will pull and cache content from S3</li>
<li>A free TLS certificate for <kbd>*.cupperslog.com</kbd></li>
<li>A Route53 record, which does the following:
<ul>
<li>Points <a href="https://cupperslog.com">https://cupperslog.com</a> to the CloudFront distribution</li>
<li>Redirects any <kbd>http://</kbd> traffic to <kbd>https://</kbd></li>
<li>Caches static assets for 10 minutes:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 60px"><strong>brianz@gold(master=)$ AWS_DEFAULT_REGION=us-east-1 ./manage-stack.sh create --domain cupperslog.com --zone-id ZNGYF5FXIUL0Z --name cupperslog </strong><br/><strong>{ </strong><br/><strong>    "StackId": "arn:aws:cloudformation:us-east-1:875406499000:stack/cupperslog/e7f15a50-b03c-11e7-97b0-5001ff34b4a6" </strong><br/><strong>}</strong> </pre>
<p>CloudFront distributions can take from several minutes to a couple of hours to be created, which is another good reason that we're doing this separate from our application code and database. Once finished, all that is required is uploading static assets to the S3 bucket that&#160;<kbd>CloudFormation</kbd> created for you. Ensure that the access control policy is set to <kbd>public-read</kbd> since this is a publicly accessible website. Uploading is accomplished via many tools, the AWS CLI tool being one of them:</p>
<pre><strong>brianz@gold$ aws s3 cp \<br/>   build/ \<br/>   --acl public-read \<br/>   --recurisve \<br/>   s3://cupperslog-s3bucketforstaticcontent-nswgo5ega4r1</strong></pre>
<div class="packt_tip">Other tools I've used for S3 file management are Cyberduck for OS X, S3 Organizer for Firefox, and the regular AWS web interface. They all do more or less the same thing, so pick what works for you.</div>
<p>For our example application, the frontend will consist of a simple React application that allows users to do the following:</p>
<ul>
<li>List coffee <kbd>cupping</kbd> sessions</li>
<li>View coffee <kbd>cupping</kbd> session details</li>
<li>Create a new coffee <kbd>cupping</kbd> session</li>
<li>Delete a coffee <kbd>cupping</kbd> session</li>
</ul>
<p>It should be clear that there is no authentication and no notion of a user in these examples. This application was built to demonstrate a serverless pattern, and even critical details such as user authentication and authorization wouldn't fit in this single chapter.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Viewing the deployed web application</h1>
                </header>
            
            <article>
                
<p>With everything in place, we can now upload our frontend assets to S3. We won't review the actual frontend React code, but if you're curious, you can take a look at that UI code in the GitHub repository at&#160;<a href="https://github.com/brianz/serverless-design-patterns/tree/master/ch2/ui">https://github.com/brianz/serverless-design-patterns/tree/master/ch2/ui</a><span>.</span></p>
<p>Using the preceding <kbd>aws s3 cp</kbd> command, a final production build of the frontend code is uploaded to S3 and ultimately serves the content as requested by the CloudFront CDN. When the first page is rendered, a request is made to our serverless backend to get a listing of all coffee <kbd>cupping</kbd> sessions:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/db072595-3241-419d-b12f-177f5ac01c14.png"/></div>
<div class="packt_infobox">A very common issue, and one that people often forget about, is cross-origin resource sharing, which is a security measure put in place by browsers. Our serverless backend was set up to sidestep this issue, making development much quicker. For a real production system, it's best to only allow CORS for your own domain or, better yet, run the serverless backend on your own domain rather than the autogenerated domain from API Gateway. Running the serverless API on your own custom domain is possible using AWS API Gateway, but this is out of the scope of this chapter.</div>
<p>Clicking on a single row, the detail page for the particular session is loaded:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="images/1cd15c11-60e5-4801-a0b6-a04b4d10a84c.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running tests</h1>
                </header>
            
            <article>
                
<p>Since we're responsible developers, we have written a full suite of unit tests for our application. For now, tests are run manually inside our Docker container. The Docker&#160;image used has <kbd>py.test</kbd> installed, as well as some coverage tools.</p>
<p>The only dependency to running tests is PostgreSQL. Docker again makes it very simple to run a PostgreSQL container and hook it up to our application container. Multiple strategies exist for this, from running Docker Compose to merely starting up a container with <kbd>docker run</kbd> and linking the containers manually. For simplicity, I use the latter option. See the targets in the repository <kbd>Makefile</kbd> for details.</p>
<p>To run tests, inside the container, we execute <kbd>make tests</kbd>. I have trimmed much of the output for brevity and clarity:</p>
<pre style="padding-left: 30px"><strong>root@d8dd5cc4bb86:/code# make tests</strong><br/><strong>py.test --cov=serverless/ --cov-report=html tests/</strong><br/><strong>Connected to: postgresql://postgres:@cupping-rltest-postgres:5432/test_cupping_log</strong><br/><strong>........</strong><br/><strong>==== test session starts ====</strong><br/><strong>platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0</strong><br/><strong>rootdir: /code, inifile:</strong><br/><strong>plugins: mock-1.6.2, cov-2.5.1</strong><br/><strong>collected 105 items</strong><br/><br/><strong>tests/test_cupping_model.py .........</strong><br/><strong>tests/test_cupping_persistence.py ......................</strong><br/><strong>tests/test_handler.py ...</strong><br/><strong>tests/test_helpers.py .</strong><br/><strong>tests/test_session_handler.py ...........................</strong><br/><strong>tests/test_session_models.py ......</strong><br/><strong>tests/test_session_persistence.py .....................................</strong><br/><br/><strong>--- coverage: platform linux, python 3.6.2-final-0 ---</strong><br/><strong>Coverage HTML written to dir htmlcov</strong><br/><br/><strong>==== 105 passed in 2.04 seconds ===</strong></pre>
<p>The result is an <kbd>htmlcov/index.html</kbd> file that visually shows test coverage throughout the application and highlights lines that were not executed during the test run:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/fbbfa9ba-735f-4f9c-8f35-3133b08bd294.png" style="width:46.67em;height:34.50em;"/></div>
<p><span>The preceding image is a test coverage report from the&#160;</span><kbd>pytest</kbd><span>&#160;and coverage libraries</span></p>
<p>Coverage may also be displayed on the console if we ask for it specifically:</p>
<pre style="padding-left: 30px"><strong>----------- coverage: platform linux, python 3.6.2-final-0 -----------</strong><br/><strong>Name Stmts Miss Branch BrPart Cover</strong><br/><strong>------------------------------------------------------------------------------</strong><br/><strong>serverless/cupping/__init__.py 0 0 0 0 100%</strong><br/><strong>serverless/cupping/constants.py 11 0 0 0 100%</strong><br/><strong>serverless/cupping/db/__init__.py 52 2 8 3 92%</strong><br/><strong>serverless/cupping/db/mixins.py 33 14 6 1 51%</strong><br/><strong>serverless/cupping/exceptions.py 5 0 0 0 100%</strong><br/><strong>serverless/cupping/handlers/__init__.py 0 0 0 0 100%</strong><br/><strong>serverless/cupping/handlers/decorators.py 9 0 0 0 100%</strong><br/><strong>serverless/cupping/handlers/helpers.py 34 0 16 1 98%</strong><br/><strong>serverless/cupping/handlers/session.py 43 4 4 0 91%</strong><br/><strong>serverless/cupping/helpers.py 3 0 0 0 100%</strong><br/><strong>serverless/cupping/models/__init__.py 2 0 0 0 100%</strong><br/><strong>serverless/cupping/models/cupping.py 20 0 4 0 100%</strong><br/><strong>serverless/cupping/models/session.py 14 0 4 0 100%</strong><br/><strong>serverless/cupping/persistence/__init__.py 2 0 0 0 100%</strong><br/><strong>serverless/cupping/persistence/base.py 7 0 0 0 100%</strong><br/><strong>serverless/cupping/persistence/cupping.py 48 0 14 0 100%</strong><br/><strong>serverless/cupping/persistence/queries.py 6 0 0 0 100%</strong><br/><strong>serverless/cupping/persistence/session.py 41 0 10 0 100%</strong><br/><strong>serverless/handler.py 32 3 2 1 88%</strong><br/><strong>------------------------------------------------------------------------------</strong><br/><strong>TOTAL 362 23 68 6 92%</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Iteration and deployment</h1>
                </header>
            
            <article>
                
<p>Inevitably, there will be multiple deployments when developing an application such as this, and even once the first production version has shipped. Serverless speeds up this process dramatically, and once you experience the increased velocity, you may have a hard time going back to your old ways.</p>
<p>A deployment with the Serverless Framework consists of one command with a couple of variations.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying the entire stack</h1>
                </header>
            
            <article>
                
<p>To deploy everything in the <kbd>serverless.yml</kbd> file, the <kbd>deploy</kbd> command is used, specifying the <kbd>stage (-s)</kbd> variable (which defaults to <kbd>dev</kbd>):</p>
<pre><strong># serverless deploy -s $ENV</strong></pre>
<div class="packt_infobox">The <kbd>make deploy</kbd> target in use for this chapter's example executes this exact command.</div>
<p>When doing a full deployment like this, Serverless will upload your Lambda resources and execute the entire <kbd>CloudFormation</kbd> template. Even with a simple <kbd>CloudFormation</kbd> template, this can take several seconds. With bigger stacks, it can be even longer. It's unfortunate that some people believe this is the only method of deploying application code with this framework. To make application code deployments even faster, we can specify precisely which functions to deploy.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying the application code</h1>
                </header>
            
            <article>
                
<p>Once you are in the state of code iteration and redeployment, you'll want to make that loop as short as possible. To accomplish this, specifying the function name when doing the deployment step goes through the process of uploading your Lambda function, but skips the <kbd>CloudFormation</kbd> update. In my experience, this results in deployments that are typically low single-digit seconds:</p>
<pre><strong># serverless deploy function -s $ENV -f $FUNCTION_NAME</strong></pre>
<p>I can hear you thinking, what about doing deployments to a production system that is serving live traffic? Behind the scenes, AWS Lambda is using container technology to respond to events. During a deployment, any Lambda invocations continue doing their jobs as instructed. At a certain point, a new Lambda function will complete its upload and configuration process. Only at that time will the new function begin serving traffic. In short, the tricky dance of draining active connections and handing off new connections to new application state is handled for you. This behavior should be a standard feature among other FaaS providers. Users of other platforms should verify this on their own.</p>
<div class="packt_infobox"><span>The <kbd>Makefile</kbd> used in this chapter's example has a target which helps speed up the deployment process even more.</span> <kbd>make deploy function=FunctionName</kbd> <span>may be used to deploy a single Lambda function, where <kbd>FunctionName</kbd> should be a name listed in the <kbd>serverless.yml</kbd> file (for example,&#160;<kbd>make deploy function=HandleSesion</kbd>). This works by skipping the <kbd>CloudFormation</kbd> update and only packaging and uploading a single function. <kbd>CloudFormation</kbd> updates will take a few to many seconds, whereas a single function deployment or update is typically low single-digit.</span></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we walked through the entire process of creating a three-tier web application with a serverless architecture consisting of a view layer, data layer, and application layer and which is powered by AWS Lambda. All services employed in the example web application are from AWS, and none require managing a virtual machine or operating system directly.</p>
<p>Readers should have a good understanding of the advantages of such a system and how to start the process of structuring their application using this pattern. I presented several helpers and shortcuts that should aid readers in speeding up their development.</p>
<p>In <a href="svrls-dsnptn-bstprac_ch03.html">Chapter 3</a>, <em>A</em>&#160;<em>Three-Tier Web Application Pattern with GraphQL</em>, we will work through a similar pattern by porting the example application from a RESTful interface to a GraphQL interface.</p>


            </article>

            
        </section>
    </div>
</body>
</html>