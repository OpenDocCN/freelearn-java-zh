<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Application Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Application Performance</h1></div></div></div><p>The earliest computing devices were built to perform automatic computations and, as computers grew in power, they became increasingly popular because of how much and how fast they could compute. Even today, this essence lives on in our anticipation that computers can execute our business calculations faster than before by means of the applications we run on them.</p><p>Compared to performance analysis and optimization at a smaller component level, as we saw in previous chapters, it takes a holistic approach to improve performance at the application level. The higher-level concerns, such as serving a certain number of users in a day, or handling an identified quantum of load through a multi-layered system, requires us to think about how the components fit together and how the load is designed to flow through it. In this chapter, we will discuss such high-level concerns. Like the previous chapter, by and large this chapter applies to applications written in any JVM language, but with a focus on Clojure. In this chapter, we will discuss general performance techniques that apply to all layers of the code:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Choosing libraries</li><li class="listitem" style="list-style-type: disc">Logging</li><li class="listitem" style="list-style-type: disc">Data sizing</li><li class="listitem" style="list-style-type: disc">Resource pooling</li><li class="listitem" style="list-style-type: disc">Fetch and compute in advance</li><li class="listitem" style="list-style-type: disc">Staging and batching</li><li class="listitem" style="list-style-type: disc">Little's law</li></ul></div><div class="section" title="Choosing libraries"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec54"/>Choosing libraries</h1></div></div></div><p>Most non-trivial applications <a class="indexterm" id="id580"/>depend a great deal on third-party libraries for various functionality, such as logging, serving web requests, connecting to databases, writing to message queues, and so on. Many of these libraries not only carry out parts of critical business functionality but also appear in the performance-sensitive areas of our code, impacting the overall performance. It is imperative that we choose libraries wisely (with respect to features versus performance trade off) after due performance analysis.</p><p>The crucial factor in choosing libraries is not identifying which library to use, rather it is having a performance model of our applications and having the use cases benchmarked under representative load. Only benchmarks can tell us whether the performance is problematic or acceptable. If the performance is below expectation, a drill-down profiling can show us whether a third-party library is causing the performance issue. In <a class="link" href="ch06.html" title="Chapter 6. Measuring Performance">Chapter 6</a>, <span class="emphasis"><em>Measuring Performance</em></span> and <a class="link" href="ch07.html" title="Chapter 7. Performance Optimization">Chapter 7</a>, <span class="emphasis"><em>Performance Optimization</em></span> we discussed how to measure performance and identify bottlenecks. You can evaluate multiple libraries for performance-sensitive use cases and choose what suits.</p><p>Libraries often <a class="indexterm" id="id581"/>improve (or occasionally lose) performance with new releases, so measurement and profiling (comparative, across versions) should be an ongoing practice for the development and maintenance lifecycle of our applications. Another factor to note is that libraries may show different performance characteristics based on the use case, load, and the benchmark. The devil is in the benchmark details. Be sure that your benchmarks are as close as possible to the representative scenario for your application.</p><div class="section" title="Making a choice via benchmarks"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec111"/>Making a choice via benchmarks</h2></div></div></div><p>Let's take a brief look at a few<a class="indexterm" id="id582"/> general use cases where performance of third-party libraries are exposed via benchmarks.</p><div class="section" title="Web servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec40"/>Web servers</h3></div></div></div><p>Web servers<a class="indexterm" id="id583"/> are<a class="indexterm" id="id584"/> typically subject to quite a bit of performance benchmarking due to their generic nature and scope. One such benchmark for Clojure web servers<a class="indexterm" id="id585"/> exists here:</p><p>
<a class="ulink" href="https://github.com/ptaoussanis/clojure-web-server-benchmarks">https://github.com/ptaoussanis/clojure-web-server-benchmarks</a>
</p><p>Web servers are complex pieces of software and they may exhibit different characteristics under various conditions. As you will notice, the performance numbers vary based on keep-alive versus non-keep-alive modes and request volume—at the time of writing, Immutant-2 came out better in keep-alive mode but fared poorly in the non-keep-alive benchmark. In production, people often front their application servers with reverse proxy servers, for example Nginx or HAProxy, which make keep-alive connections to application servers.</p></div><div class="section" title="Web routing libraries"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec41"/>Web routing libraries</h3></div></div></div><p>There are several<a class="indexterm" id="id586"/> web routing libraries<a class="indexterm" id="id587"/> for Clojure, as listed here:</p><p>
<a class="ulink" href="https://github.com/juxt/bidi#comparison-with-other-routing-libraries">https://github.com/juxt/bidi#comparison-with-other-routing-libraries</a>
</p><p>The same document also shows a performance benchmark with <span class="strong"><strong>Compojure</strong></span> as the baseline, in which (at the time of writing) Compojure<a class="indexterm" id="id588"/> turns out to be performing better than <span class="strong"><strong>Bidi</strong></span>. However, another benchmark compares Compojure, <span class="strong"><strong>Clout</strong></span> (the library that Compojure internally uses), and <span class="strong"><strong>CalfPath</strong></span>
<a class="indexterm" id="id589"/> routing <a class="indexterm" id="id590"/>here:</p><p>
<a class="ulink" href="https://github.com/kumarshantanu/calfpath#development">https://github.com/kumarshantanu/calfpath#development</a>
</p><p>In this benchmark, as of this writing, Clout performs better than Compojure, and CalfPath outperforms Clout. However, you should be aware of any caveats in the faster libraries.</p></div><div class="section" title="Data serialization"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec42"/>Data serialization</h3></div></div></div><p>There are <a class="indexterm" id="id591"/>several ways to<a class="indexterm" id="id592"/> serialize data in Clojure, for example EDN and Fressian. Nippy is another serialization library with benchmarks to demonstrate how well it performs over EDN and<a class="indexterm" id="id593"/> Fressian:</p><p>
<a class="ulink" href="https://github.com/ptaoussanis/nippy#performance">https://github.com/ptaoussanis/nippy#performance</a>
</p><p>We covered Nippy in <a class="link" href="ch02.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span> to show how it uses transients to speed up its internal computations. Even within Nippy, there are several flavors of serialization that have different features/performance trade-offs.</p></div><div class="section" title="JSON serialization"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec43"/>JSON serialization</h3></div></div></div><p>Parsing and generating <a class="indexterm" id="id594"/>JSON is a <a class="indexterm" id="id595"/>very common use case in RESTful services and web applications. The Clojure contrib library clojure/data.json (<a class="ulink" href="https://github.com/clojure/data.json">https://github.com/clojure/data.json</a>) provides this functionality. However, many people have found out that the Cheshire library<a class="indexterm" id="id596"/> <a class="ulink" href="https://github.com/dakrone/cheshire">https://github.com/dakrone/cheshire</a> performs much better than the former. The included benchmarks in Cheshire can be run using the following command:</p><div class="informalexample"><pre class="programlisting">lein with-profile dev,benchmark test</pre></div><p>Cheshire internally uses the Jackson Java library<a class="indexterm" id="id597"/> <a class="ulink" href="https://github.com/FasterXML/jackson">https://github.com/FasterXML/jackson</a>, which is known for its good performance.</p></div><div class="section" title="JDBC"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec44"/>JDBC</h3></div></div></div><p>JDBC access is <a class="indexterm" id="id598"/>another very common use case <a class="indexterm" id="id599"/>among applications using relational databases. The Clojure contrib library <code class="literal">clojure/java.jdbc</code> <a class="ulink" href="https://github.com/clojure/java.jdbc">https://github.com/clojure/java.jdbc</a> provides a Clojure JDBC API. Asphalt <a class="ulink" href="https://github.com/kumarshantanu/asphalt">https://github.com/kumarshantanu/asphalt</a> is an<a class="indexterm" id="id600"/> alternative JDBC library where the comparative benchmarks can be run as follows:</p><div class="informalexample"><pre class="programlisting">lein with-profile dev,c17,perf test</pre></div><p>As of this writing, Asphalt outperforms <code class="literal">clojure/java.jdbc</code> by several micro seconds, which may be useful in low-latency applications. However, note that JDBC performance is usually dominated by SQL queries/joins, database latency, connection pool parameters, and so on. We will discuss more about JDBC in later sections.</p></div></div></div></div>
<div class="section" title="Logging"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec55"/>Logging</h1></div></div></div><p>Logging<a class="indexterm" id="id601"/> is a prevalent activity that almost all non-trivial applications do. Logging calls are quite frequent, hence it is important to make sure our logging configuration is tuned well for performance. If you are not familiar with logging systems (especially on the JVM), you<a class="indexterm" id="id602"/> may want to take some time to get familiar with those first. We will cover the use of <code class="literal">clojure/tools.logging</code>, <span class="strong"><strong>SLF4J</strong></span>
<a class="indexterm" id="id603"/> and <span class="strong"><strong>LogBack</strong></span>
<a class="indexterm" id="id604"/> libraries (as a combination) for logging, and look into<a class="indexterm" id="id605"/> how to make them perform well:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Clojure/tools.logging <a class="ulink" href="https://github.com/clojure/tools.logging">https://github.com/clojure/tools.logging</a></li><li class="listitem" style="list-style-type: disc">SLF4J: <a class="ulink" href="http://www.slf4j.org/">http://www.slf4j.org/</a></li><li class="listitem" style="list-style-type: disc">LogBack: <a class="ulink" href="http://logback.qos.ch/">http://logback.qos.ch/</a></li></ul></div><div class="section" title="Why SLF4J/LogBack?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec112"/>Why SLF4J/LogBack?</h2></div></div></div><p>Besides SLF4J/LogBack, there are several logging libraries to choose from in the Clojure application, for <a class="indexterm" id="id606"/>example Timbre, Log4j and java.util.logging. While there is nothing <a class="indexterm" id="id607"/>wrong with these libraries, we are often constrained into choosing something that covers most other third-party libraries (also including Java libraries) in our applications for logging purposes. SLF4J is a Java logger facade that detects any available implementation (LogBack, Log4j, and so on) —we choose LogBack simply because it performs well and is highly configurable. The library clojure/tools.logging provides a Clojure logging API that detects SLF4J, Log4j or java.util.logging (in that order) in the classpath and uses whichever implementation is found first.</p></div><div class="section" title="The setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec113"/>The setup</h2></div></div></div><p>Let's walk through <a class="indexterm" id="id608"/>how to set up a logging system for your application using LogBack, SLF4J <a class="indexterm" id="id609"/>and <code class="literal">clojure/tools.logging</code> for a project built using Leiningen.</p><div class="section" title="Dependencies"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec45"/>Dependencies</h3></div></div></div><p>Your <code class="literal">project.clj</code> file should have<a class="indexterm" id="id610"/> the LogBack, SLF4J and <code class="literal">clojure/tools.logging</code> dependencies under the <code class="literal">:dependencies</code> key:</p><div class="informalexample"><pre class="programlisting">[ch.qos.logback/logback-classic "1.1.2"]
[ch.qos.logback/logback-core    "1.1.2"]
[org.slf4j/slf4j-api            "1.7.9"]
[org.codehaus.janino/janino     "2.6.1"]  ; for Logback-config
[org.clojure/tools.logging      "0.3.1"]</pre></div><p>The previously mentioned versions are current and work as of the time of writing. You may want to use updated versions, if available.</p></div><div class="section" title="The logback configuration file"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec46"/>The logback configuration file</h3></div></div></div><p>You need to create a <code class="literal">logback.xml</code> file<a class="indexterm" id="id611"/> in the <code class="literal">resources</code> directory:</p><div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;configuration&gt;

  &lt;appender name="FILE"
            class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
    &lt;file&gt;${logfile.general.name:-logs/application.log}&lt;/file&gt;
    &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt;
      &lt;!-- daily rollover --&gt;
      &lt;fileNamePattern&gt;${logfile.general.name:-logs/application.log}.%d{yyyy-MM-dd}.%i.gz&lt;/fileNamePattern&gt;
      &lt;timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"&gt;
        &lt;!-- or whenever the file size reaches 100MB --&gt;
        &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt;
      &lt;/timeBasedFileNamingAndTriggeringPolicy&gt;
      &lt;!-- keep 30 days worth of history --&gt;
      &lt;maxHistory&gt;30&lt;/maxHistory&gt;
    &lt;/rollingPolicy&gt;
    &lt;append&gt;true&lt;/append&gt;
    &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt;
      &lt;layout class="ch.qos.logback.classic.PatternLayout"&gt;
        &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;
      &lt;/layout&gt;
<span class="strong"><strong>      &lt;immediateFlush&gt;false&lt;/immediateFlush&gt;</strong></span>
    &lt;/encoder&gt;
  &lt;/appender&gt;

<span class="strong"><strong>  &lt;appender name="AsyncFile" class="ch.qos.logback.classic.AsyncAppender"&gt;</strong></span>
<span class="strong"><strong>    &lt;queueSize&gt;500&lt;/queueSize&gt;</strong></span>
<span class="strong"><strong>    &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt;</strong></span>
<span class="strong"><strong>    &lt;appender-ref ref="FILE" /&gt;</strong></span>
<span class="strong"><strong>  &lt;/appender&gt;</strong></span>

  &lt;!-- You may want to set the level to DEBUG in development --&gt;
  &lt;root level="ERROR"&gt;
<span class="strong"><strong>    &lt;appender-ref ref="AsyncFile" /&gt;</strong></span>
  &lt;/root&gt;

  &lt;!-- Replace com.example with base namespace of your app --&gt;
  &lt;logger name="com.example" additivity="false"&gt;
    &lt;!-- You may want to set the level to DEBUG in development --&gt;
    &lt;level value="INFO"/&gt;
<span class="strong"><strong>    &lt;appender-ref ref="AsyncFile" /&gt;</strong></span>
  &lt;/logger&gt;

&lt;/configuration&gt;</pre></div><p>The previous <code class="literal">logback.xml</code> file<a class="indexterm" id="id612"/> is simple on purpose (for illustration) and has just enough configuration to get you started with logging using LogBack.</p></div><div class="section" title="Optimization"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec47"/>Optimization</h3></div></div></div><p>The optimization points <a class="indexterm" id="id613"/>are highlighted in the <code class="literal">logback.xml</code> file we saw earlier in this section. We set the <code class="literal">immediateFlush</code> attribute to <code class="literal">false</code> such that the messages are buffered before flushing to the appender. We also wrapped the regular file appender with an asynchronous appender and edited the <code class="literal">queueSize</code> and <code class="literal">discardingThreshold</code> attributes, which gets us much better results than the default.</p><p>Unless optimized, logging<a class="indexterm" id="id614"/> configurations are usually a common source of suboptimal performance in many applications. Usually, the performance problems show up only at high load when the log volume is very high. The optimizations discussed previously are only a few of the many possible optimizations that one can experiment with. The chapters in LogBack documentation, such as <a class="indexterm" id="id615"/>
<span class="strong"><strong>encoders</strong></span> (<a class="ulink" href="http://logback.qos.ch/manual/encoders.html">http://logback.qos.ch/manual/encoders.html</a>), <span class="strong"><strong>appenders</strong></span> (<a class="ulink" href="http://logback.qos.ch/manual/appenders.html">http://logback.qos.ch/manual/appenders.html</a>) and<a class="indexterm" id="id616"/> <span class="strong"><strong>configuration</strong></span> (<a class="ulink" href="http://logback.qos.ch/manual/configuration.html">http://logback.qos.ch/manual/configuration.html</a>) have useful <span class="strong"><strong>information</strong></span>. There are also tips <a class="ulink" href="http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/">http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/</a> on the Internet that may provide useful pointers.</p></div></div></div>
<div class="section" title="Data sizing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec56"/>Data sizing</h1></div></div></div><p>The cost of abstractions in terms of the<a class="indexterm" id="id617"/> data size plays an important role. For example, whether or not a data element can fit into a processor cache line depends directly upon its size. On a Linux system, we can find out the cache line size and other parameters by inspecting the values in the files under the <code class="literal">/sys/devices/system/cpu/cpu0/cache/</code> directory. Refer to <a class="link" href="ch04.html" title="Chapter 4. Host Performance">Chapter 4</a>, <span class="emphasis"><em>Host Performance</em></span>, where we discussed how to compute the size of primitives, objects, and data elements.</p><p>Another concern we generally find with data sizing is how much data we hold at any time in the heap. As we noted in earlier chapters, GC has direct consequences on the application performance. While processing data, often we do not really need all the data we hold on to. Consider the example of generating a summary report of sold items for a certain period (months) of time. After the subperiod (month-wise) summary data is computed, we do not need the item details anymore, hence it's better to remove the unwanted data while we add the summaries. See the following example:</p><div class="informalexample"><pre class="programlisting">(defn summarize [daily-data]  ; daily-data is a map
  (let [s (items-summary (:items daily-data))]
    (-&gt; daily-data
      (select-keys [:digest :invoices])  ; keep required k/v pairs
      (assoc :summary s))))

;; now inside report generation code
(-&gt; (fetch-items period-from period-to :interval-day)
  (map summarize)
  generate-report)</pre></div><p>Had we not <a class="indexterm" id="id618"/>used <code class="literal">select-keys</code> in the previous <code class="literal">summarize</code> function, it would have returned a map with extra :<code class="literal">summary</code> data along with all other existing keys in the map. Now, such a thing is often combined with lazy sequences, so for this scheme to work it is important not to hold onto the head of the lazy sequence. Recall that in <a class="link" href="ch02.html" title="Chapter 2. Clojure Abstractions">Chapter 2</a>, <span class="emphasis"><em>Clojure Abstractions</em></span> we discussed the perils of holding onto the head of a lazy sequence.</p><div class="section" title="Reduced serialization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec114"/>Reduced serialization</h2></div></div></div><p>We discussed in earlier chapters <a class="indexterm" id="id619"/>that serialization over an I/O channel is a common source of latency. The perils of over-serialization cannot be overstated. Whether we read or write data from a data source over an I/O channel, all of that data needs to be prepared, encoded, serialized, de-serialized, and parsed before being worked upon. The less data that is involved, the better it is for every step in order to lower the overhead. Where there is no I/O involved (such as in-process communication), it generally makes no sense to serialize.</p><p>A common example of over-serialization is when working with SQL databases. Often, there are common SQL query functions that fetch all columns of a table or a relation—they are called by various functions that implement business logic. Fetching data that we do not need is wasteful and detrimental to performance for the same reason that we discussed in the previous paragraph. While it may seem more work to write one SQL statement and one database-query function for each use case, it pays off with better performance. Code that uses NoSQL databases is also subject to this anti-pattern—we have to take care to fetch only what we need even though it may lead to additional code.</p><p>There's a pitfall to be aware of when reducing serialization. Often, some information needs to be inferred in the absence of serialized data. In such cases, where some of the serialization is dropped so that we can infer other information, we must compare the cost of inference versus the serialization overhead. The comparison may not necessarily be only per operation, but rather on the whole, such that we can consider the resources we can allocate in order to achieve capacities for various parts of our systems.</p></div><div class="section" title="Chunking to reduce memory pressure"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec115"/>Chunking to reduce memory pressure</h2></div></div></div><p>What happens when we <a class="indexterm" id="id620"/>slurp a text file regardless of its size? The contents of the entire file will sit in the JVM heap. If the file is larger than the JVM heap capacity, the JVM will terminate, throwing <code class="literal">OutOfMemoryError</code>. If the file is large, but not enough to force the JVM into OOM error, it leaves relatively less JVM heap space for other operations to continue in the application. Similar situations take place when we carry out any operation disregarding the JVM heap capacity. Fortunately, this can be fixed by reading data in chunks and processing them before reading more. In <a class="link" href="ch03.html" title="Chapter 3. Leaning on Java">Chapter 3</a>, <span class="emphasis"><em>Leaning on Java</em></span>, we briefly discussed memory<a class="indexterm" id="id621"/> mapped<a class="indexterm" id="id622"/> buffers, which is another complementary solution that you may like to explore.</p><div class="section" title="Sizing for file/network operations"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec48"/>Sizing for file/network operations</h3></div></div></div><p>Let's take the example of a data ingestion process where a semi-automated job uploads large <span class="strong"><strong>Comma Separated File (CSV)</strong></span>
<a class="indexterm" id="id623"/> files via <span class="strong"><strong>File Transfer Protocol (FTP)</strong></span>
<a class="indexterm" id="id624"/> to a file server, and another automated job (written in Clojure) <a class="indexterm" id="id625"/>runs periodically to detect the arrival of files via a Network File System (NFS). After detecting a new file, the Clojure program processes the file, updates the result in a database, and archives the file. The program detects and processes several files concurrently. The size of the CSV files is not known in advance, but the format is predefined.</p><p>As per the previous description, one potential problem is, since there could be multiple files being processed concurrently, how do we distribute the JVM heap among the concurrent file-processing jobs? Another issue at hand could be that the operating system imposes a limit on how many files could be open at a time; on Unix-like systems you can use the <code class="literal">ulimit</code> command to extend the limit. We cannot arbitrarily slurp the CSV file contents—we must limit each job to a certain amount of memory, and also limit the number of jobs that can run concurrently. At the same time, we cannot read a very small number of rows from a file at a time because this may impact performance:</p><div class="informalexample"><pre class="programlisting">(def ^:const K 1024)

;; create the buffered reader using custom 128K buffer-size
(-&gt; filename
  java.io.FileInputStream.
  java.io.InputStreamReader.
  (java.io.BufferedReader. (* K 128)))</pre></div><p>Fortunately, we can specify the buffer size when reading from a file (or even from a network stream) so as to tune the memory usage and performance as appropriate. In the previous code example, we explicitly set the buffer size of the reader to facilitate the same.</p></div><div class="section" title="Sizing for JDBC query results"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec49"/>Sizing for JDBC query results</h3></div></div></div><p>Java's interface standard for SQL <a class="indexterm" id="id626"/>databases, JDBC (which is technically not an acronym), supports <span class="emphasis"><em>fetch size</em></span> for fetching query results via JDBC drivers. The default fetch size depends on the JDBC driver. Most of the JDBC drivers keep a low default value to avoid high memory usage and for internal performance optimization reasons. A notable exception to this norm is the MySQL JDBC driver that completely fetches and stores all rows in memory by default:</p><div class="informalexample"><pre class="programlisting">(require '[clojure.java.jdbc :as jdbc])

;; using prepare-statement directly
(with-open
  [stmt (jdbc/prepare-statement
          conn sql :fetch-size 1000 :max-rows 9000)
   rset (resultset-seq (.executeQuery stmt))]
  (vec rset))

;; using query
(jdbc/query db [{:fetch-size 1000}
           "SELECT empno FROM emp WHERE country=?" 1])</pre></div><p>When using the Clojure contrib<a class="indexterm" id="id627"/> library <code class="literal">java.jdbc</code> (<a class="ulink" href="https://github.com/clojure/java.jdbc">https://github.com/clojure/java.jdbc</a> as of version 0.3.7), the fetch size can be set while preparing a <a class="indexterm" id="id628"/>statement as shown in the previous example. Note that the fetch size does not guarantee proportional latency; however, it can be used safely for memory sizing. We must test any performance-impacting latency changes due to fetch size at different loads and use cases for the particular database and JDBC driver. Another important factor to note is that the benefit of <code class="literal">:fetch-size</code> can be useful only if the query result set is consumed incrementally and lazily—if a function extracts all rows from a result set to create a vector, then the benefit of <code class="literal">:fetch-size</code> is nullified from a memory conservation point of view. Besides fetch size, we can also pass the <code class="literal">:max-rows</code> argument to limit the maximum rows to be returned by a query—however, this implies that the extra rows will be truncated from the result, and not whether the database will internally limit the number of rows to realize.</p></div></div></div>
<div class="section" title="Resource pooling"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec57"/>Resource pooling</h1></div></div></div><p>There are several types<a class="indexterm" id="id629"/> of resources on the JVM that are rather expensive to initialize. Examples are HTTP connections, execution threads, JDBC connections, and so on. The Java API recognizes such resources and has built-in support for creating a pool of some of those resources, such that the consumer code borrows a resource from a pool when required and at the end of the job simply returns it to the pool. Java's thread pools (discussed in <a class="link" href="ch05.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>) and JDBC data sources are prominent examples. The idea is to preserve the initialized objects for reuse. Even though Java does not support pooling of a resource type directly, one can always create a pool abstraction around custom expensive resources. Note that the pooling technique is common in I/O activities, but can be equally applicable to non-I/O purposes where initialization cost is high.</p><div class="section" title="JDBC resource pooling"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec116"/>JDBC resource pooling</h2></div></div></div><p>Java supports the obtaining<a class="indexterm" id="id630"/> of JDBC connections <a class="indexterm" id="id631"/>via the <code class="literal">javax.sql.DataSource </code>interface, which can be pooled. A JDBC connection pool implements this interface. Typically, a JDBC connection pool is implemented by third-party libraries or a JDBC driver itself. Generally, very few JDBC drivers implement a connection pool, so Open Source third-party JDBC resource pooling libraries such as Apache DBCP, c3p0, BoneCP, HikariCP, and so on are popular. They also support validation queries for eviction of stale connections that might result from network timeouts and firewalls, and guard against connection leaks. Apache DBCP and HikariCP are accessible from Clojure via their respective Clojure wrapper libraries <a class="indexterm" id="id632"/>Clj-DBCP (<a class="ulink" href="https://github.com/kumarshantanu/clj-dbcp">https://github.com/kumarshantanu/clj-dbcp</a>) and HikariCP<a class="indexterm" id="id633"/> (<a class="ulink" href="https://github.com/tomekw/hikari-cp">https://github.com/tomekw/hikari-cp</a>), and there are Clojure examples describing how to construct C3P0 and BoneCP pools (<a class="ulink" href="http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html">http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html</a>).</p><p>Connections are not the only JDBC resources that need to be pooled. Every time we create a new JDBC prepared statement, depending on the JDBC driver implementation, often the entire statement template is sent to the database server in order to obtain a reference to the prepared statement. As the database servers are generally deployed on separate hardware, there may be network latency involved. Hence, the pooling of prepared statements is a very desirable property of JDBC resource pooling libraries. Apache DBCP, C3P0, and BoneCP all support statement pooling, and the Clj-DBCP wrapper enables the pooling of prepared statements out-of-the-box for better performance. HikariCP has the opinion that statement pooling, nowadays, is already done internally by JDBC drivers, hence explicit pooling is not required. I would strongly advise running your benchmarks with the connection pooling libraries to determine whether or not it really works for your JDBC driver and application.</p></div></div>
<div class="section" title="I/O batching and throttling"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec58"/>I/O batching and throttling</h1></div></div></div><p>It is well known that chatty<a class="indexterm" id="id634"/> I/O calls generally lead to poor performance. In general, the solution <a class="indexterm" id="id635"/>is to batch together several messages and send them in one payload. In databases and network calls, batching is a common and useful technique to improve throughput. On the other hand, large batch sizes may actually harm throughput as they tend to incur memory overhead, and components may not be ready to handle a large batch at once. Hence, sizing the batches and throttling are just as important as batching. I would strongly advise conducting your own tests to determine the optimum batch size under representative load.</p><div class="section" title="JDBC batch operations"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec117"/>JDBC batch operations</h2></div></div></div><p>JDBC has long had batch-update <a class="indexterm" id="id636"/>support in its API, which includes the <code class="literal">INSERT</code>, <code class="literal">UPDATE</code>, <code class="literal">DELETE</code> statements. The Clojure contrib library <code class="literal">java.jdbc</code> supports JDBC batch operations via its own API, as we can see as follows:</p><div class="informalexample"><pre class="programlisting">(require '[clojure.java.jdbc :as jdbc])

;; multiple SQL statements
(jdbc/db-do-commands
  db true
  ["INSERT INTO emp (name, countrycode) VALUES ('John Smith', 3)"
   "UPDATE emp SET countrycode=4 WHERE empid=1379"])

;; similar statements with only different parametrs
(jdbc/db-do-prepared
  db true
  "UPDATE emp SET countrycode=? WHERE empid=?"
  [4 1642]
  [9 1186]
  [2 1437])</pre></div><p>Besides batch-update support, we can also batch JDBC queries. One of the common techniques is to use the SQL <code class="literal">WHERE</code> clause to avoid the <code class="literal">N+1</code> selects issue. The <code class="literal">N+1</code> issue indicates the situation when we execute one query in another child table for every row in a rowset from a master table. A similar technique can be used to combine several similar queries on the same table into just one, and segregate the data in the program afterwards. </p><p>Consider the following example that uses clojure.java.jdbc 0.3.7 and the MySQL database:</p><div class="informalexample"><pre class="programlisting">(require '[clojure.java.jdbc :as j])

(def db {:subprotocol "mysql"
         :subname "//127.0.0.1:3306/clojure_test"
         :user "clojure_test" :password "clojure_test"})

;; the snippet below uses N+1 selects
;; (typically characterized by SELECT in a loop)
(def rq "select order_id from orders where status=?")
(def tq "select * from items where fk_order_id=?")
(doseq [order (j/query db [rq "pending"])]
  (let [items (j/query db [tq (:order_id order)])]
    ;; do something with items
    …))

;; the snippet below avoids N+1 selects,
;; but requires fk_order_id to be indexed
(def jq "select t.* from orders r, items t
  where t.fk_order_id=r.order_id and r.status=? order by t.fk_order_id")
(let [all-items (group-by :fk_order_id (j/query db [jq "pending"]))]
  (doseq [[order-id items] all-items]
    ;; do something with items
    ...))</pre></div><p>In the previous<a class="indexterm" id="id637"/> example there are two tables: <code class="literal">orders</code> and <code class="literal">items</code>. The first snippet reads all order IDs from the <code class="literal">orders</code> table, and then iterates through them to query corresponding entries in the <code class="literal">items</code> table in a loop. This is the <code class="literal">N+1</code> selects performance anti-pattern you should keep an eye on. The second snippet avoids <code class="literal">N+1</code> selects by issuing a single SQL query, but may not perform very well unless the column <code class="literal">fk_order_id</code> is indexed.</p></div><div class="section" title="Batch support at API level"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec118"/>Batch support at API level</h2></div></div></div><p>When designing any <a class="indexterm" id="id638"/>service, it is very useful to provide an API for batch operations. This builds flexibility in the API such that batch sizing and throttling can be controlled in a fine-grained manner. Not surprisingly, it is also an effective recipe for building high-performance services. A common overhead we encounter when implementing batch operations is the identification of each item in the batch and their correlation across requests and responses. The problem becomes more prominent when requests are asynchronous.</p><p>The solution to the item identification issue is resolved either by assigning a canonical or global ID to each item in the request (batch), or by assigning every request (batch) a unique ID and each item in the request an ID that is local to the batch.</p><p>The choice of the exact solution usually depends on the implementation details. When requests are synchronous, you can do away with identification of each request item (see the Facebook API<a class="indexterm" id="id639"/> for reference: <a class="ulink" href="http://developers.facebook.com/docs/reference/api/batch/">http://developers.facebook.com/docs/reference/api/batch/</a>) where the items in response follow the same order as in the request. However, in asynchronous requests, items may have to be tracked via status-check call or callbacks. The desired tracking granularity typically guides the appropriate item identification strategy.</p><p>For example, if we have a batch API for order processing, every order would have a unique Order-ID that can be used in subsequent status-check calls. In another example, let's say there is a batch API for creating API keys for <a class="indexterm" id="id640"/>
<span class="strong"><strong>Internet of Things</strong></span> (<span class="strong"><strong>IoT</strong></span>) devices—here, the API keys are not known beforehand, but they can <a class="indexterm" id="id641"/>be generated and returned in a synchronous response. However, if this has to be an asynchronous batch API, the service should respond with a batch request ID that can be used later to find the status of the request. In a batch response for the request ID, the server can include request item IDs (for example device IDs, which may be unique for the client but not unique across all clients) with their respective status.</p></div><div class="section" title="Throttling requests to services"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec119"/>Throttling requests to services</h2></div></div></div><p>As every service can <a class="indexterm" id="id642"/>handle only a certain capacity, the rate at which <a class="indexterm" id="id643"/>we send requests to a service is important. The expectations about the service behavior are generally in terms of both throughput and latency. This requires us to send requests at a specified rate, as a rate lower than that may lead to under-utilization of the service, and a higher rate may overload the service or result in failure, thus leading to client-side under-utilization.</p><p>Let's say a third-party service can accept 100 requests per second. However, we may not know how robustly the service is implemented. Though sometimes it is not exactly specified, sending 100 requests at once (within 20ms, let's say) during each second may lead to lower throughput than expected. Evenly distributing the requests across the one-second duration, for example sending one request every 10ms (1000ms / 100 = 10ms), may increase the chance of attaining the optimum throughput.</p><p>For throttling, <span class="strong"><strong>Token bucket</strong></span>
<a class="indexterm" id="id644"/> (<a class="ulink" href="https://en.wikipedia.org/wiki/Token_bucket">https://en.wikipedia.org/wiki/Token_bucket</a>) and <span class="strong"><strong>Leaky bucket</strong></span>
<a class="indexterm" id="id645"/> (<a class="ulink" href="https://en.wikipedia.org/wiki/Leaky_bucket">https://en.wikipedia.org/wiki/Leaky_bucket</a>) algorithms can be useful. Throttling at a very fine-grained level requires that we buffer the items so that we can maintain a uniform rate. Buffering consumes memory and often requires ordering; queues (covered in <a class="link" href="ch05.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>), pipeline and persistent storage usually serve that purpose well. Again, buffering and queuing may be subject to back pressure due to system constraints. We will discuss pipelines, back pressure and buffering in a later section in this chapter.</p></div></div>
<div class="section" title="Precomputing and caching"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Precomputing and caching</h1></div></div></div><p>While <a class="indexterm" id="id646"/>processing data, we<a class="indexterm" id="id647"/> usually come across instances where few common computation steps precede several kinds of subsequent steps. That is to say, some amount of computation is common and the remaining is different. For high-latency common computations (I/O to access the data and memory/CPU to process it), it makes a lot of sense to compute them once and store in digest form, such that the subsequent steps can simply use the digest data and proceed from that point onward, thus resulting in reduced overall latency. This is also known as staging of semi-computed data and is a common technique to optimize processing of non-trivial data.</p><p>Clojure has decent support for caching. The built-in <code class="literal">clojure.core/memoize</code> function performs basic caching of computed results with no flexibility in using specific caching strategies and pluggable backends. The Clojure contrib library <code class="literal">core.memoize</code> offsets the lack of flexibility in <code class="literal">memoize</code> by providing several configuration options. Interestingly, the features in <code class="literal">core.memoize</code> are also useful as a separate caching library, so the common portion is factored out as a Clojure contrib library called <code class="literal">core.cache</code> on top of which <code class="literal">core.memoize</code> is implemented.</p><p>As many <a class="indexterm" id="id648"/>applications are deployed on multiple servers for availability, scaling<a class="indexterm" id="id649"/> and maintenance reasons, they need distributed caching that is fast and space efficient. The open source memcached project is a popular in-memory, distributed key-value/object store that can act as a caching server for web applications. It hashes the keys to identify the server to store the value on, and has no out-of-the-box replication or persistence. It is used to cache database query results, computation results, and so on. For Clojure, there is a memcached client library called <a class="indexterm" id="id650"/>SpyGlass (<a class="ulink" href="https://github.com/clojurewerkz/spyglass">https://github.com/clojurewerkz/spyglass</a>). Of course, memcached is not limited to just web applications; it can be used for other purposes too.</p></div>
<div class="section" title="Concurrent pipelines"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Concurrent pipelines</h1></div></div></div><p>Imagine a situation where<a class="indexterm" id="id651"/> we have to carry out jobs at a certain throughput, such that each job includes the same sequence of differently sized I/O task (task A), a memory-bound task (task B) and, again, an I/O task (task C). A naïve approach would be to create a thread pool and run each job off it, but soon we realize that this is not optimum because we cannot ascertain the utilization of each I/O resource due to unpredictability of the threads being scheduled by the OS. We also observe that even though several concurrent jobs have similar I/O tasks, we are unable to batch them in our first approach.</p><p>As the next iteration, we split each job in stages (A, B, C), such that each stage corresponds to one task. Since the tasks are well known, we create one thread pool (of appropriate size) per stage and execute tasks in them. The result of task A is required by task B, and B's result is required by task C—we enable this communication via queues. Now, we can tune the thread pool size for each stage, batch the I/O tasks, and throttle them for an optimum throughput. This kind of an arrangement is a concurrent pipeline. Some readers may find this feebly resembling the actor model or <span class="strong"><strong>Staged Event Driven Architecture</strong></span> (<span class="strong"><strong>SEDA</strong></span>)<a class="indexterm" id="id652"/> model, which are more refined models for this kind of approach. Recall that we discussed several kinds of in-process queues in <a class="link" href="ch05.html" title="Chapter 5. Concurrency">Chapter 5</a>, <span class="emphasis"><em>Concurrency</em></span>.</p><div class="section" title="Distributed pipelines"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec120"/>Distributed pipelines</h2></div></div></div><p>With this approach, it is possible<a class="indexterm" id="id653"/> to scale out the job execution to<a class="indexterm" id="id654"/> multiple hosts in a cluster using network queues, thereby offloading memory consumption, durability, and delivery to the queue infrastructure. For example, in a given scenario there could be several nodes in a cluster, all of them running the same code and exchanging messages (requests and intermediate result data) via network queues. </p><p>The following diagram depicts how a simple invoice-generation system might be connected to network queues:</p><div class="mediaobject"><img alt="Distributed pipelines" src="graphics/3642_08_03.jpg"/></div><p>RabbitMQ, HornetQ, ActiveMQ, Kestrel and Kafka are some well-known Open Source queue systems. Once in a while, the jobs may require distributed state and coordination. The <a class="indexterm" id="id655"/>Avout (<a class="ulink" href="http://avout.io/">http://avout.io/</a>) project implements the distributed version of Clojure's atom and ref, which can be used for this purpose. Tesser <a class="indexterm" id="id656"/>(<a class="ulink" href="https://github.com/aphyr/tesser">https://github.com/aphyr/tesser</a>) is another library for local and distributed parallelism using Clojure. The<a class="indexterm" id="id657"/> Storm (<a class="ulink" href="http://storm-project.net/">http://storm-project.net/</a>) and Onyx<a class="indexterm" id="id658"/> (<a class="ulink" href="http://www.onyxplatform.org/">http://www.onyxplatform.org/</a>) projects are distributed, real-time stream processing systems implemented using Clojure.</p></div></div>
<div class="section" title="Applying back pressure"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Applying back pressure</h1></div></div></div><p>We discussed <a class="indexterm" id="id659"/>back pressure briefly in the last chapter. Without back pressure we cannot build a reasonable load-tolerant system with predictable stability and performance. In this section, we will see how to apply back pressure in different scenarios in an application. At a fundamental level, we should have a threshold of a maximum number of concurrent jobs in the system and, based on that threshold, we should reject new requests above a certain arrival rate. The rejected messages may either be retried by the client or ignored if there is no control over the client. When applying back pressure to user-facing services, it may be useful to detect system load and deny auxiliary services first in order to conserve capacity and degrade gracefully in the face of high load.</p><div class="section" title="Thread pool queues"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec121"/>Thread pool queues</h2></div></div></div><p>JVM thread pools are backed by<a class="indexterm" id="id660"/> queues, which means that when we submit a job into a thread pool that already has the maximum jobs running, the new job lands in the queue. The queue is by default an unbounded queue, which is not suitable for applying back pressure. So, we have to create the thread pool backed by a bounded queue:</p><div class="informalexample"><pre class="programlisting">(import 'java.util.concurrent.LinkedBlockingDeque)
(import 'java.util.concurrent.TimeUnit)
(import 'java.util.concurrent.ThreadPoolExecutor)
(import 'java.util.concurrent.ThreadPoolExecutor$AbortPolicy)
(def tpool
  (let [q (LinkedBlockingDeque. 100)
        p (ThreadPoolExecutor$AbortPolicy.)]
    (ThreadPoolExecutor. 1 10 30 TimeUnit/SECONDS q p)))</pre></div><p>Now, on this pool, whenever there is an attempt to add more jobs than the capacity of the queue, it will throw an exception. The caller should treat the exception as a buffer-full condition and wait until the buffer has idle capacity again by periodically pooling the <code class="literal">java.util.concurrent.BlockingQueue.remainingCapacity()</code> method.</p></div><div class="section" title="Servlet containers such as Tomcat and Jetty"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec122"/>Servlet containers such as Tomcat and Jetty</h2></div></div></div><p>In the synchronous <a class="indexterm" id="id661"/>
<span class="strong"><strong>Tomcat</strong></span> <a class="indexterm" id="id662"/>and <a class="indexterm" id="id663"/>
<span class="strong"><strong>Jetty</strong></span> <a class="indexterm" id="id664"/>versions, each HTTP request is given a dedicated thread from a common thread pool that a user can configure. The number of simultaneous requests being served is limited by the thread pool size. A common way to control the arrival rate is to set the thread pool size of the server. The <span class="strong"><strong>Ring</strong></span> library<a class="indexterm" id="id665"/> uses an embedded jetty server by default in development mode. The embedded Jetty adapter (in Ring) can be programmatically configured with a thread pool size.</p><p>In the asynchronous (Async Servlet 3.0) versions of Tomcat and Jetty beside the thread pool size, it is also possible to specify the timeout for processing each request. However, note that the thread pool size does not limit the number of requests in asynchronous versions in the way it does on synchronous versions. The request processing is transferred to an ExecutorService (thread pool), which may buffer requests until a thread is available. This buffering behavior is tricky because this may cause system overload—you can override the default behavior by defining your own thread pool instead of using the servlet container's thread pool to return a HTTP error at a certain threshold of waiting requests.</p></div><div class="section" title="HTTP Kit"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec123"/>HTTP Kit</h2></div></div></div><p>
<span class="strong"><strong>HTTP Kit</strong></span> (<a class="ulink" href="http://http-kit.org/">http://http-kit.org/</a>) is a<a class="indexterm" id="id666"/> high-performance asynchronous (based on Java NIO implementation) web server for Clojure. It has built-in support for applying <a class="indexterm" id="id667"/>back pressure to new requests via a specified queue length. As of HTTP Kit 2.1.19, see the following snippet:</p><div class="informalexample"><pre class="programlisting">(require '[org.httpkit.server :as hk])

;; handler is a typical Ring handler
(hk/run-server handler {:port 3000 :thread 32 :queue-size 600})</pre></div><p>In the previous snippet, the worker thread pool size is 32 and the max queue length is specified as 600. When not specified, 20480 is the default maximum queue length for applying back pressure.</p></div><div class="section" title="Aleph"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec124"/>Aleph</h2></div></div></div><p>Aleph (<a class="ulink" href="http://aleph.io/">http://aleph.io/</a>) is another <a class="indexterm" id="id668"/>high-performance asynchronous web server based on the Java Netty<a class="indexterm" id="id669"/> (<a class="ulink" href="http://netty.io/">http://netty.io/</a>) library, which in<a class="indexterm" id="id670"/> turn is based on Java NIO. Aleph extends Netty with its own primitives compatible with Netty. The worker thread pool in Aleph is specified via an option, as we can see in the following snippet as of Aleph 0.4.0:</p><div class="informalexample"><pre class="programlisting">(require '[aleph.http :as a])

;; handler is a typical Ring handler
(a/start-server handler {:executor tpool})</pre></div><p>Here, <code class="literal">tpool</code> refers to a bounded thread pool as discussed in the subsection <span class="emphasis"><em>Thread pool queues</em></span>. By default, Aleph uses a dynamic thread pool capped at maximum 512 threads aimed at 90 percent system utilization via the <a class="indexterm" id="id671"/>
<span class="strong"><strong>Dirigiste</strong></span> (<a class="ulink" href="https://github.com/ztellman/dirigiste">https://github.com/ztellman/dirigiste</a>) library.</p><p>Back pressure not only involves enqueuing a limited number of jobs, but slows down the processing rate of a job when the peer is slow. Aleph deals with per-request back pressure (for example, when streaming response data) by "not accepting data until it runs out of memory" — it falls back to blocking instead of dropping data, or raising exceptions and closing connections</p></div></div>
<div class="section" title="Performance and queueing theory"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec62"/>Performance and queueing theory</h1></div></div></div><p>If we observe the performance<a class="indexterm" id="id672"/> benchmark numbers across a number of runs, even though <a class="indexterm" id="id673"/>the hardware, loads and OS remain the same, the numbers are rarely exactly the same. The difference between each run may be as much as -8 percent to 8 percent for no apparent reason. This may seem surprising, but the deep-rooted reason is that the performances of computer systems are <span class="emphasis"><em>stochastic</em></span> by nature. There are many small factors in a computer system that make performance unpredictable at any given point of time. At best, the performance variations can be explained by a series of probabilities over random variables.</p><p>The basic premise is that each subsystem is more or less like a queue where requests await their turn to be served. The CPU has an instruction queue with unpredictable fetch/decode/branch-predict timings, the memory access again depends on cache hit ratio and whether it needs to be dispatched via the interconnect, and the I/O subsystem works using interrupts that may again depend on mechanical factors of the I/O device. The OS schedules threads that wait while not executing. The software built on the top of all this basically waits in various queues to get the job done.</p><div class="section" title="Little's law"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec125"/>Little's law</h2></div></div></div><p>Little's law<a class="indexterm" id="id674"/> states that, over steady state, the following holds true:</p><div class="mediaobject"><img alt="Little's law" src="graphics/3642_08_01.jpg"/></div><div class="mediaobject"><img alt="Little's law" src="graphics/3642_08_02.jpg"/></div><p>This is a rather important law that gives us insight into the system capacity as it is independent of other factors. For an example, if the average time to satisfy a request is 200 ms and the service rate is about 70 per second, then the mean number of requests being served is <span class="emphasis"><em>70 req/second x 0.2 second = 14 requests</em></span>.</p><p>Note that Little's law<a class="indexterm" id="id675"/> does not talk about spikes in request arrival rate or spikes in latency (due to GC and/or other bottlenecks) or system behavior in response to these factors. When the arrival rate spikes at one point, your system must have enough resources to handle the number of concurrent tasks required to serve the requests. We can infer here that Little's law is helpful to measure and tune average system behavior over a duration, but we cannot plan capacity based solely on this.</p><div class="section" title="Performance tuning with respect to Little's law"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec50"/>Performance tuning with respect to Little's law</h3></div></div></div><p>In order to maintain <a class="indexterm" id="id676"/>good throughput, we should strive to maintain an upper <a class="indexterm" id="id677"/>limit on the total number of tasks in the system. Since there can be many kinds of tasks in a system and lot of tasks can happily co-exist in the absence of bottlenecks, a better way to say it is to ensure that the system utilization and bottlenecks remain in limit.</p><p>Often, the arrival rate may not be within the control of a system. For such scenarios, the only option is to minimize the latency as much as possible and deny new requests after a certain threshold of total jobs in the system. You may be able to know the right threshold only through performance and load tests. If you can control the arrival rate, you can throttle the arrival (based on performance and load tests) so as to maintain a steady flow.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec63"/>Summary</h1></div></div></div><p>Designing an application for performance should be based on the use cases and patterns of anticipated system load and behavior. Measuring performance is extremely important to guide optimization in the process. Fortunately, there are several well-known optimization patterns to tap into, such as resource pooling, data sizing, pre-fetch and pre-compute, staging, batching, and so on. As it turns out, application performance is not only a function of the use cases and patterns—the system as a whole is a continuous stochastic turn of events that can be assessed statistically and is guided by probability. Clojure is a fun language to do high-performance programming. This book prescribes many pointers and practices for performance, but there is no mantra that can solve everything. The devil is in the details. Know the idioms and patterns, experiment to see what works for your applications, and know which rules you can bend for performance.</p></div></body></html>