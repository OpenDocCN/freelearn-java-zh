<html><head></head><body>
		<div id="_idContainer074">
			<h1 id="_idParaDest-107"><em class="italic"><a id="_idTextAnchor106"/>Chapter 5</em>: Integrating Microservices Using Event-Driven Architecture</h1>
			<p>The essence of microservice architecture is breaking a monolith down into decoupled or loosely coupled microservices. As a result of such a decomposition into microservices, we separate the user and/or business concerns owned by each microservice. However, for an application as a whole, all the microservices need to work together by interacting with each other in executing and serving the user requests. Event-driven architecture has gained popularity in addressing these inter-microservices interactions. </p>
			<p>In this chapter, we will explore how we can implement an event-driven architecture in the Micronaut framework. We will dive into the following topics:</p>
			<ul>
				<li>Understanding event-driven architecture</li>
				<li>Event streaming with the Apache Kafka ecosystem</li>
				<li>Integrating microservices using event streaming</li>
			</ul>
			<p>By the end of this chapter, readers will have a nifty knowledge of event-driven architecture and how to implement an event-streaming broker to integrate app microservices in the Micronaut framework.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor107"/>Technical requirements</h1>
			<p>All the commands and technical instructions in this chapter are run on Windows 10 and macOS. Code examples covered in this chapter are available in the book's GitHub repository at https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter05.</p>
			<p>The following tools need to be installed and set up in the development environment:</p>
			<ul>
				<li><strong class="bold">Java SDK</strong>: Java SDK version 13 or above (we used Java 14).</li>
				<li><strong class="bold">Maven</strong>: This is optional and only required if you would like to use Maven as the build system. However, we recommend having Maven set up on any development machine. Instructions regarding the downloading and installation of Maven can be found at <a href="https://maven.apache.org/download.cgi">https://maven.apache.org/download.cgi</a>.</li>
				<li><strong class="bold">Development IDE</strong>: Based on your preferences, any Java-based IDE can be used, but for purposes of this chapter, IntelliJ was used. </li>
				<li><strong class="bold">Git</strong>: Instructions regarding the downloading and installation of Git can be found at <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a>.</li>
				<li><strong class="bold">PostgreSQL</strong>: Instructions regarding downloading and installation can be found at <a href="https://www.postgresql.org/download/">https://www.postgresql.org/download/</a>.</li>
				<li><strong class="bold">MongoDB</strong>: MongoDB Atlas provides a free online database-as-a-service up to 512 MB storage. However, if a local database is preferred, then instructions regarding downloading and installation can be found at <a href="https://docs.mongodb.com/manual/administration/install-community/">https://docs.mongodb.com/manual/administration/install-community/</a>. We used a local installation while writing this chapter.</li>
				<li><strong class="bold">REST client</strong>: Any HTTP REST client can be used. We used the <strong class="bold">Advanced REST Client</strong> (<strong class="bold">ARC</strong>) Chrome plugin.</li>
				<li><strong class="bold">Docker</strong>: Instructions regarding the downloading and installation of Docker can be found at https://docs.docker.com/get-docker/.</li>
			</ul>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor108"/>Understanding event-driven architecture</h1>
			<p>Event-driven architecture is<a id="_idIndexMarker391"/> pivotal in connecting different microservices. Before we dive into how to implement an event-driven interaction system, let's understand its fundamentals. </p>
			<p>The following are the key components at the core of any event-driven architecture implementation: </p>
			<ul>
				<li><strong class="bold">Event</strong>: An event is<a id="_idIndexMarker392"/> simply a change in the state of the system that needs to be traced. In a microservice architecture, a microservice may make or detect a change in the data's state that might be worth noticing by other services. This state change is communicated as an event.</li>
				<li><strong class="bold">Event producer</strong>: An event producer is <a id="_idIndexMarker393"/>any microservice or component that is making or detecting a state change and generating an event for other components/services in the system.</li>
				<li><strong class="bold">Event consumer</strong>: An event consumer is <a id="_idIndexMarker394"/>any microservice or component that is consuming an event. Interestingly, this event consumption might trigger this component to produce another event. </li>
				<li><strong class="bold">Event broker</strong>: The event broker acts <a id="_idIndexMarker395"/>as a go-between between all the producer and consumer parties. It maintains a metadata quorum to keep track of events.</li>
			</ul>
			<p>These key components come together to realize an event-driven architecture. Broadly speaking, there are two implementation<a id="_idIndexMarker396"/> strategies – <strong class="bold">pub/sub</strong> (also called event messaging) and <strong class="bold">event streaming</strong>. To learn<a id="_idIndexMarker397"/> more about these strategies, let's dive into the following sections.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor109"/>Event messaging or a pub/sub model in an event-driven architecture</h2>
			<p>A <strong class="bold">pub/sub</strong> model is a <em class="italic">push-based</em> model. In<a id="_idIndexMarker398"/> a push-based model, event publishing is owned by the event producer, and events are pushed<a id="_idIndexMarker399"/> from the producer and sent to consumers. The following are the key components in a pub/sub implementation:</p>
			<ul>
				<li><strong class="bold">Event producer</strong>: Any <a id="_idIndexMarker400"/>component that is making or detecting a state change will generate an event and publish it to the event broker.</li>
				<li><strong class="bold">Event broker</strong>: The event broker <a id="_idIndexMarker401"/>will receive the generated event and push the event to all the required event queues. These event queues are subscribed to by event consumers. Therefore, events are pushed down to the event consumers by the broker.  </li>
				<li><strong class="bold">Event consumer</strong>: The event <a id="_idIndexMarker402"/>consumer will receive the event and do what is required. It may also generate a new event(s). <p>This is depicted in the following diagram:</p></li>
			</ul>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/Figure_5.1_B16585_Fixed.jpg" alt="Figure 5.1 – Pub/sub model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Pub/sub model</p>
			<p>As shown in the preceding diagram, when <strong class="bold">Event Producer 1</strong> generates the <strong class="bold">Event Foo</strong>, it is pushed to the <strong class="bold">Event Broker</strong>. The <strong class="bold">Event Broker</strong> further pushes this event to <strong class="bold">Event Consumer 1</strong> and <strong class="bold">Event Consumer 2</strong>. </p>
			<p>So, overall, for <strong class="bold">Event Bar</strong> (which is <a id="_idIndexMarker403"/>generated by <strong class="bold">Event Producer n</strong>), <strong class="bold">Event Broker</strong> pushes it to <strong class="bold">Event Consumer k</strong>.</p>
			<p>In a pub/sub model, once<a id="_idIndexMarker404"/> the event is produced and communicated to the consumer via the event broker, the event consumer<a id="_idIndexMarker405"/> must do the necessary immediately, as once an event is consumed, it perishes. The event consumer can never go back to historic events. This model is also sometimes referred to as the <strong class="bold">event messaging model</strong>. </p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>Event streaming in event-driven architecture</h2>
			<p>An <strong class="bold">event streaming</strong> model is a <em class="italic">pull-based</em> model. In a pull-based <a id="_idIndexMarker406"/>model, the onus lies on the event consumer to fetch the event. In an <a id="_idIndexMarker407"/>event streaming implementation, the key components will act as follows:</p>
			<ul>
				<li><strong class="bold">Event producer</strong>: Any<a id="_idIndexMarker408"/> component that is making or detecting a state change will generate the event and send it to the event broker.</li>
				<li><strong class="bold">Event broker</strong>: The event <a id="_idIndexMarker409"/>broker will receive the generated event and broadcast the event by putting the event in an event stream.</li>
				<li><strong class="bold">Event consumer</strong>: The event <a id="_idIndexMarker410"/>consumer continuously monitors one or more event streams on the event broker. When a <a id="_idIndexMarker411"/>new event is pushed to the event stream, the consumer fetches the event and does what is required.<p>Refer to the following diagram:</p></li>
			</ul>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/Figure_5.2_B16585_Fixed.jpg" alt="Figure 5.2 – Event streaming model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Event streaming model</p>
			<p>As shown in the preceding diagram, when <strong class="bold">Event Producer 1</strong> generates the <strong class="bold">Event Foo</strong>, it pushes the event to <strong class="bold">Event Broker</strong> and <strong class="bold">Event Broker</strong> puts it in the <strong class="bold">Foo Stream</strong>. <strong class="bold">Event Consumer 1</strong> and <strong class="bold">Event Consumer 2</strong> fetch the event from the <strong class="bold">Foo Stream</strong>. <strong class="bold">Event Bar</strong> is fetched from <strong class="bold">Bar Stream</strong> by <strong class="bold">Event Consumer k</strong>.</p>
			<p>In an event-streaming <a id="_idIndexMarker412"/>model, as event consumers fetch the data from an event stream, they can fetch any offset of the event stream. This even enables event consumers to access historic events. It especially comes in handy if you have a new consumer added to the system that might not be in touch with the recent state of the system and may start processing historic events first. For these reasons, event streaming is usually preferred over event messaging. </p>
			<p>In the next section, we will get started with hands-on event streaming using a popular event-streaming stack.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor111"/>Event streaming with the Apache Kafka ecosystem </h1>
			<p>Apache Kafka is an industry-leading <a id="_idIndexMarker413"/>event streaming system. In the Apache Kafka ecosystem, the following are some of the key components:</p>
			<ul>
				<li><strong class="bold">Event topic</strong>: An event topic<a id="_idIndexMarker414"/> consists of a stream of immutable, ordered messages belonging to a particular category. Each event topic may <a id="_idIndexMarker415"/>have one or more partitions. A partition is indexed storage that supports multi-concurrency in Apache. Apache Kafka keeps at least one partition per topic and may add more partitions as specified (at the time of topic creation) or required. When a new message is published to the topic, Apache Kafka decides which topic partition will be used to append the message. Each topic appends the most recent message at the end. This is shown in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/Figure_5.3_B16585.jpg" alt="Figure 5.3 – Apache Kafka topic anatomy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Apache Kafka topic anatomy</p>
			<p>As shown in the preceding diagram, when a new message is published to the steam, it is <a id="_idIndexMarker416"/>appended at the end. Event consumers can freely choose which topic offset to read. While <strong class="bold">Consumer 1</strong> reads at the <a id="_idIndexMarker417"/>first offset, <strong class="bold">Consumer 2</strong> reads<a id="_idIndexMarker418"/> from the sixth offset.</p>
			<ul>
				<li><strong class="bold">Event broker</strong>: An event broker<a id="_idIndexMarker419"/> is a façade that provides an interface to write or read from event topic(s). Apache Kafka usually has the leader and follower brokers. The leader broker (for a topic) will serve all the write requests. If a leader broker fails, then the follower broker chimes in as leader.</li>
				<li><strong class="bold">Kafka cluster</strong>: If Apache <a id="_idIndexMarker420"/>Kafka has a quorum consisting of more than one event broker, then it's called a cluster. In a cluster, each broker will usually lead a distinct topic and may substitute as a follower for other topics.</li>
				<li><strong class="bold">Event producer</strong>: Producers publish an event message to the topic stream. A producer interacts with the Apache Kafka ecosystem to know which broker should be used for writing an event<a id="_idIndexMarker421"/> message to a topic stream. If a broker fails or a new broker is added, Apache Kafka notifies the required producers of this.</li>
				<li><strong class="bold">Event consumer</strong>: A consumer reads the event messages from the topic stream. An event consumer will interact <a id="_idIndexMarker422"/>with the Apache Kafka ecosystem to know which broker should be used to read from a topic stream. Furthermore, Apache Kafka keeps track of the topic <a id="_idIndexMarker423"/>offset for each event consumer to resume event consumption properly.</li>
				<li><strong class="bold">Zookeeper</strong>: Zookeeper maintains the metadata quorum for the Apache Kafka ecosystem. It essentially maintains <a id="_idIndexMarker424"/>information about all brokers for event producers and consumers. It also keeps track of the topic <a id="_idIndexMarker425"/>offset for each event consumer.</li>
			</ul>
			<p>In the following diagram, we can see the various components in the Apache Kafka ecosystem and how they interact with each other in event streaming:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/Figure_5.4_B16585.jpg" alt="Figure 5.4 – Apache Kafka ecosystem&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Apache Kafka ecosystem</p>
			<p>In the preceding diagram, the <a id="_idIndexMarker426"/>Apache Kafka ecosystem is shown at a glance. For each<a id="_idIndexMarker427"/> event topic, there will be at least one leader event broker and one or more follower event brokers. Information about leaders and followers is maintained in Zookeeper. When an event producer pushes a message, a broker will write the message to the required topic stream. Similarly, when an event consumer pulls a message, an event broker will fetch the message from the required topic stream. Zookeeper maintains the offset information for each topic for all the event consumers.</p>
			<p>In the next section, we will dive into how to use the Apache Kafka ecosystem for event streaming in the pet clinic application.</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Integrating microservices using event streaming</h1>
			<p>To learn and perform <a id="_idIndexMarker428"/>hands-on exercises, we will implement a simple scenario of event streaming in the pet clinic application. Consider<a id="_idIndexMarker429"/> the following diagram:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/Figure_5.5_B16585_Fixed.jpg" alt="Figure 5.5 – Event streaming in the pet clinic application&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Event streaming in the pet clinic application</p>
			<p>In the aforementioned diagram, whenever there is a new vet review, the <strong class="bold">pet-clinic-reviews</strong> microservice will send the review to <strong class="bold">Apache Kafka Streaming</strong>. Apache Kafka appends the review to the <strong class="bold">vet-reviews</strong> topic stream. And, as the <strong class="bold">pet-clinic</strong> microservice is <a id="_idIndexMarker430"/>continuously monitoring the <strong class="bold">vet-reviews</strong> topic stream, it will fetch any new reviews appended to<a id="_idIndexMarker431"/> the topic and update the average rating accordingly. This is a simpleton diagram but will help to focus on the key learning objectives.</p>
			<p>In the next section, we will begin by setting up the Apache Kafka ecosystem locally inside Docker to learn more about Apache Kafka streaming. </p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor113"/>Setting up the Apache Kafka ecosystem locally</h2>
			<p>To set up the<a id="_idIndexMarker432"/> Apache Kafka ecosystem locally, we will use Docker. The <strong class="source-inline">docker-compose</strong> file for all the required components and configurations can be found in the chapter's GitHub workspace under <strong class="source-inline">resources</strong>: <a href="https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/blob/master/Chapter05/micronaut-petclinic/pet-clinic-reviews/src/main/resources/kafka-zookeeper-kafdrop-docker/docker-compose.yml">https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/blob/master/Chapter05/micronaut-petclinic/pet-clinic-reviews/src/main/resources/kafka-zookeeper-kafdrop-docker/docker-compose.yml</a>.</p>
			<p>Perform the following steps to install and set up Apache Kafka:</p>
			<ol>
				<li>Download the <strong class="source-inline">docker-compose</strong> file from the aforementioned URL.</li>
				<li>Open the GitBash terminal.</li>
				<li>Change the directory to where you have placed the <strong class="source-inline">docker-compose</strong> file.</li>
				<li>Run the <strong class="source-inline">docker-compose up</strong> command in the GitBash terminal.</li>
			</ol>
			<p>As a result of following these instructions, Docker will install Zookeeper, Apache Kafka, and Kafdrop. Kafdrop is an intuitive admin GUI for managing Apache Kafka. In the following section, we will verify their installation.</p>
			<h3>Testing the Apache Kafka ecosystem setup</h3>
			<p>To test whether the<a id="_idIndexMarker433"/> Apache Kafka ecosystem is installed successfully, perform the following steps:</p>
			<ol>
				<li value="1">Open the GitBash terminal and run the following command:<p class="source-code"><strong class="bold">winpty docker exec -it kafka bash</strong></p></li>
				<li>Change the directory to <strong class="bold">opt/bitnami/kafka/bin/</strong>.</li>
				<li>Add a topic stream by running the following in the GitBash terminal:<p class="source-code"><strong class="bold">command ./kafka-topics.sh --bootstrap-server kafka:9092 --create --partitions 3 --replication-factor 1 --topic foo-stream</strong></p></li>
				<li>To add a message to the topic, run the following in the GitBash terminal: <p class="source-code"><strong class="bold">command ./kafka-console-producer.sh --broker-list kafka:9092 --topic foo-stream</strong></p></li>
				<li>A terminal prompt will appear, type <strong class="source-inline">hello-world!</strong>, and then hit <em class="italic">Enter</em>.</li>
				<li>Press <em class="italic">Ctrl</em> + <em class="italic">D</em>, which should successfully add the event to the topic. </li>
			</ol>
			<p>By following these instructions, we added a <strong class="source-inline">foo-stream</strong> topic and added a message to this topic. To see this topic stream, we can open Kafdrop by opening <strong class="source-inline">http://localhost:9100/</strong> in a browser window. Refer to the following screenshot:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/Figure_5.6_B16585_Fixed.jpg" alt="Figure 5.6 – Kafdrop showing foo-stream topic messages&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Kafdrop showing foo-stream topic messages</p>
			<p>Kafdrop provides an<a id="_idIndexMarker434"/> intuitive GUI for viewing and managing all the Apache Kafka streams. In the previous screenshot, we can see the messages inside the just created <strong class="bold">foo-stream</strong>. </p>
			<p>Hitherto, we set up the Apache Kafka ecosystem locally in a Dockerized environment, and in the next section, we will use this setup for hands-on event streaming in the <strong class="source-inline">pet-clinic-reviews</strong> and <strong class="source-inline">pet-clinic</strong> microservices. We will begin by making the required changes in the <strong class="source-inline">pet-clinic-reviews</strong> microservice.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>Implementing an event-producer client in the pet-clinic-reviews microservice</h2>
			<p>We will begin making the <a id="_idIndexMarker435"/>required changes to the <strong class="source-inline">pet-clinic-reviews</strong> microservices so that it can stream out the vet reviews <a id="_idIndexMarker436"/>to Apache Kafka. For this hands-on exercise, we will keep things simple. Therefore, we will skip the security setup and resume the code base from <a href="B16585_03_Final_VK_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 3</em></a>, <em class="italic">Working on the RESTful Web Services</em>. </p>
			<p>Perform the following steps to see how this goes:</p>
			<ol>
				<li value="1">To begin, we will need to add a Kafka dependency to the <strong class="source-inline">pom.xml</strong> project:<p class="source-code">&lt;!-- Kafka --&gt;</p><p class="source-code">    &lt;dependency&gt;</p><p class="source-code">      &lt;groupId&gt;io.micronaut.kafka&lt;/groupId&gt;</p><p class="source-code">      &lt;artifactId&gt;micronaut-kafka&lt;/artifactId&gt;</p><p class="source-code">    &lt;/dependency&gt;</p><p class="source-code">…</p><p>By importing the <strong class="source-inline">micronaut-kafka</strong> dependency, we can leverage the Kafka toolkit in the <strong class="source-inline">pet-clinic-reviews</strong> microservice. </p></li>
				<li>Once the dependency has <a id="_idIndexMarker437"/>been imported, we will then need to configure <strong class="source-inline">application.properties</strong> as follows:<p class="source-code">micronaut:</p><p class="source-code">  application:</p><p class="source-code">    name: PetClinicReviews</p><p class="source-code">  server:</p><p class="source-code">    port: 8083</p><p class="source-code">kafka:</p><p class="source-code">  bootstrap:</p><p class="source-code">    servers: localhost:9094</p><p>As mentioned in<a id="_idIndexMarker438"/> preceding <strong class="source-inline">application.properties</strong>, we will fix port <strong class="source-inline">8083</strong> for the <strong class="source-inline">pet-clinic-reviews</strong> microservice and configure the Kafka connection by providing Bootstrap server details.</p></li>
				<li>Next, we will create a Kafka client in the <strong class="source-inline">pet-clinic-reviews</strong> microservice, which can send messages to the <strong class="source-inline">vet-reviews</strong> topic. Begin by creating a package, <strong class="source-inline">com.packtpub.micronaut.integration.client</strong>. This package will contain the required client and, in the future, may contain more artifacts related to service integration. We now add <strong class="source-inline">VetReviewClient</strong> to this package:<p class="source-code">@KafkaClient</p><p class="source-code">public interface VetReviewClient {</p><p class="source-code">    @Topic("vet-reviews")</p><p class="source-code">    void send(@Body VetReviewDTO vetReview);</p><p class="source-code">}</p><p><strong class="source-inline">VetReviewClient</strong> is annotated with <strong class="source-inline">@KafkaClient</strong>. Using the <strong class="source-inline">@KafkaClient</strong> annotation, we can inject <strong class="source-inline">VetReviewClient</strong> as a Kafka client. Furthermore, just by simply using <strong class="source-inline">@Topic("vet-reviews")</strong>, we can send the messages (no need to even create the topic) to the <strong class="source-inline">vet-reviews</strong> topic stream. </p></li>
			</ol>
			<p>Hitherto, we have configured <a id="_idIndexMarker439"/>application <a id="_idIndexMarker440"/>properties and created a simple Kafka client. In the following code, we will make changes to <strong class="source-inline">createVetReview()</strong> in <strong class="source-inline">VetReviewResource</strong> so it can send messages to the topic stream when a new vet review is posted:</p>
			<p class="source-code">@Post("/vet-reviews")</p>
			<p class="source-code">@ExecuteOn(TaskExecutors.IO)</p>
			<p class="source-code">public HttpResponse&lt;VetReviewDTO&gt; createVetReview(@Body VetReviewDTO vetReviewDTO) throws URISyntaxException {</p>
			<p class="source-code">    log.debug("REST request to save VetReview : {}", vetReviewDTO);</p>
			<p class="source-code">    if (vetReviewDTO.getReviewId() != null) {</p>
			<p class="source-code">        throw new BadRequestAlertException("A new vetReview cannot already have an ID", ENTITY_NAME, "idexists");</p>
			<p class="source-code">    }</p>
			<p class="source-code">    VetReviewDTO result = vetReviewService.save(vetReviewDTO);</p>
			<p class="source-code">    /** Stream to other services */</p>
			<p class="source-code">    vetReviewClient.send(result);</p>
			<p class="source-code">    URI location = new URI("/api/vet-reviews/" + result.getReviewId());</p>
			<p class="source-code">    return HttpResponse.created(result).headers(headers -&gt; {</p>
			<p class="source-code">        headers.location(location);</p>
			<p class="source-code">        HeaderUtil.createEntityCreationAlert(headers, applicationName, true, ENTITY_NAME, result.getReviewId());</p>
			<p class="source-code">    });</p>
			<p class="source-code">}</p>
			<p>From the preceding <a id="_idIndexMarker441"/>code, we<a id="_idIndexMarker442"/> can see that we can simply inject <strong class="source-inline">VetReviewClient</strong> into <strong class="source-inline">VetReviewResource</strong>. In <strong class="source-inline">createVetReview()</strong>, when a vet review is successfully inserted, we can send the message to the <strong class="source-inline">vet-reviews</strong> stream using <strong class="source-inline">VetReviewClient</strong>. </p>
			<p>In this section, we introduced the <a id="_idIndexMarker443"/>event producer<a id="_idIndexMarker444"/> in the <strong class="source-inline">pet-clinic-reviews</strong> microservice. In the following section, we will verify this event producer by invoking the HTTP <strong class="source-inline">POST</strong> endpoint to create a new vet review.</p>
			<h3>Testing the event producer in the pet-clinic-reviews microservice</h3>
			<p>To test the event producer that<a id="_idIndexMarker445"/> has just been created, boot up the <strong class="source-inline">pet-clinic-reviews</strong> microservice locally and access<a id="_idIndexMarker446"/> the HTTP <strong class="source-inline">POST</strong> endpoint. In the following screenshot, we are using a REST client to invoke the <strong class="source-inline">vet-reviews</strong> HTTP <strong class="source-inline">POST</strong> endpoint to create a vet review: </p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/Figure_5.7_B16585_Fixed.jpg" alt="Figure 5.7 – Creating a vet review for testing the event producer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Creating a vet review for testing the event producer</p>
			<p>As shown in the preceding <a id="_idIndexMarker447"/>screenshot, when we<a id="_idIndexMarker448"/> submit a request to create a new vet review, it will persist the vet review and also stream out the review to Apache Kafka. This event message can be verified by accessing Kafdrop at <strong class="source-inline">http://localhost:9100/</strong>. This is what the screen outputs:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/Figure_5.8_B16585_Fixed.jpg" alt="Figure 5.8 – A newly added review to the vet-reviews stream&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – A newly added review to the vet-reviews stream</p>
			<p>As viewed in Kafdrop, we can verify that the event from the <strong class="source-inline">pet-clinic-reviews</strong> microservice is streamed out<a id="_idIndexMarker449"/> and added to the <strong class="source-inline">vet-reviews</strong> topic.</p>
			<p>In this section, we verified the <a id="_idIndexMarker450"/>event producer in the <strong class="source-inline">pet-clinic-reviews</strong> microservice. In the following section, we will explore how to implement an event consumer in the Micronaut framework. </p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Implementing an event consumer client in the pet-clinic microservice</h2>
			<p>In this section, we<a id="_idIndexMarker451"/> will <a id="_idIndexMarker452"/>implement an event consumer in the <strong class="source-inline">pet-clinic</strong> microservice so that it can consume messages streamed in the <strong class="source-inline">vet-reviews</strong> topic. </p>
			<p>To begin, we will<a id="_idIndexMarker453"/> need to add a Kafka dependency to the <strong class="source-inline">pom.xml</strong> project. This is shown with the following code:</p>
			<p class="source-code">&lt;!-- Kafka --&gt;</p>
			<p class="source-code">    &lt;dependency&gt;</p>
			<p class="source-code">      &lt;groupId&gt;io.micronaut.kafka&lt;/groupId&gt;</p>
			<p class="source-code">      &lt;artifactId&gt;micronaut-kafka&lt;/artifactId&gt;</p>
			<p class="source-code">    &lt;/dependency&gt;</p>
			<p class="source-code">…</p>
			<p>Importing <strong class="source-inline">micronaut-kafka</strong> will enable us to leverage the Kafka consumer toolkit. Once the dependency has been imported, we will then need to configure <strong class="source-inline">application.properties</strong> as follows: </p>
			<p class="source-code">micronaut:</p>
			<p class="source-code">  application:</p>
			<p class="source-code">    name: Pet-Clinic</p>
			<p class="source-code">  server:</p>
			<p class="source-code">    port: 8082</p>
			<p class="source-code">kafka:</p>
			<p class="source-code">  bootstrap:</p>
			<p class="source-code">    servers: localhost:9094</p>
			<p>As mentioned in the preceding code, we will fix port <strong class="source-inline">8082</strong> for the <strong class="source-inline">pet-clinic</strong> microservice and configure the<a id="_idIndexMarker454"/> Kafka connection<a id="_idIndexMarker455"/> by providing Bootstrap server details.</p>
			<p>Next, to contain all the Kafka integration artifacts, we will create a <strong class="source-inline">com.packtpub.micronaut.integration</strong> package. Since we will be consuming from the <strong class="source-inline">vet-reviews</strong> topic stream, we will add <strong class="source-inline">VetReviewDTO</strong> to the <strong class="source-inline">com.packtpub.micronaut.integration.domain</strong> package. </p>
			<p>Some developers advocate keeping the DTOs in a shared repository that can be re-used in all microservices. However, keeping all the DTOs under an owning microservice is good for better encapsulation. Furthermore, there could be cases where a DTO such as <strong class="source-inline">VetReviewDTO</strong> could assume<a id="_idIndexMarker456"/> the desired object <a id="_idIndexMarker457"/>definition in one microservice and a different one in another microservice. </p>
			<p>We will create a Kafka listener in the <strong class="source-inline">com.packtpub.micronaut.integration.client</strong> package to leverage the <strong class="source-inline">micronaut-kafka</strong> toolkit. Refer to the following code block:</p>
			<p class="source-code">@KafkaListener(groupId = "pet-clinic")</p>
			<p class="source-code">public class VetReviewListener {</p>
			<p class="source-code">    private static final Logger log =  LoggerFactory.getLogger(VetReviewListener.class);</p>
			<p class="source-code">    private final VetService vetService;</p>
			<p class="source-code">    public VetReviewListener(VetService vetService) {</p>
			<p class="source-code">        this.vetService = vetService;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    @Topic("vet-reviews")</p>
			<p class="source-code">    public void receive(@Body VetReviewDTO vetReview) {</p>
			<p class="source-code">        log.info("Received: vetReview -&gt; {}", vetReview);</p>
			<p class="source-code">        try {</p>
			<p class="source-code">            vetService.updateVetAverageRating(vetReview.getVetId(), vetReview.getRating());</p>
			<p class="source-code">        } catch (Exception e) {</p>
			<p class="source-code">            log.error("Exception occurred: {}", e.toString());</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>From the preceding code, we see that we created the <strong class="source-inline">VetReviewListener</strong> using the <strong class="source-inline">@KafkaListener</strong> annotation. In the <strong class="source-inline">@KafkaListener</strong> annotation, we passed <strong class="source-inline">groupId</strong>. Assigning a group ID to a Kafka listener adds the listener to a consumer group. This may be required when<a id="_idIndexMarker458"/> there are<a id="_idIndexMarker459"/> multiple consumer services for a topic stream so that the Kafka ecosystem can maintain an isolated offset for each consumer. Using <strong class="source-inline">@Topic("vet-reviews")</strong> allows <strong class="source-inline">VetReviewListener</strong> to receive any streamed out messages from the <strong class="source-inline">vet-reviews</strong> stream. When <strong class="source-inline">VetReviewListener</strong> receives any message, it invokes <strong class="source-inline">updateVetAverageRating()</strong> in <strong class="source-inline">VetService</strong>. In the following code snippet, we added this method in <strong class="source-inline">VetService</strong> to update the average rating for a vet when a new review is added to the <strong class="source-inline">pet-clinic-reviews</strong> microservice:</p>
			<p class="source-code">public void updateVetAverageRating(Long id, Double rating) throws Exception {</p>
			<p class="source-code">    log.debug("Request to update vet rating, id: {}, rating: {}", id, rating);</p>
			<p class="source-code">    Optional&lt;VetDTO&gt; oVetDTO = findOne(id);</p>
			<p class="source-code">    if (oVetDTO.isPresent()) {</p>
			<p class="source-code">        VetDTO vetDTO = oVetDTO.get();</p>
			<p class="source-code">        Double averageRating = vetDTO.getAverageRating() != null ? vetDTO.getAverageRating() : 0D;</p>
			<p class="source-code">        Long ratingCount = vetDTO.getRatingCount() != null ? vetDTO.getRatingCount() : 0L;</p>
			<p class="source-code">        Double newAvgRating = ((averageRating * ratingCount) + rating) / (ratingCount + 1);</p>
			<p class="source-code">        Long newRatingCount = ratingCount + 1;</p>
			<p class="source-code">        vetRepository.updateVetAverageRating(id, newAvgRating, newRatingCount);</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>From the preceding code, we <a id="_idIndexMarker460"/>see that the <strong class="source-inline">updateVetAverageRating()</strong> method retrieves the last stored rating. If the<a id="_idIndexMarker461"/> last stored rating is <strong class="source-inline">null</strong>, it assumes it to be <strong class="source-inline">0</strong>. In any case, it will add on the new rating and determine a new average rating. Once the average rating has been determined, rating information is persisted in the database by making a call to the repository.</p>
			<p>In this section, we explored how we can implement an event consumer in the <strong class="source-inline">pet-clinic</strong> microservice. In the<a id="_idIndexMarker462"/> following<a id="_idIndexMarker463"/> section, we will verify this event consumer by creating a new vet review.</p>
			<h3>Testing the event consumer in the pet-clinic microservice</h3>
			<p>To test the event <a id="_idIndexMarker464"/>consumer that has just been created, we can boot up the <strong class="source-inline">pet-clinic</strong> (event consumer) and <strong class="source-inline">pet-clinic-reviews</strong> (event producer) microser<a id="_idTextAnchor116"/>vices. Once the <strong class="source-inline">pet-clinic-reviews</strong> microservice is running, add a <a id="_idIndexMarker465"/>new vet review. In the following screenshot, you can see that we are using an HTTP REST client to post a vet review:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/Figure_5.9_B16585_Fixed.jpg" alt="Figure 5.9 – Adding a new vet review to test the event consumer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Adding a new vet review to test the event consumer</p>
			<p>In the <strong class="source-inline">POST</strong> request to the <strong class="source-inline">vet-reviews</strong> resource, we are adding an abysmal rating. The <strong class="source-inline">pet-clinic-reviews</strong> microservice successfully executed the request and responded with an <strong class="source-inline">HTTP 201</strong> response, assigning a review ID to the review submitted. </p>
			<p>As shown in the following screenshot, in the <strong class="source-inline">pet-clinic</strong> microservice, if we put a debug point in <strong class="source-inline">VetReviewListener</strong>, we<a id="_idIndexMarker466"/> can verify that the <a id="_idIndexMarker467"/>Kafka topic stream is sending out the message for a new vet review: </p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/Figure_5.10_B16585_Fixed_edited.jpg" alt="Figure 5.10 – Event message received by the event consumer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – Event message received by the event consumer</p>
			<p>As shown in the preceding screenshot, when the <strong class="source-inline">pet-clinic-reviews</strong> microservice produces an event message, it is received by the <strong class="source-inline">pet-clinic</strong> microservice. This is the magic that integrates these two microservices using event-driven architecture. And this pattern can be extended to integrate microservices in a variety of different scenarios, such as a <a id="_idIndexMarker468"/>service sending out a message to multiple microservices or chained event messages, or choreographing complex microservice integrations.</p>
			<p>In this section, we verified the <a id="_idIndexMarker469"/>event consumer in the <strong class="source-inline">pet-clinic</strong> microservice such that when a new vet review is added to <strong class="source-inline">pet-clinic-reviews</strong>, <strong class="source-inline">pet-clinic</strong> receives the review information from the <strong class="source-inline">vet-reviews</strong> topic stream.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor117"/>Summary</h1>
			<p>In this chapter, we started things off with some fundamentals of event-driven architecture, discussing two different kinds of event publishing models, which are pub/sub and event streaming. We discussed the core components of each model, as well as the pros/cons of using each model. </p>
			<p>Since event streaming was better suited for the pet-clinic application, we dived into event streaming using the Apache Kafka ecosystem. For hands-on exercises, we integrated the <strong class="source-inline">pet-clinic-reviews</strong> and the <strong class="source-inline">pet-clinic</strong> microservices using an Apache Kafka topic stream. We verified the integration by creating a new vet review and received the rating in the <strong class="source-inline">pet-clinic</strong> microservice to update the average rating for a vet.</p>
			<p>This chapter has provided you with a solid understanding of event-driven architecture and a practical skillset in implementing an event-streaming system in the Micronaut framework.</p>
			<p>In the next chapter, we will explore how we can automate quality testing using built-in as well as third-party tools in the Micronaut framework.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor118"/>Questions</h1>
			<ol>
				<li value="1">What is event-driven architecture? </li>
				<li>What is the pub/sub model in event-driven architecture? </li>
				<li>What is event streaming? </li>
				<li>Describe the various components that make up the Apache Kafka ecosystem. </li>
				<li>How is the Apache ecosystem set up in Docker?</li>
				<li>How are microservices integrated in the Micronaut framework using event streaming?</li>
				<li>How is an event consumer implemented in the Micronaut framework?</li>
				<li>How is an event producer implemented in the Micronaut framework?</li>
			</ol>
		</div>
	</body></html>