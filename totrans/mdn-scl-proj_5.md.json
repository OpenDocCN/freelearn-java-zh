["```java\n2 * precision recall / precision + recall\n```", "```java\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n```", "```java\ntrait FraudDetectionWrapper {\n```", "```java\nval trainSetFileName = \"training.csv\"\n\n```", "```java\nlazy val session: SparkSession = {\n SparkSession\n .builder()\n .master(\"local\")\n .appName(\"fraud-detection-pipeline\")\n .getOrCreate()\n }\n\n```", "```java\nLogger.getLogger(\"org\").setLevel(Level.OFF)\nLogger.getLogger(\"akka\").setLevel(Level.OFF)\n\n```", "```java\nval dataSetPath = \"C:\\\\Users\\\\Ilango\\\\Documents\\\\Packt\\\\DevProjects\\\\Chapter5A\\\\\"\n\n```", "```java\nval fdFeatures_IndexedLabel_Train = (\"fd-features-vectors\",\"label\")\nval fdFeatures_IndexedLabel_CV = (\"fd-features-vectors\",\"label\")\n\n```", "```java\ndef buildTestVectors(trainPath: String): DataFrame= {\n def analyzeFeatureMeasurements: Array[(org.apache.spark.ml.linalg.Vector,String)] = {\n val featureVectors = session.sparkContext.textFile(trainPath, 2)\n .flatMap { featureLine => featureLine.split(\"\\n\").toList }\n .map(_.split(\",\")).collect.map(featureLine => ( Vectors.dense( featureLine(0).toDouble,featureLine(1).toDouble),featureLine(2)))\n featureVectors\n }\n\n```", "```java\nval fdDataFrame = session.createDataFrame(analyzeFeatureMeasurements).toDF(fdFeatures_IndexedLabel_CV._1, fdFeatures_IndexedLabel_CV._2)\n\n```", "```java\npackage com.packt.modern.chapter5\n```", "```java\nimport org.apache.spark.ml.linalg.DenseVector\nimport org.apache.spark.sql.{DataFrame, Dataset, Row}\nimport org.apache.spark.rdd.RDD\n```", "```java\nobject FraudDetectionPipeline extends App with FraudDetectionWrapper {\n```", "```java\ndef main(args: Array[String]): Unit = {\n```", "```java\nval trainSetForEda: DataFrame = session.read .format(\"com.databricks.spark.csv\") .option(\"header\", false).option(\"inferSchema\", \"true\") .load(dataSetPath + trainSetFileName)\n```", "```java\ncachedTrainSet.show()\n```", "```java\nval testingSet: DataFrame = buildTestVectors(dataSetPath + crossValidFileName)\n```", "```java\n trainSetEdaStats.show()\n```", "```java\n val trainSetEdaStats: DataFrame = cachedTrainSet.summary()\n```", "```java\ntrainSetEdaStats.show()\n```", "```java\nval meanDf: DataFrame = trainSetEdaStats.where(\"summary == 'mean'\")\n```", "```java\nmeanDf.show()\n```", "```java\nval meanDfPairs: Array[(String, String)] = meanDf.collect().map(row => (row.getString(1), row.getString(2)))\n\n```", "```java\nval transactionMean = meanDfPairs(0)._1.toDouble\nval distanceMean = meanDfPairs(0)._2.toDouble\n```", "```java\nval trainSetSdDf: DataFrame = trainSetEdaStats.where(\"summary == 'stddev' \")\n\n```", "```java\ntrainSetSdDf.show()\n```", "```java\nval sdDfPairs: Array[(String, String)] = trainSetSdDf.collect().map(row => (row.getString(1), row.getString(2)))\n```", "```java\nval transactionSD = sdDfPairs(0)._1.toDouble\n```", "```java\nval distanceSD = sdDfPairs(0)._2.toDouble\n```", "```java\nval meanSdTupleOfTuples = ( (transactionMean,distanceMean),(transactionSD, distanceSD) )\n```", "```java\nval meansVector = new DenseVector(Array(meanSdTupleOfTuples._1._1, meanSdTupleOfTuples._1._2))\n```", "```java\nprintln(\"Transaction Mean and Distance Mean Vector looks like this: \" + meansVector.toArray.mkString(\" \"))\n```", "```java\nval sdVector: DenseVector = new DenseVector(Array\n(meanSdTupleOfTuples._2._1, meanSdTupleOfTuples._2._2))\n```", "```java\nprintln(\"Distance Mean and Distance SD Vector looks like this: \" + sdVector.toArray.mkString(\" \"))\n```", "```java\nval broadcastVariable = session.sparkContext.broadcast((meansVector, sdVector))\n```", "```java\n +-------+-----------------+-----------------+\n |summary| _c0| _c1|\n +-------+-----------------+-----------------+\n | mean|97.37915046250084|6.127270023033664|\n+-------+-----------------+------------------+\n |summary| _c0| _c1|\n +-------+-----------------+------------------+\n | stddev|10.83679761471887|3.2438494882693900|\n +-------+-----------------+------------------+\n```", "```java\nimport session.implicits._\n```", "```java\nval fdProbabilityDensities: DataFrame = testingDframe.map(labelledFeatureVectorRow => probabilityDensity( labelledFeatureVectorRow.getAs(0) /* Vector containing 2 Doubles*/ , broadcastVariable.value) ).toDF(\"PDF\")\n```", "```java\nfdProbabilityDensities.show()\n```", "```java\ndef probabilityDensity(labelledFeaturesVector: Vector, broadcastVariableStatsVectorPair: (Vector / Transactions /, Vector / Distance / )): Double = {\n}\n```", "```java\ndef featureDoubles(features: Array[Double],\n                    transactionSdMeanStats: Array[Double],                  \ndistanceSdMeanStats: Array[Double]): List[(Double, Double, Double)] = {    }\n```", "```java\n(93.47397393,79.98437516250003,18.879)   (6.075334279,5.13..,1.9488924384002693)\n```", "```java\n(features, transactionSdMeanStats, distanceSdMeanStats).zipped.toList\n```", "```java\ndef featureDoubles(features: ...., \n                  transactionSdMeanStats: ..., \n                  distanceSdMeanStats: ...): List[(Double, Double, Double)] = {\n  (features, transactionSdMeanStats, distanceSdMeanStats).zipped.toList)\n\n}\n```", "```java\nval pDfCalculator: List[(Double, Double, Double)] = featureDoubles( \n labelledFeaturesVector.toArray, \n broadcastVariableStatsVectorPair._1.toArray, \n broadcastVariableStatsVectorPair._2.toArray)\n```", "```java\nval probabilityDensityValue: Double = pDfCalculator.map(pDf => new NormalDistribution(pDf._2,pDf._3).density(pDf._1)).product\n```", "```java\ndef probabilityDensity2(labelledFeaturesVector: ----,            broadcastVariableStatsVectorPair: (----,----)): Double = {\n\n  def featureDoubles(features: -----, \n                    transactionSdMeanStats: ----, \n                    distanceSdMeanStats: -----): List[(Double, Double, Double)] = {\n\n A tuple converted to a  List[(Double, Double, Double)]\n(Feature Vector, Mean and Standard Deviation of Transaction, Mean and Standard Deviation of Distance)  \n\n}\n```", "```java\nprivate def errorTermCalc(testingDframe: DataFrame, probabilityDensities: DataFrame/*Dataset[Double] */) = { }\n```", "```java\nval maxMinArray: Array[Double] = probabilityDensities.collect().map(pbRow => pbRow.getDouble(0) )\n```", "```java\nval stepsize = (maxMinPair._1 - maxMinPair._2) / 1000.0\n```", "```java\nfor (errorTerm <- maxMinPair._2 to maxMinPair._1 by stepsize) {\n```", "```java\nval broadCastedErrorTerm:Broadcast[Double] = session.sparkContext.broadcast(errorTerm)\n\nval broadcastTerm: Double = broadCastedErrorTerm.value\n```", "```java\n import session.implicits._\n```", "```java\n val finalPreds: DataFrame= probabilityDensities.map { probRow =>\n if (probRow.getDouble(0) < broadcastTerm) {\n 1.0 /* Fraud is flagged here */\n } else 0.0\n }.toDF(\"PDF\")\n```", "```java\nval labelAndPredictions: DataFrame = testingDframe.drop(\"label\").crossJoin(finalPreds).cache()\n println(\"Label And Predictions: \" )\n labelAndPredictions.show()\n```", "```java\nval fPs = positivesNegatives(labelAndPredictions, 0.0, 1.0)\nprintln(\"No of false negatives is: \" + fPs)\n```", "```java\nval tPs = positivesNegatives(labelAndPredictions, 1.0, 1.0)\n```", "```java\nval fNs = positivesNegatives(labelAndPredictions, 1.0, 0.0)\n```", "```java\n val precision = tPs / Math.max(1.0, tPs + fPs) \n```", "```java\nval recall = tPs / Math.max(1.0, tPs + fNs)\n```", "```java\nval f1Measure = 2.0 * precision * recall / (precision + recall)\n```", "```java\nif (f1Measure > bestF1Measure){ bestF1Measure = f1Measure bestErrorTermValue = errorTerm //println(\"f1Measure > bestF1Measure\") scores +( (1, bestErrorTermValue), (2, bestF1Measure) ) } }\n```", "```java\nfor (errorTerm <- maxMinPair._2 to maxMinPair._1 by stepsize) {\n\n//Step 1: We broadcast the error term (epsilon) into Spark\n\n//Step 2: We generate predictions \n\n//Step 3: We will crossjoin the final predictions dataframe with our initial Testing Dataframe\n\n//Step 4: We calculate False Negatives, True Negatives, False Negatives and True Positives\n\n//Step 5: Calculate Precision and Recall\n\n//Step 6: Calculate F1\n\nStep 7: Return Best Error Term and Best F1 Measure\n\n}\n```", "```java\ndef evalScores(testingDframe: DataFrame,probabilityDensities: DataFrame): ListMap[ Int, Double] = {\n\n/*\n  Extract the smallest value of probability density and the largest.  */\nval maxMinArray: Array[Double] = probabilityDensities.collect().map(pbRow => pbRow.getDouble(0) )\n\n/*\n  A sensible step size\n*/    \nval stepsize = (maxMinPair._1 - maxMinPair._2) / 750.0\n\n/*\n   Write the loop to calculate the best Epsilon and the best F1 at that Best Epsilon\n*/\nfor (errorTerm <- maxMinPair._2 to maxMinPair._1 by stepsize) {\n   //Step 1: We broadcast the error term (epsilon) into Spark\n     val broadCastedErrorTerm:Broadcast[Double] = ----\n\n     //Step 2: We generate predictions \n   import session.implicits._\n   val finalPreds: DataFrame= probabilityDensities.map { ...... }\n\n    //Step 3: We will crossjoin the final predictions dataframe with our initial Testing Dataframe\n     val labelAndPredictions: DataFrame = testingDframe.drop(\"label\").crossJoin(finalPreds).cache()\n\n    //Step 4: We calculate False Negatives, True Negatives, False Negatives and True Positives\n\n    //Step 5: Calculate Precision and Recall\n   val fPs = <<Invoke the positivesNegatives here >>\n   val tPs =  <<Invoke the positivesNegatives here >>\n   val tPs =  <<Invoke the positivesNegatives here >>\n\n    //The Precision and recall based on Step 5\n     val precision = tPs / Math.max(1.0, tPs + fPs)\n     val recall = tPs / Math.max(1.0, tPs + fNs)\n\n   //Step 6: Calculate F1 based on results from Step 5\n    val f1Measure = 2.0 * precision * recall / (precision + recall)\n\n    //Step 7: Return Best Error Term and Best F1 Measure\n     /*\n      //The logic to get at the Best Error Term (epsilon) and the F1 is this:\n      // At any point of time, in the looping process, if the F1 measure value from Step 6 is \n      // greater than 0,  then that F1 value is assigned to the Scala val representing the Best F1\n     // Both these value are added into a Scala ListMap\n     //When the loop is done executing we have an updated ListMap that contains two values: The Best F1     //and the Best Error Term \n\n```", "```java\ndef positivesNegatives(labelAndPredictions: DataFrame /* Dataset[(Double, Double)] */, \n targetLabel: Double, \n finalPrediction: Double): Double = {\n\n}\n```", "```java\nlabelAndPredictions.filter( labelAndPrediction => \n                                                        labelAndPrediction.getAs(\"PDF\") == targetLabel && \n                                                        labelAndPrediction.get(1) == finalPrediction ).count().toDouble\n```", "```java\ndef positivesNegatives(labelAndPredictions: DataFrame /* Dataset[(Double, Double)] */, targetLabel: Double, finalPrediction: Double): Double = {\n\n   //We do a filter operation on our labelsAndPredictions DataFrame. The filter condition is as follows:\n   // if the value under the label column matches the incoming targetLabel AND the value in the predictions column matches the finalPrediction value then count the number of datapoints that satisfy this condition. This will be your count of False Positives, for example.\n\n        labelAndPredictions.filter( <<the filter condition>>).count().toDouble \n\n}\n```"]