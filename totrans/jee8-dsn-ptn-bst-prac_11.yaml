- en: Operational Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore operational patterns, focusing in particular
    on why we use them and how they impact application projects. We will then cover
    performance and scalability patterns as well as management and monitoring patterns.
    After reading this chapter, we will have learned about the concept of operational
    patterns. The topics we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of operational patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of performance and scalability patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of management and monitoring patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining the concept of operational patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To solve a problem successfully, we need to evaluate a number of important
    steps so as to ensure that the chance of error is minimized. These steps are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Correctly identifying the problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the operations needed to solve the problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Developing the operations needed to solve the problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a business environment, both in an application project and in other areas,
    responding to problems quickly is integral to the success of a task. It is also
    important that we create a well-defined process for solving problems that will
    increase the chance of success, keep our work organized, and result in a more
    reliable solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, as mentioned previously,we identify the problem that needs
    to be resolved. Here, it is very important to define the boundaries of the problem
    and to assess whether or not the identified issue is indeed the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a high-level solution to the problem. In this step, the solution
    will be described, but without worrying about how it will be implemented. Instead,
    we focus on the threads or operations of the solution at a high level of thought. High-level,
    middle-level, and low-level thought refers to the degree of generic thinking,
    where high-level thinking is more generic than middle-level, which, in turn, is
    more generic than low-level.
  prefs: []
  type: TYPE_NORMAL
- en: In the third step, we develop the operations needed to solve the problems. In
    this step, the work is more practical, so we need to work with a middle and low
    level of thought.
  prefs: []
  type: TYPE_NORMAL
- en: The operational patterns work by promoting common high-level solutions to problems
    that occur repeatedly. However, although operational patterns describe solutions
    to a problem, they don't care about how the solutions are developed. These patterns
    therefore help us to simplify the problem and its solution and then define a good,
    executable process.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the concept of performance and scalability patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a business environment, processes and tasks need to be completed quickly
    in order to generate responses and solutions. With this in mind, the application
    also needs to be more performative and scaleable. Performance refers to how quickly
    an application responds to requests, and scalability refers to the application's
    capacity to respond to an increase in requests without affecting its performance
    or capacity. In other words, performance is more about the time it takes to serve
    a request, whereas scalability is about the system being able to upgrade and downgrade
    resources where needed.
  prefs: []
  type: TYPE_NORMAL
- en: In a business environment, problems with regard to performance are generally
    problems surrounding read data or external resources (such as file systems or
    other applications in a network). Performance problems generated by incorrect
    logic or algorithms are rarer because business environments generally have logic
    without a hard code, or codes with a long mathematical calculus. The algorithms
    of business environments generally use logic to read and save data via roles;
    therefore, performance growth generally consists of reading and writing data and
    accessing resources as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is directly linked to performance because it consists of growing
    the scale of an application. In other words, if we grow the scale of an application,
    we should be able to respond to an increase in requests without impacting performance,
    or the application will respond to requests with increased performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance and scalability patterns are operational patterns used for solving
    common problems with performance and scalability that provide applications with
    high-performance and availability at all times. We will cover the following performance
    and scalability patterns in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cache-aside**: This pattern loads data into a cache from a data store on
    demand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CQRS**: This pattern segregates operations that read data from operations
    that update data by using separate interfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event sourcing**: This pattern uses an append-only store to record the actions
    taken on data in a domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index table**: This pattern creates indexes over the fields in data stores
    that are frequently referenced by queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Materialized view**: This pattern generates pre-populated views over the
    data in one or more data stores when the data isn''t ideally formatted for the
    required query operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sharding**: This pattern divides a data store into a set of horizontal partitions
    or shards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static content hosting**: This pattern deploys static content to a cloud-based
    storage service that can deliver data directly to a client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cache-aside pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, increased performance generally means minimizing the
    time it takes to read data. One way to do this is through the cache-aside pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The cache-aside pattern is a pattern that loads data into a cache from a data
    store on demand. It also updates data to the data store if the application updates
    any data cache. The data is loaded into the cache if the user requests it and
    it is not available. So, when next accessing this data, the application will read
    from the cache and respond to the user. This pattern helps us to maintain consistency
    between data in the cache and the data store. A cache is another data source that
    maintains copies of data from the original data store, permitting the application
    to read data quickly. However, this pattern does not guarantee consistency between
    the data store and the cache, because another application can update data from
    the data store and thus make the cache stale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how the cache-aside pattern works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b75f290a-7fe0-4d86-9510-faa576b2139d.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, when an application reads any data from the data store, the application
    first consults the data on the cache. However, if the data does not exist, then
    the application consults the data in the data store. When an application consults
    data in the data store, the data returned by the data store is then loaded in
    the cache to allow the application to read the data faster in future. When an
    application updates the data contained in a cache, the data is saved in the data
    store and the data of the cache is then either updated or invalidated.
  prefs: []
  type: TYPE_NORMAL
- en: When to use the cache-aside pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is recommended that the cache-aside pattern is used when data isn't updated
    frequently, or when concurrent updates are infrequent. It is important to note
    that the cache-aside pattern should only be used when necessary. This is because
    the cache increases the complexity of applications, as dirty readings involving
    data definitions can sometimes occur. With this in mind, we recommend that you
    only apply this pattern if reading data is a slow process.
  prefs: []
  type: TYPE_NORMAL
- en: The lifetime of cached data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lifetime of cached data is a very important issue regarding the cache-aside
    pattern because the lifetime policy establishes whether or not the cache is efficient.
    The lifetime defines the data's expiry time according to an expiration policy
    that invalidates data and removes it from the cache if it's not accessed within
    a specified time limit. Defining a good lifetime policy therefore means defining
    a lifetime that is neither too long nor too short. It is also highly recommended
    that you do not create a global expiration policy but rather one for each type
    of item in the data store. This is because some data is updated more frequently
    than others.
  prefs: []
  type: TYPE_NORMAL
- en: Evicting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some caches have a limited size and require a policy in order to evict data.
    In addition to this, some caches select the least recently-used data to evict.
    However, we can customize this policy, and we can even use our own logic for removing
    data from the cache. Generally, the logic used to remove data is limited in size,
    but we can also use logic regarding updates.
  prefs: []
  type: TYPE_NORMAL
- en: Evicting data is different to setting an expiration policy because the expiration
    policy is a static logic that removes data in a cache when it expires. Eviction
    data, however, involves dynamic logic. An expiration policy allows us to minimize
    the chance of getting a dirty reading, whereas evicting data allows us to create
    logic that minimizes the chance of the cache reaching its limit and optimizes
    the consistency of the cache data.
  prefs: []
  type: TYPE_NORMAL
- en: Priming the cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We may want to start our cache with data optimized for application use. We should
    therefore populate the cache with data from the startup process of an application.
    It is very important to point out that we need to evaluate the expiration time
    of the data loaded in such a startup process; if the data expires before the application
    uses it, loading the startup process data is unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cache-aside pattern does not guarantee consistency between the data in the
    cache and the data in the data store. This is because an external application
    or resource can update data that is not reflected in the cache at any time. Consistency
    is very important to remember when implementing in order to avoid the risk of
    dirty readings.
  prefs: []
  type: TYPE_NORMAL
- en: Local (in-memory) caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to improve the process of reading data, a cache can be local and in-memory
    as long as that data is accessed frequently. As the local cache is private, another
    cache will need to be created. In this instance, data can be replicated to any
    local cache. To implement a local cache, we need to have a server with enough
    memory to permit our application and to ensure we do not cause memory overflow.
  prefs: []
  type: TYPE_NORMAL
- en: The CQRS pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an application, executing commands to read and write data from the data store
    using the same model or entity is a good approach. This is because managing data
    is generally easy in programming, and we can also use a scaffold to generate a
    project. However, when there is a lot of write data present, and the risk of concurrent
    write operations is high, the need to merge data is more urgent. When we use the
    same model to execute read and write data, the operation of reading data generally
    reads all the data within a model, and the operation of writing data generally
    writes all data in the model. Sometimes, however, we need to write fewer columns
    of data than there is in the model. In addition, using a single model to read
    and write data can increase security because some data is for querying only.
  prefs: []
  type: TYPE_NORMAL
- en: '**CQRS (Command and Query Responsibility Segregation****)** is a pattern that
    segregates read operations from write operations, creating a separate and decoupled
    interface for these operations. With this in mind, we need to create a model to
    read data and another model to write data, while only using the data needed for
    these operations.'
  prefs: []
  type: TYPE_NORMAL
- en: When to use the CQRS pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is recommended that the CQRS pattern is used when an application has a complex
    business role and its data is updated frequently by resources or events, therefore
    increasing the risk of a write operation. Implementing this pattern is also recommended
    when performance problems generated by the merging of data and write operations
    are concurrent. The use of the CQRS pattern increases the complexity of applications,
    so this pattern should only be used when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The event sourcing pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a typical approach, an application changes the state of data when a user
    works with it. However, this approach can often slow down performance and responsiveness
    and can also generate other concurrent updates. Unless we use an additional mechanism,
    we don't have a mechanism to audit and log operations or generate history.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing is a pattern that maintains the current data state and saves
    data update events in an event repository. This repository works as an append-only
    store, and when the data is accessed by a consumer, it receives the event and
    applies the update on the domain. Using this pattern promotes performance by decoupling
    the event logic and scalability and by auditing all actions applied to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the event of event sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the event sourcing pattern, an event is considered an update that will be
    applied to the data. When a user requests a data update, the application will
    not update it in real-time but will instead get the data, create an imperative
    description of an update, and save the event with that description. This event
    is then saved in the event repository. Note that the consumer of an event can
    be another application or resource.
  prefs: []
  type: TYPE_NORMAL
- en: Using the event sourcing pattern, the end user is able to see older versions
    of data because the data is only updated when the event is saved on the event
    repository and running. With this in mind, the end user will not see the update
    in real-time, as the update is scheduled and can be executed at any time. Remember
    that the event repository can be a relational database, file system, or another
    data source.
  prefs: []
  type: TYPE_NORMAL
- en: The consumer can be a component of the same application that saves the event
    in the event repository or it can be another application. The consumer of the
    event always checks whether it can run it when it receives one.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the event sourcing pattern should increase performance. This is because
    any updates can be done by a consumer at any time, and the update actions can
    lock data and impact data queries. This process, therefore, minimizes the risk
    of update operations colliding. Furthermore, the update operation can be done
    as a background process run on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting decoupling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the event sourcing pattern will also promote the decoupling of event logic
    because the updates can be done by another application or resource. The event
    publisher doesn't need to know about the event consumer, and an event can be divided
    to permit numerous applications or resources to execute part of the update process.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the event sourcing pattern will promote scalability because it also promotes
    the decoupling of the event publisher and event consumer. Therefore, we can replicate
    both the event publisher and event consumer once the measure of the request to
    an application or resource has been considered.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting auditing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the event sourcing pattern will promote the auditing of all actions applied
    to the data because the history of an event can be used to regenerate old information
    about data. This is because the event repository does not allow you to update
    or delete events. All data history is displayed in an event repository.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the index table pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days, the volume of data is ever-increasing, and applications need to
    be able to read a large quantity of data. To make it possible to read data faster,
    it is common to resort to a data structure or index. In a business environment,
    data can be organized as a collection of entities that have a primary key. However,
    we also want to be able to read data using attributes that don't contain an index
    as a filter in a query. The following diagram illustrates the data organization;
    if data stays organized, we can use an algorithm that retrieves it faster.
  prefs: []
  type: TYPE_NORMAL
- en: The relational database works with an index and allows us to create one in order
    to read data faster. Other data stores will not work with an index to read data,
    so we need to create our own index mechanism to promote faster data reading.
  prefs: []
  type: TYPE_NORMAL
- en: 'The index table pattern is a pattern that creates a table that organizes data
    using other data or a specific key. Using this, we can read data quicker. The
    pattern works using three strategies; they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicating data and organizing it in different keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating index tables organized by different keys and referencing the original
    data using the primary key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating index tables organized by different keys that duplicate frequently-retrieved
    fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the first strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c1da75f-97c5-40ad-9f51-7a0e6e3ea66c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the strategy shown in the following diagram, the data is duplicated in each
    index table. This works according to denormalization and is more appropriate for
    use with mostly static data. The following diagram illustrates the second strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d15ada3-c328-4762-a131-24baac12357f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the strategy shown in the following diagram, the data is organized according
    to different keys and references the original data using the primary key. In this
    strategy, the primary key of data is retrieved first by an attribute and then
    by using an ID. The following diagram illustrates the third strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18af210b-5e2c-4aeb-a918-d957a44a1769.png)'
  prefs: []
  type: TYPE_IMG
- en: In the strategy shown in the preceding diagram, the data is organized according
    to different keys that duplicate frequently-retrieved attributes in partially
    normalized index tables.
  prefs: []
  type: TYPE_NORMAL
- en: An index table pattern is a good mechanism for retrieving data quicker. However,
    overusing this pattern can generate a performance problem during data updates,
    as they require us to reorganize our index tables.
  prefs: []
  type: TYPE_NORMAL
- en: The materialized view pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Occasionally, business roles need read data that requires non-trivial queries.
    The user may sometimes need to see data that is made by combining data from different
    physical locations. This often causes performance problems, making the reading
    of data slow. For relational data, these locations are tables. To increase performance,
    therefore, one strategy can be to create a pre-populated view of data from multiple
    physical locations. Consequently, when the application executes a query, the data
    will already be made and will, therefore, be returned faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The materialized view pattern is a pattern that creates and materializes data
    in a format the application requires in a query. If the query consults data with
    joins or data that needs to be calculated, the materialized view pattern increase
    the performance, making it possible to read data faster. Note that data in a materialized
    view is never updated in a materialized view; it is simply a snapshot of the actual
    data. When the real data is updated, the materialized view will need to be rebuilt.
    The following diagram illustrates the materialized view pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4bbeb1c-9339-4579-a71c-c415e48e3e57.png)'
  prefs: []
  type: TYPE_IMG
- en: Rebuilding the materialized view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When data on the data source changes the materialized view, the materialized
    view must be rebuilt in order to reflect the changes. Rebuilding a materialized
    view can be done with either an automated or manual process. To define the strategy
    needed for rebuilding the materialized view, we first need to define whether the
    data will allow a dirty reading, as well as looking into the impact of such a
    reading. If the data cannot have a dirty reading, the materialized view needs
    to be rebuilt at the same time the original data is updated.
  prefs: []
  type: TYPE_NORMAL
- en: When to use the materialized view pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The materialized view pattern can promote a big increase in performance. This
    is because any data is read without performing joins or calculus to retrieve the
    data. However, if the data changes too rapidly, the materialized view will be
    rebuilt more times than necessary, which will impact performance.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using the materialized view pattern is inversely proportional
    to the speed of changes in the original data. Therefore, the use of a materialized
    view is recommended when data is rarely modified and is not dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the sharding pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have a table that grows rapidly and generates problems reading and writing
    data in a business environment, we need to apply a solution that deals with the
    problem of performance as well as the problem with data volume.
  prefs: []
  type: TYPE_NORMAL
- en: The sharding pattern is a pattern that divides a data store horizontally into
    partitions or shards. This can improve scalability and performance. The shards
    can run in the same node or in multiples nodes, but each shard has the same schema.
    When we divide a single database into shards, the rows of the database are distributed
    between them. Furthermore, a sharding pattern can be implemented by the database,
    if it has support, or by the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sharding pattern has three common strategies used during implementation.
    These strategies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The lookup strategy**: This strategy divides data into shards and the sharding
    logic implements a map that routes a request to the data using a shard key. After
    this, each shard is identified by a shard key and contains its own set of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The range strategy**: This strategy divides data into shards and the sharding
    logic implementation group related data together. These shards are identified
    by the shard key. This strategy is useful when we want to retrieve data using
    a range query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hash strategy**: This strategy divides data into shards and the sharding
    logic implementation allows an even distribution of data between the shards. These
    shards are identified by a shard key. Here, the shards are balanced and reduce
    the risk of a disproportionate amount of load in a shard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use the sharding pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are benefits to using the sharding pattern, but a downside is that it
    can also increase the complexity of a project. Because of this, this pattern should only be
    used when necessary, as increasing the complexity of a project increases the chance
    of errors occurring.
  prefs: []
  type: TYPE_NORMAL
- en: The sharding pattern should be used when the performance of retrieving data
    needs to be increased – in particular, when the data set is very large. The shards
    reduce the amount of data that is used to retrieve data, thereby increasing performance.
    Furthermore, using the sharding pattern makes it possible to scale the data store
    and also makes data highly available.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the concept of management and monitoring patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The volume of applications running in a cloud is ever-increasing and these applications
    commonly run in a remote data center. Because of this, we don't have full control
    over the infrastructure of an application, and it can become difficult to manage
    and monitor applications running remotely.
  prefs: []
  type: TYPE_NORMAL
- en: 'The management and monitoring patterns were created to allow us to manage and
    monitor our applications and expose runtime information about the application
    that supports business changes and customization without having to redeploy the
    application. Using this pattern, we can decouple the monitoring logic from the
    application logic, and we can also update the ambassador without impacting the
    application. In this section, we will explain the following management and monitoring
    patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: The ambassador pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The health endpoint monitoring pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The external configuration store pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ambassador pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, you may want to implement features such as routing, metering,
    or monitoring on a cloud application; you may also want to update network configurations.
    However, it is sometimes difficult to maintain the application and update codes. Furthermore,
    there may be some libraries that are not maintained by us and therefore cannot
    be modified.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ambassador pattern creates an external process including all of the logic,
    libraries, and frameworks needed to fulfill the requirements of management and
    monitoring. This external process acts as a proxy between the applications or
    external services. The following diagram illustrates the ambassador pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be6a4470-b874-4d69-ab30-1f1077966d69.png)'
  prefs: []
  type: TYPE_IMG
- en: When to use the ambassador pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of the ambassador pattern is to indicate when a supported cloud
    is needed for connectivity requirements, or to indicate when applications need
    to be modified. The ambassador pattern can also help with building a common set
    of client connectivity features for multiple languages or frameworks. However,
    when there is critical request latency, this pattern is not recommended. This
    is because the ambassador pattern introduces overhead on the network that can
    impact an application.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the health endpoint monitoring pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a business environment, applications always need to be available and to perform
    correctly. It is, therefore, necessary to monitor whether this is the case for
    all services. Monitoring applications on the cloud, however, is often difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'The health endpoint monitoring pattern implements a health monitor by sending
    requests to endpoints on the application and verifying the status returned by
    them. This pattern then analyzes the result returned by the application and performs
    the health verification check, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bd44735-5a2a-473c-83e3-52b4c81878c5.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see the **Application** and the **Agent**.
    This represents a solution that sends a request to the application and the service
    and checks its returns. Note that the **Agent** checks the **Data Store** too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The health endpoint monitoring pattern can perform the following checks:'
  prefs: []
  type: TYPE_NORMAL
- en: Validating the response code; on the HTTP protocol, the return of status 200 denotes
    success
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking the content of the response to detected errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the time interval between a request and its response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking resources or services located outside the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking the expiration of SSL certificates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the URL returned by the DNS lookup to ensure correct entries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use the health endpoint monitoring pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The health endpoint monitoring pattern is recommended for monitoring a web application
    or service's availability and performance. It can help with monitoring the middle-tier
    and shared service in order to identify and isolate a failure.
  prefs: []
  type: TYPE_NORMAL
- en: Using this pattern can help to identify a failure early and also apply the actions
    needed to solve the problem, something that has only a minor impact on the end
    user. Today, the capacity to identify failures quickly is very important to businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the external configuration store pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An application always needs to follow a set of configurations in order to perform
    its functions and promote services. Generally, these configurations are made by
    files that the application reads and gets information from. These configuration
    files are often deployed with an application package. Some updates in these configuration
    files require the redeployment of an application, thus increasing the complexity
    of updates in the configuration files. Furthermore, deploying the configuration
    files together with the application requires the creation of each configuration
    file for each application, and these files cannot be shared.
  prefs: []
  type: TYPE_NORMAL
- en: 'The external configuration store pattern creates an external repository for
    configuration patterns, providing an interface for permitting applications to
    read configuration files. Using this, we can update the configuration files without
    having to redeploy applications, and we can also share configurations between
    several applications, making the environment more organized and easier to manage,
    as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82947dd7-7258-49ca-a8c0-b6da2793a51f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding diagram, the applications read the configurations
    from a common location called the repository of configuration. If the configuration
    files are then updated, all applications will see the update.
  prefs: []
  type: TYPE_NORMAL
- en: When to use the external configuration store pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use of the external configuration store pattern is recommended when sharing
    configuration files with applications or when updates are required without the
    redeployment of an application. Sharing configuration files allows us to easily
    manage the configurations of an application, as all applications will see and
    be impacted by the update, which is done in one location. Using this pattern will
    also minimize the risk of error when updating the configuration file, so the use
    of this pattern is a good practice to adopt.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored operational patterns, performance and scalability
    patterns, and management and monitoring patterns. We also looked at why we should
    use these operational patterns and how to evaluate the best use for them.
  prefs: []
  type: TYPE_NORMAL
- en: On the topic of performance and scalability patterns, we looked at performance
    and scalability patterns such as cache-aside, CQRS, event sourcing, index table,
    materialized view, and sharding. Then, we explained each pattern's concept, what
    the benefits of it are, and when to implement it. We are now familiar with the
    techniques required for increasing the performance of an enterprise application,
    as well as how to make the application scalable.
  prefs: []
  type: TYPE_NORMAL
- en: On the topic of management and monitoring patterns, we explored management and
    monitoring patterns such as ambassador patterns, health endpoint monitoring patterns,
    and external configuration store patterns. We explained each pattern's concept,
    what the benefits of it are, and when to implement it. We are now familiar with
    the techniques required for managing and monitoring an enterprise application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the Eclipse MicroProfile project and its
    specifications.
  prefs: []
  type: TYPE_NORMAL
