- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing Reactive Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to develop reactive microservices, that is,
    how to develop non-blocking synchronous REST APIs and asynchronous event-driven
    services. We will also learn how to choose between these two alternatives. Finally,
    we will see how to create and run manual and automated tests of a reactive microservice
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: As already described in *Chapter 1*, *Introduction to Microservices*, the foundation
    for reactive systems is that they are message-driven—they use asynchronous communication.
    This enables them to be elastic, in other words, scalable and resilient, meaning
    that they are tolerant of failures. Elasticity and resilience together enable
    a reactive system to be responsive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between non-blocking synchronous APIs and event-driven asynchronous
    services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing non-blocking synchronous REST APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing event-driven asynchronous services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running manual tests of the reactive microservice landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running automated tests of the reactive microservice landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For instructions on how to install the tools used in this book and how to access
    the source code for this book, see:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 21*, *Installation Instructions for macOS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 22*, *Installation Instructions for Microsoft Windows with WSL 2 and
    Ubuntu*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code examples in this chapter all come from the source code in `$BOOK_HOME/Chapter07`.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to view the changes applied to the source code in this chapter,
    that is, see what it takes to make the microservices reactive, you can compare
    it with the source code for *Chapter 6*, *Adding Persistence*. You can use your
    favorite `diff` tool and compare the two folders, that is, `$BOOK_HOME/Chapter06`
    and `$BOOK_HOME/Chapter07`.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between non-blocking synchronous APIs and event-driven asynchronous
    services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing reactive microservices, it is not always obvious when to use
    non-blocking synchronous APIs and when to use event-driven asynchronous services.
    In general, to make a microservice robust and scalable, it is important to make
    it as autonomous as possible, for example, by minimizing its runtime dependencies.
    This is also known as **loose coupling**. Therefore, the asynchronous message
    passing of events is preferable over synchronous APIs. This is because the microservice
    will only depend on access to the messaging system at runtime, instead of being
    dependent on synchronous access to a number of other microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, however, a number of cases where synchronous APIs could be favorable.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: For read operations where an end user is waiting for a response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where the client platforms are more suitable for consuming synchronous APIs,
    for example, mobile apps or SPA web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where the clients will connect to the service from other organizations – where
    it might be hard to agree on a common messaging system to use across organizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the system landscape in this book, we will use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The create, read, and delete services exposed by the product composite microservice
    will be based on non-blocking synchronous APIs. The composite microservice is
    assumed to have clients on both web and mobile platforms, as well as clients coming
    from other organizations rather than the ones that operate the system landscape.
    Therefore, synchronous APIs seem like a natural match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The read services provided by the core microservices will also be developed
    as non-blocking synchronous APIs since there is an end user waiting for their
    responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The create and delete services provided by the core microservices will be developed
    as event-driven asynchronous services, meaning that they will listen for create
    and delete events on topics dedicated to each microservice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synchronous APIs provided by the composite microservices to create and delete
    aggregated product information will publish create and delete events on these
    topics. If the publish operation succeeds, it will return with a 202 (Accepted)
    response; otherwise, an error response will be returned. The 202 response differs
    from a normal 200 (OK) response – it indicates that the request has been accepted,
    but not fully processed. Instead, the processing will be completed asynchronously
    and independently of the 202 response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, diagram  Description automatically generated](img/B19825_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: The microservice landscape'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s learn how we can develop non-blocking synchronous REST APIs, and
    thereafter, we will look at how to develop event-driven asynchronous services.
  prefs: []
  type: TYPE_NORMAL
- en: Developing non-blocking synchronous REST APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to develop non-blocking versions of the
    read APIs. The composite service will make reactive, that is, non-blocking, calls
    in parallel to the three core services. When the composite service has received
    responses from all of the core services, it will create a composite response and
    send it back to the caller. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: The getCompositeProduct part of the landscape'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Project Reactor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-blocking persistence using Spring Data for MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-blocking REST APIs in the core services, including how to handle blocking
    code for the JPA-based persistence layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-blocking REST APIs in the composite service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Project Reactor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned in the *Spring WebFlux* section in *Chapter 2*, *Introduction
    to Spring Boot*, the reactive support in Spring 5 is based on **Project Reactor**
    ([https://projectreactor.io](https://projectreactor.io)). Project Reactor is based
    on the *Reactive Streams specification* ([http://www.reactive-streams.org](http://www.reactive-streams.org)),
    a standard for building reactive applications. Project Reactor is fundamental
    – it is what Spring WebFlux, Spring WebClient, and Spring Data rely on to provide
    their reactive and non-blocking features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The programming model is based on processing streams of data, and the core
    data types in Project Reactor are **Flux** and **Mono**. A `Flux` object is used
    to process a stream of *0...n* elements and a `Mono` object is used to process
    a stream that either is empty or returns at most one element. We will see numerous
    examples of their usage in this chapter. As a short introduction, let’s look at
    the following test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an explanation of the preceding source code:'
  prefs: []
  type: TYPE_NORMAL
- en: We initiate the stream with the integers `1`, `2`, `3`, and `4` using the static
    helper method `Flux.just()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we `filter` out the odd numbers – we only allow even numbers to proceed
    through the stream. In this test, these are `2` and `4`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we transform (or `map`) the values in the stream by multiplying them by
    `2`, so they become `4` and `8`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we `log` the data that flows through the stream after the `map` operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the `collectList` method to collect all items from the stream into a
    `List`, emitted once the stream completes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, we have only declared the processing of a stream. To actually get the
    stream processed, we need someone to subscribe to it. The final call to the `block`
    method will register a subscriber that waits for the processing to complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting list is saved in a member variable named `list`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can now wrap up the test by using the `assertThat` method to assert that
    `list` after the processing of the stream contains the expected result – the integers
    `4` and `8`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The log output will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B19825_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Log output for the code above'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding log output, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The processing of the stream is started by a subscriber that subscribes to the
    stream and requests its content.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the integers `4` and `8` pass through the `log` operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The processing concludes with a call to the `onComplete` method on the subscriber,
    notifying it that the stream has come to an end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the full source code, see the `ReactorTests` test class in the `util` project.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, we don’t initiate the processing of the stream. Instead, we only define
    how it will be processed, and it will be the responsibility of an infrastructure
    component to initiate the processing. For example, Spring WebFlux will do this
    as a response to an incoming HTTP request. An exception to this rule of thumb
    is the case where blocking code needs a response from a reactive stream. In these
    cases, the blocking code can call the `block()` method on the `Flux` or `Mono`
    object to get the response in a blocking way.
  prefs: []
  type: TYPE_NORMAL
- en: Non-blocking persistence using Spring Data for MongoDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Making the MongoDB-based repositories for the `product` and `recommendation`
    services reactive is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the base class for the repositories to `ReactiveCrudRepository`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the custom finder methods to return either a `Mono` or a `Flux` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ProductRepository` and `RecommendationRepository` look like the following
    after the change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: No changes are applied to the persistence code for the `review` service; it
    will remain blocking using the JPA repository. See the following section, *Dealing
    with blocking code*, for how to handle the blocking code in the persistence layer
    of the `review` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the full source code, take a look at the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductRepository` in the `product` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecommendationRepository` in the `recommendation` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the test code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to testing the persistence layer, we have to make some changes.
    Since our persistence methods now return a `Mono` or `Flux` object, the test methods
    have to wait for the response to be available in the returned reactive objects.
    The test methods can either use an explicit call to the `block()` method on the
    `Mono`/`Flux` object to wait until a response is available, or they can use the
    `StepVerifier` helper class from Project Reactor to declare a verifiable sequence
    of asynchronous events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can change the following test code to work for the reactive
    version of the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `block()` method on the `Mono` object returned by the `repository.findById()`
    method and keep the imperative programming style, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use the `StepVerifier` class to set up a sequence of
    processing steps that both executes the repository find operation and also verifies
    the result. The sequence is initialized by the final call to the `verifyComplete()`
    method like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For examples of tests that use the `StepVerifier` class, see the `PersistenceTests`
    test class in the `product` project.
  prefs: []
  type: TYPE_NORMAL
- en: For corresponding examples of tests that use the `block()` method, see the `PersistenceTests`
    test class in the `recommendation` project.
  prefs: []
  type: TYPE_NORMAL
- en: Non-blocking REST APIs in the core services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With a non-blocking persistence layer in place, it’s time to make the APIs
    in the core services non-blocking as well. We need to make the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the APIs so that they only return reactive data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the service implementations so they don’t contain any blocking code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change our tests so that they can test the reactive services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deal with blocking code – isolate the code that still needs to be blocking from
    the non-blocking code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make the APIs of the core services reactive, we need to update their methods
    so that they return either a `Mono` or `Flux` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, `getProduct()` in the `product` service now returns `Mono<Product>`
    instead of a `Product` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For the full source code, take a look at the following `core` interfaces in
    the `api` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductService`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecommendationService`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReviewService`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the service implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the implementations of the services in the `product` and `recommendation`
    projects, which use a reactive persistence layer, we can use the fluent API in
    Project Reactor. For example, the implementation of the `getProduct()` method
    looks like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine what the code does:'
  prefs: []
  type: TYPE_NORMAL
- en: The method will return a `Mono` object; the processing is only declared here.
    The processing is triggered by the web framework, Spring WebFlux, subscribing
    to the `Mono` object once it receives a request to this service!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A product will be retrieved using its `productId` from the underlying database
    using the `findByProductId()` method in the persistence repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If no product is found for the given `productId`, a `NotFoundException` will
    be thrown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `log` method will produce log output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `mapper.entityToApi()` method will be called to transform the returned entity
    from the persistence layer into an API model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final `map` method will use a helper method, `setServiceAddress()`, to set
    the DNS name and IP address of the microservices that processed the request in
    the `serviceAddress` field of the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some example log output for successful processing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B19825_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Log output when processing is successful'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example log output of a failed processing (throwing a `NotFoundException`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B19825_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Log output when processing fails'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the full source code, see the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductServiceImpl` in the `product` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecommendationServiceImpl` in the `recommendation` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the test code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The test code for service implementations has been changed in the same way as
    the tests for the persistence layer we described previously. To handle the asynchronous
    behavior of the reactive return types, `Mono` and `Flux`, the tests use a mix
    of calling the `block()` method and using the `StepVerifier` helper class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the full source code, see the following test classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductServiceApplicationTests` in the `product` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecommendationServiceApplicationTests` in the `recommendation` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with blocking code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of the `review` service, which uses JPA to access its data in a
    relational database, we don’t have support for a non-blocking programming model.
    Instead, we can run the blocking code using a `Scheduler`, which is capable of
    running the blocking code on a thread from a dedicated thread pool with a limited
    number of threads. Using a thread pool for the blocking code avoids draining the
    available threads in the microservice and avoids affecting concurrent non-blocking
    processing in the microservice, if there is any.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this can be set up in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we configure a scheduler bean and its thread pool in the main class
    `ReviewServiceApplication`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the preceding code, we can see that the scheduler bean is named `jdbcScheduler`
    and that we can configure its thread pool using the following properties:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`app.threadPoolSize`, specifying the max number of threads in the pool; defaults
    to `10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`app.taskQueueSize`, specifying the max number of tasks that are allowed to
    be placed in a queue waiting for available threads; defaults to `100`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we inject the scheduler named `jdbcScheduler` into the `review` service
    implementation class, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we use the scheduler’s thread pool in the reactive implementation
    of the `getReviews()` method, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the blocking code is placed in the `internalGetReviews()` method and is
    wrapped in a `Mono` object using the `Mono.fromCallable()` method. The `getReviews()`
    method uses the `subscribeOn()` method to run the blocking code in a thread from
    the thread pool of `jdbcScheduler`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run tests later on in this chapter, we can look at the log output from
    the `review` service and see proof that SQL statements are run in threads from
    the scheduler’s dedicated pool. We will be able to see log output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B19825_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Log output from the review service'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding log output, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first log output is from the `LOG.info()` call in the `getReviews()` method
    and it is executed on an HTTP thread, named `ctor-http-nio-4`, a thread used by
    WebFlux.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second log output, we can see the SQL statement generated by Spring Data
    JPA, using Hibernate under the hood. The SQL statement corresponds to the method
    call `repository.findByProductId()`. It is executed on a thread named `jdbc-pool-1`,
    meaning it is executed in a thread from the dedicated thread pool for blocking
    code, as expected!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code, see the `ReviewServiceApplication` and `ReviewServiceImpl`
    classes in the `review` project.
  prefs: []
  type: TYPE_NORMAL
- en: With the logic for handling blocking code in place, we are done with implementing
    the non-blocking REST APIs in the core services. Let’s move on and see how to
    also make the REST APIs in the composite services non-blocking.
  prefs: []
  type: TYPE_NORMAL
- en: Non-blocking REST APIs in the composite services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make our REST API in the composite service non-blocking, we need to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the API so that its operations only return reactive data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the service implementation so it calls the coreata services’ APIs in
    parallel and in a non-blocking way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the integration layer so it uses a non-blocking HTTP client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change our tests so that they can test the reactive service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make the API of the composite service reactive, we need to apply the same
    type of change that we applied for the APIs of the core services we described
    previously. This means that the return type of the `getProduct()` method, `ProductAggregate`,
    needs to be replaced with `Mono<ProductAggregate>`.
  prefs: []
  type: TYPE_NORMAL
- en: The `createProduct()` and `deleteProduct()` methods need to be updated to return
    a `Mono<Void>` instead of a `void`; otherwise, we can’t propagate any error responses
    back to the callers of the API.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code, see the `ProductCompositeService` interface in the
    `api` project.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the service implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To be able to call the three APIs in parallel, the service implementation uses
    the static `zip()` method on the `Mono` class. The `zip` method is capable of
    handling a number of parallel reactive requests and zipping them together once
    they all are complete. The code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter of the `zip` method is a lambda function that will receive
    the responses in an array, named `values`. The array will contain a product, a
    list of recommendations, and a list of reviews. The actual aggregation of the
    responses from the three API calls is handled by the same helper method as before,
    `createProductAggregate()`, without any changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters after the lambda function are a list of the requests that the
    `zip` method will call in parallel, one `Mono` object per request. In our case,
    we send in three `Mono` objects that were created by the methods in the integration
    class, one for each request that is sent to each core microservice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code, see the `ProductCompositeServiceImpl` class in the
    `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: For information on how the `createProduct` and `deleteProduct` API operations
    are implemented in the `product-composite` service, see the *Publishing events
    in the composite service* section later on.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the integration layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the `ProductCompositeIntegration` integration class, we have replaced the
    blocking HTTP client, `RestTemplate`, with a non-blocking HTTP client, `WebClient`,
    that comes with Spring 5.
  prefs: []
  type: TYPE_NORMAL
- en: To create a `WebClient` instance, a **builder pattern** is used. If customization
    is required, for example, setting up common headers or filters, it can be done
    using the builder. For the available configuration options, see [https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-client-builder](https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux-client-builder).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `WebClient` is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the constructor, the `WebClient` is auto-injected. We build the `WebClient`
    instance without any configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use the `webClient` instance to make our non-blocking requests for
    calling the `product` service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the API call to the `product` service fails with an HTTP error response,
    the whole API request will fail. The `onErrorMap()` method in `WebClient` will
    call our `handleException(ex)` method, which maps the HTTP exceptions thrown by
    the HTTP layer to our own exceptions, for example, a `NotFoundException` or a
    `InvalidInputException`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if calls to the `product` service succeed but the call to either the
    `recommendation` or `review` API fails, we don’t want to let the whole request
    fail. Instead, we want to return as much information as is available back to the
    caller. Therefore, instead of propagating an exception in these cases, we will
    instead return an empty list of recommendations or reviews. To suppress the error,
    we will make the call `onErrorResume(error -> empty())`. For this, the code looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `GlobalControllerExceptionHandler` class, from the `util` project, will,
    as previously, catch exceptions and transform them into proper HTTP error responses
    that are sent back to the caller of the composite API. This way we can decide
    if a specific HTTP error response from the underlying API calls will result in
    an HTTP error response or just a partly empty response.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code, see the `ProductCompositeIntegration` class in the
    `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the test code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The only change that’s required in the test classes is to update the setup
    of Mockito and its mock of the integration class. The mock needs to return `Mono`
    and `Flux` objects. The `setup()` method uses the helper methods `Mono.just()`
    and `Flux.fromIterable()`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For the full source code, see the `ProductCompositeServiceApplicationTests`
    test class in the `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: This completes the implementation of our non-blocking synchronous REST APIs.
    Now it is time to develop our event-driven asynchronous services.
  prefs: []
  type: TYPE_NORMAL
- en: Developing event-driven asynchronous services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to develop event-driven and asynchronous
    versions of the create and delete services. The composite service will publish
    create and delete events on each core service topic and then return an OK response
    back to the caller without waiting for processing to take place in the core services.
    This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: The createCompositeProduct and deleteCompositeProduct parts of
    the landscape'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling challenges with messaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining topics and events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in Gradle build files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consuming events in the core services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing events in the composite service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling challenges with messaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement the event-driven create and delete services, we will use Spring
    Cloud Stream. In *Chapter 2*, *Introduction to Spring Boot*, we have already seen
    how easy it is to publish and consume messages on a topic using Spring Cloud Stream.
  prefs: []
  type: TYPE_NORMAL
- en: The programming model is based on a functional paradigm, where functions implementing
    one of the functional interfaces `Supplier`, `Function`, or `Consumer` in the
    `java.util.function` package can be chained together to perform decoupled event-based
    processing. To trigger such functional-based processing externally, from non-functional
    code, the helper class `StreamBridge` can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to publish the body of an HTTP request to a topic, we only have
    to write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The helper class `StreamBridge` is used to trigger the processing. It will
    publish a message on a topic. A function that consumes events from a topic (not
    creating new events) can be defined by implementing the functional interface `java.util.function.Consumer`
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To tie the various functions together, we use configuration. We will see examples
    of such configuration below in the sections *Adding configuration for publishing
    events* and *Adding configuration for consuming events*.
  prefs: []
  type: TYPE_NORMAL
- en: This programming model can be used independently of the messaging system used,
    for example, RabbitMQ or Apache Kafka!
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though sending asynchronous messages is preferred over synchronous API
    calls, it comes with challenges of its own. We will see how we can use Spring
    Cloud Stream to handle some of them. The following features in Spring Cloud Stream
    will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Consumer groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retries and dead-letter queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteed orders and partitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll study each of these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The problem here is, if we scale up the number of instances of a message consumer,
    for example, if we start two instances of the `product` microservice, both instances
    of the `product` microservice will consume the same messages, as illustrated by
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Products #1 and #2 consuming the same messages'
  prefs: []
  type: TYPE_NORMAL
- en: 'This could result in one message being processed two times, potentially leading
    to duplicates or other undesired inconsistencies in the database. Therefore, we
    only want one instance per consumer to process each message. This can be solved
    by introducing a **consumer group**, as illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Consumer group'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spring Cloud Stream, a consumer group can be configured on the consumer
    side. For example, for the `product` microservice, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From this configuration, we can learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spring Cloud Stream applies, by default, a naming convention for binding a
    configuration to a function. For messages sent to a function, the binding name
    is `<functionName>-in-<index>`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`functionName` is the name of the function, `messageProcessor` in the preceding
    example.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` is set to `0`, unless the function requires multiple input or output
    arguments. We will not use multi-argument functions, so `index` will always be
    set to `0` in our examples.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For outgoing messages, the binding name convention is `<functionName>-out-<index>`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `destination` property specifies the name of the topic that messages will
    be consumed from, `products` in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `group` property specifies what consumer group to add instances of the `product`
    microservice to, `productsGroup` in this example. This means that messages sent
    to the `products` topic will only be delivered by Spring Cloud Stream to one of
    the instances of the `product` microservice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retries and dead-letter queues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If a consumer fails to process a message, it may be re-queued for the failing
    consumer until it is successfully processed. If the content of the message is
    invalid, also known as a **poisoned message**, the message will block the consumer
    from processing other messages until it is manually removed. If the failure is
    due to a temporary problem, for example, the database can’t be reached due to
    a temporary network error, the processing will probably succeed after a number
    of retries.
  prefs: []
  type: TYPE_NORMAL
- en: It must be possible to specify the number of retries until a message is moved
    to another storage for fault analysis and correction. A failing message is typically
    moved to a dedicated queue called a dead-letter queue. To avoid overloading the
    infrastructure during temporary failure, for example, a network error, it must
    be possible to configure how often retries are performed, preferably with an increasing
    length of time between each retry.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spring Cloud Stream, this can be configured on the consumer side, for example,
    for the `product` microservice, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we specify that Spring Cloud Stream should perform
    `3` retries before placing a message on the dead-letter queue. The first retry
    will be attempted after `500` ms and the two other attempts after `1000` ms.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the use of dead-letter queues is binding-specific; therefore, we have
    one configuration for RabbitMQ and one for Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Guaranteed order and partitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the business logic requires that messages are consumed and processed in the
    same order as they were sent, we cannot use multiple instances per consumer to
    increase processing performance; for example, we cannot use consumer groups. This
    might, in some cases, lead to an unacceptable latency in the processing of incoming
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: We can use **partitions** to ensure that messages are delivered in the same
    order as they were sent but without losing performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, strict order in the processing of messages is only required for
    messages that affect the same business entities. For example, messages affecting
    the product with product ID `1` can, in many cases, be processed independently
    of messages that affect the product with product ID `2`. This means that the order
    only needs to be guaranteed for messages that have the same product ID.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this is to make it possible to specify a **key** for each message,
    which the messaging system can use to guarantee that the order is kept between
    messages with the same key. This can be solved by introducing sub-topics, also
    known as **partitions**, in a topic. The messaging system places messages in a
    specific partition based on its key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Messages with the same key are always placed in the same partition. The messaging
    system only needs to guarantee the delivery order for messages in the same partition.
    To ensure the order of the messages, we configure one consumer instance per partition
    within a consumer group. By increasing the number of partitions, we can allow
    a consumer to increase its number of instances. This increases its message-processing
    performance without losing the delivery order. This is illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B19825_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Specifying keys for messages'
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding diagram, all messages with the `Key` set to `123` always
    go to the `Products-1`, partition while messages with the `Key` set to `456` go
    to the `Products-2` partition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spring Cloud Stream, this needs to be configured on both the publisher and
    consumer sides. On the publisher side, the key and number of partitions must be
    specified. For example, for the `product-composite` service, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This configuration means that the key will be taken from the message header
    with the name `partitionKey` and that two partitions will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each consumer can specify which partition it wants to consume messages from.
    For example, for the `product` microservice, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This configuration tells Spring Cloud Stream that this consumer will only consume
    messages from partition number `0`, that is, the first partition.
  prefs: []
  type: TYPE_NORMAL
- en: Defining topics and events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we already mentioned in the *Spring Cloud Stream* section in *Chapter 2*,
    *Introduction to Spring Boot*, Spring Cloud Stream is based on the publish and
    subscribe pattern, where a publisher publishes messages to topics and subscribers
    subscribe to topics they are interested in receiving messages from.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use one **topic** per type of entity: `products`, `recommendations`,
    and `reviews`.'
  prefs: []
  type: TYPE_NORMAL
- en: Messaging systems handle **messages** that typically consist of headers and
    a body. An **event** is a message that describes something that has happened.
    For events, the message body can be used to describe the type of event, the event
    data, and a timestamp for when the event occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'An event is, for the scope of this book, defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The **type** of event, for example, a create or delete event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **key** that identifies the data, for example, a product ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **data** element, that is, the actual data in the event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **timestamp**, which describes when the event occurred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The event class we will use looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explain the preceding source code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Event` class is a generic class parameterized over the types of its `key`
    and `data` fields, `K` and `T`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The event type is declared as an enumerator with the allowed values, that is,
    `CREATE` and `DELETE`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class defines two constructors, one empty and one that can be used to initialize
    the type, key, and value members
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the class defines getter methods for its member variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code, see the `Event` class in the `api` project.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the Gradle build files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To bring in Spring Cloud Stream and its binders for RabbitMQ and Kafka, we
    need to add the two starter dependencies known as `spring-cloud-starter-stream-rabbit`
    and `spring-cloud-starter-stream-kafka`. We also need a test dependency in the
    `product-composite` project, `spring-cloud-stream::test-binder`, to bring in test
    support. The following code shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To specify what version of Spring Cloud we want to use, we first declare a
    variable for the version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the variable to set up dependency management for the specified
    Spring Cloud version, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: For the full source code, see the `build.gradle` build file in each of the microservices
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: With the required dependencies added to the Gradle build files, we can start
    to learn how to consume events in the core services.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming events in the core services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to consume events in the core services, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare message processors that consume events published on the core service’s
    topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change our service implementations to use the reactive persistence layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add configuration required for consuming events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change our tests so that they can test the asynchronous processing of the events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code for consuming events is structured in the same way in all three
    core services, so we will only go through the source code for the `product` service.
  prefs: []
  type: TYPE_NORMAL
- en: Declaring message processors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The REST APIs for creating and deleting entities have been replaced with a **message
    processor** in each core microservice that consumes create and delete events on
    each entity’s topic. To be able to consume messages that have been published to
    a topic, we need to declare a Spring Bean that implements the functional interface
    `java.util.function.Consumer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The message processor for the `product` service is declared as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The class is annotated with `@Configuration`, telling Spring to look for Spring
    beans in the class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We inject an implementation of the `ProductService` interface in the constructor.
    The `productService` bean contains the business logic to perform the actual creation
    and deletions of the product entities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We declare the message processor as a Spring bean that implements the functional
    interface `Consumer`, accepting an event as an input parameter of type `Event<Integer,Product>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The implementation of the `Consumer` function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding implementation does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes an event of type `Event<Integer,Product>` as an input parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a `switch` statement, based on the event type, it will either create or
    delete a product entity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses the injected `productService` bean to perform the actual create and
    delete operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the event type is neither create nor delete, an exception will be thrown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that we can propagate exceptions thrown by the `productService` bean
    back to the messaging system, we call the `block()` method on the responses we
    get back from the `productService` bean. This ensures that the message processor
    waits for the `productService` bean to complete its creation or deletion in the
    underlying database. Without calling the `block()` method, we would not be able
    to propagate exceptions and the messaging system would not be able to re-queue
    a failed attempt or possibly move the message to a dead-letter queue; instead,
    the message would silently be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Calling a `block()` method is, in general, considered a bad practice from a
    performance and scalability perspective. But in this case, we will only handle
    a few incoming messages in parallel, one per partition, as described above. This
    means that we will only have a few threads blocked concurrently, which will not
    negatively impact the performance or the scalability.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code, see the `MessageProcessorConfig` classes in the `product`,
    `recommendation`, and `review` projects.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the service implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The service implementations of the create and delete methods for the `product`
    and `recommendation` service have been rewritten to use the non-blocking reactive
    persistence layer for MongoDB. For example, creating product entities is done
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note from the preceding code that the `onErrorMap()` method is used to map the
    `DuplicateKeyException` persistence exception to our own `InvalidInputException`
    exception.
  prefs: []
  type: TYPE_NORMAL
- en: For the `review` service, which uses the blocking persistence layer for JPA,
    the create and delete methods have been updated in the same way as described in
    the *Dealing with blocking code section*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the full source code, see the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductServiceImpl` in the `product` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecommendationServiceImpl` in the `recommendation` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReviewServiceImpl` in the `review` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding configuration for consuming events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also need to set up a configuration for the messaging system to be able
    to consume events. To do this, we need to complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We declare that RabbitMQ is the default messaging system and that the default
    content type is JSON:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we bind the input to the message processors to specific topic names,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we declare connectivity information for both Kafka and RabbitMQ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the default Spring profile, we specify hostnames to be used when we run our
    system landscape without Docker on `localhost` with the IP address `127.0.0.1`.
    In the `docker` Spring profile, we specify the hostnames we will use when running
    in Docker and using Docker Compose, that is, `rabbitmq` and `kafka`.
  prefs: []
  type: TYPE_NORMAL
- en: Added to this configuration, the consumer configuration also specifies consumer
    groups, retry handling, dead-letter queues, and partitions as they were described
    earlier in the *Handling challenges with messaging* section.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code, see the `application.yml` configuration files in the
    `product`, `recommendation`, and `review` projects.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the test code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the core services now receive events for creating and deleting their
    entities, the tests need to be updated so that they send events instead of calling
    REST APIs, as they did in the previous chapters. To be able to call the message
    processor from the test class, we inject the message processor bean into a member
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that we not only inject any `Consumer` function
    but also use the `@Qualifier` annotation to specify that we want to inject the
    `Consumer` function that has the name `messageProcessor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To send create and delete events to the message processor, we add two helper
    methods, `sendCreateProductEvent` and `sendDeleteProductEvent`, in the test class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use the `accept()` method in the `Consumer` function interface
    declaration to invoke the message processor. This means that we skip the messaging
    system in the tests and call the message processor directly.
  prefs: []
  type: TYPE_NORMAL
- en: The tests for creating and deleting entities are updated to use these helper
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the full source code, see the following test classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProductServiceApplicationTests` in the `product` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecommendationServiceApplicationTests` in the `recommendation` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReviewServiceApplicationTests` in the `review` project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen what is required to consume events in the core microservices. Now
    let’s see how we can publish events in the composite microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing events in the composite service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the composite service receives HTTP requests for the creation and deletion
    of composite products, it will publish the corresponding events to the core services
    on their topics. To be able to publish events in the composite service, we need
    to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Publish events in the integration layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add configuration for publishing events
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change tests so that they can test the publishing of events
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that no changes are required in the composite service implementation class
    – it is taken care of by the integration layer!
  prefs: []
  type: TYPE_NORMAL
- en: Publishing events in the integration layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To publish an event in the integration layer, we need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an `Event` object based on the body in the HTTP request
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `Message` object where the `Event` object is used as the payload and
    the key field in the `Event` object is used as the partition key in the header
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the helper class `StreamBridge` to publish the event on the desired topic
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code for sending create product events looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see:'
  prefs: []
  type: TYPE_NORMAL
- en: The integration layer implements the `createProduct()` method in the `ProductService`
    interface by using a helper method, `sendMessage()`. The helper method takes the
    name of an output binding and an event object. The binding name `products-out-0`
    will be bound to the topic of the `product` service in the configuration below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the `sendMessage()` uses blocking code, when calling `streamBridge`, it
    is executed on a thread provided by a dedicated scheduler, `publishEventScheduler`.
    This is the same approach as for handling blocking JPA code in the `review` microservice.
    See the section on *Dealing with blocking code* for details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The helper method, `sendMessage()`, creates a `Message` object and sets the
    `payload` and the `partitionKey` header as described above. Finally, it uses the
    `streamBridge` object to send the event to the messaging system, which will publish
    it on the topic defined in the configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code, see the `ProductCompositeIntegration` class in the
    `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: Adding configuration for publishing events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also need to set up the configuration for the messaging system, to be able
    to publish events; this is similar to what we did for the consumers. Declaring
    RabbitMQ as the default messaging system, JSON as the default content type, and
    Kafka and RabbitMQ for connectivity information is the same as for the consumers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To declare what topics should be used for the output binding names, we have
    the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When using partitions, we also need to specify the partition key and the number
    of partitions that will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding configuration, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration applies for the binding name `products-out-0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partition key used will be taken from the message header `partitionKey`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two partitions will be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full source code, see the `application.yml` configuration file in the
    `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the test code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Testing asynchronous event-driven microservices is, by its nature, difficult.
    Tests typically need to synchronize on the asynchronous background processing
    in some way to be able to verify the result. Spring Cloud Stream comes with support,
    in the form of a test binder, that can be used to verify what messages have been
    sent without using any messaging system during the tests!
  prefs: []
  type: TYPE_NORMAL
- en: See the *Changes in the Gradle build files* section earlier for how the test
    support is included in the `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test support includes an `OutputDestination` helper class, which can be
    used to get the messages that were sent during a test. A new test class, `MessagingTests`,
    has been added to run tests that verify that the expected messages are sent. Let’s
    go through the most important parts of the test class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to inject an `OutputDestination` bean in the test class, we also
    need to bring in its configuration from the class `TestChannelBinderConfiguration`.
    This is done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we declare a couple of helper methods for reading messages and also to
    be able to purge a topic. The code looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the preceding code, we can see that:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `getMessage()` method returns a message from a specified topic using the
    `OutputDestination` bean, named `target`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getMessages()` method uses the `getMessage()` method to return all messages
    in a topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `purgeMessages()` method uses the `getMessages()` method to purge a topic
    from all current messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each test starts with purging all topics involved in the tests using a `setup()`
    method annotated with `@BeforeEach`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An actual test can verify the messages in a topic using the `getMessages()`
    method. For example, see the following test for the creation of a composite product:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the preceding code, we can see an example where a test:'
  prefs: []
  type: TYPE_NORMAL
- en: First makes an HTTP POST request, requesting the creation of a composite product.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, gets all messages from the three topics, one for each underlying core
    service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For these tests, the specific timestamp for when an event was created is irrelevant.
    To be able to compare an actual event with an expected event, ignoring differences
    in the `eventCreatedAt` field, a helper class called `IsSameEvent` can be used.
    The `sameEventExceptCreatedAt()` method is a static method in the `IsSameEvent`
    class that compares `Event` objects and treats them as equal if all the fields
    are equal, except for the `eventCreatedAt` field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it verifies that the expected events can be found, and no others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the full source code, see the test classes `MessagingTests` and `IsSameEvent`
    in the `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: Running manual tests of the reactive microservice landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we have fully reactive microservices, both in terms of non-blocking synchronous
    REST APIs and event-driven asynchronous services. Let’s try them out!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will learn how to run tests using both RabbitMQ and Kafka as the message
    broker. Since RabbitMQ can be used both with and without partitions, we will test
    both cases. Three different configurations will be used, each defined in a separate
    Docker Compose file:'
  prefs: []
  type: TYPE_NORMAL
- en: Using RabbitMQ without the use of partitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using RabbitMQ with two partitions per topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kafka with two partitions per topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, before testing these three configurations, we need to add two features
    to be able to test the asynchronous processing:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving events for later inspection when using RabbitMQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A health API that can be used to monitor the state of the microservice landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After running some tests on event-driven asynchronous services, it might be
    of interest to see what events were actually sent. When using Spring Cloud Stream
    with Kafka, events are retained in the topics, even after consumers have processed
    them. However, when using Spring Cloud Stream with RabbitMQ, the events are removed
    after they have been processed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to see what events have been published on each topic, Spring Cloud
    Stream is configured to save published events in a separate consumer group, `auditGroup`,
    per topic. For the `products` topic, the configuration looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: When using RabbitMQ, this will result in extra queues being created where the
    events are stored for later inspection.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code, see the `application.yml` configuration file in the
    `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a health API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing a system landscape of microservices that uses a combination of synchronous
    APIs and asynchronous messaging is challenging.
  prefs: []
  type: TYPE_NORMAL
- en: For example, how do we know when a newly started landscape of microservices,
    together with their databases and messaging system, are ready to process requests
    and messages?
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to know when all the microservices are ready, we have added
    health APIs to the microservices. The health APIs are based on the support for
    **health endpoints** that comes with the Spring Boot module **Actuator**. By default,
    an Actuator-based health endpoint responds with `UP` (and gives 200 as the HTTP
    return status) if the microservice itself and all the dependencies Spring Boot
    knows about are available. Dependencies Spring Boot knows about include databases
    and messaging systems. If the microservice itself or any of its dependencies are
    not available, the health endpoint responds with `DOWN` (and returns 500 as the
    HTTP return status).
  prefs: []
  type: TYPE_NORMAL
- en: We can also extend health endpoints to cover dependencies that Spring Boot is
    not aware of. We will use this feature to extend to the product composite’s `health`
    endpoint, so it also includes the health of the three core services. This means
    that the product composite `health` endpoint will only respond with `UP` if itself
    and the three core microservices are healthy. This can be used either manually
    or automatically by the `test-em-all.bash` script to find out when all the microservices
    and their dependencies are up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ProductCompositeIntegration` class, we have added helper methods for
    checking the health of the three core microservices, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This code is similar to the code we used previously to call the core services
    to read APIs. Note that the health endpoint is, by default, set to `/actuator/health`.
  prefs: []
  type: TYPE_NORMAL
- en: For the full source code, see the `ProductCompositeIntegration` class in the
    `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the configuration class, `HealthCheckConfiguration`, we use these helper
    methods to register a composite health check using the Spring Actuator class `CompositeReactiveHealthContributor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: For the full source code, see the `HealthCheckConfiguration` class in the `product-composite`
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in the `application.yml` configuration file of all four microservices,
    we configure the Spring Boot Actuator so that it does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Shows details about the state of health, which not only includes `UP` or `DOWN`,
    but also information about its dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposes all its endpoints over HTTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The configuration for these two settings looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: For an example of the full source code, see the `application.yml` configuration
    file in the `product-composite` project.
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING**: These configuration settings are helpful during development, but
    it can be a security issue to reveal too much information in actuator endpoints
    in production systems. Therefore, plan to minimize the information exposed by
    the actuator endpoints in production!'
  prefs: []
  type: TYPE_NORMAL
- en: This can be done by replacing `"*"` with, for example, `health,info` in the
    setting of the `management.endpoints.web.exposure.include` property above.
  prefs: []
  type: TYPE_NORMAL
- en: For details regarding the endpoints that are exposed by Spring Boot Actuator,
    see [https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html](https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The health endpoint can be used manually with the following command (don’t
    try it yet, wait until we have started up the microservice landscape below!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in a response containing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B19825_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Health endpoint response'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see that the composite service reports that
    it is healthy; that is, its status is `UP`. At the end of the response, we can
    see that all three core microservices are also reported as healthy.
  prefs: []
  type: TYPE_NORMAL
- en: With a health API in place, we are ready to test our reactive microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Using RabbitMQ without using partitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will test the reactive microservices together with RabbitMQ
    but without using partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default `docker-compose.yml` Docker Compose file is used for this configuration.
    The following changes have been added to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RabbitMQ has been added, as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the declaration of RabbitMQ above, we can see that:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We use a Docker image for RabbitMQ v3.11.8 including the management plugin and
    Admin Web UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We expose the standard ports for connecting to RabbitMQ and the Admin Web UI,
    `5672` and `15672`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add a health check so that Docker can find out when RabbitMQ is ready to
    accept connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The microservices now have a dependency declared on the RabbitMQ service. This
    means that Docker will not start the microservice containers until the RabbitMQ
    service is reported to be healthy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To run manual tests, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build and start the system landscape with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we have to wait for the microservice landscape to be up and running. Try
    running the following command a few times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When it returns `UP`, we are ready to run our tests!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, create a composite product with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When using Spring Cloud Stream together with RabbitMQ, it will create one RabbitMQ
    exchange per topic and a set of queues, depending on our configuration. Let’s
    see what queues Spring Cloud Stream has created for us!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the following URL in a web browser: `http://localhost:15672/#/queues`.
    Log in with the default username/password `guest`/`guest`. You should see the
    following queues:![Graphical user interface, application, table  Description automatically
    generated](img/B19825_07_12.png)Figure 7.12: List of queues'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each topic, we can see one queue for the **auditGroup**, one queue for the
    consumer group that’s used by the corresponding core microservice, and one dead-letter
    queue. We can also see that the **auditGroup** queues contain messages, as expected!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on the **products.auditGroup** queue and scroll down to the **Get messages**
    section, expand it, and click on the button named **Get Message(s)** to see the
    message in the queue:![Graphical user interface, text, application, email  Description
    automatically generated](img/B19825_07_13.png)Figure 7.13: Viewing the message
    in the queue'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the preceding screenshot, note the **Payload** but also the header **partitionKey**,
    which we will use in the next section where we try out RabbitMQ with partitions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, try to get the product composite using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, delete it with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Try to get the deleted product again. It should result in a `404 - "NotFound"`
    response!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you look in the RabbitMQ audit queues again, you should be able to find new
    messages containing delete events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wrap up the test by bringing down the microservice landscape with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This completes the tests where we use RabbitMQ without partitions. Now, let’s
    move on and test RabbitMQ with partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Using RabbitMQ with partitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s try out the partitioning support in Spring Cloud Stream!
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a separate Docker Compose file prepared for using RabbitMQ with two
    partitions per topic: `docker-compose-partitions.yml`. It will also start two
    instances per core microservice, one for each partition. For example, a second
    `product` instance is configured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an explanation of the preceding configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the same source code and Dockerfile that we did for the first `product`
    instance but configure them differently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make all microservice instances aware that they will use partitions, we have
    added the Spring profile `streaming_partitioned` to their environment variable
    `SPRING_PROFILES_ACTIVE`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assign the two `product` instances to different partitions using different
    Spring profiles. The Spring profile `streaming_instance_0` is used by the first
    product instance and `streaming_instance_1` is used by the second instance, `product-p1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second `product` instance will only process asynchronous events; it will
    not respond to API calls. Since it has a different name, `product-p1` (also used
    as its DNS name), it will not respond to calls to a URL starting with `http://product:8080`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start up the microservice landscape with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Create a composite product in the same way as for the tests in the previous
    section but also create a composite product with the product ID set to `2`. If
    you take a look at the queues set up by Spring Cloud Stream, you will see one
    queue per partition and that the product audit queues now contain one message
    each; the event for product ID `1` was placed in one partition and the event for
    product ID `2` was placed in the other partition.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you go back to `http://localhost:15672/#/queues` in your web browser, you
    should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B19825_07_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: List of queues'
  prefs: []
  type: TYPE_NORMAL
- en: 'To end the test with RabbitMQ using partitions, bring down the microservice
    landscape with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We are now done with tests using RabbitMQ, both with and without partitions.
    The final test configuration we shall try out is testing the microservices together
    with Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kafka with two partitions per topic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we shall try out a very cool feature of Spring Cloud Stream: changing
    the messaging system from RabbitMQ to Apache Kafka!'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done simply by changing the value of the `spring.cloud.stream.defaultBinder`
    property from `rabbit` to `kafka`. This is handled by the `docker-compose-kafka.yml`
    Docker Compose file, which has also replaced RabbitMQ with Kafka and ZooKeeper.
    The configuration of Kafka and ZooKeeper looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Kafka is also configured to use two partitions per topic, and like before, we
    start up two instances per core microservice, one for each partition. See the
    Docker Compose file, `docker-compose-kafka.yml`, for details!
  prefs: []
  type: TYPE_NORMAL
- en: 'Start up the microservice landscape with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeat the tests from the previous section: create two products, one with the
    product ID set to `1` and one with the product ID set to `2`.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Kafka doesn’t come with any graphical tools that can be used
    to inspect topics, partitions, and the messages that are placed within them. Instead,
    we can run CLI commands in the Kafka Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see a list of topics, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect an output like the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B19825_07_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: Viewing a list of topics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what we see in the preceding output:'
  prefs: []
  type: TYPE_NORMAL
- en: The topics prefixed with `error` are the topics corresponding to dead-letter
    queues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will not find any `auditGroup` groups as in the case of RabbitMQ. Since
    events are retained in the topics by Kafka, even after consumers have processed
    them, there is no need for an extra `auditGroup` group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To see the partitions in a specific topic, for example, the `products` topic,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect an output like the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text  Description automatically generated](img/B19825_07_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Viewing partitions in the products topic'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see all the messages in a specific partition, for example, partition `1`
    in the `products` topic, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Expect an output like the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text  Description automatically generated](img/B19825_07_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Viewing all messages in partition 1 in the products topic'
  prefs: []
  type: TYPE_NORMAL
- en: The output will end with a timeout exception since we stop the command by specifying
    a timeout for the command of `1000` ms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bring down the microservice landscape with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have learned how Spring Cloud Stream can be used to switch a message
    broker from RabbitMQ to Kafka without requiring any changes in the source code.
    It just requires a few changes in the Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to the last section of this chapter, learning how to run these
    tests automatically!
  prefs: []
  type: TYPE_NORMAL
- en: Running automated tests of the reactive microservice landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to run tests of the reactive microservice landscape automatically
    instead of manually, the automated `test-em-all.bash` test script has been enhanced.
    The most important changes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script uses the new `health` endpoint to know when the microservice landscape
    is operational, as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The script has a new `waitForMessageProcessing()` function, which is called
    after the test data is set up. Its purpose is simply to wait for the creation
    of the test data to be completed by the asynchronous create services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To use the test script to automatically run the tests with RabbitMQ and Kafka,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the tests using the default Docker Compose file, that is, with RabbitMQ
    without partitions, with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the tests for RabbitMQ with two partitions per topic using the Docker Compose
    `docker-compose-partitions.yml` file with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, run the tests with Kafka and two partitions per topic using the Docker
    Compose `docker-compose-kafka.yml` file with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we have learned how to use the `test-em-all.bash` test script
    to automatically run tests of the reactive microservice landscape, which has been
    configured to use either RabbitMQ or Kafka as its message broker.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how we can develop reactive microservices!
  prefs: []
  type: TYPE_NORMAL
- en: Using Spring WebFlux and Spring WebClient, we can develop non-blocking synchronous
    APIs that can handle incoming HTTP requests and send outgoing HTTP requests without
    blocking any threads. Using Spring Data’s reactive support for MongoDB, we can
    also access MongoDB databases in a non-blocking way, that is, without blocking
    any threads while waiting for responses from the database. Spring WebFlux, Spring
    WebClient, and Spring Data rely on Project Reactor to provide their reactive and
    non-blocking features. When we must use blocking code, for example, when using
    Spring Data for JPA, we can encapsulate the processing of the blocking code by
    scheduling the processing of it in a dedicated thread pool.
  prefs: []
  type: TYPE_NORMAL
- en: We have also seen how Spring Data Stream can be used to develop event-driven
    asynchronous services that work on both RabbitMQ and Kafka as messaging systems
    without requiring any changes in the code. By doing some configuration, we can
    use features in Spring Cloud Stream such as consumer groups, retries, dead-letter
    queues, and partitions to handle the various challenges of asynchronous messaging.
  prefs: []
  type: TYPE_NORMAL
- en: We have also learned how to manually and automatically test a system landscape
    consisting of reactive microservices.
  prefs: []
  type: TYPE_NORMAL
- en: This was the final chapter on how to use fundamental features in Spring Boot
    and Spring Framework.
  prefs: []
  type: TYPE_NORMAL
- en: Next up is an introduction to Spring Cloud and how it can be used to make our
    services production-ready, scalable, robust, configurable, secure, and resilient!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is it important to know how to develop reactive microservices?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you choose between non-blocking synchronous APIs and event-/message-driven
    asynchronous services?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What makes an event different from a message?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name some challenges with message-driven asynchronous services. How do we handle
    them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the following test not failing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, ensure that the test fails. Next, correct the test so that it succeeds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What are the challenges of writing tests with reactive code using JUnit, and
    how can we handle them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
