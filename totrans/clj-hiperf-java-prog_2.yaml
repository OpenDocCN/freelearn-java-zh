- en: Part 2. Module 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分。模块2
- en: '**Clojure High Performance Programming, Second Edition**'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**《Clojure高性能编程，第二版》**'
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Become an expert at writing fast and high performant code in Clojure 1.7.0*'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*成为在Clojure 1.7.0中编写快速且高性能代码的专家*'
- en: Chapter 1. Performance by Design
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。设计性能
- en: Clojure is a safe, functional programming language that brings great power and
    simplicity to the user. Clojure is also dynamically and strongly typed, and has
    very good performance characteristics. Naturally, every activity performed on
    a computer has an associated cost. What constitutes acceptable performance varies
    from one use-case and workload to another. In today's world, performance is even
    the determining factor for several kinds of applications. We will discuss Clojure
    (which runs on the **JVM** (**Java Virtual Machine**)), and its runtime environment
    in the light of performance, which is the goal of this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure是一种安全、函数式编程语言，它为用户带来了巨大的力量和简洁性。Clojure也是动态和强类型化的，并且具有非常好的性能特性。自然地，计算机上进行的每一项活动都有相应的成本。构成可接受性能的因素因用例和工作负载而异。在当今世界，性能甚至成为几种类型应用程序的决定性因素。我们将从性能的角度讨论Clojure（它运行在**JVM**（**Java虚拟机**）上），以及其运行环境，这正是本书的目标。
- en: 'The performance of Clojure applications depend on various factors. For a given
    application, understanding its use cases, design and implementation, algorithms,
    resource requirements and alignment with the hardware, and the underlying software
    capabilities is essential. In this chapter, we will study the basics of performance
    analysis, including the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure应用程序的性能取决于各种因素。对于给定的应用程序，理解其用例、设计、实现、算法、资源需求和与硬件的匹配，以及底层软件能力是至关重要的。在本章中，我们将研究性能分析的基础，包括以下内容：
- en: Classifying the performance anticipations by the use cases types
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过用例类型对性能预期进行分类
- en: Outlining the structured approach to analyze performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述分析性能的结构化方法
- en: A glossary of terms, commonly used to discuss performance aspects
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一份术语表，通常用于讨论性能方面
- en: The performance numbers that every programmer should know
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个程序员都应该知道的性能数字
- en: Use case classification
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例分类
- en: The performance requirements and priority vary across the different kinds of
    use cases. We need to determine what constitutes acceptable performance for the
    various kinds of use cases. Hence, we classify them to identify their performance
    model. When it comes to details, there is no sure shot performance recipe for
    any kind of use case, but it certainly helps to study their general nature. Note
    that in real life, the use cases listed in this section may overlap with each
    other.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的用例的性能需求和优先级各不相同。我们需要确定各种类型用例的可接受性能构成。因此，我们将它们分类以识别其性能模型。在细节上，对于任何类型的用例，都没有一成不变的性能秘方，但研究它们的普遍性质肯定是有帮助的。请注意，在现实生活中，本节中列出的用例可能相互重叠。
- en: The user-facing software
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向用户的软件
- en: The performance of user-facing applications is strongly linked to the user's
    anticipation. Having a difference of a good number of milliseconds may not be
    perceptible for the user but at the same time, a wait of more than a few seconds
    may not be taken kindly. One important element in normalizing anticipation is
    to engage the user by providing duration-based feedback. A good idea to deal with
    such a scenario would be to start the task asynchronously in the background, and
    poll it from the UI layer to generate a duration-based feedback for the user.
    Another way could be to incrementally render the results to the user to even out
    the anticipation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 面向用户的软件性能与用户的预期紧密相关。差异可能有好几毫秒，对用户来说可能不明显，但与此同时，等待几秒钟以上可能不会受到欢迎。在正常化预期的一个重要元素是通过提供基于持续时间的反馈来吸引用户。处理此类场景的一个好主意是在后台异步启动任务，并从UI层轮询它以生成基于持续时间的用户反馈。另一种方法是对用户逐步渲染结果，以平衡预期。
- en: Anticipation is not the only factor in user facing performance. Common techniques
    like staging or precomputation of data, and other general optimization techniques
    can go a long way to improve the user experience with respect to performance.
    Bear in mind that all kinds of user facing interfaces fall into this use case
    category—the Web, mobile web, GUI, command line, touch, voice-operated, gesture...you
    name it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 预期并非用户界面性能的唯一因素。常见的技巧，如数据分阶段或预计算，以及其他一般优化技术，可以在很大程度上改善性能方面的用户体验。请记住，所有类型的用户界面都归属于此类用例范畴——网页、移动网页、图形用户界面、命令行、触摸、语音控制、手势……无论你叫它什么。
- en: Computational and data-processing tasks
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算和数据处理任务
- en: Non-trivial compute intensive tasks demand a proportional amount of computational
    resources. All of the CPU, cache, memory, efficiency and the parallelizability
    of the computation algorithms would be involved in determining the performance.
    When the computation is combined with distribution over a network or reading from/staging
    to disk, I/O bound factors come into play. This class of workloads can be further
    subclassified into more specific use cases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 非平凡的密集型计算任务需要相应数量的计算资源。所有CPU、缓存、内存、计算算法的效率和并行化都会涉及到性能的确定。当计算与网络分布或从磁盘读取/分阶段到磁盘结合时，I/O密集型因素就会发挥作用。这类工作负载可以进一步细分为更具体的用例。
- en: A CPU bound computation
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU密集型计算
- en: A CPU bound computation is limited by the CPU cycles spent on executing it.
    Arithmetic processing in a loop, small matrix multiplication, determining whether
    a number is a **Mersenne prime**, and so on, would be considered CPU bound jobs.
    If the algorithm complexity is linked to the number of iterations/operations *N*,
    such as *O(N)*, *O(N* *²)* and more, then the performance depends on how big *N*
    is, and how many CPU cycles each step takes. For parallelizable algorithms, performance
    of such tasks may be enhanced by assigning multiple CPU cores to the task. On
    virtual hardware, the performance may be impacted if the CPU cycles are available
    in bursts.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CPU密集型计算受限于执行它所花费的CPU周期。循环中的算术处理、小矩阵乘法、判断一个数是否为**梅森素数**等，都会被认为是CPU密集型工作。如果算法复杂度与迭代/操作次数*N*相关，例如*O(N)*，*O(N²)*等，那么性能取决于*N*的大小以及每一步所需的CPU周期数。对于可并行化的算法，可以通过分配多个CPU核心给任务来提高此类任务的性能。在虚拟硬件上，如果CPU周期是突发性的，性能可能会受到影响。
- en: A memory bound task
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存密集型任务
- en: A memory bound task is limited by the availability and bandwidth of the memory.
    Examples include large text processing, list processing, and more. For example,
    specifically in Clojure, the `(reduce f (pmap g coll))` operation would be memory
    bound if `coll` is a large sequence of big maps, even though we parallelize the
    operation using `pmap` here. Note that higher CPU resources cannot help when memory
    is the bottleneck, and vice versa. Lack of availability of memory may force you
    to process smaller chunks of data at a time, even if you have enough CPU resources
    at your disposal. If the maximum speed of your memory is *X* and your algorithm
    on single the core accesses the memory at speed *X/3*, the multicore performance
    of your algorithm cannot exceed three times the current performance, no matter
    how many CPU cores you assign to it. The memory architecture (for example, SMP
    and NUMA) contributes to the memory bandwidth in multicore computers. Performance
    with respect to memory is also subject to page faults.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 内存密集型任务受限于内存的可用性和带宽。例如，大文本处理、列表处理等。例如，在Clojure中，如果`coll`是一个由大映射组成的大序列，那么`(reduce
    f (pmap g coll))`操作将是内存密集型的，即使我们在这里使用`pmap`并行化操作。请注意，当内存成为瓶颈时，更高的CPU资源无法帮助，反之亦然。内存不可用可能迫使你一次处理更小的数据块，即使你有足够的CPU资源可用。如果你的内存最大速度是*X*，而你的算法在单个核心上以速度*X/3*访问内存，那么你的算法的多核性能不能超过当前性能的三倍，无论你分配多少CPU核心给它。内存架构（例如，SMP和NUMA）对多核计算机的内存带宽有贡献。与内存相关的性能也受页面错误的影响。
- en: A cache bound task
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓存密集型任务
- en: A task is cache bound when its speed is constrained by the amount of cache available.
    When a task retrieves values from a small number of repeated memory locations,
    for example a small matrix multiplication, the values may be cached and fetched
    from there. Note that CPUs (typically) have multiple layers of cache, and the
    performance will be at its best when the processed data fits in the cache, but
    the processing will still happen, more slowly, when the data does not fit into
    the cache. It is possible to make the most of the cache using **cache-oblivious**
    algorithms. A higher number of concurrent cache/memory bound threads than CPU
    cores is likely to flush the instruction pipeline, as well as the cache at the
    time of context switch, likely leading to a severely degraded performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个任务的速度受可用缓存量限制时，它就是缓存受限的。当一个任务从少量重复的内存位置检索值时，例如一个小矩阵乘法，这些值可能会被缓存并从那里获取。请注意，CPU（通常是）有多个缓存层，当处理的数据适合缓存时，性能将达到最佳，但当数据不适合缓存时，处理仍然会发生，但速度会慢一些。可以使用**缓存无关**算法最大限度地利用缓存。当并发缓存/内存受限线程的数量高于CPU核心数时，很可能会在上下文切换时清空指令流水线和缓存，这可能导致性能严重下降。
- en: An input/output bound task
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入/输出边界任务
- en: An **input/output** (**I/O**) bound task would go faster if the I/O subsystem,
    that it depends on, goes faster. Disk/storage and network are the most commonly
    used I/O subsystems in data processing, but it can be serial port, a USB-connected
    card reader, or any I/O device. An I/O bound task may consume very few CPU cycles.
    Depending on the speed of the device, connection pooling, data compression, asynchronous
    handling, application caching, and more, may help in performance. One notable
    aspect of I/O bound tasks is that performance is usually dependent on the time
    spent waiting for connection/seek, and the amount of serialization that we do,
    and hardly on the other resources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果依赖的I/O子系统运行得更快，那么**输入/输出**（**I/O**）边界任务会运行得更快。磁盘/存储和网络是数据处理中最常用的I/O子系统，但它可以是串行端口、USB连接的卡片阅读器或任何I/O设备。I/O边界任务可能消耗很少的CPU周期。根据设备速度、连接池、数据压缩、异步处理、应用缓存等，可能会有助于性能。I/O边界任务的一个显著方面是，性能通常取决于等待连接/查找的时间以及我们进行的序列化程度，而与其他资源关系不大。
- en: In practice, many data processing workloads are usually a combination of CPU
    bound, memory bound, cache bound, and I/O bound tasks. The performance of such
    mixed workloads effectively depends on the even distribution of CPU, cache, memory,
    and I/O resources over the duration of the operation. A bottleneck situation arises
    only when one resource gets too busy to make way for another.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，许多数据处理工作负载通常是CPU受限、内存受限、缓存受限和I/O受限任务的组合。这种混合工作负载的性能实际上取决于在整个操作期间CPU、缓存、内存和I/O资源的均匀分布。只有当某个资源变得过于繁忙，以至于无法为另一个资源让路时，才会出现瓶颈情况。
- en: Online transaction processing
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线事务处理
- en: '**Online transaction processing** (**OLTP**) systems process the business transactions
    on demand. They can sit behind systems such as a user-facing ATM machine, point-of-sale
    terminal, a network-connected ticket counter, ERP systems, and more. The OLTP
    systems are characterized by low latency, availability, and data integrity. They
    run day-to-day business transactions. Any interruption or outage is likely to
    have a direct and immediate impact on sales or service. Such systems are expected
    to be designed for resiliency rather than delayed recovery from failures. When
    the performance objective is unspecified, you may like to consider graceful degradation
    as a strategy.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线事务处理**（**OLTP**）系统按需处理业务交易。它们可以位于用户界面ATM机、销售点终端、网络连接的票务柜台、ERP系统等系统之后。OLTP系统以低延迟、可用性和数据完整性为特征。它们运行日常业务交易。任何中断或故障都可能对销售或服务产生直接和立即的影响。这些系统预计将被设计为具有弹性，而不是从故障中延迟恢复。当性能目标未指定时，您可能希望考虑优雅降级作为策略。'
- en: It is a common mistake to ask the OLTP systems to answer analytical queries,
    something that they are not optimized for. It is desirable for an informed programmer
    to know the capability of the system, and suggest design changes as per the requirements.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要求OLTP系统回答分析查询是一个常见的错误，它们并不是为此优化的。一个有经验的程序员了解系统的能力，并根据需求提出设计更改是可取的。
- en: Online analytical processing
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线分析处理
- en: '**Online analytical processing** (**OLAP**) systems are designed to answer
    analytical queries in a short time. They typically get data from the OLTP operations,
    and their data model is optimized for querying. They basically provide for consolidation
    (roll-up), drill-down and slicing and dicing of data for analytical purposes.
    They often use specialized data stores that can optimize ad-hoc analytical queries
    on the fly. It is important for such databases to provide pivot-table like capability.
    Often, the OLAP cube is used to get fast access to the analytical data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线分析处理**（**OLAP**）系统旨在短时间内回答分析查询。它们通常从OLTP操作中获取数据，并且其数据模型针对查询进行了优化。它们基本上提供数据合并（汇总）、钻取和切片切块，以用于分析目的。它们通常使用可以即时优化即席分析查询的特殊数据存储。对于此类数据库来说，提供类似交叉表的功能非常重要。通常，OLAP立方体用于快速访问分析数据。'
- en: Feeding the OLTP data into the OLAP systems may entail workflows and multistage
    batch processing. The performance concern of such systems is to efficiently deal
    with large quantities of data while also dealing with inevitable failures and
    recovery.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将OLTP数据输入到OLAP系统中可能涉及工作流和多阶段批量处理。这些系统的性能关注点是高效处理大量数据，同时处理不可避免的故障和恢复。
- en: Batch processing
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量处理
- en: '**Batch processing** is automated execution of predefined jobs. These are typically
    bulk jobs that are executed during off-peak hours. Batch processing may involve
    one or more stages of job processing. Often batch processing is clubbed with workflow
    automation, where some workflow steps are executed offline. Many of the batch
    processing jobs work on staging of data, and on preparing data for the next stage
    of processing to pick up.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量处理**是指预定义任务的自动化执行。这些通常是批量作业，在非高峰时段执行。批量处理可能涉及一个或多个作业处理阶段。通常，批量处理与工作流自动化结合使用，其中一些工作流步骤是在线执行的。许多批量处理作业处理数据阶段，并为下一阶段的处理准备数据。'
- en: Batch jobs are generally optimized for the best utilization of the computing
    resources. Since there is little to moderate the demand to lower the latencies
    of some particular subtasks, these systems tend to optimize for throughput. A
    lot of batch jobs involve largely I/O processing and are often distributed over
    a cluster. Due to distribution, the data locality is preferred when processing
    the jobs; that is, the data and processing should be local in order to avoid network
    latency in reading/writing data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 批量作业通常针对最佳计算资源利用率进行优化。由于对降低某些特定子任务的延迟需求很少或适中，这些系统倾向于优化吞吐量。许多批量作业涉及大量的I/O处理，并且通常分布在集群上。由于分布，处理作业时优先考虑数据局部性；也就是说，数据和处理应该是本地的，以避免在读写数据时的网络延迟。
- en: A structured approach to the performance
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能的有序方法
- en: In practice, the performance of non-trivial applications is rarely a function
    of coincidence or prediction. For many projects, performance is not an option
    (it is rather a necessity), which is why this is even more important today. Capacity
    planning, determining performance objectives, performance modeling, measurement,
    and monitoring are key.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，非平凡应用程序的性能很少是巧合或预测的结果。对于许多项目来说，性能不是一种选择（它更是一种必需品），这就是为什么今天这更加重要。容量规划、确定性能目标、性能建模、测量和监控是关键。
- en: Tuning a poorly designed system to perform is significantly harder, if not practically
    impossible, than having a system well-designed from the start. In order to meet
    a performance goal, performance objectives should be known before the application
    is designed. The performance objectives are stated in terms of latency, throughput,
    resource utilization, and workload. These terms are discussed in the following
    section in this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 调整设计不良的系统以实现性能，如果不说实际上不可能，那么至少比从一开始就设计良好的系统要困难得多。为了达到性能目标，在应用程序设计之前应该知道性能目标。性能目标用延迟、吞吐量、资源利用率和工作负载等术语来表述。这些术语将在本章下一节中讨论。
- en: The resource cost can be identified in terms of application scenarios, such
    as browsing of products, adding products to shopping cart, checkout, and more.
    Creating workload profiles that represent users performing various operations
    is usually helpful.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 资源成本可以根据应用场景来识别，例如浏览产品、将产品添加到购物车、结账等。创建代表用户执行各种操作的工作负载配置文件通常是有帮助的。
- en: '**Performance modeling** is a reality check for whether the application design
    will support the performance objectives. It includes performance objectives, application
    scenarios, constraints, measurements (benchmark results), workload objectives
    and if available, the performance baseline. It is not a replacement for measurement
    and load testing, rather, the model is validated using these. The performance
    model may include the performance test cases to assert the performance characteristics
    of the application scenarios.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能建模**是检查应用设计是否支持性能目标的一种现实检验。它包括性能目标、应用场景、约束、测量（基准结果）、工作负载目标，如果有的话，还有性能基线。它不是测量和负载测试的替代品，而是使用这些来验证模型。性能模型可能包括性能测试用例，以断言应用场景的性能特征。'
- en: Deploying an application to production almost always needs some form of **capacity
    planning**. It has to take into account the performance objectives for today and
    for the foreseeable future. It requires an idea of the application architecture,
    and an understanding of how the external factors translate into the internal workload.
    It also requires informed expectations about the responsiveness and the level
    of service to be provided by the system. Often, capacity planning is done early
    in a project to mitigate the risk of provisioning delays.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用程序部署到生产环境中几乎总是需要某种形式的**容量规划**。它必须考虑今天的性能目标和可预见的未来的性能目标。它需要了解应用程序架构，以及外部因素如何转化为内部工作负载。它还需要对系统提供的响应性和服务水平的了解。通常，容量规划在项目早期进行，以减轻配置延迟的风险。
- en: The performance vocabulary
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能词汇表
- en: There are several technical terms that are heavily used in performance engineering.
    It is important to understand these, as they form the cornerstone of the performance-related
    discussions. Collectively, these terms form a performance vocabulary. The performance
    is usually measured in terms of several parameters, where every parameter has
    roles to play—such parameters are a part of the vocabulary.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能工程中，有几个术语被广泛使用。理解这些术语非常重要，因为它们构成了性能相关讨论的基础。这些术语共同构成了一个性能词汇表。性能通常通过几个参数来衡量，每个参数都有其作用——这样的参数是词汇表的一部分。
- en: Latency
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟
- en: '**Latency** is the time taken by an individual unit of work to complete the
    task. It does not imply successful completion of a task. Latency is not collective,
    it is linked to a particular task. If two similar jobs—`j1` and `j2` took 3 ms
    and 5 ms respectively, their latencies would be treated as such. If `j1` and `j2`
    were dissimilar tasks, it would have made no difference. In many cases the average
    latency of similar jobs is used in the performance objectives, measurement, and
    monitoring results.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**延迟**是指单个工作单元完成任务所需的时间。它并不表示任务的顺利完成。延迟不是集体的，它与特定的任务相关联。如果两个类似的工作`j1`和`j2`分别耗时3毫秒和5毫秒，它们的延迟将如此处理。如果`j1`和`j2`是不同的任务，这就没有区别。在许多情况下，类似工作的平均延迟被用于性能目标、测量和监控结果。'
- en: Latency is an important indicator of the health of a system. A high performance
    system often thrives on low latency. Higher than normal latency can be caused
    due to load or bottleneck. It helps to measure the latency distribution during
    a load test. For example, if more than 25 percent of similar jobs, under a similar
    load, have significantly higher latency than others, then it may be an indicator
    of a bottleneck scenario that is worth investigating.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是衡量系统健康状况的重要指标。高性能系统通常依赖于低延迟。高于正常水平的延迟可能由负载或瓶颈引起。在负载测试期间测量延迟分布很有帮助。例如，如果超过25%的类似工作，在相似负载下，其延迟显著高于其他工作，那么这可能是值得调查的瓶颈场景的指标。
- en: When a task called `j1` consists of smaller tasks called `j2`, `j3`, and `j4`,
    the latency of `j1` is not necessarily the sum of the latencies of each of `j2`,
    `j3`, and `j4`. If any of the subtasks of `j1` are concurrent with another, the
    latency of `j1` will turn out to be less than the sum of the latencies of `j2`,
    `j3`, and `j4`. The I/O bound tasks are generally more prone to higher latency.
    In network systems, latency is commonly based on the round-trip to another host,
    including the latency from source to destination, and then back to source.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个名为`j1`的任务由名为`j2`、`j3`和`j4`的较小任务组成时，`j1`的延迟不一定是`j2`、`j3`和`j4`各自延迟的总和。如果`j1`的任何子任务与另一个任务并发，`j1`的延迟将小于`j2`、`j3`和`j4`延迟的总和。I/O受限的任务通常更容易出现更高的延迟。在网络系统中，延迟通常基于往返到另一个主机，包括从源到目的地的延迟，然后返回源。
- en: Throughput
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Throughput（吞吐量）
- en: '**Throughput** is the number of successful tasks or operations performed in
    a unit of time. The top-level operations performed in a unit of time are usually
    of a similar kind, but with a potentially different from latencies. So, what does
    throughput tell us about the system? It is the rate at which the system is performing.
    When you perform load testing, you can determine the maximum rate at which a particular
    system can perform. However, this is not a guarantee of the conclusive, overall,
    and maximum rate of performance.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**吞吐量**是在单位时间内完成的成功任务或操作的数量。在单位时间内执行的最顶层操作通常属于同一类型，但延迟可能不同。那么，吞吐量告诉我们关于系统的什么信息？它是系统执行的速度。当你进行负载测试时，你可以确定特定系统可以执行的最大速率。然而，这并不能保证结论性的、整体的和最大性能速率。'
- en: Throughput is one of the factors that determine the scalability of a system.
    The throughput of a higher level task depends on the capacity to spawn multiple
    such tasks in parallel, and also on the average latency of those tasks. The throughput
    should be measured during load testing and performance monitoring to determine
    the peak-measured throughput, and the maximum-sustained throughput. These factors
    contribute to the scale and performance of a system.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量是决定系统可扩展性的因素之一。较高层次任务的吞吐量取决于并行生成多个此类任务的能力，以及这些任务的平均延迟。吞吐量应在负载测试和性能监控期间进行测量，以确定峰值吞吐量和最大持续吞吐量。这些因素有助于系统的规模和性能。
- en: Bandwidth
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bandwidth（带宽）
- en: '**Bandwidth** is the raw data rate over a communication channel, measured in
    a certain number of bits per second. This includes not only the payload, but also
    all the overhead necessary to carry out the communication. Some examples are:
    Kbits/sec, Mbits/sec, and more. An uppercase B such as KB/sec denotes Bytes, as
    in kilobytes per second. Bandwidth is often compared to throughput. While bandwidth
    is the raw capacity, throughput for the same system is the successful task completion
    rate, which usually involves a round-trip. Note that throughput is for an operation
    that involves latency. To achieve maximum throughput for a given bandwidth, the
    communication/protocol overhead and operational latency should be minimal.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**带宽**是指通信通道上的原始数据速率，以每秒一定数量的比特来衡量。这包括不仅包括有效载荷，还包括执行通信所需的所有开销。一些例子包括：Kbits/sec，Mbits/sec，等等。大写B，如KB/sec表示字节，即每秒千字节。带宽通常与吞吐量进行比较。虽然带宽是原始容量，但对于同一系统，吞吐量是成功任务完成率，这通常涉及往返。请注意，吞吐量是指涉及延迟的操作。为了在给定的带宽下实现最大吞吐量，通信/协议开销和操作延迟应尽可能小。'
- en: For storage systems (such as hard disks, solid-state drives, and more) the predominant
    way to measure performance is **IOPS** (**Input-output per second**), which is
    multiplied by the transfer size and represented as bytes per second, or further
    into MB/sec, GB/sec, and more. IOPS is usually derived for sequential and random
    workloads for read/write operations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于存储系统（如硬盘、固态硬盘等），衡量性能的主要方式是**IOPS**（每秒输入输出），它是通过传输大小乘以的，表示为每秒字节数，或者进一步表示为MB/sec，GB/sec等等。IOPS通常用于顺序和随机工作负载的读写操作。
- en: 'Mapping the throughput of a system to the bandwidth of another may lead to
    dealing with an impedance mismatch between the two. For example, an order processing
    system may perform the following tasks:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个系统的吞吐量映射到另一个系统的带宽可能会导致处理两个系统之间的阻抗不匹配。例如，一个订单处理系统可能执行以下任务：
- en: Transact with the database on disk
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与磁盘上的数据库进行交易
- en: Post results over the network to an external system
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果通过网络发送到外部系统
- en: Depending on the bandwidth of the disk sub-system, the bandwidth of the network,
    and the execution model of order processing, the throughput may depend not only
    on the bandwidth of the disk sub-system and network, but also on how loaded they
    currently are. Parallelism and pipelining are common ways to increase the throughput
    over a given bandwidth.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据磁盘子系统的带宽、网络的带宽以及订单处理的执行模型，吞吐量可能不仅取决于磁盘子系统和网络的带宽，还取决于它们当前的负载情况。并行性和流水线是增加给定带宽吞吐量的常见方法。
- en: Baseline and benchmark
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准和基准测试
- en: The performance **baseline**, or simply baseline, is the reference point, including
    measurements of well-characterized and understood performance parameters for a
    known configuration. The baseline is used to collect performance measurements
    for the same parameters that we may benchmark later for another configuration.
    For example, collecting "throughput distribution over 10 minutes at a load of
    50 concurrent threads" is one such performance parameter that we can use for baseline
    and benchmarking. A baseline is recorded together with the hardware, network,
    OS and JVM configuration.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能基准**，或简称为基准，是参考点，包括对已知配置中良好定义和理解的性能参数的测量。基准用于收集我们可能稍后为另一个配置基准测试的相同参数的性能测量。例如，收集“在50个并发线程负载下10分钟内的吞吐量分布”是我们可以使用作基准和基准测试的这样一个性能参数。基准与硬件、网络、操作系统和JVM配置一起记录。'
- en: The performance **benchmark**, or simply benchmark, is the recording of the
    performance parameter measurements under various test conditions. A benchmark
    can be composed of a performance test suite. A benchmark may collect small to
    large amounts of data, and may take varying durations depending on the use-cases,
    scenarios, and environment characteristics.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能基准测试**，或简称为基准测试，是在各种测试条件下记录性能参数测量的过程。基准测试可以由性能测试套件组成。基准测试可能收集从小到大的数据量，并且可能根据用例、场景和环境特性而持续不同的时间。'
- en: A baseline is a result of the benchmark that was conducted at one point in time.
    However, a benchmark is independent of the baseline.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基准是某个时间点进行的基准测试的结果。然而，基准与基准无关。
- en: Profiling
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能分析
- en: '**Performance** **profiling** , or simply profiling, is the analysis of the
    execution of a program at its runtime. A program can perform poorly for a variety
    of reasons. A **profiler** can analyze and find out the execution time of various
    parts of the program. It is possible to put statements in a program manually to
    print the execution time of the blocks of code, but it gets very cumbersome as
    you try to refine the code iteratively.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能分析**，或简称为分析，是对程序在运行时执行的分析。程序可能由于各种原因表现不佳。分析器可以分析和找出程序各部分的执行时间。可以在程序中手动放置语句以打印代码块的执行时间，但随着您尝试迭代地改进代码，这会变得非常繁琐。'
- en: A profiler is of great assistance to the developer. Going by how profilers work,
    there are three major kinds—instrumenting, sampling, and event-based.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 分析器对开发者非常有帮助。根据分析器的工作原理，主要有三种类型——仪器化、采样和基于事件的。
- en: '**Event-based profilers**: These profilers work only for selected language
    platforms, and provide a good balance between the overhead and results; Java supports
    event-based profiling via the JVMTI interface.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于事件的性能分析器**：这些分析器仅适用于选定的语言平台，并在开销和结果之间提供良好的平衡；Java通过JVMTI接口支持基于事件的性能分析。'
- en: '**The instrumenting profilers**: These profilers modify code at either compile
    time, or runtime to inject performance counters. They are intrusive by nature
    and add significant performance overhead. However, you can profile the regions
    of code very selectively using the instrumenting profilers.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仪器化分析器**：这些分析器在编译时或运行时修改代码以注入性能计数器。它们本质上是侵入性的，并增加了显著的性能开销。然而，您可以使用仪器化分析器非常选择性地分析代码区域。'
- en: '**The sampling profilers**: These profilers pause the runtime and collect its
    state at "sampling intervals". By collecting enough samples, they get to know
    where the program is spending most of its time. For example, at a sampling interval
    of 1 millisecond, the profiler would have collected 1000 samples in a second.
    A sampling profiler also works for code that executes faster than the sampling
    interval (as in, the code may perform several iterations of work between the two
    sampling events), as the frequency of pausing and sampling is proportional to
    the overall execution time of any code.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样分析器**：这些分析器在“采样间隔”暂停运行时并收集其状态。通过收集足够的样本，它们可以了解程序大部分时间花在了哪里。例如，在1毫秒的采样间隔下，分析器在一秒钟内会收集1000个样本。采样分析器也适用于执行速度超过采样间隔的代码（即，代码可能在两次采样事件之间执行几个工作迭代），因为暂停和采样的频率与任何代码的整体执行时间成比例。'
- en: Profiling is not meant only for measuring execution time. Capable profilers
    can provide a view of memory analysis, garbage collection, threads, and more.
    A combination of such tools is helpful to find memory leaks, garbage collection
    issues, and so on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 分析的目的不仅仅是测量执行时间。有能力的分析器可以提供内存分析、垃圾回收、线程等方面的视图。这些工具的组合有助于找到内存泄漏、垃圾回收问题等。
- en: Performance optimization
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能优化
- en: Simply put, **optimization** is enhancing a program's resource consumption after
    a performance analysis. The symptoms of a poorly performing program are observed
    in terms of high latency, low throughput, unresponsiveness, instability, high
    memory consumption, high CPU consumption, and more. During the performance analysis,
    one may profile the program in order to identify the bottlenecks and tune the
    performance incrementally by observing the performance parameters.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，**优化**是在性能分析之后增强程序资源消耗的过程。性能不佳的程序的症状可以从高延迟、低吞吐量、无响应、不稳定、高内存消耗、高CPU消耗等方面观察到。在性能分析期间，一个人可以对程序进行性能分析，以确定瓶颈并通过观察性能参数逐步调整性能。
- en: Better and suitable algorithms are an all-around good way to optimize code.
    The CPU bound code can be optimized with computationally cheaper operations. The
    cache bound code can try using less memory lookups to keep a good hit ratio. The
    memory bound code can use an adaptive memory usage and conservative data representation
    to store in memory for optimization. The I/O bound code can attempt to serialize
    as little data as possible, and batching of operations will make the operation
    less chatty for better performance. Parallelism and distribution are other, overall
    good ways to increase performance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更好和合适的算法是优化代码的全方面好方法。CPU密集型代码可以通过计算成本更低的操作进行优化。缓存密集型代码可以尝试使用更少的内存查找来保持良好的命中率。内存密集型代码可以使用自适应内存使用和保守的数据表示来存储在内存中，以进行优化。I/O密集型代码可以尝试尽可能少地序列化数据，并且操作批处理将使操作更少地聊天，从而提高性能。并行性和分布式是其他，整体上好的提高性能的方法。
- en: Concurrency and parallelism
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发与并行性
- en: Most of the computer hardware and operating systems that we use today provide
    concurrency. On the x86 architecture, hardware support for concurrency can be
    traced as far back as the 80286 chip. **Concurrency** is the simultaneous execution
    of more than one process on the same computer. In older processors, concurrency
    was implemented using the context switch by the operating system kernel. When
    concurrent parts are executed in parallel by the hardware instead of merely the
    switching context, it is called **parallelism**. Parallelism is the property of
    the hardware, though the software stack must support it in order for you to leverage
    it in your programs. We must write your program in a concurrent way to exploit
    the parallelism features of the hardware.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天使用的绝大多数计算机硬件和操作系统都提供了并发性。在x86架构中，对并发的硬件支持可以追溯到80286芯片。**并发**是指在同一台计算机上同时执行多个进程。在较老的处理器中，并发是通过操作系统内核的上下文切换来实现的。当并发部分由硬件并行执行而不是仅仅切换上下文时，这被称为**并行性**。并行性是硬件的特性，尽管软件堆栈必须支持它，你才能在你的程序中利用它。我们必须以并发的方式编写你的程序，以利用硬件的并行性特性。
- en: While concurrency is a natural way to exploit hardware parallelism and speed
    up operations, it is worth bearing in mind that having significantly higher concurrency
    than the parallelism that your hardware can support is likely to schedule tasks
    to varying processor cores thereby, lowering the branch prediction and increasing
    cache misses.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并发是利用硬件并行性和加快操作的自然方式，但值得记住的是，如果并发显著高于硬件支持的并行度，可能会将任务调度到不同的处理器核心，从而降低分支预测并增加缓存未命中。
- en: At a low level, spawning the processes/threads, mutexes, semaphores, locking,
    shared memory, and interprocess communication are used for concurrency. The JVM
    has an excellent support for these concurrency primitives and interthread communication.
    Clojure has both—the low and higher level concurrency primitives that we will
    discuss in the concurrency chapter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在较低级别，使用进程/线程、互斥锁、信号量、锁定、共享内存和进程间通信来实现并发。JVM对这些并发原语和线程间通信有出色的支持。Clojure既有低级也有高级并发原语，我们将在并发章节中讨论。
- en: Resource utilization
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源利用率
- en: R**esource utilization** is the measure of the server, network, and storage
    resources that is consumed by an application. Resources include CPU, memory, disk
    I/O, network I/O, and more. The application can be analyzed in terms of CPU bound,
    memory bound, cache bound, and I/O bound tasks. Resource utilization can be derived
    by means of benchmarking, by measuring the utilization at a given throughput.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源利用率**是指应用程序消耗的服务器、网络和存储资源。资源包括CPU、内存、磁盘I/O、网络I/O等。可以从CPU密集型、内存密集型、缓存密集型和I/O密集型任务的角度分析应用程序。资源利用率可以通过基准测试和测量特定吞吐量下的利用率来得出。'
- en: Workload
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作负载
- en: '**Workload** is the quantification of how much work is there in hand to be
    carried out by the application. It is measured in the total numbers of users,
    the concurrent active users, the transaction volume, the data volume, and more.
    Processing a workload should take in to account the load conditions, such as how
    much data the database currently holds, how filled up the message queues are,
    the backlog of I/O tasks after which the new load will be processed, and more.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作负载**是指应用程序需要完成的工作量量化。它包括总用户数、并发活跃用户数、交易量、数据量等。处理工作负载时应考虑负载条件，例如数据库当前持有的数据量、消息队列的填充程度、I/O任务的积压情况以及更多。'
- en: The latency numbers that every programmer should know
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个程序员都应该知道的延迟数字
- en: 'Hardware and software have progressed over the years. Latencies for various
    operations put things in perspective. The latency numbers for the year 2015, reproduced
    with the permission of Aurojit Panda and Colin Scott of Berkeley University ([http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html](http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)).
    Latency numbers that every programmer should know are as shown in the following
    table:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，硬件和软件都取得了进步。各种操作的延迟使事情变得有对比性。以下表格展示了2015年的延迟数字，经加州大学伯克利分校的Aurojit Panda和Colin
    Scott许可复制([http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html](http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html))。每个程序员都应该知道的延迟数字如下所示：
- en: '| Operation | Time taken as of 2015 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 2015年所需时间 |'
- en: '| --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| L1 cache reference | 1ns (nano second) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| L1缓存引用 | 1ns (纳秒) |'
- en: '| Branch mispredict | 3 ns |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 分支预测错误 | 3 ns |'
- en: '| L2 cache reference | 4 ns |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| L2缓存引用 | 4 ns |'
- en: '| Mutex lock/unlock | 17 ns |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 互斥锁锁定/解锁 | 17 ns |'
- en: '| Compress 1KB with Zippy(Zippy/Snappy: [http://code.google.com/p/snappy/](http://code.google.com/p/snappy/))
    | 2μs (1000 ns = 1μs: micro second) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 使用Zippy(Zippy/Snappy: [http://code.google.com/p/snappy/](http://code.google.com/p/snappy/))压缩1KB
    | 2μs (1000 ns = 1μs: 微秒) |'
- en: '| Send 2000 bytes over the commodity network | 200ns (that is, 0.2μs) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 在商品网络上发送2000字节 | 200ns（即0.2μs）|'
- en: '| SSD random read | 16 μs |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SSD随机读取 | 16 μs |'
- en: '| Round-trip in the same datacenter | 500 μs |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 同一数据中心内的往返 | 500 μs |'
- en: '| Read 1,000,000 bytes sequentially from SSD | 200 μs |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 从SSD顺序读取1,000,000字节 | 200 μs |'
- en: '| Disk seek | 4 ms (1000 μs = 1 ms) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 磁盘寻道 | 4 ms (1000 μs = 1 ms) |'
- en: '| Read 1,000,000 bytes sequentially from disk | 2 ms |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 从磁盘顺序读取1,000,000字节 | 2 ms |'
- en: '| Packet roundtrip CA to Netherlands | 150 ms |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 数据包往返CA到荷兰 | 150 ms |'
- en: The preceding table shows the operations in a computer vis-a-vis the latency
    incurred due to the operation. When a CPU core processes some data in a CPU register,
    it may take a few CPU cycles (for reference, a 3 GHz CPU runs 3000 cycles per
    nanosecond), but the moment it has to fall back on L1 or L2 cache, the latency
    becomes thousands of times slower. The preceding table does not show main memory
    access latency, which is roughly 100 ns (it varies, based on the access pattern)—about
    25 times slower than the L2 cache.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格显示了计算机中的操作及其因操作而产生的延迟。当CPU核心在CPU寄存器中处理一些数据时，它可能需要几个CPU周期（以3 GHz CPU为例，每纳秒运行3000个周期），但一旦它必须回退到L1或L2缓存，延迟就会慢数千倍。前面的表格没有显示主内存访问延迟，大约为100纳秒（根据访问模式而变化）——大约是L2缓存的25倍。
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: We learned about the basics of what it is like to think more deeply about performance.
    We saw the common performance vocabulary, and also the use cases by which performance
    aspects might vary. We concluded by looking at the performance numbers for the
    different hardware components, which is how performance benefits reach our applications.
    In the next chapter, we will dive into the performance aspects of the various
    Clojure abstractions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了深入思考性能的基础知识。我们了解了常见的性能词汇，以及性能方面可能变化的用例。通过查看不同硬件组件的性能数据，我们得出了性能优势如何达到应用中的结论。在下一章中，我们将深入探讨各种Clojure抽象的性能方面。
- en: Chapter 2. Clojure Abstractions
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 Clojure 抽象
- en: Clojure has four founding ideas. Firstly, it was set up to be a functional language.
    It is not pure (as in purely functional), but emphasizes immutability. Secondly,
    it is a dialect of Lisp; Clojure is malleable enough that users can extend the
    language without waiting for the language implementers to add new features and
    constructs. Thirdly, it was built to leverage concurrency for the new generation
    challenges. Lastly, it was designed to be a hosted language. As of today, Clojure
    implementations exist for the JVM, CLR, JavaScript, Python, Ruby, and Scheme.
    Clojure blends seamlessly with its host language.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure有四个基本理念。首先，它被建立为一个函数式语言。它不是纯函数式（如纯粹函数式），但强调不可变性。其次，它是一种Lisp方言；Clojure足够灵活，用户可以在不等待语言实现者添加新特性和结构的情况下扩展语言。第三，它是为了利用并发来应对新一代挑战而构建的。最后，它被设计为托管语言。截至目前，Clojure的实现存在于JVM、CLR、JavaScript、Python、Ruby和Scheme上。Clojure与宿主语言无缝融合。
- en: 'Clojure is rich in abstractions. Though the syntax itself is very minimal,
    the abstractions are finely grained, mostly composable, and designed to tackle
    a wide variety of concerns in the least complicated way. In this chapter, we will
    discuss the following topics:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure在抽象方面非常丰富。尽管语法本身非常简洁，但抽象是细粒度的，大多数是可组合的，并且旨在以最简单的方式解决广泛的问题。在本章中，我们将讨论以下主题：
- en: Performance characteristics of non-numeric scalars
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非数值标量的性能特性
- en: Immutability and epochal time model paving the way for performance by isolation
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可变性和纪元时间模型通过隔离铺平了性能的道路
- en: Persistent data structures and their performance characteristics
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久数据结构和它们的性能特性
- en: Laziness and its impact on performance
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惰性及其对性能的影响
- en: Transients as a high-performance, short-term escape hatch
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时数据结构作为高性能、短期逃生通道
- en: Other abstractions, such as tail recursion, protocols/types, multimethods, and
    many more
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他抽象，如尾递归、协议/类型、多方法等
- en: Non-numeric scalars and interning
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非数值标量和内联
- en: 'Strings and characters in Clojure are the same as in Java. The string literals
    are implicitly interned. Interning is a way of storing only the unique values
    in the heap and sharing the reference everywhere it is required. Depending on
    the JVM vendor and the version of Java you use, the interned data may be stored
    in a string pool, Permgen, ordinary heap, or some special area in the heap marked
    for interned data. Interned data is subject to garbage collection when not in
    use, just like ordinary objects. Take a look at the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure中的字符串和字符与Java中的相同。字符串字面量是隐式内联的。内联是一种只存储唯一值在堆中并在需要的地方共享引用的方法。根据JVM供应商和您使用的Java版本，内联数据可能存储在字符串池、Permgen、普通堆或堆中标记为内联数据特殊区域。当不使用时，内联数据会像普通对象一样受到垃圾回收。请看以下代码：
- en: '[PRE0]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that `identical?` in Clojure is the same as `==` in Java. The benefit of
    interning a string is that there is no memory allocation overhead for duplicate
    strings. Commonly, applications on the JVM spend quite some time on string processing.
    So, it makes sense to have them interned whenever there is a chance of duplicate
    strings being simultaneously processed. Most of the JVM implementations today
    have an extremely fast intern operation; however, you should measure the overhead
    for your JVM if you have an older version.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Clojure 中的 `identical?` 与 Java 中的 `==` 是相同的。字符串池化的好处是对于重复的字符串没有内存分配的开销。通常，运行在
    JVM 上的应用程序在字符串处理上花费相当多的时间。因此，当有机会同时处理重复字符串时，将它们池化是有意义的。如今，大多数 JVM 实现都有一个非常快的池化操作；然而，如果你使用的是较旧版本，你应该测量
    JVM 的开销。
- en: Another benefit of string interning is that when you know that two string tokens
    are interned, you can compare them faster for equality using `identical?` than
    non-interned string tokens. The equivalence function `=` first checks for identical
    references before conducting a content check.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串池化的另一个好处是，当你知道两个字符串标记被池化时，你可以使用 `identical?` 比较它们以进行相等性检查，比非池化字符串标记更快。等价函数
    `=` 首先检查相同的引用，然后再进行内容检查。
- en: 'Symbols in Clojure always contain interned string references within them, so
    generating a symbol from a given string is nearly as fast as interning a string.
    However, two symbols created from the same string will not be identical:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 中的符号总是包含池化字符串引用，因此从给定字符串生成符号几乎与池化字符串一样快。然而，从同一字符串创建的两个符号不会是相同的：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Keywords are, on the basis of their implementation, built on top of symbols
    and are designed to work with the `identical?` function for equivalence. So, comparing
    keywords for equality using `identical?` would be faster, just as with interned
    string tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字基于其实现建立在符号之上，并设计为与 `identical?` 函数一起用于等价性。因此，使用 `identical?` 比较关键字进行相等性检查会更快，就像与池化字符串标记一样。
- en: Clojure is increasingly being used for large-volume data processing, which includes
    text and composite data structures. In many cases, the data is either stored as
    JSON or EDN ([http://edn-format.org](http://edn-format.org)). When processing
    such data, you can save memory by interning strings or using symbols/keywords.
    Remember that string tokens read from such data would not be automatically interned,
    whereas the symbols and keywords read from EDN data would invariably be interned.
    You may come across such situations when dealing with relational or NoSQL databases,
    web services, CSV or XML files, log parsing, and so on.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 越来越多地被用于大量数据处理，这包括文本和复合数据结构。在许多情况下，数据要么以 JSON 或 EDN（[http://edn-format.org](http://edn-format.org)）格式存储。在处理此类数据时，你可以通过池化字符串或使用符号/关键字来节省内存。记住，从此类数据中读取的字符串标记不会自动池化，而从
    EDN 数据中读取的符号和关键字则会不可避免地池化。当你处理关系型数据库或 NoSQL 数据库、Web 服务、CSV 或 XML 文件、日志解析等情况时，可能会遇到这种情况。
- en: Interning is linked to the JVM **Garbage Collection** (**GC**), which, in turn,
    is closely linked to performance. When you do not intern the string data and let
    duplicates exist, they end up being allocated on the heap. More heap usage leads
    to GC overhead. Interning a string has a tiny but measurable and upfront performance
    overhead, whereas GC is often unpredictable and unclear. GC performance, in most
    JVM implementations, has not increased in a similar proportion to the performance
    advances in hardware. So, often, effective performance depends on preventing GC
    from becoming the bottleneck, which in most cases means minimizing it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 池化与 JVM 的垃圾回收（**GC**）相关联，而垃圾回收又与性能密切相关。当你不池化字符串数据并允许重复存在时，它们最终会在堆上分配。更多的堆使用会导致
    GC 开销。池化字符串有一个微小但可测量且即时的性能开销，而 GC 往往是不可预测且不清晰的。在大多数 JVM 实现中，GC 性能并没有像硬件性能提升那样以相似的比例增长。因此，通常，有效的性能取决于防止
    GC 成为瓶颈，这在大多数情况下意味着最小化它。
- en: Identity, value, and epochal time model
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 身份、值和历法时间模型
- en: One of the principal virtues of Clojure is its simple design that results in
    malleable, beautiful composability. Using symbols in place of pointers is a programming
    practice that has existed for several decades now. It has found widespread adoption
    in several imperative languages. Clojure dissects that notion in order to uncover
    the core concerns that need to be addressed. The following subsections illustrate
    this aspect of Clojure.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure的一个主要优点是其简单的设计，这导致了可塑性和美丽的可组合性。用符号代替指针是一种存在了几十年的编程实践。它已经在几个命令式语言中得到广泛应用。Clojure剖析了这个概念，以揭示需要解决的核心问题。以下小节将说明Clojure的这一方面。
- en: We program using logical entities to represent values. For example, a value
    of `30` means nothing unless it is associated with a logical entity, let's say
    `age`. The logical entity `age` is the identity here. Now, even though `age` represents
    a value, the value may change with time; this brings us to the notion of `state`,
    which represents the value of the identity at a certain time. Hence, `state` is
    a function of time and is causally related to what we do in the program. Clojure's
    power lies in binding an identity with its value that holds true at the time and
    the identity remains isolated from any new value it may represent later. We will
    discuss state management in [Chapter 5](ch12.html "Chapter 5. Concurrency"), *Concurrency*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用逻辑实体来表示值。例如，`30`这个值如果没有与逻辑实体关联，比如`age`，就没有意义。逻辑实体`age`在这里是身份。现在，尽管`age`代表一个值，但这个值可能会随时间改变；这引出了`状态`的概念，它代表某个时间点的身份值。因此，`状态`是时间的函数，并且与我们在程序中执行的操作有因果关系。Clojure的力量在于将身份与其在特定时间保持为真的值绑定在一起，并且身份与其后来可能代表的任何新值保持隔离。我们将在[第5章](ch12.html
    "第5章。并发") *并发* 中讨论状态管理。
- en: Variables and mutation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量和修改
- en: If you have previously worked with an imperative language (C/C++, Java, and
    so on), you may be familiar with the concept of a variable. A **variable** is
    a reference to a block of memory. When we update its value, we essentially update
    the place in memory where the value is stored. The variable continues to point
    to the place where the older version of the value was stored. So, essentially,
    a variable is an alias for the place of storage of values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前使用过命令式语言（C/C++、Java等），你可能熟悉变量的概念。**变量**是对内存块的一个引用。当我们更新其值时，我们实际上是在更新存储值的内存位置。变量继续指向存储旧版本值的那个位置。所以，本质上，变量是存储值位置的别名。
- en: A little analysis would reveal that variables are strongly linked to the processes
    that read or mutate their values. Every mutation is a state transition. The processes
    that read/update the variable should be aware of the possible states of the variable
    to make sense of the state. Can you see a problem here? It conflates identity
    and state! It is impossible to refer to a value or a state in time when dealing
    with a variable—the value could change at any time unless you have complete control
    over the process accessing it. The mutability model does not accommodate the concept
    of time that causes its state transition.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一点分析就能揭示变量与读取或修改其值的进程之间有着强烈的联系。每一次修改都是一个状态转换。读取/更新变量的进程应该了解变量的可能状态，以便理解该状态。你在这里看到问题了吗？它混淆了身份和状态！在处理变量时，在时间上引用一个值或状态是不可能的——除非你完全控制访问它的进程，否则值可能会随时改变。可变模型无法容纳导致其状态转换的时间概念。
- en: The issues with mutability do not stop here. When you have a composite data
    structure containing mutable variables, the entire data structure becomes mutable.
    How can we mutate it without potentially undermining the other processes that
    might be observing it? How can we share this data structure with concurrent processes?
    How can we use this data structure as a key in a hash-map? This data structure
    does not convey anything. Its meaning could change with mutation! How do we send
    such a thing to another process without also compensating for the time, which
    can mutate it in different ways?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 可变性的问题并不止于此。当你有一个包含可变变量的复合数据结构时，整个数据结构就变得可变了。我们如何在不破坏可能正在观察它的其他进程的情况下修改它？我们如何与并发进程共享这个数据结构？我们如何将这个数据结构用作哈希表中的键？这个数据结构什么也没传达。它的意义可能会随着修改而改变！我们如何将这样的事物发送给另一个进程，而不补偿可能以不同方式修改它的时间？
- en: Immutability is an important tenet of functional programming. It not only simplifies
    the programming model, but also paves the way for safety and concurrency. Clojure
    supports immutability throughout the language. Clojure also supports fast, mutation-oriented
    data structures as well as thread-safe state management via concurrency primitives.
    We will discuss these topics in the forthcoming sections and chapters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 不可变性是函数式编程的一个重要原则。它不仅简化了编程模型，而且为安全和并发铺平了道路。Clojure 在整个语言中支持不可变性。Clojure 还支持通过并发原语实现快速、面向变动的数据结构以及线程安全的状态管理。我们将在接下来的章节中讨论这些主题。
- en: Collection types
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合类型
- en: 'There are a few types of collections in Clojure, which are categorized based
    on their properties. The following Venn diagram depicts this categorization on
    the basis of whether the collections are counted (so that `counted?` returns `true`)
    or associative (so that `associative?` returns `true`) or sequential (so that
    `sequential?` returns `true`):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 中有一些集合类型，它们根据其属性进行分类。以下维恩图根据集合是否计数（`counted?` 返回 `true`）、是否关联（`associative?`
    返回 `true`）或是否顺序（`sequential?` 返回 `true`）来描述这种分类：
- en: '![Collection types](img/B04596_02_01.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![集合类型](img/B04596_02_01.jpg)'
- en: The previous diagram illustrates the characteristics that different kinds of
    data structures share. The sequential structures let us iterate over the items
    in the collection, the item count of counted structures can be found constant
    with respect to time, and associative structures can be looked at with keys for
    corresponding values. The **CharSequence** box shows the character sequence Java
    types that can be converted to a Clojure sequence using (`seq charseq`).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图示展示了不同类型的数据结构所共有的特性。顺序结构允许我们对集合中的项目进行迭代，计数结构的项数可以随时间保持恒定，而关联结构可以通过键来查找相应的值。**CharSequence**
    框展示了可以转换为 Clojure 序列的 Java 字符序列类型（使用 `seq charseq`）。
- en: Persistent data structures
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久数据结构
- en: As we've noticed in the previous section, Clojure's data structures are not
    only immutable, but can produce new values without impacting the old version.
    Operations produce these new values in such a way that old values remain accessible;
    the new version is produced in compliance with the complexity guarantees of that
    data structure, and both the old and new versions continue to meet the complexity
    guarantees. The operations can be recursively applied and can still meet the complexity
    guarantees. Such immutable data structures as the ones provided by Clojure are
    called **persistent data structures**. They are "persistent", as in, when a new
    version is created, both the old and new versions "persist" in terms of both the
    value and complexity guarantee. They have nothing to do with storage or durability
    of data. Making changes to the old version doesn't impede working with the new
    version and vice versa. Both versions persist in a similar way.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中注意到的，Clojure 的数据结构不仅不可变，而且可以在不影响旧版本的情况下产生新值。操作以这种方式产生新值，使得旧值仍然可访问；新版本的产生符合该数据结构的复杂度保证，并且旧版本和新版本都继续满足复杂度保证。这些操作可以递归地应用，并且仍然可以满足复杂度保证。Clojure
    提供的这种不可变数据结构被称为 **持久数据结构**。它们是“持久”的，即当创建新版本时，旧版本和新版本在值和复杂度保证方面都“持续”存在。这与数据的存储或持久性无关。对旧版本的更改不会妨碍与新版本一起工作，反之亦然。两个版本都以类似的方式持续存在。
- en: Among the publications that have inspired the implementation of Clojure's persistent
    data structures, two of them are well known. Chris Okasaki's *Purely Functional
    Data Structures* has influenced the implementation of persistent data structures
    and lazy sequences/operations. Clojure's persistent queue implementation is adapted
    from Okasaki's *Batched Queues*. Phil Bagwell's *Ideal Hash Tries*, though meant
    for mutable and imperative data structures, was adapted to implement Clojure's
    persistent map/vector/set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在启发 Clojure 持久数据结构实现的出版物中，其中两种是众所周知的。Chris Okasaki 的 *Purely Functional Data
    Structures* 对持久数据结构和惰性序列/操作的实施产生了影响。Clojure 的持久队列实现是从 Okasaki 的 *Batched Queues*
    中改编的。Phil Bagwell 的 *Ideal Hash Tries*，尽管是为可变和命令式数据结构设计的，但被改编用于实现 Clojure 的持久映射/向量/集合。
- en: Constructing lesser-used data structures
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建较少使用的数据结构
- en: 'Clojure supports a well-known literal syntax for lists, vectors, sets, and
    maps. Shown in the following list are some less-used methods for creating other
    data structures:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure支持列表、向量、集合和映射的知名字面语法。以下列表展示了创建其他数据结构的较少使用的方法：
- en: 'Map (`PersistentArrayMap` and `PersistentHashMap`):'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '映射 (`PersistentArrayMap` 和 `PersistentHashMap`):'
- en: '[PRE2]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Sorted map (`PersistentTreeMap`):'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '排序映射 (`PersistentTreeMap`):'
- en: '[PRE3]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Sorted set (`PersistentTreeSet`):'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '排序集合 (`PersistentTreeSet`):'
- en: '[PRE4]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Queue (`PersistentQueue`):'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '队列 (`PersistentQueue`):'
- en: '[PRE5]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, abstractions such as `TreeMap` (sorted by key), `TreeSet` (sorted
    by element), and `Queue` should be instantiated by calling their respective APIs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，诸如 `TreeMap`（按键排序）、`TreeSet`（按元素排序）和 `Queue` 这样的抽象应该通过调用它们各自的API来实例化。
- en: Complexity guarantee
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂度保证
- en: 'The following table gives a summary of the complexity guarantees (using the
    Big-O notation) of various kinds of persistent data structures in Clojure:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格给出了Clojure中各种持久数据结构复杂度保证（使用大O符号）的摘要：
- en: '| Operation | PersistentList | PersistentHashMap | PersistentArrayMap | PersistentVector
    | PersistentQueue | PersistentTreeMap |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 持久列表 | 持久哈希映射 | 持久数组映射 | 持久向量 | 持久队列 | 持久树映射 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| `count` | O(1) | O(1) | O(1) | O(1) | O(1) | O(1) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `count` | O(1) | O(1) | O(1) | O(1) | O(1) | O(1) |'
- en: '| `conj` | O(1) |   |   | O(1) | O(1) |   |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `conj` | O(1) |   |   | O(1) | O(1) |   |'
- en: '| `first` | O(1) |   |   | O(<7) | O(<7) |   |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `first` | O(1) |   |   | O(<7) | O(<7) |   |'
- en: '| `rest` | O(1) |   |   | O(<7) | O(<7) |   |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `rest` | O(1) |   |   | O(<7) | O(<7) |   |'
- en: '| `doseq` | O(n) | O(n) | O(n) | O(n) | O(n) |   |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `doseq` | O(n) | O(n) | O(n) | O(n) | O(n) |   |'
- en: '| `nth` | O(n) |   |   | O(<7) | O(<7) |   |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `nth` | O(n) |   |   | O(<7) | O(<7) |   |'
- en: '| `last` | O(n) |   |   | O(n) | O(n) |   |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `last` | O(n) |   |   | O(n) | O(n) |   |'
- en: '| `get` |   | O(<7) | O(1) | O(<7) | O(<7) | O(log n) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `get` |   | O(<7) | O(1) | O(<7) | O(<7) | O(log n) |'
- en: '| `assoc` |   | O(<7) | O(1) | O(<7) |   | O(log n) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `assoc` |   | O(<7) | O(1) | O(<7) |   | O(log n) |'
- en: '| `dissoc` |   | O(<7) | O(1) | O(<7) |   | O(log n) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `dissoc` |   | O(<7) | O(1) | O(<7) |   | O(log n) |'
- en: '| `peek` |   |   |   | O(1) | O(1) |   |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `peek` |   |   |   | O(1) | O(1) |   |'
- en: '| `pop` |   |   |   | O(<7) | O(1) |   |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `pop` |   |   |   | O(<7) | O(1) |   |'
- en: A **list** is a sequential data structure. It provides constant time access
    for count and for anything regarding the first element only. For example, `conj`
    adds the element to the head and guarantees *O(1)* complexity. Similarly, `first`
    and `rest` provide *O(1)* guarantees too. Everything else provides an *O(n)* complexity
    guarantee.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表**是一种顺序数据结构。它为计数和与第一个元素相关的内容提供常数时间访问。例如，`conj` 将元素添加到头部并保证 *O(1)* 复杂度。同样，`first`
    和 `rest` 也提供 *O(1)* 保证。其他所有内容都提供 *O(n)* 复杂度保证。'
- en: Persistent hash-maps and vectors use the trie data structure with a branching
    factor of 32 under the hood. So, even though the complexity is *O(log* *[32]*
    *n)*, only 2^(32) hash codes can fit into the trie nodes. Hence, log[32] 2^(32),
    which turns out to be `6.4` and is less than `7`, is the worst-case complexity
    and can be considered near-constant time. As the trie grows larger, the portion
    to copy gets proportionately tiny due to structure sharing. Persistent hash-set
    implementation is also based on hash-map; hence, the hash-sets share the characteristics
    of the hash-maps. In a persistent vector, the last incomplete node is placed at
    the tail, which is always directly accessible from the root. This makes using
    `conj` to the end a constant time operation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 持久哈希映射和向量在底层使用32个分支因子的trie数据结构。因此，尽管复杂度是 *O(log* *[32]* *n)*，但只有2^(32)个哈希码可以放入trie节点中。因此，log[32]
    2^(32)，结果是 `6.4` 并且小于 `7`，是最坏情况下的复杂度，可以认为是接近常数时间。随着trie的增长，由于结构共享，要复制的部分成比例地变得很小。持久哈希集实现也是基于哈希映射的；因此，哈希集与哈希映射具有相同的特征。在持久向量中，最后一个不完整的节点放置在尾部，这总是可以从根直接访问。这使得使用
    `conj` 到末尾的操作是常数时间操作。
- en: Persistent tree-maps and tree-sets are basically sorted maps and sets respectively.
    Their implementation uses red-black trees and is generally more expensive than
    hash-maps and hash-sets. A persistent queue uses a persistent vector under the
    hood for adding new elements. Removing an element from a persistent queue takes
    the head off `seq`, which is created from the vector where new elements are added.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 持久性树映射和树集基本上是分别按顺序排列的映射和集合。它们的实现使用红黑树，通常比哈希映射和哈希集更昂贵。持久性队列在底层使用持久性向量来添加新元素。从持久性队列中删除一个元素需要从向量中移除头部
    `seq`，该向量是从添加新元素的位置创建的。
- en: The complexity of an algorithm over a data structure is not an absolute measure
    of its performance. For example, working with hash-maps involves computing the
    hashCode, which is not included in the complexity guarantee. Our choice of data
    structures should be based on the actual use case. For example, when should we
    use a list instead of a vector? Probably when we need sequential or **last-in-first-out**
    (**LIFO**) access, or when constructing an **abstract-syntax-tree** (**AST**)
    for a function call.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在数据结构上的复杂度并不是其性能的绝对度量。例如，使用哈希表涉及计算 hashCode，这并不包括在复杂度保证中。我们应该根据实际用例来选择数据结构。例如，我们应该在什么情况下使用列表而不是向量？可能是在我们需要顺序或
    **后进先出** (**LIFO**) 访问时，或者当为函数调用构造 **抽象语法树** (**AST**) 时。
- en: O(<7) implies near constant time
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: O(<7) 表示接近常数时间
- en: You may know that the **Big-O** notation is used to express the upper bound
    (worst case) of the efficiency of any algorithm. The variable *n* is used to express
    the number of elements in the algorithm. For example, a binary search on a sorted
    associative collection, such as a sorted vector, is a logarithmic time, that is
    an *O(log* *[2]* *n)* or simply an *O(log n)* algorithm. Since there can be a
    maximum of 2^(32) (technically 2^(31) due to a signed positive integer) elements
    in a Java collection and log[2] 2^(32) is 32, the binary search can be *O(≤32)*
    in the worst case. Similarly, though operations on persistent collections are
    O(log[32] n), in the worst case they actually turn out to be O(log[32] 2^(32))
    at maximum, which is *O(<7)*. Note that this is much lower than logarithmic time
    and approaches near constant time. This implies not so bad performance for persistent
    collections even in the worst possible scenario.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能知道，**大O** 表示法用于表达任何算法效率的上限（最坏情况）。变量 *n* 用于表示算法中的元素数量。例如，在一个排序的关联集合上的二分搜索，如排序向量，是对数时间，即
    *O(log* *[2]* *n*) 或简单地 *O(log n*) 算法。由于 Java 集合中最多可以有 2^(32)（技术上由于有符号正整数，为 2^(31)）个元素，且
    log[2] 2^(32) 是 32，因此二分搜索在最坏情况下可以是 *O(≤32)*。同样，尽管持久集合的操作是 O(log[32] n)，但在最坏情况下实际上最多是
    O(log[32] 2^(32))，即 *O(<7)*。请注意，这比对数时间低得多，接近常数时间。这意味着即使在最坏的情况下，持久集合的性能也不是很糟糕。
- en: The concatenation of persistent data structures
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久数据结构的连接
- en: 'While persistent data structures have excellent performance characteristics,
    the concatenation of two persistent data structures has been a linear time *O(N)*
    operation, except for some recent developments. The `concat` function, as of Clojure
    1.7, still provides linear time concatenation. Experimental work on **Relaxed
    Radix Balanced** (**RRB**) trees is going on in the **core.rrb-vector** contrib
    project ([https://github.com/clojure/core.rrb-vector](https://github.com/clojure/core.rrb-vector)),
    which may provide logarithmic time *O(log N)* concatenation. Readers interested
    in the details should refer to the following links:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管持久数据结构具有出色的性能特性，但两个持久数据结构的连接一直是一个线性时间 *O(N)* 操作，除了最近的一些发展。截至 Clojure 1.7，`concat`
    函数仍然提供线性时间连接。在 **core.rrb-vector** 贡献项目中正在进行对 **Relaxed Radix Balanced** (**RRB**)
    树的实验工作 ([https://github.com/clojure/core.rrb-vector](https://github.com/clojure/core.rrb-vector))，这可能提供对数时间
    *O(log N)* 连接。对细节感兴趣的读者应参考以下链接：
- en: The RRB-trees paper at [http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf](http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RRB-trees 论文在 [http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf](http://infoscience.epfl.ch/record/169879/files/RMTrees.pdf)
- en: Phil Bagwell's talk at [http://www.youtube.com/watch?v=K2NYwP90bNs](http://www.youtube.com/watch?v=K2NYwP90bNs)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phil Bagwell 的演讲在 [http://www.youtube.com/watch?v=K2NYwP90bNs](http://www.youtube.com/watch?v=K2NYwP90bNs)
- en: Tiark Rompf's talk at [http://skillsmatter.com/podcast/scala/fast-concatenation-immutable-vectors](http://skillsmatter.com/podcast/scala/fast-concatenation-immutable-vectors)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiark Rompf 在 [http://skillsmatter.com/podcast/scala/fast-concatenation-immutable-vectors](http://skillsmatter.com/podcast/scala/fast-concatenation-immutable-vectors)
    的演讲
- en: Sequences and laziness
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列和惰性
- en: '|   | *"A seq is like a logical cursor."* |   |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|   | *"一个序列就像一个逻辑游标。" |   |'
- en: '|   | --*Rich Hickey* |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|   | --*Rich Hickey* |'
- en: '**Sequences** (commonly known as **seqs**) are a way to sequentially consume
    a succession of data. As with iterators, they let a user begin consuming elements
    from the head and proceed realizing one element after another. However, unlike
    iterators, sequences are immutable. Also, since sequences are only a view of the
    underlying data, they do not modify the storage structure of the data.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列**（通常称为**seqs**）是按顺序消费一系列数据的一种方式。与迭代器类似，它们允许用户从头部开始消费元素，并逐个实现一个元素。然而，与迭代器不同，序列是不可变的。此外，由于序列只是底层数据的视图，它们不会修改数据的存储结构。'
- en: What makes sequences stand apart is they are not data structures per se; rather,
    they are a data abstraction over a stream of data. The data may be produced by
    an algorithm or a data source connected to an I/O operation. For example, the
    `resultset-seq` function accepts a `java.sql.ResultSet` JDBC instance as an argument
    and produces lazily realized rows of data as `seq`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使序列与众不同的地方在于，它们本身不是数据结构；相反，它们是对数据流的数据抽象。数据可能由算法或与I/O操作连接的数据源产生。例如，`resultset-seq`函数接受一个`java.sql.ResultSet`
    JDBC实例作为参数，并以`seq`的形式产生惰性实现的数据行。
- en: Clojure data structures can be turned into sequences using the `seq` function.
    For example, (`seq [:a :b :c :d]`) returns a sequence. Calling `seq` over an empty
    collection returns nil.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure数据结构可以通过`seq`函数转换为序列。例如，（`seq [:a :b :c :d]`）返回一个序列。对空集合调用`seq`返回nil。
- en: 'Sequences can be consumed by the following functions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 序列可以通过以下函数进行消费：
- en: '`first`: This returns the head of the sequence'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first`：这个函数返回序列的头部。'
- en: '`rest`: This returns the remaining sequence, even if it''s empty, after removing
    the head'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rest`：这个函数返回移除头部后的剩余序列，即使它是空的。'
- en: '`next`: This returns the remaining sequence or nil, if it''s empty, after removing
    the head'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next`：这个函数返回移除头部后的剩余序列或空，如果它是空的。'
- en: Laziness
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 惰性
- en: Clojure is a strict (as in, the opposite of "lazy") language, which can choose
    to explicitly make use of laziness when required. Anybody can create a lazily
    evaluated sequence using the `lazy-seq` macro. Some Clojure operations over collections,
    such as `map`, `filter`, and more are intentionally lazy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure是一种严格的（即“惰性”的对立面）语言，可以在需要时显式地利用惰性。任何人都可以使用`lazy-seq`宏创建一个惰性评估的序列。一些Clojure对集合的操作，如`map`、`filter`等，都是有意为之的惰性操作。
- en: '**Laziness** simply means that the value is not computed until actually required.
    Once the value is computed, it is cached so that any future reference to the value
    need not re-compute it. The caching of the value is called **memoization**. Laziness
    and memoization often go hand in hand.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**惰性**简单地说，就是值在真正需要时才计算。一旦值被计算，它就会被缓存，以便任何未来的值引用都不需要重新计算。值的缓存称为**记忆化**。惰性和记忆化常常是相辅相成的。'
- en: Laziness in data structure operations
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据结构操作中的惰性
- en: 'Laziness and memoization together form an extremely useful combination to keep
    the single-threaded performance of functional algorithms comparable to its imperative
    counterparts. For an example, consider the following Java code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性和记忆化结合使用，可以形成一个极其有用的组合，以保持函数式算法的单线程性能与其命令式对应物相当。例如，考虑以下Java代码：
- en: '[PRE6]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As is clear from the preceding snippet, it has a linear time complexity, that
    is, *O(n)*, and the whole operation is performed in a single pass. The comparable
    Clojure code is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码片段中可以看出，它具有线性时间复杂度，即*O(n)*，整个操作都在单次遍历中完成。与之相当的Clojure代码如下：
- en: '[PRE7]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, since we know `map` and `filter` are lazy, we can deduce that the Clojure
    version also has linear time complexity, that is, *O(n)*, and finishes the task
    in one pass with no significant memory overhead. Imagine, for a moment, that `map`
    and `filter` are not lazy—what would be the complexity then? How many passes would
    it make? It's not just that map and filter would both have taken one pass, that
    is, *O(n)*, each; they would each have taken as much memory as the original collection
    in the worst case, due to storing the intermediate results.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们知道`map`和`filter`是惰性的，我们可以推断Clojure版本也具有线性时间复杂度，即*O(n)*，并且在一个遍历中完成任务，没有显著的内存开销。想象一下，如果`map`和`filter`不是惰性的，那么复杂度会是什么？需要多少次遍历？不仅仅是map和filter各自都只进行了一次遍历，即*O(n)*，每个；在最坏的情况下，它们各自会占用与原始集合一样多的内存，因为需要存储中间结果。
- en: It is important to know the value of laziness and memoization in an immutability-emphasizing
    functional language such as Clojure. They form a basis for **amortization** in
    persistent data structures, which is about focusing on the overall performance
    of a composite operation instead of microanalyzing the performance of each operation
    in it; the operations are tuned to perform faster in those operations that matter
    the most.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在强调不可变性的函数式语言Clojure中，了解惰性和缓存的重要性非常重要。它们是持久数据结构中**摊销**的基础，这涉及到关注复合操作的整体性能，而不是微观分析其中每个操作的性能；操作被调整以在最重要的操作中更快地执行。
- en: Another important bit of detail is that when a lazy sequence is realized, the
    data is memoized and stored. On the JVM, all the heap references that are reachable
    in some way are not garbage collected. So, as a consequence, the entire data structure
    is kept in the memory unless you lose the head of the sequence. When working with
    lazy sequences using local bindings, make sure you don't keep referring to the
    lazy sequence from any of the locals. When writing functions that may accept lazy
    sequence(s), take care that any reference to the lazy `seq` does not outlive the
    execution of the function in the form of a closure or some such.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的细节是，当惰性序列被实现时，数据会被缓存并存储。在JVM上，所有以某种方式可达的堆引用都不会被垃圾回收。因此，结果就是，整个数据结构除非你丢失序列的头部，否则会一直保留在内存中。当使用局部绑定处理惰性序列时，确保你不会从任何局部变量中持续引用惰性序列。当编写可能接受惰性序列（s）的函数时，注意任何对惰性`seq`的引用都不应该超出函数执行的寿命，无论是以闭包或其他形式。
- en: Constructing lazy sequences
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建惰性序列
- en: 'Now that we know what lazy sequences are, let''s try to create a retry counter
    that should return true only as many times as the retry can be performed. This
    is shown in the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了惰性序列是什么，让我们尝试创建一个重试计数器，它应该只返回重试可以执行次数的次数。这在上面的代码中有所展示：
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `lazy-seq` macro makes sure that the stack is not used for recursion. We
    can see that this function would return endless values. Hence, in order to inspect
    what it returns, we should limit the number of elements as shown in the following
    code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`lazy-seq`宏确保栈不被用于递归。我们可以看到这个函数会返回无限值。因此，为了检查它返回的内容，我们应该限制元素的数量，如下面的代码所示：'
- en: '[PRE9]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let''s try using it in a mock fashion:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试以模拟的方式使用它：
- en: '[PRE10]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As expected, the output should print `Retrying` five times before printing
    `No more retries` and exiting as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，输出应该打印`Retrying`五次，然后打印`No more retries`并退出，如下所示：
- en: '[PRE11]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s take another simpler example of constructing a lazy sequence, which
    gives us a countdown from a specified number to zero:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再举一个更简单的例子来构建一个惰性序列，它从指定的数字倒数到零：
- en: '[PRE12]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can inspect the values it returns as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下检查它返回的值：
- en: '[PRE13]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Lazy sequences can loop indefinitely without exhausting the stack and can come
    in handy when working with other lazy operations. To maintain a balance between
    space-saving and performance, consuming lazy sequences results in the chunking
    of elements by a factor of 32\. That means lazy seqs are realized in a chunk-size
    of 32, even though they are consumed sequentially.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性序列可以无限循环而不会耗尽栈，当与其他惰性操作一起工作时可能会很有用。为了在节省空间和性能之间保持平衡，消费惰性序列会导致元素以32的倍数分块。这意味着即使它们是顺序消费的，惰性序列也是以32的块大小实现的。
- en: Custom chunking
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自定义分块
- en: 'The default chunk size 32 may not be optimum for all lazy sequences—you can
    override the chunking behavior when you need to. Consider the following snippet
    (adapted from Kevin Downey''s public gist at [https://gist.github.com/hiredman/324145](https://gist.github.com/hiredman/324145)):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的块大小32可能不是所有惰性序列的最佳选择——当你需要时可以覆盖分块行为。考虑以下片段（改编自Kevin Downey在[https://gist.github.com/hiredman/324145](https://gist.github.com/hiredman/324145)的公开gist）：
- en: '[PRE14]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As per the previous snippet, the user is allowed to pass a chunk size that
    is used to produce the lazy sequence. A larger chunk size may be useful when processing
    large text files, such as when processing CSV or log files. You would notice the
    following four less-known functions used in the snippet:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的片段，用户可以传递一个块大小，该大小用于生成惰性序列。较大的块大小在处理大型文本文件时可能很有用，例如在处理CSV或日志文件时。你会在片段中注意到以下四个不太为人所知的函数：
- en: '`clojure.core/chunk-cons`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clojure.core/chunk-cons`'
- en: '`clojure.core/chunk-buffer`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clojure.core/chunk-buffer`'
- en: '`clojure.core/chunk-append`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clojure.core/chunk-append`'
- en: '`clojure.core/chunk`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clojure.core/chunk`'
- en: While `chunk-cons` is the equivalent of `clojure.core/cons` for chunked sequences,
    `chunk-buffer` creates a mutable chunk buffer (controls the chunk size), `chunk-append`
    appends an item to the end of a mutable chunk buffer, and chunk turns a mutable
    chunk buffer into an immutable chunk.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `chunk-cons` 是分块序列中 `clojure.core/cons` 的等价物，但 `chunk-buffer` 创建一个可变的分块缓冲区（控制分块大小），`chunk-append`
    将一个项目追加到可变分块缓冲区的末尾，而分块将可变分块缓冲区转换为不可变分块。
- en: 'The `clojure.core` namespace has several functions related to chunked sequences
    listed as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`clojure.core` 命名空间中列出了与分块序列相关的几个函数，如下所示：'
- en: '`chunk`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk`'
- en: '`chunk-rest`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk-rest`'
- en: '`chunk-cons`'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk-cons`'
- en: '`chunk-next`'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk-next`'
- en: '`chunk-first`'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk-first`'
- en: '`chunk-append`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk-append`'
- en: '`chunked-seq?`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunked-seq?`'
- en: '`chunk-buffer`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk-buffer`'
- en: These functions are not documented, so although I would encourage you to study
    their source code to understand what they do, I would advise you not to make any
    assumptions about their support in future Clojure versions.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数没有文档说明，所以我鼓励你研究它们的源代码以了解它们的功能，但我建议你不要对未来 Clojure 版本中它们的支持做出任何假设。
- en: Macros and closures
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 宏和闭包
- en: 'Often, we define a macro so as to turn the parameter body of code into a closure
    and delegate it to a function. See the following example:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们定义一个宏，以便将代码参数体转换为闭包并将其委托给函数。请看以下示例：
- en: '[PRE15]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When using such code, if the body binds a local to a lazy sequence it may be
    retained longer than necessary, likely with bad consequences on memory consumption
    and performance. Fortunately, this can be easily fixed:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用此类代码时，如果主体将局部变量绑定到惰性序列，它可能比必要的保留时间更长，这可能会对内存消耗和性能产生不良影响。幸运的是，这可以很容易地修复：
- en: '[PRE16]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Notice the `^:once` hint and the `fn*` macro, which make the Clojure compiler
    clear the closed-over references, thus avoiding the problem. Let''s see this in
    action (Alan Malloy''s example from [https://groups.google.com/d/msg/clojure/Ys3kEz5c_eE/3St2AbIc3zMJ](https://groups.google.com/d/msg/clojure/Ys3kEz5c_eE/3St2AbIc3zMJ)):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `^:once` 提示和 `fn*` 宏，这使得 Clojure 编译器清除闭包引用，从而避免问题。让我们看看这个例子（来自 Alan Malloy
    的 [https://groups.google.com/d/msg/clojure/Ys3kEz5c_eE/3St2AbIc3zMJ](https://groups.google.com/d/msg/clojure/Ys3kEz5c_eE/3St2AbIc3zMJ)）：
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The manifestation of the previous condition depends on the available heap space.
    This issue is tricky to detect as it only raises `OutOfMemoryError`, which is
    easy to misunderstand as a heap space issue instead of a memory leak. As a preventive
    measure, I would suggest using `^:once` with `fn*` in all cases where you close
    over any potentially lazy sequence.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 前述条件的体现取决于可用的堆空间。这个问题很难检测，因为它只会引发 `OutOfMemoryError`，这很容易被误解为堆空间问题而不是内存泄漏。作为预防措施，我建议在所有关闭任何可能惰性序列的情况下使用
    `^:once` 与 `fn*`。
- en: Transducers
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换器
- en: Clojure 1.7 introduced a new abstraction called transducers for "composable
    algorithmic transformations", commonly used to apply a series of transformations
    over collections. The idea of transducers follows from the **reducing function**,
    which accepts arguments of the form (`result, input`) and returns `result`. A
    reducing function is what we typically use with reduce. A **transducer** accepts
    a reducing function, wraps/composes over its functionality to provide something
    extra, and returns another reducing function.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 1.7 引入了一个名为转换器的新抽象，用于“可组合的算法转换”，通常用于在集合上应用一系列转换。转换器的想法源于**减少函数**，它接受形式为
    (`result, input`) 的参数并返回 `result`。减少函数是我们通常与 `reduce` 一起使用的。**转换器**接受一个减少函数，将其功能包装/组合以提供额外的功能，并返回另一个减少函数。
- en: The functions in `clojure.core` that deal with collections have acquired an
    `arity-1` variant, which returns a transducer, namely `map`, `cat`, `mapcat`,
    `filter`, `remove`, `take`, `take-while`, `take-nth`, `drop`, `drop-while`, `replace`,
    `partition-by`, `partition-all`, `keep`, `keep-indexed`, `dedupe` and `random-sample`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`clojure.core` 中处理集合的函数已经获得了一个 `arity-1` 变体，它返回一个转换器，即 `map`、`cat`、`mapcat`、`filter`、`remove`、`take`、`take-while`、`take-nth`、`drop`、`drop-while`、`replace`、`partition-by`、`partition-all`、`keep`、`keep-indexed`、`dedupe`
    和 `random-sample`。'
- en: 'Consider the following few examples, all of which do the same thing:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下几个示例，它们都做同样的事情：
- en: '[PRE18]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, (`filter odd?`) returns a transducer—in the first example the transducer
    wraps over the reducer function `+` to return another combined reducing function.
    While we use the ordinary `reduce` function in the first example, in the second
    example we use the `transduce` function that accepts a transducer as an argument.
    In the third example, we write a transducer `filter-odd?`, which emulates what
    (`filter odd?`) does. Let''s see how the performance varies between traditional
    and transducer versions:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，（`filter odd?`）返回一个 transducer——在第一个例子中，transducer 包装了 reducer 函数 `+`，以返回另一个组合的减少函数。虽然我们在第一个例子中使用普通的
    `reduce` 函数，但在第二个例子中，我们使用接受 transducer 作为参数的 `transduce` 函数。在第三个例子中，我们编写了一个 transducer
    `filter-odd?`，它模拟了（`filter odd?`）的行为。让我们看看传统版本和 transducer 版本之间的性能差异：
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Performance characteristics
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能特征
- en: The key point behind transducers is how orthogonal each transformation is allowed
    to be, yet highly composable also. At the same time, transformations can happen
    in lockstep for the entire sequence instead of each operation producing lazy chunked
    sequences. This often causes significant performance benefits with transducers.
    Lazy sequences are still going to be useful when the final result is too large
    to realize at once—for other use cases transducers should fit the need aptly with
    improved performance. Since the core functions have been overhauled to work with
    transducers, it makes sense to model transformations more often than not in terms
    of transducers.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: transducers 的关键点在于每个转换可以允许多么正交，同时又是高度可组合的。同时，转换可以在整个序列上同步进行，而不是每个操作都产生懒加载的块序列。这通常会导致
    transducers 有显著的性能优势。当最终结果太大而无法一次性实现时，懒加载序列仍然会很有用——对于其他用例，transducers 应该能够适当地满足需求并提高性能。由于核心函数已经被彻底改造以与
    transducers 一起工作，因此，在大多数情况下，用 transducers 来建模转换是有意义的。
- en: Transients
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transients
- en: Earlier in this chapter, we discussed the virtues of immutability and the pitfalls
    of mutability. However, even though mutability is fundamentally unsafe, it also
    has very good single-threaded performance. Now, what if there was a way to restrict
    the mutable operation in a local context in order to provide safety guarantees?
    That would be equivalent to combining the performance advantage and local safety
    guarantees. That is exactly the abstraction called **transients**, which is provided
    by Clojure.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章早期，我们讨论了不可变性的优点和可变性的陷阱。然而，尽管可变性在本质上是不安全的，但它也具有非常好的单线程性能。现在，如果有一种方法可以限制局部上下文中的可变操作，以提供安全性保证，那将等同于结合性能优势和局部安全性保证。这正是
    Clojure 提供的称为 **transients** 的抽象。 '
- en: 'Firstly, let''s verify that it is safe (up to Clojure 1.6 only):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们验证它是安全的（仅限于 Clojure 1.6）：
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As we can see previously, up to Clojure 1.6, a transient created in one thread
    cannot be accessed by another. However, this operation is allowed in Clojure 1.7
    in order for transducers to play well with the `core.async` ([https://github.com/clojure/core.async](https://github.com/clojure/core.async))
    library —the developer should maintain operational consistency on transients across
    threads:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在 Clojure 1.6 之前，一个线程中创建的 transient 不能被另一个线程访问。然而，在 Clojure 1.7 中允许这种操作，以便
    transducers 能够与 `core.async` ([https://github.com/clojure/core.async](https://github.com/clojure/core.async))
    库良好地协同工作——开发者应在跨线程的 transient 上保持操作一致性：
- en: '[PRE21]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, transients cannot be converted to seqs. Hence, they cannot participate
    in the birthing of new persistent data structures and leak out of the scope of
    execution. Consider the following code:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，transients 不能转换为 seqs。因此，它们不能参与新持久数据结构的生成，也不能从执行范围中泄漏出来。考虑以下代码：
- en: '[PRE22]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `persistent!` function permanently converts `transient` into an equivalent
    persistent data structure. Effectively, transients are for one-time use only.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`persistent!` 函数将 `transient` 永久转换为等效的持久数据结构。实际上，transients 只能用于一次性使用。'
- en: 'Conversion between `persistent` and `transient` data structures (the `transient`
    and `persistent!` functions) is constant time, that is, it is an *O(1)* operation.
    Transients can be created from unsorted maps, vectors, and sets only. The functions
    that mutate transients are: `conj!`, `disj!`, `pop!`, `assoc!`, and `dissoc!`.
    Read-only operations such as `get`, `nth`, `count`, and many more work as usual
    on transients, but functions such as `contains?` and those that imply seqs, such
    as `first`, `rest`, and `next`, do not.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在`persistent`和`transient`数据结构之间（`transient`和`persistent!`函数）的转换是常数时间，即它是一个*O(1)*操作。可以从未排序的映射、向量和集合中创建瞬态。修改瞬态的函数有：`conj!`、`disj!`、`pop!`、`assoc!`和`dissoc!`。只读操作，如`get`、`nth`、`count`等，在瞬态上按常规工作，但像`contains?`这样的函数以及暗示序列的函数，如`first`、`rest`和`next`，则不行。
- en: Fast repetition
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速重复
- en: 'The function `clojure.core/repeatedly` lets us execute a function many times
    and produces a lazy sequence of results. Peter Taoussanis, in his open source
    serialization library **Nippy** ([https://github.com/ptaoussanis/nippy](https://github.com/ptaoussanis/nippy)),
    wrote a transient-aware variant that performs significantly better. It is reproduced,
    as shown, with his permission (note that the arity of the function is not the
    same as `repeatedly`):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`clojure.core/repeatedly`允许我们多次执行一个函数，并产生一个结果序列的懒序列。Peter Taoussanis在他的开源序列化库**Nippy**（[https://github.com/ptaoussanis/nippy](https://github.com/ptaoussanis/nippy)）中编写了一个瞬态感知的变体，它性能显著更好。它在他的许可下被复制，如下所示（注意，函数的arity与`repeatedly`不同）：
- en: '[PRE23]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Performance miscellanea
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能杂项
- en: Besides the major abstractions we saw earlier in the chapter, there are other
    smaller, but nevertheless very performance-critical, parts of Clojure that we
    will see in this section.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在本章前面看到的重大抽象之外，Clojure还有一些其他较小但同样非常关键的性能部分，我们将在本节中看到。
- en: Disabling assertions in production
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用生产环境中的断言
- en: Assertions are very useful to catch logical errors in the code during development,
    but they impose a runtime overhead that you may like to avoid in the production
    environment. Since `assert` is a compile time variable, the assertions can be
    silenced either by binding `assert` to false or by using `alter-var-root` before
    the code is loaded. Unfortunately, both the techniques are cumbersome to use.
    Paul Stadig's library called **assertions** ([https://github.com/pjstadig/assertions](https://github.com/pjstadig/assertions))
    helps with this exact use-case by enabling or disabling assertions via the command-line
    argument `-ea` to the Java runtime.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 断言在开发过程中非常有用，可以捕获代码中的逻辑错误，但它们在运行时会产生开销，你可能希望在生产环境中避免。由于`assert`是一个编译时变量，断言可以通过将`assert`绑定到`false`或在使用代码之前使用`alter-var-root`来静默。不幸的是，这两种技术都使用起来很麻烦。Paul
    Stadig的名为**assertions**的库（[https://github.com/pjstadig/assertions](https://github.com/pjstadig/assertions)）通过通过命令行参数`-ea`到Java运行时启用或禁用断言，帮助解决这个特定用例。
- en: 'To use it, you must include it in your Leiningen `project.clj` file as a dependency:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，你必须将其包含在你的Leiningen `project.clj`文件中作为依赖项：
- en: '[PRE24]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You must use this library''s `assert` macro instead of Clojure''s own, so each
    `ns` block in the application should look similar to this:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须使用这个库的`assert`宏而不是Clojure自己的，因此应用程序中的每个`ns`块都应该看起来像这样：
- en: '[PRE25]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'When running the application, you should include the `-ea` argument to the
    JRE to enable assertions, whereas its exclusion implies no assertion at runtime:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行应用程序时，你应该将`-ea`参数包含到JRE中，以启用断言，而排除它则意味着在运行时不进行断言：
- en: '[PRE26]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that this usage will not automatically avoid assertions in the dependency
    libraries.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种用法不会自动避免依赖库中的断言。
- en: Destructuring
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解构
- en: '**Destructuring** is one of Clojure''s built-in mini languages and, arguably,
    a top productivity booster during development. This feature leads to the parsing
    of values to match the left-hand side of the binding forms. The more complicated
    the binding form, the more work there is that needs to be done. Not surprisingly,
    this has a little bit of performance overhead.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**解构**是Clojure的内置迷你语言之一，并且可以说是开发期间的一个顶级生产力提升器。这个特性导致将值解析以匹配绑定形式的左侧。绑定形式越复杂，需要完成的工作就越多。不出所料，这会有一点性能开销。'
- en: It is easy to avoid this overhead by using explicit functions to unravel data
    in the tight loops and other performance-critical code. After all, it all boils
    down to making the program work less and do more.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用显式函数在紧密循环和其他性能关键代码中展开数据，可以轻松避免这种开销。毕竟，这一切都归结于让程序工作得少，做得多。
- en: Recursion and tail-call optimization (TCO)
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归和尾调用优化（TCO）
- en: Functional languages have this concept of tail-call optimization related to
    recursion. So, the idea is that when a recursive call is at the tail position,
    it does not take up space on the stack for recursion. Clojure supports a form
    of user-assisted recursive call to make sure the recursive calls do not blow the
    stack. This is kind of an imperative looping, but is extremely fast.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式语言有与递归相关的尾调用优化概念。因此，当递归调用处于尾位置时，它不会占用递归的栈空间。Clojure 支持一种用户辅助的递归调用形式，以确保递归调用不会耗尽栈空间。这有点像命令式循环，但速度极快。
- en: 'When carrying out computations, it may make a lot of sense to use `loop-recur`
    in the tight loops instead of iterating over synthetic numbers. For example, we
    want to add all odd integers from zero through to 1,000,000\. Let''s compare the
    code:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行计算时，在紧密循环中使用 `loop-recur` 而不是迭代合成数字可能非常有意义。例如，我们想要将0到1,000,000之间的所有奇数相加。让我们比较一下代码：
- en: '[PRE27]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'When we run the code, we get interesting results:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们得到了有趣的结果：
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `time` macro is far from perfect as the performance-benchmarking tool, but
    the relative numbers indicate a trend—in the subsequent chapters, we will look
    at the *Criterium* library for more scientific benchmarking. Here, we use `loop-recur`
    not only to iterate faster, but we are also able to change the algorithm itself
    by iterating only about half as many times as we did in the other example.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`time` 宏作为性能基准工具远非完美，但相对数值表明了一种趋势——在随后的章节中，我们将探讨 *Criterium* 库以进行更科学的基准测试。在这里，我们使用
    `loop-recur` 不仅是为了更快地迭代，而且我们还能通过只迭代大约其他示例一半的次数来改变算法本身。'
- en: Premature end of iteration
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迭代提前结束
- en: 'When accumulating over a collection, in some cases, we may want to end it prematurely.
    Prior to Clojure 1.5, `loop-recur` was the only way to do it. When using `reduce`,
    we can do just that using the `reduced` function introduced in Clojure 1.5 as
    shown:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在对集合进行累积时，在某些情况下，我们可能希望提前结束。在 Clojure 1.5 之前，`loop-recur` 是唯一的方法。当使用 `reduce`
    时，我们可以使用 Clojure 1.5 中引入的 `reduced` 函数做到这一点，如下所示：
- en: '[PRE29]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, we multiply all the numbers in a collection and, upon finding any of the
    numbers as zero, immediately return the result zero instead of continuing up to
    the last element.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们乘以集合中的所有数字，并在发现任何数字为零时，立即返回结果零，而不是继续到最后一个元素。
- en: The function `reduced?` helps detect when a reduced value is returned. Clojure
    1.7 introduces the `ensure-reduced` function to box up non-reduced values as reduced.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `reduced?` 帮助检测何时返回了已减少（reduced）的值。Clojure 1.7 引入了 `ensure-reduced` 函数，将非减少值装箱为减少值。
- en: Multimethods versus protocols
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多方法与协议的比较
- en: '**Multimethods** are a fantastic expressive abstraction for a polymorphic dispatch
    on the dispatch function''s return value. The `dispatch` functions associated
    with a multimethod are maintained at runtime and are looked up whenever a multimethod
    call is invoked. While multimethods provide a lot of flexibility in determining
    the dispatch, the performance overhead is simply too high compared to that of
    protocol implementations.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**多方法** 是一个针对分派函数返回值的泛型分派（polymorphic dispatch）的出色表达抽象。与多方法关联的分派函数在运行时维护，并在调用多方法时被查找。虽然多方法在确定分派时提供了很多灵活性，但与协议实现相比，性能开销实在太高。'
- en: Protocols (`defprotocol`) are implemented using reify, records (`defrecord`),
    and types (`deftype`, `extend-type`) in Clojure. This is a big discussion topic—since
    we are discussing the performance characteristics, it should suffice to say that
    protocol implementations dispatch on polymorphic types and are significantly faster
    than multimethods. Protocols and types are generally the implementation detail
    of an API, so they are usually fronted by functions.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 协议（`defprotocol`）在 Clojure 中使用 reify、记录（`defrecord`）和类型（`deftype`、`extend-type`）实现。这是一个大讨论话题——既然我们在讨论性能特性，那么只需说协议实现基于多态类型进行分派，并且比多方法快得多就足够了。协议和类型通常是
    API 的实现细节，因此它们通常由函数来呈现。
- en: Due to the multimethods' flexibility, they still have a place. However, in performance-critical
    code it is advisable to use protocols, records, and types instead.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多方法（multimethods）的灵活性，它们仍然有其位置。然而，在性能关键代码中，建议使用协议（protocols）、记录（records）和类型（types）。
- en: Inlining
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内联
- en: 'It is well known that macros are expanded inline at the call site and avoid
    a function call. As a consequence, there is a small performance benefit. There
    is also a `definline` macro that lets you write a function just like a normal
    macro. It creates an actual function that gets inlined at the call site:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，宏在调用位置处内联展开，避免了函数调用。因此，这带来了一点点性能上的好处。还有一个`definline`宏，允许你像写正常宏一样编写一个函数。它创建了一个实际函数，该函数在调用位置处被内联：
- en: '[PRE30]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the JVM also analyzes the code it runs and does its own inlining of
    code at runtime. While you may choose to inline the hot functions, this technique
    is known to give only a modest performance boost.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，JVM也会分析其运行的代码，并在运行时进行自己的代码内联。虽然你可以选择内联热函数，但这种技术已知只能提供适度的性能提升。
- en: 'When we define a `var` object, its value is looked up each time it is used.
    When we define a `var` object using a `:const` meta pointing to a `long` or `double`
    value, it is inlined from wherever it is called:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义一个`var`对象时，每次使用它时都会查找其值。当我们使用指向`long`或`double`值的`:const`元数据定义`var`对象时，它从调用位置处内联：
- en: '[PRE31]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is known to give a decent performance boost when applicable. See the following
    example:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当适用时，这已知可以提供相当的性能提升。请看以下示例：
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Performance is one of the cornerstones of Clojure's design. Abstractions in
    Clojure are designed for simplicity, power, and safety, with performance firmly
    in mind. We saw the performance characteristics of various abstractions and also
    how to make decisions about abstractions depending on performance use cases.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 性能是Clojure设计的基础之一。Clojure中的抽象设计用于简单性、强大性和安全性，同时牢记性能。我们看到了各种抽象的性能特征，以及如何根据性能用例做出抽象决策。
- en: In the next chapter, we will see how Clojure interoperates with Java and how
    we can extract Java's power to derive optimum performance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到Clojure如何与Java互操作，以及我们如何提取Java的力量以获得最佳性能。
- en: Chapter 3. Leaning on Java
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。依赖Java
- en: 'Being hosted on the JVM, there are several aspects of Clojure that really help
    to understand about the Java language and platform. The need is not only due to
    interoperability with Java or understanding its implementation, but also for performance
    reasons. In certain cases, Clojure may not generate optimized JVM bytecode by
    default; in some other cases, you may want to go beyond the performance that Clojure
    data structures offer—you can use the Java alternatives via Clojure to get better
    performance. This chapter discusses those aspects of Clojure. In this chapter
    we will discuss:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Clojure托管在JVM上，Clojure的几个方面确实有助于理解Java语言和平台。这种需求不仅是因为与Java的互操作性或理解其实现，还因为性能原因。在某些情况下，Clojure默认可能不会生成优化的JVM字节码；在另一些情况下，你可能希望超越Clojure数据结构提供的性能——你可以通过Clojure使用Java替代方案来获得更好的性能。本章讨论了Clojure的这些方面。在本章中，我们将讨论：
- en: Inspecting Java and bytecode generated from a Clojure source
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查从Clojure源生成的Java和字节码
- en: Numerics and primitives
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值和原始类型
- en: Working with arrays
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数组一起工作
- en: Reflection and type hinting
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反射和类型提示
- en: Inspecting the equivalent Java source for Clojure code
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查Clojure代码的等效Java源代码
- en: Inspecting the equivalent Java source for a given Clojure code provides great
    insight into how that might impact its performance. However, Clojure generates
    only Java bytecodes at runtime unless we compile a namespace out to the disk.
    When developing with Leiningen, only selected namespaces under the `:aot` vector
    in the `project.clj` file are output as the compiled `.class` files containing
    bytecodes. Fortunately, an easy and quick way to know the equivalent Java source
    for the Clojure code is to AOT-compile namespaces and then decompile the bytecodes
    into equivalent Java sources, using a Java bytecode decompiler.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 检查给定Clojure代码的等效Java源代码可以提供对它可能如何影响性能的深入了解。然而，除非我们将命名空间编译到磁盘上，否则Clojure在运行时只生成Java字节码。当使用Leiningen进行开发时，只有`project.clj`文件中`:aot`向量下的选定命名空间被输出为包含字节码的编译`.class`文件。幸运的是，有一种简单快捷的方法可以知道Clojure代码的等效Java源代码，那就是通过AOT编译命名空间，然后使用Java字节码反编译器将字节码反编译成等效的Java源代码。
- en: There are several commercial and open source Java bytecode decompilers available.
    One of the open source decompilers we will discuss here is **JD-GUI**, which you
    can download from its website ([http://jd.benow.ca/#jd-gui](http://jd.benow.ca/#jd-gui)).
    Use a version suitable for your operating system.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个商业和开源的 Java 字节码反编译器可用。我们将在这里讨论的一个开源反编译器是 **JD-GUI**，您可以从其网站下载（[http://jd.benow.ca/#jd-gui](http://jd.benow.ca/#jd-gui)）。请使用适合您操作系统的版本。
- en: Creating a new project
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个新的项目
- en: 'Let''s see how exactly to arrive at the equivalent Java source code from Clojure.
    Create a new project using Leiningen: `lein new foo`. Then edit the `src/foo/core.clj`
    file with a `mul` function to find out the product of two numbers:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何从 Clojure 生成等效的 Java 源代码。使用 Leiningen 创建一个新的项目：`lein new foo`。然后编辑 `src/foo/core.clj`
    文件，添加一个 `mul` 函数来找出两个数字的乘积：
- en: '[PRE33]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Compiling the Clojure sources into Java bytecode
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Clojure 源代码编译成 Java 字节码
- en: 'Now, to compile Clojure sources into bytecodes and output them as `.class`
    files, run the `lein compile :all` command. It creates the `.class` files in the
    `target/classes` directory of the project as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要将 Clojure 源代码编译成字节码并以 `.class` 文件的形式输出，请运行 `lein compile :all` 命令。它将在项目的
    `target/classes` 目录中创建 `.class` 文件，如下所示：
- en: '[PRE34]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can see that the `foo.core` namespace has been compiled into four `.class`
    files.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到 `foo.core` 命名空间已被编译成四个 `.class` 文件。
- en: Decompiling the .class files into Java source
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 .class 文件反编译成 Java 源代码
- en: Assuming that you have already installed JD-GUI, decompiling the `.class` files
    is as simple as opening them using the JD-GUI application.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经安装了 JD-GUI，反编译 `.class` 文件就像使用 JD-GUI 应用程序打开它们一样简单。
- en: '![Decompiling the .class files into Java source](img/B04596_03_01.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![将 .class 文件反编译成 Java 源代码](img/B04596_03_01.jpg)'
- en: 'On inspection, the code for the `foo.core/mul` function looks as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 `foo.core/mul` 函数的代码如下：
- en: '[PRE35]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: It is easy to understand from the decompiled Java source that the foo.core/mul
    function is an instance of the core$mul class in the foo package extending the
    clojure.lang.AFunction class. We can also see that the argument types are of the
    Object type in method invoke(Object, Object), which implies the numbers will be
    boxed. In a similar fashion, you can decompile class files of any Clojure code
    to inspect the equivalent Java code. If you can combine this with knowledge about
    Java types and potential reflection and boxing, you can find the suboptimal spots
    in code and focus on what to improve upon.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 从反编译的 Java 源代码中很容易理解，foo.core/mul 函数是 foo 包中 core$mul 类的一个实例，它扩展了 clojure.lang.AFunction
    类。我们还可以看到，方法调用（Object, Object）中的参数类型是 Object 类型，这意味着数字将被装箱。以类似的方式，您可以反编译任何 Clojure
    代码的类文件来检查等效的 Java 代码。如果您能结合对 Java 类型以及可能的反射和装箱的了解，您就可以找到代码中的次优位置，并专注于要改进的地方。
- en: Compiling the Clojure source without locals clearing
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不进行局部变量清除的 Clojure 源代码编译
- en: Note the Java code in the method invoke where it says `x = null; y = null;`
    —how is it possible that the code throws away the arguments, sets them to null,
    and effectively multiplies two null objects? This misleading decompilation happens
    due to locals clearing, a feature of the JVM bytecode implementation of Clojure,
    which has no equivalent in the Java language.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意方法调用中的 Java 代码，其中说 `x = null; y = null;` ——代码是如何丢弃参数，将它们设置为 null，并实际上将两个 null
    对象相乘的呢？这种误导性的反编译是由于局部变量清除引起的，这是 Clojure JVM 字节码实现的一个特性，在 Java 语言中没有等效功能。
- en: 'Starting with Clojure 1.4, the compiler supports the `:disable-locals-clearing`
    key in the dynamic `clojure.core/*compiler-options*` var that we cannot configure
    in the `project.clj` file. So, we cannot use the `lein compile` command, but we
    can start a **REPL** with the `lein repl` command to compile the classes:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Clojure 1.4 开始，编译器支持 `:disable-locals-clearing` 键，这是在 `project.clj` 文件中无法配置的
    `clojure.core/*compiler-options*` 动态变量。因此，我们无法使用 `lein compile` 命令，但我们可以使用 `lein
    repl` 命令启动一个 **REPL** 来编译类：
- en: '[PRE36]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This generates the class files in the same location as we saw earlier in this
    section, but without `x = null; y = null;` because locals clearing is omitted.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在本节前面看到的相同位置生成类文件，但不会出现 `x = null; y = null;`，因为省略了局部变量清除。
- en: Numerics, boxing, and primitives
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数字、装箱和原始类型
- en: '**Numerics** are scalars. The discussion on numerics was deferred till this
    chapter for the sole reason that the numerics implementation in Clojure has strong
    Java underpinnings. Since version 1.3, Clojure has settled with 64-bit numerics
    as the default. Now, `long` and `double` are idiomatic and the default numeric
    types. Note that these are primitive Java types, not objects. Primitives in Java
    lead to high performance and have several optimizations associated with them at
    compiler and runtime levels. A local primitive is created on the stack (hence
    does not contribute to heap allocation and GC) and can be accessed directly without
    any kind of dereferencing. In Java, there also exist object equivalents of the
    numeric primitives, known as **boxed numerics**—these are regular objects that
    are allocated on the heap. The boxed numerics are also immutable objects, which
    mean not only does the JVM need to dereference the stored value when reading it,
    but also needs to create a new boxed object when a new value needs to be created.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值**是标量。关于数值的讨论被推迟到本章，唯一的原因是 Clojure 中数值实现的强大 Java 基础。自 1.3 版本以来，Clojure
    已经确定使用 64 位数值作为默认值。现在，`long` 和 `double` 是惯用的默认数值类型。请注意，这些是原始 Java 类型，而不是对象。Java
    中的原始类型导致高性能，并在编译器和运行时级别具有多个优化。局部原始类型在栈上创建（因此不会对堆分配和 GC 贡献），可以直接访问而无需任何类型的解引用。在
    Java 中，也存在数值原始类型的对象等价物，称为 **包装数值**——这些是分配在堆上的常规对象。包装数值也是不可变对象，这意味着不仅 JVM 在读取存储的值时需要解引用，而且在需要创建新值时还需要创建一个新的包装对象。'
- en: 'It should be obvious that boxed numerics are slower than their primitive equivalents.
    The Oracle HotSpot JVM, when started with the `-server` option, aggressively inlines
    those functions (on frequent invocation) that contain a call to primitive operations.
    Clojure automatically uses **primitive numerics** at several levels. In the `let`
    blocks, `loop` blocks, arrays, and arithmetic operations (`+`, `-`, `*`, `/`,
    `inc`, `dec`, `<`, `<=`, `>`, `>=`), primitive numerics are detected and retained.
    The following table describes the primitive numerics with their boxed equivalents:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，包装数值比它们的原始等价类型要慢。当使用 `-server` 选项启动 Oracle HotSpot JVM 时，它会积极内联那些包含对原始操作调用的函数（在频繁调用时）。Clojure
    在多个级别自动使用 **原始数值**。在 `let` 块、`loop` 块、数组以及算术运算（`+`、`-`、`*`、`/`、`inc`、`dec`、`<`、`<=`、`>`、`>=`）中，会检测并保留原始数值。以下表格描述了原始数值及其包装等价类型：
- en: '| Primitive numeric type | Boxed equivalent |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 原始数值类型 | 包装等价类型 |'
- en: '| --- | --- |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| byte (1 byte) | `java.lang.Byte` |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| byte（1字节） | `java.lang.Byte` |'
- en: '| short (2 bytes) | `java.lang.Short` |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| short（2字节） | `java.lang.Short` |'
- en: '| int (4 bytes) | `java.lang.Integer` |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| int（4字节） | `java.lang.Integer` |'
- en: '| float (4 bytes) | `java.lang.Float` |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| float（4字节） | `java.lang.Float` |'
- en: '| long (8 bytes) | `java.lang.Long` |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| long（8字节） | `java.lang.Long` |'
- en: '| double (8 bytes) | `java.lang.Double` |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| double（8字节） | `java.lang.Double` |'
- en: In Clojure, sometimes you may find the numerics are passed or returned as boxed
    objects to or from functions due to the lack of type information at runtime. Even
    if you have no control over such functions, you can coerce the values to be treated
    as primitives. The `byte`, `short`, `int`, `float`, `long`, and `double` functions
    create primitive equivalents from given boxed numeric values.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Clojure 中，有时你可能会发现数值作为包装对象传递或从函数返回，这是由于运行时缺乏类型信息所致。即使你无法控制此类函数，你也可以强制转换值以将其视为原始类型。`byte`、`short`、`int`、`float`、`long`
    和 `double` 函数从给定的包装数值值创建原始等价类型。
- en: 'One of the Lisp traditions is to provide correct ([http://en.wikipedia.org/wiki/Numerical_tower](http://en.wikipedia.org/wiki/Numerical_tower))
    arithmetic implementation. A lower type should not truncate values when overflow
    or underflow happens, but rather should be promoted to construct a higher type
    to maintain correctness. Clojure follows this constraint and provides **autopromotion**
    via prime ([http://en.wikipedia.org/wiki/Prime_(symbol)](http://en.wikipedia.org/wiki/Prime_(symbol)))
    functions: `+''`, `-''`, `*''`, `inc''`, and `dec''`. Autopromotion provides correctness
    at the cost of some performance.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Lisp 传统之一是提供正确的 ([http://en.wikipedia.org/wiki/Numerical_tower](http://en.wikipedia.org/wiki/Numerical_tower))
    算术实现。当发生溢出或下溢时，低类型不应截断值，而应提升到更高类型以保持正确性。Clojure 遵循此约束，并通过素数 ([http://en.wikipedia.org/wiki/Prime_(symbol)](http://en.wikipedia.org/wiki/Prime_(symbol)))
    函数：`+'`、`-'`、`*'`、`inc'` 和 `dec'` 提供自动提升。自动提升以牺牲一些性能为代价提供正确性。
- en: There are also arbitrary length or precision numeric types in Clojure that let
    us store unbounded numbers but have poorer performance compared to primitives.
    The `bigint` and `bigdec` functions let us create numbers of arbitrary length
    and precision.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure中也有任意长度或精度的数值类型，允许我们存储无界数值，但与原始类型相比性能较差。`bigint`和`bigdec`函数允许我们创建任意长度和精度的数值。
- en: If we try to carry out any operations with primitive numerics that may result
    in a number beyond its maximum capacity, the operation maintains correctness by
    throwing an exception. On the other hand, when we use the prime functions, they
    autopromote to provide correctness. There is another set of operations called
    unchecked operations, which do not check for overflow or underflow and can potentially
    return incorrect results.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试执行任何可能产生超出其最大容量的原始数值的操作，该操作将通过抛出异常来保持正确性。另一方面，当我们使用素数函数时，它们会自动提升以提供正确性。还有另一组称为未检查操作的操作，这些操作不检查溢出或下溢，可能返回不正确的结果。
- en: 'In some cases, they may be faster than regular and prime functions. Such functions
    are `unchecked-add`, `unchecked-subtract`, `unchecked-multiply`, `unchecked-divide`,
    `unchecked-inc`, and `unchecked-dec`. We can also enable unchecked math behavior
    for regular arithmetic functions using the `*unchecked-math*` var; simply include
    the following in your source code file:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，它们可能比常规和素数函数更快。这些函数包括`unchecked-add`、`unchecked-subtract`、`unchecked-multiply`、`unchecked-divide`、`unchecked-inc`和`unchecked-dec`。我们还可以通过使用`*unchecked-math*`变量来启用常规算术函数的未检查数学行为；只需在您的源代码文件中包含以下内容：
- en: '[PRE37]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: One of the common needs in the arithmetic is the division used to find out the
    quotient and remainder after a natural number division. Clojure's `/` function
    provides a rational number division yielding a ratio, and the `mod` function provides
    a true modular arithmetic division. These functions are slower than the `quot`
    and `rem` functions that compute the division quotient and the remainder, respectively.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在算术中，一个常见的需求是用于在自然数除法后找到商和余数的除法。Clojure的`/`函数提供有理数除法，产生一个比例，而`mod`函数提供真正的模除法。这些函数比计算除法商和余数的`quot`和`rem`函数要慢。
- en: Arrays
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数组
- en: Besides objects and primitives, Java has a special type of collection storage
    structure called **arrays**. Once created, arrays cannot be grown or shrunk without
    copying data and creating another array to hold the result. Array elements are
    always homogeneous in type. The array elements are similar to places where you
    can mutate them to hold new values. Unlike collections such as list and vector,
    arrays can contain primitive elements, which make them a very fast storage mechanism
    without GC overhead.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对象和原始类型之外，Java还有一种特殊的集合存储结构类型，称为**数组**。一旦创建，数组在无需复制数据的情况下不能增长或缩小，需要创建另一个数组来存储结果。数组元素在类型上始终是同质的。数组元素类似于可以对其进行修改以保存新值的位置。与列表和向量等集合不同，数组可以包含原始元素，这使得它们成为一种非常快速的存储机制，没有GC开销。
- en: 'Arrays often form a basis for mutable data structures. For example, Java''s
    `java.lang.ArrayList` implementation uses arrays internally. In Clojure, arrays
    can be used for fast numeric storage and processing, efficient algorithms, and
    so on. Unlike collections, arrays can have one or more dimensions. So you could
    layout data in an array such as a matrix or cube. Let''s see Clojure''s support
    for arrays:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 数组通常构成可变数据结构的基础。例如，Java的`java.lang.ArrayList`实现内部使用数组。在Clojure中，数组可用于快速数值存储和处理、高效算法等。与集合不同，数组可以有一个或多个维度。因此，您可以在数组中布局数据，如矩阵或立方体。让我们看看Clojure对数组的支持：
- en: '| Description | Example | Notes |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 示例 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Create array | `(make-array Integer 20)` | Array of type (boxed) integer
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 创建数组 | `(make-array Integer 20)` | 类型为（装箱）整数的数组 |'
- en: '|   | `(make-array Integer/TYPE 20)` | Array of primitive type integer |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|   | `(make-array Integer/TYPE 20)` | 基本类型整数的数组 |'
- en: '|   | `(make-array Long/TYPE 20 10)` | Two-dimensional array of primitive long
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|   | `(make-array Long/TYPE 20 10)` | 基本类型长整数的二维数组 |'
- en: '| Create array of primitives | `(int-array 20)` | Array of primitive integer
    of size 20 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 创建原始数组 | `(int-array 20)` | 大小为20的原始整数数组 |'
- en: '|   | `(int-array [10 20 30 40])` | Array of primitive integer created from
    a vector |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|   | `(int-array [10 20 30 40])` | 由向量创建的基本整数数组 |'
- en: '| Create array from coll | `(to-array [10 20 30 40])` | Array from sequable
    |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 从集合创建数组 | `(to-array [10 20 30 40])` | 可序列的数组 |'
- en: '|   | `(to-array-2d [[10 20 30][40 50 60]])` | Two-dimensional array from collection
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|   | `(to-array-2d [[10 20 30][40 50 60]])` | 从集合中创建二维数组 |'
- en: '| Clone an array | `(aclone (to-array [:a :b :c]))` |   |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 克隆数组 | `(aclone (to-array [:a :b :c]))` |   |'
- en: '| Get array element | `(aget array-object 0 3)` | Get element at index [0][3]
    in a 2-D array |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 获取数组元素 | `(aget array-object 0 3)` | 获取二维数组中索引 [0][3] 的元素 |'
- en: '| Mutate array element | `(aset array-object 0 3 :foo)` | Set obj :foo at index
    [0][3] in a 2-D array |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 修改数组元素 | `(aset array-object 0 3 :foo)` | 在一个二维数组中设置 obj :foo 在索引 [0][3]
    |'
- en: '| Mutate primitive array element | `(aset-int int-array-object 2 6 89)` | Set
    value 89 at index [2][6] in 2-D array |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 修改原始数组元素 | `(aset-int int-array-object 2 6 89)` | 在二维数组中索引 [2][6] 设置值为 89
    |'
- en: '| Find length of array | `(alength array-object)` | `alength` is significantly
    faster than count |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 获取数组长度 | `(alength array-object)` | `alength` 比count 快得多 |'
- en: '| Map over an array | `(def a (int-array [10 20 30 40 50 60]))``(seq``(amap
    a idx ret``(do (println idx (seq ret))``(inc (aget a idx)))))` | Unlike map, `amap`
    returns a non-lazy array, which is significantly faster over array elements. Note
    that `amap` is faster only when properly type hinted. See next section for type
    hinting. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 遍历数组 | `(def a (int-array [10 20 30 40 50 60]))``(seq``(amap a idx ret``(do
    (println idx (seq ret))``(inc (aget a idx)))))` | 与 map 不同，`amap` 返回一个非惰性数组，在遍历数组元素时速度更快。注意，`amap`
    只有在正确类型提示的情况下才更快。有关类型提示，请参阅下一节。 |'
- en: '| Reduce over an array | `(def a (int-array [10 20 30 40 50 60]))``(areduce
    a idx ret 0``(do (println idx ret)``(+ ret idx)))` | Unlike reduce, `areduce`
    is significantly faster over array elements. Note that reduce is faster only when
    properly type hinted. See next section for type hinting. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 遍历数组 | `(def a (int-array [10 20 30 40 50 60]))``(areduce a idx ret 0``(do
    (println idx ret)``(+ ret idx)))` | 与 reduce 不同，`areduce` 在遍历数组元素时速度更快。注意，reduce
    只有在正确类型提示的情况下才更快。有关类型提示，请参阅下一节。 |'
- en: '| Cast to primitive arrays | `(ints int-array-object)` | Used with type hinting
    (see next section) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 转换为原始数组 | `(ints int-array-object)` | 与类型提示一起使用（见下一节） |'
- en: 'Like `int-array` and `ints`, there are functions for other types as well:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `int-array` 和 `ints` 一样，也有其他类型的函数：
- en: '| Array construction function | Primitive-array casting function | Type hinting
    (does not work for vars) | Generic array type hinting |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 数组构造函数 | 原始数组转换函数 | 类型提示（不适用于 vars） | 通用数组类型提示 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| boolean-array | booleans | `^booleans` | `^"[Z"` |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| boolean-array | booleans | `^booleans` | `^"[Z"` |'
- en: '| byte-array | bytes | `^bytes` | `^"[B"` |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| byte-array | bytes | `^bytes` | `^"[B"` |'
- en: '| short-array | shorts | `^shorts` | `^"[S"` |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| short-array | shorts | `^shorts` | `^"[S"` |'
- en: '| char-array | chars | `^chars` | `^"[C"` |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| char-array | chars | `^chars` | `^"[C"` |'
- en: '| int-array | ints | `^ints` | `^"[I"` |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| int-array | ints | `^ints` | `^"[I"` |'
- en: '| long-array | longs | `^longs` | `^"[J"` |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| long-array | longs | `^longs` | `^"[J"` |'
- en: '| float-array | floats | `^floats` | `^"[F"` |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| float-array | floats | `^floats` | `^"[F"` |'
- en: '| double-array | doubles | `^doubles` | `^"[D"` |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| double-array | doubles | `^doubles` | `^"[D"` |'
- en: '| object-array | –– | `^objects` | `^"[Ljava.lang.Object"` |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 对象数组 | –– | `^objects` | `^"[Ljava.lang.Object"` |'
- en: Arrays are favored over other data structures mainly due to performance, and
    sometimes due to interop. Extreme care should be taken to type hint the arrays
    and use the appropriate functions to work with them.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 数组之所以受到青睐，主要是因为性能，有时也因为互操作性。在为数组添加类型提示和使用适当的函数处理它们时，应特别小心。
- en: Reflection and type hints
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反射和类型提示
- en: Sometimes, as Clojure is dynamically typed, the Clojure compiler is unable to
    figure out the type of object to invoke a certain method. In such cases, Clojure
    uses **reflection**, which is considerably slower than the direct method dispatch.
    Clojure's solution to this is something called **type hints**. Type hints are
    a way to annotate arguments and objects with static types, so that the Clojure
    compiler can emit bytecodes for efficient dispatch.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，由于 Clojure 是动态类型的，Clojure 编译器无法确定要调用某个方法的对象类型。在这种情况下，Clojure 使用 **反射**，这比直接方法分派慢得多。Clojure
    的解决方案是称为 **类型提示** 的东西。类型提示是一种用静态类型注解参数和对象的方法，以便 Clojure 编译器可以生成用于高效分派的字节码。
- en: 'The easiest way to know where to put type hints is to turn on reflection warning
    in the code. Consider this code that determines the length of a string:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 知道在哪里放置类型提示的最简单方法是打开代码中的反射警告。考虑以下确定字符串长度的代码：
- en: '[PRE38]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the previous snippet, we can clearly see there is a very big difference
    in performance in the code that uses reflection versus the code that does not.
    When working on a project, you may want reflection warning to be turned on for
    all files. You can do it easily in Leiningen. Just put the following entry in
    your `project.clj` file:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们可以清楚地看到，使用反射的代码与不使用反射的代码在性能上有很大的差异。在处理项目时，你可能希望所有文件都开启反射警告。在 Leiningen
    中可以轻松实现。只需在你的 `project.clj` 文件中添加以下条目：
- en: '[PRE39]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will automatically turn on warning reflection every time you begin any
    kind of invocation via Leiningen in the dev workflow such as REPL and test.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你通过 Leiningen 在开发工作流程中开始任何类型的调用时自动开启反射警告，例如 REPL 和测试。
- en: An array of primitives
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始类型数组
- en: 'Recall the examples on `amap` and `areduce` from the previous section. If we
    run them with reflection warning on, we''d be warned that it uses reflection.
    Let''s type hint them:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下上一节中关于 `amap` 和 `areduce` 的例子。如果我们开启反射警告运行它们，我们会收到警告说它们使用了反射。让我们给它们添加类型提示：
- en: '[PRE40]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Note that the primitive array hint `^ints` does not work at the var level.
    So, it would not work if you defined the var `a`, as in the following:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，原始数组提示 `^ints` 在变量级别上不起作用。因此，如果你定义了变量 `a`，如下所示，它将不起作用：
- en: '[PRE41]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This notation is for an array of integers. Other primitive array types have
    similar type hints. Refer to the previous section for type hinting for various
    primitive array types.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这个符号表示整数数组。其他原始数组类型有类似的类型提示。请参考前面的章节了解各种原始数组类型的类型提示。
- en: Primitives
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本类型
- en: 'The type hinting of primitive locals is neither required nor allowed. However,
    you can type hint function arguments as primitives. Clojure allows up to four
    arguments in functions to be type hinted:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 原始局部变量的类型提示既不是必需的，也不允许。然而，你可以将函数参数作为原始类型进行类型提示。Clojure 允许在函数中最多有四个参数可以进行类型提示：
- en: '[PRE42]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Boxing may result in something not always being a primitive. In those cases,
    you can coerce those using respective primitive types.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 封装箱可能会导致某些情况下的对象不是基本类型。在这种情况下，你可以使用相应的原始类型强制转换它们。
- en: Macros and metadata
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宏和元数据
- en: 'In macros, type hinting does not work the way it does in the other parts of
    the code. Since macros are about transforming the **Abstract Syntax Tree** (**AST**),
    we need to have a mental map of the transformation and we should add type hints
    as metadata in the code. For example, if `str-len` is a macro to find the length
    of a string, we make use of the following code:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在宏中，类型提示的工作方式与其他代码部分不同。由于宏是关于转换**抽象语法树**（**AST**），我们需要有一个心理图来表示转换，并且我们应该在代码中添加类型提示作为元数据。例如，如果
    `str-len` 是一个用于查找字符串长度的宏，我们使用以下代码：
- en: '[PRE43]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In the preceding code, we alter the metadata of the symbol `s` by tagging it
    with the type `String`, which happens to be the `java.lang.String` class in this
    case. For array types, we can use `[Ljava.lang.String` for an array of string
    objects and similarly for others. If you try to use `str-len` listed previously,
    you may notice this works only when we pass the string bound to a local or a var,
    not as a string literal. To mitigate this, we can write the macro as follows:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过将类型 `String` 标记到符号 `s` 上来改变其元数据，在这种情况下，它恰好是 `java.lang.String` 类。对于数组类型，我们可以使用
    `[Ljava.lang.String` 来表示字符串对象的数组，以及其他类似情况。如果你尝试使用之前列出的 `str-len`，你可能会注意到这仅在将字符串绑定到本地变量或变量时才有效，而不是作为字符串字面量。为了减轻这种情况，我们可以将宏编写如下：
- en: '[PRE44]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here we bind the argument to a type-hinted gensym local, hence calling `.length`
    on it does not use reflection and there is no reflection warning emitted as such.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将参数绑定到一个带有类型提示的 gensym 本地变量上，因此在对它调用 `.length` 时不会使用反射，并且不会发出任何反射警告。
- en: 'Type hinting via metadata also works with functions, albeit in a different
    notation:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 通过元数据进行的类型提示也适用于函数，尽管符号不同：
- en: '[PRE45]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Except for the first example in the preceding snippet, they are type hinted
    to return the `java.lang.String` type.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面代码片段中的第一个例子外，它们都被类型提示为返回 `java.lang.String` 类型。
- en: String concatenation
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符串连接
- en: 'The `str` function in Clojure is used to concatenate and convert to string
    tokens. In Java, when we write `"hello" + e`, the Java compiler translates this
    to an equivalent code that uses `StringBuilder` and is considerably faster than
    the `str` function in micro-benchmarks. To obtain close-to-Java performance, in
    Clojure we can use a similar mechanism with a macro directly using Java interop
    to avoid the indirection via the `str` function. The **Stringer** ([https://github.com/kumarshantanu/stringer](https://github.com/kumarshantanu/stringer))
    library adopts the same technique to come up with fast string concatenation in
    Clojure:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure中的`str`函数用于连接和转换为字符串标记。在Java中，当我们编写`"hello" + e`时，Java编译器将其转换为使用`StringBuilder`的等效代码，在微基准测试中比`str`函数快得多。为了获得接近Java的性能，在Clojure中我们可以使用一个类似的机制，通过宏直接使用Java互操作来避免通过`str`函数的间接操作。**Stringer**
    ([https://github.com/kumarshantanu/stringer](https://github.com/kumarshantanu/stringer))库采用了相同的技巧，在Clojure中实现快速字符串连接：
- en: '[PRE46]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Here, Stringer also aggressively concatenates the literals during the compile
    phase.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Stringer在编译阶段也积极地连接了字面量。
- en: Miscellaneous
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杂项
- en: 'In a type (as in `deftype`), the mutable instance variables can be optionally
    annotated as `^:volatile-mutable` or `^:unsynchronized-mutable`. For example:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在类型（如`deftype`）中，可变实例变量可以可选地注解为`^:volatile-mutable`或`^:unsynchronized-mutable`。例如：
- en: '[PRE47]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Unlike `defprotocol`, the `definterface` macro lets us provide a return type
    hint for methods:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 与`defprotocol`不同，`definterface`宏允许我们为方法提供返回类型提示：
- en: '[PRE48]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `proxy-super` macro (which is used inside the `proxy` macro) is a special
    case where you cannot directly apply a type hint. The reason being that it relies
    on the implicit this object that is automatically created by the `proxy` macro.
    In this case, you must explicitly bind this to a type:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`proxy-super`宏（在`proxy`宏内部使用）是一个特殊情况，你不能直接应用类型提示。原因是它依赖于`proxy`宏自动创建的隐式`this`对象。在这种情况下，你必须显式地将`this`绑定到一个类型：'
- en: '[PRE49]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Type hinting is quite important for performance in Clojure. Fortunately, we
    need to type hint only when required and it's easy to find out when. In many cases,
    a gain from type hinting overshadows the gains from code inlining.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 类型提示对于Clojure的性能非常重要。幸运的是，我们只需要在需要时进行类型提示，而且很容易找出何时需要。在许多情况下，类型提示带来的收益会超过代码内联的收益。
- en: Using array/numeric libraries for efficiency
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数组/数值库以提高效率
- en: You may have noticed in the previous sections, when working with numerics, performance
    depends a lot on whether the data is based on arrays and primitives. It may take
    a lot of meticulousness on the programmer's part to correctly coerce data into
    primitives and arrays at all stages of the computation in order to achieve optimum
    efficiency. Fortunately, the high-performance enthusiasts from the Clojure community
    realized this issue early on and created some dedicated open source libraries
    to mitigate the problem.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到在前面的章节中，当处理数值时，性能很大程度上取决于数据是否基于数组和原始类型。为了实现最佳效率，程序员可能需要在计算的各个阶段都非常细致地将数据正确地强制转换为原始类型和数组。幸运的是，Clojure社区的高性能爱好者们很早就意识到了这个问题，并创建了一些专门的开源库来减轻这个问题。
- en: HipHip
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HipHip
- en: '**HipHip** is a Clojure library used to work with arrays of primitive types.
    It provides a safety net, that is, it strictly accepts only primitive array arguments
    to work with. As a result, passing silently boxed primitive arrays as arguments
    always results in an exception. HipHip macros and functions rarely need the programmer
    to type hint anything during the operations. It supports arrays of primitive types
    such as `int`, `long`, `float`, and `double`.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**HipHip**是一个Clojure库，用于处理原始类型数组。它提供了一个安全网，即它严格只接受原始数组参数来工作。因此，静默传递装箱的原始数组作为参数总是会导致异常。HipHip宏和函数很少需要在操作期间进行类型提示。它支持原始类型的数组，如`int`、`long`、`float`和`double`。'
- en: The HipHip project is available at [https://github.com/Prismatic/hiphip](https://github.com/Prismatic/hiphip).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: HipHip项目可在[https://github.com/Prismatic/hiphip](https://github.com/Prismatic/hiphip)找到。
- en: 'As of writing, HipHip''s most recent version is 0.2.0 that supports Clojure
    1.5.x or above, and is tagged as an Alpha release. There is a standard set of
    operations provided by HipHip for arrays of all of the four primitive types: integer
    array operations are in the namespace `hiphip.int`; double precision array operations
    in `hiphip.double`; and so on. The operations are all type hinted for the respective
    types. All of the operations for `int`, `long`, `float`, and `double` in respective
    namespaces are essentially the same except for the array type:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，HipHip 的最新版本是 0.2.0，支持 Clojure 1.5.x 或更高版本，并标记为 Alpha 版本。HipHip 为所有四种原始数据类型的数组提供了一套标准的操作：整数数组操作在命名空间
    `hiphip.int` 中；双精度数组操作在 `hiphip.double` 中；等等。所有操作都为相应类型提供了类型提示。在相应命名空间中，`int`、`long`、`float`
    和 `double` 的所有操作基本上是相同的，除了数组类型：
- en: '| Category | Function/macro | Description |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 函数/宏 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Core functions | `aclone` | Like `clojure.core/aclone`, for primitives |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 核心函数 | `aclone` | 类似于 `clojure.core/aclone`，用于原始数据类型 |'
- en: '|   | `alength` | Like `clojure.core/alength`, for primitives |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|   | `alength` | 类似于 `clojure.core/alength`，用于原始数据类型 |'
- en: '|   | `aget` | Like `clojure.core/aget`, for primitives |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|   | `aget` | 类似于 `clojure.core/aget`，用于原始数据类型 |'
- en: '|   | `aset` | Like `clojure.core/aset`, for primitives |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|   | `aset` | 类似于 `clojure.core/aset`，用于原始数据类型 |'
- en: '|   | `ainc` | Increment array element by specified value |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|   | `ainc` | 将数组元素按指定值增加 |'
- en: '| Equiv hiphip.array operations | `amake` | Make a new array and fill values
    computed by expression |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 等价的 hiphip.array 操作 | `amake` | 创建新数组并用表达式计算出的值填充 |'
- en: '|   | `areduce` | Like `clojure.core/areduce`, with HipHip array bindings |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|   | `areduce` | 类似于 `clojure.core/areduce`，带有 HipHip 数组绑定 |'
- en: '|   | `doarr` | Like `clojure.core/doseq`, with HipHip array bindings |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|   | `doarr` | 类似于 `clojure.core/doseq`，带有 HipHip 数组绑定 |'
- en: '|   | `amap` | Like `clojure.core/for`, creates new array |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|   | `amap` | 类似于 `clojure.core/for`，创建新数组 |'
- en: '|   | `afill!` | Like preceding `amap`, but overwrites array argument |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|   | `afill!` | 类似于前面的 `amap`，但覆盖数组参数 |'
- en: '| Mathy operations | `asum` | Compute sum of array elements using expression
    |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 数学运算 | `asum` | 使用表达式计算数组元素的总和 |'
- en: '|   | `aproduct` | Compute product of array elements using expression |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|   | `aproduct` | 使用表达式计算数组元素乘积 |'
- en: '|   | `amean` | Compute mean over array elements |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|   | `amean` | 计算数组元素的平均值 |'
- en: '|   | `dot-product` | Compute dot product of two arrays |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|   | `dot-product` | 计算两个数组的点积 |'
- en: '| Finding minimum/maximum, Sorting | `amax-index` | Find maximum value in array
    and return the index |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 查找最小/最大值，排序 | `amax-index` | 在数组中找到最大值并返回索引 |'
- en: '|   | `amax` | Find maximum value in array and return it |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|   | `amax` | 在数组中找到最大值并返回它 |'
- en: '|   | `amin-index` | Find minimum value in array and return the index |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|   | `amin-index` | 在数组中找到最小值并返回索引 |'
- en: '|   | `amin` | Find minimum value in array and return it |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|   | `amin` | 在数组中找到最小值并返回它 |'
- en: '|   | `apartition!` | Three-way partition of array: less, equal, greater than
    pivot |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|   | `apartition!` | 数组的三向划分：小于、等于、大于枢轴 |'
- en: '|   | `aselect!` | Gather smallest `k` elements at the beginning of array |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|   | `aselect!` | 将最小的 `k` 个元素收集到数组的开头 |'
- en: '|   | `asort!` | Sort array in-place using Java''s built-in implementation
    |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|   | `asort!` | 使用 Java 内置实现就地排序数组 |'
- en: '|   | `asort-max!` | Partial in-place sort gathering top `k` elements to the
    end |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|   | `asort-max!` | 部分就地排序，将前 `k` 个元素收集到末尾 |'
- en: '|   | `asort-min!` | Partial in-place sort gathering min `k` elements to the
    top |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|   | `asort-min!` | 部分就地排序，将最小的 `k` 个元素收集到顶部 |'
- en: '|   | `apartition-indices!` | Like `apartition!` but mutates index-array instead
    of values |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|   | `apartition-indices!` | 类似于 `apartition!`，但修改索引数组而不是值 |'
- en: '|   | `aselect-indices!` | Like `aselect!` but mutates index-array instead
    of values |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|   | `aselect-indices!` | 类似于 `aselect!`，但修改索引数组而不是值 |'
- en: '|   | `asort-indices!` | Like `asort!` but mutates index-array instead of values
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|   | `asort-indices!` | 类似于 `asort!`，但修改索引数组而不是值 |'
- en: '|   | `amax-indices` | Get index-array; last `k` indices pointing to max `k`
    values |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|   | `amax-indices` | 获取索引数组；最后 `k` 个索引指向最大的 `k` 个值 |'
- en: '|   | `amin-indices` | Get index-array; first `k` indices pointing to min `k`
    values |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|   | `amin-indices` | 获取索引数组；前 `k` 个索引指向最小的 `k` 个值 |'
- en: 'To include HipHip as a dependency in your Leiningen project, specify it in
    `project.clj`:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Leiningen 项目中包含 HipHip 作为依赖项，请在 `project.clj` 中指定：
- en: '[PRE50]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As an example of how to use HipHip, let''s see how to compute the normalized
    values of an array:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用 HipHip 的一个示例，让我们看看如何计算数组的归一化值：
- en: '[PRE51]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Unless we make sure that `xs` is an array of primitive doubles, HipHip will
    throw `ClassCastException` when the type is incorrect, and `IllegalArgumentException`
    in other cases. I recommend exploring the HipHip project to gain more insight
    into using it effectively.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们确保 `xs` 是原始双精度浮点数数组，否则 HipHip 在类型不正确时会抛出 `ClassCastException`，在其他情况下会抛出
    `IllegalArgumentException`。我建议您探索 HipHip 项目，以获得更深入的使用见解。
- en: primitive-math
  id: totrans-455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础数学运算
- en: We can set `*warn-on-reflection*` to true to let Clojure warn us when the reflection
    is used at invocation boundaries. However, when Clojure has to implicitly use
    reflection to perform math, the only resort is to either use a profiler or compile
    the Clojure source down to bytecode, and analyze boxing and reflection with a
    decompiler. This is where the `primitive-math` library helps, by producing extra
    warnings and throwing exceptions.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 `*warn-on-reflection*` 设置为 true，让 Clojure 在调用边界处使用反射时警告我们。然而，当 Clojure
    必须隐式使用反射进行数学运算时，唯一的办法是使用分析器或将 Clojure 源代码编译成字节码，然后使用反编译器分析装箱和反射。这就是 `primitive-math`
    库发挥作用的地方，它通过产生额外的警告和抛出异常来帮助。
- en: The `primitive-math` library is available at [https://github.com/ztellman/primitive-math](https://github.com/ztellman/primitive-math).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '`primitive-math` 库可在 [https://github.com/ztellman/primitive-math](https://github.com/ztellman/primitive-math)
    找到。'
- en: 'As of writing, primitive-math is at version 0.1.4; you can include it as a
    dependency in your Leiningen project by editing `project.clj` as follows:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，`primitive-math` 的版本为 0.1.4；您可以通过编辑 `project.clj` 文件将其作为依赖项包含到您的 Leiningen
    项目中，具体方法如下：
- en: '[PRE52]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following code is how it can be used (recall the example from the *Decompiling
    the .class files into Java source* section):'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用它的代码示例（回想一下 *将 .class 文件反编译成 Java 源代码* 部分的示例）：
- en: '[PRE53]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: While `primitive-math` is a useful library, the problem it solves is mostly
    taken care of by the boxing detection feature in Clojure 1.7 (see next section
    *Detecting boxed math*). However, this library is still useful if you are unable
    to use Clojure 1.7 or higher.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `primitive-math` 是一个有用的库，但它解决的问题大多已由 Clojure 1.7 的装箱检测功能处理（参见下一节 *检测装箱数学*）。然而，如果您无法使用
    Clojure 1.7 或更高版本，此库仍然很有用。
- en: Detecting boxed math
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测装箱数学
- en: '**Boxed math** is hard to detect and is a source of performance issues. Clojure
    1.7 introduces a way to warn the user when boxed math happens. This can be configured
    in the following way:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '**装箱数学**难以检测，是性能问题的来源。Clojure 1.7 引入了一种方法，当发生装箱数学时警告用户。这可以通过以下方式进行配置：'
- en: '[PRE54]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'When working with Leiningen, you can enable boxed math warnings by putting
    the following entry in the `project.clj` file:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Leiningen 时，您可以通过在 `project.clj` 文件中添加以下条目来启用装箱数学警告：
- en: '[PRE55]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The math operations in `primitive-math` (like HipHip) are implemented via macros.
    Therefore, they cannot be used as higher order functions and, as a consequence,
    may not compose well with other code. I recommend exploring the project to see
    what suits your program use case. Adopting Clojure 1.7 obviates the boxing discovery
    issues by means of a boxed-warning feature.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '`primitive-math` 中的数学运算（如 HipHip）是通过宏实现的。因此，它们不能用作高阶函数，并且因此可能与其他代码组合不佳。我建议您探索该项目，看看哪些适合您的程序用例。采用
    Clojure 1.7 通过装箱警告功能消除了装箱发现问题。'
- en: Resorting to Java and native code
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖 Java 和本地代码
- en: In a handful of cases, where the lack of imperative, stack-based, mutable variables
    in Clojure may make the code not perform as well as Java, we may need to evaluate
    alternatives to make it faster. I would advise you to consider writing such code
    directly in Java for better performance.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在少数情况下，由于 Clojure 缺乏命令式、基于栈的、可变变量，这可能导致代码的性能不如 Java，我们可能需要评估替代方案以提高其性能。我建议您考虑直接用
    Java 编写此类代码以获得更好的性能。
- en: Another consideration is to use native OS capabilities, such as memory-mapped
    buffers ([http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html](http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html))
    or files and unsafe operations ([http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/](http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/)).
    Note that unsafe operations are potentially hazardous and not recommended in general.
    Such times are also an opportunity to consider writing performance-critical pieces
    of code in C or C++ and then access them via the **Java Native Interface** (**JNI**).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是使用原生操作系统功能，例如内存映射缓冲区（[http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html](http://docs.oracle.com/javase/7/docs/api/java/nio/MappedByteBuffer.html)）或文件和不受保护的操作（[http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/](http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/)）。请注意，不受保护的操作可能具有潜在风险，通常不建议使用。这些时刻也是考虑将性能关键代码用
    C 或 C++ 编写，然后通过 **Java 本地接口**（**JNI**）访问它们的时机。
- en: Proteus – mutable locals in Clojure
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Proteus – Clojure 中的可变局部变量
- en: Proteus is an open source Clojure library that lets you treat a local as a local
    variable, thereby allowing its unsynchronized mutation within the local scope
    only. Note that this library depends on the internal implementation structure
    of Clojure as of Clojure 1.5.1\. The **Proteus** project is available at [https://github.com/ztellman/proteus](https://github.com/ztellman/proteus).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: Proteus 是一个开源的 Clojure 库，允许您将局部变量视为局部变量，从而使其在局部作用域内仅允许非同步修改。请注意，此库依赖于 Clojure
    1.5.1 的内部实现结构。**Proteus** 项目可在 [https://github.com/ztellman/proteus](https://github.com/ztellman/proteus)
    找到。
- en: 'You can include Proteus as a dependency in the Leiningen project by editing
    `project.clj`:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过编辑 `project.clj` 将 Proteus 包含为 Leiningen 项目的依赖项：
- en: '[PRE56]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Using Proteus in code is straightforward, as shown in the following code snippet:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中使用 Proteus 很简单，如下面的代码片段所示：
- en: '[PRE57]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Since Proteus allows mutation only in the local scope, the following throws
    an exception:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Proteus 只允许在局部作用域中进行可变操作，以下代码会抛出异常：
- en: '[PRE58]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The mutable locals are very fast and may be quite useful in tight loops. Proteus
    is unconventional by Clojure idioms, but it may give the required performance
    boost without having to write Java code.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 可变局部变量非常快，在紧密循环中可能非常有用。Proteus 在 Clojure 习惯用法上是非传统的，但它可能在不编写 Java 代码的情况下提供所需的性能提升。
- en: Summary
  id: totrans-481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Clojure has strong Java interoperability and underpinning, due to which programmers
    can leverage the performance benefits nearing those of Java. For performance-critical
    code, it is sometimes necessary to understand how Clojure interacts with Java
    and how to turn the right knobs. Numerics is a key area where Java interoperability
    is required to get optimum performance. Type hints are another important performance
    trick that is frequently useful. There are several open source Clojure libraries
    that make such activities easier for the programmer.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Clojure 具有强大的 Java 互操作性和基础，程序员可以利用接近 Java 的性能优势。对于性能关键代码，有时有必要了解 Clojure
    如何与 Java 交互以及如何调整正确的旋钮。数值是一个需要 Java 互操作以获得最佳性能的关键领域。类型提示是另一个重要的性能技巧，通常非常有用。有几个开源的
    Clojure 库使程序员更容易进行此类活动。
- en: In the next chapter, we will dig deeper below Java and see how the hardware
    and the JVM stack play a key role in offering the performance we get, what their
    constraints are, and how to use their understanding to get better performance.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨 Java 之下，看看硬件和 JVM 堆栈如何在我们获得性能中发挥关键作用，它们的限制是什么，以及如何利用对这些理解的使用来获得更好的性能。
- en: Chapter 4. Host Performance
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章。主机性能
- en: 'In the previous chapters, we noted how Clojure interoperates with Java. In
    this chapter we will go a bit deeper to understand the internals better. We will
    touch upon several layers of the entire stack, but our major focus will be the
    JVM, in particular the Oracle HotSpot JVM, though there are several JVM vendors
    to choose from ([http://en.wikipedia.org/wiki/List_of_Java_virtual_machines](http://en.wikipedia.org/wiki/List_of_Java_virtual_machines)).
    At the time of writing this, Oracle JDK 1.8 is the latest stable release and early
    OpenJDK 1.9 builds are available. In this chapter we will discuss:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们注意到了 Clojure 与 Java 的互操作性。在本章中，我们将更深入地探讨，以更好地理解内部结构。我们将触及整个堆栈的几个层次，但我们的主要焦点将是
    JVM，特别是 Oracle HotSpot JVM，尽管有多个 JVM 供应商可供选择（[http://en.wikipedia.org/wiki/List_of_Java_virtual_machines](http://en.wikipedia.org/wiki/List_of_Java_virtual_machines)）。在撰写本文时，Oracle
    JDK 1.8 是最新的稳定版本，早期 OpenJDK 1.9 构建也已可用。在本章中，我们将讨论：
- en: How the hardware subsystems function from the performance viewpoint
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从性能角度来看，硬件子系统是如何工作的
- en: Organization of the JVM internals and how that is related to performance
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JVM内部结构的组织以及它与性能的关系
- en: How to measure the amount of space occupied by various objects in the heap
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何测量堆中各种对象占用的空间量
- en: Profile Clojure code for latency using Criterium
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Criterium对Clojure代码进行延迟分析
- en: The hardware
  id: totrans-490
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件
- en: There are various hardware components that may impact the performance of software
    in different ways. The processors, caches, memory subsystem, I/O subsystems, and
    so on, all have varying degrees of performance impact depending upon the use cases.
    In the following sections we look into each of those aspects.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种硬件组件可能会以不同的方式影响软件的性能。处理器、缓存、内存子系统、I/O子系统等，根据用例的不同，都有不同程度的影响。在接下来的章节中，我们将探讨这些方面的每一个。
- en: Processors
  id: totrans-492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理器
- en: 'Since about the late 1980s, microprocessors have been employing pipelining
    and instruction-level parallelism to speed up their performance. Processing an
    instruction at the CPU level consists of typically four cycles: **fetch**, **decode**,
    **execute**, and **writeback**. Modern processors optimize the cycles by running
    them in parallel—while one instruction is executed, the next instruction is being
    decoded, and the one after that is being fetched, and so on. This style is called
    **instruction pipelining**.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 自1980年代末以来，微处理器一直采用流水线和指令级并行性来提高其性能。在CPU级别处理指令通常包括四个周期：**取指令**、**解码**、**执行**和**写回**。现代处理器通过并行运行这些周期来优化它们——当一条指令正在执行时，下一条指令正在被解码，再下一条正在被取，依此类推。这种风格被称为**指令流水线**。
- en: In practice, in order to speed up execution even further, the stages are subdivided
    into many shorter stages, thus leading to deeper super-pipeline architecture.
    The length of the longest stage in the pipeline limits the clock speed of the
    CPU. By splitting stages into substages, the processor can be run at a higher
    clock speed, where more cycles are required for each instruction, but the processor
    still completes one instruction per cycle. Since there are more cycles per second
    now, we get better performance in terms of throughput per second even though the
    latency of each instruction is now higher.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，为了进一步加快执行速度，阶段被细分为许多更短的阶段，从而导致了更深的超级流水线架构。流水线中最长阶段的长度限制了CPU的时钟速度。通过将阶段细分为子阶段，处理器可以以更高的时钟速度运行，每个指令需要更多的周期，但处理器仍然在每个周期内完成一条指令。由于现在每秒有更多的周期，尽管每个指令的延迟现在更高，但我们仍然在每秒吞吐量方面获得了更好的性能。
- en: Branch prediction
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分支预测
- en: The processor must fetch and decode instructions in advance even when it encounters
    instructions of the conditional `if-then` form. Consider an equivalent of the
    (`if (test a) (foo a) (bar a)`) Clojure expression. The processor must choose
    a branch to fetch and decode, the question is should it fetch the `if` branch
    or the `else` branch? Here, the processor makes a guess as to which instruction
    to fetch/decode. If the guess turns out to be correct, it is a performance gain
    as usual; otherwise, the processor has to throw away the result of the fetch/decode
    process and start on the other branch afresh.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 即使处理器遇到条件`if-then`形式的指令，也必须提前取指令和解码。考虑一个Clojure表达式(`if (test a) (foo a) (bar
    a)`)的等价物。处理器必须选择一个分支来取指令和解码，问题是它应该取`if`分支还是`else`分支？在这里，处理器对要取/解码的指令进行猜测。如果猜测是正确的，就像往常一样，这是一个性能提升；否则，处理器必须丢弃取/解码过程的结果，并从另一个分支重新开始。
- en: Processors deal with branch prediction using an on-chip branch prediction table.
    It contains recent code branches and two bits per branch, indicating whether or
    not the branch was taken, while also accommodating one-off, not-taken occurrences.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器使用片上分支预测表来处理分支预测。它包含最近的代码分支和每个分支两个比特，指示分支是否被采取，同时也容纳了单次未采取的情况。
- en: Today, branch prediction is extremely important in processors for performance,
    so modern processors dedicate hardware resources and special predication instructions
    to improve the prediction accuracy and lower the cost of a mispredict penalty.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，分支预测对于处理器的性能至关重要，因此现代处理器专门分配硬件资源和特殊的预测指令来提高预测准确性并降低误预测的代价。
- en: Instruction scheduling
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令调度
- en: High-latency instructions and branching usually lead to empty cycles in the
    instruction pipeline known as **stalls** or **bubbles**. These cycles are often
    used to do other work by the means of instruction reordering. Instruction reordering
    is implemented at the hardware level via out of order execution and at the compiler
    level via compile time instruction scheduling (also called **static instruction
    scheduling**).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 高延迟指令和分支通常会导致指令流水线中的空周期，称为**停顿**或**气泡**。这些周期通常通过指令重排来完成其他工作。指令重排通过乱序执行在硬件级别实现，通过编译时指令调度（也称为**静态指令调度**）在编译器级别实现。
- en: The processor needs to remember the dependencies between instructions when carrying
    out the out-of-order execution. This cost is somewhat mitigated by using renamed
    registers, wherein register values are stored into / loaded from memory locations,
    potentially on different physical registers, so that they can be executed in parallel.
    This necessitates that out-of-order processors always maintain a mapping of instructions
    and corresponding registers they use, which makes their design complex and power
    hungry. With a few exceptions, almost all high-performance CPUs today have out-of-order
    designs.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器在执行乱序执行时需要记住指令之间的依赖关系。这种成本可以通过使用重命名寄存器来在一定程度上减轻，其中寄存器值存储到/从内存位置加载，可能在不同物理寄存器上，这样它们可以并行执行。这需要乱序处理器始终维护指令及其使用的寄存器的映射，这使得它们的设计复杂且功耗大。除了一些例外，今天几乎所有高性能CPU都具有乱序设计。
- en: Good compilers are usually extremely aware of processors, and are capable of
    optimizing the code by rearranging processor instructions in a way that there
    are fewer bubbles in the processor instruction pipeline. A few high-performance
    CPUs still rely on only static instruction reordering instead of out-of-order
    instruction reordering and, in turn, save chip area due to simpler design—the
    saved area is used to accommodate extra cache or CPU cores. Low-power processors,
    such as those from the ARM and Atom family, use in-order design. Unlike most CPUs,
    the modern GPUs use in-order design with deep pipelines, which is compensated
    by very fast context switching. This leads to high latency and high throughput
    on GPUs.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的编译器通常对处理器有极高的了解，并且能够通过重新排列处理器指令来优化代码，从而在处理器指令流水线中减少气泡。一些高性能CPU仍然只依赖于静态指令重排而不是乱序指令重排，从而节省芯片面积——节省的面积用于容纳额外的缓存或CPU核心。低功耗处理器，如ARM和Atom系列，使用顺序设计。与大多数CPU不同，现代GPU使用具有深流水线的顺序设计，这通过非常快的上下文切换得到补偿。这导致GPU具有高延迟和高吞吐量。
- en: Threads and cores
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线程和核心
- en: Concurrency and parallelism via context switches, hardware threads, and cores
    are very common today and we have accepted them as a norm to implement in our
    programs. However, we should understand why we needed such a design in the first
    place. Most of the real-world code we write today does not have more than a modest
    scope for instruction-level parallelism. Even with hardware-based, out-of-order
    execution and static instruction reordering, no more than two instructions per
    cycle are truly parallel. Hence, another potential source of instructions that
    can be pipelined and executed in parallel are the programs other than the currently
    running one.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上下文切换、硬件线程和核心实现并发性和并行性在当今非常普遍，并且我们已经将它们视为实现我们程序的规范。然而，我们应该理解为什么我们最初需要这样的设计。我们今天编写的绝大多数现实世界代码在指令级并行性方面没有超过适度范围。即使基于硬件的乱序执行和静态指令重排，每个周期也真正并行执行的指令不超过两个。因此，另一个潜在的指令来源是除了当前运行的程序之外的程序，这些程序可以被流水线和并行执行。
- en: The empty cycles in a pipeline can be dedicated to other running programs, which
    assume that there are other currently running programs that need the processor's
    attention. **Simultaneous multithreading** (**SMT**) is a hardware design that
    enables such kinds of parallelism. Intel implements SMT named as **HyperThreading**
    in some of its processors. While SMT presents a single physical processor as two
    or more logical processors, a true multiprocessor system executes one thread per
    processor, thus achieving simultaneous execution. A multicore processor includes
    two or more processors per chip, but has the properties of a multiprocessor system.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 管道中的空闲周期可以被分配给其他正在运行的程序，这些程序假设还有其他当前正在运行且需要处理器注意力的程序。**同时多线程**（**SMT**）是一种硬件设计，它使得这种并行成为可能。英特尔在其某些处理器中实现了名为**HyperThreading**的SMT。虽然SMT将单个物理处理器呈现为两个或更多逻辑处理器，但真正的多处理器系统每个处理器执行一个线程，从而实现同时执行。多核处理器每个芯片包含两个或更多处理器，但具有多处理器系统的特性。
- en: In general, multicore processors significantly outperform SMT processors. Performance
    on SMT processors can vary by the use case. It peaks in those cases where code
    is highly variable or threads do not compete for the same hardware resources,
    and dips when the threads are cache-bound on the same processor. What is also
    important is that some programs are simply not inherently parallel. In such cases
    it may be hard to make them go faster without the explicit use of threads in the
    program.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，多核处理器的性能显著优于SMT处理器。SMT处理器的性能可能会根据使用案例而变化。在代码高度可变或线程不竞争相同硬件资源的情况下，性能达到峰值，而当线程在相同处理器上缓存绑定时，性能则会下降。同样重要的是，有些程序本身并不是天生并行的。在这种情况下，如果没有在程序中显式使用线程，可能很难使它们运行得更快。
- en: Memory systems
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存系统
- en: It is important to understand the memory performance characteristics to know
    the likely impact on the programs we write. Data-intensive programs that are also
    inherently parallel, such as audio/video processing and scientific computation,
    are largely limited by memory bandwidth, not by the processor. Adding processors
    would not make them faster unless the memory bandwidth is also increased. Consider
    another class of programs, such as 3D graphics rendering or database systems that
    are limited mainly by memory latency but not the memory bandwidth. SMT can be
    highly suitable for such programs, where threads do not compete for the same hardware
    resources.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 理解内存性能特性对于了解我们编写的程序可能产生的影响非常重要。数据密集型且天生并行的程序，如音频/视频处理和科学计算，主要受限于内存带宽，而不是处理器。除非增加内存带宽，否则增加处理器不会使它们更快。考虑另一类程序，如3D图形渲染或主要受限于内存延迟但不是内存带宽的数据库系统。SMT对于这类程序非常适用，在这些程序中，线程不竞争相同的硬件资源。
- en: Memory access roughly constitutes a quarter of all instructions executed by
    a processor. A code block typically begins with memory-load instructions and the
    remainder portion depends on the loaded data. This stalls the instructions and
    prevents large-scale, instruction-level parallelism. As if that was not bad enough,
    even superscalar processors (which can issue more than one instruction per clock
    cycle) can issue, at most, two memory instructions per cycle. Building fast memory
    systems is limited by natural factors such as the speed of light. It impacts the
    signal round trip to the RAM. This is a natural hard limit and any optimization
    can only work around it.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 内存访问大致占处理器执行的所有指令的四分之一。代码块通常以内存加载指令开始，其余部分取决于加载的数据。这会导致指令停滞，并防止大规模的指令级并行。更糟糕的是，即使是超标量处理器（每时钟周期可以发出多个指令）也最多只能在每个周期发出两个内存指令。构建快速内存系统受限于自然因素，如光速。它影响信号往返到RAM。这是一个自然的硬限制，任何优化都只能绕过它。
- en: Data transfer between the processor and motherboard chipset is one of the factors
    that induce memory latency. This is countered using a **faster front-side bus**
    (**FSB**). Nowadays, most modern processors fix this problem by integrating the
    memory controller directly at the chip level. The significant difference between
    the processor versus memory latencies is known as the **memory wall**. This has
    plateaued in recent times due to processor clock speeds hitting power and heat
    limits, but notwithstanding this, memory latency continues to be a significant
    problem.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器和主板芯片组之间的数据传输是导致内存延迟的因素之一。这个问题通过使用**更快的前端总线**（**FSB**）来抵消。如今，大多数现代处理器通过在芯片级别直接集成内存控制器来解决这个问题。处理器与内存延迟之间的显著差异被称为**内存墙**。由于处理器时钟速度达到功率和热量限制，近年来这一现象已经趋于平稳，但尽管如此，内存延迟仍然是一个重大问题。
- en: Unlike CPUs, GPUs typically realize a sustained high-memory bandwidth. Due to
    latency hiding, they utilize the bandwidth even during a high number-crunching
    workload.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 与CPU不同，GPU通常实现持续的高内存带宽。由于延迟隐藏，它们在高数值计算工作负载期间也利用带宽。
- en: Cache
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**缓存**'
- en: 'To overcome the memory latency, modern processors employ a special type of
    very fast memory placed onto the processor chip or close to the chip. The purpose
    of the cache is to store the most recently used data from the memory. Caches are
    of different levels: **L1** cache is located on the processor chip; **L2** cache
    is bigger and located farther away from the processor compared to L1\. There is
    often an **L3** cache, which is even bigger and located farther from the processor
    than L2\. In Intel''s Haswell processor, the L1 cache is generally 64 kilobytes
    (32 KB instruction plus 32 KB data) in size, L2 is 256 KB per core, and L3 is
    8 MB.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服内存延迟，现代处理器在处理器芯片上或靠近芯片的地方放置了一种非常快速的内存。缓存的目的就是存储最近使用过的内存数据。缓存有不同的级别：**L1**缓存位于处理器芯片上；**L2**缓存比L1大，且比L1远离处理器。通常还有一个**L3**缓存，它比L2更大，且比L2更远离处理器。在英特尔的Haswell处理器中，L1缓存的大小通常是64千字节（32
    KB指令加32 KB数据），L2每个核心256 KB，L3是8 MB。
- en: While memory latency is very bad, fortunately caches seem to work very well.
    The L1 cache is much faster than accessing the main memory. The reported cache
    hit rates in real-world programs is 90 percent, which makes a strong case for
    caches. A cache works like a dictionary of memory addresses to a block of data
    values. Since the value is a block of memory, the caching of adjacent memory locations
    has mostly no additional overhead. Note that L2 is slower and bigger than L1,
    and L3 is slower and bigger than L2\. On Intel Sandybridge processors, register
    lookup is instantaneous; L1 cache lookup takes three clock cycles, L2 takes nine,
    L3 takes 21, and main memory access takes 150 to 400 clock cycles.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然内存延迟非常糟糕，幸运的是缓存似乎工作得非常好。L1缓存比访问主内存要快得多。在现实世界的程序中报告的缓存命中率是90%，这为缓存提供了强有力的论据。缓存就像是一个内存地址到数据值块的字典。由于值是一块内存，因此相邻内存位置的缓存几乎没有额外的开销。请注意，L2比L1慢且大，L3比L2慢且大。在英特尔的Sandybridge处理器上，寄存器查找是瞬时的；L1缓存查找需要三个时钟周期，L2需要九个，L3需要21个，而主内存访问需要150到400个时钟周期。
- en: Interconnect
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**互连**'
- en: 'A processor communicates with the memory and other processors via interconnect
    that are generally of two types of architecture: **Symmetric multiprocessing**
    (**SMP**) and **Non-uniform memory access** (**NUMA**). In SMP, a bus interconnects
    processors and memory with the help of bus controllers. The bus acts as a broadcast
    device for the end points. The bus often becomes a bottleneck with a large number
    of processors and memory banks. SMP systems are cheaper to build and harder to
    scale to a large number of cores compared to NUMA. In a NUMA system, collections
    of processors and memory are connected point to point to other such groups of
    processors and memory. Every such group is called a node. Local memory of a node
    is accessible by other nodes and vice versa. Intel''s **HyperTransport** and **QuickPath**
    interconnect technologies support NUMA.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器通过两种类型的架构的互连与内存和其他处理器进行通信：**对称多处理**（**SMP**）和**非一致性内存访问**（**NUMA**）。在SMP中，总线通过总线控制器将处理器和内存互连。总线充当广播设备。当处理器和内存银行数量较多时，总线往往会成为瓶颈。与NUMA相比，SMP系统在构建成本上更低，但扩展到大量核心更困难。在NUMA系统中，处理器和内存的集合通过点到点的方式连接到其他类似的处理器和内存组。每个这样的组被称为一个节点。节点的本地内存可以被其他节点访问，反之亦然。英特尔公司的**HyperTransport**和**QuickPath**互连技术支持NUMA。
- en: Storage and networking
  id: totrans-517
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**存储和网络**'
- en: Storage and networking are the most commonly used hardware components besides
    the processor, cache, and memory. Many of the real-world applications are more
    often I/O bound than execution-bound. Such I/O technologies are continuously advancing
    and there is a wide variety of components available in the market. The consideration
    of such devices should be based on the exact performance and reliability characteristics
    for the use case. Another important criterion is to know how well they are supported
    by the target operating system drivers. Current day storage technologies mostly
    build upon hard disks and solid state drives. The applicability of network devices
    and protocols vary widely as per the business use case. A detailed discussion
    of I/O hardware is beyond the scope of this book.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 存储和网络是除了处理器、缓存和内存之外最常用的硬件组件。许多现实世界应用程序往往是 I/O 密集型而不是执行密集型。这样的 I/O 技术正在不断进步，市场上可供选择的组件种类繁多。考虑这些设备应基于具体的使用案例的性能和可靠性特征。另一个重要标准是了解它们在目标操作系统驱动程序中的支持情况。当前存储技术大多基于硬盘和固态硬盘。网络设备和协议的适用性根据业务用例而大相径庭。I/O
    硬件的详细讨论超出了本书的范围。
- en: The Java Virtual Machine
  id: totrans-519
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java 虚拟机
- en: The Java Virtual Machine is a bytecode-oriented, garbage-collected virtual machine
    that specifies its own instruction set. The instructions have equivalent bytecodes
    that are interpreted and compiled to the underlying OS and hardware by the **Java
    Runtime Environment** (**JRE**). Objects are referred to using symbolic references.
    The data types in the JVM are fully standardized as a single spec across all JVM
    implementations on all platforms and architectures. The JVM also follows the network
    byte order, which means communication between Java programs on different architectures
    can happen using the big-endian byte order. **Jvmtop** ([https://code.google.com/p/jvmtop/](https://code.google.com/p/jvmtop/))
    is a handy JVM monitoring tool similar to the top command in Unix-like systems.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: Java 虚拟机是一个以字节码为导向、具有垃圾回收功能的虚拟机，它指定了自己的指令集。指令有等效的字节码，由 **Java 运行时环境**（**JRE**）解释和编译为底层操作系统和硬件。对象通过符号引用来引用。JVM
    中的数据类型在所有平台和架构上的 JVM 实现中都是完全标准化的，作为一个单一的规范。JVM 还遵循网络字节序，这意味着在不同架构上的 Java 程序之间的通信可以使用大端字节序进行。**Jvmtop**（[https://code.google.com/p/jvmtop/](https://code.google.com/p/jvmtop/))
    是一个方便的 JVM 监控工具，类似于 Unix-like 系统中的 top 命令。
- en: The just-in-time compiler
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即时编译器
- en: The **just-in-time** (**JIT**) compiler is part of the JVM. When the JVM starts
    up, the JIT compiler knows hardly anything about the running code so it simply
    interprets the JVM bytecodes. As the program keeps running, the JIT compiler starts
    profiling the code by collecting statistics and analyzing the call and bytecode
    patterns. When a method call count exceeds a certain threshold, the JIT compiler
    applies a number of optimizations to the code. Most common optimizations are inlining
    and native code generating. The final and static methods and classes are great
    candidates for inlining. JIT compilation does not come without a cost; it occupies
    memory to store the profiled code and sometimes it has to revert the wrong speculative
    optimization. However, JIT compilation is almost always amortized over a long
    duration of code execution. In rare cases, turning off JIT compilation may be
    useful if either the code is too large or there are no hotspots in the code due
    to infrequent execution.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '**即时编译器**（**JIT**）是 JVM 的一部分。当 JVM 启动时，JIT 编译器对正在运行的代码几乎一无所知，因此它只是简单地解释 JVM
    字节码。随着程序的持续运行，JIT 编译器开始通过收集统计数据和分析调用和字节码模式来分析代码。当方法调用次数超过某个阈值时，JIT 编译器会对代码应用一系列优化。最常见的优化是内联和本地代码生成。最终和静态方法和类是内联的绝佳候选者。JIT
    编译并非没有代价；它占用内存来存储分析过的代码，有时它还必须撤销错误的推测性优化。然而，JIT 编译几乎总是被长期代码执行所摊销。在罕见的情况下，如果代码太大或者由于执行频率低而没有热点，关闭
    JIT 编译可能是有用的。'
- en: 'A JRE has typically two kinds of JIT compilers: client and server. Which JIT
    compiler is used by default depends on the type of hardware and platform. The
    client JIT compiler is meant for client programs such as command-line and desktop
    applications. We can start the JRE with the `-server` option to invoke the server
    JIT compiler, which is really meant for long-running programs on a server. The
    threshold for JIT compilation is higher in the server than the client. The difference
    in the two kinds of JIT compilers is that the client targets upfront, visible
    lower latency, and the server is assumed to be running on a high-resource hardware
    and tries to optimize for throughput.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: JRE 通常有两种类型的 JIT 编译器：客户端和服务器。默认使用哪种 JIT 编译器取决于硬件和平台类型。客户端 JIT 编译器是为客户端程序（如命令行和桌面应用程序）设计的。我们可以通过使用
    `-server` 选项启动 JRE 来调用服务器 JIT 编译器，它实际上是为服务器上运行的长运行程序设计的。服务器中 JIT 编译的阈值高于客户端。两种类型
    JIT 编译器的区别在于，客户端针对的是低延迟，而服务器假设运行在高资源硬件上，并试图优化吞吐量。
- en: The JIT compiler in the Oracle HotSpot JVM observes the code execution to determine
    the most frequently invoked methods, which are hotspots. Such hotspots are usually
    just a fraction of the entire code that can be cheap to focus on and optimize.
    The **HotSpot JIT** compiler is lazy and adaptive. It is lazy because it compiles
    only those methods to native code that have crossed a certain threshold, and not
    all the code that it encounters. Compiling to native code is a time-consuming
    process and compiling all code would be wasteful. It is adaptive to gradually
    increasing the aggressiveness of its compilation on frequently called code, which
    implies that the code is not optimized only once but many times over as the code
    gets executed repeatedly. After a method call crosses the first JIT compiler threshold,
    it is optimized and the counter is reset to zero. At the same time, the optimization
    count for the code is set to one. When the call exceeds the threshold yet again,
    the counter is reset to zero and the optimization count is incremented; and this
    time a more aggressive optimization is applied. This cycle continues until the
    code cannot be optimized anymore.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle HotSpot JVM 中的 JIT 编译器会观察代码执行以确定最频繁调用的方法，这些方法是热点。这些热点通常只是整个代码的一小部分，可以低成本地关注和优化。**HotSpot
    JIT** 编译器是懒惰和自适应的。它是懒惰的，因为它只编译那些超过一定阈值的、被调用的方法到原生代码，而不是它遇到的全部代码。编译到原生代码是一个耗时的过程，编译所有代码将是浪费的。它是自适应的，因为它会逐渐增加对频繁调用代码编译的积极性，这意味着代码不是只优化一次，而是在代码重复执行的过程中多次优化。当一个方法调用超过第一个
    JIT 编译器的阈值后，它就会被优化，计数器重置为零。同时，代码的优化计数设置为1。当调用再次超过阈值时，计数器重置为零，优化计数增加；这次应用更积极的优化。这个过程会一直持续到代码不能再被优化为止。
- en: 'The HotSpot JIT compiler does a whole bunch of optimizations. Some of the most
    prominent ones are as follows:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: HotSpot JIT 编译器执行了许多优化。其中一些最显著的优化如下：
- en: '**Inlining**: Inlining of methods—very small methods, the static and final
    methods, methods in final classes, and small methods involving only primitive
    numerics are prime candidates for inlining.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内联**：方法内联——非常小的方法、静态和最终方法、最终类中的方法以及只涉及原始数值的小方法是最适合内联的候选者。'
- en: '**Lock elimination**: Locking is a performance overhead. Fortunately, if the
    lock object monitor is not reachable from other threads, the lock is eliminated.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁消除**：锁定是一个性能开销。幸运的是，如果锁对象监视器无法从其他线程访问，则可以消除锁。'
- en: '**Virtual call elimination**: Often, there is only one implementation for an
    interface in a program. The JIT compiler eliminates the virtual call and replaces
    that with a direct method call on the class implementation object.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟调用消除**：通常，程序中一个接口只有一个实现。JIT 编译器消除虚拟调用，并用类实现对象上的直接方法调用替换它。'
- en: '**Non-volatile memory write elimination**: The non-volatile data members and
    references in an object are not guaranteed to be visible by the threads other
    than the current thread. This criterion is utilized not to update such references
    in memory and rather use hardware registers or the stack via native code.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非易失性内存写入消除**：对象中的非易失性数据成员和引用不一定能被除当前线程外的其他线程看到。这个标准被用来不在内存中更新这样的引用，而是通过原生代码使用硬件寄存器或栈。'
- en: '**Native code generation**: The JIT compiler generates native code for frequently
    invoked methods together with the arguments. The generated native code is stored
    in the code cache.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原生代码生成**：JIT编译器为频繁调用的方法及其参数生成原生代码。生成的原生代码存储在代码缓存中。'
- en: '**Control flow and local optimizations**: The JIT compiler frequently reorders
    and splits the code for better performance. It also analyzes the branching of
    control and optimizes code based on that.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制流和局部优化**：JIT编译器经常重新排序和拆分代码以提高性能。它还分析控制流的分支，并根据这些分析优化代码。'
- en: There should rarely be any reason to disable JIT compilation, but it can be
    done by passing the `-Djava.compiler=NONE` parameter when starting the JRE. The
    default compile threshold can be changed by passing `-XX:CompileThreshold=9800`
    to the JRE executable where `9800` is the example threshold. The `XX:+PrintCompilation`
    and `-XX:-CITime` options make the JIT compiler print the JIT statistics and time
    spent on JIT.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有理由禁用JIT编译，但可以通过在启动JRE时传递`-Djava.compiler=NONE`参数来实现。默认的编译阈值可以通过传递`-XX:CompileThreshold=9800`到JRE可执行文件来更改，其中`9800`是示例阈值。`XX:+PrintCompilation`和`-XX:-CITime`选项使JIT编译器打印JIT统计信息和JIT花费的时间。
- en: Memory organization
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存组织
- en: 'The memory used by the JVM is divided into several segments. JVM, being a stack-based
    execution model, one of the memory segments is the stack area. Every thread is
    given a stack where the stack frames are stored in **Last-in-First-out** (**LIFO**)
    order. The stack includes a **program counter** (**PC**) that points to the instruction
    in the JVM memory currently being executed. When a method is called, a new stack
    frame is created containing the local variable array and the operand stack. Contrary
    to conventional stacks, the operand stack holds instructions to load local variable
    / field values and computation results—a mechanism that is also used to prepare
    method parameters before a call and to store the return value. The stack frame
    itself may be allocated on the heap. The easiest way to inspect the order of stack
    frames in the current thread is to execute the following code:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: JVM使用的内存被划分为几个部分。作为基于栈的执行模型，JVM的一个内存段是栈区。每个线程都有一个栈，其中栈帧以**后进先出**（**LIFO**）的顺序存储。栈包括一个**程序计数器**（**PC**），它指向JVM内存中当前正在执行的指令。当调用一个方法时，会创建一个新的栈帧，其中包含局部变量数组和操作数栈。与传统栈不同，操作数栈包含加载局部变量/字段值和计算结果的指令——这种机制也用于在调用之前准备方法参数以及存储返回值。栈帧本身可能分配在堆上。要检查当前线程中栈帧的顺序，最简单的方法是执行以下代码：
- en: '[PRE59]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: When a thread requires more stack space than what the JVM can provide, `StackOverflowError`
    is thrown.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 当线程需要的栈空间超过JVM能提供的空间时，会抛出`StackOverflowError`。
- en: The heap is the main memory area where the object and array allocations are
    done. It is shared across all JVM threads. The heap may be of a fixed size or
    expanding, depending on the arguments passed to the JRE on startup. Trying to
    allocate more heap space than what the JVM can make room for results in `OutOfMemoryError`
    to be thrown. The allocations in the heap are subject to garbage collection. When
    an object is no more reachable via any reference, it is garbage collected, with
    the notable exception of weak, soft, and phantom references. Objects pointed to
    by non-strong references take longer to GC.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 堆是对象和数组分配的主要内存区域，它被所有JVM线程共享。堆的大小可能是固定的或可扩展的，这取决于启动JRE时传递的参数。尝试分配比JVM能提供的空间更多的堆空间会导致抛出`OutOfMemoryError`。堆上的分配受垃圾回收的影响。当一个对象不再通过任何引用可达时，它将被垃圾回收，值得注意的是，弱、软和虚引用除外。由非强引用指向的对象在GC（垃圾回收）中花费的时间更长。
- en: The method area is logically a part of the heap memory and contains per-class
    structures such as the field and method information, runtime constant pool, code
    for method, and constructor bodies. It is shared across all JVM threads. In the
    Oracle HotSpot JVM (up to Version 7), the method area is found in a memory area
    called the **permanent generation**. In HotSpot Java 8, the permanent generation
    is replaced by a native memory area called **Metaspace**.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 方法区在逻辑上是堆内存的一部分，包含诸如字段和方法信息、运行时常量池、方法代码和构造函数体等类结构。它是所有JVM线程共享的。在Oracle HotSpot
    JVM（至版本7）中，方法区位于一个称为**永久代**的内存区域。在HotSpot Java 8中，永久代被一个称为**元空间**的本地内存区域所取代。
- en: '![Memory organization](img/3642_04_01.jpg)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![内存组织](img/3642_04_01.jpg)'
- en: The JVM contains the native code and the Java bytecode to be provided to the
    Java API implementation and the JVM implementation. The native code call stack
    is maintained separately for each thread stack. The JVM stack contains the Java
    method calls. Please note that the JVM spec for Java SE 7 and 8 does not imply
    a native method stack, but for Java SE 5 and 6, it does.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: HotSpot heap and garbage collection
  id: totrans-541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Oracle HotSpot JVM uses a generational heap. The three main generations
    are **Young**, **Tenured** (old), and **Perm** (permanent) (up to HotSpot JDK
    1.7 only). As objects survive garbage collection, they move from **Eden** to **Survivor**
    and from **Survivor** to **Tenured** spaces. The new instances are allocated in
    the **Eden** segment, which is a very cheap operation (as cheap as a pointer bump,
    and faster than a C `malloc` call), if it already has sufficient free space. When
    the Eden area does not have enough free space, a minor GC is triggered. This copies
    the live objects from **Eden** into the **Survivor** space. In the same operation,
    live objects are checked in **Survivor-1** and copied over to **Survivor-2**,
    thus keeping the live objects only in **Survivor-2**. This scheme keeps **Eden**
    and **Survivor-1** empty and unfragmented to make new allocations, and is known
    as **copy collection**.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '![HotSpot heap and garbage collection](img/3642_04_02.jpg)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
- en: 'After a certain survival threshold in the young generation, the objects are
    moved to the tenured/old generation. If it is not possible to do a minor GC, a
    major GC is attempted. The major GC does not use copying, but rather relies on
    mark-and-sweep algorithms. We can use throughput collectors (**Serial**, **Parallel**,
    and **ParallelOld**) or low-pause collectors (**Concurrent** and **G1**) for the
    old generation. The following table shows a non-exhaustive list of options to
    be used for each collector type:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '| Collector name | JVM flag |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '| Serial | -XX:+UseSerialGC |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '| Parallel | -XX:+UseParallelGC |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '| Parallel Compacting | -XX:+UseParallelOldGC |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| Concurrent | -XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled
    |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '| G1 | -XX:+UseG1GC |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: 'The previously mentioned flags can be used to start the Java runtime. For example,
    in the following command, we start the server JVM with a 4 GB heap using Parallel
    compacting GC:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Sometimes, due to running full GC multiple times, the tenured space may have
    become so fragmented that it may not be feasible to move objects from Survivor
    to Tenured spaces. In those cases, a full GC with compaction is triggered. During
    this period, the application may appear unresponsive due to the full GC in action.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Measuring memory (heap/stack) usage
  id: totrans-555
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the prime reasons for a performance hit in the JVM is garbage collection.
    It certainly helps to know how heap memory is used by the objects we create and
    how to reduce the impact on GC by means of a lower footprint. Let's inspect how
    the representation of an object may lead to heap space.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 'Every (uncompressed) object or array reference on a 64-bit JVM is 16 bytes
    long. On a 32-bit JVM, every reference is 8 bytes long. As the 64-bit architecture
    is becoming more commonplace now, the 64-bit JVM is more likely to be used on
    servers. Fortunately, for a heap size of up to 32 GB, the JVM (Java 7) can use
    compressed pointers (default behavior) that are only 4 bytes in size. Java 8 VMs
    can address up to 64 GB heap size via compressed pointers as seen in the following
    table:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Uncompressed | Compressed | 32-bit |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: '| Reference (pointer) | 8 | 4 | 4 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
- en: '| Object header | 16 | 12 | 8 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| Array header | 24 | 16 | 12 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| Superclass padding | 8 | 4 | 4 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: 'This table illustrates pointer sizes in different modes (reproduced with permission
    from Attila Szegedi: [http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20](http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20)).'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw in the previous chapter how many bytes each primitive type takes. Let''s
    see how the memory consumption of the composite types looks with compressed pointers
    (a common case) on a 64-bit JVM with a heap size smaller than 32 GB:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '| Java Expression | 64-bit memory usage | Description (b = bytes, padding toward
    memory word size in approximate multiples of 8) |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '| `new Object()` | 16 bytes | 12 b header + 4 b padding |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '| `new byte[0]` | 16 bytes | 12 b `obj` header + 4 b `int` length = 16 b array
    header |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| `new String("foo")` | 40 bytes (interned for literals) | 12 b header + (12
    b array header + 6 b char-array content + 4 b length + 2 b padding = 24 b) + 4
    b hash |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| `new Integer(3)` | 16 bytes (boxed integer) | 12 b header + 4 b `int` value
    |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| `new Long(4)` | 24 bytes (boxed long) | 12 b header + 8 b `long` value +
    4 b padding |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| `class A { byte x; }``new A();` | 16 bytes | 12 b header + 1 b value + 3
    b padding |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| `class B extends A {byte y;}``new B();` | 24 bytes (subclass padding) | 12
    b reference + (1 b value + 7 b padding = 8 b) for A + 1 b for value of `y` + 3
    b padding |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| `clojure.lang.Symbol.intern("foo")``// clojure ''foo` | 104 bytes (40 bytes
    interned) | 12 b header + 12 b ns reference + (12 b name reference + 40 b interned
    chars) + 4 b `int` hash + 12 b meta reference + (12 b `_str` reference + 40 b
    interned chars) – 40 b interned `str` |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| `clojure.lang.Keyword.intern("foo")``// clojure :foo` | 184 bytes (fully
    interned by factory method) | 12 b reference + (12 b symbol reference + 104 b
    interned value) + 4 b `int` hash + (12 b `_str` reference + 40 b interned `char`)
    |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: A comparison of space taken by a symbol and a keyword created from the same
    given string demonstrates that even though a keyword has slight overhead over
    a symbol, the keyword is fully interned and would provide better guard against
    memory consumption and thus GC over time. Moreover, the keyword is interned as
    a weak reference, which ensures that it is garbage collected when no keyword in
    memory is pointing to the interned value anymore.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较由同一给定字符串创建的符号和关键字所占用的空间，可以证明尽管关键字相对于符号有轻微的开销，但关键字是完全内部化的，这将提供更好的内存消耗和随时间进行的垃圾回收保护。此外，关键字作为弱引用进行内部化，这确保了当内存中没有关键字指向内部化值时，它将被垃圾回收。
- en: Determining program workload type
  id: totrans-578
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定程序工作负载类型
- en: We often need to determine whether a program is CPU/cache bound, memory bound,
    I/O bound or contention bound. When a program is I/O or contention bound, the
    CPU usage is generally low. You may have to use a profiler (we will see this in
    [Chapter 7](ch14.html "Chapter 7. Performance Optimization"), *Performance Optimization*)
    to find out whether threads are stuck due to resource contention. When a program
    is CPU/cache or memory bound, CPU usage may not be a clear indicator of the source
    of the bottleneck. In such cases, you may want to make an educated guess by inspecting
    cache misses in the program. On Linux systems tools such as **perf** ([https://perf.wiki.kernel.org/](https://perf.wiki.kernel.org/)),
    **cachegrind** ([http://valgrind.org/info/tools.html#cachegrind](http://valgrind.org/info/tools.html#cachegrind))
    and **oprofile** ([http://oprofile.sourceforge.net/](http://oprofile.sourceforge.net/))
    can help determine the volume of cache misses—a higher threshold may imply that
    the program is memory bound. However, using these tools with Java is not straightforward
    because Java's JIT compiler needs a warm-up until meaningful behavior can be observed.
    The project **perf-map-agent** ([https://github.com/jrudolph/perf-map-agent](https://github.com/jrudolph/perf-map-agent))
    can help generate method mappings that you can correlate using the `perf` utility.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要确定程序是CPU/缓存绑定、内存绑定、I/O绑定还是竞争绑定。当一个程序是I/O或竞争绑定时，CPU使用率通常较低。你可能需要使用分析器（我们将在第7章[性能优化](ch14.html
    "第7章。性能优化")中看到，*性能优化*)来确定线程是否因为资源竞争而陷入停滞。当一个程序是CPU/缓存或内存绑定时，CPU使用率可能不是瓶颈来源的明确指标。在这种情况下，你可能想要通过检查程序中的缓存缺失来进行有根据的猜测。在Linux系统上，工具如**perf**
    ([https://perf.wiki.kernel.org/](https://perf.wiki.kernel.org/))、**cachegrind**
    ([http://valgrind.org/info/tools.html#cachegrind](http://valgrind.org/info/tools.html#cachegrind))和**oprofile**
    ([http://oprofile.sourceforge.net/](http://oprofile.sourceforge.net/))可以帮助确定缓存缺失的数量——更高的阈值可能意味着程序是内存绑定的。然而，由于Java的JIT编译器需要预热才能观察到有意义的行为，因此使用这些工具与Java结合并不简单。项目**perf-map-agent**
    ([https://github.com/jrudolph/perf-map-agent](https://github.com/jrudolph/perf-map-agent))可以帮助生成你可以使用`perf`实用程序关联的方法映射。
- en: Tackling memory inefficiency
  id: totrans-580
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决内存效率低下问题
- en: 'In earlier sections in this chapter we discussed that unchecked memory access
    may become a bottleneck. As of Java 8, due to the way the heap and object references
    work, we cannot fully control the object layout and memory access patterns. However,
    we can take care of the frequently executed blocks of code to consume less memory
    and attempt to make them cache-bound instead of memory-bound at runtime. We can
    consider a few techniques to lower memory consumption and randomness in access:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面的部分，我们讨论了未经检查的内存访问可能成为瓶颈。截至Java 8，由于堆和对象引用的工作方式，我们无法完全控制对象布局和内存访问模式。然而，我们可以关注频繁执行的代码块，以减少内存消耗，并尝试在运行时使它们成为缓存绑定而不是内存绑定。我们可以考虑一些降低内存消耗和访问随机性的技术：
- en: Primitive locals (long, double, boolean, char, etc) in the JVM are created on
    the stack. The rest of the objects are created on the heap and only their references
    are stored in the stack. Primitives have a low overhead and do not require memory
    indirection for access, and are hence recommended.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JVM中的原始本地变量（如long、double、boolean、char等）是在栈上创建的。其余的对象是在堆上创建的，并且只有它们的引用存储在栈上。原始变量具有较低的开销，并且不需要内存间接访问来访问，因此推荐使用。
- en: Data laid out in the main memory in a sequential fashion is faster to access
    than randomly laid out data. When we use a large (say more than eight elements)
    persistent map, the data stored in tries may not be sequentially laid out in memory,
    rather they would be randomly laid out in the heap. Moreover both keys and values
    are stored and accessed. When you use records (`defrecord`) and types (`deftype`),
    not only do they provide array/class semantics for the layout of fields within
    them, they do not store the keys, which is very efficient compared to regular
    maps.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以顺序方式在主内存中布局的数据比随机布局的数据访问更快。当我们使用一个大的（比如说超过八个元素）持久映射时，存储在tries中的数据可能不会在内存中顺序布局，而是在堆中随机布局。此外，键和值都会被存储和访问。当你使用记录（`defrecord`）和类型（`deftype`）时，它们不仅为它们内部的字段布局提供数组/类语义，而且它们不存储键，与常规映射相比，这非常高效。
- en: Reading large content from a disk or the network may have an adverse impact
    on performance due to random memory roundtrips. In [Chapter 3](ch10.html "Chapter 3. Leaning
    on Java"), *Leaning on Java*, we briefly discussed memory-mapped byte buffers.
    You can leverage memory-mapped buffers to minimize fragmented object allocation/access
    on the heap. While libraries such as `nio` ([https://github.com/pjstadig/nio/](https://github.com/pjstadig/nio/))
    and `clj-mmap` ([https://github.com/thebusby/clj-mmap](https://github.com/thebusby/clj-mmap))
    help us deal with memory-mapped buffers, `bytebuffer` ([https://github.com/geoffsalmon/bytebuffer](https://github.com/geoffsalmon/bytebuffer)),
    and `gloss` ([https://github.com/ztellman/gloss](https://github.com/ztellman/gloss))
    let us work with byte buffers. There are also alternate abstractions such as iota
    ([https://github.com/thebusby/iota](https://github.com/thebusby/iota)) that help
    us deal with large files as collections.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从磁盘或网络读取大量内容可能会由于随机内存往返而对性能产生不利影响。在[第3章](ch10.html "第3章。依赖Java")《依赖Java》中，我们简要讨论了内存映射字节数组缓冲区。你可以利用内存映射缓冲区来最小化堆上的碎片化对象分配/访问。虽然像`nio`([https://github.com/pjstadig/nio/](https://github.com/pjstadig/nio/))和`clj-mmap`([https://github.com/thebusby/clj-mmap](https://github.com/thebusby/clj-mmap))这样的库帮助我们处理内存映射缓冲区，`bytebuffer`([https://github.com/geoffsalmon/bytebuffer](https://github.com/geoffsalmon/bytebuffer))和`gloss`([https://github.com/ztellman/gloss](https://github.com/ztellman/gloss))则让我们能够处理字节数组缓冲区。还有其他抽象，如iota([https://github.com/thebusby/iota](https://github.com/thebusby/iota))，它帮助我们以集合的形式处理大文件。
- en: Given that memory bottleneck is a potential performance issue in data-intensive
    programs, lowering memory overhead goes a long way in avoiding performance risk.
    Understanding low-level details of the hardware, the JVM and Clojure's implementation
    helps us choose the appropriate techniques to tackle the memory bottleneck issue.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存瓶颈是数据密集型程序中潜在的性能问题，降低内存开销在很大程度上有助于避免性能风险。了解硬件、JVM和Clojure实现的低级细节有助于我们选择适当的技巧来解决内存瓶颈问题。
- en: Measuring latency with Criterium
  id: totrans-586
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Criterium测量延迟
- en: 'Clojure has a neat little macro called `time` that evaluates the body of code
    passed to it, and then prints out the time it took and simply returns the value.
    However, we can note that often the time taken to execute the code varies quite
    a bit across various runs:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure有一个叫做`time`的小巧的宏，它会评估传递给它的代码的主体，然后打印出所花费的时间，并简单地返回值。然而，我们可以注意到，执行代码所需的时间在不同的运行中变化很大：
- en: '[PRE61]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: There are several reasons associated to this variance in behavior. When cold
    started, the JVM has its heap segments empty and is unaware of the code path.
    As the JVM keeps running, the heap fills up and the GC patterns start becoming
    noticeable. The JIT compiler gets a chance to profile the different code paths
    and optimize them. Only after quite some GC and JIT compilation rounds, does the
    JVM performance become less unpredictable.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 与此行为差异相关的有几个原因。当JVM冷启动时，其堆段为空，并且对代码路径一无所知。随着JVM的持续运行，堆开始填满，GC模式开始变得明显。JIT编译器有机会分析不同的代码路径并进行优化。只有在经过相当多的GC和JIT编译轮次之后，JVM的性能才变得不那么不可预测。
- en: 'Criterium ([https://github.com/hugoduncan/criterium](https://github.com/hugoduncan/criterium))
    is a Clojure library to scientifically measure the latency of Clojure expressions
    on a machine. A summary of how it works can be found at the Criterium project
    page. The easiest way to use Criterium is to use it with Leiningen. If you want
    Criterium to be available only in the REPL and not as a project dependency, add
    the following entry to the `~/.lein/profiles.clj` file:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Another way is to include `criterium` in your project in the `project.clj`
    file:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once done with the editing of the file, launch REPL using `lein repl`:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Now, we can see that, on average, the expression took 31.6 ms on a certain test
    machine.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: Criterium and Leiningen
  id: totrans-597
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, Leiningen starts the JVM in a low-tiered compilation mode, which
    causes it to start up faster but impacts the optimizations that the JRE can perform
    at runtime. To get the best effects when running tests with Criterium and Leiningen
    for a server-side use case, be sure to override the defaults in `project.clj`
    as follows:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The `^:replace` hint causes Leiningen to replace its own defaults with what
    is provided under the `:jvm-opts` key. You may like to add more parameters as
    needed, such as a minimum and maximum heap size to run the tests.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-601
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of a software system is directly impacted by its hardware components,
    so understanding how the hardware works is crucial. The processor, caches, memory,
    and I/O subsystems have different performance behaviors. Clojure, being a hosted
    language, understanding the performance properties of the host, that is, the JVM,
    is equally important. The Criterium library is useful for measuring the latency
    of the Clojure code—we will discuss Criterium again in [Chapter 6](ch13.html "Chapter 6. Measuring
    Performance"), *Measuring Performance*. In the next chapter we will look at the
    concurrency primitives in Clojure and their performance characteristics.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5. Concurrency
  id: totrans-603
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Concurrency was one of the chief design goals of Clojure. Considering the concurrent
    programming model in Java (the comparison with Java is due to it being the predominant
    language on the JVM), it is not only too low level, but rather tricky to get right
    that without strictly following the patterns, one is more likely to shoot oneself
    in the foot. Locks, synchronization, and unguarded mutation are recipes for the
    concurrency pitfalls, unless exercised with extreme caution. Clojure''s design
    choices deeply influence the way in which the concurrency patterns can be achieved
    in a safe and functional manner. In this chapter, we will discuss:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: The low level concurrency support at the hardware and JVM level
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concurrency primitives of Clojure—atoms, agents, refs and vars
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The built-in concurrency that features in Java safe, and its usefulness with
    Clojure
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelization with the Clojure features and reducers
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-level concurrency
  id: totrans-609
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency cannot be achieved without explicit hardware support. We discussed
    about SMT and the multi-core processors in the previous chapters. Recall that
    every processor core has its own L1 cache, and several cores share the L2 cache.
    The shared L2 cache provides a fast mechanism to the processor cores to coordinate
    their cache access, eliminating the comparatively expensive memory access. Additionally,
    a processor buffers the writes to memory into something known as a **dirty write-buffer**.
    This helps the processor to issue a batch of memory update requests, reorder the
    instructions, and determine the final value to write to memory, known as **write
    absorption**.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 没有显式的硬件支持，无法实现并发。我们在前几章讨论了SMT和多核处理器。回想一下，每个处理器核心都有自己的L1缓存，几个核心共享L2缓存。共享的L2缓存为处理器核心提供了一个快速机制来协调它们的缓存访问，消除了相对昂贵的内存访问。此外，处理器将写入内存的操作缓冲到一个称为**脏写缓冲区**的东西中。这有助于处理器发布一系列内存更新请求，重新排序指令，并确定写入内存的最终值，称为**写吸收**。
- en: Hardware memory barrier (fence) instructions
  id: totrans-611
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件内存屏障（fence）指令
- en: Memory access reordering is great for a sequential (single-threaded) program
    performance, but it is hazardous for the concurrent programs where the order of
    memory access in one thread may disrupt the expectations in another thread. The
    processor needs the means of synchronizing the access in such a way that memory
    reordering is either compartmentalized in code segments that do not care, or is
    prevented where it might have undesirable consequences. The hardware supports
    such a safety measure in terms of a "memory barrier" (also known as "fence").
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 内存访问重排序对于顺序（单线程）程序性能很有用，但对于并发程序来说却很危险，因为一个线程中内存访问的顺序可能会破坏另一个线程的预期。处理器需要同步访问的手段，以便内存重排序要么被限制在代码段中，这些代码段不关心，要么在可能产生不良后果的地方被阻止。硬件通过“内存屏障”（也称为“fence”）提供这种安全措施。
- en: There are several kinds of memory barrier instructions found in different architectures,
    with potentially different performance characteristics. The compiler (or the JIT
    compiler in the case of the JVM) usually knows about the fence instructions on
    the architectures that it runs on. The common fence instructions are read, write,
    acquire, and release barrier, and more. The barriers do not guarantee the latest
    data, rather they only control the relative ordering of memory access. Barriers
    cause the write-buffer to be flushed after all the writes are issued, before the
    barrier is visible to the processor that issued it.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的架构中发现了多种内存屏障指令，具有不同的性能特征。编译器（或JVM中的JIT编译器）通常了解它在上面运行的架构上的fence指令。常见的fence指令有读、写、获取和释放屏障等。屏障不保证最新的数据，而是只控制内存访问的相对顺序。屏障导致在屏障对发出它的处理器可见之前，将写缓冲区刷新完毕。
- en: Read and write barriers control the order of reads and writes respectively.
    Writes happen via a write-buffer; but reads may happen out of order, or from the
    write-buffer. To guarantee the correct ordering, acquire, and release, blocks/barriers
    are used. Acquire and release are considered "half barriers"; both of them together
    (acquire and release) form a "full barrier". A full barrier is more expensive
    than a half barrier.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 读写屏障分别控制读和写的顺序。写操作通过写缓冲区进行；但读操作可能发生顺序混乱，或者来自写缓冲区。为了保证正确的顺序，使用获取和释放块/屏障。获取和释放被认为是“半屏障”；两者结合（获取和释放）形成一个“全屏障”。全屏障比半屏障更昂贵。
- en: Java support and the Clojure equivalent
  id: totrans-615
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java支持和Clojure的等效功能
- en: 'In Java, the memory barrier instructions are inserted by the higher level coordination
    primitives. Even though fence instructions are expensive (hundreds of cycles)
    to run, they provide a safety net that makes accessing shared variables safe within
    the critical sections. In Java, the `synchronized` keyword marks a "critical section",
    which can be executed by only one thread at a time, thus making is a tool for
    "mutual exclusion". In Clojure, the equivalent of Java''s `synchronized` is the
    `locking` macro:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中，内存屏障指令是通过高级协调原语插入的。尽管fence指令的运行成本很高（数百个周期），但它们提供了一个安全网，使得在关键部分内访问共享变量是安全的。在Java中，`synchronized`关键字标记一个“关键部分”，一次只能由一个线程执行，因此它是一个“互斥”工具。在Clojure中，Java的`synchronized`的等价物是`locking`宏：
- en: '[PRE66]'
  id: totrans-617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The `locking` macro builds upon two special forms, `monitor-enter` and `monitor-exit`.
    Note that the `locking` macro is a low-level and imperative solution just like
    Java's `synchronized` – their use is not considered idiomatic Clojure. The special
    forms `monitor-enter` and `monitor-exit` respectively enter and exit the lock
    object's "monitor" – they are even lower level and not recommended for direct
    use.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Someone measuring the performance of the code that uses such locking should
    be aware of its single-threaded versus the multi-threaded latencies. Locking in
    a single thread is cheap. However, the performance penalty starts kicking in when
    there are two or more threads contending for a lock on the same object monitor.
    A lock is acquired on the monitor of an object called the "intrinsic" or "monitor"
    lock. Object equivalence (that is, when the `=` function returns as true) is never
    used for the purpose of locking. Make sure that the object references are the
    same (that is, when `identical?` returns as true) when locking from different
    threads.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring a monitor lock by a thread entails a read barrier, which invalidates
    the thread-local cached data, the corresponding processor registers, and the cache
    lines. This forces a reread from the memory. On the other hand, releasing the
    monitor lock results in a write barrier, which flushes all the changes to memory.
    These are expensive operations that impact parallelism, but they ensure consistency
    of data for all threads.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: 'Java supports a `volatile` keyword for the data members in a class that guarantees
    read and write to an attribute outside of a synchronized block that would not
    be reordered. It is interesting to note that unless an attribute is declared `volatile`,
    it is not guaranteed to be visible in all the threads that are accessing it. The
    Clojure equivalent of Java''s `volatile` is the metadata called `^:volatile-mutable`
    that we discussed in [Chapter 3](ch10.html "Chapter 3. Leaning on Java"), *Leaning
    on Java*. An example of `volatile` in Java and Clojure is as follows:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Reading and writing a `volatile` data requires read-acquire or write-release
    respectively, which means we need only a half-barrier to individually read or
    write the value. Note that due to a half-barrier, the read-followed-by-write operations
    are not guaranteed to be atomic. For example, the `age++` expression first reads
    the value, then increments and sets it. This makes two memory operations, which
    is no more a half-barrier.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: 'Clojure 1.7 introduced a first class support for the volatile data using a
    new set of functions: `volatile!`, `vswap!`, `vreset!,` and `volatile?` These
    functions define volatile (mutable) data and work with that. However, make a note
    that these functions do not work with the volatile fields in `deftype`. You can
    see how to use them as follows:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Operations on volatile data are not atomic, which is why even creating a volatile
    (using `volatile!`) is considered potentially unsafe. In general, volatiles may
    be useful where read consistency is not a high priority but writes must be fast,
    such as real-time trend analysis, or other such analytics reporting. Volatiles
    may also be very useful when writing stateful transducers (refer to [Chapter 2](ch09.html
    "Chapter 2. Clojure Abstractions"), *Clojure Abstractions*), serving as very fast
    state containers. In the next sub-section, we will see the other state abstractions
    that are safer (and mostly slower) than volatiles.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: Atomic updates and state
  id: totrans-627
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a common use case to read a data element, execute some logic, and update
    with a new value. For single-threaded programs, it bears no consequences; but
    for concurrent scenarios, the entire operation must be carried out in a lockstep,
    as an atomic operation. This case is so common that many processors support this
    at the hardware level using a special Compare-and-swap (CAS) instruction, which
    is much cheaper than locking. On x86/x64 architectures, the instruction is called
    CompareExchange (CMPXCHG).
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it is possible that another thread updates the variable with
    the same value that the thread, which is working on the atomic update, is going
    to compare the old value against. This is known as the "ABA" problem. The set
    of instructions such as "Load-linked" (LL) and "Store-conditional" (SC), which
    are found in some other architectures, provide an alternative to CAS without the
    ABA problem. After the LL instruction reads the value from an address, the SC
    instruction to update the address with a new value will only go through if the
    address has not been updated since the LL instruction was successful.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: Atomic updates in Java
  id: totrans-630
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Java has a bunch of built-in lock free, atomic, thread safe compare-and-swap
    abstractions for the state management. They live in the `java.util.concurrent.atomic`
    package. For primitive types, such as boolean, integer, and long, there are the
    `AtomicBoolean`, `AtomicInteger`, and `AtomicLong` classes respectively. The latter
    two classes support additional atomic add/subtract operations. For atomic reference
    updates, there are the `AtomicReference`, `AtomicMarkableReference`, and `AtomicStampedReference`
    classes for the arbitrary objects. There is also a support available for arrays
    where the array elements can be updated atomically—`AtomicIntegerArray`, `AtomicLongArray`,
    and `AtomicReferenceArray`. They are easy to use; here is the example:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: However, where and how to use it is subjected to the update points and the logic
    in the code. The atomic updates are not guaranteed to be non-blocking. Atomic
    updates are not a substitute to locking in Java, but rather a convenience, only
    when the scope is limited to a compare and swap operation for one mutable variable.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: Clojure's support for atomic updates
  id: totrans-634
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clojure''s atomic update abstraction is called "atom". It uses `AtomicReference`
    under the hood. An operation on `AtomicInteger` or `AtomicLong` may be slightly
    faster than on the Clojure `atom`, because the former uses primitives. But neither
    of them are too cheap, due to the compare-and-swap instruction that they use in
    the CPU. The speed really depends on how frequently the mutation happens, and
    how the JIT compiler optimizes the code. The benefit of speed may not show up
    until the code is run several hundred thousand times, and having an atom mutated
    very frequently will increase the latency due to the retries. Measuring the latency
    under actual (or similar to actual) load can tell better. An example of using
    an atom is as follows:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The `swap!` function provides a notably different style of carrying out atomic
    updates than the `compareAndSwap(oldval, newval)` methods. While `compareAndSwap()`
    compares and sets the value, returning true if it's a success and false if it's
    a failure, `swap!` keeps on trying to update in an endless loop until it succeeds.
    This style is a popular pattern that is followed among Java developers. However,
    there is also a potential pitfall associated with the update-in-loop style. As
    the concurrency of the updaters gets higher, the performance of the update may
    gradually degrade. Then again, high concurrency on the atomic updates raises a
    question of whether or not uncoordinated updates was a good idea at all for the
    use-case. The `compare-and-set!` and `reset!` are pretty straightforward.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: The function passed to `swap!` is required to be pure (as in side effect free),
    because it is retried several times in a loop during contention. If the function
    is not pure, the side effect may happen as many times as the retries.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: 'It is noteworthy that atoms are not "coordinated", which means that when an
    atom is used concurrently by different threads, we cannot predict the order in
    which the operations work on it, and we cannot guarantee the end result as a consequence.
    The code we write around atoms should be designed with this constraint in mind.
    In many scenarios, atoms may not be a good fit due to the lack of coordination—watch
    out for that in the program design. Atoms support meta data and basic validation
    mechanism via extra arguments. The following examples illustrate these features:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The second important thing is that atoms support is adding and removing watches
    on them. We will discuss watches later in the chapter.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: Faster writes with atom striping
  id: totrans-642
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that atoms present contention when multiple threads try to update the
    state at the same time. This implies that atoms have great performance when the
    writes are infrequent. There are some use cases, for example metrics counters,
    where the writes need to be fast and frequent, but the reads are fewer and can
    tolerate some inconsistency. For such use cases, instead of directing all the
    updates to a single atom, we can maintain a bunch of atoms where each thread updates
    a different atom, thus reducing contention. Reads from these atoms cannot be guaranteed
    to be consistent. Let''s develop an example of such a counter:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'In the previous example, we created a vector called `counters` of the same
    size as the number of CPU cores in the computer, and initialize each element with
    an atom of initial value 0\. The function called `inc!` updates the counter by
    picking up a random atom from `counters`, and incrementing the value by 1\. We
    also assumed that `rand-int` distributes the picking up of atom uniformly across
    all the processor cores, so that we have almost zero contention. The `value` function
    simply walks over all the atoms and adds up their `deref`''ed values to return
    the counter value. The example uses `clojure.core/rand-int`, which depends on
    `java.lang.Math/random` (due to Java 6 support) to randomly find out the next
    counter atom. Let''s see how we can optimize this when using Java 7 or above:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Here, we `import` the `java.util.concurrent.ThreadLocalRandom` class, and define
    the `inc!` function to pick up the next random atom using `ThreadLocalRandom`.
    Everything else remains the same.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous agents and state
  id: totrans-648
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While atoms are synchronous, agents are the asynchronous mechanism in Clojure
    to effect any change in the state. Every agent is associated with a mutable state.
    We pass a function (known as "action") to an agent with the optional additional
    arguments. This function gets queued for processing in another thread by the agent.
    All the agents share two common thread pools—one for the low-latency (potentially
    CPU-bound, cache-bound, or memory-bound) jobs, and one for the blocking (potentially
    I/O related or lengthy processing) jobs. Clojure provides the `send` function
    for the low-latency actions, `send-off` for blocking actions, and `send-via` to
    have the action executed on the user-specified thread-pool, instead of either
    of the preconfigured thread pools. All of `send`, `send-off`, and `send-via` return
    immediately. Here is how we can use them:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'When we inspect the Clojure (as of version 1.7.0) source code, we can find
    that the thread-pool for the low-latency actions is named as `pooledExecutor`
    (a bounded thread-pool, initialized to max ''2 + number of hardware processors''
    threads), and the thread-pool for the high-latency actions is named as `soloExecutor`
    (an unbounded thread pool). The premise of this default configuration is that
    the CPU/cache/memory-bound actions run most optimally on a bounded thread-pool,
    with the default number of threads. The I/O bound tasks do not consume CPU resources.
    Hence, a relatively larger number of such tasks can execute at the same time,
    without significantly affecting the performance of the CPU/cache/memory-bound
    jobs. Here is how you can access and override the thread-pools:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: If a program carries out a large number of I/O or blocking operations through
    agents, it probably makes sense to limit the number of threads dedicated for such
    actions. Overriding the `send-off` thread-pool using `set-agent-send-off-executor!`
    is the easiest way to limit the thread-pool size. A more granular way to isolate
    and limit the I/O actions on the agents is to use `send-via` with the thread-pools
    of appropriate sizes for various kinds of I/O and blocking operations.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: Asynchrony, queueing, and error handling
  id: totrans-654
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sending an action to an agent returns immediately without blocking. If the agent
    is not already busy in executing any action, it "reacts" by enqueuing the action
    that triggers the execution of the action, in a thread, from the respective thread-pool.
    If the agent is busy in executing another action, the new action is simply enqueued.
    Once an action is executed from the action queue, the queue is checked for more
    entries and triggers the next action, if found. This whole "reactive" mechanism
    of triggering actions obviates the need of a message loop, polling the queue.
    This is only possible, because the entry points to an agent's queue are controlled.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: 'Actions are executed asynchronously on agents, which raises the question of
    how the errors are handled. Error cases need to be handled with an explicit, predefined
    function. When using a default agent construction, such as `(agent :foo)`, the
    agent is created without any error handler, and gets suspended in the event of
    any exception. It caches the exception, and refuses to accept any more actions.
    It throws the cached exception upon sending any action until the agent is restarted.
    A suspended agent can be reset using the `restart-agent` function. The objective
    of such suspension is safety and supervision. When the asynchronous actions are
    executed on an agent and suddenly an error occurs, it will require attention.
    Check out the following code:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'There are two optional parameters `:error-handler` and `:error-mode, which`
    we can configure on an agent to have finer control over the error handling and
    suspension as shown in the following code snippet:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Why you should use agents
  id: totrans-660
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as the "atom" implementation uses only compare-and-swap instead of locking,
    the underlying "agent" specific implementation uses mostly the compare-and-swap
    operations. The agent implementation uses locks only when dispatching action in
    a transaction (discussed in the next section), or when restarting an agent. All
    the actions are queued and dispatched serially in the agents, regardless of the
    concurrency level. The serial nature makes it possible to execute the actions
    in an independent and contention-free manner. For the same agent, there can never
    be more than one action being executed. Since there is no locking, reads (`deref`
    or `@`) on agents are never blocked due to writes. However, all the actions are
    independent of each other—there is no overlap in their execution.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: The implementation goes so far as to ensure that the execution of an action
    blocks other actions, which follow in the queue. Even though the actions are executed
    in a thread-pool, actions for the same agent are never executed concurrently.
    This is an excellent ordering guarantee that also extends a natural coordination
    mechanism, due to its serial nature. However, note that this ordering coordination
    is limited to only a single agent. If an agent action sends actions to two other
    agents, they are not automatically coordinated. In this situation, you may want
    to use transactions (which will be covered in the next section).
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: Since agents distinguish between the low-latency and blocking jobs, the jobs
    are executed in an appropriate kind of thread-pools. Actions on different agents
    may execute concurrently, thereby making optimum use of the threading resources.
    Unlike atoms, the performance of the agents is not impeded by high contention.
    In fact, for many cases, agents make a lot of sense due to the serial buffering
    of actions. In general, agents are great for high volume I/O tasks, or where the
    ordering of operations provides a win in the high contention scenarios.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: Nesting
  id: totrans-664
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an agent action sends another action to the same agent, that is a case
    of nesting. This would have been nothing special if agents didn't participate
    in STM transactions (which will be covered in the next section). However, agents
    do participate in STM transactions and that places certain constraints on agent
    implementation that warrants a second-layer buffering of actions. For now, it
    should suffice to say that the nested sends are queued in a thread-local queue
    instead of the regular queue in the agent. The thread-local queue is visible only
    to the thread in which the action is executed. Upon executing an action, unless
    there was an error, the agent implicitly calls the equivalent of `release-pending-sends`
    function, which transfers the actions from second level thread-local queue to
    the normal action queue. Note that nesting is simply an implementation detail
    of agents and has no other impact.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: Coordinated transactional ref and state
  id: totrans-666
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in an earlier section that an atom provides atomic read-and-update operation.
    What if we need to perform an atomic read-and-update operation across two or even
    more number of atoms? This clearly poses a coordination problem. Some entity has
    to watch over the process of reading and updating, so that the values are not
    corrupted. This is what a ref provides—a **Software Transactional Memory** (**STM**)
    based system that takes care of concurrent atomic read-and-update operations across
    multiple refs, such that either all the updates go through, or in the case of
    failure, none does. Like atoms, on failure, refs retry the whole operation from
    scratch with the new values.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: Clojure's STM implementation is coarse grained. It works at the application
    level objects and aggregates (that is, references to aggregates), scoped to only
    all the refs in a program, constituting the "Ref world". Any update to a ref can
    only happen synchronously, in a transaction, in a `dosync` block of code, within
    the same thread. It cannot span beyond the current thread. The implementation
    detail reveals that a thread-local transaction context is maintained during a
    lifetime of a transaction. The same context ceases to be available, the moment
    the control reaches another thread.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: Like the other reference types in Clojure, reads on a ref are never blocked
    by the updates, and vice versa. However, unlike the other reference types, the
    implementation of ref does not depend on a lock-free spinning, but rather, it
    internally uses locks, a low-level wait/notify, a deadlock detection, and the
    age-based barging.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: 'The `alter` function is used to read-and-update the value of a ref, and `ref-set`
    is used to reset the value. Roughly, `alter` and `ref-set,` for the refs, are
    analogous to `swap!` and `reset!` for the atoms. Just like `swap!`, `alter` accepts
    a function (and arguments) with no side effects, and may be retried several times
    during the contention. However, unlike with the atoms, not only `alter` but also
    `ref-set` and simple `deref`, may cause a transaction to be retried during the
    contention. Here is a very simple example on how we may use a transaction:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Ref characteristics
  id: totrans-672
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clojure maintains the **Atomicity**, **Consistency**, and **Isolation** (**ACI**)
    characteristics in a transaction. This overlaps with A, C, and I of the ACID guarantee
    that many databases provide. Atomicity implies that either all of the updates
    in a transaction will complete successfully or none of them do. Consistency means
    that the transaction must maintain general correctness, and should honor the constraints
    set by the validation—any exception or validation error should roll back the transaction.
    Unless a shared state is guarded, concurrent updates on it may lead a multi-step
    transaction into seeing different values at different steps. Isolation implies
    that all the steps in a transaction will see the same value, no matter how concurrent
    the updates are.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: The Clojure refs use something known as **Multi Version Concurrency Control**
    (**MVCC**) to provide **Snapshot Isolation** to the transactions. In MVCC, instead
    of locking (which could block the transactions), the queues are maintained, so
    that each transaction can occur using its own snapshot copy, taken at its "read
    point", independent of other transactions. The main benefit of this approach is
    that the read-only out-of-transaction operations can go through without any contention.
    Transactions without the ref contention go through concurrently. In a rough comparison
    with the database systems, the Clojure ref isolation level is "Read Committed"
    for reading a Ref outside of a transaction, and "Repeatable Read" by default when
    inside the transaction.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: Ref history and in-transaction deref operations
  id: totrans-675
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed earlier that both, read and update operations, on a ref, may cause
    a transaction to be retried. The reads in a transaction can be configured to use
    the ref history in such a manner that the snapshot isolation instances are stored
    in the history queues, and are used by the read operations in the transactions.
    The default, which is not supposed to use the history queues, conserves heap space,
    and provides strong consistency (avoids the staleness of data) in the transactions.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the ref history reduces the likelihood of the transaction retries caused
    by read contention, thereby providing a weak consistency. Therefore, it is a tool
    for performance optimization, which comes at the cost of consistency. In many
    scenarios, programs do not need strong consistency—we can choose appropriately
    if we know the trade-off, and what we need. The snapshot isolation mechanism in
    the Clojure ref implementation is backed by the adaptive history queues. The history
    queues grow dynamically to meet the read requests, and do not overshoot the maximum
    limit that is set for the ref. By default, the history is not enabled, so we need
    to specify it during the initialization or set it later. Here is an example of
    how to use the history:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Minimum/maximum history limits are proportional to the length of the staleness
    window of the data. It also depends on the relative latency difference of the
    update and read operations to see what the range of the min-history and the max-history
    works well on a given host system. It may take some amount of trial and error
    to get the range right. As a ballpark figure, read operations only need as many
    min-history elements to avoid the transaction retries, as many updates can go
    through during one read operation. The max-history elements can be a multiple
    of min-history to cover for any history overrun or underrun. If the relative latency
    difference is unpredictable, then we have to either plan a min-history for the
    worst case scenario, or consider other approaches.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: Transaction retries and barging
  id: totrans-680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A transaction can internally be in one of the five distinct states—Running,
    Committing, Retry, Killed, and Committed. A transaction can be killed for various
    reasons. Exceptions are the common reasons for killing a transaction. But let's
    consider the corner case where a transaction is retried many times, but it does
    not appear to commit successfully—what is the resolution? Clojure supports age-based
    barging, wherein an older transaction automatically tries to abort a younger transaction,
    so that the younger transaction is retried later. If the barging still doesn't
    work, as a last resort, the transaction is killed after a hard limit of 10,000
    retry attempts, and then the exception is thrown.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: Upping transaction consistency with ensure
  id: totrans-682
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clojure's transactional consistency is a good balance between performance and
    safety. However, at times, we may need the **Serializable** consistency in order
    to preserve the correctness of the transaction. Concretely, in the face of the
    transaction retries, when a transaction's correctness depends on the state of
    a ref, in the transaction, wherein the ref is updated simultaneously in another
    transaction, we have a condition called "write skew". The Wikipedia entry on the
    write skew, [https://en.wikipedia.org/wiki/Snapshot_isolation](https://en.wikipedia.org/wiki/Snapshot_isolation),
    describes it well, but let's see a more concrete example. Let's say we want to
    design a flight simulation system with two engines, and one of the system level
    constraints is not to switch off both engines at the same time. If we model each
    engine as a ref, and certain maneuvers do require us to switch off an engine,
    we must ensure that the other engine is on. We can do it with `ensure`. Usually,
    `ensure` is required when maintaining a consistent relationship (invariants) across
    the refs is necessary. This cannot be ensured by the validator functions, because
    they do not come into play until the transaction commits. The validator functions
    will see the same value hence cannot help.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: The write-skew can be solved using the namesake `ensure` function that essentially
    prevents a ref from modification by other transactions. It is similar to a locking
    operation, but in practice, it provides better concurrency than the explicit read-and-update
    operations, when the retries are expensive. Using `ensure` is quite simple—`(ensure
    ref-object).` However, it may be performance-wise expensive, due to the locks
    it holds during the transaction. Managing performance with `ensure` involves a
    trade-off between the retry latency, and the lost throughput due to the ensured
    state.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: Lesser transaction retries with commutative operations
  id: totrans-685
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Commutative operations are independent of the order in which they are applied.
    For example, incrementing a counter ref c1 from transactions t1 and t2 would have
    the same effect irrespective of the order in which t1 and t2 commit their changes.
    Refs have a special optimization for changing functions that are commutative for
    transactions—the `commute` function, which is similar to `alter` (same syntax),
    but with different semantics. Like `alter`, the `commute` functions are applied
    atomically during the transaction commit. However, unlike `alter`, `commute` does
    not cause the transaction retry on contention, and there is no guarantee about
    the order in which the `commute` functions are applied. This effectively makes
    `commute` nearly useless for returning a meaningful value as a result of the operation.
    All the commute functions in a transaction are reapplied with the final in transaction
    ref values during the transaction commit.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, commute reduces the contention, thereby optimizing the performance
    of the overall transaction throughput. Once we know that an operation is commutative
    and we are not going to use its return value in a meaningful way, there is hardly
    any trade-off deciding on whether to use it—we should just go ahead and use it.
    In fact, a program design, with respect to the ref transactions, with commute
    in mind, is not a bad idea.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: Agents can participate in transactions
  id: totrans-688
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section on agents, we discussed how agents work with the queued
    change functions. Agents can also participate in the ref transactions, thereby
    making it possible to combine the use of refs and agents in the transactions.
    However, agents are not included in the "Ref world", hence a transaction scope
    is not extended till the execution of the change function in an agent. Rather,
    the transactions only make sure that the changes sent to the agents are queued
    until the transaction commit happens.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: The *Nesting* sub-section, in the earlier section on agents, discusses about
    a second-layer thread-local queue. This thread-local queue is used during a transaction
    to hold the sent changes to an agent until the commit. The thread-local queue
    does not block the other changes that are being sent to an agent. The out-of-transaction
    changes are never buffered in the thread-local queue; rather, they are added to
    the regular queue in the agent.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: The participation of agents in the transactions provides an interesting angle
    of design, where the coordinated and independent/sequential operations can be
    pipelined as a workflow for better throughput and performance.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: Nested transactions
  id: totrans-692
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clojure transactions are nesting aware and they compose well. But, why would
    one need a nested transaction? Often, independent units of code may have their
    own low-granularity transactions that a higher level code can make use of. When
    the higher level caller itself needs to wrap actions in a transaction, nested
    transactions occur. Nested transactions have their own lifecycle and run-state.
    However, an outer transaction can abort an inner transaction on the detection
    of failure.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: The "ref world" snapshot `ensure`s and `commute`s are shared among all (that
    is, outer and inner) levels of a nested transaction. Due to this, the inner transaction
    is treated as any other ref change operation (similar to `alter`, `ref-set` and
    so on) within an outer transaction. The watches and internal lock implementation
    are handled at the respective nesting level. The detection of contention in the
    inner transactions causes a restart of not only the inner but also the outer transaction.
    Commits at all the levels are effected as a global state finally when the outermost
    transaction commits. The watches, even though tracked at each individual transaction
    level, are finally effected during the commit. A closer look at the nested transaction
    implementation shows that nesting has little or no impact on the performance of
    transactions.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: Performance considerations
  id: totrans-695
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clojure Ref is likely to be the most complex reference type implemented yet.
    Due to its characteristics, especially its transaction retry mechanism, it may
    not be immediately apparent that such a system would have good performance during
    the high-contention scenarios.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding its nuances and best ways of use should help:'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: We do not use changes with the side effects in a transaction, except for possibly
    sending the I/O changes to agents, where the changes are buffered until the commit.
    So by definition, we do not carry out any expensive I/O work in a transaction.
    Hence, a retry of this work would be cheap as well.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A change function for a transaction should be as small as possible. This lowers
    the latency and hence, the retries will also be cheaper.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any ref that is not updated along with at least one more ref simultaneously
    needs not be a ref—atoms would do just fine in this case. Now that the refs make
    sense only in a group, their contention is directly proportional to the group
    size. Small groups of refs used in the transactions lead to a low contention,
    lower latency, and a higher throughput.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commutative functions provide a good opportunity to enhance the transaction
    throughput without any penalty. Identifying such cases and designing with commute
    in mind can help performance significantly.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refs are very coarse grained—they work at the application aggregate level. Often
    a program may need to have more fine-grained control over the transaction resources.
    This can be enabled by Ref striping, such as Megaref ([https://github.com/cgrand/megaref](https://github.com/cgrand/megaref)),
    by providing a scoped view on the associative refs, thereby allowing higher concurrency.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the high contention scenarios in which the ref group size in a transaction
    cannot be small, consider using agents, as they have no contention due to the
    serial nature. Agents may not be a replacement for the transactions, but rather
    we can employ a pipeline consisting of atoms, refs, and agents to ease out the
    contention versus latency concerns.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refs and transactions have an intricate implementation. Fortunately, we can
    inspect the source code, and browse through available online and offline resources.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic var binding and state
  id: totrans-705
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fourth kind among the Clojure''s reference types is the dynamic var. Since
    Clojure 1.3, all the vars are static by default. A var must be explicitly declared
    so in order to be dynamic. Once declared, a dynamic var can be bound to new values
    on per-thread basis. Binding on different threads do not block each other. An
    example is shown here:'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: As the dynamic binding is thread-local, it may be tricky to use in multi-threaded
    scenarios. Dynamic vars have been long abused by libraries and applications as
    a means to pass in a common argument to be used by several functions. However,
    this style is acknowledged to be an anti-pattern, and is discouraged. Typically,
    in the anti-pattern dynamic, vars are wrapped by a macro to contain the dynamic
    thread-local binding in the lexical scope. This causes problems with the multi-threading
    and lazy sequences.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: So, how can the dynamic vars be used effectively? A dynamic var lookup is more
    expensive than looking up a static var. Even passing a function argument is performance-wise
    much cheaper than looking up a dynamic var. Binding a dynamic var incurs additional
    cost. Clearly, in performance sensitive code, dynamic vars are best not used at
    all. However, dynamic vars may prove to be useful to hold a temporary thread-local
    state in a complex, or recursive call-graph scenario, where the performance does
    not matter significantly, without being advertised or leaked into the public API.
    The dynamic var bindings can nest and unwind like a stack, which makes them both
    attractive and suitable for such tasks.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: Validating and watching the reference types
  id: totrans-710
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vars (both static and dynamic), atoms, refs, and agents provide a way to validate
    the value being set as state—a `validator` function that accepts new value as
    argument, and returns the logical as true if it succeeds, or throws exception/returns
    logical as false (the false and nil values) if there''s an error. They all honor
    what the validator function returns. If it is a success, the update goes through,
    and if an error, an exception is thrown instead. Here is the syntax on how the
    validators can be declared and associated with the reference types:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Validators cause actual failure within a reference type while updating them.
    For vars and atoms, they simply prevent the update by throwing an exception. In
    an agent, a validation failure causes agent failure, and needs the agent to restart.
    Inside a ref, the validation failure causes the transaction to rollback and rethrow
    the exception.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: 'Another mechanism to observe the changes to the reference types is a "watcher".
    Unlike validators, a watcher is passive—it is notified of the update after the
    fact. Hence, a watcher cannot prevent updates from going through, because it is
    only a notification mechanism. For transactions, a watcher is invoked only after
    the transaction commit. While only one validator can be set on a reference type,
    it is possible to associate multiple watchers to a reference type on the other
    hand. Secondly, when adding a watch, we can specify a key, so that the notifications
    can be identified by the key, and be dealt accordingly by the watcher. Here is
    the syntax on how to use watchers:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Like validators, the watchers are executed synchronously in the thread of the
    reference type. For atoms and refs, this may be fine, since the notification to
    the watchers goes on, the other threads may proceed with their updates. However
    in agents, the notification happens in the same thread where the update happens—this
    makes the update latency higher, and the throughput potentially lower.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: Java concurrent data structures
  id: totrans-717
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java has a number of mutable data structures that are meant for concurrency
    and thread-safety, which implies that multiple callers can safely access these
    data structures at the same time, without blocking each other. When we need only
    the highly concurrent access without the state management, these data structures
    may be a very good fit. Several of these employ lock free algorithms. We discussed
    about the Java atomic state classes in the *Atomic updates and state section*,
    so we will not repeat them here. Rather, we will only discuss the concurrent queues
    and other collections.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: All of these data structures live in the `java.util.concurrent` package. These
    concurrent data structures are tailored to leverage the JSR 133 "Java Memory Model
    and Thread Specification Revision" ([http://gee.cs.oswego.edu/dl/jmm/cookbook.html](http://gee.cs.oswego.edu/dl/jmm/cookbook.html))
    implementation that first appeared in Java 5.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent maps
  id: totrans-720
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Java has a mutable concurrent hash map—`java.util.concurrent.ConcurrentHashMap`
    (CHM in short). The concurrency level can be optionally specified when instantiating
    the class, which is 16 by default. The CHM implementation internally partitions
    the map entries into the hash buckets, and uses multiple locks to reduce the contention
    on each bucket. Reads are never blocked by writes, therefore they may be stale
    or inconsistent—this is countered by built-in detection of such situations, and
    issuing a lock in order to read the data again in the synchronized fashion. This
    is an optimization for the scenarios, where reads significantly outnumber writes.
    In CHM, all the individual operations are near constant-time unless stuck in a
    retry loop due to the lock contention.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast with Clojure''s persistent map, CHM cannot accept `null` (`nil`)
    as the key or value. Clojure''s immutable scalars and collections are automatically
    well-suited for use with CHM. An important thing to note is that only the individual
    operations in CHM are atomic, and exhibit strong consistency. As CHM operations
    are concurrent, the aggregate operations provide a rather weak consistency than
    the true operation-level consistency. Here is how we can use CHM. The individual
    operations in CHM, which provide a better consistency, are safe to use. The aggregate
    operations should be reserved for when we know its consistency characteristics,
    and the related trade-off:'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-723
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The `java.util.concurrent.ConcurrentSkipListMap` class (CSLM in short) is another
    concurrent mutable map data structure in Java. The difference between CHM and
    CSLM is that CSLM offers a sorted view of the map at all times with the O(log
    N) time complexity. The sorted view has the natural order of keys by default,
    which can be overridden by specifying a Comparator implementation when instantiating
    CSLM. The implementation of CSLM is based on the Skip List, and provides navigation
    operations.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: The `java.util.concurrent.ConcurrentSkipListSet` class (CSLS in short) is a
    concurrent mutable set based on the CSLM implementation. While CSLM offers the
    map API, CSLS behaves as a set data structure while borrowing features of CSLM.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent queues
  id: totrans-726
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Java has a built-in implementation of several kinds of mutable and concurrent
    in-memory queues. The queue data structure is a useful tool for buffering, producer-consumer
    style implementation, and for pipelining such units together to form the high-performance
    workflows. We should not confuse them with durable queues that are used for similar
    purpose in the batch jobs for a high throughput. Java's in-memory queues are not
    transactional, but they provide atomicity and strong consistency guarantee for
    the individual queue operations only. Aggregate operations offer weaker consistency.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: 'The `java.util.concurrent.ConcurrentLinkedQueue` (CLQ) is a lock-free, wait-free
    unbounded "First In First Out" (FIFO) queue. FIFO implies that the order of the
    queue elements will not change once added to the queue. CLQ''s `size()` method
    is not a constant time operation; it depends on the concurrency level. Few examples
    of using CLQ are here:'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-729
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '| Queue | Blocking? | Bounded? | FIFO? | Fairness? | Notes |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
- en: '| CLQ | No | No | Yes | No | Wait-free, but the size() is not constant time
    |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
- en: '| ABQ | Yes | Yes | Yes | Optional | The capacity is fixed at instantiation
    |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
- en: '| DQ | Yes | No | No | No | The elements implement the Delayed interface |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
- en: '| LBQ | Yes | Optional | Yes | No | The capacity is flexible, but with no fairness
    option |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
- en: '| PBQ | Yes | No | No | No | The elements are consumed in a priority order
    |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
- en: '| SQ | Yes | – | – | Optional | It has no capacity; it serves as a channel
    |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
- en: In the `java.util.concurrent` package, `ArrayBlockingQueue` (ABQ), `DelayQueue`
    (DQ), `LinkedBlockingQueue` (LBQ), `PriorityBlockingQueue` (PBQ), and `SynchronousQueue`
    (SQ) implement the `BlockingQueue` (BQ) interface. Its Javadoc describes the characteristics
    of its method calls. ABQ is a fixed-capacity, FIFO queue backed by an array. LBQ
    is also a FIFO queue, backed by the linked nodes, and is optionally bounded (default
    `Integer.MAX_VALUE`). ABQ and LBQ generate "Back pressure" by blocking the enqueue
    operations on full capacity. ABQ supports optional fairness (with performance
    overhead) in the order of the threads that access it.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
- en: DQ is an unbounded queue that accepts the elements associated with the delay.
    The queue elements cannot be null, and must implement the `java.util.concurrent.Delayed`
    interface. Elements are available for removal from the queue only after the delay
    has been expired. DQ can be very useful for scheduling the processing of the elements
    at different times.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: PBQ is unbounded and blocking while letting elements be consumed from the queue
    as per priority. Elements have the natural ordering by default that can be overridden
    by specifying a Comparator implementation when instantiating the queue.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: SQ is not really a queue at all. Rather, it's just a barrier for a producer
    or consumer thread. The producer blocks until a consumer removes the element and
    vice versa. SQ does not have a capacity. However, SQ supports optional fairness
    (with performance overhead), in the order, in which the threads access it.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: There are some new concurrent queue types introduced after Java 5\. Since JDK
    1.6, in the `java.util.concurrent` package Java has **BlockingDeque** (**BD**)
    with **LinkedBlockingDeque** (**LBD**) as the only available implementation. BD
    builds on BQ by adding the **Deque** (**double-ended queue**) operations, that
    is, the ability to add elements and consume the elements from both the ends of
    the queue. LBD can be instantiated with an optional capacity (bounded) to block
    the overflow. JDK 1.7 introduced **TransferQueue** (**TQ**) with **LinkedTransferQueue**
    (**LTQ**) as the only implementation. TQ extends the concept of SQ in such a way
    that the producers and consumers block a queue of elements. This will help utilize
    the producer and consumer threads better by keeping them busy. LTQ is an unbounded
    implementation of TQ where the `size()` method is not a constant time operation.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: Clojure support for concurrent queues
  id: totrans-743
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered the persistent queue in [Chapter 2](ch09.html "Chapter 2. Clojure
    Abstractions"), *Clojure Abstractions* earlier. Clojure has a built-in `seque`
    function that builds over a BQ implementation (LBQ by default) to expose a write-ahead
    sequence. The sequence is potentially lazy, and the write-ahead buffer throttles
    how many elements to realize. As opposed to the chunked sequences (of chunk size
    32), the size of the write-ahead buffer is controllable and potentially populated
    at all times until the source sequence is exhausted. Unlike the chunked sequences,
    the realization doesn't happen suddenly for a chunk of 32 elements. It does so
    gradually and smoothly.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, Clojure's `seque` uses an agent to the backfill data in the
    write-ahead buffer. In the arity-2 variant of `seque`, the first argument should
    either be a positive integer, or an instance of BQ (ABQ, LBQ, and more) that is
    preferably bounded.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency with threads
  id: totrans-746
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the JVM, threads are the de-facto fundamental instrument of concurrency.
    Multiple threads live in the same JVM; they share the heap space, and compete
    for the resources.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
- en: JVM support for threads
  id: totrans-748
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The JVM threads are the Operating System threads. Java wraps an underlying
    OS thread as an instance of the `java.lang.Thread` class, and builds up an API
    around it to work with threads. A thread on the JVM has a number of states: New,
    Runnable, Blocked, Waiting, Timed_Waiting, and Terminated. A thread is instantiated
    by overriding the `run()` method of the `Thread` class, or by passing an instance
    of the `java.lang.Runnable` interface to the constructor of the `Thread` class.'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: Invoking the `start()` method of a `Thread` instance starts its execution in
    a new thread. Even if just a single thread runs in the JVM, the JVM would not
    shut down. Calling the `setDaemon(boolean)` method of a thread with argument `true`
    tags the thread as a daemon that can be automatically shut down if no other non-daemon
    thread is running.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: 'All Clojure functions implement the `java.lang.Runnable` interface. Therefore,
    invoking a function in a new thread is very easy:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-752
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The `run()` method does not accept any argument. We can work around it by creating
    a higher order function that needs no arguments, but internally applies the argument
    `3`.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: Thread pools in the JVM
  id: totrans-754
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating threads leads to the Operating System API calls, which is not always
    a cheap operation. The general practice is to create a pool of threads that can
    be recycled for different tasks. Java has a built-in support for threads pools.
    The interface called `java.util.concurrent.ExecutorService` represents the API
    for a thread pool. The most common way to create a thread pool is to use a factory
    method in the `java.util.concurrent.Executors` class:'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The previous example is equivalent of the examples with raw threads that we
    saw in the previous sub-section. Thread pools are also capable of helping to track
    the completion, and the return value of a function, executed in a new thread.
    An ExecutorService accepts an instance of the `java.util.concurrent.Callable`
    instance as an argument to several methods that launch a task, and return `java.util.concurrent.Future`
    to track the final result.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: 'All the Clojure functions also implement the `Callable` interface, so we can
    use them as follows:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-759
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The thread pools described here are the same as the ones that we saw briefly
    in the Agents section earlier. Thread pools need to be shut down by calling the
    `shutdown()` method when no longer required.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: Clojure concurrency support
  id: totrans-761
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clojure has some nifty built-in features to deal with concurrency. We already
    discussed about the agents, and how they use the thread pools, in an earlier section.
    There are some more concurrency features in Clojure to deal with the various use
    cases.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: Future
  id: totrans-763
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We saw earlier in this section how to use the Java API to launch a new thread,
    to execute a function. Also, we learned how to get the result back. Clojure has
    a built-in support called "futures" to do these things in a much smoother and
    integrated manner. The basis of the futures is the function `future-call` (it
    takes a `no-arg` function as an argument), and the macro `future` (it takes the
    body of code) that builds on the former. Both of them immediately start a thread
    to execute the supplied code. The following snippet illustrates the functions
    that work with the future, and how to use them:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'One of the interesting aspects of `future-cancel` is that it can sometimes
    not only cancel tasks that haven''t started yet, but may also abort those that
    are halfway through execution:'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The previous scenario happens because Clojure's `future-cancel` cancels a future
    in such a way that if the execution has already started, it may be interrupted
    causing `InterruptedException`, which, if not explicitly caught, would simply
    abort the block of code. Beware of exceptions arising from the code that is executed
    in a future, because, by default, they are not reported verbosely! Clojure futures
    use the "solo" thread pool (used to execute the potentially blocking actions)
    that we discussed earlier with respect to the agents.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: Promise
  id: totrans-769
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A promise is a placeholder for the result of a computation that may or may not
    have occurred. A promise is not directly associated with any computation. By definition,
    a promise does not imply when the computation might occur, hence realizing the
    promise.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a promise originates from one place in the code, and is realized
    by some other portion of the code that knows when and how to realize the promise.
    Very often, this happens in a multi-threaded code. If a promise is not realized
    yet, any attempt to read the value blocks all callers. If a promise is realized,
    then all the callers can read the value without being blocked. As with futures,
    a promise can be read with a timeout using `deref`.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a very simple example showing how to use promises:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: A promise is a very powerful tool that can be passed around as function arguments.
    It can be stored in a reference type, or simply be used for a high level coordination.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: Clojure parallelization and the JVM
  id: totrans-775
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We observed in [Chapter 1](ch15.html "Chapter 8. Application Performance"),
    *Performance by Design* that parallelism is a function of the hardware, whereas
    concurrency is a function of the software, assisted by the hardware support. Except
    for the algorithms that are purely sequential by nature, concurrency is the favored
    means to facilitate parallelism, and achieve better performance. Immutable and
    stateless data is a catalyst to concurrency, as there is no contention between
    threads, due to absence of mutable data.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: Moore's law
  id: totrans-777
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 1965, Intel's cofounder, Gordon Moore, made an observation that the number
    of transistors per square inch on Integrated Circuits doubles every 24 months.
    He also predicted that the trend would continue for 10 years, but in practice,
    it has continued till now, marking almost half a century. More transistors have
    resulted in more computing power. With a greater number of transistors in the
    same area, we need higher clock speed to transmit signals to all of the transistors.
    Secondly, transistors need to get smaller in size to fit in. Around 2006-2007,
    the clock speed that the circuitry could work with topped out at about 2.8GHz,
    due to the heating issues and the laws of physics. Then, the multi-core processors
    were born.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl's law
  id: totrans-779
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The multi-core processors naturally require splitting up computation in order
    to achieve parallelization. Here begins a conflict—a program that was made to
    be run sequentially cannot make use of the parallelization features of the multi-core
    processors. The program must be altered to find the opportunity to split up computation
    at every step, while keeping the cost of coordination in mind. This results in
    a limitation that a program can be no more faster than its longest sequential
    part (*contention*, or *seriality*), and the coordination overhead. This characteristic
    was described by Amdahl's law.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: Universal Scalability Law
  id: totrans-781
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dr Neil Gunther''s Universal Scalability Law (USL) is a superset of Amdahl''s
    Law that makes both: *contention (α)* and *coherency (β)* the first class concerns
    in quantifying the scalability very closely to the realistic parallel systems.
    Coherency implies the coordination overhead (latency) in making the result of
    one part of a parallelized program to be available to another. While Amdahl''s
    Law states that contention (seriality) causes performance to level off, USL goes
    to show that the performance actually degrades with excessive parallelization.
    USL is described with the following formula:'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: C(N) = N / (1 + α ((N – 1) + β N (N – 1)))
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: Here, C(N) implies relative capacity or throughput in terms of the source of
    concurrency, such as physical processors, or the users driving the software application.
    α implies the degree of contention because of the shared data or the sequential
    code, and β implies penalty incurred for maintaining the consistency of shared
    data. I would encourage you to pursue USL further ([http://www.perfdynamics.com/Manifesto/USLscalability.html](http://www.perfdynamics.com/Manifesto/USLscalability.html)),
    as this is a very important resource for studying the impact of concurrency on
    scalability and the performance of the systems.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: Clojure support for parallelization
  id: totrans-785
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A program that relies on mutation cannot parallelize its parts without creating
    contention on the mutable state. It requires coordination overhead, which makes
    the situation worse. Clojure's immutable nature is better suited to parallelize
    the parts of a program. Clojure also has some constructs that are suited for parallelism
    by the virtue of Clojure's consideration of available hardware resources. The
    result is, the operations execute optimized for certain use case scenarios.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: pmap
  id: totrans-787
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pmap` function (similar to `map`) accepts as arguments a function and one,
    or more collections of data elements. The function is applied to each of the data
    elements in such a way that some of the elements are processed by the function
    in parallel. The parallelism factor is chosen at runtime by the `pmap` implementation,
    as two greater than the total number of available processors. It still processes
    the elements lazily, but the realization factor is same as the parallelism factor.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following code:'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-790
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: To use `pmap` effectively, it is imperative that we understand what it is meant
    for. As the documentation says, it is meant for computationally intensive functions.
    It is optimized for CPU-bound and cache-bound jobs. High latency and low CPU tasks,
    such as blocking I/O, are a gross misfit for `pmap`. Another pitfall to be aware
    of is whether the function used in `pmap` performs a lot of memory operations
    or not. Since the same function will be applied across all the threads, all the
    processors (or cores) may compete for the memory interconnect and the sub-system
    bandwidth. If the parallel memory access becomes a bottleneck, `pmap` cannot make
    the operation truly parallel, due to the contention on the memory access.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: Another concern is what happens when several `pmap` operations run concurrently?
    Clojure does not attempt to detect multiple `pmap`s running concurrently. The
    same number of threads will be launched afresh for every new `pmap` operation.
    The developer is responsible to ensure the performance characteristics, and the
    response time of the program resulting from the concurrent pmap executions. Usually,
    when the latency reasons are paramount, it is advisable to limit the concurrent
    instances of `pmap` running in the program.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
- en: pcalls
  id: totrans-793
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pcalls` function is built using `pmap`, so it borrows properties from the
    latter. However, the `pcalls` function accepts zero or more functions as arguments
    and executes them in parallel, returning the result values of the calls as a list.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: pvalues
  id: totrans-795
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pvalues` macro is built using `pcalls`, so it transitively shares the properties
    of `pmap`. It's behavior is similar to `pcalls`, but instead of functions, it
    accepts zero or more S-expressions that are evaluated in the parallel using `pmap`.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
- en: Java 7's fork/join framework
  id: totrans-797
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Java 7 introduced a new framework for parallelism called "fork/join," based
    on divide-and-conquer and the work-stealing scheduler algorithms. The basic idea
    of how to use the fork/join framework is fairly simple—if the work is small enough,
    then do it directly in the same thread; otherwise, split the work into two pieces,
    invoke them in a fork/join thread pool, and wait for the results to combine.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: This way, the job gets recursively split into smaller parts such as an inverted
    tree, until the smallest part can be carried out in just a single thread. When
    the leaf/subtree jobs return, the parent combines the result of all children,
    and returns the results.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: 'The fork/join framework is implemented in Java 7 in terms of a special kind
    of thread pool; check out `java.util.concurrent.ForkJoinPool`. The specialty of
    this thread pool is that it accepts the jobs of `java.util.concurrent.ForkJoinTask`
    type, and whenever these jobs block, waiting for the child jobs to finish, the
    threads used by the waiting jobs are allocated to the child jobs. When the child
    finishes its work, the thread is allocated back to the blocked parent jobs in
    order to continue. This style of dynamic thread allocation is described as "work-stealing".
    The fork/join framework can be used from within Clojure. The `ForkJoinTask` interface
    has two implementations: `RecursiveAction` and `RecursiveTask` in the `java.util.concurrent`
    package. Concretely, `RecursiveTask` maybe more useful with Clojure, as `RecursiveAction`
    is designed to work with mutable data, and does not return any value from its
    operation.'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: Using the fork-join framework entails choosing the batch size to split a job
    into, which is a crucial factor in parallelizing a long job. Too large a batch
    size may not utilize all the CPU cores enough; on the other hand, a small batch
    size may lead to a longer overhead, coordinating across the parent/child batches.
    As we will see in the next section, Clojure integrates with the Fork/join framework
    to parallelize the reducers implementation.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism with reducers
  id: totrans-802
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reducers are a new abstraction introduced in Clojure 1.5, and are likely to
    have a wider impact on the rest of the Clojure implementation in the future versions.
    They depict a different way of thinking about processing collections in Clojure—the
    key concept is to break down the notion that collections can be processed only
    sequentially, lazily, or producing a seq, and more. Moving away from such a behavior
    guarantee raises the potential for eager and parallel operations on one hand,
    whereas incurring constraints on the other. Reducers are compatible with the existing
    collections.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: For an example, a keen observation of the regular `map` function reveals that
    its classic definition is tied to the mechanism (recursion), order (sequential),
    laziness (often), and representation (list/seq/other) aspects of producing the
    result. Most of this actually defines "how" the operation is performed, rather
    than "what" needs to be done. In the case of `map`, the "what" is all about applying
    a function to each element of its collection arguments. But since the collection
    types can be of various types (tree-structured, sequence, iterator, and more),
    the operating function cannot know how to navigate the collection. Reducers decouple
    the "what" and "how" parts of the operation.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: Reducible, reducer function, reduction transformation
  id: totrans-805
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collections are of various kinds, hence only a collection knows how to navigate
    itself. In the reducers model at a fundamental level, an internal "reduce" operation
    in each collection type has access to its properties and behavior, and access
    to what it returns. This makes all the collection types essentially "reducible".
    All the operations that work with collections can be modeled in terms of the internal
    "reduce" operation. The new modeled form of such operations is a "reducing function",
    which is typically a function of two arguments, the first argument being the accumulator,
    and the second being the new input.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
- en: How does it work when we need to layer several functions upon another, over
    the elements of a collection? For an example, let's say first we need to "filter",
    "map," and then "reduce". In such cases, a "transformation function" is used to
    model a reducer function (for example, for "filter") as another reducer function
    (for "map") in such a way that it adds the functionality during the transformation.
    This is called "reduction transformation".
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
- en: Realizing reducible collections
  id: totrans-808
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the reducer functions retain the purity of the abstraction, they are not
    useful all by themselves. The reducer operations in the namespace called as `clojure.core.reducers`
    similar to `map`, `filter`, and more, basically return a reducible collection
    that embed the reducer functions within themselves. A reducible collection is
    not realized, not even lazily realized—rather, it is just a recipe that is ready
    to be realized. In order to realize a reducible collection, we must use one of
    the `reduce` or `fold` operations.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: The `reduce` operation that realizes a reducible collection is strictly sequential,
    albeit with the performance gains compared to `clojure.core/reduce`, due to reduced
    object allocations on the heap. The `fold` operation, which realizes a reducible
    collection, is potentially parallel, and uses a "reduce-combine" approach over
    the fork-join framework. Unlike the traditional "map-reduce" style, the use of
    fork/join the reduce-combine approach reduces at the bottom, and subsequently
    combines by the means of reduction again. This makes the `fold` implementation
    less wasteful and better performing.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: Foldable collections and parallelism
  id: totrans-811
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel reduction by `fold` puts certain constraints on the collections and
    operations. The tree-based collection types (persistent map, persistent vector,
    and persistent set) are amenable to parallelization. At the same time, the sequences
    may not be parallelized by `fold`. Secondly, `fold` requires that the individual
    reducer functions should be "associative", that is, the order of the input arguments
    applied to the reducer function should not matter. The reason being, `fold` can
    segment the elements of the collection to process in parallel, and the order in
    which they may be combined is not known in advance.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fold` function accepts few extra arguments, such as the "combine function,"
    and the partition batch size (default being 512) for the parallel processing.
    Choosing the optimum partition size depends on the jobs, host capabilities, and
    the performance benchmarking. There are certain functions that are foldable (that
    is, parallelizable by `fold`), and there are others that are not, as shown here.
    They live in the `clojure.core.reducers` namespace:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: '**Foldable**: `map`, `mapcat`, `filter`, `remove`, and `flatten`'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-foldable**: `take-while`, `take`, and `drop`'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine functions**: `cat`, `foldcat`, and `monoid`'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A notable aspect of reducers is that it is foldable in parallel only when the
    collection is a tree type. This implies that the entire data set must be loaded
    in the heap memory when folding over them. This has the downside of memory consumption
    during the high load on a system. On the other hand, a lazy sequence is a perfectly
    reasonable solution for such scenarios. When processing large amount of data,
    it may make sense to use a combination of lazy sequences and reducers for performance.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-818
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and parallelism are extremely important for performance in this
    multi-core age. Effective use of concurrency requires substantial understanding
    of the underlying principles and details. Fortunately, Clojure provides safe and
    elegant ways to deal with concurrency and state. Clojure's new feature called
    "reducers" provides a way to achieve granular parallelism. In the coming years,
    we are likely to see more and more processor cores, and an increasing demand to
    write code that takes advantage of these. Clojure places us in the right spot
    to meet such challenges.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the performance measurement, analysis,
    and monitoring.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6. Measuring Performance
  id: totrans-821
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on the expected and actual performance, and the lack or presence
    of a measuring system, performance analysis and tuning can be a fairly elaborate
    process. Now we will discuss the analysis of performance characteristics and ways
    to measure and monitor them. In this chapter we will cover the following topics:'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance and understanding the results
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What performance tests to carry out for different purposes
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring performance and obtaining metrics
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling Clojure code to identify performance bottlenecks
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance measurement and statistics
  id: totrans-827
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring performance is the stepping stone to performance analysis. As we noted
    earlier in this book, there are several performance parameters to be measured
    with respect to various scenarios. Clojure's built-in `time` macro is a tool to
    measure the amount of time elapsed while executing a body of code. Measuring performance
    factors is a much more involved process. The measured performance numbers may
    have linkages with each other that we need to analyze. It is a common practice
    to use statistical concepts to establish the linkage factors. We will discuss
    some basic statistical concepts in this section and use that to explain how the
    measured data gives us the bigger picture.
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: A tiny statistics terminology primer
  id: totrans-829
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we have a series of quantitative data, such as latency in milliseconds
    for the same operation (measured over a number of executions), we can observe
    a number of things. First, and the most obvious, are the minimum and maximum values
    in the data. Let''s take an example dataset to analyze further:'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: '| 23 | 19 | 21 | 24 | 26 | 20 | 22 | 21 | 25 | 168 | 23 | 20 | 29 | 172 | 22
    | 24 | 26 |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
- en: Median, first quartile, third quartile
  id: totrans-832
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can see that the minimum latency here is 19 ms whereas the maximum latency
    is 172ms. We can also observe that the average latency here is about 40ms. Let''s
    sort this data in ascending order:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: '| 19 | 20 | 20 | 21 | 21 | 22 | 22 | 23 | 23 | 24 | 24 | 25 | 26 | 26 | 29
    | 168 | 172 |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
- en: 'The center element of the previous dataset, that is the ninth element (value
    23), is considered the **median** of the dataset. It is noteworthy that the median
    is a better representative of the center of the data than the **average** or **mean**.
    The center element of the left half, that is the fifth element (value 21), is
    considered the **first quartile**. Similarly, the value in the center of the right
    half, that is the 13th element (value 26), is considered the **third quartile**
    of the dataset. The difference between the third quartile and the first quartile
    is called **Inter Quartile Range (IQR)**, which is 5 in this case. This can be
    illustrated with a **boxplot** , as follows:'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: '![Median, first quartile, third quartile](img/3642_06_01.jpg)'
  id: totrans-836
  prefs: []
  type: TYPE_IMG
- en: A boxplot highlights the first quartile, median and the third quartile of a
    dataset. As you can see, two "outlier" latency numbers (168 and 172) are unusually
    higher than the others. Median makes no representation of outliers in a dataset,
    whereas the mean does.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: '![Median, first quartile, third quartile](img/3642_06_02.jpg)'
  id: totrans-838
  prefs: []
  type: TYPE_IMG
- en: A histogram (the diagram shown previously) is another way to display a dataset
    where we batch the data elements in **periods** and expose the **frequency** of
    such periods. A period contains the elements in a certain range. All periods in
    a histogram are generally the same size; however, it is not uncommon to omit certain
    periods when there is no data.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: Percentile
  id: totrans-840
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **percentile** is expressed with a parameter, such as 99 percentile, or 95
    percentile etc. A percentile is the value below which all the specified percentage
    of data elements exist. For example, 95 percentile means the value *N* among a
    dataset, such that 95 percent of elements in the dataset are below *N* in value.
    As a concrete example, 85 percentile from the dataset of latency numbers we discussed
    earlier in this section is 29, because out of 17 total elements, 14 (which is
    85 percent of 17) other elements in the dataset have a value below 29\. A quartile
    splits a dataset into chunks of 25 percent elements each. Therefore, the first
    quartile is actually 25 percentile, the median is 50 percentile, and the third
    quartile is 75 percentile.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: Variance and standard deviation
  id: totrans-842
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The spread of the data, that is, how far away the data elements are from the
    center value, gives us further insight into the data. Consider the *i^(th)* deviation
    as the difference between the *i^(th)* dataset element value (in statistics terms,
    a "variable" value) and its mean; we can represent it as ![Variance and standard
    deviation](img/image006.jpg). We can express its "variance" and "standard deviation"
    as follows:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: Variance = ![Variance and standard deviation](img/image008.jpg), standard deviation
    (σ) = ![Variance and standard deviation](img/image010.jpg) = ![Variance and standard
    deviation](img/image012.jpg)
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard deviation is shown as the Greek letter "sigma", or simply "s". Consider
    the following Clojure code to determine variance and standard deviation:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: You can use the Clojure-based platform Incanter ([http://incanter.org/](http://incanter.org/))
    for statistical computations. For example, you can find standard deviation using
    `(incanter.stats/sd tdata)` in Incanter.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
- en: The **empirical rule** states the relationship between the elements of a dataset
    and SD. It says that 68.3 percent of all elements in a dataset lie in the range
    of one (positive or negative) SD from the mean, 95.5 percent of all elements lie
    in two SDs from the mean, and 99.7 percent of all data elements lie in three SDs
    from the mean.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the latency dataset we started out with, one SD from the mean is
    ![Variance and standard deviation](img/image014.jpg)(![Variance and standard deviation](img/image016.jpg)
    range -9 to 89) containing 88 percent of all elements. Two SDs from the mean is
    ![Variance and standard deviation](img/image014.jpg) range -58 to 138) containing
    the same 88 percent of all elements. However, three SDs from the mean is(![Variance
    and standard deviation](img/image018.jpg)range -107 to 187) containing 100 percent
    of all elements. There is a mismatch between what the empirical rule states and
    the results we found, because the empirical rule applies generally to uniformly
    distributed datasets with a large number of elements.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Criterium output
  id: totrans-850
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 4](ch11.html "Chapter 4. Host Performance"), *Host Performance*,
    we introduced the Clojure library *Criterium* to measure the latency of Clojure
    expressions. A sample benchmarking result is as follows:'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-852
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: We can see that the result has some familiar terms we discussed earlier in this
    section. A high mean and low standard deviation indicate that there is not a lot
    of variation in the execution times. Likewise, the lower (first) and upper (third)
    quartiles indicate that they are not too far away from the mean. This result implies
    that the body of code is more or less stable in terms of response time. Criterium
    repeats the execution many times to collect the latency numbers.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: However, why does Criterium attempt to do a statistical analysis of the execution
    time? What would be amiss if we simply calculate the mean? It turns out that the
    response times of all executions are not always stable and there is often disparity
    in how the response time shows up. Only upon running sufficient times under correctly
    simulated load we can get complete data and other indicators about the latency.
    A statistical analysis gives insight into whether there is something wrong with
    the benchmark.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: Guided performance objectives
  id: totrans-855
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We only briefly discussed performance objectives in [Chapter 1](ch15.html "Chapter 8. Application
    Performance"), *Performance by Design* because that discussion needs a reference
    to statistical concepts. Let's say we identified the functional scenarios and
    the corresponding response time. Should response time remain fixed? Can we constrain
    throughput in order to prefer a stipulated response time?
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: The performance objective should specify the worst-case response time, that
    is, maximum latency, the average response time and the maximum standard deviation.
    Similarly, the performance objective should also mention the worst-case throughput,
    maintenance window throughput, average throughput, and the maximum standard deviation.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  id: totrans-858
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing for performance requires us to know what we are going to test, how we
    want to test, and what environment to set up for the tests to execute. There are
    several pitfalls to be aware of, such as a lack of near-real hardware and resources
    of production use, similar OS and software environments, diversity of representative
    data for test cases, and so on. Lack of diversity in test inputs may lead to a
    monotonic branch prediction, hence introducing a "bias" in test results.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
- en: The test environment
  id: totrans-860
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concerns about the test environment begin with the hardware representative of
    the production environment. Traditionally, the test environment hardware has been
    a scaled-down version of the production environment. The performance analysis
    done on non-representative hardware is almost certain to skew the results. Fortunately,
    in recent times, thanks to the commodity hardware and cloud systems, provisioning
    test environment hardware that is similar to the production environment is not
    too difficult.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
- en: The network and storage bandwidth, operating system, and software used for performance
    testing should of course be the same as in production. What is also important
    is to have a "load" representative of the test scenarios. The load comes in different
    combinations including the concurrency of requests, the throughput and standard
    deviation of requests, the current population level in the database or in the
    message queue, CPU and heap usage, and so on. It is important to simulate a representative
    load.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: Testing often requires quite some work on the part of the piece of code that
    carries out the test. Be sure to keep its overhead to a minimum so that it does
    not impact the benchmark results. Wherever possible, use a system other than the
    test target to generate requests.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
- en: What to test
  id: totrans-864
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any implementation of a non-trivial system typically involves many hardware
    and software components. Performance testing a certain feature or a service in
    the entire system needs to account for the way it interacts with the various sub-systems.
    For example, a web service call may touch multiple layers such as the web server
    (request/response marshaling and unmarshaling), URI-based routing, service handler,
    application-database connector, the database layer, logger component, and so on.
    Testing only the service handler would be a terrible mistake, because that is
    not exactly the performance what the web client will experience. The performance
    test should test at the perimeter of a system to keep the results realistic, preferably
    with a third-party observer.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: The performance objectives state the criteria for testing. It would be useful
    to test what is not required by the objective, especially when the tests are run
    concurrently. Running meaningful performance tests may require a certain level
    of isolation.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: Measuring latency
  id: totrans-867
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The latency obtained by executing a body of code may vary slightly on each run.
    This necessitates that we execute the code many times and collect samples. The
    latency numbers may be impacted by the JVM warm-up time, garbage collection and
    the JIT compiler kicking in. So, the test and sample collection should ensure
    that these conditions do not impact the results. Criterium follows such methods
    to produce the results. When we test a very small piece of code this way, it is
    called a **Micro-benchmark**.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: As the latency of some operations may vary during runs, it is important to collect
    all samples and segregate them into periods and frequencies turning up into a
    histogram. The maximum latency is an important metric when measuring latency—it
    indicates the worst-case latency. Besides the maximum, the 99 percentile and 95
    percentile latency numbers are also important to put things in perspective. It's
    important to actually collect the latency numbers instead of inferring them from
    standard deviation, as we noted earlier that the empirical rule works only for
    normal distributions without significant outliers.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: The outliers are an important data point when measuring latency. A proportionately
    higher count of outliers indicates a possibility of degradation of service.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
- en: Comparative latency measurement
  id: totrans-871
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When evaluating libraries for use in projects, or when coming up with alternate
    solutions against some baseline, comparative latency benchmarks are useful to
    determine the performance trade-offs. We will inspect two comparative benchmarking
    tools based on Criterium, called Perforate and Citius. Both make it easy to run
    Criterium benchmarks grouped by context, and to easily view the benchmark results.
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: 'Perforate ([https://github.com/davidsantiago/perforate](https://github.com/davidsantiago/perforate))
    is a Leiningen plugin that lets one define goals; a goal (defined using `perforate.core/defgoal`)
    is a common task or context having one or more benchmarks. Each benchmark is defined
    using `perforate.core/defcase`. As of version 0.3.4, a sample benchmark code may
    look like the following code snippet:'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: You can declare the test environments in `project.clj` and provide the setup/cleanup
    code when defining the goal. Perforate provides ways to run the benchmarks from
    the command-line.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: Citius ([https://github.com/kumarshantanu/citius](https://github.com/kumarshantanu/citius))
    is a library that provides integration hooks for clojure.test and other forms
    of invocation. It imposes more rigid constraints than Perforate, and renders additional
    comparative information about the benchmarks. It presumes a fixed number of targets
    (cases) per test suite where there may be several goals.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: 'As of version 0.2.1, a sample benchmark code may look like the following code
    snippet:'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: In the previous example, the code runs the benchmarks, reports the comparative
    summary, and draws a bar chart image of the mean latencies.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
- en: Latency measurement under concurrency
  id: totrans-880
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we benchmark a piece of code with Criterium, it uses just a single thread
    to determine results. That gives us a fair output in terms of single-threaded
    result, but there are many benchmarking scenarios where single-threaded latency
    is far from what we need. Under concurrency, the latency often differs quite a
    bit from single-threaded latency. Especially when we deal with *stateful* objects
    (e.g. drawing a connection from a JDBC connection pool, updating shared in-memory
    state etc.), the latency is likely to vary in proportion with the contention.
    In such scenarios it is useful to find out the latency patterns of the code under
    various concurrency levels.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: 'The Citius library we discussed in the previous sub-section supports tunable
    concurrency levels. Consider the following benchmark of implementations of shared
    counters:'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: When I ran this benchmark on a 4th generation quad-core Intel Core i7 processor
    (Mac OSX 10.10), the mean latency at concurrency level 04 was 38 to 42 times the
    value of the mean latency at concurrency level 01\. Since, in many cases, the
    JVM is used to run server-side applications, benchmarking under concurrency becomes
    all the more important.
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: Measuring throughput
  id: totrans-885
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughput is expressed per unit of time. Coarse-grained throughput, that is,
    the throughput number collected over a long period of time, may hide the fact
    when the throughput is actually delivered in bursts instead of a uniform distribution.
    Granularity of the throughput test is subject to the nature of the operation.
    A batch process may process bursts of data, whereas a web service may deliver
    uniformly distributed throughput.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
- en: Average throughput test
  id: totrans-887
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Though Citius (as of version 0.2.1) shows extrapolated throughput (per second,
    per thread) in benchmark results, that throughput number may not represent the
    actual throughput very well for a variety of reasons. Let''s construct a simple
    throughput benchmark harness as follows, beginning with the helper functions:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Now that we have the helper functions defined, let''s see the benchmarking
    code:'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Let''s now see how to test some code for throughput using the harness:'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: This harness provides only a simple throughput test. To inspect the throughput
    pattern you may want to bucket the throughput across rolling fixed-duration windows
    (e.g. per second throughput.) However, that topic is beyond the scope of this
    text, though we will touch upon it in the *Performance monitoring* section later
    in this chapter.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: The load, stress, and endurance tests
  id: totrans-895
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the characteristics of tests is each run only represents the slice of
    time it is executed through. Repeated runs establish their general behavior. But
    how many runs should be enough? There may be several anticipated load scenarios
    for an operation. So, there is a need to repeat the tests at various load scenarios.
    Simple test runs may not always exhibit the long-term behavior and response of
    the operation. Running the tests under varying high load for longer duration allows
    us to observe them for any odd behavior that may not show up in a short test cycle.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
- en: When we test an operation at a load far beyond its anticipated latency and throughput
    objectives, that is **stress testing**. The intent of a stress test is to ascertain
    a reasonable behavior exhibited by the operation beyond the maximum load it was
    developed for. Another way to observe the behavior of an operation is to see how
    it behaves when run for a very long duration, typically for several days or weeks.
    Such prolonged tests are called **endurance tests**. While a stress test checks
    the graceful behavior of the operation, an endurance test checks the consistent
    behavior of the operation over a long period.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: There are several tools that may help with load and stress testing. Engulf ([http://engulf-project.org/](http://engulf-project.org/))
    is a distributed HTTP-based, load-generation tool written in Clojure. Apache JMeter
    and Grinder are Java-based load-generation tools. Grinder can be scripted using
    Clojure. Apache Bench is a load-testing tool for web systems. Tsung is an extensible,
    high-performance, load-testing tool written in Erlang.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: Performance monitoring
  id: totrans-899
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During prolonged testing or after the application has gone to production, we
    need to monitor its performance to make sure the application continues to meet
    the performance objectives. There may be infrastructure or operational issues
    impacting the performance or availability of the application, or occasional spikes
    in latency or dips in throughput. Generally, monitoring alleviates such risk by
    generating a continuous feedback stream.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
- en: Roughly there are three kinds of components used to build a monitoring stack.
    A **collector** sends the numbers from each host that needs to be monitored. The
    collector gets host information and the performance numbers and sends them to
    an **aggregator**. An aggregator receives the data sent by the collector and persists
    them until asked by a **visualizer** on behalf of the user.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: The project **metrics-clojure** ([https://github.com/sjl/metrics-clojure](https://github.com/sjl/metrics-clojure))
    is a Clojure wrapper over the **Metrics** ([https://github.com/dropwizard/metrics](https://github.com/dropwizard/metrics))
    Java framework, which acts as a collector. **Statsd** is a well-known aggregator
    that does not persist data by itself but passes it on to a variety of servers.
    One of the popular visualizer projects is **Graphite** that stores the data as
    well as produces graphs for requested periods. There are several other alternatives
    to these, notably **Riemann** ([http://riemann.io/](http://riemann.io/)) that
    is written in Clojure and Ruby. Riemann is an event processing-based aggregator.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring through logs
  id: totrans-903
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the popular performance monitoring approaches that has emerged in recent
    times is via logs. The idea is simple—the application emits metrics data as logs,
    which are shipped from the individual machine to a central log aggregation service.
    Then, those metrics data are aggregated for each time window and further moved
    for archival and visualization.
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
- en: 'As a high-level example of such a monitoring system, you may like to use **Logstash-forwarder**
    ([https://github.com/elastic/logstash-forwarder](https://github.com/elastic/logstash-forwarder))
    to grab the application logs from the local filesystem and ship to **Logstash**
    ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)),
    where it forwards the metrics logs to **StatsD** ([https://github.com/etsy/statsd](https://github.com/etsy/statsd))
    for metrics aggregation or to Riemann ([http://riemann.io/](http://riemann.io/))
    for events analysis and monitoring alerts. StatsD and/or Riemann can forward the
    metrics data to Graphite ([http://graphite.wikidot.com/](http://graphite.wikidot.com/))
    for archival and graphing of the time-series metrics data. Often, people want
    to plug in a non-default time-series data store (such as **InfluxDB**: [https://influxdb.com/](https://influxdb.com/))
    or a visualization layer (such as **Grafana**: [http://grafana.org/](http://grafana.org/))
    with Graphite.'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion on this topic is out of the scope of this text, but I
    think exploring this area would serve you well.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
- en: Ring (web) monitoring
  id: totrans-907
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you develop web software using Ring ([https://github.com/ring-clojure/ring](https://github.com/ring-clojure/ring))
    then you may find the Ring extension of the metrics-clojure library useful: [http://metrics-clojure.readthedocs.org/en/latest/ring.html](http://metrics-clojure.readthedocs.org/en/latest/ring.html)
    —this tracks a number of useful metrics that can be queried in JSON format and
    integrated with visualization via the web browser.'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
- en: To emit a continuous stream of metrics data from the web layer, **Server-Sent
    Events** (**SSE**) may be a good idea due to its low overhead. Both **http-kit**
    ([http://www.http-kit.org/](http://www.http-kit.org/)) and **Aleph** ([http://aleph.io/](http://aleph.io/)),
    which work with Ring, support SSE today.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
- en: Introspection
  id: totrans-910
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Oracle JDK and OpenJDK provide two GUI tools called **JConsole** (executable
    name `jconsole`) and **JVisualVM** (executable name `jvisualvm`) that we can use
    to introspect into running JVMs for instrumentation data. There are also some
    command-line tools ([http://docs.oracle.com/javase/8/docs/technotes/tools/](http://docs.oracle.com/javase/8/docs/technotes/tools/))
    in the JDK to peek into the inner details of the running JVMs.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
- en: A common way to introspect a running Clojure application is to have an **nREPL**
    ([https://github.com/clojure/tools.nrepl](https://github.com/clojure/tools.nrepl))
    service running so that we can connect to it later using an nREPL client. Interactive
    introspection over nREPL using the Emacs editor (embedded nREPL client) is popular
    among some, whereas others prefer to script an nREPL client to carry out tasks.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
- en: JVM instrumentation via JMX
  id: totrans-913
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The JVM has a built-in mechanism to introspect managed resources via the extensible
    **Java Management Extensions** (**JMX**) API. It provides a way for application
    maintainers to expose manageable resources as "MBeans". Clojure has an easy-to-use
    `contrib` library called `java.jmx` ([https://github.com/clojure/java.jmx](https://github.com/clojure/java.jmx))
    to access JMX. There is a decent amount of open source tooling for visualization
    of JVM instrumentation data via JMX, such as `jmxtrans` and `jmxetric`, which
    integrate with Ganglia and Graphite.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting quick memory stats of the JVM is pretty easy using Clojure:'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Profiling
  id: totrans-917
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We briefly discussed profiler types in [Chapter 1](ch15.html "Chapter 8. Application
    Performance"), *Performance by Design*. The JVisualVM tool we discussed with respect
    to introspection in the previous section is also a CPU and memory profiler that
    comes bundled with the JDK. Let''s see them in action— consider the following
    two Clojure functions that stress the CPU and memory respectively:'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Using JVisualVM is pretty easy—open the Clojure JVM process from the left pane.
    It has sampler and regular profiler styles of profiling. Start profiling for CPU
    or memory use when the code is running and wait for it to collect enough data
    to plot on the screen.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling](img/3642_06_03.jpg)'
  id: totrans-921
  prefs: []
  type: TYPE_IMG
- en: 'The following shows memory profiling in action:'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling](img/3642_06_04.jpg)'
  id: totrans-923
  prefs: []
  type: TYPE_IMG
- en: Note that JVisualVM is a very simple, entry-level profiler. There are several
    commercial JVM profilers on the market for sophisticated needs.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: OS and CPU/cache-level profiling
  id: totrans-925
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling only the JVM may not always tell the whole story. Getting down to
    OS and hardware-level profiling often provides better insight into what is going
    on with the application. On Unix-like operating systems, command-line tools such
    as `top`, `htop`, `perf`, `iota`, `netstat`, `vista`, `upstate`, `pidstat` etc
    can help. Profiling the CPU for cache misses and other information is a useful
    source to catch performance issues. Among open source tools for Linux, **Likwid**
    ([http://code.google.com/p/likwid/](http://code.google.com/p/likwid/) and [https://github.com/rrze-likwid/likwid](https://github.com/rrze-likwid/likwid))
    is small yet effective for Intel and AMD processors; **i7z** ([https://code.google.com/p/i7z/](https://code.google.com/p/i7z/)
    and [https://github.com/ajaiantilal/i7z](https://github.com/ajaiantilal/i7z))
    is specifically for Intel processors. There are also dedicated commercial tools
    such as **Intel VTune Analyzer** for more elaborate needs.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: I/O profiling
  id: totrans-927
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling I/O may require special tools too. Besides `iota` and `blktrace`,
    `ioping` ([https://code.google.com/p/ioping/](https://code.google.com/p/ioping/)
    and [https://github.com/koct9i/ioping](https://github.com/koct9i/ioping)) is useful
    to measure real-time I/O latency on Linux/Unix systems. The **vnStat** tool is
    useful to monitor and log network traffic on Linux. The IOPS of a storage device
    may not tell the whole truth unless it is accompanied by latency information for
    different operations, and how many reads and writes can simultaneously happen.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
- en: In an I/O bound workload one has to look for the read and write IOPS over time
    and set a threshold to achieve optimum performance. The application should throttle
    the I/O access so that the threshold is not crossed.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-930
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delivering high-performance applications not only requires care for performance
    but also systematic effort to measure, test, monitor, and optimize the performance
    of various components and subsystems. These activities often require the right
    skill and experience. Sometimes, performance considerations may even bring system
    design and architecture back to the drawing board. Early structured steps taken
    to achieve performance go a long way to ensuring that the performance objectives
    are being continuously met.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into performance optimization tools and techniques.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7. Performance Optimization
  id: totrans-933
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance optimization is additive by nature, as in it works by adding performance
    tuning to the knowledge of how the underlying system works, and to the result
    of performance measurement. This chapter builds on the previous ones that covered
    "how the underlying system works" and "performance measurement". Though you will
    notice some recipe-like sections in this chapter, you already know the pre-requisite
    in order to exploit those well. Performance tuning is an iterative process of
    measuring performance, determining bottlenecks, applying knowledge in order to
    experiment with tuning the code, and repeating it all until performance improves.
    In this chapter, we will cover:'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
- en: Setting up projects for better performance
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying performance bottlenecks in the code
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling code with VisualVM
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance tuning of Clojure code
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JVM performance tuning
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project setup
  id: totrans-940
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While finding bottlenecks is essential to fixing performance problems in the
    code, there are several things one can do right from the start to ensure better
    performance.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
- en: Software versions
  id: totrans-942
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Usually, new software versions include bug fixes, new features, and performance
    improvements. Unless advised to the contrary, it is better to use newer versions.
    For development with Clojure, consider the following software versions:'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
- en: '**JVM version**: As of this writing, Java 8 (Oracle JDK, OpenJDK, Zulu) has
    been released as the latest stable production-ready version. It is not only stable,
    it also has better performance in several areas (especially concurrency) than
    the earlier versions. If you have a choice, choose Java 8 over the older versions
    of Java.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clojure version**: As of this writing, Clojure 1.7.0 is the latest stable
    version that has several performance improvements over the older versions. There
    are also new features (transducers, volatile) that can make your code perform
    better. Choose Clojure 1.7 over the older versions unless you have no choice.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leiningen project.clj configuration
  id: totrans-946
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As of version 2.5.1, the default Leiningen template (`lein new foo`, `lein new
    app foo`) needs few tweaks to make the project amenable to performance. Ensure
    your Leiningen `project.clj` file has the following entries, as appropriate.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
- en: Enable reflection warning
  id: totrans-948
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most common pitfalls in Clojure programming is to inadvertently
    let the code resort to reflection. Recall that we discussed this in [Chapter 3](ch10.html
    "Chapter 3. Leaning on Java"), *Leaning on Java. Enabling*, reflection warning
    is quite easy, let''s fix it by adding the following entry to `project.clj`:'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-950
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: In the previous configuration, the first setting `*unchecked-math* :warn-on-boxed`
    works only in Clojure 1.7—it emits numeric boxing warnings. The second setting
    `*warn-on-reflection* true` works on earlier Clojure versions as well as Clojure
    1.7, and emits reflection warning messages in the code.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
- en: However, including these settings in `project.clj` may not be enough. Reflection
    warnings are emitted only when a namespace is loaded. You need to ensure that
    all namespaces are loaded in order to search for reflection warnings throughout
    the project. This can be done by writing tests that refer to all namespaces, or
    via scripts that do so.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
- en: Enable optimized JVM options when benchmarking
  id: totrans-953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [Chapter 4](ch11.html "Chapter 4. Host Performance"), *Host Performance*
    we discussed that Leiningen enables tiered compilation by default, which provides
    low startup time at the cost of poor JIT compiler optimization. The default setting
    is quite misleading for performance benchmarking, so you should enable JVM options
    that are representative of what you would use in production:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'For example, the previous setting defines a Leiningen profile that overrides
    the default JVM options to configure a `server` Java runtime with 2 GB of fixed-size
    heap space. It also sets test paths to a directory `perf-test`. Now you can run
    performance tests as follows:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-957
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: If your project has performance test suites that require different JVM options,
    you should define multiple profiles for running tests, as appropriate.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
- en: Distinguish between initialization and runtime
  id: totrans-959
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most non-trivial projects need a lot of context to be set up before they can
    function. Examples of such contexts could be app configuration, in-memory state,
    I/O resources, thread pools, caches, and so on. While many projects start with
    ad hoc configuration and initialization, eventually projects need to isolate the
    initialization phase from runtime. The purpose of this distinction is not only
    to sanitize the organization of code, but also to pre-compute as much as possible
    once before the runtime can take over to repeatedly respond to demands. This distinction
    also allows the initialization phase to easily (and conditionally, based on configuration)
    instrument the initialized code for performance logging and monitoring.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
- en: Non-trivial programs are usually divided into layers, such as business logic,
    caching, messaging, database access, and so on. Each layer has a dependency relationship
    with one or more of the other layers. It is feasible to carry out the isolation
    of the initialization phase by writing code using first principles, and many projects
    actually do that. However, there are a few libraries that simplify this process
    by letting you declare the dependency relationship between layers. **Component**
    ([https://github.com/stuartsierra/component](https://github.com/stuartsierra/component))
    and **Prismatic Graph** ([https://github.com/Prismatic/plumbing](https://github.com/Prismatic/plumbing))
    are notable examples of such libraries.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
- en: 'The Component library is well documented. It may not be easily apparent how
    to use Prismatic Graph for dependency resolution; following is a contrived example
    for illustration:'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-963
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: This example merely shows the construction of a layer dependency graph, but
    often you may need different construction scope and order for testing. In that
    case you may define different graphs and resolve them, as and when appropriate.
    If you need teardown logic for testing, you can add extra `fnk` entries for each
    teardown step and use those for teardown.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
- en: Identifying performance bottlenecks
  id: totrans-965
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed in previous chapters that random performance tuning of code rarely
    works, because we may not be tuning in the right place. It is crucial to find
    the performance bottlenecks before we can tune those areas in the code. Upon finding
    the bottleneck, we can experiment with alternate solutions around it. In this
    section we will look into finding the bottlenecks.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
- en: Latency bottlenecks in Clojure code
  id: totrans-967
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Latency is the starting, and the most obvious, metric to drill-down in order
    to find bottlenecks. For Clojure code, we observed in [Chapter 6](ch13.html "Chapter 6. Measuring
    Performance"), *Measuring Performance* that code profiling tools can help us find
    the areas of bottleneck. Profilers are, of course, very useful. Once you discover
    hotspots via profilers, you may find ways to tune those for latency to a certain
    extent.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
- en: 'Most profilers work on aggregates, a batch of runs, ranking the hotspots in
    code by resources consumed. However, often the opportunity to tune latency lies
    in the long tail that may not be highlighted by the profilers. In such circumstances,
    we may employ a direct drill-down technique. Let''s see how to carry out such
    drill-down using **Espejito** ([https://github.com/kumarshantanu/espejito](https://github.com/kumarshantanu/espejito)),
    a Clojure library for measuring latency (as of version 0.1.0) across measurement
    points in single-threaded execution paths. There are two parts of using **Espejito**,
    both requiring change to your code—one to wrap the code being measured, and the
    other to report the collected measurement data. The following code illustrates
    a contrived E-commerce use case of adding an item to a cart:'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-970
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Reporting a call is required to be made only once at the outermost (top-level)
    layer of the code. Measurement calls can be made at any number of places in the
    call path. Be careful not to put measurement calls inside tight loops, which may
    shoot memory consumption up. When this execution path is triggered, the functionality
    works as usual, while the latencies are measured and recorded alongside transparently
    in memory. The `e/report` call prints a table of recorded metrics. An example
    output (edited to fit) would be:'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Here we can observe that the database call is the most expensive (individual
    latency), followed by the web layer. Our tuning preference may be guided by the
    order of expensiveness of the measurement points.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
- en: Measure only when it is hot
  id: totrans-974
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One important aspect we did not cover in the drill-down measurement is whether
    the environment is ready for measurement. The `e/report` call is invoked unconditionally
    every time, which would not only have its own overhead (table printing), but the
    JVM may not be warmed up and the JIT compiler may not have kicked in to correctly
    report the latencies. To ensure that we report only meaningful latencies, let''s
    trigger the `e/report` call on an example condition:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Now, let''s assume it is a **Ring**-based ([https://github.com/ring-clojure/ring](https://github.com/ring-clojure/ring))
    web app and you want to trigger the reporting only when the web request contains
    a parameter `report` with a value `true`. In that case, your call might look like
    the following:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Condition-based invocation expects the JVM to be up across several calls, so
    it may not work with command-line apps.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: This technique can also be used in performance tests, where non-reporting calls
    may be made during a certain warm-up period, followed by a reporting call that
    provides its own reporter function instead of `e/print-table`. You may even write
    a sampling reporter function that aggregates the samples over a duration and finally
    reports the latency metrics. Not only for performance testing, you can use this
    for latency monitoring where the reporter function logs the metrics instead of
    printing a table, or sends the latency breakup to a metrics aggregation system.
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection bottlenecks
  id: totrans-981
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Clojure runs on the JVM, one has to be aware of the GC behavior in the
    application. You can print out the GC details at runtime by specifying the respective
    JVM options in `project.clj` or on the Java command-line:'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-983
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'This causes a detailed summary of GC events to be printed as the application
    runs. To capture the output in a file, you can specify the following parameter:'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-985
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'It is also useful to see the time between and during full GC events:'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-987
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'The other useful options to troubleshoot GC are as follows:'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: '`-XX:+HeapDumpOnOutOfMemoryError`'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:+PrintTenuringDistribution`'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-XX:+PrintHeapAtGC`'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the previous options may help you identify GC bottlenecks that
    you can try to fix by choosing the right garbage collector, other generational
    heap options, and code changes. For easy viewing of GC logs, you may like to use
    GUI tools such as **GCViewer** ([https://github.com/chewiebug/GCViewer](https://github.com/chewiebug/GCViewer))
    for this purpose.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
- en: Threads waiting at GC safepoint
  id: totrans-993
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When there is a long tight loop (without any I/O operation) in the code, the
    thread executing it cannot be brought to safepoint if GC happens when the loop
    ends or goes out of memory (for example, fails to allocate). This may have a disastrous
    effect of stalling other critical threads during GC. You can identify this category
    of bottleneck by enabling safepoint logs using the following JVM option:'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-995
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: The safepoint logs emitted by the previous option may help you identify the
    impact of a tight-loop thread on other threads during GC.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
- en: Using jstat to probe GC details
  id: totrans-997
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Oracle JDK (also OpenJDK, Azul''s Zulu) comes with a utility called `jstat`
    that can be handy to inspect GC details. You can find details on this utility
    at [https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html)
    —the following examples show how to use it:'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-999
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: The first command mentioned previously monitors object allocations and freeing
    in various heap generations, together with other GC statistics, one in every 10
    seconds. The second command also prints the reason for GC, along with other details.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting generated bytecode for Clojure source
  id: totrans-1001
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed in [Chapter 3](ch10.html "Chapter 3. Leaning on Java"), *Leaning
    on Java* how to see the generated equivalent Java code for any Clojure code. Sometimes,
    there may not be a direct correlation between the generated bytecode and Java,
    which is when inspecting the generated bytecode is very useful. Of course, it
    requires the reader to know at least a bit about the JVM instruction set ([http://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html](http://docs.oracle.com/javase/specs/jvms/se8/html/jvms-6.html)).
    This tool can allow you to very effectively analyze the cost of the generated
    bytecode instructions.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: 'The project **no.disassemble** ([https://github.com/gtrak/no.disassemble](https://github.com/gtrak/no.disassemble))
    is a very useful tool to discover the generated bytecode. Include it in your `project.clj`
    file as a Leiningen plugin:'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-1004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Then, at the REPL, you can inspect the generated bytecodes one by one:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: The previous snippet prints out the bytecode of the Clojure expression entered
    there.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: Throughput bottlenecks
  id: totrans-1008
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The throughput bottlenecks usually arise from shared resources, which could
    be CPU, cache, memory, mutexes and locks, GC, disk, and other I/O devices. Each
    of these resources has a different way to find utilization, saturation, and load
    level. This also heavily depends on the operating system in use, as it manages
    the resources. Delving into the OS-specific ways of determining those factors
    is beyond the scope of this text. However, we will look at profiling some of these
    for bottlenecks in the next section.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
- en: The net effect of throughput shows up as an inverse relationship with latency.
    This is natural as per Little's law—as we will see in the next chapter. We covered
    throughput testing and latency testing under concurrency in [Chapter 6](ch13.html
    "Chapter 6. Measuring Performance"), *Measuring Performance*. This should be roughly
    a good indicator of the throughput trend.
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: Profiling code with VisualVM
  id: totrans-1011
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Oracle JDK (also OpenJDK) comes with a powerful profiler called **VisualVM**;
    the distribution that comes with the JDK is known as Java VisualVM and can be
    invoked using the binary executable:'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-1013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: This launches the GUI profiler app where you can connect to running instances
    of the JVM. The profiler has powerful features ([https://visualvm.java.net/features.html](https://visualvm.java.net/features.html))
    that can be useful for finding various bottlenecks in code. Besides analyzing
    heap dump and thread dump, VisualVM can interactively graph CPU and heap consumption,
    and thread status in near real time. It also has sampling and tracing profilers
    for both CPU and memory.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: The Monitor tab
  id: totrans-1015
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Monitor** tab has a graphical overview of the runtime, including CPU,
    heap, threads and loaded classes:'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: '![The Monitor tab](img/3642_07_01.jpg)'
  id: totrans-1017
  prefs: []
  type: TYPE_IMG
- en: This tab is useful for "at a glance" information, leaving further drill-down
    for other tabs.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: The Threads tab
  id: totrans-1019
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following screenshot, the **Threads** tab shows the status of all threads:'
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: '![The Threads tab](img/3642_07_02.jpg)'
  id: totrans-1021
  prefs: []
  type: TYPE_IMG
- en: 'It is very useful to find out if any threads are undergoing contention, entering
    deadlock, are underutilized, or they are taking up more CPU. Especially in concurrent
    apps with in-memory state, and in apps that use limited I/O resources (such as
    connection pools, or network calls to other hosts) shared by threads, this feature
    provides a great insight if you set the thread names:'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the threads named **citius-RollingStore-store-1** through **citius-RollingStore-store
    - 4**. In an ideal no-contention scenario, those threads would have a green **Running**
    status. See the legend at the bottom right of the image, which explains thread
    state:'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: '**Running**: A thread is running, which is the ideal condition.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sleeping**: A thread has yielded control temporarily.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wait**: A thread is waiting for notification in a critical section. `Object.wait()`
    was called, and is now waiting for `Object.notify()` or `Object.notifyAll()` to
    wake it up.'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Park**: A thread is parked on a permit (binary semaphore) waiting for some
    condition. Usually seen with concurrent blocking calls in the `java.util.concurrent`
    API.'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor**: A thread has reached object monitor waiting for some lock, perhaps
    waiting to enter or exit a critical section.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can install the *Threads Inspector* plugin for details on threads of interest.
    To inspect thread dumps from the command line you can use the `jstack` or `kill
    -3` commands.
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: The Sampler tab
  id: totrans-1030
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Sampler** tab is the lightweight sampling profiler tab that can sample
    both CPU and memory consumption. You can easily find hotspots in code that may
    benefit from tuning. However, sampler profiling is limited by sampling period
    and frequency, inability to detect inlined code, and so on. It is a good general
    indicator of the bottlenecks and looks similar to the screenshots we saw in [Chapter
    6](ch13.html "Chapter 6. Measuring Performance"), *Measuring Performance*. You
    can profile either CPU or memory at a time.
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: The **CPU** tab displays both the overall CPU time distribution and per-thread
    CPU consumption. You can take a thread dump while sampling is in progress and
    analyze the dump. There are several VisualVM plugins available for more analysis.
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
- en: The **Memory** tab displays heap histogram metrics with distribution and instance
    count of objects. It also shows a PermGen histogram and per thread allocation
    data. It is a very good idea and highly recommended to set thread names in your
    project so that it is easy to locate those names in such tools. In this tab, you
    can force a GC, take a heap dump for analysis, and view memory metrics data in
    several ways.
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
- en: Setting the thread name
  id: totrans-1034
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Setting a thread name in Clojure is quite straightforward using Java interop:'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-1036
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'However, since threads often transcend several contexts, in most cases you
    should do so in a limited scope as follows:'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-1038
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Now you can use this macro to execute any body of code with a specified thread
    name:'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-1040
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: This style of setting a thread name makes sure that the original name is restored
    before leaving the thread-local scope. If your code has various sections and you
    are setting a different thread name for each section, you can detect which code
    sections are causing contention by looking at the name when any contention appears
    on profiling and monitoring tools.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
- en: The Profiler tab
  id: totrans-1042
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Profiler** tab lets you instrument the running code in the JVM, and profile
    both CPU and memory consumption. This option adds a larger overhead than the **Sampler**
    tab, and poses a different trade off in terms of JIT compilation, inlining, and
    accuracy. This tab does not have as much diversity in visualization as the **Sampler**
    tab. The main difference this tab has with the **Sampler** tab is it changes the
    bytecode of the running code for accurate measurement. When you choose CPU profiling,
    it starts instrumenting the code for CPU profiling. If you switch from CPU to
    memory profiling, it re-instruments the running code for memory profiling, and
    re-instruments every time you want a different profiling. One downside of such
    instrumentation is that it may massively slow down everything if your code is
    deployed in application containers, such as Tomcat.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
- en: While you can get most of the common CPU bottleneck information from **Sampler**,
    you may need the **Profiler** to investigate hotspots already discovered by **Sampler**
    and other profiling techniques. You can selectively profile and drill-down only
    the known bottlenecks using the instrumenting profiler, thereby restricting its
    ill-effects to only small parts of the code.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: The Visual GC tab
  id: totrans-1045
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Visual GC** is a VisualVM plugin that visually depicts the GC status in
    near real time.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: '![The Visual GC tab](img/3642_07_03.jpg)'
  id: totrans-1047
  prefs: []
  type: TYPE_IMG
- en: If your application uses a lot of memory and potentially has GC bottlenecks,
    this plugin may be very useful for various troubleshooting purposes.
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
- en: The Alternate profilers
  id: totrans-1049
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides VisualVM, there are several third-party profilers and performance-monitoring
    tools for the Java platform. Among open source tools, Prometheus ([http://prometheus.io/](http://prometheus.io/))
    and Moskito ([http://www.moskito.org/](http://www.moskito.org/)) are relatively
    popular. A non-exhaustive list of Open Source performance tools is here: [http://java-source.net/open-source/profilers](http://java-source.net/open-source/profilers)'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: There are several commercial proprietary profilers that you may want to know
    about. The YourKit ([https://www.yourkit.com/](https://www.yourkit.com/)) Java
    profiler is probably the most notable profiler that many people have found much
    success with for profiling Clojure code. There are also other profiling tools
    for the JVM, such as JProfiler ([https://www.ej-technologies.com/products/jprofiler/overview.html](https://www.ej-technologies.com/products/jprofiler/overview.html)),
    which is a desktop-based profiler and web-based hosted solutions such as New Relic
    ([http://newrelic.com/](http://newrelic.com/)) and AppDynamics ([https://www.appdynamics.com/](https://www.appdynamics.com/)).
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning
  id: totrans-1052
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we get insight into the code via testing and profiling results, we need
    to analyze the bottlenecks worth considering for optimization. A better approach
    is to find the most under-performing portion and optimize it, thereby eliminating
    the weakest link. We discussed performance aspects of hardware and JVM/Clojure
    in previous chapters. Optimization and tuning requires rethinking the design and
    code in light of those aspects, and then refactoring for performance objectives.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
- en: Once we establish the performance bottlenecks, we have to pinpoint the root
    cause and experiment with improvisations, one step at a time, to see what works.
    Tuning for performance is an iterative process that is backed by measurement,
    monitoring and experimentation.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Clojure code
  id: totrans-1055
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Identifying the nature of the performance bottleneck helps a lot in order to
    experiment with the right aspects of the code. The key is to determine the origin
    of cost and whether the cost is reasonable.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
- en: CPU/cache bound
  id: totrans-1057
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we noted in the beginning of this chapter, setting up a project with the
    right JVM options and project settings informs us of reflection and boxing, the
    common sources of CPU-bound performance issues after poor design and algorithm
    choice. As a general rule, we have to see whether we are doing unnecessary or
    suboptimal operations, especially inside loops. For example, transducers are amenable
    to better performance than lazy sequences in CPU-bound operations.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: While public functions are recommended to work with immutable data structures,
    the implementation details can afford to use transients and arrays when performance
    is necessary. Records are a great alternative to maps, where appropriate, due
    to type hints and tight field layout in the former. Operations on primitive data
    types is faster (hence recommended) than their boxed equivalents.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
- en: In tight loops, besides transients and arrays you may prefer loop-recur with
    unchecked math for performance. You may also like to avoid using multi-methods
    and dynamic vars in tight loops, rather than pass arguments around. Using Java
    and macros may be the last resort, but still an option if there is such a need
    for performance.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
- en: Memory bound
  id: totrans-1061
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Allocating less memory in code is always going to reduce memory-related performance
    issues. Optimization of memory-bound code is not only about reducing memory consumption,
    but it is also about memory layout and utilizing the CPU and cache well. We have
    to see whether we are using the data types that fit well in CPU registers and
    cache lines. For cache and memory-bound code, we have to know whether there are
    cache misses and the reason—often the data might be too large to fit in a cache
    line. For memory-bound code we have to care about data locality, whether the code
    is hitting the interconnect too often, and whether memory representation of data
    can be slimmed down.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: Multi-threaded
  id: totrans-1063
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shared resources with side effects are the main source of contention and performance
    bottlenecks in multi-threaded code. As we saw in the *Profiling VisualVM code*
    section in this chapter, profiling the threads better informs us about the bottlenecks.
    The best way to improve performance of multi-threaded code is to reduce contention.
    The easy way to reduce contention is to increase the resources and reduce concurrency,
    though only optimal levels of resources and concurrency would be good for performance.
    While designing for concurrency, append only, single writer, and shared nothing
    approaches work well.
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce contention may be to exploit thread-local queueing of
    data until resources are available. This technique is similar to what Clojure
    agents use, though it is an involved technique. [Chapter 5](ch12.html "Chapter 5. Concurrency"),
    *Concurrency* covers agents in some detail. I would encourage you to study the
    agents source code for better understanding. When using CPU-bound resources (for
    example `java.util.concurrent.atomic.AtomicLong`) you may use the contention-striping
    technique used by some Java 8 classes (such as `java.util.concurrent.atomic.LongAdder`,
    which also balances between memory consumption and contention striping across
    processors.) This technique is also quite involved and generic contention-striping
    solutions may have to trade off read consistency to allow fast updates.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: I/O bound
  id: totrans-1066
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I/O-bound tasks could be limited by bandwidth or IOPS/latency. Any I/O bottleneck
    usually manifests in chatty I/O calls or unconstrained data serialization. Restricting
    I/O to only minimum required data is a common opportunity to minimize serialization
    and reduce latency. I/O operations can often be batched for higher throughput,
    for example *SpyMemcached* library employs an asynchronous batched operation for
    high throughput.
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
- en: I/O-bound bottlenecks are often coupled with multi-threaded scenarios. When
    the I/O calls are synchronous (for example, the JDBC API), one naturally has to
    depend upon multiple threads working on a bounded resource pool. Asynchronous
    I/O can relieve our threads from blocking, letting the threads do other useful
    work until the I/O response arrives. In synchronous I/O, we pay the cost of having
    threads (each allocated with memory) block on I/O calls while the kernel schedules
    them.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: JVM tuning
  id: totrans-1069
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often Clojure applications might inherit bloat from Clojure/Java libraries or
    frameworks, which cause poor performance. Hunting down unnecessary abstractions
    and unnecessary layers of code may bring decent performance gains. Reasoning about
    the performance of dependency libraries/frameworks before inclusion in a project
    is a good approach.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: The JIT compiler, garbage collector and safepoint (in Oracle HotSpot JVM) have
    a significant impact on the performance of applications. We discussed the JIT
    compiler and garbage collector in [Chapter 4](ch11.html "Chapter 4. Host Performance"),
    *Host Performance*. When the HotSpot JVM reaches a point when it cannot carry
    out concurrent, incremental GC anymore, it needs to suspend the JVM safely in
    order to carry out a full GC. It is also called the stop-the-world GC pause that
    may run up to several minutes while the JVM appears frozen.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: The Oracle and OpenJDK JVMs accept many command-line options when invoked, to
    tune and monitor the way components in the JVM behave. Tuning GC is common among
    people who want to extract optimum performance from the JVM.
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: 'You may like to experiment with the following JVM options (Oracle JVM or OpenJDK)
    for performance:'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: '| JVM option | Description |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
- en: '| `-XX:+AggressiveOpts` | Aggressive options that enable compressed heap pointers
    |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
- en: '| `-server` | Server class JIT thresholds (use -client for GUI apps) |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
- en: '| `-XX:+UseParNewGC` | Use Parallel GC |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
- en: '| `-Xms3g` | Specify min heap size (keep it less on desktop apps) |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
- en: '| `-Xmx3g` | Specify max heap size (keep min/max same on servers) |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
- en: '| `-XX:+UseLargePages` | Reduce Translation-Lookaside Buffer misses (if OS
    supports), see [http://www.oracle.com/technetwork/java/javase/tech/largememory-jsp-137182.html](http://www.oracle.com/technetwork/java/javase/tech/largememory-jsp-137182.html)
    for details |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
- en: 'On the Java 6 HotSpot JVM, the **Concurrent Mark and Sweep** (**CMS**) garbage
    collector is well regarded for its GC performance. On the Java 7 and Java 8 HotSpot
    JVM, the default GC is a parallel collector (for better throughput), whereas at
    the time of writing this, there is a proposal to use the G1 collector (for lower
    pauses) by default in the upcoming Java 9\. Note that the JVM GC can be tuned
    for different objectives, hence the same exact configuration for one application
    may not work well for another. Refer to the documents Oracle published for tuning
    the JVM at the following links:'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.oracle.com/technetwork/java/tuning-139912.html](http://www.oracle.com/technetwork/java/tuning-139912.html)'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/)'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back pressure
  id: totrans-1085
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not uncommon to see applications behaving poorly under load. Typically,
    the application server simply appears unresponsive, which is often a combined
    result of high resource utilization, GC pressure, more threads that lead to busier
    thread scheduling, and cache misses. If the capacity of a system is known, the
    solution is to apply **back pressure** by denying services after the capacity
    is reached. Note that back pressure cannot be applied optimally until the system
    is load-tested for optimum capacity. The capacity threshold that triggers back
    pressure may or may not be directly associated with individual services, but rather
    can be defined as load criteria.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1087
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is worth reiterating that performance optimization begins with learning about
    how the underlying system works, and measuring the performance of systems we build
    under representative hardware and load. The chief component of performance optimization
    is identifying the bottlenecks using various kinds of measurements and profiling.
    Thereafter, we can apply experiments to tune the performance of code and measure/profile
    once again to verify. The tuning mechanism varies depending on the type of bottleneck.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to address performance concerns when building
    applications. Our focus will be on the several common patterns that impact performance.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8. Application Performance
  id: totrans-1090
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The earliest computing devices were built to perform automatic computations
    and, as computers grew in power, they became increasingly popular because of how
    much and how fast they could compute. Even today, this essence lives on in our
    anticipation that computers can execute our business calculations faster than
    before by means of the applications we run on them.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to performance analysis and optimization at a smaller component level,
    as we saw in previous chapters, it takes a holistic approach to improve performance
    at the application level. The higher-level concerns, such as serving a certain
    number of users in a day, or handling an identified quantum of load through a
    multi-layered system, requires us to think about how the components fit together
    and how the load is designed to flow through it. In this chapter, we will discuss
    such high-level concerns. Like the previous chapter, by and large this chapter
    applies to applications written in any JVM language, but with a focus on Clojure.
    In this chapter, we will discuss general performance techniques that apply to
    all layers of the code:'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: Choosing libraries
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sizing
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource pooling
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch and compute in advance
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Staging and batching
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little's law
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing libraries
  id: totrans-1100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most non-trivial applications depend a great deal on third-party libraries for
    various functionality, such as logging, serving web requests, connecting to databases,
    writing to message queues, and so on. Many of these libraries not only carry out
    parts of critical business functionality but also appear in the performance-sensitive
    areas of our code, impacting the overall performance. It is imperative that we
    choose libraries wisely (with respect to features versus performance trade off)
    after due performance analysis.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: The crucial factor in choosing libraries is not identifying which library to
    use, rather it is having a performance model of our applications and having the
    use cases benchmarked under representative load. Only benchmarks can tell us whether
    the performance is problematic or acceptable. If the performance is below expectation,
    a drill-down profiling can show us whether a third-party library is causing the
    performance issue. In [Chapter 6](ch13.html "Chapter 6. Measuring Performance"),
    *Measuring Performance* and [Chapter 7](ch14.html "Chapter 7. Performance Optimization"),
    *Performance Optimization* we discussed how to measure performance and identify
    bottlenecks. You can evaluate multiple libraries for performance-sensitive use
    cases and choose what suits.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: Libraries often improve (or occasionally lose) performance with new releases,
    so measurement and profiling (comparative, across versions) should be an ongoing
    practice for the development and maintenance lifecycle of our applications. Another
    factor to note is that libraries may show different performance characteristics
    based on the use case, load, and the benchmark. The devil is in the benchmark
    details. Be sure that your benchmarks are as close as possible to the representative
    scenario for your application.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
- en: Making a choice via benchmarks
  id: totrans-1104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's take a brief look at a few general use cases where performance of third-party
    libraries are exposed via benchmarks.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: Web servers
  id: totrans-1106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Web servers are typically subject to quite a bit of performance benchmarking
    due to their generic nature and scope. One such benchmark for Clojure web servers
    exists here:'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ptaoussanis/clojure-web-server-benchmarks](https://github.com/ptaoussanis/clojure-web-server-benchmarks)'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: Web servers are complex pieces of software and they may exhibit different characteristics
    under various conditions. As you will notice, the performance numbers vary based
    on keep-alive versus non-keep-alive modes and request volume—at the time of writing,
    Immutant-2 came out better in keep-alive mode but fared poorly in the non-keep-alive
    benchmark. In production, people often front their application servers with reverse
    proxy servers, for example Nginx or HAProxy, which make keep-alive connections
    to application servers.
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: Web routing libraries
  id: totrans-1110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several web routing libraries for Clojure, as listed here:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/juxt/bidi#comparison-with-other-routing-libraries](https://github.com/juxt/bidi#comparison-with-other-routing-libraries)'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
- en: 'The same document also shows a performance benchmark with **Compojure** as
    the baseline, in which (at the time of writing) Compojure turns out to be performing
    better than **Bidi**. However, another benchmark compares Compojure, **Clout**
    (the library that Compojure internally uses), and **CalfPath** routing here:'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kumarshantanu/calfpath#development](https://github.com/kumarshantanu/calfpath#development)'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: In this benchmark, as of this writing, Clout performs better than Compojure,
    and CalfPath outperforms Clout. However, you should be aware of any caveats in
    the faster libraries.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
- en: Data serialization
  id: totrans-1116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several ways to serialize data in Clojure, for example EDN and Fressian.
    Nippy is another serialization library with benchmarks to demonstrate how well
    it performs over EDN and Fressian:'
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ptaoussanis/nippy#performance](https://github.com/ptaoussanis/nippy#performance)'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: We covered Nippy in [Chapter 2](ch09.html "Chapter 2. Clojure Abstractions"),
    *Clojure Abstractions* to show how it uses transients to speed up its internal
    computations. Even within Nippy, there are several flavors of serialization that
    have different features/performance trade-offs.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: JSON serialization
  id: totrans-1120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Parsing and generating JSON is a very common use case in RESTful services and
    web applications. The Clojure contrib library clojure/data.json ([https://github.com/clojure/data.json](https://github.com/clojure/data.json))
    provides this functionality. However, many people have found out that the Cheshire
    library [https://github.com/dakrone/cheshire](https://github.com/dakrone/cheshire)
    performs much better than the former. The included benchmarks in Cheshire can
    be run using the following command:'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-1122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Cheshire internally uses the Jackson Java library [https://github.com/FasterXML/jackson](https://github.com/FasterXML/jackson),
    which is known for its good performance.
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
- en: JDBC
  id: totrans-1124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'JDBC access is another very common use case among applications using relational
    databases. The Clojure contrib library `clojure/java.jdbc` [https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    provides a Clojure JDBC API. Asphalt [https://github.com/kumarshantanu/asphalt](https://github.com/kumarshantanu/asphalt)
    is an alternative JDBC library where the comparative benchmarks can be run as
    follows:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-1126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: As of this writing, Asphalt outperforms `clojure/java.jdbc` by several micro
    seconds, which may be useful in low-latency applications. However, note that JDBC
    performance is usually dominated by SQL queries/joins, database latency, connection
    pool parameters, and so on. We will discuss more about JDBC in later sections.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  id: totrans-1128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logging is a prevalent activity that almost all non-trivial applications do.
    Logging calls are quite frequent, hence it is important to make sure our logging
    configuration is tuned well for performance. If you are not familiar with logging
    systems (especially on the JVM), you may want to take some time to get familiar
    with those first. We will cover the use of `clojure/tools.logging`, **SLF4J**
    and **LogBack** libraries (as a combination) for logging, and look into how to
    make them perform well:'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: Clojure/tools.logging [https://github.com/clojure/tools.logging](https://github.com/clojure/tools.logging)
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SLF4J: [http://www.slf4j.org/](http://www.slf4j.org/)'
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LogBack: [http://logback.qos.ch/](http://logback.qos.ch/)'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why SLF4J/LogBack?
  id: totrans-1133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides SLF4J/LogBack, there are several logging libraries to choose from in
    the Clojure application, for example Timbre, Log4j and java.util.logging. While
    there is nothing wrong with these libraries, we are often constrained into choosing
    something that covers most other third-party libraries (also including Java libraries)
    in our applications for logging purposes. SLF4J is a Java logger facade that detects
    any available implementation (LogBack, Log4j, and so on) —we choose LogBack simply
    because it performs well and is highly configurable. The library clojure/tools.logging
    provides a Clojure logging API that detects SLF4J, Log4j or java.util.logging
    (in that order) in the classpath and uses whichever implementation is found first.
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: The setup
  id: totrans-1135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's walk through how to set up a logging system for your application using
    LogBack, SLF4J and `clojure/tools.logging` for a project built using Leiningen.
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  id: totrans-1137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Your `project.clj` file should have the LogBack, SLF4J and `clojure/tools.logging`
    dependencies under the `:dependencies` key:'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-1139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: The previously mentioned versions are current and work as of the time of writing.
    You may want to use updated versions, if available.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: The logback configuration file
  id: totrans-1141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You need to create a `logback.xml` file in the `resources` directory:'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-1143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: The previous `logback.xml` file is simple on purpose (for illustration) and
    has just enough configuration to get you started with logging using LogBack.
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-1145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The optimization points are highlighted in the `logback.xml` file we saw earlier
    in this section. We set the `immediateFlush` attribute to `false` such that the
    messages are buffered before flushing to the appender. We also wrapped the regular
    file appender with an asynchronous appender and edited the `queueSize` and `discardingThreshold`
    attributes, which gets us much better results than the default.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: Unless optimized, logging configurations are usually a common source of suboptimal
    performance in many applications. Usually, the performance problems show up only
    at high load when the log volume is very high. The optimizations discussed previously
    are only a few of the many possible optimizations that one can experiment with.
    The chapters in LogBack documentation, such as **encoders** ([http://logback.qos.ch/manual/encoders.html](http://logback.qos.ch/manual/encoders.html)),
    **appenders** ([http://logback.qos.ch/manual/appenders.html](http://logback.qos.ch/manual/appenders.html))
    and **configuration** ([http://logback.qos.ch/manual/configuration.html](http://logback.qos.ch/manual/configuration.html))
    have useful **information**. There are also tips [http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/](http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/)
    on the Internet that may provide useful pointers.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: Data sizing
  id: totrans-1148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cost of abstractions in terms of the data size plays an important role.
    For example, whether or not a data element can fit into a processor cache line
    depends directly upon its size. On a Linux system, we can find out the cache line
    size and other parameters by inspecting the values in the files under the `/sys/devices/system/cpu/cpu0/cache/`
    directory. Refer to [Chapter 4](ch11.html "Chapter 4. Host Performance"), *Host
    Performance*, where we discussed how to compute the size of primitives, objects,
    and data elements.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: 'Another concern we generally find with data sizing is how much data we hold
    at any time in the heap. As we noted in earlier chapters, GC has direct consequences
    on the application performance. While processing data, often we do not really
    need all the data we hold on to. Consider the example of generating a summary
    report of sold items for a certain period (months) of time. After the subperiod
    (month-wise) summary data is computed, we do not need the item details anymore,
    hence it''s better to remove the unwanted data while we add the summaries. See
    the following example:'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-1151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: Had we not used `select-keys` in the previous `summarize` function, it would
    have returned a map with extra :`summary` data along with all other existing keys
    in the map. Now, such a thing is often combined with lazy sequences, so for this
    scheme to work it is important not to hold onto the head of the lazy sequence.
    Recall that in [Chapter 2](ch09.html "Chapter 2. Clojure Abstractions"), *Clojure
    Abstractions* we discussed the perils of holding onto the head of a lazy sequence.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: Reduced serialization
  id: totrans-1153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed in earlier chapters that serialization over an I/O channel is a
    common source of latency. The perils of over-serialization cannot be overstated.
    Whether we read or write data from a data source over an I/O channel, all of that
    data needs to be prepared, encoded, serialized, de-serialized, and parsed before
    being worked upon. The less data that is involved, the better it is for every
    step in order to lower the overhead. Where there is no I/O involved (such as in-process
    communication), it generally makes no sense to serialize.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: A common example of over-serialization is when working with SQL databases. Often,
    there are common SQL query functions that fetch all columns of a table or a relation—they
    are called by various functions that implement business logic. Fetching data that
    we do not need is wasteful and detrimental to performance for the same reason
    that we discussed in the previous paragraph. While it may seem more work to write
    one SQL statement and one database-query function for each use case, it pays off
    with better performance. Code that uses NoSQL databases is also subject to this
    anti-pattern—we have to take care to fetch only what we need even though it may
    lead to additional code.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: There's a pitfall to be aware of when reducing serialization. Often, some information
    needs to be inferred in the absence of serialized data. In such cases, where some
    of the serialization is dropped so that we can infer other information, we must
    compare the cost of inference versus the serialization overhead. The comparison
    may not necessarily be only per operation, but rather on the whole, such that
    we can consider the resources we can allocate in order to achieve capacities for
    various parts of our systems.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: Chunking to reduce memory pressure
  id: totrans-1157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What happens when we slurp a text file regardless of its size? The contents
    of the entire file will sit in the JVM heap. If the file is larger than the JVM
    heap capacity, the JVM will terminate, throwing `OutOfMemoryError`. If the file
    is large, but not enough to force the JVM into OOM error, it leaves relatively
    less JVM heap space for other operations to continue in the application. Similar
    situations take place when we carry out any operation disregarding the JVM heap
    capacity. Fortunately, this can be fixed by reading data in chunks and processing
    them before reading more. In [Chapter 3](ch10.html "Chapter 3. Leaning on Java"),
    *Leaning on Java*, we briefly discussed memory mapped buffers, which is another
    complementary solution that you may like to explore.
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: Sizing for file/network operations
  id: totrans-1159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's take the example of a data ingestion process where a semi-automated job
    uploads large **Comma Separated File (CSV)** files via **File Transfer Protocol
    (FTP)** to a file server, and another automated job (written in Clojure) runs
    periodically to detect the arrival of files via a Network File System (NFS). After
    detecting a new file, the Clojure program processes the file, updates the result
    in a database, and archives the file. The program detects and processes several
    files concurrently. The size of the CSV files is not known in advance, but the
    format is predefined.
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the previous description, one potential problem is, since there could
    be multiple files being processed concurrently, how do we distribute the JVM heap
    among the concurrent file-processing jobs? Another issue at hand could be that
    the operating system imposes a limit on how many files could be open at a time;
    on Unix-like systems you can use the `ulimit` command to extend the limit. We
    cannot arbitrarily slurp the CSV file contents—we must limit each job to a certain
    amount of memory, and also limit the number of jobs that can run concurrently.
    At the same time, we cannot read a very small number of rows from a file at a
    time because this may impact performance:'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-1162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: Fortunately, we can specify the buffer size when reading from a file (or even
    from a network stream) so as to tune the memory usage and performance as appropriate.
    In the previous code example, we explicitly set the buffer size of the reader
    to facilitate the same.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: Sizing for JDBC query results
  id: totrans-1164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Java''s interface standard for SQL databases, JDBC (which is technically not
    an acronym), supports *fetch size* for fetching query results via JDBC drivers.
    The default fetch size depends on the JDBC driver. Most of the JDBC drivers keep
    a low default value to avoid high memory usage and for internal performance optimization
    reasons. A notable exception to this norm is the MySQL JDBC driver that completely
    fetches and stores all rows in memory by default:'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-1166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: When using the Clojure contrib library `java.jdbc` ([https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    as of version 0.3.7), the fetch size can be set while preparing a statement as
    shown in the previous example. Note that the fetch size does not guarantee proportional
    latency; however, it can be used safely for memory sizing. We must test any performance-impacting
    latency changes due to fetch size at different loads and use cases for the particular
    database and JDBC driver. Another important factor to note is that the benefit
    of `:fetch-size` can be useful only if the query result set is consumed incrementally
    and lazily—if a function extracts all rows from a result set to create a vector,
    then the benefit of `:fetch-size` is nullified from a memory conservation point
    of view. Besides fetch size, we can also pass the `:max-rows` argument to limit
    the maximum rows to be returned by a query—however, this implies that the extra
    rows will be truncated from the result, and not whether the database will internally
    limit the number of rows to realize.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: Resource pooling
  id: totrans-1168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several types of resources on the JVM that are rather expensive to
    initialize. Examples are HTTP connections, execution threads, JDBC connections,
    and so on. The Java API recognizes such resources and has built-in support for
    creating a pool of some of those resources, such that the consumer code borrows
    a resource from a pool when required and at the end of the job simply returns
    it to the pool. Java's thread pools (discussed in [Chapter 5](ch12.html "Chapter 5. Concurrency"),
    *Concurrency*) and JDBC data sources are prominent examples. The idea is to preserve
    the initialized objects for reuse. Even though Java does not support pooling of
    a resource type directly, one can always create a pool abstraction around custom
    expensive resources. Note that the pooling technique is common in I/O activities,
    but can be equally applicable to non-I/O purposes where initialization cost is
    high.
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: JDBC resource pooling
  id: totrans-1170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Java supports the obtaining of JDBC connections via the `javax.sql.DataSource`
    interface, which can be pooled. A JDBC connection pool implements this interface.
    Typically, a JDBC connection pool is implemented by third-party libraries or a
    JDBC driver itself. Generally, very few JDBC drivers implement a connection pool,
    so Open Source third-party JDBC resource pooling libraries such as Apache DBCP,
    c3p0, BoneCP, HikariCP, and so on are popular. They also support validation queries
    for eviction of stale connections that might result from network timeouts and
    firewalls, and guard against connection leaks. Apache DBCP and HikariCP are accessible
    from Clojure via their respective Clojure wrapper libraries Clj-DBCP ([https://github.com/kumarshantanu/clj-dbcp](https://github.com/kumarshantanu/clj-dbcp))
    and HikariCP ([https://github.com/tomekw/hikari-cp](https://github.com/tomekw/hikari-cp)),
    and there are Clojure examples describing how to construct C3P0 and BoneCP pools
    ([http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html](http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html)).
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: Connections are not the only JDBC resources that need to be pooled. Every time
    we create a new JDBC prepared statement, depending on the JDBC driver implementation,
    often the entire statement template is sent to the database server in order to
    obtain a reference to the prepared statement. As the database servers are generally
    deployed on separate hardware, there may be network latency involved. Hence, the
    pooling of prepared statements is a very desirable property of JDBC resource pooling
    libraries. Apache DBCP, C3P0, and BoneCP all support statement pooling, and the
    Clj-DBCP wrapper enables the pooling of prepared statements out-of-the-box for
    better performance. HikariCP has the opinion that statement pooling, nowadays,
    is already done internally by JDBC drivers, hence explicit pooling is not required.
    I would strongly advise running your benchmarks with the connection pooling libraries
    to determine whether or not it really works for your JDBC driver and application.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: I/O batching and throttling
  id: totrans-1173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is well known that chatty I/O calls generally lead to poor performance. In
    general, the solution is to batch together several messages and send them in one
    payload. In databases and network calls, batching is a common and useful technique
    to improve throughput. On the other hand, large batch sizes may actually harm
    throughput as they tend to incur memory overhead, and components may not be ready
    to handle a large batch at once. Hence, sizing the batches and throttling are
    just as important as batching. I would strongly advise conducting your own tests
    to determine the optimum batch size under representative load.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: JDBC batch operations
  id: totrans-1175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JDBC has long had batch-update support in its API, which includes the `INSERT`,
    `UPDATE`, `DELETE` statements. The Clojure contrib library `java.jdbc` supports
    JDBC batch operations via its own API, as we can see as follows:'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-1177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Besides batch-update support, we can also batch JDBC queries. One of the common
    techniques is to use the SQL `WHERE` clause to avoid the `N+1` selects issue.
    The `N+1` issue indicates the situation when we execute one query in another child
    table for every row in a rowset from a master table. A similar technique can be
    used to combine several similar queries on the same table into just one, and segregate
    the data in the program afterwards.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example that uses clojure.java.jdbc 0.3.7 and the MySQL
    database:'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-1180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'In the previous example there are two tables: `orders` and `items`. The first
    snippet reads all order IDs from the `orders` table, and then iterates through
    them to query corresponding entries in the `items` table in a loop. This is the
    `N+1` selects performance anti-pattern you should keep an eye on. The second snippet
    avoids `N+1` selects by issuing a single SQL query, but may not perform very well
    unless the column `fk_order_id` is indexed.'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: Batch support at API level
  id: totrans-1182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing any service, it is very useful to provide an API for batch operations.
    This builds flexibility in the API such that batch sizing and throttling can be
    controlled in a fine-grained manner. Not surprisingly, it is also an effective
    recipe for building high-performance services. A common overhead we encounter
    when implementing batch operations is the identification of each item in the batch
    and their correlation across requests and responses. The problem becomes more
    prominent when requests are asynchronous.
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: The solution to the item identification issue is resolved either by assigning
    a canonical or global ID to each item in the request (batch), or by assigning
    every request (batch) a unique ID and each item in the request an ID that is local
    to the batch.
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the exact solution usually depends on the implementation details.
    When requests are synchronous, you can do away with identification of each request
    item (see the Facebook API for reference: [http://developers.facebook.com/docs/reference/api/batch/](http://developers.facebook.com/docs/reference/api/batch/))
    where the items in response follow the same order as in the request. However,
    in asynchronous requests, items may have to be tracked via status-check call or
    callbacks. The desired tracking granularity typically guides the appropriate item
    identification strategy.'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have a batch API for order processing, every order would
    have a unique Order-ID that can be used in subsequent status-check calls. In another
    example, let's say there is a batch API for creating API keys for **Internet of
    Things** (**IoT**) devices—here, the API keys are not known beforehand, but they
    can be generated and returned in a synchronous response. However, if this has
    to be an asynchronous batch API, the service should respond with a batch request
    ID that can be used later to find the status of the request. In a batch response
    for the request ID, the server can include request item IDs (for example device
    IDs, which may be unique for the client but not unique across all clients) with
    their respective status.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
- en: Throttling requests to services
  id: totrans-1187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As every service can handle only a certain capacity, the rate at which we send
    requests to a service is important. The expectations about the service behavior
    are generally in terms of both throughput and latency. This requires us to send
    requests at a specified rate, as a rate lower than that may lead to under-utilization
    of the service, and a higher rate may overload the service or result in failure,
    thus leading to client-side under-utilization.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
- en: Let's say a third-party service can accept 100 requests per second. However,
    we may not know how robustly the service is implemented. Though sometimes it is
    not exactly specified, sending 100 requests at once (within 20ms, let's say) during
    each second may lead to lower throughput than expected. Evenly distributing the
    requests across the one-second duration, for example sending one request every
    10ms (1000ms / 100 = 10ms), may increase the chance of attaining the optimum throughput.
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
- en: For throttling, **Token bucket** ([https://en.wikipedia.org/wiki/Token_bucket](https://en.wikipedia.org/wiki/Token_bucket))
    and **Leaky bucket** ([https://en.wikipedia.org/wiki/Leaky_bucket](https://en.wikipedia.org/wiki/Leaky_bucket))
    algorithms can be useful. Throttling at a very fine-grained level requires that
    we buffer the items so that we can maintain a uniform rate. Buffering consumes
    memory and often requires ordering; queues (covered in [Chapter 5](ch12.html "Chapter 5. Concurrency"),
    *Concurrency*), pipeline and persistent storage usually serve that purpose well.
    Again, buffering and queuing may be subject to back pressure due to system constraints.
    We will discuss pipelines, back pressure and buffering in a later section in this
    chapter.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
- en: Precomputing and caching
  id: totrans-1191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While processing data, we usually come across instances where few common computation
    steps precede several kinds of subsequent steps. That is to say, some amount of
    computation is common and the remaining is different. For high-latency common
    computations (I/O to access the data and memory/CPU to process it), it makes a
    lot of sense to compute them once and store in digest form, such that the subsequent
    steps can simply use the digest data and proceed from that point onward, thus
    resulting in reduced overall latency. This is also known as staging of semi-computed
    data and is a common technique to optimize processing of non-trivial data.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
- en: Clojure has decent support for caching. The built-in `clojure.core/memoize`
    function performs basic caching of computed results with no flexibility in using
    specific caching strategies and pluggable backends. The Clojure contrib library
    `core.memoize` offsets the lack of flexibility in `memoize` by providing several
    configuration options. Interestingly, the features in `core.memoize` are also
    useful as a separate caching library, so the common portion is factored out as
    a Clojure contrib library called `core.cache` on top of which `core.memoize` is
    implemented.
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
- en: As many applications are deployed on multiple servers for availability, scaling
    and maintenance reasons, they need distributed caching that is fast and space
    efficient. The open source memcached project is a popular in-memory, distributed
    key-value/object store that can act as a caching server for web applications.
    It hashes the keys to identify the server to store the value on, and has no out-of-the-box
    replication or persistence. It is used to cache database query results, computation
    results, and so on. For Clojure, there is a memcached client library called SpyGlass
    ([https://github.com/clojurewerkz/spyglass](https://github.com/clojurewerkz/spyglass)).
    Of course, memcached is not limited to just web applications; it can be used for
    other purposes too.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent pipelines
  id: totrans-1195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a situation where we have to carry out jobs at a certain throughput,
    such that each job includes the same sequence of differently sized I/O task (task
    A), a memory-bound task (task B) and, again, an I/O task (task C). A naïve approach
    would be to create a thread pool and run each job off it, but soon we realize
    that this is not optimum because we cannot ascertain the utilization of each I/O
    resource due to unpredictability of the threads being scheduled by the OS. We
    also observe that even though several concurrent jobs have similar I/O tasks,
    we are unable to batch them in our first approach.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
- en: As the next iteration, we split each job in stages (A, B, C), such that each
    stage corresponds to one task. Since the tasks are well known, we create one thread
    pool (of appropriate size) per stage and execute tasks in them. The result of
    task A is required by task B, and B's result is required by task C—we enable this
    communication via queues. Now, we can tune the thread pool size for each stage,
    batch the I/O tasks, and throttle them for an optimum throughput. This kind of
    an arrangement is a concurrent pipeline. Some readers may find this feebly resembling
    the actor model or **Staged Event Driven Architecture** (**SEDA**) model, which
    are more refined models for this kind of approach. Recall that we discussed several
    kinds of in-process queues in [Chapter 5](ch12.html "Chapter 5. Concurrency"),
    *Concurrency*.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
- en: Distributed pipelines
  id: totrans-1198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With this approach, it is possible to scale out the job execution to multiple
    hosts in a cluster using network queues, thereby offloading memory consumption,
    durability, and delivery to the queue infrastructure. For example, in a given
    scenario there could be several nodes in a cluster, all of them running the same
    code and exchanging messages (requests and intermediate result data) via network
    queues.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts how a simple invoice-generation system might
    be connected to network queues:'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed pipelines](img/3642_08_03.jpg)'
  id: totrans-1201
  prefs: []
  type: TYPE_IMG
- en: RabbitMQ, HornetQ, ActiveMQ, Kestrel and Kafka are some well-known Open Source
    queue systems. Once in a while, the jobs may require distributed state and coordination.
    The Avout ([http://avout.io/](http://avout.io/)) project implements the distributed
    version of Clojure's atom and ref, which can be used for this purpose. Tesser
    ([https://github.com/aphyr/tesser](https://github.com/aphyr/tesser)) is another
    library for local and distributed parallelism using Clojure. The Storm ([http://storm-project.net/](http://storm-project.net/))
    and Onyx ([http://www.onyxplatform.org/](http://www.onyxplatform.org/)) projects
    are distributed, real-time stream processing systems implemented using Clojure.
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
- en: Applying back pressure
  id: totrans-1203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed back pressure briefly in the last chapter. Without back pressure
    we cannot build a reasonable load-tolerant system with predictable stability and
    performance. In this section, we will see how to apply back pressure in different
    scenarios in an application. At a fundamental level, we should have a threshold
    of a maximum number of concurrent jobs in the system and, based on that threshold,
    we should reject new requests above a certain arrival rate. The rejected messages
    may either be retried by the client or ignored if there is no control over the
    client. When applying back pressure to user-facing services, it may be useful
    to detect system load and deny auxiliary services first in order to conserve capacity
    and degrade gracefully in the face of high load.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
- en: Thread pool queues
  id: totrans-1205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JVM thread pools are backed by queues, which means that when we submit a job
    into a thread pool that already has the maximum jobs running, the new job lands
    in the queue. The queue is by default an unbounded queue, which is not suitable
    for applying back pressure. So, we have to create the thread pool backed by a
    bounded queue:'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-1207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Now, on this pool, whenever there is an attempt to add more jobs than the capacity
    of the queue, it will throw an exception. The caller should treat the exception
    as a buffer-full condition and wait until the buffer has idle capacity again by
    periodically pooling the `java.util.concurrent.BlockingQueue.remainingCapacity()`
    method.
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
- en: Servlet containers such as Tomcat and Jetty
  id: totrans-1209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the synchronous **Tomcat** and **Jetty** versions, each HTTP request is given
    a dedicated thread from a common thread pool that a user can configure. The number
    of simultaneous requests being served is limited by the thread pool size. A common
    way to control the arrival rate is to set the thread pool size of the server.
    The **Ring** library uses an embedded jetty server by default in development mode.
    The embedded Jetty adapter (in Ring) can be programmatically configured with a
    thread pool size.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
- en: In the asynchronous (Async Servlet 3.0) versions of Tomcat and Jetty beside
    the thread pool size, it is also possible to specify the timeout for processing
    each request. However, note that the thread pool size does not limit the number
    of requests in asynchronous versions in the way it does on synchronous versions.
    The request processing is transferred to an ExecutorService (thread pool), which
    may buffer requests until a thread is available. This buffering behavior is tricky
    because this may cause system overload—you can override the default behavior by
    defining your own thread pool instead of using the servlet container's thread
    pool to return a HTTP error at a certain threshold of waiting requests.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: HTTP Kit
  id: totrans-1212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**HTTP Kit** ([http://http-kit.org/](http://http-kit.org/)) is a high-performance
    asynchronous (based on Java NIO implementation) web server for Clojure. It has
    built-in support for applying back pressure to new requests via a specified queue
    length. As of HTTP Kit 2.1.19, see the following snippet:'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-1214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: In the previous snippet, the worker thread pool size is 32 and the max queue
    length is specified as 600\. When not specified, 20480 is the default maximum
    queue length for applying back pressure.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
- en: Aleph
  id: totrans-1216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aleph ([http://aleph.io/](http://aleph.io/)) is another high-performance asynchronous
    web server based on the Java Netty ([http://netty.io/](http://netty.io/)) library,
    which in turn is based on Java NIO. Aleph extends Netty with its own primitives
    compatible with Netty. The worker thread pool in Aleph is specified via an option,
    as we can see in the following snippet as of Aleph 0.4.0:'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-1218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: Here, `tpool` refers to a bounded thread pool as discussed in the subsection
    *Thread pool queues*. By default, Aleph uses a dynamic thread pool capped at maximum
    512 threads aimed at 90 percent system utilization via the **Dirigiste** ([https://github.com/ztellman/dirigiste](https://github.com/ztellman/dirigiste))
    library.
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
- en: Back pressure not only involves enqueuing a limited number of jobs, but slows
    down the processing rate of a job when the peer is slow. Aleph deals with per-request
    back pressure (for example, when streaming response data) by "not accepting data
    until it runs out of memory" — it falls back to blocking instead of dropping data,
    or raising exceptions and closing connections
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
- en: Performance and queueing theory
  id: totrans-1221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we observe the performance benchmark numbers across a number of runs, even
    though the hardware, loads and OS remain the same, the numbers are rarely exactly
    the same. The difference between each run may be as much as -8 percent to 8 percent
    for no apparent reason. This may seem surprising, but the deep-rooted reason is
    that the performances of computer systems are *stochastic* by nature. There are
    many small factors in a computer system that make performance unpredictable at
    any given point of time. At best, the performance variations can be explained
    by a series of probabilities over random variables.
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise is that each subsystem is more or less like a queue where
    requests await their turn to be served. The CPU has an instruction queue with
    unpredictable fetch/decode/branch-predict timings, the memory access again depends
    on cache hit ratio and whether it needs to be dispatched via the interconnect,
    and the I/O subsystem works using interrupts that may again depend on mechanical
    factors of the I/O device. The OS schedules threads that wait while not executing.
    The software built on the top of all this basically waits in various queues to
    get the job done.
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
- en: Little's law
  id: totrans-1224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Little''s law states that, over steady state, the following holds true:'
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
- en: '![Little''s law](img/3642_08_01.jpg)![Little''s law](img/3642_08_02.jpg)'
  id: totrans-1226
  prefs: []
  type: TYPE_IMG
- en: This is a rather important law that gives us insight into the system capacity
    as it is independent of other factors. For an example, if the average time to
    satisfy a request is 200 ms and the service rate is about 70 per second, then
    the mean number of requests being served is *70 req/second x 0.2 second = 14 requests*.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: Note that Little's law does not talk about spikes in request arrival rate or
    spikes in latency (due to GC and/or other bottlenecks) or system behavior in response
    to these factors. When the arrival rate spikes at one point, your system must
    have enough resources to handle the number of concurrent tasks required to serve
    the requests. We can infer here that Little's law is helpful to measure and tune
    average system behavior over a duration, but we cannot plan capacity based solely
    on this.
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning with respect to Little's law
  id: totrans-1229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to maintain good throughput, we should strive to maintain an upper
    limit on the total number of tasks in the system. Since there can be many kinds
    of tasks in a system and lot of tasks can happily co-exist in the absence of bottlenecks,
    a better way to say it is to ensure that the system utilization and bottlenecks
    remain in limit.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
- en: Often, the arrival rate may not be within the control of a system. For such
    scenarios, the only option is to minimize the latency as much as possible and
    deny new requests after a certain threshold of total jobs in the system. You may
    be able to know the right threshold only through performance and load tests. If
    you can control the arrival rate, you can throttle the arrival (based on performance
    and load tests) so as to maintain a steady flow.
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing an application for performance should be based on the use cases and
    patterns of anticipated system load and behavior. Measuring performance is extremely
    important to guide optimization in the process. Fortunately, there are several
    well-known optimization patterns to tap into, such as resource pooling, data sizing,
    pre-fetch and pre-compute, staging, batching, and so on. As it turns out, application
    performance is not only a function of the use cases and patterns—the system as
    a whole is a continuous stochastic turn of events that can be assessed statistically
    and is guided by probability. Clojure is a fun language to do high-performance
    programming. This book prescribes many pointers and practices for performance,
    but there is no mantra that can solve everything. The devil is in the details.
    Know the idioms and patterns, experiment to see what works for your applications,
    and know which rules you can bend for performance.
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
