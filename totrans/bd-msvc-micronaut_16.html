<html><head></head><body>
		<div id="_idContainer122">
			<h1 id="_idParaDest-171"><em class="italic"><a id="_idTextAnchor172"/>Chapter 10</em>: IoT with Micronaut</h1>
			<p><strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) is one of the fastest emerging technologies. It is a network of devices or things. These devices have the same capabilities as sensors or software and can communicate with other devices over the internet. A device or thing can be from various fields and can include things such as light bulbs, door locks, heartbeat monitors, location sensors, and many devices that can be enabled with sensors. It is an ecosystem of smart devices or things with internet capabilities. IoT is trending in various fields. A few of the top trending fields are as follows:</p>
			<ul>
				<li>Home automation</li>
				<li>Manufacturing and industrial applications</li>
				<li>Healthcare and medical science</li>
				<li>Military and defense</li>
				<li>Automotive, transportation, and logistics</li>
			</ul>
			<p>Along with these fields, in this chapter, we will learn about the following topics:</p>
			<ul>
				<li>Basics of IoT</li>
				<li>Working with Micronaut Alexa skills </li>
			</ul>
			<p>By the end of this chapter, you will be well versed in the preceding aspects concerning IoT with Micronaut integration. </p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>Technical requirements</h1>
			<p>All the commands and technical instructions in this chapter can be run on Windows 10 and macOS. The code examples in this chapter are available in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter10/">https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter10/</a>.</p>
			<p>The following tools need to be installed and set up in the development environment:</p>
			<ul>
				<li><strong class="bold">Java SDK</strong>: Version 13 or above (we used Java 14).</li>
				<li><strong class="bold">Maven</strong>: This is optional and only required if you would like to use Maven as the build system. However, we recommend having Maven set up on any development machine. Instructions to download and install Maven can be found at <a href="https://maven.apache.org/download.cgi">https://maven.apache.org/download.cgi</a>.</li>
				<li><strong class="bold">Development IDE</strong>: Based on your preference, any Java-based IDE can be used, but for the purpose of writing this chapter, IntelliJ was used. </li>
				<li><strong class="bold">Git</strong>: Instructions for downloading and installing Git can be found at <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a>.</li>
				<li><strong class="bold">PostgreSQL</strong>: Instructions for downloading and installing PostgreSQL can be found at <a href="https://www.postgresql.org/download/">https://www.postgresql.org/download/</a>.</li>
				<li><strong class="bold">MongoDB</strong>: MongoDB Atlas provides a free online Database-as-a-Service (DBaaS) with up to 512 MB storage. However, if you would prefer to use a local database, then the instructions for downloading and installing MongoDB can be found at <a href="https://docs.mongodb.com/manual/administration/install-community/">https://docs.mongodb.com/manual/administration/install-community/</a>. We used a local installation to write this chapter.</li>
				<li><strong class="bold">REST client</strong>: Any HTTP REST client can be used. We used the Advanced REST Client Chrome plugin in this chapter.</li>
				<li><strong class="bold">Docker</strong>: Instructions for downloading and installing Docker can be found at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li><strong class="bold">Amazon</strong>: You will need an Amazon account for Alexa, which you can set up at <a href="https://developer.amazon.com/alexa">https://developer.amazon.com/alexa</a>.</li>
			</ul>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/>Basics of IoT </h1>
			<p><strong class="bold">IoT</strong> is a network of <a id="_idIndexMarker742"/>devices or things. These things can be anything – it can be a human wearing a health monitor, a pet wearing a geolocation sensor, a car with a tire pressure sensor, a television with voice/visual capability, or a smart speaker. IoT can<a id="_idIndexMarker743"/> also use advanced <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) capabilities in the cloud to provide next-level services. IoT can<a id="_idIndexMarker744"/> make things smart with data collection and automation. The following diagram illustrates IoT: </p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/Figure_10.1_B16585.jpg" alt="Figure 10.1 – Internet of Things (IoT)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Internet of Things (IoT)</p>
			<p>These devices or things have internet capabilities and are interconnected, so they act as an ecosystem. This ecosystem can collect, send, and act based on data it acquires from other things. For example, you can turn on the lights at your home when you arrive. </p>
			<p>IoT provides <a id="_idIndexMarker745"/>significant benefits to individuals, businesses, and organizations. IoT can reduce manual work and intervention with seamless data transfer between two systems or devices. IoT devices are become more significant every day in the consumer market, be it as locks, doorbells, light bulbs, speakers, televisions, healthcare products, or fitness systems. IoT is mainly accessed now in voice enabled ecosystems such as Google Home, Apple Siri, Amazon Alexa, Microsoft Cortana, Samsung Bixby, and more. There are numerous positive aspects of IoT; however, there are a few cons regarding security and privacy issues.</p>
			<p>Now that we have learned about the basics of IoT and its applications, let's understand the basics of Alexa skills. </p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Working on the basics of Alexa skills </h1>
			<p>Alexa is a cloud-based <a id="_idIndexMarker746"/>voice recognition service available on millions of devices from Amazon and third-party device manufacturers, such as televisions, Bluetooth speakers, headphones, automobiles, and so on. You can build interactive voice-based request-response applications using Alexa. </p>
			<p>Alexa can be integrated into various <a id="_idIndexMarker747"/>applications. Alexa also has screen capabilities for displaying responses visually, and Echo Show is an Alexa speaker with a display screen. The following diagram illustrates the Amazon Alexa architecture: </p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/Figure_10.2_B16585.jpg" alt="Figure 10.2 – Amazon Alexa architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Amazon Alexa architecture</p>
			<p>Users can say the wake-up word for the device, which is <strong class="bold">Alexa</strong>, and perform an operation. For example, to find the weather in your current location, you can say <em class="italic">Alexa, what is the current weather?</em> and you will receive a response, such as <em class="italic">The current weather in your location is 28 degrees</em>. Alexa skills are like apps, and you can enable or disable skills using the Alexa app for a specific device. Skills are voice-based Alexa capabilities. </p>
			<p>Alexa can do the <a id="_idIndexMarker748"/>following:</p>
			<ul>
				<li>Set an alarm.</li>
				<li>Play music from Spotify, Apple Music, or Google Music.</li>
				<li>Create a to-do list and add items to your shopping list. </li>
				<li>Check the weather.</li>
				<li>Check your calendar.</li>
				<li>Read news briefings. </li>
				<li>Check bank accounts. </li>
				<li>Order in a restaurant.</li>
				<li>Check facts on the internet. </li>
			</ul>
			<p>These are a few of the many things that Alexa can perform. Now, let's move on and understand more about Alexa.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor176"/>Basics of Alexa skills</h2>
			<p>Users with any <a id="_idIndexMarker749"/>voice-based assistants or tools can use the wake-up word to open the skill or application. For example, with Google Home, we use <em class="italic">Hey Google</em> or <em class="italic">OK Google</em>, for Apple Siri, we use <em class="italic">Hey Siri</em> or <em class="italic">Siri</em>, and for Amazon Alexa, we use <em class="italic">Alexa</em>. This wake-up word can be replaced with <em class="italic">Amazon</em>, <em class="italic">Echo</em>, or <em class="italic">computer</em>. All Alexa skills have been designed based on the voice interaction model; that is, phrases you can say to make the skill do something you want, such as <em class="italic">Alexa, turn on the lights</em> or <em class="italic">Alexa, what is the current temperature?</em> </p>
			<p>Alexa supports the following two<a id="_idIndexMarker750"/> types of voice interaction models:</p>
			<ul>
				<li><strong class="bold">Pre-built voice interaction model</strong>: Alexa defines the <a id="_idIndexMarker751"/>phrases for each skill for you.</li>
				<li><strong class="bold">Custom voice interaction model</strong>: You define the phrases that the user can say to interact with <a id="_idIndexMarker752"/>your skills.</li>
			</ul>
			<p>For our working <a id="_idIndexMarker753"/>example code, we will use the custom voice interaction model. The following diagram illustrates the process of opening a skill using a custom voice interaction model:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/Figure_10.3_B16585.jpg" alt="Figure 10.3 – Opening a skill&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Opening a skill</p>
			<p>Now that we know about the wake-up word, the phrase following it is the <strong class="bold">launch</strong> word, followed by the <strong class="bold">invocation name</strong>. For our sample application, <strong class="bold">Pet Clinic</strong>, the launch word will be <strong class="bold">open</strong>, followed by the invocation <strong class="bold">Pet Clinic</strong>. </p>
			<p>The following diagram illustrates the relationship between utterances and intent:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Figure_10.4_B16585.jpg" alt="Figure 10.4 – Opening a skill&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Opening a skill</p>
			<p>An utterance is a <a id="_idIndexMarker754"/>word users say to Alexa to convey what they want to do such as <em class="italic">Turn on the lights</em>, <em class="italic">What is the current temperature?</em>, and so on. Users can say the same thing in different ways, such as <em class="italic">find the temperature</em>, <em class="italic">current temperature</em>, <em class="italic">outside temperature</em>, <em class="italic">the temperature in [location]</em>, and Alexa will provide pre-build utterances and associated requests as part of the custom voice interaction model. This list of utterances can be mapped to a request or intent. </p>
			<p>The following diagram illustrates the custom voice interaction model with a <strong class="bold">wake word</strong>, <strong class="bold">launch</strong>, <strong class="bold">invocation name</strong>, <strong class="bold">utterance</strong>, and <strong class="bold">intent</strong>:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/Figure_10.5_B16585.jpg" alt="Figure 10.5 – Opening a skill – Pet Clinic&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Opening a skill – Pet Clinic</p>
			<p>For our code example, we <a id="_idIndexMarker755"/> will use the sequences <em class="italic">Alexa, Open Pet Clinic</em>, and <em class="italic">Alexa, find nearby Pet Clinics</em>. Here, the wake-up word is <strong class="bold">Alexa</strong>, the launch word is <strong class="bold">Open</strong>, and the invocation name is <strong class="bold">Pet Clinic</strong>. The utterance can be <strong class="bold">find the nearest pet clinic</strong>. We can also have other variations of utterances, such as <strong class="bold">find pet clinic</strong>. All these utterances can be mapped to <strong class="bold">GetFactByPetClinicIntent</strong>. We will learn about intents in the next section of this chapter. </p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor177"/>Basics of intents</h2>
			<p>One of the fundamental<a id="_idIndexMarker756"/> designs for voice in Alexa is intents. Intents capture events the end user wants to do with voice. Intents represent an action that is triggered by the user's spoken request. Intents in Alexa are specified in a JSON structure called an <strong class="bold">intent schema</strong>. The <a id="_idIndexMarker757"/>built-in intents include <strong class="bold">Cancel</strong>, <strong class="bold">Help</strong>, <strong class="bold">Stop</strong>, <strong class="bold">Navigate Home</strong>, and <strong class="bold">Fallback</strong>. Some intents are basic, such as help, and the skills should have a Help Intent. </p>
			<p>The following diagram illustrates the built-in intents in the Alexa developer console: </p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Figure_10.6_B16585.jpg" alt="Figure 10.6 – Built-in intents &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Built-in intents </p>
			<p>If we have a website for logging in that has <a id="_idIndexMarker758"/>username and password fields and a submit button, there will be a submit intent in the Alexa skill world. However, one big difference is that users can say the submit in different ways; for example, <em class="italic">Submit</em>, <em class="italic">submit it</em>, <em class="italic">confirm</em>, <em class="italic">ok</em>, <em class="italic">get</em>, <em class="italic">continue</em>, and so on. These different ways of saying the same thing are<a id="_idIndexMarker759"/> called <strong class="bold">utterances</strong>. Each intent should include a list of utterances; that is, all the things a user might say to invoke these intents. Intents can <a id="_idIndexMarker760"/>have arguments called <strong class="bold">slots</strong>, which will not be discussed in this chapter. </p>
			<p>Now that we have learned about the basics of Alexa skills by covering utterances, intents, and built-in intents, let's create our first functional Alexa skill. </p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>Your first HelloWorld Alexa skill </h2>
			<p>To start creating our <a id="_idIndexMarker761"/>Alexa skill, we must navigate to <a href="https://developer.amazon.com/">https://developer.amazon.com/</a>, select <strong class="bold">Amazon Alexa</strong>, and click <strong class="bold">Create Alexa Skills</strong>. This will open the Alexa developer console. If you don't have an Amazon developer account, you can create one for free. The following screenshot illustrates the <strong class="bold">Create Skill</strong> screen:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/Figure_10.7_B16585.jpg" alt="Figure 10.7 – Create Skill screen &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Create Skill screen </p>
			<p>In the preceding screenshot, you can see how to create a new skill name called <strong class="source-inline">Pet Clinic</strong>, choose a model to add to your skill option called <strong class="source-inline">Custom</strong>, and choose a method to host your skill's backend resources called <strong class="source-inline">Provision your own</strong>. Choose a template to add to your skill called <strong class="source-inline">Start from Scratch</strong>. </p>
			<p>By using the custom<a id="_idIndexMarker762"/> voice interaction model, we have learned that we need to create and configure our wake word, launch, invocation name, utterances, and intent. The wake word is configured for the device and is the same for all the skills, so we don't need to change it. In our configuration, we will configure the code launch, invocation, utterances, and intent. The following diagram illustrates the basics of developing Alexa skills:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_10.8_B16585.jpg" alt="Figure 10.8 – Alexa skills &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Alexa skills </p>
			<p>Alexa skills are<a id="_idIndexMarker763"/> based on the <strong class="bold">voice interaction model</strong> and <strong class="bold">programming logic</strong>. Programming logic <a id="_idIndexMarker764"/>can be created using Node.js, Java, Python, C#, or Go. This programming logic allows us <a id="_idIndexMarker765"/>to connect to web services, microservices, APIs, and interfaces. With this, you can invoke an internet-accessible endpoint for Alexa skills. The following diagram illustrates the skills developer console:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/Figure_10.9_B16585.jpg" alt="Figure 10.9 – Alexa skills &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Alexa skills </p>
			<p>We can set <strong class="bold">Skill Invocation Name</strong> to <strong class="source-inline">pet clinic</strong> and save it. You can also select the <strong class="source-inline">HelloWorldIntent</strong> intent and rename it <strong class="source-inline">PetClinicWelcomeIntent</strong>. There will be sample utterances <a id="_idIndexMarker766"/>listed in the intent that you can modify manually or use the JSON Editor and copy the <strong class="source-inline">alexa_petclinic_intent_schema.json</strong> code from this book's GitHub repository. The following code illustrates the JSON schema for the intent:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "interactionModel": {</p>
			<p class="source-code">    "languageModel": {</p>
			<p class="source-code">      "invocationName": "pet clinic",</p>
			<p class="source-code">      "intents": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "name": "AMAZON.CancelIntent",</p>
			<p class="source-code">          "samples": [</p>
			<p class="source-code">            "cancel"</p>
			<p class="source-code">          ]</p>
			<p class="source-code">        ………………………</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "name": "PetClinicWelcomeIntent",</p>
			<p class="source-code">          "slots": [],</p>
			<p class="source-code">          "samples": [</p>
			<p class="source-code">            "find near by pet clinics",</p>
			<p class="source-code">            "find pet clinics"</p>
			<p class="source-code">          ]</p>
			<p class="source-code">        }</p>
			<p class="source-code">      ],</p>
			<p class="source-code">      "types": []</p>
			<p class="source-code">    }</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>You can configure the<a id="_idIndexMarker767"/> intent's invocation name and sample utterance using the JSON configuration file. </p>
			<p>Once you have copied the JSON file to the Alexa developer console's JSON editor, click <strong class="bold">Save Model</strong> and then <strong class="bold">Build Model and Evaluate Model</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding configuration is a sample from this chapter's folder on GitHub. The actual schema can be copied from GitHub.</p>
			<p>Once you have built the model, click <strong class="bold">Test</strong> in the Alexa developer console and enable the skill testing process. Now, we<a id="_idIndexMarker768"/> need to develop our backend code for the response. </p>
			<p>Create a Maven Java project using your favorite IDE. The following dependencies are required for this:</p>
			<p class="source-code">&lt;dependencies&gt;</p>
			<p class="source-code">    &lt;dependency&gt;</p>
			<p class="source-code">        &lt;groupId&gt;com.amazon.alexa&lt;/groupId&gt;</p>
			<p class="source-code">        &lt;artifactId&gt;ask-sdk&lt;/artifactId&gt;</p>
			<p class="source-code">        &lt;version&gt;2.20.2&lt;/version&gt;</p>
			<p class="source-code">    &lt;/dependency&gt;</p>
			<p class="source-code">&lt;/dependencies&gt;</p>
			<p>We will be using Amazon's <strong class="source-inline">ask-sdk</strong> for our backend Java development. You can also configure the dependencies using Gradle. A sample Gradle configuration can be seen in the following code:</p>
			<p class="source-code">dependencies {</p>
			<p class="source-code">    compile 'com.amazon.alexa:alexa-skills-kit:1.1.2'</p>
			<p class="source-code">}</p>
			<p>We need to create a Java class for all the intents. In our JSON schema, we have defined the <strong class="source-inline">CancelIntent</strong>, <strong class="source-inline">HelpIntent</strong>, <strong class="source-inline">StopIntent</strong>, <strong class="source-inline">NavigateHomeIntent</strong>, <strong class="source-inline">FallbackIntent</strong>, and <strong class="source-inline">PetClinicWelcomeIntent</strong> intents. For every intent, we need to create a handler; for example, <strong class="source-inline">PetClinicWelcomeIntent</strong> should have <strong class="source-inline">PetClicWelcomeIntentHandler</strong>. The handler's name will be added to the end of each intent name. We must also create one additional handler that hasn't been configured in the JSON schema, and this is called <strong class="source-inline">LaunchRequestHandler</strong>. This is the first intent that is triggered whenever their skill is launched. The following code illustrates <strong class="source-inline">LaunchRequestHandler</strong>:</p>
			<p class="source-code">public class LaunchRequestHandler implements RequestHandler {</p>
			<p class="source-code">    @Override</p>
			<p class="source-code">    public boolean canHandle(HandlerInput handlerInput) {</p>
			<p class="source-code">        return handlerInput.matches</p>
			<p class="source-code">        (requestType(LaunchRequest.class));</p>
			<p class="source-code">    }</p>
			<p class="source-code">    @Override</p>
			<p class="source-code">    public Optional&lt;Response&gt; handle(HandlerInput </p>
			<p class="source-code">     handlerInput) {</p>
			<p class="source-code">        String speechText = "Welcome to Pet Clinic, You can </p>
			<p class="source-code">         say find near by Pet Clinics";</p>
			<p class="source-code">        return handlerInput.getResponseBuilder()</p>
			<p class="source-code">                .withSpeech(speechText)</p>
			<p class="source-code">                .withSimpleCard("PetClinic", speechText)</p>
			<p class="source-code">                .withReprompt(speechText)</p>
			<p class="source-code">                .build();</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">LaunchRequestHandler</strong> will <a id="_idIndexMarker769"/>override the handler method and the response voice message when the skill is launched. This is defined in the code block. In the code, we have a speech text response of <em class="italic">Welcome to Pet Clinic, you can say find near by Pet Clinics</em>, along with the title of <strong class="source-inline">PetClinic</strong>.</p>
			<p>Now that we have created the handlers (<strong class="source-inline">CancelandStopIntentHandler</strong>, <strong class="source-inline">HelpIntentHandler</strong>, <strong class="source-inline">LaunchRequestHandler</strong>, <strong class="source-inline">PetClinicWelcomeIntentHandler</strong>, and <strong class="source-inline">SessionEndedRequestHandler</strong>), we need to create <strong class="source-inline">StreamHandler</strong>. <strong class="source-inline">StreamHandler</strong> is the entry point for the AWS Lambda function. All requests that are sent by the end user to Alexa, which invokes your skill, will pass through this class. You need to configure the copy of the skill ID from the Amazon Alexa developer<a id="_idIndexMarker770"/> console in the endpoint. Refer to the following code:</p>
			<p class="source-code">public class PetClinicStreamHandler extends SkillStreamHandler {</p>
			<p class="source-code">    private static Skill getSkill() {</p>
			<p class="source-code">        return Skills.standard()</p>
			<p class="source-code">                .addRequestHandlers(</p>
			<p class="source-code">                        new CancelandStopIntentHandler(),</p>
			<p class="source-code">                        new </p>
			<p class="source-code">                         PetClinicWelcomeIntentHandler(),</p>
			<p class="source-code">                        new HelpIntentHandler(),</p>
			<p class="source-code">                        new LaunchRequestHandler(),</p>
			<p class="source-code">                        new SessionEndedRequestHandler())</p>
			<p class="source-code">                .withSkillId("amzn1.ask.skill.de392a0b-</p>
			<p class="source-code">                 0a95-451a-a615-dcba1f9a42c6")</p>
			<p class="source-code">                .build();</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public PetClinicStreamHandler() {</p>
			<p class="source-code">        super(getSkill());</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>With that, we have learned about how to use stream handlers and how to invoke intent handlers. Let's learn about the use of the skill ID, which is where you can get information about the skill ID. The<a id="_idIndexMarker771"/> following screenshot illustrates the skill ID's location in the developer console: </p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/Figure_10.10_B16585.jpg" alt="Figure 10.10 – Endpoint skill ID and default region &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Endpoint skill ID and default region </p>
			<p>The skill ID can be found on the Alexa developer console's <strong class="bold">Endpoint</strong> screen. The next step is to create the <strong class="source-inline">.jar</strong> file with dependencies for the code. You can execute the <strong class="source-inline">mvn assembly:assembly -DdescriptorId=jar-with-dependencies package</strong> command to create the <strong class="source-inline">.jar</strong> file. This <strong class="source-inline">.jar</strong> file will be located in the target directory, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/Figure_10.11_B16585.jpg" alt="Figure 10.11 – Maven JAR file location &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Maven JAR file location </p>
			<p>The next step is to create the <a id="_idIndexMarker772"/>Amazon Lambda function, which is our backend service code. </p>
			<p>Navigate to <a href="https://console.aws.amazon.com/lambda/">https://console.aws.amazon.com/lambda/</a> to create a function. Name the function <strong class="source-inline">lambda_for_petclinic</strong>, set options to <strong class="source-inline">Author from scratch</strong>, and set runtime to <strong class="source-inline">Java 11</strong>. The user interface is illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/Figure_10.12_B16585_edited.jpg" alt="Figure 10.12 – Alexa Create function screen&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Alexa Create function screen</p>
			<p>The next step is to <a id="_idIndexMarker773"/>create the trigger with <strong class="bold">Trigger configuration</strong> set to <strong class="bold">Alexa Skills Kit</strong>, as shown in the following screenshot. You need to copy the Alexa <strong class="bold">Skill Id</strong> from the Alexa developer console's <strong class="bold">Endpoint</strong> screen. Also, you need to copy the <strong class="bold">Function ARN</strong> (<strong class="bold">Amazon Resource Name</strong>) property from the Lambda developer console to the <a id="_idIndexMarker774"/>Alexa skill developer console's <strong class="bold">Endpoint</strong> screen. The following screenshot illustrates the location of the AWS Lambda function's ARN:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/Figure10.13_B16585.jpg" alt="Figure 10.13 – AWS Function ARN\&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – AWS Function ARN\</p>
			<p>The <strong class="bold">Function ARN</strong> value must be copied from <a id="_idIndexMarker775"/>the Alexa developer console <strong class="bold">Endpoint</strong> screen to the <strong class="bold">Default Region</strong> section or to the location-specific regions, as shown in <em class="italic">Figure 10.10</em>. The skill ID shown in <em class="italic">Figure 10.10</em> should be copied to the AWS Lambda trigger screen, as illustrated here: </p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Figure10.14_B16585.jpg" alt="Figure 10.14 – AWS Lambda – Add trigger&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – AWS Lambda – Add trigger</p>
			<p>Once the trigger has been<a id="_idIndexMarker776"/> added with the necessary skill ID and the <strong class="bold">Function ARN</strong> ID has been copied to the skill endpoint, click <strong class="bold">Save Endpoints</strong>. The next step is to upload the <strong class="source-inline">.jar</strong> file, along with any dependencies (<strong class="source-inline">petclinic-Alexa-maven-1.0-SNAPSHOT-jar-with-dependencies.jar</strong>), to the Lambda function. </p>
			<p>The following screenshot illustrates the process of uploading the <strong class="source-inline">.jar</strong> file to the Amazon Lambda function:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/Figure10.15_B16585.jpg" alt="Figure 10.15 –Uploading code to Amazon Lambda&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.15 –Uploading code to Amazon Lambda</p>
			<p>Now that we have<a id="_idIndexMarker777"/> created our first Alexa skill and uploaded the necessary code, let's test it out. </p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor179"/>Testing your code </h2>
			<p>The last and final process is to test the <a id="_idIndexMarker778"/>code with the Amazon Alexa Simulator, which is located in the developer console. The following screenshot illustrates how to request the Alexa Simulator:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/Figure10.16_B16585.jpg" alt="Figure 10.16 – Alexa Simulator request/response &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.16 – Alexa Simulator request/response </p>
			<p>The request/response testing screen accepts text or speech. You can type or say <strong class="source-inline">open pet clinic</strong> and <strong class="source-inline">find nearby pet clinics</strong> here. You should be able to see the response from the Java code in the <strong class="bold">JSON Output 1</strong> section. Once you can see the response, this means we have successfully created our <a id="_idIndexMarker779"/>first basic IoT pet clinic example with a request and a response. </p>
			<p>We will integrate Micronaut and Alexa in the next section. You can find the complete working example for the <strong class="source-inline">petclinic-alexa-maven</strong> project in this book's GitHub repository.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor180"/>Integrating Micronaut with Alexa </h1>
			<p>As we discussed in the<a id="_idIndexMarker780"/> preceding section, in this section, we will <a id="_idIndexMarker781"/>start to understand how to integrate Micronaut with Alexa. Micronaut<a id="_idIndexMarker782"/> provides various extensions that support <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>). The primary focus here will be on AWS Lambda. The <strong class="source-inline">micronaut-function-aws-alexa</strong> module includes support for building Alexa skills with Micronaut. Micronaut Alexa support can wire up your Alexa <a id="_idIndexMarker783"/>application with <strong class="bold">AlexaFunction</strong> and supports dependency injection for the following types: </p>
			<ul>
				<li><strong class="source-inline">com.amazon.ask.dispatcher.request.handler.RequestHandler</strong></li>
				<li><strong class="source-inline">com.amazon.ask.dispatcher.request.interceptor.RequestInterceptor</strong></li>
				<li><strong class="source-inline">com.amazon.ask.dispatcher.exception.ExceptionHandler</strong></li>
				<li><strong class="source-inline">com.amazon.ask.builder.SkillBuilder</strong></li>
			</ul>
			<p>Micronaut's <strong class="source-inline">aws-alexa</strong> module<a id="_idIndexMarker784"/> simplifies how we can develop Alexa skills with Java, Kotlin, or Groovy. The following code is the Java Maven dependency<a id="_idIndexMarker785"/> for the <strong class="source-inline">aws-alexa</strong> module:</p>
			<p class="source-code">&lt;dependency&gt;</p>
			<p class="source-code">    &lt;groupId&gt;io.micronaut.aws&lt;/groupId&gt;</p>
			<p class="source-code">    &lt;artifactId&gt;micronaut-aws-alexa&lt;/artifactId&gt;</p>
			<p class="source-code">&lt;/dependency&gt;</p>
			<p>As we learned in the previous chapters, Micronaut uses Java annotations. To change any Alexa Java handler so that it can work with Micronaut, all we need to do is add the necessary <strong class="source-inline">@Singleton</strong> annotation; that is, <strong class="source-inline">javax.inject.Singleton</strong>. A sample <strong class="source-inline">LaunchRequestHandler</strong> with the <strong class="source-inline">Singleton</strong> annotation is as follows:</p>
			<p class="source-code">@Singleton</p>
			<p class="source-code">public class LaunchRequestHandler implements RequestHandler {</p>
			<p class="source-code">    @Override</p>
			<p class="source-code">    public boolean canHandle(HandlerInput handlerInput) {</p>
			<p class="source-code">        return handlerInput.matches</p>
			<p class="source-code">          (requestType(LaunchRequest.class));</p>
			<p class="source-code">    }</p>
			<p class="source-code">    @Override</p>
			<p class="source-code">    public Optional&lt;Response&gt; handle(HandlerInput </p>
			<p class="source-code">     handlerInput) {</p>
			<p class="source-code">        String speechText = "Welcome to Pet Clinic, You can </p>
			<p class="source-code">         say find near by Pet Clinics";</p>
			<p class="source-code">        return handlerInput.getResponseBuilder()</p>
			<p class="source-code">                .withSpeech(speechText)</p>
			<p class="source-code">                .withSimpleCard("PetClinic", speechText)</p>
			<p class="source-code">                .withReprompt(speechText)</p>
			<p class="source-code">                .build();</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>With the help of Micronaut, you can<a id="_idIndexMarker786"/> perform unit testing for your intents easily. This is<a id="_idIndexMarker787"/> because the <strong class="source-inline">@MicronautTest</strong> annotation provides seamless unit testing capabilities. Here, we can inject the handler into the unit test cases. The Micronaut framework leverages the Amazon <strong class="source-inline">LaunchRequest</strong> class to do the following:</p>
			<p class="source-code">@MicronautTest</p>
			<p class="source-code">public class LaunchRequestIntentHandlerTest {</p>
			<p class="source-code">    @Inject</p>
			<p class="source-code">    LaunchRequestHandler handler;</p>
			<p class="source-code">    @Test</p>
			<p class="source-code">    void testLaunchRequestIntentHandler() {</p>
			<p class="source-code">        LaunchRequest request = </p>
			<p class="source-code">         LaunchRequest.builder().build();</p>
			<p class="source-code">        HandlerInput input = HandlerInput.builder()</p>
			<p class="source-code">                .withRequestEnvelope</p>
			<p class="source-code">                     (RequestEnvelope.builder()</p>
			<p class="source-code">                        .withRequest(request)</p>
			<p class="source-code">                        .build()</p>
			<p class="source-code">                ).build();</p>
			<p class="source-code">        assertTrue(handler.canHandle(input));</p>
			<p class="source-code">        Optional&lt;Response&gt; responseOptional = </p>
			<p class="source-code">          handler.handle(input);</p>
			<p class="source-code">        assertTrue(responseOptional.isPresent());</p>
			<p class="source-code">        Response = responseOptional.get();</p>
			<p class="source-code">        assertTrue(response.getOutputSpeech() instanceof </p>
			<p class="source-code">         SsmlOutputSpeech);</p>
			<p class="source-code">        String speechText = "Welcome to Pet Clinic, You can </p>
			<p class="source-code">         say find near by Pet Clinics";</p>
			<p class="source-code">        String expectedSsml = "&lt;speak&gt;" + speechText + </p>
			<p class="source-code">          "&lt;/speak&gt;";</p>
			<p class="source-code">        assertEquals(expectedSsml, ((SsmlOutputSpeech) </p>
			<p class="source-code">         response.getOutputSpeech()).getSsml());</p>
			<p class="source-code">        assertNotNull(response.getReprompt());</p>
			<p class="source-code">        assertNotNull(response.getReprompt()</p>
			<p class="source-code">          .getOutputSpeech());</p>
			<p class="source-code">        assertTrue(response.getReprompt().getOutputSpeech() </p>
			<p class="source-code">         instanceof SsmlOutputSpeech);</p>
			<p class="source-code">        assertEquals(expectedSsml,((SsmlOutputSpeech) </p>
			<p class="source-code">          response.getReprompt().getOutputSpeech())</p>
			<p class="source-code">          .getSsml());</p>
			<p class="source-code">        assertTrue(response.getCard() instanceof </p>
			<p class="source-code">          SimpleCard);</p>
			<p class="source-code">        assertEquals("PetClinic", ((SimpleCard) </p>
			<p class="source-code">          response.getCard()).getTitle());</p>
			<p class="source-code">        assertEquals(speechText, ((SimpleCard) </p>
			<p class="source-code">         response.getCard()).getContent());</p>
			<p class="source-code">        assertFalse(response.getShouldEndSession());</p>
			<p class="source-code">    }</p>
			<p>You can find the complete <a id="_idIndexMarker788"/>working example for the <strong class="source-inline">petclinic-alexa-micronaut-maven</strong> project in this book's GitHub repository. You can connect to <a id="_idIndexMarker789"/>a web service or to a backend database in the handler to send a request and receive a response. The following diagram illustrates the design for Alexa skill integration with the backend: </p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/Figure_10.17_B16585.jpg" alt="Figure 10.17 – Alexa skills with a custom backend&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – Alexa skills with a custom backend</p>
			<p>Your <strong class="bold">backend</strong> logic can be<a id="_idIndexMarker790"/> embedded in the intent handler. For <a id="_idIndexMarker791"/>example, to find<a id="_idIndexMarker792"/> the pet clinic's location, you can invoke a web service and add it as a response to <strong class="source-inline">speechText</strong>, as shown in the following code snippet:</p>
			<p class="source-code">    @Override</p>
			<p class="source-code">    public Optional&lt;Response&gt; handle(HandlerInput </p>
			<p class="source-code">       handlerInput) {</p>
			<p class="source-code">        speechText =  service.getPetClinicLocation();</p>
			<p class="source-code">        return handlerInput.getResponseBuilder()</p>
			<p class="source-code">                .withSpeech(speechText)</p>
			<p class="source-code">                .withSimpleCard("PetClinic", speechText)</p>
			<p class="source-code">                .withReprompt(speechText)</p>
			<p class="source-code">                .build();</p>
			<p class="source-code">    }</p>
			<p>The <strong class="source-inline">speechText</strong> handle method can be added from a microservice call and can retrieve information from a <a id="_idIndexMarker793"/>database or service. </p>
			<p>Now that we have integrated<a id="_idIndexMarker794"/> Micronaut with Alexa, we can control IoT devices with the Voice and Micronaut microservices. </p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor181"/>Summary</h1>
			<p>In this chapter, we explored how to use the basics of IoT and Amazon Alexa. Then, we dived into creating a Micronaut microservice and integrating it with Amazon Alexa. With this integration, we can control IoT devices with the Voice and Micronaut microservices. </p>
			<p>This chapter enhanced your Micronaut microservices journey in IoT. It has equipped you with first-hand knowledge of IoT and Amazon Alexa. Micronaut also supports <strong class="bold">Speech Synthesis Markup Language</strong> (<strong class="bold">SSML</strong>) and Flash Briefings. </p>
			<p>In the next chapter, we will bring all the topics that we have learned about together and take things to the next level by architecting enterprise microservices, looking at OpenAPI, scaling Micronaut, and deep diving into building enterprise-grade microservices.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor182"/>Questions</h1>
			<ol>
				<li>What is IoT?</li>
				<li>Name a few devices that are IoT devices.</li>
				<li>What is an Alexa skill?</li>
				<li>What are Alexa intents?</li>
				<li>Which programming languages does Alexa support? </li>
				<li>What is the default launch handler class's name?  </li>
				<li>What is the one change you will need to make to your annotate handlers so that they're compatible with Micronaut?</li>
			</ol>
		</div>
	</body></html>