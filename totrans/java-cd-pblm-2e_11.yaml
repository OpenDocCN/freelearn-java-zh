- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Concurrency ‒ Virtual Threads and Structured Concurrency: Diving Deeper'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes 18 problems meant to dive deep into how *virtual threads*
    and *structured concurrency* work and how they should be used in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have a background in concurrency in Java then I strongly recommend
    postponing this chapter until you read some good introductory coverage on this
    topic. For instance, you could try out *Chapter 10* and *Chapter 11* from *Java
    Coding Problems*, *First Edition*.
  prefs: []
  type: TYPE_NORMAL
- en: We start this chapter by explaining how virtual threads work internally. This
    will be helpful to help you better understand the subsequent problems about extending
    and assembling `StructuredTaskScope`, hooking `ThreadLocal` and virtual threads,
    avoiding *pinning*, solving *producer-consumer* problems, implementing an HTTP
    web server, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll have comprehensive and crystal-clear knowledge
    about working with *virtual threads* and *structured concurrency*.
  prefs: []
  type: TYPE_NORMAL
- en: Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use the following problems to test your advanced programming prowess in virtual
    threads and structured concurrency in Java. I strongly encourage you to give each
    problem a try before you turn to the solutions and download the example programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tackling continuations**: Provide a detailed explanation of what *continuations*
    are and how they work in the context of virtual threads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tracing virtual thread states and transitions**: Build a meaningful diagram
    of virtual thread states and transitions and explain it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Extending StructuredTaskScope**: Explain and demonstrate the steps for extending
    the `StructuredTaskScope`. Explain why we cannot extend `ShutdownOnSuccess` and
    `ShutdownOnFailure`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assembling StructuredTaskScope**: Write a Java application that assembles
    (nests) multiple `StructuredTaskScope` instances.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assembling StructuredTaskScope with timeout**: Modify the application developed
    in *Problem 228* to add a timeout/deadline to the forked tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hooking ThreadLocal and virtual threads**: Demonstrate the use of `ThreadLocal`
    and virtual threads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hooking ScopedValue and virtual threads**: Provide a comprehensive introduction
    with examples of the `ScopedValue` API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Using ScopedValue and executor services**: Write a snippet of code that emphasizes
    the usage of the `ScopedValue` API in the context of executor services.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chaining and rebinding scoped values**: Provide a few snippets of code that
    show how scoped values can be chained and rebound.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Using ScopedValue and StructuredTaskScope**: Write a Java application that
    highlights the usage of `ScopedValue` and `StructuredTaskScope`. Explain in your
    code where every `ScopedValue` is bound and not bound.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Using Semaphore instead of Executor**: In the context of virtual threads,
    explain the benefits and exemplify the usage of `Semaphore` instead of an executor
    (for instance, instead of `newFixedThreadPool()`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Avoiding pinning via locking**: Explain and exemplify how we can avoid pinned
    virtual threads by refactoring `synchronized` code via `ReentrantLock`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solving the producer-consumer problem via virtual threads**: Write a program
    that simulates, via the producer-consumer pattern, an assembly line for checking
    and packing up bulbs using multiple workers (virtual threads).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solving the producer-consumer problem via virtual threads (fixed via Semaphore)**:
    Adapt the application developed in *Problem 237* to use `Semaphore` instead of
    executor services.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Solving the producer-consumer problem via virtual threads (increase/decrease
    consumers)**: Write a program that simulates an assembly line for checking and
    packing up bulbs using workers as needed (e.g., adapt the number of packers (increase
    or decrease them) to ingest the incoming flux produced by the checker). Use virtual
    threads and `Semaphore`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Implementing an HTTP web server on top of virtual threads**: Rely on Java
    `HttpServer` to write a simple HTTP web server implementation capable of supporting
    platform threads, virtual threads, and locking (for simulating a database connection
    pool).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hooking CompletableFuture and virtual threads**: Demonstrate the usage of
    `CompletableFuture` and virtual threads to solve asynchronous tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Signaling virtual threads via wait() and notify()**: Write several examples
    that use `wait()` and `notify()` to coordinate access to resources (objects) via
    virtual threads. Demonstrate the good signal and missed signal scenarios.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following sections describe solutions to the preceding problems. Remember
    that there usually isn’t a single correct way to solve a particular problem. Also,
    remember that the explanations shown here include only the most interesting and
    important details needed to solve the problems. Download the example solutions
    to see additional details and to experiment with the programs at [https://github.com/PacktPublishing/Java-Coding-Problems-Second-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/Java-Coding-Problems-Second-Edition/tree/main/Chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: 225\. Tackling continuations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept that sits behind virtual threads is known as *delimited continuations*
    or simply *continuations*. This concept is used internally by the JVM in the following
    piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we create and start five virtual threads but we only log information
    about one thread (thread #22 – of course, the id value may vary among executions).
    So, the output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Thread #22 has started running on *worker-1*, but after the blocking operation
    (`sleep(1000)`), it continues to run on *worker-4*. Behind this *thread context
    switching*, we have the so-called *continuations*.'
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the behavior of *continuations* can easily be explained via a popular
    debugger use case. When we debug the code, we set a breakpoint and run the code.
    When the flow hits this breakpoint, the execution freezes and we can inspect the
    current status of the application. Later on, when we’ve done the inspection, we
    continue running the code from this breakpoint forward. The debugger knows how
    to resume the execution from where it was left off (frozen). So, the execution
    continues until it hits the end of the application or until another breakpoint
    is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly, virtual threads follow the same behavior. A virtual thread is mounted
    on a platform thread (*worker-x*) and starts running. When the execution hits
    a blocking operation (for instance, a `sleep()` call), then the virtual thread
    is unmounted from its worker. Later on, after the blocking operation ends, the
    thread execution is resumed by scheduling and mounting it on a platform thread
    (same worker, *worker-x*, or another *worker-y*).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing continuations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Going deeper, we have to introduce *subroutines* and *coroutines*. Subroutines
    are functions that can be called and get back a response, while coroutines are
    *cooperating subroutines* that run at the same time and talk to each other like
    in a human conversation. Exactly like how two people talk to each other, coroutines
    set up a conversational state via two subroutines that are talking to each other.
    Via this paradigm, an application can perform some tasks, do nothing for a while,
    and then perform more tasks later.
  prefs: []
  type: TYPE_NORMAL
- en: But, how can coroutines remember the data involved in the conversations? The
    short answer is *continuations*. Continuations are data structures capable of
    carrying data (the conversational state) between coroutines. They can resume processing
    from where it was left off.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual threads take advantage of continuations by being capable of doing some
    work, then unmounting, and, later on, resuming from where they left off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Loom provides the API for working with continuations as an internal
    API, so it is not meant to be used directly in applications (we shouldn’t try
    to use this low-level API unless our goal is to write some higher-level API (libraries)
    on top of it). However, this API relies on two main classes and three methods.
    As classes, we have the `ContinuationScope`, which is the scope for handling nested
    `Continuation` instances. As methods, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`run()` – run a continuation from where it was left off'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yield()` – freeze (suspend) the continuation at this point and give control
    to the continuation’s caller (`run()` will be able to resume the execution from
    here)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isDone()` – test if the current continuation is complete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, under a `ContinuationScope` umbrella, we can have multiple nested continuations
    that set up a conversational state via `run()`, `yield()`, and `isDone()`. For
    virtual threads, there is a single `ContinuationScope` named `VTHREAD_SCOPE`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we call `continuation.run()`, this code will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is quite straightforward. Next, let’s suspend the continuation via `yield()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'At the moment, the output is the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Practically, when we call the `yield()` method, the continuation is suspended
    and control is given to the caller. We can easily see this by adding some logs
    after calling the `run()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `logger.info("Continuation keeps running ...");` code line
    was not executed. The `yield()` method has frozen the execution before this line
    and returned the control to the caller. In order to resume the continuation from
    where it was left off, we have to call `run()` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the output will be as follows (you can check if the continuation
    is done via `isDone()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, when we call `run()` again, the execution is resumed from where
    it was left off, not from the beginning. This is how continuations work.
  prefs: []
  type: TYPE_NORMAL
- en: Continuations and virtual threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s see how virtual threads and `sleep()` work via continuations in
    our example. Our virtual thread (#22) starts its journey by logging a simple message.
    Afterward, it hits the `Thread.sleep(1000);` code line, as in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1.png](img/B19665_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The virtual thread #22 running on worker-1'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'So, if the thread that has called `sleep()` is a virtual thread, then the code
    simply calls the internal `sleepNanos()` method from the `VirtualThread` class.
    The relevant code that we are interested in is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'So, here the code can call the `tryYield()` method (if `nanos` is 0) or the
    `parkNanos()` method. If `tryYield()` is called, then the thread state is set
    as `YIELDING`. On the other hand, if `parkNanos()` is called, then the thread
    state is set as `PARKING`. In both cases (via `tryYield()` or `parkNanos()`),
    the execution hits the `yieldContinuation()`, which is the climax of our journey:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, here the virtual thread is unmounted, and `yield()` is called.
    So, the virtual thread stack is copied into the heap and the thread is unmounted
    from the carrier thread (it becomes `PARKED`). We can see this via the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2.png](img/B19665_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: The virtual thread #22 is unmounted and moved to the heap'
  prefs: []
  type: TYPE_NORMAL
- en: 'This scenario takes place for any blocking operation, not just for `sleep()`.
    Once virtual thread #22 is uncounted, *worker-1* is ready to serve another virtual
    thread or do some other processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the blocking operation finishes (here, `sleep(1000)`), the `private`
    method `runContinuation()` from the `VirtualThread` class is called and the execution
    of #22 is resumed. As you can see in the following diagram, #22 is mounted now
    on *worker-4* since *worker-1* is not available (it has to execute some hypothetical
    virtual thread, #41).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3.png](img/B19665_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: The execution of virtual thread #22 is resumed on worker-4'
  prefs: []
  type: TYPE_NORMAL
- en: The execution continues with the second logging instruction and terminates.
    This is how continuations and virtual threads work internally to sustain a massive
    throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 226\. Tracing virtual thread states and transitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you know, a thread can be in one of the following states: `NEW`, `RUNNABLE`,
    `BLOCKED`, `WAITING`, `TIMED_WAITING`, or `TERMINATED`. These states are elements
    of the `State` enum and are exposed via the `Thread.currentThread().getState()`
    call. These states are valid for platform threads and for virtual threads as well
    and we can use them in our applications. (If you’re unfamiliar with this, you
    can find more details about it in *Java Coding Problems*, *First Edition*, *Chapter
    10*, *Problem 199.*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, internally speaking, a virtual thread works on a state transition
    model, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19665_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Virtual thread state transitions'
  prefs: []
  type: TYPE_NORMAL
- en: These states are declared in the `VirtualThread` class as `private static final
    int`. So, they are not public. However, they are essential for understanding the
    lifecycle of a virtual thread, so let’s briefly attempt to trace a virtual thread’s
    states during its lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: NEW
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a virtual thread is created (for instance, via the `unstarted()` method),
    it is in the `NEW` state. In this state, the thread is not mounted and not even
    started. However, at that moment, JVM calls the constructor of the `VirtualThread`
    listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, this constructor is responsible for choosing the scheduler to create a `Continuation`
    (which is a `VThreadContinuation` object that stores the information of what has
    to be run as a `task`) and prepare the `runContinuation private` field, which
    is a `Runnable` used to run the `Continuation` when the virtual thread is started.
  prefs: []
  type: TYPE_NORMAL
- en: STARTED
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A virtual thread passes from `NEW` to `STARTED` when we call the `start()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Moreover, at the moment, the `runContinuation` runnable is scheduled on the
    virtual thread scheduler via `submitRunContinuation()`.
  prefs: []
  type: TYPE_NORMAL
- en: RUNNING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `runContinuation` runnable moves the virtual thread state from `STARTED`
    to `RUNNING` and calls `cont.run()`. The virtual thread is mounted (it could be
    for the first time or just a subsequent mounting that resumes the execution from
    where it was left off) on a platform thread and starts running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: From this point forward, the virtual thread state can be moved to `TERMINATED`
    (the execution is done), `PARKING` (a blocking operation has been encountered),
    or `YIELDING` (the effect of calling `Thread.yield()`).
  prefs: []
  type: TYPE_NORMAL
- en: PARKING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The virtual thread is running until its job is done or it reaches a blocking
    operation. At this moment, the virtual thread should be unmounted from the platform
    thread (should be parked). In order to accomplish this, the JVM moves the virtual
    thread state from `RUNNING` to `PARKING` via the `park()` method. This is a transitional
    state to `PARKED` (park on the heap) or `PINNED` (park on its carrier thread).
  prefs: []
  type: TYPE_NORMAL
- en: PARKED/PINNED
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Further, the `yieldContinuation()` is called from `park()` and the result of
    unmounting the virtual thread is signaled via the flag returned by `Continuation.yield(VTHREAD_SCOPE)`.
    In other words, if the unmounting operation is a success, then the virtual thread
    state is moved from `PARKING` to `PARKED` (the virtual thread was successfully
    parked on the heap). Otherwise, if the unmounting operation fails, then the `parkOnCarrierThread()`
    method is called and the virtual thread state is moved to `PINNED` (the virtual
    thread is parked on the carrier thread). A `PINNED` virtual thread is moved to
    the `RUNNING` state when the execution can be resumed (since it was parked on
    its carrier thread). On the other hand, a `PARKED` virtual thread is moved to
    the `RUNNABLE` state when it is unparked (or interrupted). In this case, the virtual
    thread (which is not mounted) is mounted and the execution continues from where
    it was left by moving the state from `RUNNABLE` to `RUNNING`.
  prefs: []
  type: TYPE_NORMAL
- en: YIELDING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A virtual thread state is moved from `RUNNING` to `YIELDING` when a `Thread.yield()`
    call is encountered (for instance, this happens when we call `Thread.yield()`
    or `Thread.sleep(0)`). If the yield fails, then the virtual thread state is moved
    back to `RUNNING`. Otherwise, it is moved to `RUNNABLE`.
  prefs: []
  type: TYPE_NORMAL
- en: RUNNABLE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A virtual thread is in the `RUNNABLE` state when it is not mounted but it wants
    to resume its execution. It comes into this state from the `PARKED` or `YIELDING`
    states. At this moment, the virtual thread state is moved from `RUNNABLE` to `RUNNING`
    and the execution continues from where it was left off (this happens in `runContinuation()`).
  prefs: []
  type: TYPE_NORMAL
- en: TERMINATED
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, the circle is closed. The virtual thread finishes its execution and gets
    into the `TERMINATED` state. Moreover, a virtual thread that couldn’t be started
    is also moved to this state.
  prefs: []
  type: TYPE_NORMAL
- en: 227\. Extending StructuredTaskScope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We cannot extend `StructuredTaskScope.ShutdownOnSuccess` (*Chapter 10*, *Problem
    221*) or `ShutdownOnFailure` (*Chapter 10*, *Problem 222*) since these are `final`
    classes. But, we can extend `StructuredTaskScope` and provide a custom behavior
    via its `handleComplete()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that we want to travel from our current location to a certain
    destination in our town:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'On our phone, we have an application that can query a ridesharing service and
    the public transport service. The ridesharing service can simultaneously query
    multiple ridesharing servers to find the cheapest offer. On the other hand, the
    public transport service can simultaneously query the public transport servers
    to find the offer that leaves the earliest, no matter whether it is by bus, train,
    tram, or subway. In a diagram, we can represent these statements as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5.png](img/B19665_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Querying ridesharing and public transport services'
  prefs: []
  type: TYPE_NORMAL
- en: Both services are implemented via a `StructuredTaskScope`, but the one that
    queries the public transport servers uses a custom `StructuredTaskScope`, while
    the one that queries the ridesharing servers uses a classical `StructuredTaskScope`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are already familiar with the classical `StructuredTaskScope`, let’s
    quickly cover the ridesharing service. An offer received from this service is
    shaped as a record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The core of our code starts by forking a task for each of the three ridesharing
    servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After `scope.join()` finishes, we know that all subtasks have been finished
    successfully or exceptionally. We filter the results to extract the cheapest offer.
    If no offer is available, then we collect all exceptions and wrap them in a custom
    `RidesharingException`. Writing this as a functional programming snippet of code
    can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we return the offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s focus on the public transport service. This service queries the
    public transport servers via a custom `StructuredTaskScope`. A public transport
    offer is wrapped in the following record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The custom `StructuredTaskScope` is named `PublicTransportScope` and its goal
    is to analyze each subtask (`Subtask`) and to fetch the best offer. Extending
    the `StructuredTaskScope` is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A public transport server returns a `List<PublicTransportOffer>`. For instance,
    there can be three trains, or five buses in a day that cover our route. We will
    get them all on a separate list.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we extend `StructuredTaskScope`, we have to override a single method named
    `handleComplete()`. This method is automatically invoked for each `Subtask` that
    completes successfully or exceptionally. It is our job to collect and store the
    results for analysis later. To collect the results, we need a collection for valid
    results and a collection for exceptional results. These should be thread-safe
    collections since multiple `Subtask` instances may complete (almost) at the same
    time, which leads to race conditions. For instance, we can use `CopyOnWriteArrayList`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we override `handleComplete()`, and based on the `Subtask` state, we
    collect the results accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'When we reach this point, we have collected all successful and exceptional
    results. It is time to analyze this data and recommend the best offer. We consider
    that the best offer is the offer that leaves the earliest no matter whether it
    is by bus, train, tram, or subway. So, we just have to find the best `goTime`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If we cannot find any valid offer, then we collect the exceptions and wrap
    them in a custom `PublicTransportException` via the following helper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Notice that both of these methods are calling the `ensureOwnerAndJoined()` method.
    This built-in method guarantees that the current thread is the owner of this task
    scope (otherwise, it throws `WrongThreadException`) and that it joined after forking
    subtasks via `join()`/`joinUntil()` (otherwise, it throws an `IllegalStateException`).
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb it is good practice to rely on the `ensureOwnerAndJoined()`
    check on every `StructuredTaskScope` that needs to be called by the main task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Done! Our custom `StructuredTaskScope` is ready. Next, we can use it by forking
    our tasks and calling the `recommendedPublicTransport()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can call both services (ridesharing and public transport) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: So far, these two services run sequentially. In the next problem, we will run
    these two services concurrently by introducing another custom `StructuredTaskScope`.
    Until then, you can challenge yourself to write a custom `StructuredTaskScope`
    for the ridesharing service as well.
  prefs: []
  type: TYPE_NORMAL
- en: 228\. Assembling StructuredTaskScope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous problem (*Problem 227*), we developed an application containing
    a ridesharing service and a public transport service. In both services, we used
    `StructuredTaskScope` to concurrently query the proper servers. However, only
    the servers were called concurrently while these two services were executed sequentially
    – first, we run the ridesharing service (which queries concurrently three servers),
    and after we have a result from this service, we run the public transport service
    (which queries concurrently four servers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Going further, we want to assemble these two services into a third service
    capable of running them concurrently as in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6.png](img/B19665_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Running the ridesharing and public transport services concurrently'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by assembling the `RidesharingOffer` and `PublicTransportOffer` into
    a record named `TravelOffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write a custom `StructuredTaskScope` that forks the two `Callable`
    objects created in *Problem 227*. One `Callable` represents the ridesharing services
    (already implemented in *Problem 227* via a classic `StructuredTaskScope`), and
    the second `Callable` represents the public transport services (already implemented
    in *Problem 227* via the custom `PublicTransportScope`). We can name this `StructuredTaskScope`
    as `TravelScope`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `StructuredTaskScope` is parametrized – notice the `StructuredTaskScope<Travel>`.
    Since we have to fork different types of `Callable` instances, it would be handy
    to rely on `Object` and write `StructuredTaskScope<Object>`. But this will not
    be very expressive and neat. We better define an interface that narrows down the
    `Object` domain and that is implemented by our `Callable` instance’s results as
    follows (*sealed interfaces* were covered in detail in *Chapter 8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting back to `TravelScope`, we have to override the `handleComplete()` method
    to handle each completed `Subtask`. We know that the ridesharing service can return
    a valid result as a `RidesharingOffer` or an exceptional result as a `RidesharingException`.
    Moreover, the public transport service can return a valid result as `PublicTransportOffer`
    or an exceptional result as a `PublicTransportException`. We have to store these
    results in order to analyze them later when we create the `TravelOffer` answer.
    So, we define the following variables to cover all possible cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we override the `handleComplete()` and, exactly as in the case of `PublicTransportScope`,
    we rely on a simple `switch` to collect the results (for the `SUCCESS` and `FAILED`
    states, we need a nested `switch` to distinguish between the offer/exception received
    from the ridesharing service and the offer/exception received from the public
    transport service):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we analyze these results and create the proper `TravelOffer`. One
    way to accomplish this is as follows (feel free to think of a cooler/smarter implementation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `TravelScope` is ready to be used. All we need to do is to fork our two
    services to be executed concurrently and call the `recommendedTravelOffer()` method
    to get the best offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, instead of sequentially calling `fetchRidesharingOffers()` and `fetchPublicTransportOffers()`,
    we simply call `fetchTravelOffers()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Mission accomplished! Now you know how to write custom `StructuredTaskScope`
    instances and how to assemble/nest them to shape complex concurrent models.
  prefs: []
  type: TYPE_NORMAL
- en: 229\. Assembling StructuredTaskScope instances with timeout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s continue our journey from *Problem 228* by assuming that the ridesharing
    service should be implemented with a timeout/deadline. In other words, if any
    of the ridesharing servers don’t answer in 10 milliseconds, then we abort the
    request and report the thrown `TimeoutException` via a meaningful message to the
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that instead of `scope.join()`, which waits indefinitely, we should
    use `joinUntil(Instant deadline)`, which waits only for the given `deadline` before
    throwing a `TimeoutException`. So, the `fetchRidesharingOffers()` method should
    be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'By simply simulating a delay bigger than 10 milliseconds in any of the ridesharing
    servers, we help this `joinUntil()` to fail with a `TimeoutException`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to capture this `TimeoutException` and replace it with a friendly
    message for the end user, we have to adapt the `TravelScope` class. First, we
    define a variable to store the potential `TimeoutException`. Second, we adapt
    the `case FAILED` to populate this variable accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Third, we modify the `recommendedTravelOffer()` method to return a friendly
    message if `timeoutException` is not `null`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If the ridesharing service timeouts and the public transport service provides
    an offer, then the output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Done! Check out the bundled code to practice this example. Challenge yourself
    to add a timeout/deadline for the public transport service as well.
  prefs: []
  type: TYPE_NORMAL
- en: 230\. Hooking ThreadLocal and virtual threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a nutshell, `ThreadLocal` was introduced in JDK 1.2 (in 1998) as a solution
    to provide dedicated memory for each thread in order to share information with
    untrusted code (maybe some of your code has been written externally as third-party
    components) or between different components (that may run in multiple threads)
    of your application. Basically, if you are in such a scenario, then you don’t
    want to (or you cannot) share information via method arguments. If you need a
    more in-depth introduction to the `ThreadLocal` API, then consider *Java Coding
    Problems*, *First Edition*, *Chapter 11*, *Problem 220*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread-local variable is of type `ThreadLocal` and relies on `set()` to set
    a value and on `get()` to get a value. In *Java Coding Problems*, *First Edition,*
    it was said that: “*If thread* `A` *stores the* `x` *value and thread* `B` *stores
    the* `y` *value in the same instance of* `ThreadLocal`*, then later on, thread*
    `A` *retrieves the* `x` *value and thread* `B` *retrieves the* `y` *value. So,
    we can say that the* `x` *value is local to thread* `A`*, while the* `y` *value
    is local to thread* `B`*.”* Each thread that calls `get()` or `set()` has its
    own copy of the `ThreadLocal` variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: Internally, `ThreadLocal` manages a map (`ThreadLocalMap`). The keys of the
    map are the threads, and the values are those values given via the `set()` method.
    `ThreadLocal` variables fit well for implementing the *one thread per request*
    model (for instance, one thread per HTTP request) since they allow us to easily
    manage the lifecycle of a request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, a thread-local variable is global and static and it can be declared
    and initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'A thread-local variable should be reachable in the code from where it is needed
    (sometimes from everywhere in the code). Basically, the current thread and all
    the threads spawned by this thread should have access to the thread-local variables.
    In our case, it is reachable from everywhere in the current class. Next, let’s
    consider the following `Runnable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set a thread-local value representing information about the current
    thread, we get and log that value, we sleep for a random number of seconds (between
    0 and 5), and we log that value again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s execute 10 tasks via a classical fixed thread pool (platform threads):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily see that each of threads #24, #31, and #22 sets information about
    themselves, and this information is available after sleeping. For instance, thread
    #22 sets the value `[Thread[#22,pool-1-thread-1,5,main]]`, and this value is exactly
    what it gets after sleeping for 4 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s switch to virtual threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily see that each of threads #25, #27, and #28 sets information about
    themselves and this information is available after the sleeping period. For instance,
    thread #25 sets the value `[VirtualThread[#25]/runnable@ForkJoinPool-1-worker-3]`,
    and this value is exactly what it gets after sleeping for 3 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we get this information, thread #25 is executed on *worker-4*,
    not on *worker-3*, as the information reveals. Practically, thread #25 has been
    executed on *worker-3* when the information was set, and it has been executed
    on *worker-4* when the information (which remains unchanged) was get.'
  prefs: []
  type: TYPE_NORMAL
- en: This is perfectly normal since the thread was unmounted from *worker-3* when
    the execution hit the `Thread.sleep()` blocking operation. After sleeping, it
    was mounted on *worker-4*. However, the information was not altered, so virtual
    threads and `ThreadLocal` work together as expected. In other words, the mounting-unmounting
    cycles of a virtual thread don’t affect how `ThreadLocal` works. `ThreadLocal`
    variables are fully supported by virtual threads.
  prefs: []
  type: TYPE_NORMAL
- en: 231\. Hooking ScopedValue and virtual threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ScopedValue` API was added to handle the shortcomings of `ThreadLocal`.
    But what are the shortcomings of `ThreadLocal`?
  prefs: []
  type: TYPE_NORMAL
- en: Thread-local variables’ shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, it is hard to say and track who’s mutating a thread-local variable.
    This is a shortcoming of the API design. Basically, a `ThreadLocal` variable is
    globally available (at the application level or at a lower level), so it is hard
    to say from where it is mutated. Imagine that it is your responsibility to read,
    understand, and debug an application that uses several thread-local variables.
    How will you manage to follow the code logic and how will you know, at any given
    time, what values are stored by these thread-local variables? It would be a nightmare
    to track these variables from class to class and to signal when they mutated.
  prefs: []
  type: TYPE_NORMAL
- en: Second, thread-local variables may live forever or longer than they should.
    How is this possible? Thread-local variables will live as long as the platform
    threads that use them will live, or even longer. It is true that we can remove
    a thread-local variable from the internal map by explicitly calling `remove()`.
    But, if we forget to call `remove()`, then we just open the gate for memory leaks
    (we just hope that the garbage collector will collect this data at some point).
    Never forget to call `remove()` when you are done with a thread-local variable
    used by a platform thread! On the other hand, if you are using thread-local variables
    with virtual threads, then there is no need to call `remove()` because the thread-local
    variable is removed once the virtual thread dies.
  prefs: []
  type: TYPE_NORMAL
- en: Third, thread-local variables are prone to being duplicated. When we create
    a new thread (child thread) from the current thread (parent thread), the child
    thread copies all thread-local variables of the parent thread. So, if we spawn
    multiple threads from the current thread that has a significant number of thread-local
    variables, then we will duplicate a significant number of these variables. This
    is true for platform threads and for virtual threads. Since thread-local variables
    are not immutable, we cannot simply share the reference between threads. We have
    to copy them. Of course, this cannot be good for the application since it will
    negatively impact the memory footprint of these variables (imagine a million virtual
    threads having copies of thread-local variables).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing scoped values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting with JDK 20 (JEP 429) we have an alternative to thread-local variables
    called *scoped values*. This is meant to work with virtual threads and to overcome
    the shortcomings of thread-local variables. In JDK 20, this feature is in the
    incubator phase, and in JDK 21 (JEP 446) it is in the preview phase, so don’t
    forget to run the code using the `–code-preview` VM option.
  prefs: []
  type: TYPE_NORMAL
- en: Scoped values allow us to share immutable information (no need to copy) across
    the application’s components and have a limited lifetime (no risk of memory leaks).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you’ll see, the `ScopedValue` API is very neat and easy to use. To create
    a `ScopedValue`, we just call a factory method named `newInstance()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’ve created a `ScopedValue` (not bound) that is capable of carrying
    a value of type `String` (of course, it could be anything else). You can declare
    it locally, globally, or however you need it to be declared depending on the place(s)
    that it should be accessible from. However, the value mapped to a `ScopedValue`
    is available for the current thread and all threads spawned by the current thread
    (so far, this is like `ThreadLocal`) but it is restricted to a method call. We
    will clarify this shortly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: A `ScopedValue` is considered bound if a value is mapped to it. Otherwise, the
    `ScopedValue` is considered not bound. We can check if a `ScopedValue` is bound
    via the `isBound()` flag method. This is an important check because if we attempt
    to get a value of a `ScopedValue` that is not bound, then we will get back `NoSuchElementException`.
    Besides `isBound()`, we also have `orElse()` and `orElseThrow()`. Using `orElse()`,
    we can set a default value for a `ScopedValue` that is not bound, while via `orElseThrow()`,
    we can throw a default exception.
  prefs: []
  type: TYPE_NORMAL
- en: 'A value can be mapped to a `ScopedValue` (so, the `ScopedValue` becomes bound
    to a value) via the `where()` method. The syntax of this method is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, via the `runWhere()` and `callWhere()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'These three methods have in common the `key` and `value` parameters. The `key`
    represents the `ScopedValue` key (for instance, `SCOPED_VALUE`), while the `value`
    is the value mapped to this key. Whereas the `where()` method just creates a `ScopedValue`
    bound to a value, the `runWhere()` method can create a `ScopedValue` bound to
    a value and calls a `Runnable` operation (`op`) in the current thread, while `callWhere()`
    calls a `Callable` operation (`op`) in the current thread. If you prefer to rely
    only on the `where()` method, then simply rely on the following syntaxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: So, chaining `where().run()` acts as `runWhere()`, while `where().call()` acts
    as `callWhere()`. The advantage of using `where()` suffixed with `run()`/`call()`
    consists of the fact that we can write `ScopedValue.where(key1, value1).where(key2,
    value2),` …`.run()/call()` to obtain multiple `ScopedValue` instances bound to
    their values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: A `ScopedValue` has no `set()` method. Once we map a value to a `ScopedValue`,
    we cannot change it (it is immutable). This means that the JVM doesn’t need to
    copy values around (remember that this is a shortcoming specific to `ThreadLocal`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The value is mapped to this key only for a method (`Runnable` or `Callable`)
    call. For instance, let’s assume the following `Runnable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Via the `isBound()` method, we can check if a `ScopedValue` is bound (if it
    has a value). If a value is present, then we can successfully access it via the
    `get()` method. Calling `get()` for a `ScopedValue` that is not bound will result
    in a `NoSuchElementException` exception. For instance, if we run this task now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This is normal since we didn’t map any values to `SCOPED_VALUE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use the `where()` method to map a value to `SCOPED_VALUE` and
    share this value with the previous `Runnable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Carrier` object is an immutable and thread-safe accumulator of key-value
    mappings that can be shared with a `Runnable`/`Callable`. By calling `cr.run(taskr)`,
    we share the value `Kaboooom!` with the `Runnable`, so the output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'But we can write this example more compactly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, by using `runWhere()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Calling `taskr.run()` will output `Not bound` again. This is happening because
    the `ScopedValue` is bound only for a method call. *Figure 11.7* highlights this
    via a more expressive example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7.png](img/B19665_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: ThreadLocal vs. ScopedValue'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from this figure (left-hand side), once the thread local sets
    the value, `Mike`, this value is available in `sayHelloTL()` and in `sayGoodByeTL()`.
    The value is bound to this thread. On the other hand (right-hand side), the value
    `Mike` is mapped to a `ScopedValue`, but this value is available only in `sayHelloSV()`.
    This is happening because we bound the `SCOPED_VALUE` only to the `sayHelloSV()`
    method call. When the execution leaves the `sayHelloSV()`, the `SCOPED_VALUE`
    is not bound, and the value `Mike` is not available anymore. If `sayGoodByeSV()`
    was called from `sayHelloSV()`, then the value `Mike` would have been available.
    Or, if we call `sayGoodByeSV()` as follows, then the value `Mike` is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '`ScopedValue` works with `Callable` as well, but we have to replace `run()`
    with `call()`. For instance, let’s assume the following pretty dummy `Callable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following sequence of calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Can you intuit the output? It should be `Not bound`, `Kaboooom-1!`, `Kaboooom-2!`,
    `Kaboooom-3!`, and `Not bound` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: So, let me emphasize this once again. A `ScopedValue` is bound (it has a value)
    during a method call’s lifetime. The `isBound()` will return `false` outside this
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in this example, a `ScopedValue` can have different values in
    the same thread. Here, the same `Callable` was executed five times in the same
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting a `ScopedValue` from a certain thread (other than the main thread)
    can be done quite easily. For instance, we can set a `ScopedValue` from a platform
    thread as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, via the `ofPlatform()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Mapping a `ScopedValue` from a certain virtual thread can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have two threads, and each of them maps a different value to `SCOPED_VALUE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: So, the first thread (`tpcx`) maps the value `Kaboooom-tpcx!`, while the second
    thread (`tpcy`) maps the value `Kaboooom-tpcy!`. When `taskr` is executed by `tpcx`,
    the mapped value will be `Kaboooom-tpcx!`, while when `taskr` is executed by `tpcy`,
    the mapped value will be `Kaboooom-tpcy!`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example where `tpca` maps the value `Kaboooom-tpca!`, and `tpcb`
    doesn’t map any value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to not conclude from this that a `ScopedValue` is bound to a particular
    thread. The following note should clarify this aspect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `ScopedValue` is bound to a particular method call not to a particular thread
    (as in the case of `ThreadLocal`). In other words, a method can get a value of
    a `ScopedValue` if the code that calls it has mapped it. Data/values are passed
    in one way only: from *caller* to *callee*. Otherwise, `ScopedValue` is not bound
    and cannot be bounded and used in the current context of the method. As in the
    case of `ThreadLocal`, a `ScopedValue` is passed (and available) to all threads
    spawned by the task executed in the context of the current `ScopedValue`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the `isBound()` method, a `ScopedValue` also has `orElse()` and `orElseThrow()`.
    Via `orElse()`, we can specify an alternative/default value when the `ScopedValue`
    is not bound, while via `orElseThrow()`, we can throw a default exception. Here
    is an example of two `Runnable` objects that use these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we can use these methods outside of `Runnable`/`Callable` as well.
    Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: In the first virtual thread, we rely on `orElse()` to map the value of `SCOPED_VALUE`,
    so the `SCOPED_VALUE.get()` from `taskr` will return the `Kaboooom` value. In
    the second virtual thread, we rely on `orElseThrow()`, so `taskr` will not be
    executed since the `RuntimeException` will be thrown.
  prefs: []
  type: TYPE_NORMAL
- en: In the next problems, we will tackle more aspects of scoped values.
  prefs: []
  type: TYPE_NORMAL
- en: 232\. Using ScopedValue and executor services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Problem 230*, we wrote an application that combines `ThreadLocal` and executor
    services (we have used `newVirtualThreadPerTaskExecutor()` and `newFixedThreadPool()`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this problem, we re-write the code from *Problem 230* in order to use `ScopedValue`.
    First, we have the following `Runnable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is straightforward. We retrieve the value mapped to `SCOPED_VALUE`,
    we sleep from a random number of seconds (between 0 and 5), and we retrieve the
    value mapped to `SCOPED_VALUE` again. Next, let’s run this code via `newFixedThreadPool()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have 10 platform threads and 10 tasks. Each thread maps the value `Kaboooom-I`
    to `SCOPED_VALUE` and calls the `Runnable`. A possible output would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s arbitrarily check out thread #27\. Before sleeping, this thread sees
    the scoped value, `Kabooom-2`. After sleeping, thread #27 sees the same value,
    `Kabooom-2`. Each platform thread sees the scoped value that was mapped when the
    thread was created and the task was submitted. So, we have the same behavior as
    in the case of using `ThreadLocal`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s switch to `newVirtualThreadPerTaskExecutor()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, a possible output would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Again, we can conclude that each virtual thread sees the scoped value that was
    mapped when the thread was created and the task was submitted. The only difference
    is that virtual threads are running on different workers before and after sleeping.
  prefs: []
  type: TYPE_NORMAL
- en: So, we can rely on `ScopedValue` instead of `ThreadLocal` and take advantage
    of all the goodies (see *Problem 231*) brought by this API in comparison to `ThreadLocal`.
  prefs: []
  type: TYPE_NORMAL
- en: 233\. Chaining and rebinding scoped values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this problem, you’ll see how to *chain* and *rebind* scoped values. These
    are very handy operations that you’ll love to use.
  prefs: []
  type: TYPE_NORMAL
- en: Changing scoped values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume that we have three `ScopedValue` instances, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have a `Runnable` that uses all three `ScopedValue` instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We can map the values to these three `ScopedValue` instances by simply chaining
    the `where()` calls. This is a very convenient way to set up multiple scoped values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: That’s all! Quite simple!
  prefs: []
  type: TYPE_NORMAL
- en: Rebinding scoped values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine that we have two `Runnable` objects, `taskA`, and `taskB`. We
    start with `taskB`, which is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'So, `taskB` simply logs three `ScopedValue` instances. Next, `taskA` needs
    only `SCOPED_VALUE_1`, but it also has to call `taskB`. So, `taskA` should map
    the proper values for `SCOPED_VALUE_2` and `SCOPED_VALUE_3`. How about `SCOPED_VALUE_1`?
    Well, `taskA` doesn’t want to pass the current value of `SCOPED_VALUE_1` to `taskB`,
    so it must rebind this scoped value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `taskA` maps a value only to `SCOPED_VALUE_1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows (the comments have been manually added; they
    are not part of the output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: So, `taskA` sees the value `Kabooom-1` for `SCOPED_VALUE_1`, but it doesn’t
    pass this value to `taskB`. It rebinds this scoped value to `No kaboooom`. This
    is the value that lands in `taskB` next to `Kaboooom -2` and `Kaboooom -3`, which
    has been mapped for `SCOPED_VALUE_2` and `SCOPED_VALUE_3`. This technique is useful
    if you don’t want to allow a certain scoped value to go beyond your goals or if
    you just need another value. Once the execution gets back in `taskA`, the `SCOPED_VALUE_1`
    is restored to `Kaboooom - 1`, so the initial value is not lost and is available
    in `taskA`. On the other hand, `SCOPED_VALUE_2` and `SCOPED_VALUE_3` are not bound.
    They have been bound only for the execution of `taskB`. How cool is that?!
  prefs: []
  type: TYPE_NORMAL
- en: 234\. Using ScopedValue and StructuredTaskScope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this problem, we will reiterate the application developed in *Problems 227*
    and *228*, and we will enrich it with a few `ScopedValue` variables for implementing
    new features. I’ll consider that you are already familiar with that application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ScopedValue` that we plan to add are listed here (these are added in the
    main class because we want them to be accessible at the application level):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s focus on the `fetchTravelOffers()` method, which is the point
    from where we fork our two tasks, `fetchRidesharingOffers()` and `fetchPublicTransportOffers()`.
    The code that calls `fetchTravelOffers()` gets modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our travel page needs the user credentials (for simplicity, only the username).
    If the user is logged in, then we should have a valid username and we can share
    it with `fetchTravelOffers()` via the `USER` scoped value. If the user is not
    logged in, then `USER` remains unbound. The `fetchTravelOffers()` gets modified
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: There are a lot of things happening in this code, so let’s take it one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ridesharing service is accessible only for logged-in users, so we call
    it only if `USER` is bound. Otherwise, we log a message for the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the public transport service doesn’t require the user to
    be logged in. However, in order to use public transport we need a special ticket.
    We have such a ticket and we share it with the public transport service via the
    `PUBLIC_TRANSPORT_TICKET` scoped value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: The `PUBLIC_TRANSPORT_TICKET` scoped value will be accessible only from the
    public transport service (only from `fetchPublicTransportOffers()` and other methods
    called from this one).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the ridesharing and public transport services need our location and destination.
    This information is collected from the user/client and passed as arguments in
    `fetchTravelOffers()`. Thereafter, we map this information to `LOC` and `DEST`
    scoped values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Now, `LOC` and `DEST` are bound and are accessible only from ridesharing and
    public transport services. They will be shared with all threads forked from these
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s check out the ridesharing service, `fetchRidesharingOffers()`.
    This service checks if the user is logged in and logs a meaningful message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the ridesharing companies (`CarOne`) provides a random discount to its
    clients. We have a discount of 0.5 that we can map to `CAR_ONE_DISCOUNT` scoped
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: If we visit the scoped values status in the context of `fetchRidesharingOffers()`,
    then we can say that the `USER` scoped value was bound at the application level,
    so it should be available everywhere in the application. The `LOC` and `DEST`
    scoped values have been bound in `fetchTravelOffers()`, so they are also available
    in `fetchRidesharingOffers()`. On the other hand, `PUBLIC_TRANSPORT_TICKET` is
    not available (not bound) in `fetchRidesharingOffers()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s focus on the public transport service, `fetchPublicTransportOffers()`.
    This service doesn’t require the user to be logged in, but it can use this information
    to log a friendly message as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: If we briefly review the current status of the scoped values in the context
    of `fetchPublicTransportOffers()`, then we can say that the `USER` scoped value
    was bound at the application level, so it should be available everywhere in the
    application. The `LOC` and `DEST` scoped values have been bound in `fetchTravelOffers()`,
    so they are also available in `fetchPublicTransportOffers()`. On the other hand,
    `PUBLIC_TRANSPORT_TICKET` and `CAR_ONE_DISCOUNT` are not available (not bound)
    in `fetchPublicTransportOffers()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have used all five scoped values. We continue to track them
    in the `Ridesharing` class, which simulates the ridesharing servers. In this class,
    we have access to `USER`, `DEST`, and `LOC` scoped values. Moreover, only in `carOneServer()`
    do we have access to `CAR_ONE_DISCOUNT`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: So, if we have a discount (and we have one), the server of `CarOne` will apply
    it. If no drivers are available for our route, then the server will throw a meaningful
    exception. This exception is thrown from `topCarServer()` and `starCarServer()`
    as well. These are the servers of `TopCar` company and the `StarCar` company respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ok, so far so good! Next, let’s check out the `PublicTransport` class, which
    simulates the public transport servers. In this class, we have access to `USER`,
    `DEST`, `LOC`, and `PUBLIC_TRANSPORT_TICKET` scoped values. We arbitrarily choose
    one of the servers (all of them use the same core code) and list here the code
    that we are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the public transport services can make an offer only if we have
    a special ticket that is verified via the `PUBLIC_TRANSPORT_TICKET` scoped value.
    If no public tram transport is available for our route, then the server throws
    an exception that uses the `LOC` and `DEST` scoped values to build a meaningful
    message.
  prefs: []
  type: TYPE_NORMAL
- en: Done! Using `ScopedValue` with `StructuredTaskScope` allows us to design complex
    concurrent models.
  prefs: []
  type: TYPE_NORMAL
- en: 235\. Using Semaphore instead of Executor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s say that we have the following task (`Runnable`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'And we plan to execute this task 15 times by 3 threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily solve this problem via `Executors.newFixedThreadPool()` and platform
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'A snippet of the possible output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the application has only three platform threads (#22, #23,
    and #24).'
  prefs: []
  type: TYPE_NORMAL
- en: But, we already know that platform threads are expensive and it will be better
    to rely on virtual threads. The problem is that we cannot simply replace this
    fixed thread pool with `newVirtualThreadPerTaskExecutor()` because we can’t control
    the number of threads. While we want to use only 3 threads, the virtual thread
    executor will allocate a virtual thread per task, so we will end up with 15 threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to control the number of virtual threads, we can rely on `Semaphore`
    (if you want more details about this topic you can check out *Java Coding Problems*,
    *First Edition*, *Chapter 10*, *Problem 211*). First, we declare a `Semaphore`
    with `NUMBER_OF_THREADS` permits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we rely on `semaphore.acquire()` and `semaphore.release()` to control
    the access to these permits and execute `NUMBER_OF_TASKS` tasks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have six virtual threads (#27, #33, #28, #30, #25, and #31), not three.
    The idea is that the `Semaphore` allows only three virtual threads to be created
    and to run concurrently at a time. You can probe this statement by running the
    code by yourself. After the first three virtual threads are created, they will
    sleep for 5 seconds. But because virtual threads are cheap, they do not go back
    into a thread pool, so they are not reused. It is much cheaper to create another
    three to use and throw. The idea is that there will not be more than three at
    a time.'
  prefs: []
  type: TYPE_NORMAL
- en: 236\. Avoiding pinning via locking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember from *Chapter 10*, *Problem 213*, that a virtual thread is pinned
    (not unmounted from its carrier thread) when the execution goes through a `synchronized`
    block of code. For instance, the following `Runnable` will cause virtual threads
    to be pinned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The `synchronized` block contains a blocking operation (`sleep()`), but the
    virtual thread that hits this point of execution is not unmounted. It is pinned
    on its carrier thread. Let’s try to capture this behavior via the following executor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible output would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Check out the workers! Because the virtual threads are pinned on their carriers,
    the application uses all the available workers (eight on my machine). The workers
    are not accessible during that `sleep(1000)`, so they are not available to execute
    other tasks. In other words, a carrier thread is available only after the virtual
    thread finishes its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, we can avoid this situation by re-writing the application via `ReentrantLock`
    instead of `synchronized`. If you want more details about `ReentrantLock`, then
    you can check out *Java Coding Problems*, *First Edition*, *Chapter 11*, *Problems
    222* and *223*. So, considering that you are familiar with `ReentrantLock`, we
    can come up with the following non-pinned solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute this code via the same `newVirtualThreadPerTaskExecutor()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s analyze a snippet of a possible output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: This time, we can see that only three workers are used, *worker-1*, *3*, and
    *5*. Because the virtual threads are not pinned, they can free up their carrier
    threads. This way, the platform threads can be reused and we save the rest of
    the resources for other tasks. If pinning is intensive, then it will affect the
    scalability of the application, so it is recommended to revisit your `synchronized`
    code and, whenever achievable, replace it with `ReentrantLock`.
  prefs: []
  type: TYPE_NORMAL
- en: 237\. Solving the producer-consumer problem via virtual threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume that we want to write a program simulating an assembly line (or
    a conveyor) for checking and packing bulbs using two workers. By checking, we
    mean that the worker tests if the bulb lights up or not. By packing, we mean that
    the worker takes the verified build and puts it in a box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s assume a fixed number of producers (3), and a fixed number of consumers
    (2); let’s represent it via the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8.png](img/B19665_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: The producer-consumer problem with a fixed number of workers'
  prefs: []
  type: TYPE_NORMAL
- en: We can implement this scenario via the well-known `Executors.newFixedThreadPool(PRODUCERS)`,
    `Executors.newFixedThreadPool(CONSUMERS)`, and `ConcurrentLinkedQueue` as the
    temporary storage for checked bulbs, as you can see at [https://github.com/PacktPublishing/Java-Coding-Problems/tree/master/Chapter10/P203_ThreadPoolFixed_ConcurrentLinkedQueue](https://github.com/PacktPublishing/Java-Coding-Problems/tree/master/Chapter10/P203_ThreadPoolFixed_ConcurrentLinkedQueue).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider this code as legacy and let’s refactor it via virtual threads.
    All we have to do is to replace `Executors.newFixedThreadPool()` (executor used
    for producers and consumers) with `newVirtualThreadPerTaskExecutor()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s all! Isn’t it astonishing how easily we refactor this code to move from
    platform threads to virtual threads? The possible output is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you’ll find the complete code on GitHub. Take your time to get familiar
    with it especially if you didn’t read the first edition of this book. We will
    rely on this code in the following two problems as well.
  prefs: []
  type: TYPE_NORMAL
- en: 238\. Solving the producer-consumer problem via virtual threads (fixed via Semaphore)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous problem, we implemented the producer-consumer problem via a
    fixed number of producers (three virtual threads) and consumers (two virtual threads).
    Moreover, since our application works as an assembly line, we can say that the
    number of tasks is boundless. Practically, the producers and consumers work without
    breaks until the assembly line is stopped. This means the virtual threads assigned
    by the executor as producers and consumers remain exactly the same between a start-stop
    lifecycle of the assembly line.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s assume that we want to use `Semaphore` objects instead of `newVirtualThreadPerTaskExecutor()`
    to obtain the exact same behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on *Problem 235*, we can implement the fixed number of producers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'And the fixed number of consumers is shaped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: In the next problem, we will complicate things a little bit.
  prefs: []
  type: TYPE_NORMAL
- en: 239\. Solving the producer-consumer problem via virtual threads (increase/decrease
    consumers)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s continue our producer-consumer problem with another scenario that starts
    with three producers and two consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s assume that each producer checks a bulb in no more than one second. However,
    a consumer (packer) needs a maximum of 10 seconds to pack a bulb. The producer
    and consumer times can be shaped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, in these conditions, the consumers cannot face the incoming flux.
    The queue (here, `LinkedBlockingQueue`) used for storing bulbs until they are
    packed will continuously increase. The producers will push into this queue much
    faster than the consumers can poll.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have only two consumers, we have to increase their number to be able
    to handle and stabilize the queue’s load. But, after a while, the producers will
    get tired and will need more time to check each bulb. If the producers slow down
    the production rate, the number of consumers should be decreased as well since
    many of them will just sit there. Later on, the producers may speed up again,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of problem can be solved via `newCachedThreadPool()` and platform
    threads. If you are not familiar with this topic then you can find more details
    in *Java Coding Problems*, *First Edition*, *Chapter 10*, *Problem 204*.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about solving it via virtual threads? We can start the producers and consumers
    via two `Semaphore` objects exactly as we did in *Problem 238*. Next, we need
    to monitor the queue size and act accordingly. Let’s assume that we should take
    action only if the queue size is greater than five bulbs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, let’s assume that we can increase the number of consumers up to 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to monitor the queue every 3 seconds with an initial delay of 5 seconds,
    so we can rely on a `ScheduledExecutorService`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'The `monitorQueueSize()` method is responsible for initializing the `monitorService`
    and calling `addNewConsumer()`, `removeConsumer()`, and log the status as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'So, if the queue size is above `MAX_QUEUE_SIZE_ALLOWED` and the number of consumers
    is under `MAX_NUMBER_OF_CONSUMERS`, then we should add a new consumer. This can
    be done by releasing a new permit for the `Semaphore` that handles the consumers.
    Releasing a permit when the number of current permits is zero will simply add
    a new permit, so a new virtual thread can acquire this permit and become a new
    consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Most likely, when the producers slow down the production rate, there will be
    too many consumers that just hand on. In such cases, we have to interrupt virtual
    threads until we manage to balance the work between producers and consumers. The
    `removeConsumer()` method is responsible for signaling that a consumer must be
    interrupted, and for this, it sets an `AtomicBoolean` to `true`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Consumer` checks this flag at each run and when it is `true`, it will
    simply interrupt the currently running virtual thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to simulate the decrease in the production rate for producers, we
    can rely on `ScheduledExecutorService` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we start the assembly line and the producers will check bulbs at a very
    high rate. After 2.5 minutes, we decrease this rate by adding an extra time of
    4 seconds for each producer via the `extraProdTime` variable. Initially, this
    is 0, but after 2.5 minutes it becomes 4000\. Since it is part of the production
    time it will slow down the producer by 4 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try to trace a run of our assembly line to see how it works. So, we start
    the assembly line and, pretty soon, we notice that the number of bulbs in the
    queue (27) is greater than 5 and the application started to add consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'The application continues to add consumers while the number of unprocessed
    bulbs keeps growing (here we have 237 bulbs in the queue and 32 consumers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'When the application reaches around 37 consumers, the queue size enters a descending
    trend. Here you can see two consecutive logs of queue status (meanwhile, the application
    is still adding more consumers – it does this until the `queue.size()` is less
    than 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'The queue size continues to decrease and the number of consumers reaches the
    maximum of 50\. At some point, the queue is depleted. The number of customers
    is much higher than needed so they are removed one by one. Here is the moment
    when the last consumer (consumer #50 was removed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'While the application continues to calibrate itself by removing consumers,
    the producers slow down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the producers have slowed down, the number of consumers continues to
    decrease and the queue load can be processed by two consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the assembly line is calibrated. If the producers increase their
    production rate again then the application is ready to respond. As you can see,
    the consumer’s `Semaphore` has 48 permits available, so we shouldn’t create them
    again. If you want to remove the permit corresponding to an interrupted consumer
    then you have to extend the `Semaphore` class and override the `protected` method
    `reducePermits()`. The number of permits is just a counter; so, in this scenario,
    removing permits is not really necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 240\. Implementing an HTTP web server on top of virtual threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a simple HTTP web server in Java is quite easy since we already
    have an API ready to guide and serve our goals. We start from the `HttpServer`
    class (this class is present in the `com.sun.net.httpserver` package), which allows
    us to achieve our goal straightforwardly in a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into the code, let’s quickly mention that our web server will
    allow us to choose between platform and virtual threads and between non-locking
    or locking (for instance, to simulate access to a database). We will make these
    choices via two boolean parameters of our `startWebServer(boolean virtual, boolean
    withLock)` method, named `virtual` and `withLock`, respectively. So, we will have
    four possible configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an `HttpServer` via the `create()` method. At this point,
    we also set up the port of our web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the web server context by specifying the access page and the
    handler that will deal with the HTTP requests (the `WebServerHandler` is implemented
    later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can choose the executor service (override the default one) that will
    orchestrate the threads of our web server. This can be done via the `setExecutor()`
    method. Since we can choose between platform threads (we arbitrarily chose to
    have 200 such threads) and virtual threads, we have to cover both cases as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the `start()` method to start the web server, and we log this
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we focus on the `WebServerHandler` class, which implements the `com.sun.net.httpserver.HttpHandler`
    interface and is responsible for handling the incoming HTTP requests. We simulate
    an HTTP request processing by sleeping 200 milliseconds and create a simple `String`
    response via a `Callable` named `task`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: When the `WebServerHandler` is initiated, we also set up the `withLock` value.
    If this value is `true`, then our implementation will rely on a `Semaphore` with
    20 permits to limit the access of the platform threads (200) or of the unbounded
    number of virtual threads. This `Semaphore` simulates an external resource, such
    as a database that relies on a connection pool of 20 connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HTTP requests (we focus only on `GET`) are handled in the overridden `handle(HttpExchange
    exchange)` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the HTTP `GET` request is processed, we have to prepare the response for
    our client and send it. This job is done via the `HttpExchange` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: Done! Our HTTP web server is ready to rock and roll.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we start the web server via `startWebServer(false, false)`, then we will
    get a web server that has 200 platform threads ready to serve in a non-locking
    context. If we set the first argument to `true`, then we switch to an unbounded
    number of virtual threads. In the following figure, you can see the heap usage
    in these two scenarios for 400 requests ramped up in 2 seconds via a JMeter test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9.png](img/B19665_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Memory usage (lock free)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, virtual threads used less heap memory than platform threads
    being more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we add locking into the equation (we set the second argument of `startWebServer()`
    to `true`), then a possible profile of heap memory looks as in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10.png](img/B19665_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: Memory usage (using locking)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, even with locking, the virtual threads are still using less
    memory than platform threads. In *Chapter 13*, we will dive deeper into creating
    web servers via the JDK API, including the new features brought by JDK 18.
  prefs: []
  type: TYPE_NORMAL
- en: 241\. Hooking CompletableFuture and virtual threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CompletableFuture` is one of the main asynchronous programming APIs in Java
    (if you need deep coverage of this topic, then you could consider checking out
    *Java Coding Problems*, *First Edition*, *Chapter 11*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use `CompletableFuture` with virtual threads, we just have to use
    the proper executor for virtual threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use this executor to fetch three application testers in asynchronous
    mode via `CompletableFuture`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are interested in returning a `TestingTeam` only after all three of
    these `CompletableFuture` instances have completed. For this, we rely on `allOf()`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code, then the output reveals the usage of three virtual threads
    in asynchronous mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: Done! Employing virtual threads for `CompletableFuture` is quite easy.
  prefs: []
  type: TYPE_NORMAL
- en: 242\. Signaling virtual threads via wait() and notify()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `wait()`, `notify()`, and `notifyAll()` are three methods defined in the
    `Object` class that allow multiple threads to communicate with each other and
    coordinate their access to resources without issues.
  prefs: []
  type: TYPE_NORMAL
- en: The `wait()` method must be called only by the thread that owns the object’s
    *monitor* to force this thread to wait indefinitely until another thread calls
    `notify()` or `notifyAll()` on the same object. In other words, the `wait()` method
    must be called in a `synchronized` context (instance, block, or static method).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a virtual thread calling the `wait()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is another virtual thread that wakes up the previous one via the `notify()`
    call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, nothing has happened, since `wThread` and `nThread` are not
    started. We start `wThread` and we give it 1 second to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we start `nThread` and give it 1 second to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we log the state of `wThread`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code reveals the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'The virtual thread #22 is our `wThread`. Initially (before calling `wait()`),
    it is in the `RUNNABLE` state, so the thread is in execution on the JVM. After
    `wait()` is called, the state of this thread is set to `WAITING`, so thread #22
    is waiting indefinitely for another thread to wake it up. This is the moment when
    the virtual thread #23 (`nThread`) calls the `notify()` method on the same object.
    After calling the `notify()` method, thread #22 wakes up, and its state is `RUNNABLE`
    again. After finishing its execution, the `wThread` state is `TERMINATED`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, this scenario is the happy path or a *good signal*. Let’s check out the
    following scenario based on the same `wThread` and `nThread`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be (#22 is `wThread` and #23 is `nThread`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: This time, `nThread` starts first and calls `notify()`. This is just a shot
    in the dark since `wThread` is not in the `WAITING` state. Later on, `wThread`
    calls `wait()` and waits indefinitely to be woken up by `nThread`. But this will
    never happen since `notify()` was already triggered, so `wThread` is blocked forever.
    In short, this is called a *missed signal*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we develop concurrent applications that involve `wait()`, `notify()`,
    and `notifyAll()`, we have to ensure that the application complexity will not
    hide such *missed signals*. We can avoid *missed signals* by simply counting the
    number of `wait()` and `notify()` calls and acting accordingly. For instance,
    let’s move this logic in the object that should be signaled, and let’s call it
    `SignaledObject`. First, we have the `callWait()` method, which uses the `counter`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: If no signal was missed, then the `counter` variable should be 0\. Otherwise,
    we have at least one *missed signal* (`notify()` call) so there is no reason to
    wait. We return immediately, without calling `wait()`, since this may lead to
    a deadly trap.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, `callNotify()` increases the `counter` at each call as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the happy path (*good signal*) scenario, then the output will be
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything works as expected since `counter` is 0\. If we try out the missed
    signal scenario, then we have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we avoided the indefinite blocking by not calling `wait()`.
    We managed to elegantly handle the *missed signal*. Cool, right!?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered 18 advanced problems about virtual threads and structured
    concurrency. You can see this chapter as a masterclass designed to help you speed
    up your learning and get ready for production with strong confidence in your knowledge.
    With that covered, you have now finished the chapter and the book.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/8mgytp5DGQ](https://discord.gg/8mgytp5DGQ )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1139613064111216156.png)'
  prefs: []
  type: TYPE_IMG
