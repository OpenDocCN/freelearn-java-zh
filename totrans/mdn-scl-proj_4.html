<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a Spam Classification Pipeline</h1>
                </header>
            
            <article>
                
<p class="mce-root">Two pillars of Google's Gmail service stand out<span>. These are an</span> <span class="packt_screen">Inbox</span> <span>folder, receiving benign or wanted email messages, and a</span> <span class="packt_screen">Spam</span><span> folder, receiving unsolicited, junk emails, or simply spam.</span><br/></p>
<p><span>The emphasis of this chapter is on identifying spam and classifying it as such. It explores the following topics concerning spam detection:</span></p>
<ul>
<li><span>What are the techniques of separating spam from ham?</span></li>
<li><span>If spam filtering is one suitable technique, how can it be formalized as a supervised learning classification task?</span></li>
<li><span>Why is a certain algorithm better than another for spam filtering, and in what respect?</span></li>
<li><span>Where are the tangible benefits of effective spam filtering most felt?</span></li>
</ul>
<p> This chapter implements a spam filtering data analysis pipeline. </p>
<p>Implementing a spam classifier with Scala and <strong>machine learning</strong> (<strong>ML</strong>) is the overall learning objective of this chapter. Starting from the datasets we created for you, we will rely on the Spark ML library's machine learning APIs and its supporting libraries to build a spam classification pipeline.</p>
<p>The following list is a section-wise breakdown of the individual learning outcomes:</p>
<ul>
<li>Introduction to the spam classification problem</li>
<li>The project's problem formulation</li>
<li>Implementing the spam binary classification pipeline using the <span>various algorithms</span></li>
<li>The goal is to start with <kbd>DataFrame</kbd> and proceed towards analysis</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spam classification problem</h1>
                </header>
            
            <article>
                
<p><span>Any email service should process incoming mail intelligently. This could be classifications that produces two distinct, sorted streams of email, ham and spam. Email processing at the sentry level entails a smart vetting process—a classification task that produces two distinct, sorted streams of email—ham and spam. Gmail's sophisticated spam filtering engine filters out spam by a classification process, fulfilling, in a proverbial sense, the separation of the wheat from the chaff.</span></p>
<p><span><span>S</span></span>pam can be a pernicious phenomenon in our daily lives, which is intimately tied to an increasingly connected world. For example, a binary classification is an ongoing deceptive link to apparently innocent looking websites hosting malware. Readers can learn why a spam filter can minimize problems spam can cause. These are summarized as follows:</p>
<ul>
<li>Unethical companies harvest email addresses from the web and send out a flood of bulk emails to people. A Gmail user, say, <kbd>gmailUser@gmail.com</kbd>, is enticed to click on an innocent-looking website masquerading as a popular website people commonly recognize as reputable. One kind of vile intention is to trap the user into giving up personal information on entering a supposedly popular reputable website.</li>
<li>Another spam email like the one sent out by operators of a shady website, <kbd>frz7yblahblah.viral.sparedays.net</kbd>, is preying on people's predilection for making easy money. For example, the innocent looking link <span>contains a </span><span>deceptive link to some shady website hosting malware, a rootkit virus for example. A rootkit is very hard to remove. It is a virus that embeds itself into your OS kernel. It can be so elusive and potentially destructive that a remote hacker can gain control of your system, and before you know it, your network may come to a grinding halt. Several man hours will be lost and, if you are a company, revenue will be lost as well.</span></li>
</ul>
<p>The following is a screenshot of a spam email sitting in Gmail's <span class="packt_screen">Spam</span> folder, indicative of phishing. <strong>Phishing</strong> refers to deliberate attempts at maliciously gaining fraudulent access to personal information by deception, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a636ff4b-2a50-472b-91e3-d8e32d41d807.jpg" style="font-size: 1em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An example of a spam email</div>
<p>In the next section, we will explore a handful of topics that are relevant to the development of the spam classification pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Relevant background topics </h1>
                </header>
            
            <article>
                
<p>The following topics are reviewed in this section prior to developing the spam classifier:</p>
<ul>
<li>Multidimensional data</li>
<li>Importance of features</li>
<li>Classification task</li>
<li>Individual feature importance in relation to another feature</li>
<li>Term frequency-inverse document frequency (TF-IDF)</li>
<li>Hashing trick</li>
<li>Stop word removal</li>
<li>Normalization</li>
</ul>
<p>In the next section, we will talk more about each topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multidimensional data</h1>
                </header>
            
            <article>
                
<p>Multidimensional data is data bearing more than one feature. <span>We have dealt with many features in the earlier chapters. That said, let's restate this with an example, explaining what a feature means and why features are such a big deal.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Features and their importance</h1>
                </header>
            
            <article>
                
<p>Each feature in a multidimensional dataset is a contributing factor in arriving at a prediction:</p>
<ul>
<li>A prediction is to be made on a new sample; for example, a new breast cancer mass sample belonging to a certain individual </li>
<li>Each contributing factor has a certain feature importance number or feature weight</li>
</ul>
<p>Some features are more important than others in contributing to the final prediction. In other words, a (final) prediction is made on what category a new sample belongs to. <span>For example, in the breast cancer dataset in <a href="a7305f5b-bea1-485e-9b14-411a5003dd01.xhtml">Chapter 2</a>, <em>Build a Breast Cancer Prognosis Pipeline with Spark and Scala</em></span>, <span>the Random Forest algorithm can be used to estimate feature importance. In the following list <span>the top-most features have the highest weight; the feature at the bottom of the list has the lowest weight (</span>in order of decreasing importance):</span></p>
<ol>
<li><strong>Uniformity_of_Cell_Size</strong></li>
<li><strong>Uniformity_of_Cell_Shape</strong></li>
<li><strong>Bare_Nuclei</strong></li>
<li><strong>Bland_Chromatin</strong></li>
<li><strong>Single_Epithelial_Cell_Size</strong></li>
<li><strong>Normal_Nucleoli</strong></li>
<li><strong>Clump_Thickness</strong></li>
<li><strong>Marginal_Adhesion</strong></li>
<li><strong>Mitosis</strong></li>
</ol>
<p>This means that the first feature has the most impact on the final predicted outcome, second feature has the second biggest impact, and so on.</p>
<p>We have just covered features, feature importance, weight, and so on. This exercise has laid the groundwork for this chapter.</p>
<p>In the following section, we will look at classification.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification task</h1>
                </header>
            
            <article>
                
<p><strong>Classification</strong><span> implies a categorization action, a task involving categories. A classification task then typically denotes a supervised learning technique that lets us categorize a new sample previously unseen (such as an Iris flower, whose species we do not know yet). By categorizing, we allude to the fact that the </span>classification<span> task is tagging an unseen sample with one of the predicted labels in the training dataset.</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d01dd528-4984-4ba1-ad03-116f94e54ccd.png" style="width:17.58em;height:12.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"> What is classification?</div>
<p>Before we move on the to next question, we will draw your attention to the term <strong>training dataset</strong>. In the next topic on classification outcomes, we discuss classification outcomes as binary or categorical and explain supporting concepts like <strong>target</strong> variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification outcomes</h1>
                </header>
            
            <article>
                
<p><span>Up until <a href="dda92a07-faff-410a-952c-cb41d4c4ad75.xhtml">Chapter 3</a>, <em>Stock Price Predictions</em><em>,</em> we worked with ML problems related to classification as a supervised learning technique. The classification tasks were data-centered (data as in samples, </span><span>observations, or measurements) for which you already know the target answer. That brings us to the term <strong>t</strong></span><strong>arget variable</strong>. This is another commonly-used alternative name for the <strong>response variable</strong>, a term from statistics. A target variable, in the ML context, is the variable that is typically the output or outcome. For example, it could be a binary outcome variable with only two classification outcomes—<kbd>0</kbd> or <kbd>1</kbd>. With that point made, we know that data<span> for which the target answer is readily known or predetermined is called </span><strong>labeled data</strong><span>.</span></p>
<p class="mce-root"/>
<p><span>The classification task developed in this chapter is all about supervised learning, where an algorithm is teaching itself to learn from the </span>labeled<span> samples that we provide. </span>A notable example from a previous chapter is the breast cancer dataset, which is also a supervised learning classification task. In that chapter, we classified breast cancer samples in two categories—benign and malignant. These are the two classification outcomes, outcome values that we can use to<strong> </strong>label or tag so-called training data that an algorithm will be trained on. On the other hand, unlabeled data is a new breast cancer sample data waiting to be diagnosed. More pertinently, a new incoming corpus possibly containing both ham and spam contains unlabeled data. Based on labeled samples from the training set, you could try to classify the unlabeled samples. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Two possible classification outcomes</h1>
                </header>
            
            <article>
                
<p><span>Spam filtering is a binary classification task, an ML task that generates predicted values that only contain one of two possible classification outcomes. In this chapter, we will set out to build a spam classifier. </span>Labels in the spam classification set belong to a finite set consisting of text from two types of emails, spam and ham. The binary classification task then becomes one of predicting the (output) label from previously unseen data. Deciding whether an email is spam or not, therefore, become a binary classification problem. By convention, we assign the ham mutually exclusive state a value of <kbd>1</kbd>, and <kbd>0</kbd> for the other state, spam. In the next section, we will formulate the spam classification problem at hand. This will give us an overview of the project as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project overview – problem formulation</h1>
                </header>
            
            <article>
                
<p>In this chapter, the stated goal is to build a spam classifier, one that is capable of distinguishing spam terms in email messages that are mixed in with regular or expected email content as well. It is important to know that spam messages are email messages that are sent out to multiple recipients with the same content, as opposed to regular messages. We start with two email datasets, one that represents ham and one that represents spam. After stages of preprocessing, we fit the model on a training set, say 70% of the entire dataset.</p>
<p>This application is a typical spam filtering application in the sense that it works on text. We then put algorithms to work that help the ML process detect words, phrases, and terms most likely found in spam emails. Next, will go over the ML workflow at a high level in relation to spam filtering.</p>
<p class="mce-root"/>
<p>The ML workflow is as follows:</p>
<ul>
<li>We will be developing a pipeline that will use dataframes</li>
<li>A dataframe contains a <kbd>predictions</kbd> column and another column containing preprocessed text</li>
<li>The classification process involves transformation operations—one <kbd>DataFrame</kbd> is transformed into another</li>
<li><span>Our pipeline runs a series of stages, involving TF-IDF, a hashing trick, stop word removal, and Naive Bayes algorithms</span></li>
</ul>
<p>In essence, the spam filtering or classification problem is a supervised learning task, where we supply labeled data to the pipeline. A natural language processing step in this task entails labeled feature data being converted into a bag of feature vectors. </p>
<p>At this point, we can lay out the steps needed to build the spam classifier:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/48f4e0eb-d68e-4f20-a159-d6d7fbd1ffcf.jpg" style="width:23.33em;height:29.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">S<span>teps needed to build the spam classifier</span></div>
<p>The steps described in the preceding list are useful and help us come up with an outline of the spam classifier.</p>
<p>The following diagram represents a formulation of the spam classification problem:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/de19765f-3762-4f3b-a671-a0c335125a7b.png" style="width:30.67em;height:20.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Spam classifier outline</div>
<p>Here is a refresher on the following topics:</p>
<ul>
<li>Stop words</li>
<li>Punctuation marks</li>
<li>Regular expressions</li>
</ul>
<p>We want to eliminate two categories of text in our spam and ham datasets. These are as follows:</p>
<ul>
<li>Punctuation marks may be characterized in three categories:
<ul>
<li style="padding-left: 30px">Terminal points or marks</li>
<li style="padding-left: 30px">Dashes and hyphens</li>
<li style="padding-left: 30px">Pausing points, or marks</li>
</ul>
</li>
</ul>
<ul>
<li>Stop words.</li>
</ul>
<p>The following is a representative list of punctuation marks:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f1993867-b611-4904-ac75-f5d2031318da.jpg" style="width:34.00em;height:26.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Punctuation marks</div>
<p>We covered a list of commonly encountered punctuation marks in a text corpus that we want to be removed from our spam and ham datasets.</p>
<p>We also need to remove stop words—words that are common. Our spam classifier will remove these in the preliminary preprocessing steps. </p>
<p class="mce-root"/>
<p>Here is a representative list of stop words:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb476fb0-89b2-4270-a6be-6f61b5631b7e.jpg" style="width:24.67em;height:31.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A representative list of stop words</div>
<p>The following is a representative list of regular expressions to help with punctuation mark removal. A spam corpus can be daunting. Regular expressions can get as complex as can be in order to cope with spam:</p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d8d2580b-95fb-479a-9d78-e0c68ab15dc0.jpg" style="width:35.17em;height:35.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Some relevant regular expressions</div>
<p>In the <em>Getting started</em> section, we will get started implementing the project. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p><span>In order to get started, download the dataset from the <kbd>ModernScalaProjects_Code</kbd> folder and drop it into the root folder of your project. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up prerequisite software</h1>
                </header>
            
            <article>
                
<p>You may use your existing software setup from previous chapters. Apache Log4j 2 Scala API is the notable exception. T<span>his is a Scala wrapper over Log4j 2, which is an upgraded <kbd>Logger</kbd> implementation of Log4j version 1.x (the version provided by Spark). </span></p>
<p><span>Simply override the existing Log4j from Spark (version 1.6) w</span>ith Log4j 2 Scala by adding in appropriate entries in the <kbd>build.sbt</kbd> file.</p>
<p>The following table lists two choices of prerequisite software:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ada1086f-6195-4c7f-9d64-c10b4004edc7.jpg" style="width:27.75em;height:35.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Implementation infrastructure</div>
<p>Download the dataset from the <kbd>ModernScalaProjects_Code</kbd> folder and drop it into the root folder of your project.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spam classification pipeline </h1>
                </header>
            
            <article>
                
<p>The most important development objective of this chapter is to perform spam classification tasks with the following algorithms:</p>
<ul>
<li>Stop word remover</li>
<li>Naive Bayes</li>
<li>Inverse document frequency</li>
<li>Hashing trick transformer</li>
<li>Normalizer</li>
</ul>
<p>The practical goal of our spam classification task is this: Given a new incoming document, say, a collection of random emails from either <span class="packt_screen">Inbox</span> or <span class="packt_screen">Spam</span>, the classifier must be able to identify spam in the corpus. After all, this is the basis of an effective classifier. The real-world benefit behind developing this classifier to give our readers experience of developing their own spam filters. After learning how to put together the classifier, we will develop it. </p>
<p>The implementation steps are in the next section. This takes us straight into the development of Scala code in a Spark environment. Given that Spark allows us to write powerful distributed ML programs such as pipelines, that is exactly what we will set out to do. We will start by understanding the individual implementation steps required to reach our goal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation steps</h1>
                </header>
            
            <article>
                
<p>The spam detection (or classification) pipeline involves five stages of implementation, grouped by typical ML steps that are needed. These are as follows:</p>
<ol>
<li>Loading data</li>
<li>Preprocessing data</li>
<li>Extracting features</li>
<li>Training the spam classifier</li>
<li>Generating predictions</li>
</ol>
<p><span>In the next step, we will set up a Scala project in IntelliJ. </span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – setting up your project folder</h1>
                </header>
            
            <article>
                
<p>Here is what the project looks like in IntelliJ:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5993f097-c76c-4a0d-8e3c-b66c52adf47b.jpg" style="width:36.08em;height:33.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Project outline in IntelliJ</div>
<p>In the next section, we will upgrade the <kbd>build.sbt</kbd> file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – upgrading your build.sbt file</h1>
                </header>
            
            <article>
                
<p>Here is the upgraded <kbd>build.sbt</kbd> file. What is new here? Remember, earlier, we talked about a new <kbd>Logging</kbd> library. Those new entries you see in the following screenshot are the new dependencies you need to move from Log4j 1.6 to the new Scala wrapper for Log4j 2:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/36d57b4b-0780-4597-94ac-e28ad944b4ef.jpg" style="width:30.42em;height:9.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">New entries in the build.sbt file</div>
<p>In the next section, we will start with the Scala code, starting with a trait.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – creating a trait called SpamWrapper</h1>
                </header>
            
            <article>
                
<p>In IntelliJ, using <span class="packt_screen">File</span> | <span class="packt_screen">New</span> | <span class="packt_screen">Scala</span> class, create an empty Scala trait called <kbd>SpamWrapper</kbd> in a file called <kbd>SpamWrapper.scala</kbd>.</p>
<p>First things first. At the top of the file, we will set up the following imports for implementing classes to take advantage of this trait:</p>
<ul>
<li><kbd>SparkSession</kbd>—the entry point to programming with Spark</li>
<li>The appropriate Log4J library imports, so that we can dial down logging messages from Spark:</li>
</ul>
<p>These are the minimum imports. Next, create an empty <kbd>trait</kbd>. The following is an updated <kbd>trait</kbd>:</p>
<pre><span>trait </span>SpamWrapper {  }</pre>
<p class="mce-root"/>
<p>Inside the <kbd>SpamWrapper</kbd> trait, create a <kbd>SparkSession</kbd> instance called <kbd>session</kbd>. At this point, here is a refresher on Spark:</p>
<ul>
<li>We need a <kbd>SparkSession</kbd> object instance to be the entry point to programming with Spark.</li>
<li>We do not need a separate <kbd>SparkContext</kbd>. This is provided by <kbd>SparkSession</kbd>. The underlying context is available to us easily as <kbd>session.sparkContext</kbd>.</li>
<li>To create a <kbd>SparkSession</kbd> object instance or get an existing <kbd>SparkSession</kbd>, a builder pattern is used.</li>
<li>The <kbd>SparkSession</kbd> instance is available for the entire time frame of a Spark job.</li>
</ul>
<p>Here is the updated <kbd>SpamWrapper</kbd> trait:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/162c02a7-0964-4834-9f57-13d71a19cb94.jpg" style="width:37.00em;height:28.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">SpamWrapper trait with SparkSession value</div>
<p>To recap, we create a <kbd>val</kbd> called <kbd>session</kbd> that our pipeline class will use. Of course, this will be the entry point to programming with Spark to build this spam classifier. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – describing the dataset</h1>
                </header>
            
            <article>
                
<p>Download the dataset from the <kbd>ModernScalaProjects_Code</kbd> folder. It consists of two text files: </p>
<ul>
<li><kbd>inbox.txt</kbd>: Ham emails (I created this file from my Gmail <span class="packt_screen">Inbox</span> folder)</li>
<li><kbd>junk.txt</kbd>: Spam emails (I created this out of spam from my Gmail <span class="packt_screen">Spam</span> folder)</li>
</ul>
<p>Drop these files into the root of your project folder. In the next section, we will describe the dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the SpamHam dataset</h1>
                </header>
            
            <article>
                
<p>Before we present the actual dataset, here are a few real-world spam samples: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9fbb4607-4350-4b47-b611-78f1cbf2b1ed.jpg" style="width:53.33em;height:24.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Spam email with phishing example</div>
<p class="mce-root"/>
<p>Here is an example of regular or wanted mail, also known as ham:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2601a3a7-7c2f-4190-95bd-14a4981fa7f3.jpg" style="width:39.83em;height:32.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A perfectly normal email from Lightbend</div>
<p>The following is a glimpse into the actual dataset used in our spam-ham classification task. There are two datasets:</p>
<ul>
<li><kbd>inbox.txt</kbd>: A ham dataset compiled from a small collection of regular emails from my <span class="packt_screen">Inbox</span> folder</li>
<li><kbd>junk.txt</kbd>: A spam dataset <span>compiled from a small collection of junk email from my <span class="packt_screen">Spam</span></span>/<span><span class="packt_screen">Junk</span> folder</span></li>
</ul>
<p class="mce-root"/>
<p>Here is the regular dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d01fdfb1-2a11-47ea-a21f-178684369570.jpg" style="width:16.50em;height:22.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A portion of the regular email dataset</div>
<p>Here is the spam dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7942ce19-5c5f-43d5-aa06-f9ee35a68a72.jpg" style="width:16.08em;height:22.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A portion of the spam dataset</div>
<p class="mce-root"/>
<p>The preceding email wants you to confirm here. This is an attempt at phishing. This completes our description of our datasets. In the next step, we will proceed with data preprocessing. We need a new Scala object called <kbd>SpamClassifierPipeline</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – creating a new spam classifier class</h1>
                </header>
            
            <article>
                
<p>We will create a new Scala file called <kbd>SpamClassifierPipeline.scala</kbd>. First of all, we need the following imports:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a622b9e-8f5f-4813-b51f-5c4dd991df3d.jpg" style="width:41.17em;height:10.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Required imports </div>
<p><span>Now that the imports have been created, let's create an empty <kbd>SpamClassifierPipeline</kbd> object in the same package as the <kbd>SpamWrapper</kbd> trait, as follows:</span></p>
<pre>object SpamClassifierPipeline extends App with SpamWrapper {  }</pre>
<p>A prototype spam classifier is ready. We need to create code in it to do such things as <span>preprocessing the data and, of course, much more. In the next step, we will list the necessary preprocessing steps to take up.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 – listing the data preprocessing steps</h1>
                </header>
            
            <article>
                
<p><span>There is a purpose to preprocessing. In most data analytics tasks, the question that begs to be asked is—is our data necessarily usable? The answer lies in the fact that most real-world datasets need preprocessing, a massaging step meant to give data a new usable form.</span></p>
<p class="mce-root"/>
<p><span>With the spam and ham datasets, we identify two important </span>preprocessing steps:</p>
<ol>
<li>Removing punctuation marks:</li>
</ol>
<ul>
<li style="padding-left: 30px">Processing punctuation using regular expressions</li>
</ul>
<ol start="2">
<li>Removing stop words</li>
</ol>
<p><span>In the next step, we will write Scala code for two regular expressions, expressions that are fairly simple and only address a small subset of spam. However, it's a start.</span></p>
<p>In the next step, we will load our datasets into Spark. Naturally, we want a ham dataframe and a spam dataframe. We will take up the task of creating a ham dataframe first. Both our ham and spam datasets are ready for preprocessing. That brings us to the next step.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 7 – regex to remove punctuation marks and whitespaces</h1>
                </header>
            
            <article>
                
<p>Here is a regular expression for our immediate needs: </p>
<pre>val punctRegex = <span>raw"[^A-Za-z0-9</span><span>]+"</span></pre>
<p><kbd>raw</kbd> is a method in the string interpolation <kbd>StringContext</kbd> class of the standard Scala library.</p>
<p>Our corpus is likely to contain trailing and leading whitespaces which are sometimes ill-formatted. For example, we will run into whitespaces where lines are indented too much. To get rid of whitespaces, we will use regular expressions. This will work with the anchors, the hat <kbd>^</kbd>, and the <kbd>$</kbd> sign to extract the text without whitespaces. The updated regular expression now looks like this:</p>
<pre><span>//matches whitespaces and punctuation marks<br/></span><span>val regex2</span> = <span>raw"[^A-Za-z0-9</span><span>\s</span><span>]+"</span><span> </span></pre>
<p><span>We just created a regular expression, <kbd>regex2</kbd>, a whitespace, and punctuation remover. Very soon, we will need this <kbd>regex2</kbd>. In the next section, we will create a new ham dataframe after applying the first of a few essential preprocessing steps—removing punctuation.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 8 – creating a ham dataframe with punctuation removed</h1>
                </header>
            
            <article>
                
<p><span>We will have the regex work on each row of our ham <strong>resilient distributed dataset</strong> (<strong>RDD</strong>) and then use the <kbd>replaceAll</kbd> method. The underlying regex engine will use the regex to conduct a search and match on our ham corpus to come up with matched occurrences, as follows: </span></p>
<pre><span>val </span>hamRDD2 = hamRDD.map(_.replaceAll(regex2, <span>""</span>).trim)<br/>hamRDD2: org.apache.spark.rdd.RDD[String] = inbox.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:23</pre>
<p><span> The <kbd>replaceAll</kbd> method kicked in and replaced all occurrences of whitespaces and punctuation. In the next step, we will convert this RDD into a dataframe.</span></p>
<p>We created a new ham RDD with leading and trailing whitespaces, and with punctuation removed. Let's display this new ham dataframe:</p>
<pre>hamRDD3.take(10)<br/>println("The HAM RDD looks like: " + hamRDD3.collect())</pre>
<p>We created a new ham dataframe. We need to transform this dataframe by assigning a <kbd>label</kbd> of <kbd>0.0</kbd> to each ham sentence. We shall do that in the next step:</p>
<pre><span>case class LabeledHamSpam</span>(label: Double, mailSentence: <span>String</span>)</pre>
<p><span>Therefore, we will create a case called <kbd>LabeledHamSpam</kbd> to model a sentence as a feature with a <kbd>Double</kbd> <kbd>label</kbd>. Next, create a new ham RDD that has exactly four partitions. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a labeled ham dataframe</h1>
                </header>
            
            <article>
                
<p>We will repartition our ham datafame and apply a <kbd>transform</kbd> operation to each <kbd>"Ham sentence"</kbd> in the ham dataframe, as follows:</p>
<pre><span>val </span>hamRDD3: RDD[LabeledHamSpam] = hamRDD2.repartition(<span>4</span>).map(w =&gt; <span>LabeledHamSpam</span>(<span>0.0</span>,w))</pre>
<p><span>We repartitioned and created a new RDD that is structured as a row of ham sentences labeled with <kbd>0.0</kbd>. Now, d</span>isplay the first <kbd>10</kbd> rows of the new ham RDD:</p>
<pre>hamRDD3.take(<span>10</span>)<br/><span>println</span>(<span>"The HAM RDD looks like: " </span>+ hamRDD3.collect())</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>So, we assigned <kbd>0.0</kbd> for all ham sentences. </span>It is time to create the spam RDD:</p>
<pre><span>val </span>spamRDD = <span>session</span>.sparkContext.textFile(<span>spamFileName</span>)<br/><strong><span>spamRDisDset: org.apache.spark.rdd.RDD[String] = junk2.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:23</span></strong></pre>
<p>Repeat the same set of preprocessing steps for the spam dataset as well. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 9 – creating a spam dataframe devoid of punctuation</h1>
                </header>
            
            <article>
                
<p><span>We will use the same <kbd>LabeledHamSpam</kbd> case class to assign a <kbd>Double</kbd> <kbd>value</kbd> of <kbd>1.0</kbd> for spam sentences, as follows:</span></p>
<pre><span>/*<br/>Replace all occurrences of punctuation and whitespace<br/>*/<br/></span><span>val </span>spamRDD2 = spamRDD.map(_.replaceAll(regex2, <span>""</span>).trim.toLowerCase)<br/>/*<br/>Repartition the above RDD and transform it into a labeled RDD<br/>*/<br/><span>val </span>spamRDD3 = spamRDD2.repartition(<span>4</span>).map(w =&gt; LabeledHamSpam(<span>0.0</span>,w))</pre>
<p>In the next step, we want a combined dataframe containing both spam and ham dataframes. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 10 – joining the spam and ham datasets</h1>
                </header>
            
            <article>
                
<p>In this step, we will use the <kbd>++</kbd> method to join both dataframes in a <kbd>Union</kbd> operation:</p>
<pre>val hamAndSpamNoCache: org.apache.spark.rdd.RDD[LabeledHamSpam] = (hamRDD3 ++ spamRDD3)<br/>hamAndSpam: org.apache.spark.rdd.RDD[LabeledHamSpam] = UnionRDD[20] at<br/>$plus$plus at &lt;console&gt;:34</pre>
<p class="mce-root"/>
<p>In the next section, let's create a dataframe, with two columns:</p>
<ul>
<li>A row containing <kbd>feature sentences</kbd> with punctuation removed</li>
<li>A predetermined <kbd>label</kbd> column </li>
</ul>
<p><span>Check for the following code snippet for better understanding</span>:</p>
<pre><span>val hamAndSpamDFrame"</span> = hamAndSpam.select(hamAndSpam(<span>"punctLessSentences"</span>), hamAndSpam(<span>"label"</span>))<br/><span>dataFrame2: org.apache.spark.sql.DataFrame = [features: string, label: double]</span></pre>
<p>We created the new dataframe. Let's display this:</p>
<pre><strong>hamAndSpamDFrame.show</strong><br/><strong>+--------------------+-----+</strong><br/><strong>| lowerCasedSentences|label|</strong><br/><strong>+--------------------+-----+</strong><br/><strong>|this coming tuesd...| 0.0|</strong><br/><strong>|pin free dialing ...| 0.0|</strong><br/><strong>|regards support team| 0.0|</strong><br/><strong>| thankskat| 0.0|</strong><br/><strong>|speed dialing let...| 0.0|</strong><br/><strong>|keep your user in...| 0.0|</strong><br/><strong>| user name ilangostl| 0.0|</strong><br/><strong>|now your family m...| 0.0|</strong><br/><br/></pre>
<p>Next, let's run the following optional checks:</p>
<ul>
<li>The schema of the dataframe</li>
<li>Columns present in the dataframe</li>
</ul>
<p>Here is how we will print out the schema:</p>
<pre>hamAndSpamDFrame.printSchema<br/><strong><span>  root<br/></span><span>  |-- features: string (nullable = true)<br/></span><span>  |-- label: double (nullable = false)</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>They look good! Let's read off the <kbd>columns</kbd> now:</p>
<pre>hamAndSpamDFrame.columns<br/><strong><span>res23: Array[String] = Array(features, label)</span></strong></pre>
<p><span>Up until now, we have created a dataframe that was punctuation-free, but not necessarily free of rows that contain null values. Therefore, in order to drop any rows containing null values, we will need to import the <kbd>DataFrameNaFunctions</kbd> class, that is, if you have not already imported it:</span></p>
<pre><span>import </span>org.apache.spark.sql.DataFrameNaFunctions<br/><span><br/>val </span>naFunctions: DataFrameNaFunctions = hamAndSpamDFrame.na</pre>
<p>In order to drop nulls from the punctuation-free dataframe, there is a column called <kbd>punctFreeSentences</kbd>. We will invoke the <kbd>drop()</kbd> <span>method as shown in the following</span> code:</p>
<pre><span>val </span>nonNullBagOfWordsDataFrame = naFunctions.drop(<span>Array</span>(<span>"punctFreeSentences"</span>))</pre>
<p>The call to the <kbd>drop</kbd> method in the preceding code causes any rows of sentences that contain null values to be dropped. If you so wish, display the first 20 rows of the dataframe:</p>
<pre><span>println</span>(<span>"Non-Null Bag Of punctuation-free DataFrame looks like this:"</span>)</pre>
<p>Display the dataframe. The following code will help you do just that:</p>
<pre>nonNullBagOfWordsDataFrame.show()</pre>
<p>At this point, a good next step relates to tokenizing punctuation-free rows, which also feature what we want to be tokenized. <span>Tokenizing the current dataframe is the focus of the next section</span>. Tokenizing brings us one step closer to the next preprocessing step—removing stop words.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 11 – tokenizing our features</h1>
                </header>
            
            <article>
                
<p>Tokenizing is simply an operation executed by an algorithm. It results in the tokenization of each row. All the following terms define tokenization:</p>
<ul>
<li>Breaking up</li>
<li>Splitting</li>
</ul>
<p class="mce-root"/>
<p>The appropriate term appears to be <span>the second term in the preceding list</span>. For each row in the current dataframe, a tokenizer splits a <kbd>feature</kbd> row into its constituent tokens by splitting it along separating whitespaces. Each resulting Spark supplies two tokenizers:</p>
<ul>
<li><kbd>Tokenizer</kbd> from the package <kbd>org.apache.spark.ml</kbd></li>
<li><kbd>RegexTokenizer</kbd> from the same package</li>
</ul>
<p>Both tokenizers are transformers. A transformer in Spark is an algorithm that accepts an input column as a (hyper) parameter and spits out a new <kbd>DataFrame</kbd> with a transformed output column, as follows:</p>
<pre><span>import </span>org.apache.spark.ml.feature.Tokenizer<br/><br/><span>val </span>mailTokenizer = <span>new </span>Tokenizer().setInputCol(<span>"lowerCasedSentences"</span>).setOutputCol(<span>"mailFeatureWords"</span>)<br/><span>mailTokenizer: org.apache.spark.ml.feature.Tokenizer = tok_0b4186779a55<br/></span></pre>
<p>Calling the <kbd>transform</kbd> method on <kbd>mailTokenizer</kbd> will give us a newly transformed dataframe:</p>
<pre><span>val </span>tokenizedBagOfWordsDataFrame: DataFrame = mailTokenizer2.transform(nonNullBagOfWordsDataFrame)<br/><br/></pre>
<p>The resulting dataframe, <kbd>tokenizedBagOfWordsDataFrame</kbd>, is a tokenized non-null bag of lowercased words. It looks like this:</p>
<pre><strong>+--------------------+-----+--------------------+</strong><br/><strong>| lowerCasedSentences|label| mailFeatureWords|</strong><br/><strong>+--------------------+-----+--------------------+</strong><br/><strong>|This coming tuesd...| 0.0|[this, coming, tu...|</strong><br/><strong>|Pin free dialing ...| 0.0|[pin, free, diali...|</strong><br/><strong>|Regards support team| 0.0|[regards, support...|</strong><br/><strong>| Thanks kat| 0.0| [thankskat]|</strong><br/><strong>|Speed dialing let...| 0.0|[speed, dialing, ...|</strong><br/><strong>|Keep your user in...| 0.0|[keep, your, user...|</strong><br/><strong>| User name ilangostl| 0.0|[user, name, ilan...|</strong><br/><strong>|Now your family m...| 0.0|[now, your, famil...|</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The important thing to notice here is that a row in the transformed column, <kbd>mailFeatureWords</kbd>, resembles an array of words. Readers will not fail to notice that there are words in <kbd>mailFeatureWords</kbd> known as stop words. These are words that do not significantly contribute to our spam classification task. These words can be safely eliminated by Spark's <kbd>StopWordRemover</kbd> algorithm. In the next step, we will see how to put <kbd>StopWordRemover</kbd> to work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 12 – removing stop words</h1>
                </header>
            
            <article>
                
<p>First, make sure you have the <kbd>StopWordRemover</kbd> import in the <kbd>SpamClassifierPipeline</kbd> class. Next, we will create an instance of <kbd>StopWordRemover</kbd> and pass into it a (hyper) parameter column, <kbd>mailFeatureWords</kbd>. We want an output column that is devoid of stop words:</p>
<pre><span>val </span>stopWordRemover = <span>new </span>StopWordsRemover().setInputCol(<span>"mailFeatureWords"</span>).setOutputCol(<span>"noStopWordsMailFeatures"</span>) </pre>
<p><span>Just like with <kbd>mailTokenizer</kbd>, we call the <kbd>transform</kbd> method to get a new <kbd>noStopWordsDataFrame</kbd>:</span></p>
<pre><span>val </span>noStopWordsDataFrame = stopWordRemover.transform(tokenizedBagOfWordsDataFrame)</pre>
<p><span>The resulting dataframe, a tokenized, non-null bag of lowercase words with no stop words looks like this:</span></p>
<pre><strong>noStopWordsDataFrame.show()</strong><br/><strong>+-----------------------+-----+</strong><br/><strong>|noStopWordsMailFeatures|label|</strong><br/><strong>+-----------------------+-----+</strong><br/><strong>| coming| 0.0|</strong><br/><strong>| tuesday| 0.0|</strong><br/><strong>| going| 0.0|</strong><br/><strong>| take| 0.0|</strong><br/><strong>| time| 0.0|</strong><br/><strong>| meeting| 0.0|</strong><br/><strong>| get| 0.0|</strong><br/><strong>| everyone| 0.0|</strong><br/><strong>| running| 0.0|</strong><br/><strong>| pathways| 0.0|</strong><br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>In the next step, we will do a second transformation on our current dataframe:</p>
<pre><span>import </span><span>session</span>.implicits._<br/><br/>val noStopWordsDataFrame2 = noStopWordsDataFrame.select(explode($"noStopWordsMailFeatures").alias("noStopWordsMailFeatures"),noStopWordsDataFrame("label"))</pre>
<p class="mce-root">The exploded, tokenized non-null bag of lowercase words with no stop words looks like this:</p>
<pre class="mce-root">noStopWordsDataFrame2.show()</pre>
<p>This completes data preprocessing. This sets the stage for feature extraction, an extremely important ML step. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 13 – feature extraction</h1>
                </header>
            
            <article>
                
<p>In this step, we will extract the features of this dataset.<span> We will do the following:</span></p>
<ul>
<li><span>Creating feature vectors</span></li>
<li><span>Creating features entails converting the text into bigrams of characters using an n-gram model</span></li>
<li>These bigrams of characters will be <span>hashed to a length <kbd>10000</kbd> feature vector </span></li>
<li>The final feature vector will be <span>passed into Spark ML</span></li>
</ul>
<p><span>Check out the following code snippet</span>:</p>
<pre style="padding-left: 60px">import org.apache.spark.ml.feature.HashingTF<br/><br/>val hashMapper = new HashingTF().setInputCol("words").<br/>setOutputCol("noStopWordsMailFeatures").setOutputCol("mailFeatureHashes").setNumFeatures(10000)<br/>hashFeatures: org.apache.spark.ml.feature.HashingTF = hashingTF_5ff221eac4b4</pre>
<p>Next, we will <kbd>transform</kbd> the featured version of the <kbd>noStopWordsDataFrame</kbd>:</p>
<pre><span>val</span><span> </span>featurizedDF = hashMapper.transform(noStopWordsDataFrame)<br/><br/>//Display the featurized dataframe<br/>featurizedDF1.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>With a hash-featured and tokenized non-null bag of lowercased words with no stop words, this <kbd>DataFrame</kbd> looks like this:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/85365a2b-f55c-4efd-a102-c14d0f542532.jpg" style="width:47.67em;height:23.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Dataframe with h<span>ash-featured and tokenized non-null bag of lowercase words</span></div>
<p>At this point, we are ready to create training and test sets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 14 – creating training and test datasets</h1>
                </header>
            
            <article>
                
<p>This step is important because we are going to create a model that we want to train with a training set. One way to create a training set is to partition the current dataframe and assign 80% of it to a new training dataset:</p>
<pre><span>val </span>splitFeaturizedDF = featurizedDF.randomSplit(<span>Array</span>(<span>0.80</span>, <span>0.20</span>), <span>98765L</span>)<br/><span>splitFeaturizedDF1: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([filteredMailFeatures: string, label: double ... 2 more fields],    [filteredMailFeatures: string, label: double ... 2 more fields])</span></pre>
<p>Now, let's retrieve the training set:</p>
<pre><span>val </span>trainFeaturizedDF = splitFeaturizedDF(<span>0</span>)</pre>
<p class="mce-root"/>
<p>The testing dataset follows. Here is how we will create it:</p>
<pre><span>val </span>testFeaturizedDF = splitFeaturizedDF(<span>1</span>)</pre>
<p>We need to go one step further. A modified version of the training set that has the following columns removed is needed:</p>
<ul>
<li><kbd>mailFeatureWords</kbd></li>
<li><kbd>noStopWordsMailFeatures</kbd></li>
<li><kbd>mailFeatureHashes</kbd></li>
</ul>
<p>Here is the new training set, after doing <kbd>drop</kbd> on the preceding columns:</p>
<pre><span>val </span>trainFeaturizedDFNew = trainFeaturizedDF1.drop(<span>"mailFeatureWords"</span>,<span>"noStopWordsMailFeatures"</span>,<span>"mailFeatureHashes"</span>)<br/><br/>trainFeaturizedDFNew.show()</pre>
<p><span>The invocation of the <kbd>show()</kbd> method results in the following display of the new training dataset:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/711f0e12-e97a-4639-ad3f-2ba687a881a5.jpg" style="width:15.50em;height:23.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Training dataframe</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The next important step right before training (fitting) the model is what is known as <strong>inverse document frequency</strong> (<strong>IDF</strong>). Spark provides an estimator called IDF that will compute the IDF for us. The IDF is an algorithm that will train (fit) models on our current dataframe:</p>
<pre>val mailIDF = new IDF().setInputCol("mailFeatureHashes").setOutputCol("mailIDF")</pre>
<p>Now we will pass in our <kbd>featurizedDF</kbd> dataframe into the <kbd>fit</kbd> method on the IDF algorithm. This will produce our model:</p>
<pre>val mailIDFFunction = mailIDF.fit(featurizedDF)</pre>
<p>The next step is a normalizing step. <span>A <kbd>normalizer</kbd> normalizes the scales for the different <kbd>features</kbd> so that different-sized articles are not weighted differently:</span></p>
<pre class="mce-root">val normalizer = new Normalizer().setInputCol("mailIDF").setOutputCol("features")</pre>
<p class="mce-root">Let's use the Naive Bayes algorithm now. Initialize it and pass into it the hyperparameters it needs:</p>
<pre class="mce-root">val naiveBayes = new NaiveBayes().setFeaturesCol("features").setPredictionCol("prediction")</pre>
<p>Now it's time to create the pipeline and set up all the stages in it. These are as follows:</p>
<ul>
<li><kbd>StopWordRemover</kbd></li>
<li><kbd>HashingTF</kbd></li>
<li><kbd>mailIDF</kbd></li>
<li><kbd>normalizer</kbd></li>
<li><kbd>naiveBayes</kbd></li>
</ul>
<p>The code snippet sets up the stages as follows:</p>
<pre class="mce-root">val spamPipeline = new Pipeline().setStages(Array[PipelineStage](mailTokenizer2) ++<br/>                                                                                            Array[PipelineStage](stopWordRemover) ++<br/>                                                                                           Array[PipelineStage](hashMapper) ++<br/>                                                                                          Array[PipelineStage](mailIDF) ++<br/>                                                                                          Array[PipelineStage](normalizer) ++<br/>                                                                                          Array[PipelineStage](naiveBayes)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Fit the pipeline to the training documents:</p>
<pre>val mailModel1 = spamPipeline1.fit(trainFeaturizedDFNew)</pre>
<p class="mce-root">Make predictions on the test dataset:</p>
<pre class="mce-root">val rawPredictions = mailModel1.transform(testFeaturizedDF.drop("mailFeatureWords","noStopWordsMailFeatures","mailFeatureHashes"))<br/> </pre>
<p>Now we will display generated raw predictions:</p>
<pre class="mce-root">rawPredictions.show(20))</pre>
<p>Note that they are not two tables. It is one table broken in two for visual clarity:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/582c3cee-80bb-4450-9080-93be48bd420e.jpg" style="width:53.75em;height:25.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Raw predictions table</div>
<p>This is the final step, where we only want to show the relevant columns in the predictions table. The following line of code will drop those columns that are not required:</p>
<pre>val predictions = rawPredictions.select($"lowerCasedSentences", $"prediction").cache</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Display the final <kbd>predictions</kbd> table. We only need the first and the last column. The <kbd>label</kbd> column is the predictions the model generated:</p>
<pre class="mce-root">predictions.show(50)</pre>
<p>It displays predictions as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cfb85d22-fda0-4bcd-8228-3ecaa8009df4.jpg" style="width:20.33em;height:15.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Predictions</div>
<p>We are done, so <kbd>stop</kbd> the <kbd>session</kbd>:</p>
<pre class="mce-root">session.stop()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we created a spam classifier. We started with two datasets, one representing ham and the other, spam. We combined both datasets into one combined corpus that we put through a set of preprocessing steps, as mentioned in the <em>Implementation steps</em> section.</p>
<p><br/>
In the next chapter, we will build on some of the techniques learned so far to create a fraud detection ML application.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>Here are 10 questions that will help to reinforce all of the learning material presented in this chapter:</p>
<ol>
<li> Is the spam classification task a binary classification task?</li>
<li>What was the significance of the hashing trick in the spam classification task?</li>
<li>What is hashing collision and how is it minimized?</li>
<li>What do we mean by inverse document frequency?</li>
<li>What are stop words and why do they matter?</li>
<li>What is the role played by the Naive Bayes algorithm in spam classification?</li>
<li>How do you use the <kbd>HashingTF</kbd> class in Spark to implement the hashing trick in your spam classification process?</li>
<li>What is meant by the vectorization of features?</li>
<li>Is there a better algorithm that you can think of to implement the spam classification process? </li>
<li>What are the benefits of spam filtering, and why do they matter in business terms?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>The following paper is a comprehensive work that deserves reading:</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2405882316300412">https://www.sciencedirect.com/science/article/pii/S2405882316300412</a></p>


            </article>

            
        </section>
    </body></html>