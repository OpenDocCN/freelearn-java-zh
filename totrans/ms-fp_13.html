<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Use Case - A Parallel Web Crawler</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed the actors model and a framework that you can use to program with actors. However, the actor model is a paradigm just like functional programming. In principle, if you know the actors model from one language, you can use it in another language even if it doesn't have a framework that supports the actor model. This is because an actor model is an approach to reasoning about parallel computations, not some language-specific set of tools.</p>
<p>This state of things, just like functional programming, has its own benefits and drawbacks. The benefit is that you are not language-dependent if you depend on concepts. Once you know a concept, you can come to any programming language at all, and be capable of using them. However, the learning curve is steep. Precisely because it is all about paradigm and approach, it is not enough to install a library and start using it after skimming its documentation, as it is frequently the case with many other libraries or general-purpose languages that are similar one to another. Since it is all about shifting the paradigm, you need to apply some learning effort in order to understand the paradigm and how to use it in practice.</p>
<p>In the previous chapter, we built up a theoretical foundation on the toolset the actor model provides, as well as discussed the main concepts of the actor model and how to use it in practice. However, since it is a set of ideas and not just a library, it is necessary to develop an intuition for how it works. The best way to develop an intuition for a new paradigm is to have a look at some practical examples that will demonstrate how this paradigm is applicable in practice.</p>
<p>For this reason, in this chapter, we will be looking at a practical example of where an actors model can be applied. We will be looking at an example of web crawling.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Problem statement</li>
<li>Sequential solution</li>
<li>A parallel solution with Akka</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem statement</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be addressing the problem of building a web crawler. Web crawlers are important, for example, in the domain of indexing and searching the web.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The graph structure of the web</h1>
                </header>
            
            <article>
                
<p>All the websites can be imagined as a graph of pages. Every page contains some HTML markup and content. As part of this content, most web pages contain links to other pages. Since links are supposed to take you from one page to another, we can visualize the web as a graph. We can visualize links as edges that take you from one node to another node.</p>
<p><span>Given such a model for the entire internet, it's possible to address the problem of searching for information over the web:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/00be260a-4a75-4bdd-975a-388e35442e46.png" style="width:34.08em;height:10.17em;"/></p>
<p> We are talking about a problem that is faced by the search engines. The objective of a search engine is to index the information stored online and devise algorithms that will find the required pages—nodes of the graph—efficiently based on certain queries by the end user. The algorithms required to index those nodes and to match the user requests to the information stored in the nodes are complex, and we will not be addressing them in this chapter. Often, these algorithms involve advanced machine learning and natural-language processing solutions to understand and evaluate the content stored in the pages. The brightest minds in machine learning and natural language processing work on search tasks for companies such as Google.</p>
<p>Therefore, in this chapter, we will be addressing a task that is also faced by search engines but is easier to tackle. Before indexing the information stored online, this information needs to be collected. A collection of information is a task of traversing the graph of all the pages and storing the contents in a database. The task of web crawling is precisely the task of traversing the graph starting from one node and following to other nodes through links. In the example in this chapter, we will not be storing website information in a database but will focus on the web crawling task.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting information from the graph</h1>
                </header>
            
            <article>
                
<p>Collecting information from a graph can be depicted as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58b28bfb-db22-430a-9b50-688c11ad0df5.png" style="width:34.08em;height:17.17em;"/></p>
<p>In the preceding illustration, the blue nodes are the ones currently being processed, the green ones are the ones that have already been processed, and the white ones are the ones not yet processed. This preceding task can be implemented as follows:</p>
<ol>
<li> Specify which URL you would like to start from, that is, the starting node from which the traversal will start.</li>
</ol>
<p> </p>
<ol start="2">
<li>The crawler will visit the URL by issuing a <kbd>GET</kbd> request to it, and receive some HTML:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9cd5cfe4-0b52-4532-b1cb-7493a19da150.png"/></p>
<ol start="3">
<li>After receiving this HTML text, the crawler will extract the edges of the current node—the links to other nodes. The links are well defined in HTML markup. They are specified as follows:</li>
</ol>
<pre style="padding-left: 60px">&lt;a herf="link_address"&gt;link_text&lt;/a&gt;</pre>
<p>So, once we have the HTML markup, we can look it up for the contents matching the preceding pattern. After that, we may want to collect all of the links presented on the page into one single collection structure, for example, a list. Once we have that, we may want to execute the same operation recursively on every node linked to the current one.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parallel nature of the task</h1>
                </header>
            
            <article>
                
<p>The preceding task was chosen as an example for this chapter because it is parallelizable by nature. Basically, whenever you have a bunch of tasks that are not dependent one on another, it may be a good idea to parallelize the tasks. In our example, the preceding task can be very well conceptualized as a set of independent operations. A single unit of operation is the task of visiting a URL, abstracting all of the other nodes that are linked by the page of this URL, and then executing the same task on these URLs recursively.</p>
<p>This makes our crawling example a very good case for parallel actor application. Running all these tasks sequentially for every URL may be very time consuming, and therefore it is much more efficient to apply some strategy to the tasks. Even on a single-core CPU computer  processing power, even in the case when most of the tasks are processed in a simulated parallelism manner, the operation will still be dozens of times more efficient than in the sequential case. This is because a lot of waiting is involved in requesting HTML from the internet or a <kbd>GET</kbd> request.</p>
<p><span>In the succeeding diagram, you can see an example of a request to have a process performed from a single thread:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c90f1fef-650c-4480-9c22-3a781cfad746.png" style="width:12.42em;height:15.08em;"/></p>
<p> So, as you can see, the thread issues a request, and then it waits for the response to be derived. From your own browsing experience, you can easily say that sometimes a response from a website may take seconds to arrive. So essentially, these seconds will be wasted by the thread because it will block on this task, and the processor will do nothing useful at the time.</p>
<p class="mce-root"/>
<p>Even when you have a single core and even when you have only an option of simulated parallelism, the processor can still be utilized much more efficiently, if when one thread waits for the request to arrive at it, another thread issues a request or processes a response that has already arrived at it.</p>
<p><span>As you can see from the diagram, requests are issued much more efficiently, and while one thread waits for a request and sleeps, the processor is busy with other threads that have a real job to do:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/68bdca1f-90ae-44a2-84c5-23f3594327a5.png" style="width:31.75em;height:24.50em;"/></p>
<p>In general, such kind of blocking can be very frequently the case. For example, you may have some operations that wait on a database to give a response, or on a message to arrive from the outer world. So even in the case of simulated parallelism, these kinds of situations can be greatly sped up. Needless to say, most modern machines work in a setting of several cores that will speed up the execution greatly.</p>
<p class="mce-root"/>
<p>Now that we know the exact specification of the task, and we why exactly it makes sense to parallelize this task, let's get our hands dirty and see some implementation in action. However, before we dive into the details of the actor-based implementation of the task, let's first have a look at how this can be implemented sequentially, just so that we have a baseline to work against.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequential solution</h1>
                </header>
            
            <article>
                
<p>First of all, let's have a look at how we would like to use the solution once it is built. Also, let's have a look at exactly which output we expect from the solution:</p>
<pre>val target = new URL("http://mvnrepository.com/")<br/>val res = fetchToDepth(target, 1)<br/> println(res.take(10).mkString("\n"))<br/> println(res.size)</pre>
<p>In the preceding code, we perform the following things. First of all, we would like to be able to define our URL in terms of a Java-native URL object. In this example, we are using the Maven repository website as a crawling target. <a href="http://mvnrepository.com">mvnrepository.com</a> is a website that provides an easy search through all of the Maven packages.</p>
<p>After that, we have a call to <kbd>fetchToDepth</kbd>. This is the method that is supposed to do the actual work for us and actually crawls the website for links. As a first argument, we supply it a URL we want to process. As the second argument, we supply it the depth to which you want to search the graph.</p>
<p>The idea behind having the depth is to avoid infinite recursion. The web is a very interconnected place. So, when we start from one node and begin to descend recursively into the nodes it is connected to, it may take a very long time to arrive at the terminal nodes, that is, a node that does not have any further links. It is quite possible that for the majority of the websites out there, such a lookup will be infinite or will take an unreasonably long time to accomplish. Therefore, we would like to limit our lookup depth. The semantics of how it will work is that for every edge the algorithm searches, the depth will be decreased by one. Once the depths reach zero, the algorithm will not try to follow the edges any further.</p>
<p>The result of this from function-execution will be a collection. Precisely, a set of links that were collected from this website. After that, we take the first 10 links and output them to the standard output, one link per line. Also, we output the number of links that the system managed to extract in total. We do not print all the links because there will be too many of them.</p>
<p>The output we are aiming to get is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/43cc0e31-f16f-4d3e-9980-5359a30fd57e.png"/></p>
<p>Next, let's explore the sequential implementation of the preceding objectives. Let's have a look at <kbd>fetchToDepth</kbd>:</p>
<pre>def fetchToDepth(url: URL, depth: Int, visited: Set[URL] = Set()): Set[URL]</pre>
<p>As you can see, the function accepts the three arguments. Two of them are the ones that we have already seen before and discussed in our example API usage. The third one is the set of URLs that the crawler has already visited. Why might we have such a set? Why does the crawler need it at all? How will it behave if we do not store the set of all visited links?</p>
<p>In fact, the same set must be stored for any similar graph-traversal situation. The problem here is the presence of cycles in a graph.</p>
<p class="mce-root"/>
<p><span>In the diagram below, you can see how a graph with cycles might emerge in a web crawler application:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8380c4c1-2f62-4079-8de7-7fb31dcfa4ae.png" style="width:23.75em;height:4.83em;"/></p>
<p> It is enough for website <strong>A</strong> to reference website <strong>B</strong>, and then website <strong>B</strong> references website <strong>A</strong> for a cycle to emerge. If the graph has a cycle, there is a risk that our traversal algorithm will enter an infinite loop if it tries to traverse it without keeping track of the nodes that it has already visited. For this reason, we need to keep track of the visited nodes. <span>The first thing to do in the </span><kbd>fetchToDepth</kbd><span> function is to fetch the given URL:</span></p>
<pre>val links = fetch(url).getOrElse(Set())</pre>
<p>We are using the following function to do fetch the given URL:</p>
<pre>def fetch(url: URL): Option[Set[URL]] = Try {<br/>  Jsoup.connect(url.toString)<br/>    .get<br/>    .getElementsByAttribute("href")<br/>    .asScala.map( h =&gt; new URL(url, h.attr("href")) ).toSet<br/>}.toOption</pre>
<p>The <kbd>fetch</kbd> function is a function that takes a URL and outputs <kbd>Option[Set[URL]]</kbd>, the data structure with the links that this page contains.</p>
<p>Inside this function, nothing really remarkable happens except the side effects. We are using <kbd>Try</kbd> to capture the side effect of an error. Then, we convert <kbd>Try</kbd> to <kbd>Option</kbd> because we are not interested in the error type, as explained below.</p>
<p>The logic of this function is straightforward. We are using a popular Java library, called <kbd>Jsoap</kbd>, in order to connect to the given URL, issue a get request, retrieve all of the elements that have an <kbd>href</kbd> attribute that contains a link, and afterwards, map the resulting elements by extracting their links.</p>
<p>The logic is wrapped into <kbd>Try</kbd>, which is later converted into <kbd>Option</kbd>. Why have such a system involving <kbd>Try</kbd> and <kbd>Option</kbd>? As we learned in the first part of this book, both <kbd>Try</kbd> and <kbd>Option</kbd> are effect types. The entire idea behind an effect type is to abstract away some side effects that might happen. Since our business logic is wrapped into the <kbd>Try</kbd> effect, some side effects that are abstracted by <kbd>Try</kbd> might happen in this logic.</p>
<p><kbd>Try</kbd> abstracts away the possibility of an error or an exception. In fact, a lot of things can go wrong when we are performing a request to a remote server. For example, the request can time out, or we may lose internet connection, or the server can return an erroneous status code.</p>
<p class="mce-root"/>
<p>In the context of massive data processing, things inevitably go around for some of the data nodes. This means that when performing the crawling operation discussed previously, we can be almost certain that sometimes it will fail. What happens if you design a function that does not account for such errors? Most likely, our algorithm will encounter an exception and will fail midway through the dataset. This is why fault tolerance is a crucial property of this kind of system.</p>
<p>In the preceding example, fault tolerance is achieved by wrapping the possibility of an error into the <kbd>Try</kbd> effect type. Another thing to notice here is not only the <kbd>Try</kbd> type but the fact that it is converted into <kbd>Option</kbd>. <kbd>Option</kbd> indicates that a function may or may not produce a result. In the context of converting from <kbd>Try</kbd> to <kbd>Option</kbd>, <kbd>Try</kbd> is mapped to <kbd>Some</kbd> if it is a successful attempt, and it is mapped to <kbd>None</kbd> if it is a failure. In the case of a failure, some information about the error that has happened gets lost when we convert <kbd>Try</kbd> to <kbd>Option</kbd>. This fact reflects the attitude that we don't care about the type of the error that might occur.</p>
<p>This kind of attitude demands further explanation, as it is usually the case everywhere where we intentionally drop data. We discussed that in this kind of massive data-processing, errors are inevitable. Of course, it is nice to have your algorithm catch and handle gracefully most of the errors that it will encounter, however, it is not always possible. You cannot remove the fact that sooner or later the algorithm will encounter an error since the algorithm works not on some dummy data but on real-world websites. Whenever you are working with real-world data, errors are a fact of life, and you cannot plan to handle every possible error that occurs gracefully.</p>
<p>Hence, a proper way to structure a program that is certain to encounter errors is fault tolerance. This means that we build the program as if it is certain to encounter errors and we plan for recovering from any error without actually specifying the type of the error. The fact that we are converting our <kbd>Try</kbd> to <kbd>Option</kbd> here reflects the attitude toward errors. The attitude is that we don't care about which error might happen during execution, we only care that an error can happen.</p>
<p>Fault tolerance is one of the fundamental principles of the design of Akka. Akka is built with fault tolerance and resilience in mind, which means that actors are designed as if they are certain to encounter errors, and Akka provides you with a framework to specify how to recover from them.</p>
<p>Let's now return to our <kbd>fetch</kbd> line from the <kbd>fetchToDepth</kbd> method  and see how exactly this attitude plays out in the logic of our example:</p>
<pre>fetch(url).getOrElse(Set())</pre>
<p>Let's return to our <kbd>fetchToDepth</kbd> example. After we perform the affect statement on a given URL, we also perform a <kbd>getOrElse</kbd> call on it. <kbd>getOrElse</kbd> is a method defined on a Scala <kbd>Option</kbd> that has the following signature:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f482272d-d39f-40fc-ab68-f4b6fbea04c7.png"/></p>
<p>So basically, this method tries to extract a value from <kbd>Option</kbd>. If the value is not present in <kbd>Option</kbd>, it outputs the default value that is supplied to the method. <kbd>getOrElse</kbd> is equivalent to the following code:</p>
<pre>type A<br/>val opt: Option[A]<br/>val default: A<br/><br/>opt match {<br/>  case Some(x) =&gt; x<br/>  case None =&gt; default<br/>}</pre>
<p>So now we have a set of links extracted from a given link. Let's now have a look at how it is traversed in our sequential example:</p>
<pre>if (depth &gt; 0) links ++ links<br/> .filter(!visited(_))<br/> ./*...*/</pre>
<p>In the preceding code, you can see a chunk of the code in the <kbd>fetchToDepth</kbd> method. First of all, we check whether the depth is greater than <kbd>0</kbd>. Previously, we discussed how specifying depth constraints could save us from infinite execution.</p>
<p>The first statement after the <kbd>if</kbd> check is the current link accumulator, the <kbd>links</kbd> variable, being united with some larger right-hand statement. In the snippet above, only a fragment of this statement is presented, and we will discuss it step by step. The big picture is that this statement applies <kbd>fetchToDepth</kbd> recursively to all the links in the <kbd>links</kbd> set.</p>
<p class="mce-root"/>
<p>This means that the right side of the <kbd>++</kbd> operator takes every link from the set that we have retrieved and extracts all the links present in its page. This must be done recursively. But first, we need to clean up the set of all abstracted URL from the current link so that it doesn't contain the links that we have previously visited. This is to tackle the problem of cycles in the graph:</p>
<pre>.toList<br/>.zipWithIndex</pre>
<p>Next, we have the resulting set further transformed by the call to the  <kbd>toList</kbd> and <kbd>zipWithIndex</kbd> methods. Basically, these two methods are required for logging purposes. Inside data-processing operations, you would like to have some reporting in order to keep track of the operations. In our case, we would like to assign a numeric ID to every link that we are going to visit, and we are going to log the fact that we are visiting a link with a given ID into the standard output. The signature of <kbd>zipWithIndex</kbd> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6548e0a2-7673-4eb5-b667-92c1edb3e21a.png"/></p>
<p>So basically, a list of elements, <kbd>A</kbd>, becomes a list of pairs of <kbd>A</kbd> and <kbd>Int</kbd>:</p>
<pre>.foldLeft(Set[URL]()) { case (accum, (next, id)) =&gt;<br/> println(s"Progress for depth $depth: $id of ${links.size}")<br/> accum ++ (if (!accum(next)) fetchToDepth(next, depth - 1, accum) else Set())<br/> }</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>In the preceding code, you can see the logic that actually performs the processing of every given URL. We are using a <kbd>foldLeft</kbd> method. Let's have a look at its signature:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d6848006-5a06-4b69-8003-96f70212be9a.png"/></p>
<p>Basically, this method folds your entire collection into a single value. For a simpler example of its use case, consider the following chunk of code:</p>
<pre>(1 to 10).toList.foldLeft(0) { (total, nextElement) =&gt; total + nextElement }</pre>
<p>So basically, we start with some empty accumulator and specify what to do for each element given a value accumulated so far in scope. Every subsequent element of the list must be added to the accumulator.</p>
<p>Returning to our example of <kbd>fetchToDepth</kbd>, the justification for the usage of <kbd>foldLeft</kbd> is as follows. Ultimately, we have a set of links, and for every element of this set, we need to calculate the set of URLs it links to. However, we are interested in a combined set with all the links from all of the URLs we have just called. Hence, the example is similar to the example of the addition of integers. Except here, we are computing a union over a collection of sets of links.</p>
<p>Let's now have a look at the <span>code block that is passed as the second argument to</span> <kbd>foldLeft</kbd>. First, we perform a logging statement. Next, we perform a step of calculating the links that belong to the current URL. We are then adding these links into the combined accumulator.</p>
<p class="mce-root"/>
<p>Notice also that before actually crawling a link, we perform a check to see whether it is contained already in the accumulator. It is crawled only if it is not contained in the current accumulator. This check is used to prevent redundant work. If the link is already contained in the accumulator, this means it was already processed by the algorithm since it is a depth-first traversal of the graph. So we do not need to process it again.</p>
<p>Also, notice that we perform the processing using a recursive call to the <kbd>fetchToDepth</kbd> function with the depth reduced by one:</p>
<pre>else links</pre>
<p>Finally, if the depth is <kbd>0</kbd>, we will return the set of links that we have extracted from the current page. Which means we will stop the algorithm at this point.</p>
<p>The entire code for the <kbd>fetchToDepth</kbd> function looks as follows:</p>
<pre>def fetchToDepth(url: URL, depth: Int, visited: Set[URL] = Set()): Set[URL] = {<br/>    val links = fetch(url).getOrElse(Set())<br/><br/>    if (depth &gt; 0) links ++ links<br/>      .filter(!visited(_))<br/>      .toList<br/>      .zipWithIndex<br/>      .foldLeft(Set[URL]()) { case (accum, (next, id)) =&gt;<br/>        println(s"Progress for depth $depth: $id of ${links.size}")<br/>        accum ++ (if (!accum(next)) fetchToDepth(next, depth - 1, accum) else Set())<br/>      }<br/>      .toSet<br/>    else links<br/>  }</pre>
<p>Next, let's discuss what a parallel solution to the preceding problem might look like.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A parallel solution with Akka</h1>
                </header>
            
            <article>
                
<p>Before approaching the problem of parallelizing our crawler, let's discuss our strategy of how we are going to approach it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategy</h1>
                </header>
            
            <article>
                
<p>We are going to model our problem of creating the actors' system with a tree of actors. This is because the task itself naturally forms a tree.</p>
<p>We have already discussed how all the links and pages on the internet comprise a graph. However, we have also discussed, in our example, two things that are not desirable when processing and traversing this graph—cycles and repetitions of work. So, if we have visited a certain node, we are not going to visit it anymore.</p>
<p>This means that our graph becomes a tree on processing time. This means you cannot get to a parent node from a child node when descending from a child node. This tree may look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c37fdf35-76cc-4ae1-bee1-f217dfb18333.png" style="width:16.17em;height:16.33em;"/></p>
<p>We are going to communicate with the root-level node. So basically, we will send this node a link that we are going to start crawling from, and afterward, this actor is going to crawl it. Once it extracts all the links from that link, it will for each such link spawn a child actor that will do the same operation on that link. This is a recursive strategy.</p>
<p>These worker actors will report the results to their parent actors. The results from the bottom-most actors are going to get propagated to the top-level actors all the way through the tree.</p>
<p>In this implementation, notice how we do not restrict our usage of actors. Every link gets worked on by a dedicated actor, and we don't really care about how many actors will get spawned. This is because actors are very lightweight primitives, and you should not be afraid to be generous with spawning actors.</p>
<p>Let's have a look at how to implement this strategy.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article>
                
<p>First of all, let's have a look at the API we want and its usage:</p>
<pre>val system = ActorSystem("PiSystem")<br/>val root = system actorOf Worker.workerProps<br/><br/>(root ? Job(new URL("http://mvnrepository.com/"), 1)).onSuccess {<br/>  case Result(res) =&gt;<br/>  println("Crawling finished successfully")<br/>  println(res.take(10).mkString("\n"))<br/>  println(res.size)<br/><br/>}</pre>
<p>In the preceding code, you can see the main method of the parallelized actor application. As you can see, we create an actor system and a root-level worker actor. The idea is to represent all of the processing actors as a single class, <kbd>Worker</kbd>. The job of a single worker is to process a single URL and spawn additional workers to process the links extracted from it.</p>
<p>Next, you can see an example of querying the actor. First of all, we have the following line:</p>
<pre>(root ? Job(new URL("http://mvnrepository.com/"), 1)).onSuccess {</pre>
<p>Ordinarily, you send messages to actors using the <kbd>!</kbd> operator. However, here we are using <kbd>?</kbd> because we are using an ask pattern. In essence, <kbd>?</kbd> is the act of sending a message to an actor and waiting for it to respond with another message. The ordinary <kbd>!</kbd> returns <kbd>Unit</kbd>, however, the <kbd>?</kbd> ask pattern returns a <kbd>Future</kbd> of a possible response:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ca415ddb-fd39-4a22-9aa4-5026564482b7.png" style="width:47.33em;height:5.58em;"/></p>
<p>So essentially, we are sending a message <kbd>Job</kbd> to the root actor, and we are expecting some response in <kbd>Future</kbd>.</p>
<p><kbd>Job</kbd> is an ordinary case class that specifies the URL we are going to scrape, as well as the depth to which we are going to descend. The semantics of these two parameters are the same as that in the sequential example.</p>
<p>After that, we call the <kbd>onSuccess</kbd> method on <kbd>Future</kbd> to set the callbacks. So, once the result arrives at our location, we are going to report to the standard output.</p>
<p>Now, let's have a look at how the worker actor is implemented and how it works:</p>
<pre>def receive = awaitingForTasks</pre>
<p>In the preceding code, the <kbd>receive</kbd> method is implemented in terms of another method called <kbd>awaitingForTasks</kbd>. This is because we are going to change the implementation of <kbd>receive</kbd> in the future based on the information that is sent to the actor in the messages. Basically, our actor is going to have several states in which it is going to accept different sets of messages and react to them differently, but the idea is to encapsulate every state in a separate callback method, and then switch between the implementations of these callbacks.</p>
<p>Let's take a look at the default state of the actor, <kbd>awaitingForTasks</kbd>:</p>
<pre>def awaitingForTasks: Receive = {<br/> case Job(url, depth, visited) =&gt;<br/> replyTo = Some(sender)<br/><br/> val links = fetch(url).getOrElse(Set()).filter(!visited(_))<br/> buffer = links<br/>/*...*/</pre>
<p><kbd>awaitingForTasks</kbd> specifies how an actor should react when it receives the <kbd>Job</kbd> message. The message is supposed to tell the actor to start crawling a certain URL for a certain depth. Also, as you can see, we store all the visited nodes in a <kbd>visited</kbd> collection. Visited is a set of all visited URLs, and the semantics of it and some motivation for it are the same as in the sequential example, to avoid repeating unnecessary work.</p>
<p>After that, we set the <kbd>replyTo</kbd> variable. It specifies the sender of the task, which is interested in receiving the result of our crawling.</p>
<p>After setting this variable, we start the process of crawling. First of all, we are using the <kbd>fetch</kbd> method we are already familiar with from the sequential example to obtain the set of all the links present on a given URL's page, and filter out the links that we have already visited.</p>
<p class="mce-root"/>
<p>After that, similar to our sequential example, we check whether the depth permits further descent, and if it does, we are going to perform the recursive processing of links as follows:</p>
<pre>if (depth &gt; 0) {<br/>  println(s"Processing links of $url, descending now")<br/><br/>  children = Set()<br/>  answered = 0<br/><br/>  for { l &lt;- links } dispatch(l, depth - 1, visited ++ buffer)<br/>  context become processing<br/>}</pre>
<p>First, we are going to define an empty set of the child actors so that we can keep track of their processing and be able to control our own state depending on the state change of the child actors. For example, we must know when exactly to report to the requesting actor with the results of the job. That must be done only once all the child actors have finished their work.</p>
<p>Also, we set the <kbd>answered</kbd> variable to <kbd>0</kbd>. This is a variable to keep track of the number of actors who have successfully replied to this actor with the result of their processing. The idea is that once this metric reaches the <kbd>children</kbd> size number, we are going to reply to the <kbd>replyTo</kbd> actor with the result of the processing. The most interesting method here is <kbd>dispatch</kbd>:</p>
<pre>def dispatch(lnk: URL, depth: Int, visited: Set[URL]): Unit = {<br/>  val child = context actorOf Worker.workerProps<br/>  children += child<br/>  child ! Job(lnk, depth, visited)<br/>}</pre>
<p>So, <kbd>dispatch</kbd> creates a new worker actor and adds it to the set of all the child actors, and finally, they are requested to perform a processing job on a given URL. A separate worker actor is initialized for a separate URL.</p>
<p>Finally, let's pay attention to <kbd>context become processing</kbd> line of the <kbd>Job</kbd> clause. Essentially, <kbd>context become</kbd> is a method that switches the implementation of this actor's <kbd>receive</kbd>. Previously, we had an implementation of <kbd>awaitingForTasks</kbd>. However, now we are switching it to <kbd>processing</kbd>, which we will be discussing further in this chapter.</p>
<p class="mce-root"/>
<p>But before discussing it, let's have a look at their <kbd>else</kbd> branch of our <kbd>if</kbd> statement:</p>
<pre>else {<br/> println(s"Reached maximal depth on $url - returning its links only")<br/> sender ! Result(buffer)<br/> context stop self<br/>}</pre>
<p>So, as we can see, once you have reached a certain depth, we are going to return the links collected into a buffer to the requesting actor.</p>
<p>Now let's have a look at the <kbd>processing</kbd> state of the actor:</p>
<pre>def processing: Receive = {<br/>  case Result(urls) =&gt;<br/>    replyTo match {<br/>      case Some(to) =&gt;<br/>        answered += 1<br/>        println(s"$self: $answered actors responded of ${children.size}")<br/>        buffer ++= urls<br/>        if (answered == children.size) {<br/>          to ! Result(buffer)<br/>          context stop self<br/>        }<br/><br/>      case None =&gt; println("replyTo actor is None, something went wrong")<br/>    }<br/>}</pre>
<p>As you can see, once this actor becomes processing actor, it is going to react to the <kbd>Result</kbd> messages, and it will stop reacting to the <kbd>Job</kbd> messages. This means that once you have sent a <kbd>Job</kbd> message to the actor and it starts processing it, it will no longer accept any other job request.</p>
<p>In the body of <kbd>processing</kbd>, we make sure that the <kbd>replyTo</kbd> actor is set. In principle, it should be set at all times once we reach this point. However, <kbd>replyTo</kbd> is an <kbd>Option</kbd>, and a nice way to handle optionality would be to have a <kbd>match</kbd> statement that explicitly checks for this <kbd>Option</kbd> to be defined. You never know what bugs can occur in such a program, so it's better to be safe than sorry.</p>
<p>The logic of <kbd>processing</kbd> is as follows. The <kbd>Result</kbd> is a message that is supposed to arrive at this actor from its child actors. First, we are going to increment the number of actors that have answered this actor. We are doing that with <kbd>answered += 1</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>After some debugging output, we add the payload that was sent to this actor by the child actor to the set of all the links collected by this actor—<kbd>buffer ++ = urls</kbd>.</p>
<p>Finally, we check whether all of the child actors have replied. We do so by checking the <kbd>answered</kbd> counter to be equal to the size of all the children. If it is, then we respond to the requesting actor with the links that we have collected, <kbd>to ! Result(buffer)</kbd>, and finally we stop this actor because it has nothing else to do, <kbd>context stop self</kbd>.</p>
<p>The result of running this actor system is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/57ec1db9-1c9d-4473-a6a5-b3fd289d3800.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caveats</h1>
                </header>
            
            <article>
                
<p>Actor applications, although much more convenient to write and reason about compared to synchronization-based applications, are still much trickier than your ordinary sequential applications. In this section, we will discuss some caveats of the application.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visited links</h1>
                </header>
            
            <article>
                
<p>The caveat that has the most impact here is the partiality of tracking of the actors that this actor has already visited. If you remember, in the sequential example, we are using the <kbd>foldLeft</kbd> function to accumulate the results of every URL processing, and at all times we have a complete, up-to-date list of all the URLs collected by the entire application. This means the recursive crawling calls always have a full picture of what the application has collected so far.</p>
<p><span>In the diagram, we see a sequential example of processing with </span><kbd>foldLeft</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/51aacb5f-28ed-45a8-8e98-195ebc6998cd.png" style="width:41.33em;height:15.08em;"/></p>
<p>All the nodes that were already processed are highlighted in green, and the current URL is highlighted in blue. The current URL has the entire list of links that were collected previously. So, it is not going to process them. This situation is possible because the processing is done sequentially.</p>
<p>However, the situation is different for the parallel example described in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/87ba4746-11ba-42f2-8e09-1a0afa45c645.png" style="width:35.25em;height:12.50em;"/></p>
<p class="mce-root"/>
<p>In the preceding diagram, in blue, you can see the current node. The green nodes are all the nodes the current node is aware of. Notice that although it is aware of its sibling nodes, it is not aware of the result of processing these sibling nodes. This is because the processing is done in parallel. In the sequential example, we had a depth-first tree-traversal from left to right. However, in the parallel example, the children are processed in parallel with each other. This means that one node may have information about its siblings. However, it will not have the information of the results collected from its siblings. This is because these results are collected in parallel with this node computing its own results. And we do not know which node will finish first. This means the preceding application is not ideal in terms of eliminating redundant work.</p>
<p>The problem of storing all the links the actors have visited is a classic problem of a shared mutable resource. A solution to this problem within an actor model would be to create a single actor that has a list of all the links already visited and that do not require further visiting. And so, before descending through the tree, every actor should consult that actor on the subject of whether or not to process certain links.</p>
<p>Another caveat that you should consider is fault tolerance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fault tolerance</h1>
                </header>
            
            <article>
                
<p>To keep the example simple, in the parallel example we have used the <kbd>fetch</kbd> function from the sequential example to fetch the contents of a certain URL:</p>
<pre>val links = fetch(url).getOrElse(Set()).filter(!visited(_))</pre>
<p>The motivation for having this function return an <kbd>Option</kbd> was fault tolerance in the sequential example—if the result cannot be computed, we return a <kbd>None</kbd>. However, in an actor setting, Akka provides you with a framework to specify what to do if an actor has failed. So in principle, we could further refine our example with a dedicated <kbd>fetch</kbd> function that is perfectly capable of throwing exceptions. However, you might want to specify the actor-level logic on how to restart itself and how to keep its state through this kind of emergency.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Counting the responded actors</h1>
                </header>
            
            <article>
                
<p>In the example, we counted the child actors that responded to an actor in order to determine when an actor is ready to respond to its parent actor:</p>
<pre>answered += 1<br/>// ...<br/>if (answered == children.size) {<br/>  to ! Result(buffer)<br/>  context stop self<br/>}</pre>
<p>This scenario can give rise to certain undesirable outcomes. First of all, it means the time that it takes for the system to respond is equal to the time it takes for the deepest and slowest link to be resolved and processed.</p>
<p>The logic behind this reasoning is as follows. The <kbd>processing</kbd> callback determines when a  node of a processing tree is considered completed:</p>
<pre>if (answered == children.size)</pre>
<p>So a  node is completed whenever its slowest child is completed. However, we should remember that we are dealing with real-world processing, and we should not forget about all of the side effects that can happen in the real world. The first effect we've discussed is failures and errors. We address this side effect by engineering our application to be fault-tolerant. The other side effect is time. It can still be the case that some of the pages take unforgivably long to get fetched. So we must not forget that such a side effect can occur and might want to devise a strategy to tackle such side effects.</p>
<p>An intuitive strategy is timeouts. Just like in the case of fault tolerance, whenever a chunk of data takes too long to get processed, we can drop the chunk of data. The idea is that we still have enough data and for many applications, it is not crucial to have a 100% recall of all the target data.</p>
<p>Concretely, you might want to have a message scheduled to be sent to the current actor. Upon receiving this message, the actor will immediately send back all its results to the <kbd>replyTo</kbd> actor and terminate itself so that it doesn't react to any further messages. Also, the reaction to such a message may be to kill all the child actors recursively, because there is no longer a need for them to exist since they will not be able to report this error data through the parent actor. Another strategy may be to propagate this timeout message recursively to the child actors without killing them immediately. The child actors will then return whatever progress they have made at once and terminate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-world side effects</h1>
                </header>
            
            <article>
                
<p>In this subsection, we have already seen two instances of side effects—errors and time. The nature of the real world is such that often you don't know which side effects you will encounter.</p>
<p>Such side effects can include the necessity to limit the number of requests sent to a given domain because certain websites tend to block entities who send too many requests, or we might want to use proxy servers to access certain websites that are not accessible otherwise. Certain websites might have the data stored and fetched via Ajax, and ordinary techniques of scraping usually do not work.</p>
<p>All these scenarios can be modeled as their own side effects. The idea when working with a real-world application is always to have a look at the side effects that might arise in your particular scenario.</p>
<p>Once you have decided what you're going to encounter, you are able to decide how you are going to tackle and abstract the side effects. The tooling you can use includes the actor system's built-in capabilities or the capabilities of purely functional programming that we discussed in the first half of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at developing an application using Akka. First, we developed a sequential solution to the problem. Then, we determined the independent sub-tasks of this solution and discussed how to parallelize them. Finally, we devised a parallel solution using Akka.</p>
<p>Also, we discussed certain caveats that you might encounter when developing this kind of application. Most of them have to do with side effects that may occur, as well as actor-model-specific peculiarities when building parallel applications.</p>


            </article>

            
        </section>
    </body></html>