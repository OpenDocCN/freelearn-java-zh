<html><head></head><body>
      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Benchmarking Your Application</h1>
            
         </header>
         
         
         <article>
            
            
            <p class="mce-root">In the previous chapters, we saw how to develop a Java EE application to ensure it
               could scale later using multiple threads, asynchronous handling, pooled resources,
               and so on. We also saw how to get metrics on the performance and resource (CPU, memory)
               usage of your application and optimize the performance thanks to JVM or container
               tuning, as well as more aggressive techniques such as adding caching to your application.
            </p>
            
            <p>At this point, you should be able to work on the performance. However it does not
               mean you are safe to get surprises when going into production. The main reason is
               that the work we talked about previously is rarely done in an environment close enough
               to the production or final environment the application will be deployed to.
            </p>
            
            <p>To avoid these surprises, benchmarks can (or should) be organized, but it is not as
               easy as taking all we previously learned and putting it all together. It, most of
               the time, requires more preparation that you should be aware of, if you don't want
               to lose precious time when you go to production.
            </p>
            
            <p>In this chapter, you will prepare a benchmark, going through the following points:</p>
            
            <ul>
               
               <li>What a benchmark is</li>
               
               <li>Preparing a benchmark</li>
               
               <li>Iterating during a benchmark</li>
               
               <li>What to do after a benchmark</li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Benchmarking – validating your application against your SLA</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Benchmarks often enter into play when you have a <strong>Service Level Agreement</strong> (<strong>SLA</strong>) to respect. The SLA can be more or less explicit. Typically, you may have a very
               blurry definition of your SLA, <span>such as <em>the application must provide good user experience</em>, or you may have </span>them in a very precise manner in a contract, such as <em>the application must support Black Friday weekend and 10 million users a day, and
                  each user action must be executed in less than one second</em>. There are even some standards to describe the SLA, such as the <strong>Web Service Level Agreement</strong> (<strong>WSLA</strong>) to define how to measure and expose your SLA.
            </p>
            
            <p>In any case, if an SLA is identified, and even more so if you have some compensation
               in your contract if it is not met, it is very important to go through a benchmark
               phase in your project to make sure you increase your performance when going to production.
            </p>
            
            <div class="packt_tip">The next and last chapter of the book will deal with the continuous evaluation of
               your performance and will help you to do it continuously and avoid this <em>phase</em> effect. Although, it is still common to have a dedicated phase because of the infrastructure
               constraints required by a benchmark, so we will consider it the case in this chapter.
            </div>
            
            <p>At this point, you know that you need to validate the performance of your application
               and your project manager, or you, has planned a benchmark. But what is this task about?
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Benchmark, benchmark, benchmark</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Working on performance is not uniform work. We saw that in the previous section; there
               are lots of tools doing that and each of them gives more or less information, but
               also has more or less impact on the actual performance. For instance, instrumenting
               all the code of the application to get metrics on all layers will make the application
               very slow, but the report very rich. On the contrary, instrumenting only some parts—such
               as the outbounds—will not impact the application <span>that much</span>, yet the report will give you only a very small set of data. This means that depending
               on the layer you work on, you will not use the same tools to ensure you have the right
               level of information.
            </p>
            
            <p>Thus, we can distinguish multiple potential benchmark types:</p>
            
            <ul>
               
               <li>The <em>algorithm benchmark</em>: You develop some code sections and want to validate the performance is correct or
                  there is no bottleneck.
               </li>
               
               <li>The <em>layer benchmark</em>: You develop a layer—the persistence layer, front layer, and so on—and want to ensure
                  the performance is correct before adding another layer or integrating it with another
                  part of the application.
               </li>
               
               <li>The <em>sizing</em> benchmark: You get the figures of the application performance to identify the number
                  of machines to use. This is directly related to horizontal scaling—this doesn't work
                  as smoothly as for a vertical one since the performance can't be linear. Note that
                  this is exactly the same kind of logic big data frameworks are based on to distribute
                  their work.
               </li>
               
               <li>The <em>deliverable benchmark</em>: This is the benchmark validating that the application (delivery) and the performance
                  of the final application matches expectations (SLA).
               </li>
               
            </ul>
            
            <p>Of course, we can split the sort of benchmarks you can do into more precise categories,
               but these are the three you will encounter in most projects. Each kind of benchmark
               will use different tools and will have different preparation steps and output. However,
               each of them will validate criteria (one or more) against expectations.
            </p>
            
            <p>In previous benchmarks, we can clearly split the criteria into two very high-level
               categories, but this split will have a huge impact on the way you drive your benchmark:
            </p>
            
            <ul>
               
               <li>The <em>development benchmark</em></li>
               
               <li>The <em>deliverable benchmark</em></li>
               
            </ul>
            
            <p>Even if a benchmark is done at delivery, by definition, this split means that the
               two first categories of benchmarks we identified are about validating the code is
               correctly done, and therefore it belongs to developer work in general, and it is rarely
               split from the development itself. The <em>layer benchmark</em> is generally done during multiple development iterations; it stays a development
               benchmark as it is still about validating an application internally, and not something
               exposed to the end user normally. The <em>deliverable</em> benchmark is about ensuring final performance is acceptable for the end user (or
               contract). It is therefore different from the previous categories of benchmarks because
               you need to have a deliverable complete enough to be tested.
            </p>
            
            <p>In terms of implications, the fact you will work on a deliverable benchmark mainly
               means you will not be able to do it on <em>your machine</em>. What you want is to validate your performance against a contract, so you will need
               to validate the application on the machine it is installed on.
            </p>
            
            <p>At this point, it is important not to get confused between a benchmark to validate
               the SLA and a benchmark to size the infrastructure needed for the application. Both
               will almost look the same and are organized the same way. But in the case of a <em>sizing benchmark</em>, you will define an infrastructure (machine power, memory, and so on) and measure
               performance to then deduce how many machines you need if you horizontally scale. However,
               the <em>SLA benchmark</em> already assumes the infrastructure is fixed and then you just validate the performance
               to encounter the SLA. In practice, both are often done at the same time which leads
               to this confusion between both types of benchmarks. This mainly comes from the fact
               that developers or project managers have an idea of the infrastructure needed for
               an application, so the starting infrastructure for sizing is close to the target one,
               and then the game is only to validate the performance to match the expectations. Nonetheless,
               if you start a sizing benchmark then you will need another benchmark <em>phase</em> to validate the SLA, which can be seen as a second benchmark. Never forget the phase
               you are in; otherwise, you may change too many parameters at the same time and lose
               track of the current state of the application (it is crucial to be able to compare
               benchmark iterations, as we will see later).
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Preparing your benchmark</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Preparing a benchmark is probably the most important task you will have to do. In
               fact, if you miss it, it is guaranteed the benchmark will be a failure and useless.
               Even if tasks are not very complicated in general, they will not be done by themselves.
               So take your time to ensure they are done before the benchmark starts.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Defining the benchmark criteria</h1>
            
         </header>
         
         
         <article>
            
            
            <p>A benchmark is always done to make sure we encounter a metric. Therefore, the first
               step of benchmark preparation is to <em>clearly</em> define this metric.
            </p>
            
            <p class="mce-root">Defining a metric means clearly defining what is measured and how to measure it.</p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Defining the metric</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Defining what is measured means to define the bounds of the measurement. In other
               words, when the metric starts and when the metric ends. This can sound simple to do,
               but don't forget we work in a multi-layer environment and that you can miss some layers
               if your monitoring is not well defined.
            </p>
            
            <p>Here are some examples, based on our quote-manager application, where not defining
               the bounds of the metric well enough can lead to incorrectly validating the application:
            </p>
            
            <ul>
               
               <li>Measuring an endpoint execution duration with a CDI interceptor: You miss the JAX-RS
                  layer
               </li>
               
               <li>Measuring an endpoint execution duration with a JAX-RS filter: You miss the servlet
                  layer
               </li>
               
               <li>Measuring an endpoint execution duration with a servlet filter if the metric is the
                  processing time of the request: You miss the container processing
               </li>
               
            </ul>
            
            <p>These examples are all server-side mistakes but illustrate the fact that being explicit
               about the metrics is not as trivial as it may seem, since the three mentioned solutions
               are easy and also very tempting ones.
            </p>
            
            <p>There is a case that is worse: the client side. When the metric is a client-side metric—often
               the case for an SLA, since in that case we generally don't care about what the server
               does if it is fast for the clients—then the measurement definition is very important.
               The client case implies some infrastructure you don't always control. Thus, ensuring
               the definition is well done will avoid ambiguities and potential disagreements with
               customers or reviewers of the benchmark. Here are some examples of different interpretations
               of the same metric:
            </p>
            
            <ul>
               
               <li>The client execution time is measured from a client connected to the application server</li>
               
               <li>The client execution time is measured from a client <span>connected to</span><span> </span>the load balancer in front of the application servers
               </li>
               
               <li>The client execution time is measured from a client <span>connected to</span> an API gateway that redirects the call to a load balancer
               </li>
               
               <li>The client execution time is measured from a client <span>connected to</span><span> a proxy in another <strong>Wide Area Network</strong> (<strong>WAN</strong>) that</span> routes the request to an API gateway and so on
               </li>
               
            </ul>
            
            <p>Each of these lines adds an infrastructure layer on top of the previous one, and thus,
               adds some latency for the client. They all measure the <em>client execution time</em>. This is why precisely defining the infrastructure, and moreover how the metric is
               designed, is very important, before starting to benchmark the application.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Defining acceptance criteria</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Once you have a metric clearly defined, you need to define the criteria based on that
               metric that will make the benchmark validated or rejected—is your application fast
               enough to rephrase it at a high level?
            </p>
            
            <p>Generally, it is a number that can be expressed as a time unit or percentage, depending
               on the metric. If the measure is lower (or higher) than this number then the benchmark
               is rejected.
            </p>
            
            <p>Most of the time, the metric is not self-sufficient and needs some additional parameters
               to be able to define the acceptance criteria in a measurable way. Here are some common
               examples:
            </p>
            
            <ul>
               
               <li>The <em>client execution duration</em> must be under 1 second for <em>64 concurrent users</em></li>
               
               <li>The <em>client latency</em> must be under 250 milliseconds when <em>128 messages per second</em> are received
               </li>
               
               <li>The <em>insertion rate of the data into the database</em> must be higher than 1,500 records per second for <em>two connections</em></li>
               
            </ul>
            
            <p>In these examples, the bold expression is the metric we build our criteria on, and
               the italic one is another potential metric fixed in the context of the defined criteria
               (the underlined number).
            </p>
            
            <p>Of course, it is possible to use more than two metrics in the same criteria and even
               to make them all vary at the same time. However, this leads to complicated acceptance
               criteria, and it is generally always possible to rephrase them based on acceptance
               criteria that are using constants. Don't hesitate to rebuild a criteria database from
               the ones you get in inputs, to ensure they are easy to validate and measure. A very
               simple example of this sort of rephrasing can be represented by changing <em>the client execution duration must be under 1 second for a number of concurrent users
                  between 1 and 64</em> into <em>the client execution duration must be under 1 second for 64 concurrent users</em>. This change is not strictly equivalent and you will need to validate the first statement,
               but the second phrase is easier to work with, in particular, if you need some tuning.
               It is worth using this simpler one to start work and to get a rough estimate of your
               metrics and then, once it passes, just validate the original one.
            </p>
            
            <p>One criteria, which was not mentioned before, is the <em>time</em>. Generally, all criteria are defined for an <em>infinite</em> duration. This means you will need to make sure that once they are reached they are
               respected for <em>long enough</em> to assume it will not be degraded after some time. This is something to take into
               account when you prepare your tooling, as lots of factors can degrade the performance:
            </p>
            
            <ul>
               
               <li>A database that slows down after a certain number of records are inserted</li>
               
               <li>A cache that is wrongly sized and starts being too big for its configuration</li>
               
               <li>A badly tuned memory, and so on</li>
               
            </ul>
            
            <p>All these factors will not always prevent you from reaching your performance in a
               <em>short</em> period of time, but they will likely degrade the performance after some duration.
            </p>
            
            <p>The idea here will be to ensure you can associate the acceptance criteria with some
               environment metrics, such as the memory behavior. For instance, you can associate
               the acceptance of your criteria with memory usage and/or a garbage collector profile,
               such as the following one:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="127" src="assets/e3d7ce8f-72ae-4c05-b887-e43cf053d961.jpg" width="338"/></div>
            
            <p>Assuming the <em>X</em> axis is the time and the <em>Y</em> axis the memory used, then this profile shows that the garbage collection is regular
               (almost each vertical line) and that the memory usage is bounded and regular since
               it doesn't go over the red line representing the maximum, even after a few cycles.
            </p>
            
            <p>This sort of definition is rarely self-sufficient as it implicitly defines that this
               validation happens when the application has already reached the criteria we measure,
               and that <em>some time</em> has passed. Although, it is better than just measuring the criteria and not validating
               that the result is true for a long-running instance.
            </p>
            
            <p>Indeed, the best case is to be able to test the application for days, but it is generally
               costly and not doable. If you can't do it, using this kind of strategy and high-level
               validation is generally a good fallback.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Defining the scenario</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Defining the scenario is linked to the criteria but removes the constant constraint.
               This allows you to define more complex cases where all metrics can vary at the same
               time.
            </p>
            
            <p>A common example is to make the user (client) number a variable moving with the time:
               <em>the response time will be constant from 5 users to 1,000 users with an increment of
                  5 users every 10 seconds</em>.
            </p>
            
            <p>A scenario is generally very close to the actual application usage but also harder
               to work on if you don't encounter it immediately because you are no longer running
               the application under a constant load. This is why they are seen more as validation
               checkpoints than work criteria.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Defining the environment</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Once you know what you want to test, you will need to set up your application <em>somewhere</em> to be able to validate it and potentially optimize it.
            </p>
            
            <p>It may sound obvious but, here, you have to be very strict on that point and benchmark
               your application in an environment comparable to your final environment. What does
               it mean? The same machine, same network setup, same load balancing strategy, same
               backends/databases, and so on.
            </p>
            
            <p>Any part of the setup must match where you will run. Otherwise, when deploying in
               production, you may be surprised by some unexpected factors you should have seen coming
               and evaluated far before this final deployment. The best case is that the application
               is not functional, which is generally identified by some smoke tests done after the
               deployment. The worse case is that the application is functional, but its scalability
               or performance are affected by an unexpected factor. We rarely run performance tests
               in a production environment. Thus, limiting the potential error factors due to the
               environment is very important.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The machines</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Before installing any application, you need a machine. From what we just said, the
               machine must be close to the final one, but what does it mean in the machine's context?
            </p>
            
            <p>The machine is often seen as its resources:</p>
            
            <ul>
               
               <li>The CPU: The computing power the application can use</li>
               
               <li>The memory: The space the application can use</li>
               
               <li>The disk space: The local storage the application can rely upon</li>
               
            </ul>
            
            <p>Your first choice will be to pick the same CPU/memory/disk as the production machine.
               Yet make sure, before going this way, that the machine is not shared with other services
               (like another application), which can completely deserve the 1-1 choice in terms of
               resources (CPU, memory, disk, ...) because the resources would be consumed by the
               other application. That is to say, if the application is sharing its resources with
               other software, you will need to find a way to either estimate the available resources
               for your application and limit them to this amount, or isolate both applications to
               guarantee each of them will have a well-defined set of resources.
            </p>
            
            <div class="packt_tip">If you rely on Docker/Kubernetes for deployments, these recommendations apply as well,
               except they are no longer at <em>machine</em> level but <em>pod</em> level. Also, make sure your JVM is configured to support the pod (or container) settings
               that require some JVM tuning to use cgroup configuration instead of the whole machine
               setup—the Java default.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The network</h1>
            
         </header>
         
         
         <article>
            
            
            <p>The network is, nowadays, a very important factor in a deployment. If your application
               is self-sufficient, it is not very crucial, but this is almost never the case. All
               applications have either an HTTP layer (through a UI or web services), a (remote)
               database, or are remotely connected to other applications. This is becoming even more
               crucial in a microservices architecture where some libraries are even designed to
               handle that part more specifically (with fallbacks, bulhead, and concurrency, as we
               saw in previous chapters).
            </p>
            
            <p>In this context, it is very important to be able to rely on a good network. In the
               same spirit as for the machine selection, you must choose a network comparable to
               the production network. Assuming the material is almost the same, this means that
               you will select networks with the same throughput, but this is not enough.
            </p>
            
            <p>When working with a network, there are two other criteria to take into account very
               quickly to avoid surprises:
            </p>
            
            <ul>
               
               <li>The <em>distance</em> between the machines/hosts: If remote services are <em>far</em> then the latency will be increased and the code relying on these services will be
                  <em>slower</em>. Ensuring you benchmark in conditions close to the production ones—the same latency
                  and response times—is very important to be able to rely on the figures you obtain.
               </li>
               
               <li>The network usage: If the network is used a lot by other applications, the bandwidth
                  available for your <em>new</em> application will be reduced, and the performance will be very quickly impacted. A
                  common error in a benchmark is to have a network dedicated to the benchmark, whereas
                  in production it is shared with some other applications. Ensuring you get a consistent
                  setup here will avoid big surprises during your deployments.
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Databases and remote services</h1>
            
         </header>
         
         
         <article>
            
            
            <p>If your application uses a remote service, which can be a classical <strong>relational database management system</strong> database (<strong>RDBMS</strong>), a NoSQL database, or another application, it is important to make sure you are
               benchmarking under realistic conditions. Concretely, if we take back our quote-manager
               application, which uses an <span>RDBMS database, we should not test with local MySQL if our production database will
                  be an Oracle instance. The idea is to get as close to the reality as possible—the
                  latency our production environment will get.</span></p>
            
            <p>In some cases you (or your company) will own the other services/databases and can
               tune them to make them scale more. But in some other cases, you use external services
               you can't optimize, such as CBOE and Yahoo! Finance, in the quote-manager application.
               In any case, it will always be saner to come to the other node (service/database)
               manager to ask to make it faster. Realizing you are slow in production because you
               don't have the same setup as during the benchmark will slow you down and impact you
               more.
            </p>
            
            <p>This doesn't mean that <em>mocking</em> an external service is stupid. It can be very handy during the optimization phase
               of your own application, since it will make the external service interaction as fast
               as is potentially feasible. However, you must ensure you remove the mock when doing
               your <em>validation</em> benchmark.
            </p>
            
            <div class="packt_tip">If you enable the application to be configured to use mocks or fast alternative systems,
               don't forget to write a log message (in the INFO or WARNING levels) during startup
               to ensure you can find this information later. It can save you a lot of time and avoid
               you re-running a benchmark because you are not sure of the <em>actual</em> running setup.
            </div>
            
            <p>During the benchmark, in particular the tuning phase, you will likely configure your
               pools (connection pools). Thus, it is important to ensure you can rely on the database
               (or service) scalability. The goal is to avoid successfully passing a benchmark with
               a pool of 1,024 connections and realizing you can only use 20 connections in production
               (20 being the maximum number of connections your database accepts).
            </p>
            
            <p>More than the database/service type, more than the version, more than the environment
               (OS, machine), you need to make sure the configuration of the database is copied from
               the production instance (or, if you are in the tuning phase, that the final configuration
               you used can be copied to the production instance).
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">The server</h1>
            
         </header>
         
         
         <article>
            
            
            <p>We are talking about a Java EE application—but it could be generalized to any application
               if we talk about <em>packaging—</em>so we deploy the application into a server. Even embedded applications are packaging
               (<em>bundling</em>) a server in their deliverable. As with all the previous points, it is important
               to align it with the target system: the production environment.
            </p>
            
            <p>Concretely, it means that you shouldn't test against a WildFly server if your production
               environment is using Apache TomEE or GlassFish.
            </p>
            
            <p>A server is not far from your application if we talk about the way it is developed
               and packaged. This means that it embeds several libraries selected by the server vendor.
               The direct implication is that a server version embeds several library versions. Since
               Java EE is between ~15 and ~30 specifications, it is at least as important as libraries
               packed together. Because it is software and you can't avoid some changes between versions—particularly
               during the early stages of a new specification—you should try to ensure you are using
               not only the same server as in production but also the same version.
            </p>
            
            <div class="packt_tip">This statement should be extended to all the code that is <em>outside</em> of your application. It can include your JDBC drivers, directly deployed in the container,
               or even some infrastructure/operation team services.
            </div>
            
            <p>Once you have picked your server, you need to ensure you use a setup (configuration)
               close enough to the production one. This includes the logging configuration you will
               need (typically, if you use a log aggregator, you may need a specific log format)
               and the resources deployed to it. If you auto-deploy resources from an infrastructure
               service, ensure you deploy them all to have the same thread usage, network usage (if
               it implies remote resources, such as JMS), and so on.
            </p>
            
            <p>Finally (and linked to machine selection), ensure the setup is consistent with the
               production one. If you log on a <strong>Solid State Drive</strong> (<strong>SSD</strong>) disk in production, ensure you log on an SSD during your benchmark.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Automatize your scenarios</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Once you have your scenarios, you can just describe them and manually execute them
               for simple ones you can script or code directly without much effort. But most of the
               time, you will need to automate them. This is typically the case for load testing
               scenarios. The advantage of automating them is that you can run them on demand (<em>in one click</em>), and thus, it is easy to test and retest them without a huge investment.
            </p>
            
            <p>There are several tools to automate the scenarios, and they mainly depend on the scenario
               you need to test. We will go through some mainstream ones you can use if you don't
               know where to start.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">JMeter</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Apache JMeter (<a href="http://jmeter.apache.org/">http://jmeter.apache.org/</a>) is a historical solution to load test an application. It supports several modes
               and is fully written in Java, which makes it easy to integrate and use for most Java
               developers. It supports main <em>connections</em> used by applications:
            </p>
            
            <ul>
               
               <li>HTTP/HTTPS, SOAP/REST for JavaEE, NodeJs, and so on</li>
               
               <li>FTP</li>
               
               <li>JDBC</li>
               
               <li>LDAP</li>
               
               <li>JMS</li>
               
               <li>Mail</li>
               
               <li>TCP and so on</li>
               
            </ul>
            
            <p>What is immediately interesting for you is that you will be able to test your Java
               EE application but also your other backends, and thus can compare the performance
               (of the database and application, for instance) to potentially be able to report that
               the database is the bottleneck.
            </p>
            
            <p>It provides a nice UI, which looks like this:</p>
            
            <div class="CDPAlignCenter CDPAlign"><img src="assets/d124e64e-8f7a-44c2-bb2f-da11c842251d.png"/></div>
            
            <p>This interface is designed for building your test plans (scenarios); it allows you
               to create it without any configuration or deep knowledge of the tool. If you start
               the software from a command line, you will even have a warning message saying not
               to use it for actual load testing and to use the <strong>command line interface</strong> (<strong>CLI</strong>) for real runs.
            </p>
            
            <p>Then, once you have started JMeter, you will build a <em>Test Plan</em> composed of steps. It will allow you to configure the threads and the way the number
               of <em>clients</em> is defined, add some assertions (validations of the output) to the scenario, and
               reuse variables between steps (for instance, a first step can get an OAuth2 token
               used to authenticate the next request or even handle the warm-up of your testing).
               In the elements you can add to the plan, there are some reports allowing you to get
               the figures you expect as output from a benchmark, such as the percentage of error,
               the min/max duration, the throughput, KB/sec, and so on:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img src="assets/016d88b5-0a47-4a0a-b33c-383b7aef4c5b.png"/></div>
            
            <p>This screenshot represents the <em>aggregated report</em> of JMeter which contains statistics about the plan execution—or a subpart of it.
               What is interesting here is the error rate (100% in the previous example) which allows
               you also to validate that the execution was <em>good enough,</em> that is, there were not too many errors saying we didn't test anything.
            </p>
            
            <p>Once your plan is defined, you can save it in a <kbd>.jmx</kbd> file (JMeter default extension), which will allow you to replay it. At that point,
               you should be able to <em>locally</em> test your scenario (changing the URL of the plan a bit to adjust it to your local
               instance), but you can't yet test a cluster.
            </p>
            
            <p>Finally, for real load testing, you will need to use the <em>remote testing</em> solution of JMeter. It will allow you to orchestrate multiple client nodes (often
               called <em>injectors</em> since they will <em>inject</em> requests into the system). The big advantages are:
            </p>
            
            <ul>
               
               <li>You don't rely on your local machine anymore</li>
               
               <li>You control which networks are used by the client (it can be the same as the server
                  or not)
               </li>
               
               <li>You can horizontally scale, using <em>N</em> client machines instead of one
               </li>
               
            </ul>
            
            <p>The last point is very important because of the network usage. When doing HTTP requests,
               you will use the machine network, and one of the most limiting criteria will be the
               number of clients per nodes. The more clients you have, the slower they will globally
               be as they add noise for other clients. That is to say, before launching a full run,
               make sure to size your injector correctly to establish how many clients you can use
               per injector node, without being limited by the infrastructure. You will rarely have
               tons of clients for a single machine in a real deployment. Thus, it is acceptable
               to have only one or two clients per machine, in some cases.
            </p>
            
            <p>If you want to download JMeter, you can go to its download page (<a href="http://jmeter.apache.org/download_jmeter.cgi">http://jmeter.apache.org/download_jmeter.cgi</a>) on the Apache website.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Gatling</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Gatling (<a href="https://gatling.io/">https://gatling.io/</a>) is an alternative to JMeter. You will find the same features as in JMeter (of course,
               there are some differences, but we will not list them here). The main difference is
               that you script your scenarios instead of configuring them, either in an XML file,
               or visually in a nice UI.
            </p>
            
            <p>The scripting is based on a <strong>Domain Specific Language</strong> (<strong>DSL</strong>) and relies on the Scala language. This can sound like a <strong>blocker</strong> for a Java developer, since Scala is not very friendly if you have never done any
               Scala development. However, it is the strength of Gatling compared to JMeter; it is
               an Akka-Netty-based load testing solution. This means it is coded with technologies
               trying to be lock-free in their own backbone and enabling the injector code to scale.
               JMeter was known to be self-limiting in the way it was designed if you were requesting
               it to scale to too many users. In reality, this is not a huge limitation since, as
               we saw in the previous section, you will often also scale in terms of infrastructure
               to test your application reliably. Yet, it is interesting in development and in some
               highly scaling applications as you will not need so many machines to reach the same
               level of scalability of the injector.
            </p>
            
            <div class="packt_tip">This is often a point we forget during a benchmark, and this is why it is important
               to prepare it before; to ensure the injector does not throttle the benchmark. Otherwise,
               you are testing the client/injector instead of the server/application.
            </div>
            
            <p>Just to give you an idea, here is a simple Gatling script:</p>
            <pre><span class="k">package</span> <span class="nn">packt</span>

<span class="k">import</span> <span class="nn">io.gatling.core.Predef._</span>
<span class="k">import</span> <span class="nn">io.gatling.http.Predef._</span>
<span class="k">import</span> <span class="nn">scala.concurrent.duration._</span>

<span class="k">class</span> <span class="nc">QuotesSimulation</span> <span class="k">extends</span> <span class="nc">Simulation</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">httpConf</span> <span class="k">=</span> <span class="n">http</span><span class="o">.</span><span class="n">baseURL</span><span class="o">(</span><span class="s">"http://benchmark.test.app.quote<br/>  -manager.io"</span><span class="o">)</span>

  <span class="k">val</span> <span class="n">quotesScenario</span> <span class="k">=</span> <span class="n">scenario</span><span class="o">(</span><span class="s">"QuotesScenario"</span><span class="o">)</span>
    <span class="o">.</span><span class="n">exec</span><span class="o">(</span><span class="n">http</span><span class="o">(</span><span class="s">"quotes"</span><span class="o">)</span>
      <span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">"/quote-manager/api/quote"</span><span class="o">))</span>

  <span class="n">setUp</span><span class="o">(</span>
    <span class="n">quotesScenario</span><span class="o">.</span><span class="n">inject</span><span class="o">(rampUsers(64) over (60 seconds)</span><span class="o">)<br/></span><span class="o">    ).</span><span class="n">protocols</span><span class="o">(</span><span class="n">httpConf</span><span class="o">)</span>
<span class="o">}</span></pre>
            <p>This simple script defines a scenario named <kbd>QuotesScenario</kbd><em>.</em> It will request our <kbd>findAll</kbd> quote endpoint.
            </p>
            
            <p>If you put this script in <kbd>$GATLING_HOME/user-files/simulations/packt/QuotesSimulation.scala</kbd>, be careful, as Scala uses the concept of packages as in Java, so you need the right
               nested folder compared to the <kbd>simulations</kbd> folder. Then you can run <kbd>$GATLING_HOME/bin/gatling.sh</kbd>, which will scan and compile the files inside the previous folder to find the simulations
               and ask you to select the one you want to launch:
            </p>
            <pre>$ ./bin/gatling.sh <br/>GATLING_HOME is set to /home/rmannibucau/softwares/gatling-charts-highcharts-bundle-2.3.0<br/>18:12:52.364 [WARN ] i.g.c.ZincCompiler$ - Pruning sources from previous analysis, due to incompatible CompileSetup.<br/>Choose a simulation number:<br/>     [0] computerdatabase.BasicSimulation<br/>     [1] computerdatabase.advanced.AdvancedSimulationStep01<br/>     [2] computerdatabase.advanced.AdvancedSimulationStep02<br/>     [3] computerdatabase.advanced.AdvancedSimulationStep03<br/>     [4] computerdatabase.advanced.AdvancedSimulationStep04<br/>     [5] computerdatabase.advanced.AdvancedSimulationStep05<br/>     [6] packt.QuotesSimulation<br/>6<br/>Select simulation id (default is 'quotessimulation'). Accepted characters are a-z, A-Z, 0-9, - and _<br/>quotessimulation<br/>Select run description (optional)<br/>Test our quotes endpoint<br/>Simulation packt.QuotesSimulation started...</pre>
            <p>The <kbd>computerdatabase</kbd> simulations are the default ones; our simulation is the last one. Once selected,
               Gatling requests some metadata about the simulation, such as its <kbd>id</kbd><em> </em>and <kbd>description</kbd>.
            </p>
            
            <div class="packt_tip">The first time you launch Gatling, the startup can be lengthy as it will compile the
               simulation—there are some samples with the default distribution.
            </div>
            
            <p>When the simulation runs, you will get some progress in the console (whereas with
               JMeter, you were able to get it in the UI for your tests and see the reports in real
               time):
            </p>
            <pre><strong>================================================================================</strong><br/><strong>2017-11-01 18:14:49 50s elapsed</strong><br/><strong>---- Requests ------------------------------------------------------------------</strong><br/><strong>&gt; Global (OK=54 KO=0 )</strong><br/><strong>&gt; quotes (OK=54 KO=0 )</strong><br/><br/><strong>---- QuotesScenario ------------------------------------------------------------</strong><br/><strong>[############################################################## ] 84%</strong><br/><strong>          waiting: 10 / active: 0 / done:54 </strong><br/><strong>================================================================================</strong></pre>
            <p>These small reports show the progress of the test. We can identify that we are at
               84% of the simulation we configured, representing the 54/64 requests (users) we requested
               and that 50 seconds has elapsed already.
            </p>
            
            <p>Once the test is finished, a small report is generated in the console:</p>
            <pre>Simulation packt.QuotesSimulation completed in 59 seconds<br/>Parsing log file(s)...<br/>Parsing log file(s) done<br/>Generating reports...<br/><br/>================================================================================<br/>---- Global Information --------------------------------------------------------<br/>&gt; request count 64 (OK=64 KO=0 )<br/>&gt; min response time 20 (OK=20 KO=- )<br/>&gt; max response time 538 (OK=538 KO=- )<br/>&gt; mean response time 63 (OK=63 KO=- )<br/>&gt; std deviation 63 (OK=63 KO=- )<br/>&gt; response time 50th percentile 53 (OK=53 KO=- )<br/>&gt; response time 75th percentile 69 (OK=69 KO=- )<br/>&gt; response time 95th percentile 98 (OK=98 KO=- )<br/>&gt; response time 99th percentile 280 (OK=280 KO=- )<br/>&gt; mean requests/sec 1.067 (OK=1.067 KO=- )<br/>---- Response Time Distribution ------------------------------------------------<br/>&gt; t &lt; 800 ms 64 (100%)<br/>&gt; 800 ms &lt; t &lt; 1200 ms 0 ( 0%)<br/>&gt; t &gt; 1200 ms 0 ( 0%)<br/>&gt; failed 0 ( 0%)<br/>================================================================================</pre>
            <p>This report contains the statistics about the execution and the response time distribution.</p>
            
            <p>Finally, Gatling generates an HTML report (by default). Its location is logged at
               the very end of the program, just before it exits. You can open it with any browser
               and here is what it looks like:
            </p>
            
            <div class="CDPAlignCenter CDPAlign"><img height="589" src="assets/35dc2f5e-70b5-48d1-a37f-e52a97f32304.png" width="732"/></div>
            
            <p>Back on the report, we find the statistics (on the left, in orange) and some detailed
               indicators in the center of the page. Between them, you have the number of requests,
               the response time distribution (allowing you to see if the response time is in an
               acceptable range or if the response time is constant enough for your target users),
               and so on. You can see, in the previous screenshot, that there are two tabs: <span class="packt_screen">GLOBAL</span> and <span class="packt_screen">DETAILS</span>. The <span class="packt_screen">DETAILS</span> tab has this small menu on the left (with <span class="packt_screen">quotes</span> on our screenshot), allowing you to drill down the details, per step, of the simulation/scenario.
               The <span class="packt_screen">quotes</span> references the name we gave to the <kbd>http</kbd> request we defined in our simulation.
            </p>
            
            <p>Gatling has a lot more features and ways to compose scenarios, and since it is code,
               it is also quite flexible. This is not the topic of the book, but don't hesitate to
               have a deeper look.
            </p>
            
            <p>Again, don't forget the injector machines (the machines where you put the processes
               simulating the clients, that is, the Gatling process here) may not be powerful enough
               or may have not enough bandwidth to scale very highly. For that reason, you will need
               to distribute your injectors across several machines to reach the right amount of
               users in general.
            </p>
            
            <p>Gatling, like JMeter, supports this mode even if it requires more work. The procedure
               is explained on their website (<a href="https://gatling.io/docs/2.3/cookbook/scaling_out/">https://gatling.io/docs/2.3/cookbook/scaling_out/</a>), but at a high level you will run the simulation on several nodes then grab all
               their outputs and aggregate them post-execution.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Other tools</h1>
            
         </header>
         
         
         <article>
            
            
            <p>There are many other tools you can use to define your scenarios; even some DIY solutions
               can be used. In all cases, you should ensure it scales well enough to not limit your
               benchmark since you want to test your application and not your benchmark tooling.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Ensuring you have support</h1>
            
         </header>
         
         
         <article>
            
            
            <p>When you start benchmarking, you will probably encounter some issues, such as:</p>
            
            <ul>
               
               <li>A network setup not correctly done</li>
               
               <li>A bug in a framework/a library/your application/the injector</li>
               
               <li>A remote service or database not absorbing enough load, and so on</li>
               
            </ul>
            
            <p>In all potential cases you may encounter—given any <em>brick</em> of your software can have an issue—you should be able to have somebody you can call
               to help you fix the issue, or at least evaluate it quickly. This is particularly crucial
               if part of the benchmark costs <em>some</em> money (if you are renting some machines, consulting, and so on). The idea here is
               to be able to get rid of any blocker as fast as possible to not waste time on details.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Time boxing your optimization benchmark</h1>
            
         </header>
         
         
         <article>
            
            
            <p>An evaluation benchmark is time boxed by design; you run the benchmark and report
               the figures. Although, an optimization benchmark is more blurry. Concretely, you can
               spend a whole year optimizing a simple web service just because of the layers it uses
               and all the small tuning options you can test, from the network configuration, through
               the JVM memory, to the caching solutions.
            </p>
            
            <p>Thus, it is crucial before starting to benchmark an application and optimizing it
               to define how long you will spend benchmarking your application. It can also be linked
               to a renting period and may require an estimation phase to work with the operation
               and development teams. But if you don't do it, the risk is that you will spend a lot
               of time on the details and not make the best of the benchmark.
            </p>
            
            <div class="packt_tip">Even if it is a high-level approximation, the Pareto principle can be used here to
               try to optimize the benchmark time. Concretely, try to do 20% of the optimization,
               which will give you 80% of the boost for your application. Then, if you have time,
               you can work on the remaining 20%.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Installing and resetting procedures</h1>
            
         </header>
         
         
         <article>
            
            
            <p>It may sound obvious, but before starting the benchmark, you must know how to properly
               install your application and inter connect it with other systems (databases, other
               applications, and so on). This part should be written down in a document to make sure
               it is easy to find when needed, and that it has been tested at least once before the
               benchmark.
            </p>
            
            <p>The part we forget more often is the reset part, and it would be ideal if this part
               is automatized as well in the scenarios. This is mainly about ensuring each execution
               is repeatable, and executions are comparable.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Benchmark iterations</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In the previous section, we prepared all we needed to start a benchmark in an efficient
               way. Now we need to see how we will work during the benchmark.
            </p>
            
            <p>The first important thing is to establish that we only deal with optimization iterations
               here and not evaluation ones, which are straightforward—you run the scenarios and
               gather the reports.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Iterating</h1>
            
         </header>
         
         
         <article>
            
            
            <p>It is probably natural, but you will iterate with your optimizations. This means that
               you will run the same test again and again to measure the result of a change—for instance,
               increasing the pool size.
            </p>
            
            <p>The direct implication of such a work structure is that you need to prepare yourself
               to store lots of reports in an organized way. There are many solutions for that and
               it mainly depends on the tools you are used to relying on. But at a very high level,
               you need to, at least, store:
            </p>
            
            <ul>
               
               <li>The benchmark report.</li>
               
               <li>The benchmark date (to be able to sort them, it is often useful to replay the iterations
                  done afterwards).
               </li>
               
               <li>The benchmark configuration (you can store the full configuration or just write it
                  in a file, named <kbd>CHANGES.txt</kbd>, for instance, where you list what you changed from the previous run). Note that
                  it is important here to include the changes of external systems—such as databases—since
                  they can directly impact your performance.
               </li>
               
            </ul>
            
            <p>In terms of storage, you can just use a <kbd>benchmark</kbd><em> </em>folder on your hard drive and create a folder per iteration containing the previous
               information. The folder name can contain the date. A common pattern is <kbd>&lt;iteration number&gt;_&lt;date&gt;_&lt;short description&gt;</kbd>, for instance <kbd>001_2017-11-14_increasing-pool-size</kbd>. Using a number (padded with <em>0)</em> will allow you to use your operating system sorting to sort the folder. The date
               gives you another entry point—when somebody tells you <em>yesterday, it was working better</em>, for instance. Finally, the small description allows you to more easily identify
               the reports to compare them.
            </p>
            
            <div class="packt_tip">It is not mandatory, but if you have a small tool (like a script or a small Java program)
               parsing the reports and configuration to store them in an index, you can more easily
               find the data and you will get a more powerful search. In the same way, if you already
               did the work to parse the data, you can easily implement a small <kbd>diff</kbd> tool to compare two reports, which will allow you to show the configuration changes
               and the impact on the performance—the reports. Generally, the reports are visual.
               Thus, being able to merge two reports allows you to compare them more efficiently
               (visually) than using two windows.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Changing a single setting at a time</h1>
            
         </header>
         
         
         <article>
            
            
            <p>While you are tuning the application, it is important to identify if a setting is
               the factor enhancing the performance or not, and thus identify it as important or
               not.
            </p>
            
            <p>If you change multiple settings for a single run, you will not be able to say which
               setting triggered a change, or you can even neutralize a good setting by using another
               bad one and missing an optimization factor.
            </p>
            
            <p>Resist the temptation to change everything at the same time, and try to keep a <em>scientific</em> approach, changing a single setting at a time.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Resetting any state between runs</h1>
            
         </header>
         
         
         <article>
            
            
            <p>We saw, in the previous section, that you must prepare as much data as the production
               database will work with, but also don't forget to reset the database state between
               each run.
            </p>
            
            <p>If you don't do it, the risk is you will slow down the execution between each run
               and make the optimizations you do completely invisible. This is because databases
               have a sort of size limit (quite huge), but while you benchmark, you will generally
               insert a lot of data very quickly so it wouldn't be surprising to reach that limit.
               Once you reach this size limit, the database is less efficient and performance degrades.
               To ensure you can compare the runs and validate some tuning options, you must run
               in the same conditions. So, you should ensure the database has the same data between
               each run.
            </p>
            
            <div class="packt_tip">This explanation used the database as the main illustration because it is the most
               common pitfall, but it is true for any part of your system.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Warming up your system</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Another crucial step to doing a benchmark is to not measure the data on a <em>cold</em> system. The reason is that a Java EE application generally intends to be long-running
               software; for that reason, it is common to have a <em>hot</em> system which already got optimizations<span> after having ran</span> during weeks or hours. These optimizations can be in action:
            </p>
            
            <ul>
               
               <li>The <strong>JVM</strong> <strong>Just-In-Time</strong> (<strong>JIT</strong>) compilation: This will optimize some common code paths. You can also investigate
                  the <kbd><span>-XX:-TieredCompilation</span></kbd><span> </span>option of the JVM to <em>pre-compile</em> the code, but you can encounter some issues with it on some servers.
               </li>
               
               <li>You can use some caching and therefore the application will be faster once the cache
                  has all the data you test.
               </li>
               
               <li>If you use some external system you may need to do some expensive connections you
                  will reuse later on (SSL connections are slow, secured connections are slow, and so
                  on).
               </li>
               
            </ul>
            
            <p>Having some warm-up iterations before the actual measures start is very important
               to hide all these initializations and just measure the <em>final</em> performance of your application.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">After your benchmark</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Once you have done your benchmark, you should have a <em>database</em> of your runs (the folder we talked about earlier with the reports, configuration,
               and so on). Now, to ensure you did the benchmark for a reason, there are few actions
               you should take.
            </p>
            
            <div class="packt_tip">In this section, we assume that you will do these steps after the benchmark, but you
               can do them during the benchmark itself. It is presented this way because it is something
               you can do <em>offline</em>, and if you have some costs associated with the benchmark, these are tasks you can
               postpone easily.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Writing a report</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Normally, at this point, you have collected all the data corresponding to the hard
               work you did during the benchmark. It is quite important to aggregate this in a report.
               The report will mainly explain the investigation (why you changed some settings and
               so on) and expose the results of the runs.
            </p>
            
            <p>You can, of course, ignore the useless runs (no significant change in the performance),
               but it is always interesting to integrate the ones corresponding to a performance
               boost or degradation.
            </p>
            
            <p>The last part of the report should explain how to properly configure the server for
               production. It can be done inline in the report or point to another document such
               as a reference guide if it is about a product or a white book for an application.
            </p>
            
            <p>Here is a high-level structure for a report:</p>
            
            <ul>
               
               <li>Application description: What the application does.</li>
               
               <li>Metrics: If you have some not so obvious or specific metrics, explain them here (before
                  the next part).
               </li>
               
               <li>Scenarios: What your test did.</li>
               
               <li>Infrastructure/environment: How you deployed machines and external systems, how you
                  set up the monitoring, and so on.
               </li>
               
               <li>Injection: How you stimulated the application (you can explain that you had <em>N</em> JMeter nodes, for instance).
               </li>
               
               <li>Runs: All the relevant iterations you did and their results.</li>
               
               <li>Conclusions: What do you keep from the benchmark? Which configuration should be used?
                  You can also mention some tests you didn't get time to run.
               </li>
               
            </ul>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Updating your default configurations</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Even if, as shown in the previous section, the <em>final</em> configuration is part of the report, it will not prevent you from propagating all
               the good practices you deduced from the benchmark in the code base. The goal is to
               reduce the mandatory configuration.
            </p>
            
            <p>For instance, if you identified that you need a timeout of 1 second instead of the
               default 30 seconds to have a good performance, updating your defaults to 1 second
               directly in the code base will avoid having a bad performance if the configuration
               is forgotten. This part is a trade-off between default usability and performance,
               but generally you can still enhance the default user/operation team experience by
               doing it.
            </p>
            
            <div class="packt_tip">If you have some provisioning recipes or a Dockerfile, don't forget to update them
               as well, if relevant.
            </div>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Reviewing the conclusions</h1>
            
         </header>
         
         
         <article>
            
            
            <p>Depending on your conclusions, you may need to cross-check, with developers or other
               members, that the outcome of the benchmark is valid.
            </p>
            
            <p>For instance, if you deduced on our quote-manager that you needed to cache the quotes,
               then you may desire to validate:
            </p>
            
            <ul>
               
               <li>If it is OK to cache them business-wise (you can check it with your product owner
                  or manager)
               </li>
               
               <li>How long you can cache them for, as you will probably want some updates on the prices
                  at some point
               </li>
               
            </ul>
            
            <p>Another common example is to validate that you can bypass or change the way you secured
               some part of the application because the security layer was too slow (switching from
               OAuth2 to HTTP signature, or some authentication mechanism to network security, for
               instance).
            </p>
            
            <p>Once the conclusions are validated, you can also extract the part of the report related
               to the original SLA and make them validated by your customers, or the people you report
               to.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Enriching your project backlog</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In some cases, you may have identified some issues in the code. They may or may not
               impact the performance, but in any case you need to create corresponding tasks to
               fix them upstream.
            </p>
            
            <div class="packt_tip">If you used some hotfix or patch during the benchmark, don't forget to mention it
               and reference it inside the report to let people track whether it is actually fixed
               or not. Note that it can also be related to external libraries or containers and not
               only your application.
            </div>
            
            <p>The more you work across teams, the more this phase is important. Otherwise, you get
               a report where the SLA is reached, and a product is never able to respect that because
               the enhancements are never integrated into the mainstream source code.
            </p>
            
            
            
         </article>
         
         
         
      </section>
      
   

      
      <section>
         
         
         <header>
            
            <h1 class="header-title">Summary</h1>
            
         </header>
         
         
         <article>
            
            
            <p>In this chapter, we saw that a benchmark is something you need to prepare before ensuring
               you can benefit the most from the benchmark time, and that it requires some organization.
               We also saw that to be useful you need to extract, from the work done, the conclusions
               it implies. This is really a scientific procedure—but an easy one—and you need to
               respect it if you want to optimize your time.
            </p>
            
            <p>The next and last chapter will go one step further and look at how to reduce the distance
               between the development and the benchmark to reach a continuous performance evaluation,
               making your benchmark no longer harmful, since everything is already prepared and
               under control.
            </p>
            
            <p class="mce-root"/>
            
            
            
         </article>
         
         
         
      </section>
      
   </body></html>