- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling and Performance Optimization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21843_09.xhtml#_idTextAnchor240) introduced skills such as quantitative
    measurements to observe how a system connected via APIs behaves at runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: That chapter provides a good basis for this chapter on **performance** and **scalability**
    , two non-functional requirements that are strongly concerned with time, size,
    and other quantitative aspects of software systems and the data they process.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by explaining what developers need to know about performance and
    scalability in general. Then, we will dive into specifics, describing the most
    common strategies and techniques for improving performance and scalability. We
    will demonstrate some of these techniques using our Product and Order Management
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We highlight the potential of Java **virtual threads** for increasing application
    throughput by improving CPU usage efficiency in the context of API development.
  prefs: []
  type: TYPE_NORMAL
- en: We also show that to support performance and scalability, your API should be
    prepared to work with specialized infrastructure components.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will show how load testing helps you avoid unpleasant surprises once
    your application starts to receive the production load. Using performance testing,
    developers can get information that is vital to target their optimization efforts.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to prevent performance and scalability
    issues starting from the analysis phase, through appropriate API design, to subsequent
    optimizations triggered by the findings from load tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding performance and scalability in API development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying performance optimization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the throughput with virtual threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using infrastructure support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing and executing effective load tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate some of the techniques described in this chapter, we will use
    the example code of the Product and Order Management APIs that were developed
    in previous chapters. The changes that we will make to the code for this chapter
    can be found in the repository at [https://github.com/PacktPublishing/Mastering-RESTful-Web-Services-with-Java/tree/main/chapter10](https://github.com/PacktPublishing/Mastering-RESTful-Web-Services-with-Java/tree/main/chapter10)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Understanding performance and scalability in API development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance in computing refers to how efficiently a system or application
    executes tasks under a given workload. Efficiency has two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed of processing** : This is measured mostly in terms of response time
    (latency) and throughput (how many operations or how much data the system can
    handle per unit of time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumption of resources** : This refers to the amount of resources required,
    such as CPU, memory, and network bandwidth utilization, to do the work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics are an integral part of performance measurement because it makes
    sense to measure performance when the system processes a large number of different
    requests involving different amounts of data. Therefore, the time and other resources
    required to fulfill a request inevitably fluctuate and depend on a number of factors,
    making the actual measured values of the performance characteristics virtually
    random in nature.
  prefs: []
  type: TYPE_NORMAL
- en: It is usually sufficient to use average values to calculate throughput. For
    response times, the average value is less useful because it does not capture well
    how the speed of response is perceived by users. The maximum value is often of
    particular interest because it can indicate potential issues, such as the occurrence
    of timeout errors. Moreover, response times exceeding a reasonable limit discourage
    human users from continuing to use the application.
  prefs: []
  type: TYPE_NORMAL
- en: Even more useful than the extreme values that occur very rarely are the percentile
    values. If we rank all response times from shortest to longest, then, for example,
    the 95th percentile is the time at which 95% of the measured times are shorter.
    In other words, there is only a 5% probability that a randomly selected request
    will take longer.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability refers to the ability of a system to handle an increasing load (increased
    number of users, higher data volumes, or additional transactions) while maintaining
    the performance within the limits expected by users. You can see that we cannot
    speak about scalability without considering performance.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability is one of the main motivating factors behind moving from monoliths
    to distributed (microservices) architectures. We can assume that our example Product
    API will need to handle significantly more requests than the Order Management
    API because not every product being browsed will be bought. We can scale the deployment
    of the Product API to use more instances of the service than the Order Management
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'With RESTful APIs, the tasks whose response time and throughput we are interested
    in are the HTTP requests. The time and computing resources needed to execute an
    HTTP request are composed of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The processing needed by the API technology itself, such as serialization and
    deserialization of the data, network transmission, and protocol overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The processing inside the service providing the API (such as executing algorithms,
    accessing databases, and downstream API calls), which is often influenced by the
    API request and response payload design, such as the amount of data processed
    in one request, the possibility of performing concurrent processing and data streaming,
    and the cacheability of the responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When designing APIs, you usually focus on the functional requirements because
    what the application should do is at the front of the users’ minds. How the application
    is expected to perform and cope with increasing traffic is often considered obvious
    or implied.
  prefs: []
  type: TYPE_NORMAL
- en: It is also hard to predict the actual load, the load progression over time,
    and which parts of the system will be the most impacted. Donald Knuth’s famous
    saying “ *Premature optimization is the root of all evil* ” warns us that we should
    not try to optimize everything.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, performance and scalability considerations should be part
    of the design process, and you often can prevent a lot of problems with just a
    few simple adjustments to the APIs. Ideally, you should design the APIs to be
    as simple as possible to fulfill the functional requirements, while keeping the
    options open for future extensions and optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When in doubt, let the REST principles guide you towards a more flexible and
    standard solution that will likely support the performance and scalability requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Using URLs based on a resource structure reflecting the business domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using correct HTTP methods and providing respective idempotency support for
    `GET` , `PUT` , and `DELETE`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using correct HTTP headers and status codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using clearly defined parameters to let the client choose what data and operations
    are really needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using appropriate `Content-Type` and `Content-Encoding` for the data (especially
    with large binary documents)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having explained what performance and scalability mean for API development in
    general, we can move on to describe the strategies that, when applied appropriately,
    can have the greatest impact on the performance and scalability of your applications
    integrated using APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Applying performance optimization and scalability improvement strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you detect an existing or imminent performance or scalability problem with
    your API, you should be armed with the tools to solve it. Let’s take a closer
    look at some of the strategies and techniques for optimizing performance in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the performance requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous section, the performance requirements are often
    implied and not clearly specified by users and customers.
  prefs: []
  type: TYPE_NORMAL
- en: For some applications, performance may not be critical, but if we take the example
    of most e-commerce sites, an application with a response time reaching a few seconds
    is equivalent to an application that does not work at all because it immediately
    discourages potential customers from buying.
  prefs: []
  type: TYPE_NORMAL
- en: It is therefore necessary to include at least some rough performance expectations
    for the system to be developed. If you can be more rigorous, you should ask the
    customer to define **service-level agreements** ( **SLAs** ) specifying maximum
    response times or the number of requests processed for each operation. The SLAs
    may vary for peak and off-peak times. Remember to capture size limits for large
    data objects, large numbers of concurrent users, and their geographical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The collected performance requirements can be used to design load tests. The
    load tests will show whether the system can handle the expected load and, if not,
    which parts are the bottlenecks. Information about the geographical distribution
    of users and the quality of their network connections should be used to add simulated
    network latencies in the load test environment.
  prefs: []
  type: TYPE_NORMAL
- en: To design the system correctly, it is important to know whether the system should
    be optimized for response time (systems with human interaction) or throughput
    (batch processing). To achieve better (perceived) response times, it may make
    sense to split the API requests so that a request performs the essential operation
    only, delivering the essential data only. This way the user experience is improved,
    while the non-essential request(s) can start later and/or take longer.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to that, grouping multiple operations or data items in one request
    may improve the throughput by reducing the API overhead. However, you must ensure
    that a request does not become so big that it fails due to a timeout.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, you cannot estimate or simulate the load and environment attributes
    precisely enough, so you must react to performance problems that start occurring
    in production. To support that, your APIs should be ready to evolve without breaking
    the existing clients, which is the topic of [*Chapter 5*](B21843_05.xhtml#_idTextAnchor116)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Providing only what is really needed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security, another non-functional requirement, is often regarded as going against
    performance, but in many instances, their solutions can in fact overlap. Limiting
    the data that we transfer in APIs is good for security and can be good for performance,
    too, because time and resources are not wasted on items that the API client does
    not need.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should identify expensive items that are large or take a long time to get.
    If an expensive item is not used by all requests, you can do either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a parameter to the API endpoint to only return the expensive item when the
    client asks for it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a separate endpoint (resource) for the expensive item
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if our Product API stored photos of the product, we would not have
    to return the photos in every request for a product. Instead, we can return a
    list of image IDs and define a sub-resource of the product resource, returning
    an image by its image ID.
  prefs: []
  type: TYPE_NORMAL
- en: A common mistake is to design API resources to be a copy of the database entities.
    Some developers might even be tempted to use the clear antipattern of using the
    JPA entity classes in APIs directly without defining separate API DTOs.
  prefs: []
  type: TYPE_NORMAL
- en: Special attention should be paid to fields that define relationships between
    JPA entities. Exposing the whole graphs of objects resulting from following the
    relationships without due justification often leads to bloated APIs, creating
    a ticking time bomb with performance (besides security and maintainability) problems.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that at some point we extend the database schema of our
    Product API service with additional database tables and JPA entities containing
    information about stock availability, location, historical prices, and so on.
    These additional database tables could be linked to the product table using foreign
    keys, and the JPA model could include relationships that would map to the respective
    SQL joins. It would be a mistake to include all the additional information automatically
    in the Product API resource JSON representation, except in the unlikely scenario
    where business analysis confirms that all possible use cases for reading a product
    will always need the full set of detailed data. Instead, to support the use cases
    that require access to the related entities, the API could send the extra data
    only when a parameter is set or via a separate endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining statelessness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statelessness (as explained in [*Chapter 1*](B21843_01.xhtml#_idTextAnchor015)
    ) is one of the key principles of REST architecture. All the input data needed
    to perform an operation should be in the URL path and the request body. The request
    processing must not depend on some implicit session data.
  prefs: []
  type: TYPE_NORMAL
- en: Statelessness is important for performance and scalability because it allows
    horizontal scaling of the service providing the API. A load balancer (briefly
    explained in the upcoming section on infrastructure components) can route any
    request to be processed by any of the service instances because the instance does
    not have to remember the session state (history of the previous requests belonging
    to the same user journey). Avoiding the session state can also help in reducing
    memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the application may need to support a session-like user experience.
    For example, you may want to let the user create an order product by product,
    so the application needs to remember an incomplete order, commonly represented
    by a shopping cart on most e-commerce sites. To fulfill this requirement, you
    have two main options:'
  prefs: []
  type: TYPE_NORMAL
- en: Maintain the session state on the client (web browser) using JavaScript code,
    local storage, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the shopping cart as a first-class entity of the backend service. This
    means creating a RESTful resource identified by URL for the new entity and using
    standard HTTP methods to manipulate it. This makes the data stored in the backend
    explicit, as opposed to an implicit session without a clear structure that would
    be harder to manage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting large collections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When your API transfers a collection of items (usually represented by a JSON
    list), you should try to find out the expected number of items in the collection.
  prefs: []
  type: TYPE_NORMAL
- en: If the number of items frequently exceeds a reasonable limit, you should consider
    implementing a pagination solution for the collection (see [*Chapter 6*](B21843_06.xhtml#_idTextAnchor135)
    ). The number of items in a collection and the number of items per page that calls
    for the introduction of pagination depend on how expensive the API operation is.
    For small items that are quick to retrieve, the number could be higher.
  prefs: []
  type: TYPE_NORMAL
- en: You should also think about how large a set it makes sense to work with on the
    client side at the same time. If the items are displayed on a screen, the user
    can usually not see more than a few items at the same time. You can base your
    pagination policy on that number.
  prefs: []
  type: TYPE_NORMAL
- en: Besides pagination, you can limit the number of items returned by ensuring that
    queries are sufficiently specific. For instance, in a substring-based search,
    you can enforce a rule that the search string must have a minimum length of three
    characters, which helps narrow down the results and improve efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Or, before the full data retrieval, you can first execute a cheaper database
    query returning only the count of the matching items, and if it exceeds a certain
    limit, you can stop there and return a response asking the client to provide a
    more specific query. Cheaper in the context of performance optimization means
    taking less time and fewer resources.
  prefs: []
  type: TYPE_NORMAL
- en: When deciding about the collection size limits, you should also think about
    the nature of the data sources your API operation needs to access. In distributed
    (microservices) architectures, you usually need to call another API to fulfill
    an API request.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to make separate API calls to get some details for each item of
    a collection, you can try to shorten the response time by making several such
    API calls in parallel. However, you should limit the number of parallel API invocations
    to avoid overwhelming the API. The *Increasing the throughput with virtual threads*
    section later in this chapter may be useful for implementing concurrent API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to check whether the API your service needs to call supports
    (or can be made to support) bulk operations, such as getting details for a list
    of IDs in one request. But you must be reasonable about the size of the request
    because requests that are too large may take too long and hit timeouts at various
    points along their path from the client to the server and back.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Order Management API calls the Product API for each product
    to get its price. We could optimize this integration so that the Product API provides
    a bulk “get multiple products” endpoint. The API would take a list of product
    IDs as its input and return the prices for all the products at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing large objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already mentioned that it is not wise to try to optimize everything.
    The Pareto principle says that the vast majority (around 80%) of (performance)
    problems are caused by a small minority (around 20%) of the items comprising your
    API.
  prefs: []
  type: TYPE_NORMAL
- en: A relatively common case where a small part of an API is responsible for most
    of the bytes transferred and time spent is when you want to transfer large blocks
    of data whose internal structure is not important to the API, such as photos,
    videos, or documents, which are usually encoded using binary formats. These items
    have a significant impact on the network transfer volume, memory consumption,
    and response times.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a good practice to avoid including large (binary) objects in structured
    (JSON) payloads. Instead, you can define separate resources (endpoints) for the
    large objects. This has the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: The payload can use an encoding that is suitable for the large object format.
    Trying to embed binary data in JSON usually leads to the use of inefficient encodings
    such as BASE64.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the text-based encoding of large data objects cannot be avoided, the efficiency
    of the network transfer can be improved by combining it with compression, such
    as gzip.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large objects can be cached independently of other data. This makes sense given
    the size of the objects and because it is likely that the large objects change
    less frequently than the structured data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where possible, you should also try to limit the size of large objects. When
    displaying images on a client device, the performance can be improved by scaling
    down the images to a lower resolution on the server.
  prefs: []
  type: TYPE_NORMAL
- en: You can limit the size of large objects at the very beginning. If your application
    allows users to upload files, you should consider constraining the uploaded file
    size.
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented a separate endpoint for product photos in our example Product
    API in the *Uploading and downloading files via the REST API* section in [*Chapter
    6*](B21843_06.xhtml#_idTextAnchor135) .
  prefs: []
  type: TYPE_NORMAL
- en: In that example code, we used a method of the `MultipartFile` class to get the
    file content as a byte array. We also return a byte array in the method used to
    download the file. This approach allows our code to stay simple, but it has the
    drawback that the whole file is stored in the heap memory at once, although we
    do not do any processing that requires having the whole file in memory.
  prefs: []
  type: TYPE_NORMAL
- en: We could optimize the memory usage by using `InputStream` / `OutputStream` instead
    of the byte array. Or we could use a reactive framework such as WebFlux (more
    on that in [*Chapter 11*](B21843_11.xhtml#_idTextAnchor310) ). However, we would
    have to go all the way, that is, also persisting the content using methods supporting
    streaming access.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Caching may be the most popular performance improvement strategy. However, caching
    is not a silver bullet, and designing a correct caching setup is not easy, so
    before adopting this strategy, you should make sure you have considered the option
    of improving performance by applying a proper API design, as described in the
    previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching involves a trade-off: it provides shorter response times at the cost
    of additional memory to store cached data and the risk of potential inconsistency
    if the cached data becomes stale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching is closely tied to the challenge of cache invalidation—determining
    when data items in the cache should be removed. This may occur either because
    the cached data has become stale or to free up memory when it is unlikely that
    the cached items will be accessed again. Cache invalidation is considered one
    of the hardest problems in computing. The two most common ways to detect items
    that need to be removed from the cache are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Least recently used* , based on the assumption that a resource that was last
    used a long time ago is unlikely to be used again'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Least frequently used* , based on the assumption that resources that have
    been used frequently are likely to be used again'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To speed up the operations of an API, we can consider caching on the consumer
    (client) side of the API (caching the HTTP responses) and on the provider (server)
    side of the API (caching data needed to perform the operation and return a result).
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching on the provider (server) side can take many forms: it can be data from
    a database, results of expensive computations, or semi-processed results of downstream
    API calls. Caching on the service provider side is not specific to just API implementations,
    and therefore, it is beyond the scope of this book. For detailed guidance, you
    can refer to Mastering Spring Boot 3.0 by Ahmet Meric which covers broader caching
    strategies within Spring-based applications. For database access using JPA/Hibernate,
    it is advisable to understand the concept of first- and second-level cache. For
    general caching at the Spring component level, you should know about Spring Cache
    ( [https://docs.spring.io/spring-boot/reference/io/caching.html](https://docs.spring.io/spring-boot/reference/io/caching.html)
    ) and the `@Cacheable` annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: To support the cacheability of data, the operations producing the data should
    be stateless, meaning we should get the same output data if the inputs are the
    same. This allows us to share the cached data across multiple instances of a service
    or multiple services using distributed caches such as Hazelcast or Redis. This
    way, data put in the cache by one instance of a service can be reused by another
    instance needing the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Caching on the client side
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let’s focus on the caching on the client side. As the client does not have
    access to the implementation details of the service, it must use some hints to
    decide for which API responses and for how long a cached response can be used
    instead of calling the API again. Remember that client-side caching need not be
    done in the end client; it can also be done by a proxy server sitting between
    the server and the end client.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the details of cache control, it is important to verify the
    structure and granularity of the resources (endpoints) the API consists of. Consider
    placing data items that rarely change (they are good candidates for caching) in
    a separate resource from that of other items that change frequently. Only then
    can the caching be aligned well with the modification patterns of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Requests using the `GET` method are considered cacheable by default. This rule
    is used by most web browsers; therefore, users sometimes need to force the browser
    to reload the web page (e.g., pressing *Ctrl* + *F5* on the Windows operating
    system) in case the page content has changed faster than the browser expected.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, the `PUT` , `PATCH` , and `DELETE` methods used to modify data on
    the server cannot be implemented without contacting the server, so they are not
    suitable for caching.
  prefs: []
  type: TYPE_NORMAL
- en: The `POST` method can be used to modify data on the server, such as creating
    a new order in our example Order Management API. It can also be used for read-only
    operations, usually for complex queries when you want to use a request body instead
    of putting the operation inputs in the URL or request headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more detailed caching control, irrespective of the HTTP method used, can
    be achieved with standard HTTP response headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Cache-Control` : Supports a detailed specification of who (proxies or the
    end client) can cache the response with the same URL and when (for how long, based
    on a validation request)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ETag` : Short for “entity tag,” a value that can be used to check whether
    a resource has changed or not (in combination with the `If-None-Match` request
    header in the subsequent request to the same URL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Old headers ( `Expires` , `Last-Modified` , `Pragma` ), still supported for
    backward compatibility, are already superseded by the `Cache-Control` header.
  prefs: []
  type: TYPE_NORMAL
- en: Example – caching product photos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will demonstrate client-side caching using the standard HTTP headers on the
    product photo download endpoint created in [*Chapter 6*](B21843_06.xhtml#_idTextAnchor135)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Photos are relatively large data objects that are not expected to change very
    frequently, hence, they are a good candidate for caching.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will use the `Cache-Control` header to specify the time the client
    can store the photo in the cache. Inside the `ProductsApiController` class, we
    will change the `downloadProductPhoto` method body to include the header, specifying
    20 seconds as the time to cache the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start the Product API application and add a test product and its
    image by calling the `PUT` endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the caching header tells the client it can store the resource
    in its cache, but the client is not required to do so. Simple clients may ignore
    the caching headers completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a web browser because it understands and supports the caching headers;
    however, it does not load the `main` resource (the one whose URL is entered in
    the address bar) from the cache. This is why we will create an HTML file named
    `refer_img.html` that we will use as the `main` resource, and inside the HTML
    file we will refer to the image we want to download or cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can open a new browser tab and turn on the developer tools (by pressing
    *F12* on Windows/Linux or *Option* + *⌘* + *I* on Mac). Within the developer tools,
    we switch to the **Network** tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will open the `refer_img.html` file in the browser by dragging and dropping
    the file in the browser window. When the HTML file is opened for the first time,
    the browser must make the HTTP request to download the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Downloading a new resource that is not available in the cache
    yet](img/B21843_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Downloading a new resource that is not available in the cache
    yet
  prefs: []
  type: TYPE_NORMAL
- en: We can see the response headers, including **Cache-Control:** **max-age=20**
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'If we reload the page within 20 seconds, we should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Browser using the cached resource](img/B21843_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Browser using the cached resource
  prefs: []
  type: TYPE_NORMAL
- en: The text next to **Status Code** and the warning in the **Request Headers**
    section indicate that this time, no real network transmission has been done, and
    the cached image is displayed instead. In the **Response Headers** section, we
    can see that the access time (the **Date** header) is the same as before because
    the headers are cached as well.
  prefs: []
  type: TYPE_NORMAL
- en: If we reload the page after more than 20 seconds, the image is freshly downloaded
    from the server, and we will see a new time as the value of the **Date** header.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make the caching more sophisticated with the `ETag` header. To make the
    value of the header change if and only if the image changes, we will compute the
    entity tag as a hash of the image bytes. The following method computes the hash
    using the SHA-1 algorithm and converts it to a printable string using Base64 encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can change the controller method to use the `ETag` header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After restarting the application, we can start testing again. The first load
    of the page will download the image normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Browser downloading the resource with a new ETag](img/B21843_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Browser downloading the resource with a new ETag
  prefs: []
  type: TYPE_NORMAL
- en: The **Response Headers** section contains the **Etag** header with the computed
    hash string.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, no matter how long we wait, when reloading the page we will see
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Server responding with “304 Not Modified” when the ETag value
    is matched](img/B21843_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Server responding with “304 Not Modified” when the ETag value
    is matched
  prefs: []
  type: TYPE_NORMAL
- en: The **Date** header is fresh, meaning the browser did send a real request to
    the server. However, the **Status code** is **304 – Not Modified** , and no content
    is sent back.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – No content is returned with the “304 Not Modified” response](img/B21843_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – No content is returned with the “304 Not Modified” response
  prefs: []
  type: TYPE_NORMAL
- en: This is because the browser, when making the request, sent the `If-None-Match`
    request header containing the `ETag` value it got in the previous request. Spring
    Framework automatically compares the value with the newly computed one, and if
    they match, it returns the `304` status in the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check what happens when we upload a new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On the next reload of the page, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Server responding with “200 OK” and the full content when ETag
    not matched](img/B21843_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Server responding with “200 OK” and the full content when ETag
    not matched
  prefs: []
  type: TYPE_NORMAL
- en: As the **Etag** value has changed, the full content is returned, and the status
    code is **200 OK** .
  prefs: []
  type: TYPE_NORMAL
- en: Command Query Responsibility Segregation (CQRS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Command Query Responsibility Segregation** ( **CQRS** ) pattern is designed
    to separate read and write operations, improving performance and scalability in
    applications. By separating data modification (commands) from data retrieval (queries),
    each operation can be optimized independently. This separation is particularly
    useful in applications where read and write operations require different scalability
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, an application with high read traffic can scale read operations
    without affecting write performance. This prevents slowdowns during read operations
    when resource-intensive write processes occur. In our example from [*Chapter 2*](B21843_02.xhtml#_idTextAnchor050)
    , we applied CQRS principles in a single application, but in practice, CQRS typically
    involves deploying two separate applications: one handling reads and the other
    handling writes.'
  prefs: []
  type: TYPE_NORMAL
- en: By segregating read and write responsibilities, CQRS improves application performance
    by eliminating dependency between these operations, enabling independent scaling.
    To ensure both applications share the same API endpoint, a gateway can be used
    to redirect traffic to the correct application based on the operation being performed.
  prefs: []
  type: TYPE_NORMAL
- en: Echoing request data is not required
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When implementing a POST or PUT endpoint, a commonly used convention is to
    return the created or updated entity in the HTTP response body. We even use this
    pattern in the design and code examples in our book. This pattern is usually connected
    to the reuse of data models: we already have a model for the entity that is used
    in the request and in the GET response, so why not use it also for the POST and
    PUT responses?'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is nothing in the HTTP protocol or REST architecture telling us to echo
    back what we got in a POST or PUT request. On the contrary, there are some arguments
    against it:'
  prefs: []
  type: TYPE_NORMAL
- en: Waste of network bandwidth and an increase in response time, especially for
    large entities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client already has the data because it has just sent them in the request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can lead to incorrect assumptions on the client side that the returned data
    are always up to date, but that might not be true if some other request on the
    same entity is performed in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if there is some additional information generated on the server, why do
    we automatically assume that the client needs the modified parts or the whole
    entity data? It breaks the single responsibility principle: a POST or PUT is expected
    to do a create/update. If the client needs to read the data, it can send a GET
    request. Different models for the read and write operations are also a natural
    consequence of applying the CQRS principle explained earlier in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HTTP, the heart of RESTful web services, is built around the synchronous request-response
    style of communication, making it easy to understand and implement. When talking
    about asynchronous APIs, we usually think of systems based on message brokers
    using protocols other than HTTP. Messaging-based communication is one of the main
    alternatives to RESTful web services mentioned in [*Chapter 1*](B21843_01.xhtml#_idTextAnchor015)
    . However, HTTP can also support asynchronous processing.
  prefs: []
  type: TYPE_NORMAL
- en: If the operation to be exposed via a REST API involves processing that takes
    a long time, waiting for the processing to finish before returning a response
    blocks the API client makes the API seem unresponsive and may lead to time-outs.
    The responsiveness of such an API can be improved by returning a response immediately
    after reading the request and saving the input data from it in reliable storage.
  prefs: []
  type: TYPE_NORMAL
- en: The standard response status code in this case is `202 - Accepted` . The long-running
    processing can continue asynchronously without blocking the client. A separate
    API endpoint can be provided for the client to poll the status of the processing
    and receive its results when finished.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we wanted to improve the photo upload endpoint of our example
    Product API so that it scales the photo to standard dimensions or performs other
    graphics enhancements on it. This kind of processing can be time-consuming. In
    such cases, we can decide to change the upload endpoint to an asynchronous one.
    We would store the unprocessed photo and return a `202` status code. The photo
    enhancement would be done asynchronously and a `GET` endpoint would be available
    for checking whether the photo is ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: After explaining how to ensure performance and scalability using general approaches
    at the level of analysis, design, and the HTTP standard, in the next section,
    we will move more to the level of implementation and focus on one feature of current
    versions of the Java platform that is particularly relevant to the performance
    and scalability of applications communicating using APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the throughput with virtual threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtual threads are a Java feature (final since Java 21) related to the performance
    of concurrent processing, a topic highly relevant to API implementation. All server-side
    applications are concurrent because they must handle concurrent incoming requests.
    However, with Java server-side frameworks such as Spring Boot, the code can focus
    on just one request using the so-called thread-per-request model. More advanced
    applications may need multiple threads per request. We will explore both usages
    of concurrency in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collector for threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before going into more exact and detailed descriptions, let’s use a simple metaphor
    to explain the point of virtual threads even to developers who may find threads
    and concurrency difficult to understand. All Java developers must understand the
    concept of garbage collection, which is an abstraction that the Java Virtual Machine
    provides to liberate developers from the responsibility for allocating and freeing
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can still get an `OutOfMemoryError` , but if your application
    uses the objects in a typical way, at some point objects that are not used anymore
    are not referenced from any other object, so the runtime can detect those objects
    automatically, and thus the memory they occupy can be freed and reused. This abstraction,
    used by programmers since the very first version of Java, simplifies the code
    and prevents many hard-to-detect errors related to direct memory access and allocation
    known from programming in C or other languages without a garbage collector.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to what a garbage collector does with memory, virtual threads provide
    the illusion of an (almost) infinite number of threads. What really happens is,
    in a typical application using one thread per incoming network request (more on
    that in the next section), the JVM, together with the standard libraries, can
    automatically detect when the platform thread would be blocked due to waiting
    (e.g., for I/O). To prevent this, the virtual thread state is moved to the heap,
    and the platform thread is reused to work on some other request.
  prefs: []
  type: TYPE_NORMAL
- en: For most applications, relying on virtual threads to efficiently utilize platform
    threads, rather than using platform threads directly, will likely depend on the
    garbage collector to clean up memory objects at the end of each request.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Illustration of how virtual threads work, split into layers](img/B21843_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Illustration of how virtual threads work, split into layers
  prefs: []
  type: TYPE_NORMAL
- en: Thread-per-request model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our example APIs are implemented using Spring Web, a framework using the thread-per-request
    model. This means that for every HTTP request, the web server used by the framework
    takes a dedicated Java thread and, within that thread, it invokes the controller
    method matching the request path and method.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of the thread-per-request model is that it allows the classic
    imperative programming style with code blocks containing statements executed in
    the order they appear in the source code. This code style is easy to read and
    understand. It is also easy to debug because when the execution stays within one
    thread, the call stack displayed in debugging tools is complete, letting the developer
    track the method invocations n icely at all code levels.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the thread-per-request model without virtual threads has limited
    scalability because the threads provided by the operating system (so-called platform
    threads) are an expensive resource. Every platform thread occupies a considerable
    amount of memory, so the number of platform threads a JVM can use is limited.
    Web servers using platform threads must take a thread that is needed to handle
    an incoming request from a limited thread pool. If there are no threads left in
    the pool, the request waits until a thread is available.
  prefs: []
  type: TYPE_NORMAL
- en: The controller method and any methods invoked from the controller method (let’s
    call it handling code) hold the dedicated thread until the controller method returns
    or throws, transferring the execution back to the Spring Web framework. It is
    quite common that the handling code contains blocking operations. A blocking operation
    prevents the CPU from performing useful work because it halts execution while
    waiting for required data.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let’s look at the execution of the “create order” operation
    of the Order Management API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first look at `OrderManagementApiController.java` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s see `OrdersCommandUseCaseImpl.java` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Blocking operations (commonly caused by the need for network I/O communication)
    can have a disastrous performance impact because, in many applications, they take
    orders of magnitude longer than the surrounding non-blocking (CPU-heavy) operations.
    Communication is an essential part of the processing within distributed applications
    (microservices). It is important to understand that the time spent waiting for
    data to be transferred over a remote API (or from a complex database query) is
    typically hundreds of milliseconds to seconds. In contrast, the CPU processing
    time needed to perform the business logic of most enterprise applications once
    all the data is available (even if we consider more complex operations such as
    JSON parsing and serialization) is orders of magnitude shorter – it is counted
    in microseconds.
  prefs: []
  type: TYPE_NORMAL
- en: This means that with a thread-per-request model, the CPU that is blocked by
    the input/output operations cannot be used to serve other requests; in a service
    under heavy load, more than 99% of the time, the CPU will be idle.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to address this problem is to use a reactive programming framework
    (more on that in [*Chapter 11*](B21843_11.xhtml#_idTextAnchor310) ), but that
    means abandoning the thread-per-request model with its advantages described previously:
    code that is easy to understand and debug.'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual threads are clever because they let developers keep the thread-per-request
    model while allowing efficient use of the platform threads. When a virtual thread
    needs to use the CPU, it is mounted on a platform thread (also called a *carrier
    thread* ). When a virtual thread hits a blocking operation, its state of execution
    is stored in the heap memory and its carrier thread is released so that it can
    be used to execute a different virtual thread. When the blocking operation finishes,
    the virtual thread can mount any free carrier thread (that can be different from
    the one it used before getting blocked), restore its state from the heap, and
    continue its execution.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual threads, in contrast to platform threads, are cheap, so an application
    can use millions of them. An HTTP server and an application framework can provide
    a new virtual thread for every incoming request, no matter how many requests are
    already waiting for an I/O operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use virtual threads to process incoming requests, you just need to configure
    the framework to turn on the feature. With Spring Boot 3.2 or later and Java 21
    or later, you can use the following configuration property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can continue using the code style containing the blocking operations, knowing
    that the blocked threads will be virtual ones, and the CPU will be used efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Besides handling many incoming requests using the thread-per-request model,
    an application may need to explicitly perform multiple operations in parallel
    to handle just one incoming request.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing within one request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of integration-heavy applications, a common case is optimizing
    the latency when handling the request requires multiple independent remote APIs
    to be called.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a price quote API that calls the APIs of multiple vendors, compares
    their prices, and responds with the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Java provides a useful abstraction in the form of the `ExecutorService` class.
    Each operation that should be executed in parallel is a task that you submit to
    `ExecutorService` .
  prefs: []
  type: TYPE_NORMAL
- en: When the `ExecutorService` class is created, you specify a strategy it should
    use to manage the threads that execute the tasks. Without virtual threads, the
    typical strategy would be to use a thread pool of a fixed size. The thread pool
    allows a certain number of parallel tasks, but the number is limited because the
    number of platform threads is limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'With virtual threads, we can let `ExecutorService` use a new virtual thread
    for every task without a thread pool and without having to specify the size of
    that pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We have briefly explained that, in a nutshell, virtual threads enhance CPU efficiency
    by minimizing idle threads and ensuring more time is spent performing useful operations.
    The immediate effect of that is an increase in throughput (the amount of work
    done per unit of time). However, throughput and response time are usually connected.
    The increased throughput means that a given CPU power can handle a higher load
    without some of the requests having to wait for a thread to become available.
    Consequently, the response time can be improved as well.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed ways to improve performance and scalability that can
    be applied mostly within individual services. However, the infrastructure components
    that reside between the individual components connected by APIs are usually also
    involved in achieving satisfactory performance and scalability of the entire distributed
    system (the application). The following section will briefly introduce you to
    these infrastructure components.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual thread pinning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are situations when a virtual thread performing a blocking operation cannot
    release the platform thread for other virtual threads. Using the official terminology
    for this situation, we say that the virtual thread cannot unmount from its carrier
    because the virtual thread is pinned to the carrier thread.
  prefs: []
  type: TYPE_NORMAL
- en: As of Java 21, thread pinning can occur in two cases.
  prefs: []
  type: TYPE_NORMAL
- en: The first case of thread pinning is when the virtual thread is performing a
    native code. This is inevitable because the JVM cannot take away the platform
    thread from the non-Java code safely.
  prefs: []
  type: TYPE_NORMAL
- en: The second case is when the virtual thread runs code inside a synchronized block
    or method. This is a limitation of the implementation of the support for virtual
    threads within JVM. If your application code or the code of some of its dependencies
    (web server or libraries) uses Java synchronized constructs, it may limit the
    performance of virtual threads compared to the expectation. In some cases, it
    can even lead to deadlocks where all the available platform threads are blocked
    by pinned threads waiting on a synchronized lock.
  prefs: []
  type: TYPE_NORMAL
- en: This means you should not turn on virtual threads blindly. It is recommended
    to run performance tests on your application with and without virtual threads.
    You should be aware of your application dependencies and try to use the latest
    versions of them. Remember that Spring Boot uses the embedded Tomcat as the web
    server (servlet container) by default, but it also supports switching to other
    web servers.
  prefs: []
  type: TYPE_NORMAL
- en: The pinning of virtual threads caused by the synchronized constructs is eliminated
    in Java 24. So, even if the Java version you use supports all the features your
    application needs, it is always a good idea to upgrade to a new Java version as
    soon as possible to get fixes and improvements to the implementation of the Java
    features you use. New versions of Java/OpenJDK also come with improvements in
    tooling that can help to diagnose and fix various performance problems, possibly
    including virtual thread pinning.
  prefs: []
  type: TYPE_NORMAL
- en: Using infrastructure support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, you want to design and implement your APIs so they can work well without
    depending on a specific infrastructure setup. However, software developers should
    be aware that components external to the services they develop can be used to
    enhance the performance and scalability of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several types of infrastructure components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancer** : A component distributes the requests among multiple service
    instances providing the same API. A load balancer is inevitable for horizontal
    scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proxy** : A component that is an HTTP server and client at the same time.
    It receives a request and fulfills it by making another request to the actual
    service providing the API. Proxies let us add various functionalities between
    the client and the server, of which caching is the most important one for performance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A proxy that receives requests with public URLs and turns them into requests
    within a private network is called a **reverse proxy** .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxies combining different functionalities supporting APIs are also called
    **API gateways** .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content delivery network (CDN)** : This is a network of proxies that are
    geographically distributed so that a client can access a URL via a nearby proxy,
    shortening the response times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components mentioned here often combine performance and security concerns.
    One problem where the overlap is obvious is the deflecting of **denial of service**
    ( **DoS** ) and **distributed denial of service** ( **DDoS** ) attacks. It involves
    throttling, which is limiting excessive traffic from one source to ensure that
    the service remains available to all its clients.
  prefs: []
  type: TYPE_NORMAL
- en: It follows from this that the architecture in which an API-providing service
    is embedded may contain multiple components that are often beyond the control
    of the API developers. This is another reason the best possible approach to API
    design is to adhere to the standards defined by HTTP and other internet standards
    (RFCs). This will ensure that your API works well with the surrounding infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: We have gone through several possible performance and scalability issues, along
    with ways to address them. However, in the last section of this chapter, it is
    essential to show how to examine the performance of a particular service under
    a particular workload, because only in this way can the real problems be detected
    and targeted for resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Designing and executing effective load tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21843_08.xhtml#_idTextAnchor223) explained various the tests
    that can be used to ensure the correctness of APIs, meaning that these tests check
    that the APIs and services behind them process the input data as `expected` and
    respond with the expected output data. They focus on the exact values of the detailed
    attributes of the input and output data and their semantics to verify the functional
    requirements of the application. To do so, they try different test data combinations
    to cover the most common use cases as well as negative scenarios and edge cases.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of performance and scalability, we test the application from
    a different angle. Instead of categorizing test cases by functional requirements
    and checking the exact data values, we usually try a smaller set of use cases
    characterized by the expected load they put on the application.
  prefs: []
  type: TYPE_NORMAL
- en: However, the number of repetitions of identical or very similar requests sent
    to the application will be significantly higher. The timing aspect is very important,
    so the requests are made at a frequency that simulates real traffic, including
    load fluctuations. Multiple simultaneous requests are sent to simulate many users
    using the application at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: These differences in load testing in comparison to the other types of test imply
    that although it is possible to use the same tools for both, doing load testing
    using tools that specialize in it is easier, more resource-efficient, and provides
    results that are more accurate and presented in an appropriate form.
  prefs: []
  type: TYPE_NORMAL
- en: Example – load-testing the Order Management API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will load-test our example Order Management API using the tool called Gatling
    (its documentation is available at [https://docs.gatling.io/](https://docs.gatling.io/)
    ). Gatling integrates with Java very well, so we can include the load test in
    our Java source code and run it using a Maven plugin without having to install
    a separate program.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will add the Gatling dependencies (with the `test` scope) to the
    `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can start writing the test class, also known as a Gatling simulation.
    Our simulation will try to create many new orders via the Order Management API:'
  prefs: []
  type: TYPE_NORMAL
- en: '`public class CreateOrderSimulation extends Simulation {`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The top level of the test is in the class constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The code refers to the `HTTP_PROTOCOL_BUILDER` and `POST_SCENARIO_BUILDER`
    constants, which describe the requests that should be sent to the tested API.
    The `injectOpen` method specifies the timing and parallelization of the requests:'
  prefs: []
  type: TYPE_NORMAL
- en: During the first 10 seconds of the simulation, the rate of users (requests)
    grows from 10 per second to 300 per second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the following 80 seconds, the rate of requests is constant at 300 per
    second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end, we can see the test assertions that expect the maximum response
    time of 5,000 milliseconds and that more than 90% of the requests should be successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the performance differences more visible, we will modify the code of
    the Product API (the API the Order Management API depends on), adding an artificial
    delay of 1 second in the `getProductById` method of the `ProductsQueryUseCaseImpl`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start the Product API application. Our test orders contain the
    product ID AK21101, so we need to create it using the Product API. We can use
    the Swagger UI, the `curl` command, or, if we use IntelliJ IDEA, we can use a
    simple text file with the `.http` extension to send an HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we start the Product Management API with these two configuration properties
    disabled by commenting them out using the `#` character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The load test is started by running the following Maven command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As the test is running, we can observe that after the load reaches a certain
    point, many requests fail (the count of `KO` , meaning `not OK` , is growing).
  prefs: []
  type: TYPE_NORMAL
- en: The whole test fails because neither the response time nor the percentage of
    successful requests is fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The detailed graphical visualization of the test can be found in the `target/gatling`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Most of the requests failed as the service was not able to
    cope with the load](img/B21843_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Most of the requests failed as the service was not able to cope
    with the load
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try to improve the performance by enabling virtual threads in the
    Order Management API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After restarting the application and the Gatling test, we get different results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Enabling virtual threads revealed a bottleneck in the database
    connection pool](img/B21843_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Enabling virtual threads revealed a bottleneck in the database
    connection pool
  prefs: []
  type: TYPE_NORMAL
- en: The number of successful responses increased slightly, but still, most of them
    failed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the console log output of the Order Management API application,
    we find many repetitions of the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: By enabling virtual threads, we removed the limiting caused by a thread pool
    on the number of requests our application was able to process in parallel. However,
    another part of the application has now become the bottleneck, namely the database
    connection pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default limit for the Hikari connection pool is 10 database connections,
    and that is exhausted very quickly given the 1-second delay of the Product API.
    The Order Management API must wait until it can finish the database transaction
    and release the connection. To get rid of the bottleneck, we set the maximum number
    of connections to `500` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is just a simple example to make the impact of virtual threads
    very visible. In a real application, such a high number of connections could cause
    a problem with the database, and we would probably have to use a different solution,
    such as starting the transaction only after we get the price from the Product
    API or fixing the long response time of the Product API.
  prefs: []
  type: TYPE_NORMAL
- en: After this change, we can restart the Order Management API and the Gatling test
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, finally, our test passes, and the results are very satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Service using virtual threads can handle the high frequency
    of requests](img/B21843_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Service using virtual threads can handle the high frequency of
    requests
  prefs: []
  type: TYPE_NORMAL
- en: All requests were OK, and all the response times were very close to 1 second
    from the Product API.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the specifics of performance and scalability in
    the context of RESTful APIs. We explored various design approaches and techniques
    to improve the two non-functional requirements. We saw an example of using caching
    headers to reduce the volume of network traffic between the client and the server.
    We delved deeper into virtual threads, a highly performance-relevant feature of
    new Java versions. Finally, we verified the achievement of the expected performance
    improvements using a load test. In the next chapter, we will show how the principles
    described in this book can be applied to back-end Java frameworks beyond Spring
    Boot, and how you can make your application more future-proof and vendor-neutral
    by using community-driven standards.
  prefs: []
  type: TYPE_NORMAL
