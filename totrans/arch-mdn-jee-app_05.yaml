- en: Container and Cloud Environments with Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last years have shown a lot of interest in container as well as cloud technology.
    The vast majority of companies building software is at least considering migrating
    environments to these modern approaches. In all of my recent projects these technologies
    have been a point of discussion. Especially, introducing container orchestration
    technologies greatly affects the way how applications are run.
  prefs: []
  type: TYPE_NORMAL
- en: What are the benefits of container technologies? And why should companies care
    about the cloud? It seems a lot of these concerns are used as buzzwords, as a
    *silver bullet* approach. This chapter will examine the motivations behind these
    technologies. We will also see if and how the Java EE platform is ready for this
    new world.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: How infrastructure as code supports operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container technologies and orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why especially Java EE fits these technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud platforms and their motivations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12-factor, cloud native enterprise applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivations and goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the motivations behind containers, container orchestration, and cloud
    environments? Why do we see such momentum in this area?
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, enterprise application deployment worked something like the following.
    Application developers implemented some business logic and built the application
    into a packaged artifact. This artifact was deployed manually on an application
    server that was managed manually as well. During deployment or reconfiguration
    of the server, the application usually faced a downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this approach is a rather high-risk process. Human, manual tasks
    are error-prone and are not guaranteed to be executed in the same manner each
    and every time. Humans are rather bad at executing automated, repetitive work.
    Processes such as installing application servers, operating systems and servers
    in general, require precise documentation, especially for future reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, tasks for operation teams typically were ordered using a ticket
    system and performed manually. By doing so, installation and configuration of
    servers held the risk of transforming the system into a non-reproducible state.
    Setting up a new environment identical to the current one required a lot of manual
    investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Operational tasks need to be automated and reproducible. Installing a new server,
    operating system or runtime should always execute in exactly the same manner.
    Automated processes not only speed up the execution but introduce transparency,
    revealing which precise steps have been executed. Reinstalling environments should
    produce exactly the same runtime including all configuration and setup as before.
  prefs: []
  type: TYPE_NORMAL
- en: This also includes deployment and configuration of the application. Instead
    of manually building and shipping applications, Continuous Integration servers
    are in charge of building software in an automated, reliable, and reproducible
    way. CI servers act as *golden source of truth* for software builds. The artifacts
    produced there are deployed on all involved environments. A software artifact
    is built once, on the Continuous Integration server, and then verified in integration
    and end-to-end tests, until it ends up in production. The same application binary
    that is deployed to production is therefore reliably tested upfront.
  prefs: []
  type: TYPE_NORMAL
- en: Another very important aspect is to be explicit in the software versions that
    are being used. This includes all used software dependencies, from the application
    server and Java runtime, down to the operating system and its binaries. Rebuilding
    or reinstalling software should result in exactly the same state each and every
    time. Software dependencies are a complex subject which comes with a lot of possibilities
    for potentials errors. Applications are tested to work properly on specific environments
    with specific configurations and dependencies. In order to guarantee that the
    application will work as expected, it is shipped in exactly that configuration
    that has been verified before.
  prefs: []
  type: TYPE_NORMAL
- en: This aspect also implies that test and staging environments which are used to
    verify the application's behavior should be as similar to production as possible.
    In theory this constraint sounds reasonable. From experience the used environments
    vary quite a lot from production in terms of software versions being used, network
    configuration, databases, external systems, number of server instances, and so
    on. In order to test applications properly these differences should be erased
    as much as possible. In section *Containers* we will see how container technology
    supports us here.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A logical conclusion to enable reproducible environments is to make use of **infrastructure
    as code** (**IaC**). The idea is that all required steps, configuration, and versions
    are explicitly defined as code. These code definitions are directly used to configure
    the infrastructure. Infrastructure as code can be implemented in a procedural
    form, such as scripts, or in a declarative way. The latter approach specifies
    the desired target state and is executed using additional tooling. No matter which
    approach is preferred, the point is that the whole environment is specified as
    code, being executed in an automated, reliable, and reproducible way, always producing
    the same results.
  prefs: []
  type: TYPE_NORMAL
- en: In any way, the approach implies that the manual steps are kept to a minimum.
    The easiest form of infrastructure as code are shell scripts. The scripts should
    be executed from soup to nuts without human involvement. The same holds true for
    all IaC solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally the responsibility of installing and configuring environments moves
    from an operations team more toward developers. Since the development team sets
    certain requirements on the required runtime it makes sense for all engineering
    teams to work together. This is the idea behind the DevOps movement. In the past
    the mindset and method of operating too often was that application developers
    implemented software and literally passed the software and responsibilities toward
    operations - without further involvement on their side. Potential errors in production
    primarily concerned the operations team. This unfortunate process not only leads
    to tensions between engineering teams but ultimately lower quality. However, the
    overall goal should be to deliver high quality software that fulfills a purpose.
  prefs: []
  type: TYPE_NORMAL
- en: This goal requires the accountability of application developers. By defining
    all required infrastructure, configuration, and software as code, all engineering
    teams naturally move together. DevOps aims toward accountability of the software
    team as a whole. Infrastructure as code is a prerequisite which increases reproducibility,
    automation, and ultimately software quality.
  prefs: []
  type: TYPE_NORMAL
- en: In the topic *Containers* and *Container orchestration frameworks*, we will
    see how the presented technologies implement IaC.
  prefs: []
  type: TYPE_NORMAL
- en: Stability and production readiness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The practices of Continuous Delivery include what needs to be done in order
    to increase the quality and value of the software. This includes the stability
    of the application. Reconfiguring and redeploying software does not have to result
    in any downtime. New features and bugfixes do not have to be shipped exclusively
    during maintenance windows. Ideally the enterprise software can continuously improve
    and move forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *zero downtime* approach involves a certain effort. In order to avoid an
    application being unavailable, at least one other instance of the software needs
    to be present at a time. A load balancer or proxy server upfront needs to direct
    the traffic to an available instance. *Blue-green* deployments make use this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e8d1123-39ac-485a-971c-22ecd5258517.png)'
  prefs: []
  type: TYPE_IMG
- en: The application instances including their databases are replicated and proxied
    by a load balancer. The involved applications typically represent different software
    versions. Switching the traffic from the *blue* to the *green* path and vice versa
    instantly changes the version, without any downtime. Other forms of blue-green
    deployments can include scenarios of multiple application instances that are all
    configured to use the same database instance.
  prefs: []
  type: TYPE_NORMAL
- en: This approach obviously does not have to be realized using some shiny new technology.
    We have seen blue-green deployments that enable zero-downtime in the past using
    home-grown solutions. However, modern technologies support these techniques increasing
    stability, quality, and production-readiness out of the box without much engineering
    effort required.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last years have shown a lot of interest in **Linux container** technology.
    Technically this approach is not that new. Linux operating systems such as **Solaris**
    supported containers a long time ago. However, **Docker** made a the breakthrough
    in this technology by providing features to build, manage and ship containers
    in a uniform way.
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between containers and **virtual machines** (**VMs**)
    and what makes containers that interesting?
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines act like a computer in a computer. They allow the runtime to
    be easily managed from the outside such as creating, starting, stopping, and distributing
    machines in a fast and ideally automated way. If new servers need to be setup,
    a blueprint, an image, of the required type can be deployed without installing
    software from scratch every time. Snapshots of running environments can be taken
    to easily backup the current state.
  prefs: []
  type: TYPE_NORMAL
- en: In many ways containers behave like virtual machines. They are separated from
    the host as well as other containers, run in their own network with their own
    file system and potentially own resources. The difference is that virtual machines
    run on a hardware abstraction layer, emulating a computer including operating
    system, whereas containers run directly in the host's kernel. Unlike other kernel
    processes, containers are separated from the rest of the system using operating
    system functionality. They manage their own file system. Therefore, containers
    behave like separate machines but with native performance without the overhead
    of an abstraction layer. The performance of virtual machines is naturally decreased
    by their abstraction. Whereas virtual machines provide full flexibility in choosing
    operating systems, containers will always run in the same kernel and therefore
    in the same version as the host operating system. Containers therefore do not
    ship their own Linux kernel and can be minimized to their required binaries.
  prefs: []
  type: TYPE_NORMAL
- en: Container technologies such as Docker provide functionality to build, run, and
    distribute containers in a uniform way. Docker defines building container images
    as IaC which again enables automation, reliability, and reprocibility. Dockerfiles
    define all the steps that are required to install the application including its
    dependencies, for example, an application container and the Java runtime. Each
    step in the Dockerfile corresponds to a command that is executed at image build
    time. Once a container is started from an image it should contain everything which
    is required to fulfill its task.
  prefs: []
  type: TYPE_NORMAL
- en: Containers usually contain a single Unix process which represents a running
    service, for example an application server, a web server, or a database. If an
    enterprise system consists of several running servers, they run in individual
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest advantages of Docker containers is that they make use of
    a **copy-on-write** file system. Every build step, as well as every running container
    later on, operates on a layered file system, which does not change its layers
    but only adds new layers on top. Built images therefore comprise multiple layers.
  prefs: []
  type: TYPE_NORMAL
- en: Containers that are created from images are always started with the same initial
    state. Running containers potentially modify files as new, temporary layers in
    the file system, which are discarded as soon as the containers are stopped. By
    default, Docker containers are therefore stateless runtime environments. This
    encourages the idea of reproducibility. Every persistent behavior needs to be
    defined explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: The multiple layers are beneficial when rebuilding and redistributing images.
    Docker caches intermediate layers and only rebuilds and retransmits what has been
    changed.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an image build may consist of multiple steps. System binaries are
    added first, then the Java runtime, an application server, and finally our application.
    When changes are made to the application and a new build is required, only the
    last step is re-executed; the previous steps are cached. The same is true for
    transmitting images over the wire. Only the layers that have been changed and
    that are not yet existent on the target registry, are actually retransmitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following illustrates the layers of a Docker image and their individual
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b978dac1-a27b-435c-8c26-dfda3b4ad020.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker images are either built from scratch, that is from an empty starting
    point, or built upon an existing base image. There are tons of base images available,
    for all major Linux distributions containing package managers, for typical environment
    stacks as well as for Java-based images. Base images are a way to build upon a
    common ground and provide basic functionality for all resulting images. For example,
    it makes sense to use a base image including a Java runtime installation. If this
    image needs to be updated, for example, to fix security issues, all dependent
    images can be rebuilt and receive the new contents by updating the base image
    version. As said before, software builds need to be repeatable. Therefore we always
    need to specify explicit versions for software artifacts such as images.
  prefs: []
  type: TYPE_NORMAL
- en: Containers that are started from previously built Docker images need access
    to these images. They are distributed using Docker registries such as the publicly
    available DockerHub or company-internal registries to distribute own images. Locally
    built images are pushed to these registries and retrieved on the environments
    that will start new containers later on.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE in the container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As it turns out the approach of a layered file system matches Java EE's approach
    of separating the application from the runtime. Thin deployment artifacts only
    contain the actual business logic, the part which changes and which is rebuilt
    each and every time. These artifacts are deployed onto an enterprise container
    which does not change that often. Docker container images are built step-by-step,
    layer-by-layer. Building an enterprise application image includes an operating
    system base image, a Java runtime, an application server and finally the application.
    If only the application layer changes, only this step will have to be re-executed
    and retransmitted - all the other layers are touched only once and then cached.
  prefs: []
  type: TYPE_NORMAL
- en: Thin deployment artifacts leverage the advantages of layers since only a matter
    of kilobytes has to be rebuilt and redistributed, respectively. Therefore, zero-dependency
    applications is the advisable way of using containers.
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the previous chapter, it makes sense to deploy one application per
    application server. Containers execute a single process which in this case is
    the application server containing the application. The application server therefore
    needs to run on a dedicated container that is included in the container as well.
    Both the application server and the application are added at image build time.
    Potential configuration, for example regarding datasources, pooling, or server
    modules, is also made during build time, usually by adding custom configuration
    files. Since the container is owned by the single application these components
    are configured without affecting anything else.
  prefs: []
  type: TYPE_NORMAL
- en: Once a container is started from the image it should already contain everything
    that is required to fulfill its job. The application as well as all required configuration
    must already be present. Therefore, applications are not deployed on a previously
    running container anymore but added during the image build time, to be present
    at container runtime. This is usually achieved by placing the deployment artifact
    into the container's auto-deployment directory. As soon as the configured application
    server starts, the application is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: The container image is built only once and then executed on all the environments.
    Following the idea of reproducible artifacts before, the same artifacts that run
    in production have to be tested upfront. Therefore the same Docker image that
    has been verified will be published to production.
  prefs: []
  type: TYPE_NORMAL
- en: But what if applications are configured differently in various environments?
    What if different external systems or databases need to be communicated with?
    In order to not interfere with several environments, at least the used database
    instances will differ. Applications shipped in containers are started from the
    same image but sometimes still need some variations.
  prefs: []
  type: TYPE_NORMAL
- en: Docker offers the possibility of changing several aspects of running containers.
    This includes networking, adding volumes, that is, injecting files and directories
    that reside on the Docker host, or adding Unix environment variables. The environment
    differences are added by the container orchestration from outside of the container.
    The images are only built once for a specific version, used and potentially modified
    in different environments. This brings the big advantage that these configuration
    differences are not modeled into the application rather than managed from the
    outside. The same is true for networking and connecting applications and external
    systems, which we will see in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Linux containers, by the way, solve the business-politically motivated issue
    of shipping the application together with the implementation in a single package
    for the reason of flexibility. Since containers include the runtime and all dependencies
    required, including the Java runtime, the infrastructure only has to provide a
    Docker runtime. All used technology including the versions are the responsibility
    of the development team.
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet shows the definition of a `Dockerfile` building an
    enterprise application `hello-cloud` onto a **WildFly** base image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `Dockerfile` specifies the `jboss/wildfly` base image in a specific version
    which already contains a Java 8 runtime and the WildFly application server. It
    resides in the application's project directory, pointing to the `hello-cloud.war`
    archive which was previously built by a Maven build. The WAR file is copied to
    WildFly's auto-deployment directory and will be available at that location at
    container runtime. The `jboss/wildfly` base image already specifies a run command,
    how to run the application server, which is inherited by the Dockerfile. Therefore
    it doesn't have to specify a command as well. After a Docker build the resulting
    image will contain everything from the `jboss/wildfly` base image including the
    *hello-cloud* application. This matches the same approach of installing a WildFly
    application server from scratch and adding the WAR file to the auto-deployment
    directory. When distributing the built image, only the added layer including the
    thin WAR file needs to be transmitted.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment model of the Java EE platform fits the container world. Separating
    the application for the enterprise container leverage the use of copy-on-write
    file systems, minimizing the time spent on builds, distribution, or deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go up one abstraction layer from containers. Containers include everything
    required to run specific services as stateless, self-contained artifacts. However,
    the containers need to be orchestrated to run in the correct network, being able
    to communicate with other services and being started with the correct configuration,
    if required. The straightforward approach is to develop home-grown scripts that
    run the required containers. However, in order to realize a more flexible solution
    that also enables production-readiness such as zero-downtime, the use of container
    orchestration frameworks is advisable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Container orchestration frameworks such as **Kubernetes**, **DC/OS** or **Docker
    Compose** are not only responsible to run containers, but to orchestrate, connect
    and configure them appropriately. The same motivations and principles apply that
    are true for container technologies as well: automation, reproducibility and IaC.
    Software engineers define the desired target state as code and let the orchestration
    tool reliably setup the environments as required.'
  prefs: []
  type: TYPE_NORMAL
- en: Before going into a specific orchestration solution, let's have a closer look
    at the rough concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration frameworks enable us to connect multiple containers together.
    This usually involves service lookup using logic names via DNS. If multiple physical
    hosts are used, the framework resolves IP addresses over these nodes. Ideally
    an application running in a container just connects to an external system using
    a logical service name that is resolved by the container orchestration. For example,
    a car manufacturing application using the *vehicle* database connects using the
    `vehicle-db` hostname. This hostname is then resolved via DNS, depending on the
    environment which the application runs in. Connecting via logical names reduces
    the required configuration in the application code, since the configured connection
    is always the same. The orchestration just connects the desired instance.
  prefs: []
  type: TYPE_NORMAL
- en: This is true for all offered systems. Applications, databases, and other servers
    are abstracted to logical service names which are accessed and resolved during
    runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring containers depending on their environment is another aspect that
    orchestration frameworks solve. In general it's advisable to reduce the required
    configuration in applications. However, there are cases where some configuration
    effort is required. It is the framework's responsibility to provide container
    configuration by dynamically injecting files or environment variables depending
    on the circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: The production-readiness features that some of the container orchestration frameworks
    offer represent one of their biggest advantages. Ongoing development of an application
    triggers new project builds and result in new container image versions. The running
    containers need to be replaced by containers that are started from these new versions.
    In order to avoid downtime the container orchestration swaps the running containers
    using a zero-downtime deployment approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, container orchestration makes it possible to increase the workload
    by scaling up the number of container instances. In the past, certain applications
    ran on multiple instances simultaneously. If the number of instances needed to
    be increased, more application servers had to be provisioned. In a container world
    the same goal is achieved by simply starting more of the stateless application
    containers. The developers increase the configured number of container replicas;
    the orchestration framework implements this change by starting more container
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: In order to run containers in production some orchestration aspects have to
    be considered. Experience shows that some companies tend to build their own solutions
    rather than using de facto standard technology. However, container orchestration
    frameworks already solve these issues well and it is highly advisable to at least
    consider them.
  prefs: []
  type: TYPE_NORMAL
- en: Realizing container orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've now seen which challenges container orchestration framework tackle. This
    section will show you the core concepts of **Kubernetes**, a solution originally
    developed by Google to run their workloads. At the time of writing this book Kubernetes
    has a enormous momentum and is also the basis for other orchestration solutions
    such as **OpenShift** by RedHat. I chose this solution because of its popularity
    but also because I believe that it does the job of orchestration very well. However,
    the important point is less about comprehending the chosen technology rather than
    the motivations and concepts behind it.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes runs and manages Linux containers in a cluster of nodes. The Kubernetes
    master node orchestrates the worker nodes which do the actual work, that is, to
    run the containers. The software engineers control the cluster using the API provided
    by the master node, via a web-based GUI or command-line tool.
  prefs: []
  type: TYPE_NORMAL
- en: The running cluster consists of so-called resources of a specific type. The
    core resource types of Kubernetes are **pods**, **deployments**, and **services**.
    A pod is an atomic workload unit, running one or more Linux container. This means
    the application runs in a pod.
  prefs: []
  type: TYPE_NORMAL
- en: The pods can be started and managed as standalone, single resources. However,
    it makes a lot of sense to not directly specify separate pods but to define a
    deployment, which encapsulates and manages running pods. Deployments enable the
    functionality that provide production-readiness such as upscaling and downscaling
    of pods or rolling updates. They are responsible for reliably running our applications
    in the specified versions.
  prefs: []
  type: TYPE_NORMAL
- en: A system defines services in order to connect to running applications from outside
    of the cluster or within other containers. The services provide the logical abstraction
    described in the last section that embraces a set of pods. All pods that run a
    specific application are abstracted by a single service which directs the traffic
    onto active pods. The combination of services routing to active pods and deployments
    managing the rolling update of versions enables zero-downtime deployments. Applications
    are always accessed using services which direct to corresponding pods.
  prefs: []
  type: TYPE_NORMAL
- en: All core resources are unique within a Kubernetes **namespace**. Namespaces
    encapsulate aggregates of resources and can be used to model different environments.
    For example, services that point to external systems outside of the cluster can
    be configured differently in different namespaces. The applications that use the
    external systems always use the same logical service name which are directed to
    different endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports resources definition as IaC using JSON or YAML files. The
    YAML format is a human-readable data serialization format, a superset of JSON.
    It became the de facto standard within Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the definition of a service of the `hello-cloud`
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The example specifies a service which directs traffic on port `8080` toward
    `hello-cloud` pods that are defined by the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the `hello-cloud` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The deployment specifies one pod from the given template with the provided Docker
    image. As soon as the deployment is created Kubernetes tries to satisfy the pod
    specifications by starting a container from the image and testing the container's
    health using the specified probes.
  prefs: []
  type: TYPE_NORMAL
- en: The container image `docker.example.com/hello-cloud:1` includes the enterprise
    application which was built and distributed to a Docker registry earlier.
  prefs: []
  type: TYPE_NORMAL
- en: All these resource definitions are applied to the Kubernetes cluster by either
    using the web-based GUI or the CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating both the deployment and the service, the *hello-cloud* application
    is accessible from within the cluster via the service. To be accessed from the
    outside of the cluster a route needs to be defined, for example using an **ingress**.
    Ingress resources route traffic to services using specific rules. The following
    shows an example ingress resource that makes the `hello-cloud` service available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These resources now specify the whole application, which is deployed onto a
    Kubernetes cluster, accessible from the outside and abstracted in a logical service
    inside of the cluster. If other applications need to communicate with the application,
    they can do so via the Kubernetes-internal, resolvable `hello-cloud` DNS hostname
    and port `8080`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example setup of the *hello-cloud* application
    with a replica of three pods that runs in a Kubernetes cluster of two nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb7873c2-fc69-4174-a05d-7ccd2a9bfaca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Besides service lookup using logical names, some applications still need additional
    configuration. Therefore Kubernetes as well as other orchestration technology
    has the possibility of inserting files and environment variables into the container
    dynamically at runtime. The concept of **config maps**, key-value-based configuration
    is used for this. The contents of config maps can be made available as files,
    dynamically mounted into a container. The following defines an example config
    map, specifying the contents of a properties file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The config map is being used to mount the contents as files into containers.
    The config map''s keys will be used as file names, mounted into a directory, with
    the value representing the file contents. The pod definitions specify the usage
    of config maps mounted as volumes. The following shows the previous deployment
    definition of the *hello-cloud* application, using `hello-cloud-config` in a mounted
    volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The deployment defines a volume which references to the `hello-cloud-config`
    config map. The volume is mounted to the path `/opt/config` resulting in all key-value
    pairs of the config map being inserted as files in this directory. With the config
    map demonstrated previously this would result in a `application.properties` file
    containing the entries for keys `hello.greeting` and `hello.name`. The application
    expects that at runtime the file resides under this location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Separate environments will specify different contents of the config maps, depending
    on the desired configuration values. Configuring applications using dynamic files
    is one approach. It is also possible to inject and override specific environment
    variables. The following code snippet demonstrates this example as well. This
    approach is advisable when the number of configuration values is limited:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Applications need to configure credentials, used for example to authorize against
    external systems or as database accesses. These credentials are ideally configured
    in a different place than uncritical configuration values. Besides config maps,
    Kubernetes therefore also includes the concept of **secrets**. These are similar
    to config maps, also representing key-value pairs, but obfuscated for humans as
    Base64-encoded data. Secrets and their contents are typically not serialized as
    infrastructure as code since the credentials should not have unrestricted access.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common practice is to make credentials accessible in containers using environment
    variables. The following code snippet shows how to include a value configured
    in secret `hello-cloud-secret` into the *hello-cloud* application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The environment variable `TOP_SECRET` is created from referencing the `topsecret`
    key in secret `hello-cloud-secret`. This environment variable is available at
    container runtime and can be used from the running process.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications packaged in containers cannot solely run as stateless applications.
    Databases are a typical example of this. Since containers are discarded after
    their processes have exited, the contents of their file system are also gone.
    Services such as databases need persistent state though. To solve this issue Kubernetes
    includes **persistent volumes**. As the name suggests these volumes are available
    beyond the life cycle of the pods. Persistent volumes dynamically make files and
    directories available which are used within the pod and retain after it has exited.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent volumes are backed by network attached storage or cloud storage offerings,
    depending on the cluster installation. They make it possible to run storage services
    such as databases in container orchestration clusters as well. However, as a general
    advise, persistent state in containers should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: The YAML IaC definitions are kept under version control in the application repository.
    The next chapter covers how to apply the file contents to a Kubernetes cluster
    as part of a Continuous Delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE in orchestrated containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The orchestration framework orchestrates and integrates enterprise applications
    in clustered environments. It takes a lot of work off the used application technology.
    Container orchestration also vastly simplifies how to configure applications and
    how to connect to external services. This section will showcase this.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting external services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Client controls require URLs to connect against in order to integrate external
    services. The URLs traditionally have been configured in files, which potentially
    differed in various environments. In an orchestrated environment the application
    can resolve external services using a logical name, via DNS. The following code
    snippet shows how to connect against the *cloud processor* application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The same holds true for other URLs, for example datasources definitions. The
    application server configuration can simply point to the name of the database
    service and use it to resolve the corresponding instance at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring orchestrated applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Resolving services by logical names already eliminates a lot of configuration
    in the application. Since the same container image is being used in all environments,
    potentially different configuration needs to be inserted from the orchestration
    environment. As shown in the previous example, Kubernetes config maps tackle this
    situation. The *hello-cloud* application expects that at runtime a properties
    file will reside under `/opt/config/application.properties`. The project code
    will therefore access this location. The following demonstrates the integration
    of the properties file using a CDI producer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The CDI producer is defined similarly to the configuration example shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The definition of the `@Config` qualifier is similar to the previous example
    in Chapter 3, *Implementing Modern Java Enterprise Applications*. The application
    loads the contents of the properties file into the properties map and produces
    the configured values using CDI. All managed beans can inject these values which
    emerge from the Kubernetes config map.
  prefs: []
  type: TYPE_NORMAL
- en: In order to realize secret configuration values, Kubernetes includes the concept
    of secrets as previously shown. A common practice is to make the contents of the
    secrets accessible in containers using environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: Java applications use the `System.getenv()` method to access environment variables.
    This functionality is used for both secrets and config map values, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The demonstrated approaches and examples enable an enterprise application to
    be deployed, managed, and configured in a container orchestration cluster. They
    are sufficient for the majority of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 12-factor applications and Java EE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of writing this book, **12-factor** applications has emerged as a way of
    developing **Software as a Service** (**SaaS**) applications. The 12-factor application
    approach define 12 software development principles. The motivations behind these
    principles aim to minimize time and effort, avoid software erosion, and embrace
    Continuous Delivery and cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: In other words the 12-factors aim to to implement enterprise applications in
    a modern way. Some of the principles sound obvious to most engineers, while others
    seem to contradict the common practice of building enterprise applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of the 12-factors includes:'
  prefs: []
  type: TYPE_NORMAL
- en: I. Have one codebase tracked in revision control, many deploys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: II. Explicitly declare and isolate dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: III. Store config in the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IV. Treat backing services as attached resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: V. Strictly separate build and run stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VI. Execute the app as one or more stateless processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VII. Export services via port binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VIII. Scale out via the process model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IX. Maximize robustness with fast startup and graceful shutdown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X. Keep development, staging, and production as similar as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XI. Treat logs as event streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XII. Run admin/management tasks as one-off processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following explains the motivations of each principle and its realization
    with Java EE.
  prefs: []
  type: TYPE_NORMAL
- en: Have one codebase tracked in revision control, many deploys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This principle sounds pretty obvious to software engineers, declaring that a
    software code should be kept under version control, a single repository, even
    for multiple *deploys*. Deploys relate to software instances, running on specific
    environments. Therefore the codebase of a single application is tracked in a single
    repository, not several codebases per application or vice versa, containing all
    specifications for potentially different environments.
  prefs: []
  type: TYPE_NORMAL
- en: This principle leverages developer productivity since all information is found
    under one repository. It is indifferent to the chosen technology and therefore
    supported by Java EE applications, as well.
  prefs: []
  type: TYPE_NORMAL
- en: The repository should contain all source files that are required to build and
    run the enterprise application. Besides Java sources and configuration files,
    this includes infrastructure as code.
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly declare and isolate dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software dependencies and their versions that are required in order to run the
    application must be specified explicitly. This includes not only dependencies
    which the application is programmed against, for example third-party APIs, but
    also implicit dependencies on the Java runtime or operating system, respectively.
    Explicitly specifying the required versions leads to far less compatibility issues
    in production. A composition of software versions is sufficiently tested during
    the development workflow. Dependency versions that differ when rebuilding binaries
    introduce potential issues. It is therefore advisable to explicitly declare all
    software versions to reduce probability of error and enable reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Container technology simplifies this principle by explicitly specifying all
    software installation steps. Versions of used base images should be explicitly
    declared, so that image rebuilds result in the same result. The Docker `latest`
    tag should therefore be avoided in favor of definite versions. All software installations
    specified in Dockerfiles should point to explicit versions as well. Docker rebuilds,
    with or without cache, should produce the same outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Java applications specify their dependencies using build systems. The first
    chapter already covered what is necessary to enable reproducible builds using
    both Maven and Gradle. In Java EE applications these dependencies are ideally
    minimized to the Java EE API.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever possible, it's advisable to specify explicit dependency versions, not
    just *latest* ones. Only software using explicit versions can be tested reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating dependencies is a necessity for distributed development throughout
    the software team. Software artifacts should be accessible via well-defined processes,
    for example artifact repositories. Dependencies, which are added during the software
    build, no matter whether Java runtime installations, Java artifacts, or operating
    system components, need to be distributed from a central place. Repositories such
    as **Maven Central**, **DockerHub** or company-internal repositories enable this
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Store config in the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application configuration, that differ for separate environments, such as databases,
    external systems, or credentials, need to be existent at runtime. This configuration
    should not be reflected in the source code but dynamically modifiable from outside
    of the application. This implies that configuration is retrieved via files, environment
    variables or other external concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Container technology and orchestration frameworks support these approaches as
    previously shown. Configuration for different environments, such as *test*, *staging*,
    and *production* is stored in Kubernetes config maps and dynamically used in pods
    in volumes or environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: The 12-factor principles state that an application "[...] stores config in environment
    variables". Environment variables are a straightforward way of inserting specific
    variations that is supported by all kinds of technology. However, if configuring
    the application involves a lot of individual configuration values, engineers may
    consider to use configuration files contained in container volumes, instead.
  prefs: []
  type: TYPE_NORMAL
- en: Treat backing services as attached resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Databases and external systems that are accessed in the application are called
    *resources*. It should make no difference to the system where an external service
    or database is part of the application. The *resources* should be attached to
    the application in a loosely coupled way. External systems and databases should
    be able to be replaced by new instances without affecting the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications abstract the accessed external system, first of all in the communication
    technology being used. Communication via HTTP or JDBC, for example, abstracts
    the implementations and enables systems to be replaced by others. By doing so,
    applications are only coupled to their contract: the communication protocol and
    defined schemas. JPA, JAX-RS, and JSON-B are examples that support this approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration frameworks take this approach even further and abstract
    services into logic names. As shown previously applications can use service names
    as hostnames, resolved by DNS.
  prefs: []
  type: TYPE_NORMAL
- en: In general, application developers should loosely couple systems together, ideally
    only depending on protocols and schemas. At a code level backing services are
    abstracted into own components, such as individual controls with clean interfaces.
    This minimizes changes if attached resources change.
  prefs: []
  type: TYPE_NORMAL
- en: Strictly separate build and run stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This principle advises to separate the application build, the deployment, and
    the run processes. This is a well-known approach to Java enterprise developers.
    Application binaries are built, deployed, and run in separate steps. Software
    or configuration changes happen in the source code or in the deployment step,
    respectively, and not directly in production. The deployment step brings application
    binaries and potential configuration together. Well-defined change and release
    management processes keep the integrity of the enterprise software.
  prefs: []
  type: TYPE_NORMAL
- en: For the vast majority of software projects, it is common practice to separate
    these steps and orchestrate stages in a Continuous Integration server. This is
    necessary to ensure reliability and reproducibility. [Chapter 6](599c6821-8971-4489-931c-9e11b5e23afd.xhtml),
    *Application Development Workflows* covers this topic in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Execute the app as one or more stateless processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, applications run as stateless processes where every use case is executed
    self-sufficiently, without affecting other running processes. Potential state
    is either stored in an attached resource such as a database or discarded. Session
    state that lives longer than a single request is therefore a violation of this
    principle. The challenge with traditional user session state is that it only resides
    in a local application instance and not accessible from other instances. The need
    for so-called *sticky sessions* on load balancers is an indicator for not having
    a stateless application.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of modern technology supports this approach. Docker containers with their
    copy-on-write file system are an example. Stopped containers will be discarded
    and therefore all of their state is gone as well. Stateless EJBs are based on
    a similar motivation. However, instances of stateless session beans are pooled
    and reused, therefore developers need to ensure that no state retains after the
    business use case invocations.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise applications should be able to be restarted from scratch without
    affecting their behavior. This also implies that applications share no state except
    via well-defined attached resources.
  prefs: []
  type: TYPE_NORMAL
- en: Export services via port binding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web applications are traditionally deployed to a certain software stack. Java
    enterprise applications for examples are deployed to an enterprise or Servlet
    container whereas server-side scripting languages such as PHP run on top of a
    web server. The applications therefore depend on their immediate runtime.
  prefs: []
  type: TYPE_NORMAL
- en: This 12-factor principle advise to develop self-sufficient applications that
    expose their functionality via network ports. Since web-based enterprise applications
    will communicate via the network, binding services to ports is the way of least
    coupling.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE applications that run in a container support this approach, only exporting
    a port which is used to communicate with the application. Containers only depend
    on the Linux kernel, the application runtime is therefore transparent. Container
    orchestration frameworks leverage this idea, connecting services to pods via logical
    names and ports, as shown in a previous example. Java EE supports the use of containers
    and therefore this principle as well.
  prefs: []
  type: TYPE_NORMAL
- en: Scale out via the process model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern applications as well as their environments should enable scalability
    when the workload on them increases. Applications ideally are able to scale out
    horizontally, rather than just vertically. The difference is that scaling horizontally
    aims to adds more individual, self-contained nodes to the software whereas scaling
    vertically increases the resources on single nodes or processes. However, scaling
    vertically is limited, since resources on physical nodes cannot be increased infinitely.
  prefs: []
  type: TYPE_NORMAL
- en: 12-factor applications describe the procedure of adding concurrency to the software
    with adding more self-contained, *shared-nothing* processes. Workloads should
    be distributable within several physical hosts, by increasing the number of processes.
    The processes represent the request or worker threads who handle the system's
    workload.
  prefs: []
  type: TYPE_NORMAL
- en: This approach shows the necessity of implementing stateless application in a
    shared-nothing manner. Containers that run stateless Java enterprise applications
    enable the system to scale out. Kubernetes managed scalability in deployments
    via managing the number of replicas.
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck of enterprise applications, however, is typically not the application
    instances rather than central databases. Chapter 8, *Microservices and System
    Architecture* and Chapter 9, *Monitoring, Performance, and Logging* cover the
    topics of scalability in distributed systems as well as performance in Java EE
    projects in general.
  prefs: []
  type: TYPE_NORMAL
- en: Maximize robustness with fast startup and graceful shutdown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 4](f0a49441-e411-49c4-a4b6-c6193ba36094.xhtml), *Lightweight Java
    EE* already showed the necessity of fast turnarounds. This principle of 12-factor
    applications requires technology that enables velocity and elasticity. In order
    to rapidly scale up, the software should startup in a matter of seconds, making
    it possible to tackle a growing workload.'
  prefs: []
  type: TYPE_NORMAL
- en: Application shutdowns should gracefully finish in-flight requests and properly
    close all open connections and resources. Especially requests and transactions
    that are executed while the shutdown signal occurs should be finished properly
    not to maliciously abort client use cases. In a Unix process approach shutdown
    signals are sent as `SIGTERM` signals. Linux containers are stopped in the same
    way, giving the container process a chance to shutdown properly. When building
    container images, developers should pay attention that the process handles Unix
    signals properly, resulting in a graceful shutdown of the application server when
    it receives a `SIGTERM` signal.
  prefs: []
  type: TYPE_NORMAL
- en: Java EE supports both fast startups and graceful shutdowns. As shown previously,
    modern application servers start up and deploy applications in a matter of seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the application servers manage beans, resources, pooling, and threading,
    they take care of closing the resources properly at JVM shutdown. The developers
    don''t need to take care of this aspect themselves. Beans that manage custom resources
    or handles that need to be closed, use pre-destroy methods to implemented proper
    closing. The following shows a client control using a JAX-RS client handle which
    is closed on server shutdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The platform guarantees that the pre-destroy methods of all managed beans are
    called once the application server shuts down.
  prefs: []
  type: TYPE_NORMAL
- en: Keep development, staging, and production as similar as possible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This 12-factor principle aims to minimize differences between environments.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise applications traditionally have quite some differences between the
    environments of the development process. There are development environments, maybe
    several of them, such as local workstations or dedicated server environments and
    finally there is the production environment. These environments differ in regard
    of time when software artifacts in certain versions and configuration are deployed
    during the development process. The longer the time span of simultaneously having
    different versions in the set of environments the greater this difference becomes.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a difference in teams and people. Traditionally software developers
    maintain their own development environment while an operations team takes care
    of production. This introduces potential gaps in communication, processes, and
    used technology.
  prefs: []
  type: TYPE_NORMAL
- en: The technical difference between environments contains the biggest risk. Development
    or test environments that use different tools, technology, external services and
    configuration than production introduce the risk that these differences will lead
    to errors. Software is tested automatically on these environments before going
    to production. Every difference from production that is not tested can and eventually
    will introduce bugs that could have been prevented. The same is true for exchanging
    tools, backend services, or used stacks for lightweight alternatives in development
    or local environments.
  prefs: []
  type: TYPE_NORMAL
- en: It is therefore advisable to keep the environments as similar as possible. Especially,
    container technologies and orchestration frameworks highly support this approach.
    As we saw previously, differences in configuration, services, and technology are
    minimized or at least explicitly defined via the environment. Ideally, software
    landscapes are identical on development, test environments, staging, and production.
    If that is not possible, service abstractions as well as environment-managed configuration
    support to manage the differences.
  prefs: []
  type: TYPE_NORMAL
- en: The difference in time and people is tackled by usage of Continuous Delivery,
    not just from a technical but also organizational point of view. The overall time
    to production should be as small as possible, enabling fast delivery of features
    and bug fixes. Implementing Continuous Delivery naturally moves teams and responsibilities
    together. The DevOps movement describes how all engineers are responsible for
    the overall software. This leads to a culture where all teams closely work together
    or merge into single teams of software engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Treat logs as event streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enterprise applications traditionally write logs to log files on disk. Some
    engineers argue that this information is one of the most important insights into
    the application. The software project usually includes configuration of the contents
    and format of these logfiles. However, storing log data in log files is first
    of all just an output format, usually having a single log event per line.
  prefs: []
  type: TYPE_NORMAL
- en: This principle of 12-factor applications argues that logging should be treated
    as a stream of log events, that are emitted by the application. Applications should,
    however, not concern themselves with routing and storing the log file into specific
    output formats. Instead they log to the process' standard output. The standard
    out is captured and processed by the runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is uncommon to most enterprise developers with all logging frameworks,
    output formats and tools being around. However, environments where a lot of services
    run in parallel need to capture and process log events externally anyway. Solutions
    such as **Elasticsearch**, **Logstash**, and **Kibana** have proven themselves
    well to process and comprehend complex situations with log events from several
    sources. Storing log events in log files not necessarily supports these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Logging to the application's standard out not only simplifies development, since
    routing and storing is not a responsibility of the application anymore. It also
    reduces the need for external dependencies, such as logging frameworks. Zero-dependency
    applications support this approach. The environment such as a container orchestration
    framework takes care of capturing and routing the event stream. In Chapter 9,
    *Monitoring, Performance, and Logging*, we will cover the topic of logging, its
    necessity and shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Run admin/management tasks as one-off processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This principle describes that administrative or management tasks should be executed
    as separate short-lived processes. The technology ideally supports command execution
    in a shell that operates on the running environment.
  prefs: []
  type: TYPE_NORMAL
- en: Although containers encapsulate Unix processes, they provide additional functionality
    to execute single commands or to open a remote shell into the container. Engineers
    can therefore execute the management and administration scripts provided by the
    Java EE application server. Still, in Java EE applications, the number of required
    administration and management tasks are limited. A Container runs the application
    server process, which auto-deploys the application; no further application life
    cycle management is required.
  prefs: []
  type: TYPE_NORMAL
- en: Administrative tasks are usually required for debugging and troubleshooting
    purposes. Therefore containers and container orchestration frameworks offer possibilities
    to open remote shells into the containers or execute one-time commands. Apart
    from that, the Chapter 9, *Monitoring, Performance, and Logging* will show you
    what is necessary to gather further monitoring information about enterprise applications.
  prefs: []
  type: TYPE_NORMAL
- en: The motivations of the 12-factors are to develop stateless, scalable enterprise
    applications that embrace Continuous Delivery and modern environment platforms,
    optimize time and effort spent in development and try to avoid software erosion.
    12-factor application have a clean contract with their underlying system and ideally
    declarative infrastructure definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud, Cloud native, and their benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of writing this book, there is a lot of interest in cloud platforms. We currently
    see big companies moving their IT infrastructure into cloud offerings. But what
    benefits does *the cloud* have to offer?.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we have to be aware that modern environments do not necessarily
    have to run on top of a cloud platform. All the benefits of container technology
    and container orchestration frameworks can equally be achieved using company-internal
    infrastructure. On premise installations of platforms such as Kubernetes or OpenShift
    at first provide the same advantages for software teams. In fact, one of the biggest
    benefits of container runtimes is to abstract the environment where the containers
    are running. Why are cloud platforms interesting for companies then?
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the beginning of this book, the software world is moving faster
    than ever. The key for companies to keep pace with the trends in their business
    is to embrace agility and velocity in terms of moving fast. The time to market
    of new products and features thereof need to be as short as possible. Moving in
    iterative steps, adapting to customers' needs and continuously improving software
    meets this demand. In order to realize this goal, IT infrastructure, as well as
    all other aspects of software engineering, needs to be fast and flexible. New
    environments should be setup via automated, reliable and reproducible processes.
    The same principles for continuous software delivery apply to server environments.
    Cloud platforms offer this possibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Companies that want to embrace agility and to adapt to their customers'' demands
    need to ask themselves the question: *How long does it take to provision new environments?*
    This is the prerequisite of being able to adapt quickly. Provisioning whole new
    environments should be a matter of minutes, should not require overly complex
    processes and ideally no human intervention. As said before it is definitely possible
    to realize such approaches on premises. Cloud offerings, however, offer these
    benefits out of the box with sufficient, scalable resources. **Infrastructure
    as a Service** (**IaaS**) or **Platform as a Service** (**PaaS**) offerings take
    a lot of work off the hands of companies, enabling them to focus on building their
    products.'
  prefs: []
  type: TYPE_NORMAL
- en: Still, big companies are often skeptical when it comes to cloud services, especially
    in terms of data security. Interestingly, experience of projects shows that when
    comparing infrastructure environments down to earth, cloud platforms run by sophisticated
    enterprises offer more secure environments than most on premises. Cloud platform
    providers put a lot of time and effort into building proper solutions. Especially
    combining cloud platform offerings with orchestration solutions, such as Docker
    Compose, Kubernetes, or OpenShift hold a lot of potential.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, one of the main arguments of companies to move their IT into
    the cloud is because of economic reasons. From experience, a lot of companies
    want to save costs by using cloud platforms. In fact, when taking the whole process
    of migrating and transforming environments, teams, technology, and most of all
    know-how, into account, on premises solutions are usually still cheaper. However,
    the main advantage of cloud offerings is flexibility and the ability to move fast.
    If an IT company maintains a well-orchestrated landscape, including automation,
    reliable and reproducible processes, it is advisable to keep, and continuously
    improve, this approach. That said, the question about modern environments is less
    about whether to use cloud platforms than about processes, team mindsets, and
    reasonable technology.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the interest in cloud technology there is a lot of interest in the term
    **cloud native** which describes applications that, besides following the 12-factors,
    have a strong relationship to cloud platforms. Cloud native and 12-factor applications
    are not synonymous; rather than cloud native includes the 12-factors, among other
    things.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native applications are designed to run on cloud PaaS offerings with all
    their benefits and challenges, embrace container technology and elastic scalability.
    They are built with the claim to provide modern, scalable, stateless and resilient
    applications, manageable within modern orchestration environments. Unlike the
    term *native* suggests, applications that follow this approach do not necessarily
    have to be built as *green-field* projects that support cloud technology from
    day one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important aspects for cloud native applications beyond the 12-factors are monitoring
    and application health concerns, which can be summarized as telemetry. Telemetry
    for enterprise applications include responsiveness, monitoring, domain-specific
    insights, health checks, and debugging. As we have seen previously, container
    orchestration supports us at least with the last two concerns: health checks and
    debugging. Running applications are probed whether they are still alive and healthy.
    Debugging and troubleshooting is possible by evaluating the log event streams,
    connecting into the running containers or executing processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Application monitoring need to be exposed by the running container. This requires
    a bit more effort from software developers. Domain-specific metrics need to be
    defined by the business experts first. It depends which metrics are interesting
    to business departments and will be exposed by the application. Technical metrics
    are gathered from the running application as well. Chapter 9, *Monitoring, Performance,
    and Logging* covers the topic of monitoring in regard to modern environments.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect that the 12-factors don't include are APIs and security thereof.
    SaaS applications communicate via exposed APIs that have to be made known to other
    teams of developers. The nature and structure of web services needs to be documented
    and agreed upon during development. This is especially the case when HTTP APIs
    don't implement Hypermedia. The applications need to know the nature and structure
    of exchanged information - ideally as early as possible in the development process.
    This also covers authentication and authorization. Application developers should
    be aware of security mechanisms they need to address before communicating to other
    services. In general it is not advisable to only think of security aspects after
    development. [Chapter 10](2c990001-2bf1-4ede-b2cd-f4939754b6df.xhtml), *Security*
    covers this topic concerning cloud environments and integration into Java EE applications.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build an umbrella for all technologies that embrace cloud platforms,
    the **Cloud Native Computing Foundation** was formed by several software vendors.
    It is part of the Linux Foundation, representing an foundation for cloud native
    Open Source Software. It contains technology that orchestrates, manages, monitors,
    traces or in some other way supports containerized **microservices** running in
    modern environments. As of writing this book, examples for technology projects
    being part of the Cloud Native Computing Foundation are **Kubernetes**, **Prometheus**,
    **OpenTracing**, or **containerd**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operational tasks need to be automated. Setting up application environments
    should always produce the same outcome, including installations, network, and
    configuration. Container technologies as well as infrastructure as code support
    this by defining, automating and distributing software installations and configuration.
    They fulfill the necessity of rebuilding software and systems in a fast and reproducible
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code definitions specify the required infrastructure together
    with all dependencies as part of the application code, kept under version control.
    This approach supports the ideas behind the DevOps movement. The responsibilities
    of not only defining the application but also its runtime with all requirements
    move different teams together. It should be a responsibility of all engineers
    to deliver quality software that serves a business purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Container technologies such as Docker provides functionality to build, manage,
    and ship containers in a uniform way. Docker's copy-on-write layered file system
    enables us to minimize build and publishing times by only re-executing steps that
    have changed. Java EE zero-dependency applications encourage the use of container
    technology by separating the application logic from its implementation. The changing
    layer therefore only contains business code.
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration frameworks such as Kubernetes manage containers in their
    life cycle, network, and external configuration. They are responsible to lookup
    services, provide production readiness such as zero-downtime deployments and scale
    up and down application instances. Container orchestration supports infrastructure
    as code definitions, that contain the configuration of the whole runtime environment
    required by the application.
  prefs: []
  type: TYPE_NORMAL
- en: The 12-factor and cloud native approaches aim to develop modern enterprise applications
    with minimal time and effort, avoiding software erosion, and supporting Continuous
    Delivery and cloud platforms. The 12-factor principles target software dependencies,
    configuration, dependent services, runtime environments, logging and administration
    processes. Similarly, cloud native applications aim to build enterprise software
    that works well on cloud platforms, supporting monitoring, resilience, application
    health, and security. Since these approaches are not bound to a specific technology,
    they are realizable using Java EE. We have seen the motivations why to follow
    these principles.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapter will show you how to build productive application development
    workflows, that are based on container technologies.
  prefs: []
  type: TYPE_NORMAL
