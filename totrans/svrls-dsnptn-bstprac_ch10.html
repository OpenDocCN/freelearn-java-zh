<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Error Handling and Best Practices</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Error Handling and Best Practices</h1>
                </header>
            
            <article>
                
<p>Serverless architectures are different enough that techniques and best practices need to be thought through and evaluated to be successful. Many traditional methods for debugging, application development, monitoring, and so on are still applicable in a server-based architecture. However, many tried-and-tested techniques that you may rely on when working with virtual machines or real hardware will not necessarily work with serverless systems. When building on top of a FaaS platform, then, you need to keep these differences in mind and have a plan for monitoring, debugging, testing, and developing your serverless application.</p>
<p>In this chapter, we'll review common best practices that will help you to focus on building your application rather than getting stuck in the details of organization or deployment. We'll also cover the tools and methods available for keeping your serverless application secure, easy to develop locally, and observable. We'll also<span>&#160;discuss the changes you will need to make to track errors and monitor serverless applications reliably.</span></p>
<p>It's worth noting here that one could dedicate an entire book to best practices for serverless applications. This chapter is not exhaustive by any means, but it will cover many topics which will definitely help to improve your serverless development experience and the overall quality of your application.</p>
<p><span>By the end o</span><span>f this chapter, you can expect to understand the following topics:</span></p>
<ul>
<li>How to set up your application to track unexpected errors with Sentry and Rollbar</li>
<li><span>Working with cold starts</span></li>
<li><span>Monitoring and alerting around errors, exceptions, or performance degradation</span></li>
<li><span><span>Local development and testing</span></span></li>
<li><span>How to manage configuration via environment variables across different stacks (development versus production)</span></li>
<li>How to encrypt sensitive environment variables to keep your application secure</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Error tracking</h1>
                </header>
            
            <article>
                
<p>Practically speaking, all software systems crash at some point. One of the reasons I love working with serverless systems so much is that they, by their very nature, keep an application relatively small and more akin to a microservice, rather than a broad monolithic application. This fact by itself can drastically reduce&#160;the number of ways an application can fail. However, at some point, it will fail, and an exception you didn't expect will occur. What, then, is the best way to handle unexpected exceptions in a serverless system?</p>
<p>The good news here is that we have multiple options, and that some systems you may already be familiar with can work in the same way as they would in a non-serverless system. In the following sections, we'll walk through the steps for integrating two popular error tracking services, Sentry and Rollbar. Both services offer similar functionality and are equally easy to set up. In the following examples, we'll be using Python, but both Sentry and Rollbar support a myriad of languages including Node.js, Java, Golang, and C#.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Integrating Sentry for error tracking</h1>
                </header>
            
            <article>
                
<p>I have used Sentry for many years and highly recommend it. It has a vast feature set and many prominent companies use the service. Sentry's free plan gives you 10,000 events per month, a single login, and a seven-day history of quickly. Whether it's for a hobby project or even a medium-scale production system, this free plan works out quite well.</p>
<p>To integrate Sentry with your serverless function, you'll, of course, need a Sentry account. Following is a code block for an elementary AWS Lambda function. All it will do is calculate the quotient of two numbers. Our goal is to ensure that any unhandled exceptions are captured and reported somewhere so that we have visibility into what our application is doing and have as much information as possible with which to debug it:</p>
<pre style="padding-left: 30px">def divide(event, context):<br/>    params = event.get('queryStringParameters') or {}<br/>    numerator = int(params.get('numerator', 10))<br/>    denominator = int(params.get('denominator', 2)) <br/>    body = { <br/>        "message": "Results of %s / %s = %s" % ( <br/>            numerator, <br/>            denominator,<br/>            numerator // denominator,<br/>        ) <br/>    } <br/><br/>    response = { <br/>        "statusCode": 200,<br/>        "body": json.dumps(body)<br/>    } <br/><br/>    return response</pre>
<p>We've set this up with API Gateway, so we can execute it using <kbd>curl</kbd> and get results for two numbers, as shown in the following snippet:</p>
<pre><strong>$ curl "https://5gj9zthyv1.execute-api.us-west-2.amazonaws.com/dev?numerator=12&amp;denominator=3"</strong><br/><strong>{"message": "Results of 12 / 3 = 4"}</strong></pre>
<p>Let's see what happens when we divide this by <kbd>0</kbd>, which we know is undefined in mathematics:</p>
<pre><strong>$ curl "https://5gj9zthyv1.execute-api.us-west-2.amazonaws.com/dev?numerator=10&amp;denominator=0"</strong><br/><strong>{"message": "Internal server error"}</strong></pre>
<p>As an application developer, I have no way of knowing that this error has occurred as there is no monitoring in place. The only way I can know that an error has happened is if I log into the AWS console and look at the execution metrics for my Lambda function, or if&#160;I happen to be reading through the CloudWatch Logs. Of course, you can't be manually watching for errors day and night. The following screenshot shows <span class="packt_screen">Invocation errors</span> from the AWS Lambda monitoring page for the&#160;<kbd>divide</kbd> function:</p>
<div class="CDPAlignCenter"><img src="images/d9bfc0e4-0e6f-4ec6-9acb-42b4e03bca55.png" style="width:25.67em;height:26.08em;"/></div>
<div class="mce-root CDPAlignCenter packt_figref">AWS CloudWatch chart from the Lambda screen, showing a count of the number of errors</div>
<p>CloudWatch will capture&#160;<kbd>stdout</kbd> and <kbd>stderr</kbd> for Lambda functions. Because unhandled exceptions are written to <kbd>stderr</kbd>, we can see the details when looking at the CloudWatch logs, as shown in the following screenshot:&#160;</p>
<div class="CDPAlignCenter"><img src="images/893e8ebd-3c60-4132-adbc-7478e7454366.png" style="width:50.08em;height:20.83em;"/></div>
<div class="mce-root CDPAlignCenter packt_figref"><span>AWS CloudWatch logs, showing an unhandled exception due to division by zero</span></div>
<p>Integrating Sentry will capture unexpected errors, store them, and notify us via various delivery mechanisms. Getting Sentry reporting for our Lambda functions is quite easy. For Python, you can use the <kbd>raven-python-lambda</kbd> (<a href="https://github.com/Netflix-Skunkworks/raven-python-lambda">https://github.com/Netflix-Skunkworks/raven-python-lambda</a>) library and add a decorator around handler functions, as shown in the following snippet:</p>
<pre style="padding-left: 30px">from raven_python_lambda import RavenLambdaWrapper<br/><br/>@RavenLambdaWrapper()<br/>def divide(event, context):<br/>    # Code</pre>
<p>The only other bit of configuration we need to take care of here is setting the Sentry DSN, which tells the library where to send its payload when an error occurs. Doing this is it's just a matter of passing the values from the host system's environment variables into the Lambda function's environment variables. Using the Serverless Framework, this is quite easy, as you can see:</p>
<pre style="padding-left: 30px">provider:<br/>  name: aws<br/>  runtime: python3.6<br/>  region: ${env:AWS_REGION}<br/>  state: ${env:$ENV}<br/>  environment:<br/>    SENTRY_ENVIRONMENT: ${env:ENV}<br/>    SENTRY_DSN: ${env:SENTRY_DSN}</pre>
<p>Now Sentry will capture any unhandled exceptions and, at a minimum, email them to us. In the following screenshot, you can see a list of various exceptions. What is neat is that some of these errors were not even deliberate. As you can see in the last row, I misspelled a variable name which caused my division calculation to throw an error:</p>
<div class="CDPAlignCenter"><img src="images/3eb588c9-6f0c-4ab4-9a3c-9fd7972095f5.png" style="width:42.33em;height:17.00em;"/></div>
<p>&#160;</p>
<p class="mce-root">Clicking on any of these errors gives us more context into the state of our application when the exception was triggered, as shown in the following screenshot:</p>
<div class="CDPAlignCenter"><img src="images/e17540fc-b1e4-4260-8077-10f423973e56.png" style="width:47.17em;height:28.75em;"/></div>
<p>Another lovely feature of the <kbd>raven-python-lambda</kbd> library is that it will raise warnings when your function is getting too close to its timeout value or its maximum allocated memory. To test this, we need to set the timeout of the <kbd>divide</kbd> function to 4 seconds and put&#160;<kbd>time.sleep(3)</kbd> in the middle of the application code. After executing the divide Lambda function, you should get a result as expected. You will also receive an email about the slow execution speed and see the same warning on the Sentry website, as shown in the following screenshot:</p>
<div class="CDPAlignCenter"><img src="images/ca4319f3-7be6-4f5f-9b9f-f5cd85f49138.png" style="width:40.92em;height:30.75em;"/></div>
<p>&#160;</p>
<p>There is much more information included with each exception to help you while debugging on the Sentry website. There are also many more features in Sentry that we don't have room for here; however, a few features worth noting are as follows:</p>
<ul>
<li>Chat integration (Slack, IRC, etc.)</li>
<li>Tracking deployments</li>
<li>Issue rollup and status tracking</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Integrating Rollbar</h1>
                </header>
            
            <article>
                
<p>Rollbar plays the same role as Sentry. Integration is equally as simple. We will still use a decorator for our handler functions, but we'll need a different library with Rollbar. Rollbar provides an official library for Python (<a href="https://github.com/rollbar/pyrollbar">https://github.com/rollbar/pyrollbar</a>) and many other languages.</p>
<p>The setup changes slightly with <kbd>pyrollbar</kbd>, but it's nothing too complicated. The following code block shows how to set up an AWS Lambda function for error tracking with Rollbar.</p>
<pre style="padding-left: 30px">import rollbar<br/><br/>token = os.getenv('ROLLBAR_KEY', 'missing_api_key')<br/>rollbar.init(token, os.getenv('ENV', 'dev'))<br/><br/>@rollbar.lambda_function<br/>def divide(event, context):<br/>    # code</pre>
<p>When we hit an exception, information is delivered to Rollbar. Again, an email notification about the <kbd>ZeroDivisionError</kbd>&#160;should be received. Just like Sentry, there are plenty of integrations from Rollbar.&#160;</p>
<p><span>The following screenshot shows error details displayed on the Rollbar website:</span></p>
<div class="CDPAlignCenter"><img src="images/d90ac957-6dfc-43b4-8f62-aeffa0946987.png" style="width:39.58em;height:24.25em;"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logging</h1>
                </header>
            
            <article>
                
<p>Tracking exceptions and problems within your application is&#160;critical; however, there will inevitably be cases where you wish you had more insight into the state of your application when a problem occurs. For this task, you will need to set yourself up with a good logging strategy. Log messages are a tool we have used for a very long time - and still use often. Very often, log messages are sent to files on disk and then shipped off to a log aggregator. Since we don't have access to these same types of logging system in a serverless architecture, we'll need to come up with something new.</p>
<p><span>AWS Lambda functions and other FaaS providers offer some mechanisms for keeping track of</span> <kbd>stdout</kbd> <span>and</span> <kbd>stderr</kbd> <span>streams. In the case of Lambda, any <kbd>print</kbd> statements or other error messages will end up in CloudWatch</span> Logs<span>. This delivery to CloudWatch happens automatically, and is especially useful as you'll always know where to go to check for errors or debugging statements. While this is a helpful feature, there are a few improvements we can make to your logging statements so that they're easier to search through, find, and categorize.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Structuring log messages</h1>
                </header>
            
            <article>
                
<p>Log messages are often used as plain strings via <kbd>console.log</kbd> or <kbd>print</kbd> statements. These quick and dirty additions to code can be helpful during development but won't suffice in a production-level system. Rather than logging flat strings, log statements need to be structured so that you can easily find the bits of information you're looking for. JSON is an excellent choice for a format since it's widely used among different log aggregator services and easy to implement in practically any language.</p>
<p>Let's take the simple case of our previous divide function. At some point, we may want to understand how people are using our service: specifically, what numerator and denominators they're sending us. To do this, we will need to structure some log messages so that we can quickly search through them, pull out the pertinent information, and finally analyze it. The following code block shows some Python code to bootstrap the <kbd>structlog</kbd> library. This library will take care of logging structured messages, rather than the flat messages we usually get from the standard library's <kbd>logging</kbd> module:</p>
<pre>import structlog<br/>structlog.configure(<br/>        processors=[structlog.processors.JSONRenderer()]<br/>)<br/>log = structlog.get_logger()</pre>
<p>In our divide function, we can now log any data we find interesting as key-value pairs, shown as follows:</p>
<pre style="padding-left: 30px">def divide(event, context):<br/>    params = event.get('queryStringParameters') or {}<br/>    log.msg('start', **params)<br/><br/>    numerator = int(params.get('numerator', 10))<br/>    denominator = int(params.get('denominator', 2))<br/><br/>    # do division<br/><br/>    log.msg('finish',<br/>            numerator=numerator,<br/>            denominator=denominator,<br/>            quotient=numerator // denominator)</pre>
<p>Log messages will now arrive in CloudWatch as JSON-formatted objects rather than Python strings, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/423f0ced-92b0-4a3c-aaec-b420d60f7aef.png" style="width:40.67em;height:14.67em;"/></div>
<p>This has merely set us up for success; next, we'll work on getting these structured log messages to a log aggregator for better discoverability, analysis, and integration with other services.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Digesting structured logs</h1>
                </header>
            
            <article>
                
<p>Today, there is a myriad of dedicated logging services, such as:</p>
<ul>
<li>Loggly</li>
<li>Sumo Logic</li>
<li>Splunk</li>
<li>Papertrail</li>
<li>Hosted ELK</li>
</ul>
<p>The list could go on and on. Many of these hosted services may be more accessible, and possibly more powerful, when you send them JSON messages. I have used Loggly in different applications and know that this is the case.</p>
<p>There are many ways to ship logs to Loggly, and likely for other services. However, shipping logs can change when the destination is somewhere other than your FaaS provider. CloudWatch logging is built-in to AWS Lamba and offers free performance, so how can we get these same logs out to an external service?</p>
<p>With AWS CloudWatch, it's possible to trigger another Lambda function when new log messages arrive. That may seem a bit odd, but it's a great trick to keep your application lean and decoupled from any logging service while also solving the problem of getting your log message to a more powerful service. We won't go into all the details on how to set this up here, but there is detailed documentation available on the Loggly site:&#160;<a href="https://www.loggly.com/docs/cloudwatch-logs/">https://www.loggly.com/docs/cloudwatch-logs/</a>.</p>
<p>This pattern is not unique to Loggly in any way. If you are using another logging service and wish to follow the same pattern, it's merely a matter of implementing a Lambda function, which is then triggered by CloudWatch events and sent away to your logging provider of choice.</p>
<p>Once you have JSON messages arriving at your logging provider, you have many more options in terms of data analysis and discovery. Being able to quickly and easily find information when a problem occurs is critical for any production-level system, serverless or not. Regardless of which FaaS or logging service you're using, just make sure that you can easily find the data you need when it's time.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cold starts</h1>
                </header>
            
            <article>
                
<p>One commonality between most, if not all,&#160;FaaS providers is the issue of cold starts. Cold starts are defined as the behavior where an invocation of a cloud function which has not been executed in a while takes a considerable amount of time to initialize before fulfilling the request. If you have used Docker, for example, you'll know that creating a new container from an existing image takes slightly longer than starting up a container you have previously run. This Docker container behavior is analogous to the way cloud functions, whether it be AWS Lambda, Google Cloud Functions, or Azure Cloud Functions, behave. If you do any searching around the internet for serverless cold starts, you'll find several blog posts and documentation on the matter.</p>
<p><span>There isn't a silver bullet for bypassing the cold start issue. However, there are several things to be aware of so that you can minimize their impact on your application.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Keeping cloud functions warm</h1>
                </header>
            
            <article>
                
<p>There are several tips and tricks you can employ to work around cold starts. The most common recommendation is to implement another function on a timer, say every 5 minutes, which then invokes your target function. With this pattern, the target function is always kept warm, which means it can fulfill a legitimate request more quickly. This can be a useful trick; however, it does not always solve the problem.</p>
<p>Remember, cloud functions will scale automatically. Think back to some of the patterns in this book, specifically the Fanout and MapReduce patterns. In those examples, multiple instances of our functions were being executed concurrently. In the case of our Fanout pattern for image resizing, a single invocation of our initial Lambda function would result in three concurrent image resizing functions. If we had a <kbd>ping</kbd> function to keep the resizing function active, we would have a single function warm and ready to process that resizing task. However, when three simultaneous invocations occur, a single <kbd>ping</kbd> function will not help. In this scenario, a single resize function would be warm, but the other two would pay the cold start cost. If we changed our application to resize an image into five different sizes, we would then have four different functions that would start from a <em>cold</em> state.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AWS Lambda functions and VPCs</h1>
                </header>
            
            <article>
                
<p>If you are using AWS, keep in mind that cold starts are much worse when running Lambda functions inside a VPC. The reason for this is that Lambda functions in a VPC are allocated an <strong>Elastic Network Interface</strong> (<strong>ENI</strong>) to access VPC resources such as databases. If you ran the example code <a href="svrls-dsnptn-bstprac_ch02.html">Chapter 2,&#160;</a><em>A Three-Tier Web Application Using REST</em> and <a href="svrls-dsnptn-bstprac_ch03.html">Chapter 3,&#160;</a><em>A Three-Tier Web Application Pattern with GraphQL</em>, you may have noticed that the first API call took several seconds. This&#160;initial lag is mainly because the Lambda functions needed access to the RDS instance inside of a VPC, which means the Lambda functions themselves are required to be inside the same VPC.</p>
<p>If at all possible, avoid putting Lambda functions inside a VPC. If your functions do not rely on any external resources, or non-VPC resources such as DynamoDB, do not put them inside of a VPC. However, if you do need access to VPC resources, there aren't many options available. If you are running an API that is talking to a VPC resource such as an RDS instance, you could run a <kbd>pinger</kbd>&#160;function, but we advise raising the concurrency from 1 to something like 10. In this case, you would then have at least 10 functions always warmed up and ready to serve traffic.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Start-up times for different languages</h1>
                </header>
            
            <article>
                
<p>Each supported language comes with its unique behavior in cloud functions. We have not thorough profiled all the different FaaS platforms and languages in this book, but we do know that it has been reported that Node.js and Python have lower cold start times compared with Java and C# on AWS. However, there are also claims that C# functions based on .NET Core 2.0 are significantly faster. AWS recently rolled out support for Golang; however, we are currently unclear on its relative cold start performance.</p>
<p>I may be a bit biased, but I do believe using a language with a lower cold start time is a better choice, at least on AWS, as you can accomplish pretty much anything you need between Node.js and Python. Reading some of the tests people have made, the difference between Java or C# and other languages is two to three orders of magnitude; in other words, cold start times range from 1,000-4,000 ms with Java and C#, whereas Node.js and Python score in the range of 1-10 ms.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Allocating more memory</h1>
                </header>
            
            <article>
                
<p>At least in the case of AWS Lambda, allocating more memory to your functions can result in a faster start-up time. Just as running larger EC2 instances affords you more CPU power, so too does allocating more memory to your Lambda functions. Allocating more memory to your functions may improve performance, but note that this will affect your billing as Lambda functions are billed by the combination of execution duration and allocated memory.</p>
<div class="packt_tip">You can read about CPU allocation relating to AWS Lambda in the following document:&#160;<a href="https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html">https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html</a>.</div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Local development and testing</h1>
                </header>
            
            <article>
                
<p class="mce-root">One challenge we face as serverless engineers is that of convenience. To be more specific, it's a swift process writing code, deploying it, and beginning testing. Testing a live system will often result in some code or configuration issue, but it is quickly fixed and redeployed. The problem we face, therefore, is that it's so easy to fix issues and then redeploy that we can get into the habit of skipping testing or not running our stack locally.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Local development</h1>
                </header>
            
            <article>
                
<p><span>One question I answer with some regularity is, <em>How do I run this locally?</em> When writing a server-based application, one of the first tasks to undertake is getting the system set up so that it can be run during development. When building a serverless-based application, however, there really is no server to run. So, how do we develop our application?</span></p>
<p>The truthful answer is that this is a challenge, and one that has not been solved perfectly yet; to be fair, this issue is difficult with any microservice-based system. So, how can we run a system and ensure it's fully functional when it's composed of multiple disparate pieces? My belief here is that we need to lean on the principles and strengths of a component-based architecture and use common tools that make local development and testing easier. As you write your serverless application, it's best to focus on the service itself and ensure via unit testing that it works as expected. Don't expect to run a full serverless map-reduce system on your local machine.</p>
<p>In the case of a serverless web API, I rely on unit tests rather than a local server during development. After all, we've long been taught that unit tests are a better approach to development than the manual testing of an API or UI. Regardless of where you stand on that topic, local development of serverless systems can move along quite quickly when writing unit tests, and testing in these systems is relatively simple, as we'll cover in the upcoming section.</p>
<div class="packt_tip">You can read through the repository of community plugins in Serverless Framework here: <a href="https://github.com/serverless/plugins">https://github.com/serverless/plugins</a>.</div>
<p>There <em>are</em> options for running a serverless application locally, as mentioned previously; however, I have not used these tools myself and cannot speak for the ease or difficulty of using them. For Serverless Framework, however, there are some plugins with the word offline in the name, where the commonality is that they all aim to help you run your application locally. Outside these plugins, DynamoDB has, for a long time, offered an offline version that can be run on your local system.</p>
<p>Serverless systems are still relatively new, and the landscape is maturing and changing quickly. It's almost certain that vendors recognize that there are areas for improvements in the software development lifecycle of serverless applications; I would not be surprised if local development and testing became more comfortable in the coming years.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Learning about testing locally</h1>
                </header>
            
            <article>
                
<p>Concerning local development, I believe the best strategy is to set up robust and thorough unit tests. Unit-testing serverless applications is no different from testing traditional server-based applications. As long as you follow the mantra of keeping your serverless code separate from your business logic, it's quite simple to get to a very high test coverage.</p>
<p>But what do we do when our application relies on backing services such as databases, caches, and the like? Additionally, what do we do when our serverless application calls other services that don't exist locally, such as AWS, SNS, and so on?</p>
<p>My approach to common systems such as Postgres or Redis is to use Docker. If you look back at the unit tests from <a href="svrls-dsnptn-bstprac_ch02.html">Chapter 2</a>, <a href="svrls-dsnptn-bstprac_ch02.html"></a><em>A Three-Tier Web Application Using REST</em> and <a href="svrls-dsnptn-bstprac_ch03.html">Chapter 3</a>,&#160; <em>A Three-Tier Web Application Pattern with GraphQL</em>, you will see that they rely on a PostgreSQL database. When developing that application, we ran a Docker image that the unit tests used.</p>
<p>Another method for dealing with services you cannot easily run locally or code which <em>is</em> focused on serverless-specific logic is the judicious use of mocks. Take, for example, our Messaging Pattern, where our handler function sends messages to SQS. To test this code, we would not want to invoke SQS as that would make our tests slower and they would likely end up brittle. What's better, in this case, is to instead mocking out the API call to SQS and simply test whether the request to the SQS publish function was made.</p>
<p>Likewise, when we want to test some code which is specific to our serverless implementation, mock can come in handy. This is best explained with an example; the following code block shows a single function from our REST API, at the top-level <kbd>handler.py</kbd> function:</p>
<pre style="padding-left: 30px">def session_detail(event, context):<br/>    http_method = event['httpMethod']<br/><br/>    status_code = 200<br/>    response = {}<br/><br/>    try:<br/>        response = handle_session_detail(http_method, event)<br/>    except Http404 as e:<br/>        status_code = 404<br/>        response = {'errors': [str(e)]}<br/>    except Exception as e:<br/>        status_code = 500<br/>        response = {'errors': ['Unexpected server error']}<br/><br/>    response = {<br/>        'statusCode': status_code,<br/>        'body': json.dumps(response),<br/>        'headers': CORS_HEADERS,<br/>    }<br/><br/>    return response </pre>
<p>As you can see, this code has a bit more going on than delegation. Here, the <kbd>session_detail</kbd> function is catching various errors and setting the HTTP response code and message based on those exceptions, if any are raised. Testing the <kbd>handle_session_detail</kbd> function is simple, as it is working solely on our application and doesn't contain any reliance on or knowledge of AWS Lambda. However, we do need to test the handling of errors in <kbd>session_detail</kbd>.</p>
<p>To do this, we use a mock object to patch the <kbd>handle_session_detail</kbd> method. The aim of the following code block is to trigger an <kbd>Http404</kbd> exception so that we can verify that the static code and error message are correct. The following code block&#160; shows this unit test, where <kbd>mocker</kbd>&#160;is a fixture which comes from the <kbd>pytest-mock</kbd> library:</p>
<pre style="padding-left: 30px">def test_session_detail_hanlder_404(mocker):<br/>    mock_handler = mocker.patch('handler.handle_session_detail')<br/>    mock_handler.side_effect = Http404('Test 404 error')<br/><br/>    event = {'httpMethod': 'POST'}<br/>    response = handler.session_detail(event, None)<br/>    assert_404(response)<br/>    body = get_body_from_response(response)<br/>    assert body == {'errors': ['Test 404 error']}<br/>    mock_handler.assert_called_once_with('POST', event)</pre>
<p>Testing is as much an art as it is a science, and so I cannot overstate the importance of testing i<span>n serverless applications</span>. As usual, the better your test are, the more confident you'll be when it's time to refactor your code or deploy changes.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Managing different environments</h1>
                </header>
            
            <article>
                
<p>With most production-level applications, teams maintain multiple environments for different purposes. A <kbd>QA</kbd> environment may exist for the QA team to run automated tests, a <kbd>staging</kbd> environment may exist for the DevOps team to tests their infrastructure changes, and the <kbd>production</kbd> environment exists to serve live traffic. Very often, building and maintaining these different environments can be a full-time job.</p>
<p>With serverless systems, I've found that maintaining different environments can be much more straightforward. Some of this may come from the fact that, by their nature, serverless applications are inherently smaller. Writing a monolithic application in a serverless architecture isn't wise - or even natural. How best, then, can we manage and maintain different environments for serverless systems?</p>
<p>For this, turning to <span>tenant</span> III of the 12-Factor App Methodology helps. This tenant can be found at <a href="https://12factor.net/config">https://12factor.net/config</a> and states:</p>
<div class="packt_quote">Store config in the environment<br/>
An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc.).</div>
<p><span>Throughout this book, I used the Serverless Framework to manage and deploy my systems. This framework has built-in support for environment variables, which we can use to our advantage to efficiently manage different systems without making any code changes. The following code block shows a small snippet from the&#160;<kbd>serverless.yml</kbd> file from <a href="svrls-dsnptn-bstprac_ch02.html">Chapter 2</a>, <a href="svrls-dsnptn-bstprac_ch02.html"></a><em>A Three-Tier Web Application Using REST</em> for the coffee cupping REST API:</span></p>
<pre style="padding-left: 30px">provider:<br/>  name: aws <br/>  runtime: python3.6<br/>  memorySize: 128 <br/>  region: ${env:AWS_REGION}<br/>  environment:<br/>    CUPPING_DB_USERNAME: ${env:CUPPING_DB_USERNAME}<br/>    CUPPING_DB_PASSWORD: ${env:CUPPING_DB_PASSWORD}<br/>    CUPPING_DB_NAME: ${env:CUPPING_DB_NAME}</pre>
<p>Here, any reference to <kbd>${env:NAME}</kbd> will pull the actual value for <kbd>NAME</kbd> from the environment. Additionally, Serverless Framework helps us to keep stacks separate by using the <kbd>stage</kbd> variable to name resources. Whenever deploying code, the deployment step includes the stage variable, which we also pull out of the environment:</p>
<pre style="padding-left: 30px"><span>sls deploy -s </span><span>$ENV</span></pre>
<p>These two techniques combined mean that deploying a <kbd>dev</kbd> stack or <kbd>qa</kbd> stack is just a matter of loading different environment variables. You can load environment variables from a file with tools such as <kbd>dotenv</kbd>, your shell script, or some other tool. My technique uses Docker and a Makefile to load up different variables based on the <kbd>ENV</kbd> I wish to work with. The result is the same, regardless of how you solve the problem of variable management. If you can quickly change variables, you can easily switch between managing completely different stacks. Remember, if you're using the Serverless Framework you will also need to handle the <kbd>ENV</kbd> setting. This variable is a single setting which will control the stack that is updated during any deployment.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Securing sensitive configuration</h1>
                </header>
            
            <article>
                
<p><span>Throughout this book, and in the previous section about managing environments, we've relied heavily on environment variables. One very nice feature of pulling a configuration from the environment is that sensitive information never needs to be checked into the source control. All of our application code and any framework code (such as the Serverless Framework) can look up variable values from the environment when needed.</span></p>
<p>Configuration via environment variables is all well and good, but our usage of these variables is not perfect. The problem with our usage of environment variables and Lambda is that the data pulled from the deployment environment is uploaded and stored in AWS Lambda functions as plain text. For example, take a look at <kbd>serverless.yml</kbd> from the previous section about error handling using either Sentry or Rollbar:</p>
<pre style="padding-left: 30px">provider:<br/>  name: aws <br/>  runtime: python3.6<br/>  region: ${env:AWS_REGION}<br/>  state: ${env:$ENV}<br/>  environment:<br/>    SENTRY_ENVIRONMENT: ${env:ENV}<br/>    SENTRY_DSN: ${env:SENTRY_DSN}<br/>    ROLLBAR_KEY: ${env:ROLLBAR_KEY}</pre>
<p>The keys under the environment key are all set on the AWS Lambda functions. While we never check the values of those variables into source control, they persist inside AWS. In this case, our&#160;<kbd>SENTRY_DSN</kbd> and <kbd>ROLLBAR</kbd> values should not be shared with anyone. However, if you're working in a team environment, anyone with access to the AWS Lambda console can very easily peek inside your Lambda functions and see the values for any of these variables.</p>
<div class="CDPAlignCenter"><img src="images/b9f8b4a3-1619-4fd4-ad8e-b74fe0074b50.png" style="width:40.33em;height:12.67em;"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Encrypting variables</h1>
                </header>
            
            <article>
                
<p>To fix this, we can leverage another AWS service called Key Management Service (KMS). KMS works by encrypting data into a string that can only be decrypted using KMS itself. What's nice about using KMS is that you can then store, share, or even check into source control your encrypted variables, since nobody can decrypt them unless they have access to KMS. Your one attack vector here then becomes AWS and KMS itself. If anyone has permission to use your KMS key or can gain access to a privileged AWS account, they can decrypt any KMS-managed variable.</p>
<div class="packt_infobox">Azure has something similar called Key Vault, which is something you should look into if building on top of Azure. I'm unaware of a similar service within Google Compute or other FaaS providers.</div>
<p>Encrypting data with KMS is quite simple. First, you'll need to create a KMS key. Once you have a key generated you will need to copy the AWS <kbd>arn</kbd> for your newly created key. From there, you can use a variety of APIs to encrypt a plaintext string. Using the previous example, I'm going to encrypt my <kbd>DB_PASSWORD</kbd> of <kbd>supersecret</kbd>. The following code block shows how to encrypt a password using Python and the <kbd>boto3</kbd> library:</p>
<pre style="padding-left: 30px">import boto3<br/>import base64<br/><br/>key_id = "arn:aws:kms:us-west-2:9802947738:key/fc5753bd-2842-4ff8-b9a7-61299f4a88c2"<br/>client = boto3.client('kms')<br/><br/>def encrypt():<br/>    stuff = client.encrypt(KeyId=key_id, Plaintext='supersecret')<br/>    binary_encrypted = stuff[u'CiphertextBlob']<br/>    encrypted_password = base64.b64encode(binary_encrypted)<br/>    print("Encrypted password:\n", encrypted_password.decode())</pre>
<p>The output of this code is an encrypted string, which you can share throughout your infrastructure:</p>
<pre style="padding-left: 30px">Encrypted password:<br/>AQICAHjgjPISkW/L824chyDIq2L43l5kDvqZM/+/CA8zfz94vQGfycexNX4Jq6mbciymbUh7AAAAaTBnBgkqhkiG9w0BBwagWjBYAgEAMFMGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMymQpnyP3KXAODTaZAgEQgCZ7+oORwCkkT0DUYfILp3Vg1sVGhx0acy1TU2jZAvB54IwrJg6cuA==</pre>
<p>You can accomplish the same task using&#160;<kbd>aws-cli</kbd>, as shown in the following snippet:</p>
<pre><strong>$ aws kms encrypt --key-id arn:aws:kms:us-west-2:9802947738:key/fc5753bd-2842-4ff8-b9a7-61299f4a88c2 --plaintext supersecret</strong><br/><strong>{</strong><br/><strong>    "KeyId": "arn:aws:kms:us-west-2:679892560156:key/cc0e82fe-bf27-4362-9bf0-292546c81aa8", </strong><br/><strong>    "CiphertextBlob": "AQICAHjgjPISkW/L824chyDIq2L43l5kDvqZM/+/CA8zfz94vQEON2GSPC5mwzgXBO1bYb4CAAAAaTBnBgkqhkiG9w0BBwagWjBYAgEAMFMGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMEuLd/v+4Hi4M4U6RAgEQgCY1j2xQQG3kRrrSZ2vq0l2uTuQb4GVJTb7pbVd3AbEV7U2HfWGx9A=="</strong><br/><strong>}</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Decrypting variables</h1>
                </header>
            
            <article>
                
<p>The question, of course, is how we can use this within our application; the answer is the inverse of what we just did. Now that we have an encrypted variable, our application code will need to read that value and decrypt it. Nothing changes from the standpoint of using environment variables. All that has changed now is that our sensitive variables are no longer stored in plain text anywhere within AWS.</p>
<p>After setting the <kbd>DB_PASSWORD</kbd> environment variable to this new encrypted string and redeploying, we can verify that the Lambda console is no longer storing the&#160;<kbd>supersecret</kbd> password. The following screenshot shows the value for the <kbd>DB_PASSWORD</kbd> variable from the AWS Lambda page for my function:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/a9b6d563-5b3f-40d6-a048-306caf139aaf.png" style="width:50.50em;height:2.83em;"/></div>
<p>In order for our Lambda functions to use KMS to decrypt this data, we need to authorize it explicitly. To accomplish this, let's add an IAM permission in <kbd>serverless.yml</kbd>. In the following snippet, <kbd>KMs_KEY_ARN</kbd> is referencing the KMS <kbd>arn</kbd>&#160;, as explained previously. This value can also be stored as an environment variable which, going back to the section on managing different environments, lets us quickly switch between different stacks where we'd otherwise be using different KMS keys.</p>
<pre>  iamRoleStatements:<br/>    - Effect: Allow<br/>      Action:<br/>        - KMS:Decrypt<br/>      Resource: ${env:KMS_KEY_ARN}</pre>
<p>Once that is done, we can get the database password with a few lines of code to fetch the value and decrypt it into plaintext. The following code block shows how to decrypt the password, where the encrypted value is still being pulled out of the environment:</p>
<pre style="padding-left: 30px">print("Decrypt the secret password")<br/>client = boto3.client('kms')<br/>binary_encrypted = base64.b64decode(os.environ['DB_PASSWORD'])<br/>results = client.decrypt(CiphertextBlob=binary_encrypted)<br/>db_password = results['Plaintext'].decode()</pre>
<p>With that, we can now use the <kbd>db_password</kbd> value to connect to the database as usual.</p>
<p>There are a few things to take note of here. First, this adds a small bit of latency to your code since each call to <kbd>decrypt</kbd> is an API call to AWS; you can take advantage of the statefulness of warm functions and only perform the decryption if it hasn't already been done, using a global variable or some other application-level variable that can be initialized on startup. Second, once you have decrypted sensitive values like this, the onus is on you to not log them in plain text or otherwise advertise or record the plaintext values.</p>
<p>There are many things to consider when dealing with services such as KMS. This&#160;section is only a very brief introduction, and we've barely scratched the surface. I encourage you to read more about the subject and carefully think through how you can make your application as secure as you need to.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Trimming AWS Lambda versions</h1>
                </header>
            
            <article>
                
<p><span>This last tip is specific to AWS Lambda. You may have noticed in other chapters that there are the following lines in the&#160;<kbd>serverless.yml</kbd> file:</span></p>
<pre style="padding-left: 30px">plugins:<br/>  - serverless-prune-plugin</pre>
<p>By default, each time you deploy a new version of an AWS Lambda function, AWS will help out by keeping the old version around. In a development system where you may be deploying dozens of times a day, this can become quite wasteful and, as cheap as storage is, it's not unlimited. Also, in the case of a production system that has a lifetime of years, the cost of all the old versions can add up.</p>
<p>If you're using the Serverless Framework, there is an easy way around this. If you're not using the Serverless Framework, however, it would be no more than a day's work to write a small script to do this for you. The <kbd>serverless-prune-plugin</kbd> will keep only a certain number of Lambda versions for you and delete the rest. The number of versions to keep is configurable and trimming happens whenever you perform a full deployment. Additionally, you are given some nice CLI hooks to manually delete old versions. You can read the details about this plugin on its GitHub page:&#160;<a href="https://github.com/claygregory/serverless-prune-plugin">https://github.com/claygregory/serverless-prune-plugin</a>.</p>
<p>If I add this to my previous divide function, configure the plugin to run automatically, and only keep two versions around, you can guess what will happen when I redeploy. That configuration I just mentioned will go into a <kbd>custom</kbd> block in my <kbd>serverless.yml</kbd>, shown as follows:</p>
<pre style="padding-left: 30px">custom:<br/>  prune:<br/>    automatic: true<br/>    number: 2</pre>
<p>Next, I'll deploy my code to see the plugin prune the old versions for me. The following code block shows the output after running a full deployment, with some lines taken out for brevity:</p>
<pre><strong># sls deploy -s dev </strong><br/><strong>Serverless: Packaging service... </strong><br/><strong>Serverless: Excluding development dependencies... </strong><br/><strong>Serverless: Checking Stack update progress... </strong><br/><strong>.................... </strong><br/><strong>Serverless: Stack update finished... </strong><br/><strong>stack: error-handler-dev </strong><br/><strong>api keys: </strong><br/><strong>  None </strong><br/><strong>endpoints: </strong><br/><strong>  GET - https://3gz9zt2yv1.execute-api.us-west-2.amazonaws.com/dev </strong><br/><strong>  GET - https://3gz9zt2yv1.execute-api.us-west-2.amazonaws.com/dev/process </strong><br/><strong>functions: </strong><br/><strong>  divide: error-handler-dev-divide </strong><br/><strong>  process: error-handler-dev-process </strong><br/><strong>Serverless: Prune: Running post-deployment pruning </strong><br/><strong>Serverless: Prune: Querying for deployed versions </strong><br/><strong>Serverless: Prune: error-handler-dev-divide has 11 additional versions published and 0 aliases, 9 versions selected for deletion </strong><br/><strong>Serverless: Prune: Deleting error-handler-dev-divide v9... </strong><br/><strong>.....</strong><br/><strong>Serverless: Prune: Deleting error-handler-dev-divide v1... </strong><br/><strong>Serverless: Prune: error-handler-dev-process has 11 additional versions published and 0 aliases, 9 versions selected for deletion </strong><br/><strong>Serverless: Prune: Deleting error-handler-dev-process v9... </strong><br/><strong>.....</strong><br/><strong>Serverless: Prune: Deleting error-handler-dev-process v1... </strong><br/><strong>Serverless: Prune: Pruning complete. </strong><br/><strong>Serverless: Removing old service versions...</strong></pre>
<p>I recommend always using this plugin for AWS and Serverless Framework, as versioned Lambda functions aren't very useful.&#160;Another option is to simply disable function versioning completely. This can be accomplished by adding&#160;<kbd>versionFunctions: false</kbd> under the <kbd>provider</kbd> key in the <kbd>serverless.yml</kbd> file.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered general best practices when deploying serverless applications and error tracking. We looked at examples of how to integrate Rollbar and Sentry, two error tracking and reporting services, in AWS Lambda functions so that unexpected errors do not go unnoticed. We also discussed some strategies regarding application logging and methods to ensure you get the metrics and telemetry you need. We also addressed the issue of cold starts in cloud functions, and we discussed ways of working around them. From there, we walked through some techniques to help you with local testing and setting up serverless functions and systems. Finally, we reviewed the management of different environments or stacks using environment variables and the encryption of sensitive variables using AWS's Key Management Service.</p>
<p class="mce-root">Best practices for serverless applications could fill an entire book by themselves. We touched on many significant topics in this chapter that put you on the right trajectory moving forward. While this chapter cannot solve all of the challenges you may face in serverless application development, it does provide solutions to some of the most common issues and gives you the background necessary to find answers to your unique problems. At this point, readers should feel confident setting up and managing their own production-level serverless application.</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    </div>
</body>
</html>