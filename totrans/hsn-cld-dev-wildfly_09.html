<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Configuring Continuous Integration Using Jenkins</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">In this chapter, we will teach you how to integrate the pet store application with Jenkins, a <strong>Continuous Integration</strong> (<strong>CI</strong>) server. We will introduce CI concepts and how they can be implemented using Jenkins. We will configure a sample <kbd>pipeline</kbd> so that you can see how changes in application code are propagated to the deployed application. </p>
<p>Let's start with the builds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning OpenShift builds</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">In the previous chapters, we did some serious magic in order to build our application. To be able to run the builds, we executed the following command:</p>
<pre class="mce-root CDPAlignLeft CDPAlign"><strong>oc create -f https://raw.githubusercontent.com/wildfly-swarm/sti-wildflyswarm/master/1.0/wildflyswarm-sti-all.json</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">In previous chapters, when we wanted to build our application, we invoked the following command:</p>
<pre class="mce-root CDPAlignLeft CDPAlign"><strong>oc new-app wildflyswarm-10-centos7~https://github.com/PacktPublishing/Hands-On-Cloud-Development-with-WildFly.git (...)</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">After a lot of mysterious stuff had happened (as indicated by growing logs), we were able to see our application working. Now, it's time to explain what actually happened under the hood. Let's get to know OpenShift builds.</p>
<p class="mce-root CDPAlignLeft CDPAlign">In general, an OpenShift build is an operation that transforms input parameters into a resulting object that is used to start an application. In most cases, the build will transform the source code into an image that will be later deployed on the cluster.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The details of the build process operation depend on the build type (about which we will learn in a moment), but the general algorithm looks as <span>follows</span>:</p>
<ol>
<li class="mce-root">The build container starts from the build image</li>
<li class="mce-root">Sources from all the inputs are being injected into the container</li>
<li class="mce-root">The build scripts are being run</li>
<li class="mce-root">The output docker image is created</li>
</ol>
<p>The new concept that is being introduced here is the build container. Let's take a look at it a little bit closer. What actually is its purpose? The container in which you are building your application has to contain all the libraries, tools, and runtimes that are necessary to build and run your application. For example, if you use the WildFly AS builder image, it will contain Java, Maven, and WildFly runtimes among others. After the application is built, the same image is used as a base for the Docker image that will be deployed to OpenShift. Speaking precisely, your application will be added as another layer on top of the builder image, resulting in a runnable image with your application. The good news here is that although you can easily create an image yourself, in most cases those images will be created by the tool provider. </p>
<p class="mce-root CDPAlignLeft CDPAlign">The input types can be provided from any resources, such as GitHub repositories, existing images, and Dockerfile config. All the sources that you provide are unpacked and merged together in the build directory, which will be processed by the builder image during the build. The option that we will use (and actually have used a few times already) in this book is GitHub repositories.</p>
<p class="mce-root CDPAlignLeft CDPAlign">As we mentioned previously, the precise way in which a build works depends on the build type. You will be able to define the type of build by specifying the build strategy. You can create images using Docker, Source-to-image of custom builds. The build type that is most interesting for us is the source-to-image build, which we will explain in the next section. </p>
<p>There is also another type of build—<kbd>pipeline</kbd>. The <kbd>pipeline</kbd> build is connected to the Jenkins CI server and allows you to create a fully featured <strong>Continuous Deployment</strong> (<strong>CD</strong>) <kbd>pipeline</kbd>. We will describe this kind of build thoroughly in the second part of this chapter.</p>
<p>Let's turn to the source-to-image build now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning about the source-to-image build</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">As we mentioned, the source-to-image build needs a builder image and you have to provide it each time when you are configuring such a build. The builder images contain scripts that are responsible for assembling and running the application. The assembling scripts will be run in phase 3 of the build algorithm, and the run script will be used as the start command of the resulting Docker image. During the build, the layer that contains the runnable application will be added on top of the builder image, the run script will be set as the image starting command, and the resulting image will be committed.</p>
<p class="mce-root CDPAlignLeft CDPAlign">We know the basics of source-to-image builds, so now we can explain what we did when deploying our application in the last chapters. Let's start with the following command that we have invoked before running any builds:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">oc create -f https://raw.githubusercontent.com/wildfly-swarm/sti-wildflyswarm/master/1.0/wildflyswarm-sti-all.json</pre>
<p class="mce-root CDPAlignLeft CDPAlign">The preceding command is responsible for including a YAML object file into our cluster. The main object created by this script is the Docker build configuration. If we examine our cluster using command-line tools, we will find that the new build config is created:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/10100531-2072-4a6f-862c-8a199630347d.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">This is the build config for our builder image. We may now examine builds in the Web Console. We will be able to see that the build based on <kbd>wildfyswarm-10-centos7</kbd> config has already been executed:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/27d3b465-578f-4500-8bfd-7f6939ed0253.png" style=""/></div>
<p class="mce-root CDPAlignLeft CDPAlign CDPAlignCenter">After the execution of the first command, the builder image was created and stored in the cluster. We can confirm this by navigating to <span class="packt_screen">Build</span> | <span class="packt_screen">Images</span> in the web console:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/c4082b65-aeed-49d5-98c6-44dc07906834.png" style=""/></div>
<p class="mce-root CDPAlignLeft CDPAlign">As you will have noticed in the preceding screenshot, we have a new image, <span><span class="packt_screen">wildflyswarm-10-centos7</span></span>, <span>available in the cluster. </span><span>An important thing to note here is that these images have been described as</span> <kbd>ImageStreams</kbd>. What does that actually mean? <kbd>ImageStream</kbd>, as its name suggests, is an object that represents a stream of related objects. In our scenario, the <kbd>ImageStream</kbd> contains all images that are the result of the build of the builder image.</p>
<div class="mce-root CDPAlignLeft CDPAlign packt_infobox">We created the BuildConfig for the builder image. The source for this image can change; if that happens, OpenShift will create a new version of this image and add it to the <kbd>ImageStream</kbd>.</div>
<p class="mce-root CDPAlignLeft CDPAlign">The images in the stream can be tagged, and there is always the latest tag, which represents the latest image in the stream. </p>
<p><span>Let's now examine the <kbd>new-app</kbd> command that we have used before:</span></p>
<pre class="mce-root CDPAlignLeft CDPAlign"><strong>oc new-app wildflyswarm-10-centos7~https://github.com/PacktPublishing/Hands-On-Cloud-Development-with-WildFly.git (...)</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">We are now ready to explain what the <kbd>new-app</kbd> syntax means. It has two parts separated by a tilde. The first one is the name of the builder-image stream. The second one is the GitHub repository from which the application will be built. </p>
<p class="mce-root CDPAlignLeft CDPAlign">After we know the internals of the source-to-image build, we can run the build again and examine the build log.</p>
<p class="mce-root CDPAlignLeft CDPAlign">First, we have to remove the <kbd>pricing-service</kbd> that we have deployed previously:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">oc delete all -l app=pricing-service</pre>
<p class="mce-root CDPAlignLeft CDPAlign">After that, we are ready to execute the <kbd>new-app</kbd> command and use web console to inspect the log:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6d2191fd-ea40-40bd-8421-8d167f128521.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Oops! We have to download all the dependencies. This fact will result in build taking a substantial amount of time:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ea37e902-8fa9-457e-b7e6-fcdaa2c0f279.png" style=""/></div>
<p class="mce-root">This was just a first build. So, what will happen when we run the build for the second time?<br/>
You can use the web console to force the second build and inspect the log to verify that the dependencies are downloaded again.</p>
<p class="mce-root CDPAlignLeft CDPAlign">This is a serious inconvenience, as it results in much longer build types. Can we do something about it? Yes, we can use incremental builds.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The incremental build is a feature of the source-to-image build, which extracts the build artifacts from the previously created image and uses them to build the next one.</p>
<div class="mce-root CDPAlignLeft CDPAlign packt_infobox">Our builder image uses the Maven plugin to build a Swarm application, so the artifacts that are being downloaded are the Maven dependency JARs. Usually, different build tools and different types of the artifact will be used. As a result, the specific type of incremental build has to be implemented by the image provider.</div>
<p class="mce-root CDPAlignLeft CDPAlign">In the case of a Swarm builder image, the Maven artifacts are being extracted from the last image and placed in the Maven repo of the new one. As a result, artifacts that are being used many times have to be downloaded only once. Furthermore, in order to <span>decrease the time spent downloading the JARs, you can use a Maven mirror.</span></p>
<p class="mce-root CDPAlignLeft CDPAlign">OK. However, how can we turn the incremental build on? We have to edit the YAML of our build.<br/>
Let's use the web console for that. We have to select the <kbd>pricing-service</kbd> build and navigate to <span class="packt_screen">Actions</span> | <span class="packt_screen">Edit YAML</span> in the top-<span>right</span> corner of the screen. The YAML has to be edited in the following way:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/42b76cfa-1eb5-4675-90ed-4105e381b73e.png"/></div>
<p class="mce-root">As you will have noticed in the preceding screenshot, we found the <kbd>sourceStrategy</kbd> section of the build config and added an incremental property with a value set to <kbd>true</kbd>. Let's run our build again to see what happens.</p>
<p class="mce-root">In our new build log, we can see two optimistic lines: </p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6bc49bc1-f288-4f34-a5c4-08638921b56b.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span>The first optimistic line is at the beginning</span> where Maven informs us that the artifacts are being restored and the second one is at the end:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/3aca6699-48da-44b9-92be-8621be134dbf.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">The build has taken only <kbd>16.347</kbd> seconds, not much longer than the standalone Maven build.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring environment variables</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">When we were deploying our services, we provided an environment variables script for catalog and <kbd>pricing-services</kbd> <span>that needs to interact with our</span> database. Processing this configuration file is also the responsibility of the source-to-image build. If a user wants to provide environment properties to the build, they have to create a <kbd>.s2i</kbd> directory in the root of the service's GitHub repository and create an environment file that will contain a list of key-value pairs.</p>
<p class="mce-root CDPAlignLeft CDPAlign">For example, let's recall the configuration file for the <kbd>pricing-service</kbd>:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">POSTGRESQL_HOST=pricing<span>-service.petstore.svc</span><br/>POSTGRESQL_USER=pricing<br/>POSTGRESQL_PASSWORD=pricing<br/>POSTGRESQL_SCHEMA=pricingdb</pre>
<p class="mce-root CDPAlignLeft CDPAlign">Properties set in this file will be available as environment variables during the image build and during its execution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The whole source-to-image algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">After covering the specifics of the source-to-image build operation, let's recap the steps in Swarm's <kbd>s2i</kbd> build:</p>
<ol>
<li>The container on which the build will take place is created from the builder image.</li>
<li>The sources of an application are injected into the container.</li>
<li>If incremental builds are enabled, the Maven artifacts will be restored from the previous build image.</li>
<li>If provided, environment variables are set.</li>
<li>The assembly script, provided by the image creator, is executed.</li>
<li>The image is committed with the start command set to the run script provided by the image creator.</li>
</ol>
<p class="mce-root CDPAlignLeft CDPAlign">A developer who will like to build their applications using the source-to-image build has to provide the name of the builder image and the source code of an application. A developer can enable the incremental build and provide environment variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Source-to-image summary</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">Now that we have covered how the source-to-image build works internally, it's time to look at it from a wider perspective.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The source-to-image build is another tool provided by OpenShift that abstracts away the details of the Kubernetes cluster, providing a simple interface for the developer. The role of the developer is to provide the source code and the name of the image that will be used to build it. It is the responsibility of the image creator to assemble the Docker image that will be deployed on the cluster.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Again, this leads to the separation of concerns—the builder image provider is responsible for assembling source in an optimal way and the details of those optimizations don't have to be known by the developer.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The performance implications of builds resulting from the build architecture are as follows. The libraries that are needed to perform the build and create the runnable container are located in the builder image, which is created once (and later only updated) inside the cluster. The artifacts that are being downloaded during the build can be restored from previous builds if the incremental build is enabled. Owing to that, the dependencies of the application can be downloaded only once and later reused. This leads to a very fast build time. As you may remember, the build of our pricing service took only about 16 seconds, which is only a few seconds more than standalone Maven builds on a modern workstation.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Moreover, the reproducibility, which is one of the constant benefits of using Docker, holds for builder images<span> also</span>. All the builds are performed using exactly the same image. As a result, it is guaranteed that the build result will be the same on all of your environments.</p>
<p class="mce-root CDPAlignLeft CDPAlign">In addition, since builder images are just standard Docker containers and the explicit builder contract allows tool creators to write builder images easily, there is an wide variaty of Docker builder images that you can use. You, as a developer, already have access to a wide variety of builder images dedicated to number of development tools.</p>
<p class="mce-root CDPAlignLeft CDPAlign">In the end, a source-to-image build tool is a tool that represents the core of the OpenShift philosophy. It provides a simple developer interface, which abstracts away the cluster internals, and under the hood it implements the an optimized build process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The developer view</h1>
                </header>
            
            <article>
                
<p>Till now, we have explained in detail how the source-to-image build builds an image based on your code. The new-app command does not just create the build though. As you remember, after its execution, we were able to test the working application. Clearly, the build and image are not the only product of the command. </p>
<p>Apart from the <kbd>BuildConfiguration</kbd>, the new-app command creates the <kbd>DeploymentConfiguration</kbd> (that we described in <a href="461aee71-984a-4158-addc-fc49341d3455.xhtml">Chapter 6</a>, <em>Deploying Applications on the Cloud with OpenShift</em>) and an <kbd>ImageStream</kbd> for our application.</p>
<p>Let's take a look at the created objects in the following <span>diagram</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2bbdeae3-1b57-4028-979a-edb786780b6a.png" style=""/></div>
<p>In the preceding diagram, the objects related to the builder image are colored red, build-related objects are colored blue, and deployment-related objects are colored green. The build is triggered by a developer by pushing changes to GitHub. It results in the creation of the build objects. If the build <span>is successful,</span> the image is pushed to the image stream. This further triggers the deployment of the application, which, if successful, results in the creation of application services.</p>
<p>The important thing to note is that, in the simplest scenario, a developer may be responsible only for pushing the changes to the repository—in other words, programming and their changes will be propagated to the cluster.</p>
<p>That's nice again, but, in some scenarios, we will like to have more than that: a full CD <kbd>pipeline</kbd> with integration tests, checking the deployed application, or staging the changes in different environments. As we hinted earlier, we can integrate an OpenShift cluster with Jenkins to use its full power to implement the CD <kbd>pipeline</kbd> for our services. Let's learn how to do it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pipeline build</h1>
                </header>
            
            <article>
                
<p><span>In the first chapter, when we were explaining why you may be considering implementing the microservice architecture in your applications, we mentioned the challenges that are being currently faced by application developers and architects. </span></p>
<p>One of the key tools that may enable us to deal with providing software in a way that enables us to meet those challenges is automation. As we covered in the preceding chapter, OpenShift enables us to automate infrastructure provisioning. However, we need more than that. </p>
<p>We will also like to automate the process of deploying software into production. Ideally, we will like to have tools that will enable us to release software immediately. OpenShift provides such a tool in the form of the build <kbd>pipeline</kbd>. Let's introduce the rationale behind this concept.</p>
<p>Let's start with CI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous integration</h1>
                </header>
            
            <article>
                
<p>As a developer, you know too well what the development of projects looks like. There are many developers working on different functionalities, which they contribute to the same repository. Contributions from all the developers have to be integrated into the code repository so that stable code is created. After that, the code can be published into the production environment.</p>
<p>This sounds simple, but if you don't create an organized order according to which this process is executed, you will quickly end up with a huge mess. If the developers will integrate rarely, they are asking for problems. Their repositories will be highly diverged, and the application's functionality will be scattered between their repositories. As a result, during the development, there will be no <em>current state</em> source repository, and we will have no information about the state of an application. The new version of an application will emerge during the time people decide to push their contribution to the main code (which will presumably happen the day before the release). The process of integration at this point will be painful, where incompatible contributions are being discovered, and errors will emerge. Such a situation was described in the past as <em>integration hell</em>. </p>
<p>Owing to the preceding problems, it became clear that it will be a good idea to integrate code frequently. The methodology that advocates such a behavior and, more importantly, gives hints on how to do it, is called CI.</p>
<p>Obviously, pushing the code frequently to the repository is not helping us much. At each commit, we need to make sure that the current version of the code at least compiles, and passes unit and integration tests. This is by no means a comprehensive list: to declare your code correctly, you may also need automatic code inspections or code reviews to name a few. </p>
<p>In order for this process to be executed consistently, it has to be automated and executed each time the user wants to make the change to the code. Also, developers are supposed to integrate their code frequently, with each logical functionality developed, and are supposed to fix any errors that appear as soon as possible.</p>
<p>If this procedure is observed, this will lead to a number of benefits:</p>
<ul>
<li>Problems are detected quickly. As the result, their source can be debugged and fixed quickly.</li>
<li>The current version of the application is always present—it is the result of the last successful build. At each point, we can tell the status of the application, how it works, and what functionalities have been currently implemented.</li>
<li>The automated process works as a trigger for quality control. The build is guaranteed to be run and be reproducible. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous deployment</h1>
                </header>
            
            <article>
                
<p><span>Continuous Integration ensures continuous builds of source code</span>. It demands that fixes are pushed often and provides instant feedback to the developers. What if we extend this notion and configure our build infrastructure so that <span>it will ensure that our services will be built and deployed automatically? </span></p>
<p>Such an approach, which is an extension of CI, is called Continuous Deployment. To implement it, we will need to automate the release process<span> also</span>. This means that we will have to keep all the resources that are needed to release the software to the given environment, such as environment properties or configuration scripts. </p>
<p>As a reward, we will be able to obtain reliable and repeatable releases. First of all, as the release process is no longer manual, all the magic is taken away from the release process. The release is executed by the release script using environment properties, which are both parts of the versioned build configuration. Those files are the one source-of-truth regarding the build process. As a result, if an error occurs during the build, those scripts have to be fixed. There is no place for manual patches or ad hoc fixes. Also, builds happen often, so configuration bugs will have an opportunity to occur and be fixed. On the other hand, after builds and releases start to work correctly, each next correct build adds more confidence in the release process. As a result, the release becomes a well-tested and an automated event.</p>
<div class="packt_tip">Such an approach changes the way the team works by changing the speed at which features are developed. With CD, you are not releasing the software in large chunks to the client. Instead, small functionalities are released often and are immediately visible to the client.</div>
<p>This is the expected behavior for a number of reasons. First, customers will like to respond to client demand as quickly as possible. Having the tool that enables them to do that will be a big market advantage for the customer. However, there is more to it: because new functionalities are released often, they are visible to the customer immediately. As a result, a customer can immediately assess the actually implemented functionality. This creates an efficient feedback loop between the developers and the customers, which allow for faster convergence to the functionality actually expected by the client.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment pipeline</h1>
                </header>
            
            <article>
                
<p>The process of automatic delivery is implemented using a <kbd>pipeline</kbd>. A <kbd>pipeline</kbd> is a chain of steps that takes the source code as its input and provides a working application on it's output.</p>
<p>The goal of the <kbd>pipeline</kbd> is to make sure that the source code is ready to be deployed in production. As a result, a <kbd>pipeline</kbd> should be able to catch errors as soon as possible and provide feedback to the developers immediately. </p>
<p>Also, because the final product is the released application, a <kbd>pipeline</kbd> should automate the release process so that it is run the same in all environments.</p>
<p>Although a <kbd>pipeline</kbd> is a configurable script and its direct operation depends on your concrete environment, there are a number of <span>common</span> steps that are executed in the deployment <kbd>pipeline</kbd>: commit, build, automatic tests, manual tests, release, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Continuous Deployment in OpenShift environments</h1>
                </header>
            
            <article>
                
<p>After this quick theory recap, now let's return to our cluster and configure CD for our application.</p>
<p>At the beginning of this chapter, we described the source-to-image build, which we have used in previous chapters. We also hinted that there is a <kbd>pipeline</kbd> build available. As you probably have guessed by now, this is the kind of build that we will use to implement CD of our services.</p>
<p>The <kbd>pipeline</kbd> build uses the Jenkins server to configure the <kbd>pipeline</kbd> configuration. Before moving further, let's introduce it quickly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Jenkins</h1>
                </header>
            
            <article>
                
<p>Jenkins is an open source software automation server. It allows for <kbd>pipeline</kbd> creation and provides the relevant syntax. So, how are <span>we </span>able to use Jenkins in OpenShift cluster and configure <kbd>pipeline</kbd> execution? Let's find out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Our first pipeline</h1>
                </header>
            
            <article>
                
<p>Let's start by creating our first <kbd>pipeline</kbd>. We have to log in to our web console and navigate to <span class="packt_screen"><span class="packt_screen">Add</span> to project</span> | <span class="packt_screen">Import YAML</span>.</p>
<p>In order to do that, we have to go the<span> </span>web console's<span> </span>main web page and <span>navigate to</span> <span class="packt_screen">Add to Project</span> | <span class="packt_screen">Import YAML/Json</span> and enter the following script there:</p>
<pre>apiVersion: v1<br/>kind: BuildConfig<br/>metadata:<br/>  name: pricing-service-pipeline<br/>  labels:<br/>    name: pricing-service-pipeline<br/>spec:<br/>  runPolicy: Serial<br/>  strategy:<br/>    type: JenkinsPipeline<br/>    jenkinsPipelineStrategy:<br/>      jenkinsfile:"pipeline { \n agent any\n stages {\n stage('Build') {\n steps {\n echo 'Pipeline is running'\n }\n }\n }\n }\n"</pre>
<p><span>After the script is created, we can click on the <span class="packt_screen">Create</span> button:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/36d6de0a-fbfe-4258-90d7-67401a187b42.png"/></div>
<p>Before we look further at the <kbd>pipeline</kbd> code, let's note the other things that are happening. If we get to the main view of the web console, we will note that there is a new resource:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/55b9a649-623f-46d3-b307-cc79a79bd3ae.png" style=""/></div>
<p>Let's take a look at the currently available <span class="packt_screen">Pods</span> too:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c055352a-525b-4747-a18a-617bbbc0ec87.png" style=""/></div>
<p>Indeed, there is a new deployment of Jenkins server running, and the container for the Jenkins server is being created. OpenShift runs a <kbd>pipeline</kbd> build using the Jenkins server. Therefore, whenever you create a <kbd>pipeline</kbd>, OpenShift must check whether there is a Jenkins server present in the cluster. If not, OpenShift will start one automatically. </p>
<p>The creation of the Jenkins server takes some time, so we have to wait till it has been deployed. After we are able to see <span>that the application is running </span>in the <span class="packt_screen">Pods</span> view, we are ready to start the build of our first <kbd>pipeline</kbd>.</p>
<p>In order to do that, let's navigate to <span class="packt_screen">Build<span><span> | </span></span></span><span class="packt_screen">Pipelines</span>. You will be able to see that there is a new <kbd>pipeline</kbd> present:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3d6f27c8-2cb8-46a8-a715-4e9def53639d.png" style=""/></div>
<p>Let's click on the <span class="packt_screen">Start Pipeline</span> button and see what happens:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9994127a-285b-4024-8a8c-5b96cb1ec677.png"/></div>
<p>Note in the preceding screenshot that the build has run. The dot with the tick described as Print means that one stage has been run and that it has been successful. We will be talking about the Jenkins <kbd>pipeline</kbd> structure in just a moment. Now, let's take a look at more information about our current build by clicking on the <span class="packt_screen">View Log</span> button:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/d7e6c34b-3b7c-4150-a09b-bf2e49a2921b.png"/></div>
<p>As you will have noticed in the preceding screenshot, we have been redirected to the Jenkins console. The build has been created, the <span class="packt_screen">Print</span> stage has been executed, and the print message that we have echoed has indeed been written to a log. </p>
<p>As you can see, the <kbd>pipeline</kbd> build configuration has been automatically turned into the Jenkins build and run in the Jenkins console. We will get more information about the build when we click on <span class="packt_screen">petstore/pricing-service-pipeline</span> in the top-<span>left</span> corner of the screen:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f82c848c-08f8-40bd-a744-5dac8d233a5f.png"/></div>
<p>From this window, we can trace the build history, view the logs and time of the latest execution, or edit the <kbd>pipeline</kbd>, among others. At this point, it is good to look again at the script that we have written in order to create the <kbd>pipeline</kbd>. You probably have noted immediately that the Jenkins <kbd>pipeline</kbd> was squashed into one line, making it hard to read and edit. Before we take any other steps, let's find a human way to edit our <kbd>pipeline</kbd>.</p>
<p>In order to do that, let's click on the <span class="packt_screen">Configure</span> button on the left-hand side menu and scroll down:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/30400538-e076-458d-8030-156f5eb4ee70.png" style=""/></div>
<p>We have a good editor for our <kbd>pipeline</kbd> here. Let's make our first edit of the file:<span> </span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9e60e66a-3ad4-4f58-baf9-829871ab8bf2.png" style=""/></div>
<p><span>We will then test it to check whether it works. In</span> order to do that, we have to save the <kbd>pipeline</kbd> and click on the <span class="packt_screen">Build Now</span> button in the build view. After that, we are ready to examine the log by clicking on the second build that has just been executed:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/aad14228-9d6e-4506-b097-35c58f78752e.png" style=""/></div>
<p>We will see the new log as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4bec7ff-ce59-47e8-acda-b9fe45d04c66.png" style=""/></div>
<p>Also, let's log <span>in </span>again to the web console and examine that <kbd>pipeline</kbd> there:</p>
<div class="CDPAlignCenter CDPAlign"><span class="packt_screen"><span class="packt_screen"><img src="assets/5e53124c-6629-4e92-bf37-c43f370dc39b.png" style="width:46.00em;height:26.75em;"/></span></span></div>
<p>As you will have noticed, the <kbd>pipeline</kbd> build config was modified accordingly to the changes that we have made in Jenkins. We will perform our future changes using the Jenkins server.</p>
<p>The new message that we are printing in the build promises that our build will do something useful<span> at some point</span>. After all, we want to create a CD <kbd>pipeline</kbd> for our services and not print messages. Before we can do it though, we will need to learn a few more things. In the beginning, we will need to say a few more words about the language that we are using to define the <kbd>pipeline</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pipeline syntax language</h1>
                </header>
            
            <article>
                
<p>When we wrote our first <kbd>pipeline</kbd>, we used the Jenkins declarative pipeline language. We will describe the essentials of the <strong>Declarative Pipeline Language</strong> (<strong>DPL</strong>) in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Core pipeline elements</h1>
                </header>
            
            <article>
                
<p>In order to do that, let's return to the <kbd>pipeline</kbd> that we executed in the preceding section:</p>
<pre>//1<br/>pipeline {<br/>    //2<br/>    agent any<br/>    //3<br/>    stages {<br/>        //4<br/>        stage('Print') {<br/>            steps {<br/>                echo 'This pipeline will build pricing-service one day'<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>Each <kbd>pipeline</kbd> in DPL must be enclosed with the <kbd>pipeline</kbd> block (1).</p>
<p>The <kbd>pipeline</kbd> must begin with the <kbd>agent</kbd> directive (2). This directive specifies the Jenkins builder machines in which the build stages (more about them in a moment) can be executed. This setting can be overridden in each of the stages. In our examples, we will use any agent for all the stages.</p>
<p>The core <kbd>pipeline</kbd> build blocks are the stages. The stages are meant to map to the stages in the CD <kbd>pipeline</kbd>. They are defined in a serial order, and each stage can execute only if the stage before has succeeded.</p>
<p>The stages have to be enclosed with the <kbd>stages</kbd> (3) block. Each stage (there need to be at least one of them) has its own <kbd>stage</kbd> block with the name specified as a parameter.</p>
<p>Each stage block can contain a bunch of directives followed by the steps block, which encloses one or more steps that will be executed in the <kbd>pipeline</kbd>.</p>
<p>Now, we are getting to the key point. What are the available steps that we can execute? Jenkins provides a very large number of different steps provided by different plugins. We will concentrate on one specific plugin that makes it easy to develop and execute operations on OpenShift clusters—let's discuss OpenShift, the <kbd>pipeline</kbd> Jenkins plugin (Further reading, link 1).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standard Maven operation</h1>
                </header>
            
            <article>
                
<p>The first stage that we will implement is the unit testing stage. In the beginning, we will add a simple unit test in the same way that we did in <a href="c1be724d-e5fd-4c33-bd27-c04887d5cc8e.xhtml">Chapter 5</a>, <em>Testing Your Services with Arquillian</em>. We have to extend <kbd>pom.xml</kbd>:</p>
<pre><span><br/></span><span>(...)<br/></span><span><br/></span><span>    <br/></span><span>    &lt;dependencies&gt;<br/></span><span>        (...)<br/>        &lt;dependency&gt;<br/></span><span>            &lt;groupId&gt;</span>org.postgresql<span>&lt;/groupId&gt;<br/></span><span>            &lt;artifactId&gt;</span>postgresql<span>&lt;/artifactId&gt;<br/></span><span>            &lt;version&gt;${version.postgresql}</span><span>&lt;/version&gt;<br/></span><span>        &lt;/dependency&gt;<br/></span><span><br/><strong>        //1</strong><br/></span><strong>        &lt;dependency&gt;<br/>            &lt;groupId&gt;junit&lt;/groupId&gt;<br/>            &lt;artifactId&gt;junit&lt;/artifactId&gt;<br/>            &lt;version&gt;${version.junit}&lt;/version&gt;<br/>            &lt;scope&gt;test&lt;/scope&gt;<br/></strong><span><strong>        &lt;/dependency&gt;</strong><br/></span><span><br/><strong>        //2</strong><br/></span><strong>        &lt;dependency&gt;<br/>            &lt;groupId&gt;org.jboss.arquillian.junit&lt;/groupId&gt;<br/>            &lt;artifactId&gt;arquillian-junit-container&lt;/artifactId&gt;<br/>            &lt;scope&gt;test&lt;/scope&gt;<br/></strong><span><strong>        &lt;/dependency&gt;</strong><br/></span><span><br/><strong>        //3</strong><br/></span><strong>        &lt;dependency&gt;<br/>            &lt;groupId&gt;org.wildfly.swarm&lt;/groupId&gt;<br/>            &lt;artifactId&gt;arquillian&lt;/artifactId&gt;<br/>            &lt;version&gt;${version.wildfly.swarm}&lt;/version&gt;<br/>            &lt;scope&gt;test&lt;/scope&gt;<br/></strong><span><strong>        &lt;/dependency&gt;</strong><br/></span><span><br/><strong>        //4</strong><br/></span><strong>        &lt;dependency&gt;<br/>            &lt;groupId&gt;com.h2database&lt;/groupId&gt;<br/>            &lt;artifactId&gt;h2&lt;/artifactId&gt;<br/>            &lt;version&gt;${version.h2}&lt;/version&gt;<br/>            &lt;scope&gt;test&lt;/scope&gt;<br/></strong><span><strong>        &lt;/dependency&gt;</strong><br/></span><span><br/></span><span>    &lt;/dependencies&gt;<br/></span><span><br/></span><span>    (...)<br/></span><span><br/></span><span><br/></span><span><br/></span><span>&lt;/project&gt;<br/></span></pre>
<p>Recall that we had to add dependencies for JUnit (1), Arquillian (2), Swarm's Arquillian adapter (3), and the in-memory database that we will use (4).</p>
<p>Secondly, we have to provide test resources, namely <kbd>persistence.xml</kbd>:</p>
<pre><span>&lt;?</span><span>xml version=</span><span>"1.0" </span><span>encoding=</span><span>"UTF-8"</span><span>?&gt;<br/></span><span>&lt;persistence<br/></span><span>        </span><span>xmlns:</span><span>xsi</span><span>=</span><span>"http://www.w3.org/2001/XMLSchema-instance"<br/></span><span>        </span><span>version=</span><span>"2.1"<br/></span><span>        </span><span>xmlns=</span><span>"http://xmlns.jcp.org/xml/ns/persistence"<br/></span><span>        </span><span>xsi</span><span>:schemaLocation=</span><span>"http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd"</span><span>&gt;<br/></span><span>    </span><span>&lt;!-- 1 --&gt;<br/></span><span>    </span><span>&lt;persistence-unit </span><span>name=</span><span>"PricingPU" </span><span>transaction-type=</span><span>"JTA"</span><span>&gt;<br/></span><span>        </span><span>&lt;!-- 2 --&gt;<br/></span><span>        </span><span>&lt;jta-data-source&gt;</span>java:jboss/datasources/PricingDS<span>&lt;/jta-data-source&gt;<br/></span><span>        &lt;properties&gt;<br/></span><span>            </span><span>&lt;!-- 3 --&gt;<br/></span><span>            </span><span>&lt;property </span><span>name=</span><span>"javax.persistence.schema-generation.database.action" </span><span>value=</span><span>"drop-and-create"</span><span>/&gt;<br/></span><span>            &lt;property </span><span>name=</span><span>"javax.persistence.schema-generation.create-source" </span><span>value=</span><span>"metadata"</span><span>/&gt;<br/></span><span>            &lt;property </span><span>name=</span><span>"javax.persistence.schema-generation.drop-source" </span><span>value=</span><span>"metadata"</span><span>/&gt;<br/></span><span><br/></span><span>            &lt;property </span><span>name=</span><span>"javax.persistence.sql-load-script-source" </span><span>value=</span><span>"META-INF/load.sql"</span><span>/&gt;<br/></span><span>        &lt;/properties&gt;<br/></span><span>    &lt;/persistence-unit&gt;<br/></span><span>&lt;/persistence&gt;</span></pre>
<p><span>An</span><span>d</span> the load script that we are going to use to test the database:</p>
<pre><span>DROP TABLE </span>IF <span>EXISTS </span>PRICE;<br/><br/><span>CREATE TABLE </span>PRICE (id serial <span>PRIMARY KEY</span>, name <span>varchar</span>, price <span>smallint</span>);<br/><br/><span>INSERT INTO </span>PRICE(name, price) <span>VALUES </span>(<span>'test-pet'</span>, <span>5</span>);</pre>
<p>Ensure that we also add the <kbd>h2</kbd> driver module:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/17e1078e-59f2-4af1-b8f3-efa2582103ca.png" style=""/></div>
<p>We are now ready to write a test:</p>
<pre><span>package </span>org.packt.swarm.petstore.pricing<span>;<br/></span><span><br/></span><span>import </span>org.jboss.arquillian.container.test.api.<span>Deployment</span><span>;<br/></span><span>import </span>org.jboss.arquillian.junit.Arquillian<span>;<br/></span><span>import </span>org.jboss.shrinkwrap.api.ShrinkWrap<span>;<br/></span><span>import </span>org.jboss.shrinkwrap.api.asset.EmptyAsset<span>;<br/></span><span>import </span>org.jboss.shrinkwrap.api.spec.JavaArchive<span>;<br/></span><span>import </span>org.junit.Assert<span>;<br/></span><span>import </span>org.junit.<span>Test</span><span>;<br/></span><span>import </span>org.junit.runner.<span>RunWith</span><span>;<br/></span><span>import </span>org.wildfly.swarm.Swarm<span>;<br/></span><span>import </span>org.wildfly.swarm.arquillian.<span>CreateSwarm</span><span>;<br/></span><span>import </span>org.wildfly.swarm.datasources.DatasourcesFraction<span>;<br/></span><span>import </span>org.wildfly.swarm.jaxrs.JAXRSArchive<span>;<br/></span><span>import </span>org.wildfly.swarm.spi.api.Module<span>;<br/></span><span><br/></span><span>import </span>javax.inject.<span>Inject</span><span>;<br/></span><span><br/></span><span>//1<br/></span><span>@RunWith</span>(Arquillian.<span>class</span>)<br/><span>public class </span>PricingServiceTest {<br/><br/>    <span>//2<br/></span><span>    </span><span>@Deployment<br/></span><span>    </span><span>public static </span>JavaArchive <span>createDeployment</span>() {<br/>        <span>return </span>ShrinkWrap.<span>create</span>(JavaArchive.<span>class</span>)<br/>                .addClasses(Price.<span>class, </span>PricingService.<span>class</span>)<br/>                .addAsResource(<span>"META-INF/persistence.xml"</span>)<br/>                .addAsResource(<span>"META-INF/load.sql"</span>)<br/>                .addAsManifestResource(EmptyAsset.<span>INSTANCE</span><span>, </span><span>"beans.xml"</span>)<span>;<br/></span><span>    </span>}<br/><br/>    <span>//2<br/></span><span>    </span><span>@CreateSwarm<br/></span><span>    </span><span>public static </span>Swarm <span>createSwarm</span>() <span>throws </span>Exception {<br/>        DatasourcesFraction datasourcesFraction = <span>new </span>DatasourcesFraction()<br/>                <span>//3<br/></span><span>                </span>.jdbcDriver(<span>"h2"</span><span>, </span>(d) -&gt; {<br/>                    d.driverClassName(<span>"org.h2.Driver"</span>)<span>;<br/></span><span>                    </span>d.xaDatasourceClass(<span>"org.h2.jdbcx.JdbcDataSource"</span>)<span>;<br/></span><span>                    </span>d.driverModuleName(<span>"com.h2database.h2"</span>)<span>;<br/></span><span>                </span>})<br/>                .dataSource(<span>"PricingDS"</span><span>, </span>(ds) -&gt; {<br/>                    ds.driverName(<span>"h2"</span>)<span>;<br/></span><span>                    </span>ds.connectionUrl(<span>"jdbc:h2:mem:test;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE"</span>)<span>;<br/></span><span>                    </span>ds.userName(<span>"sa"</span>)<span>;<br/></span><span>                    </span>ds.password(<span>"sa"</span>)<span>;<br/></span><span>                </span>})<span>;<br/></span><span><br/></span><span>        </span>Swarm swarm = <span>new </span>Swarm()<span>;<br/></span><span>        </span>swarm.fraction(datasourcesFraction)<span>;<br/></span><span><br/></span><span>        return </span>swarm<span>;<br/></span><span>    </span>}<br/><br/>    <span>//3<br/></span><span>    </span><span>@Inject<br/></span><span>    </span>PricingService <span>pricingService</span><span>;<br/></span><span><br/></span><span>    </span><span>//4<br/></span><span>    </span><span>@Test<br/></span><span>    </span><span>public void </span><span>testSearchById</span>() {<br/>       Assert.<span>assertEquals</span>(<span>pricingService</span>.findByName(<span>"test-pet"</span>).getPrice()<span>,</span><span>5</span>)<span>;<br/><span>    </span>}<br/>}</span></pre>
<p>Now, we are finally ready to write the testing stage. We will like to make this stage run fast and fail immediately if there are some problems, without creating an image or changing anything in our OpenShift model. For this, we will use standard Maven and git from the command line. </p>
<p>In order to do this, we need to configure those tools. To do this, we will have to go to Jenkins configuration in the main menu, click on <span class="packt_screen">Manage Jenkins</span> and select the tool configuration for <span class="packt_screen">JDK</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fe72f1b1-c6c4-4c95-b002-32250f3c9e9a.png" style=""/></div>
<p>And <span class="packt_screen">Maven</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3c0d6bbc-025d-4dda-8e3b-ae8420bccb40.png" style=""/></div>
<p>We are finally ready to update our <kbd>pipeline</kbd>. Let's take a look:</p>
<pre>pipeline { <br/>//1<br/> agent any<br/>//2 <br/>tools {<br/>    maven 'maven3.5.2'<br/>    jdk 'jdk8u152'<br/>    git 'Default'<br/> }<br/> stages {<br/>//3<br/> stage('Unit tests') {<br/>     steps {<br/>      //4<br/>      git url: 'https://github.com/PacktPublishing/Hands-On-Cloud-Development-with-WildFly.git'<br/> //5<br/> sh 'mvn clean test -Dswarm.build.modules=target/test-classes/modules'<br/> }<br/> }<br/>}</pre>
<p>We have provided the mandatory agent any (1) and configured Maven, JDK, and git tools, providing the versions for all of them. We have replaced our print stage with the unit test stage (3), which consists of the following two steps:</p>
<ol>
<li>The first step clones the <span class="packt_screen">pricing-service</span>'s git repository (4)</li>
<li>The second step runs the Maven tests (5)</li>
</ol>
<div class="packt_tip">We have to provide the modules directory in order for the tests to work.</div>
<p>OK. So, we have our first stage. Now, what next? If the unit tests pass, we will like to build and deploy an image with our application. In order to do that, we will have to interact with our cluster object from within the <kbd>pipeline</kbd>. The tool that will help us do that with ease is the OpenShift Pipeline Plugin. Let's learn more about it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenShift Pipeline Plugin</h1>
                </header>
            
            <article>
                
<p class="mce-root">Jenkins has a pluggable architecture, which allows for plugin development. OpenShift provides its own plugin, which allows for straightforward operations on OpenShift cluster objects in a declarative manner. The plugin provides a number of commands. We will introduce them one by one during the <kbd>pipeline</kbd> development.</p>
<p class="mce-root">In the beginning, we will write a build stage, which will assemble the image and ensure that the application works correctly.</p>
<p>The first command that we will use is the <kbd>openShiftBuild</kbd> command. It allows running one of the builds defined in the OpenShift cluster. This command takes one mandatory parameter, <kbd>buildCfg</kbd>, which is the name of the build that will be executed. </p>
<p>The second command that we will use is verified as <kbd>Build</kbd>. This command also takes <span><kbd>buildCfg</kbd> and checks whether the last build of this type has finished successfully within a reasonable time period. To set the period, we will use the <kbd>waitTime</kbd> parameter.</span></p>
<p>Let's take a look at our new <kbd>pipeline</kbd>:</p>
<pre>pipeline { <br/> agent any<br/> tools {<br/>    maven 'maven3.5.2'<br/>    jdk 'jdk8u152'<br/>    git 'Default'<br/> }<br/> stages {<br/> stage('Test') {<br/>     steps {<br/>      git url: 'https://github.com/PacktPublishing/Hands-On-Cloud-Development-with-WildFly.git'<br/>      sh 'mvn clean install -Dswarm.build.modules=target/test-classes/modules'<br/>     }<br/>    }<br/> //1<br/> stage('Build') {<br/>     steps {<br/>        //2<br/>        openshiftBuild(bldCfg: 'pricing-service', showBuildLogs: 'true')<br/>        //3<br/>        openshiftVerifyBuild(bldCfg: 'pricing-service', waitTime: '300000')<br/>    }<br/> }<br/> }<br/>}</pre>
<p>We have introduced the <kbd>Build</kbd> stage (1) and added two steps to it, as mentioned in the preceding paragraph. The <kbd>Build</kbd> command runs the <kbd>pricing-service</kbd> <kbd>s2i</kbd> build that we configured at the beginning of this chapter (2). The verify command checks whether the build was executed successfully within 5 minutes.</p>
<div class="packt_infobox">We will like to only build the image here and not deploy it yet. So, we will need to modify our build and remove the image change as the trigger for the deployment.</div>
<p>After that, we are ready to start our <kbd>Build</kbd> in Jenkins. If you do it and click on console output, you will be able to see the execution log. Let's take a look at it:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8c9a2cb3-37e8-40a6-8bed-7eebc8a8fabf.png" style=""/></div>
<p>Oops! If you look again at the test, you will note that there is an error, as the price of the test-pet is 5 none 7. Before we fix it, let's note how the <kbd>pipeline</kbd> works. Our first unit test stage failed immediately. As a result, no further stages were started. No images were built and no applications were deployed.  Let's also look at the <kbd>pipeline</kbd> view on the web console:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a83f88ec-9570-4368-a320-41fd7161fd8c.png" style=""/></div>
<p>The console presents the <kbd>pipeline</kbd> execution<span> in a graphic way</span>, showing that the test stage failed. Let's fix our tests and run the application again. If you do it and look at the console log, you will be able to see that the test has passed and the <kbd>Build</kbd> stage has been executed:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e93b5cfa-0a69-4adb-88fc-e5fde152b404.png" style=""/></div>
<p>When you take a look at the web console, you will be able to see that the <kbd>Build</kbd> has been finished and that the image has been created:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a5954008-32cf-4af6-975c-ce9979401a6d.png" style=""/></div>
<p>Let's look at the currently available deployments: </p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/7e93af2d-4331-425c-ab7a-2fa18207df4e.png" style=""/></div>
<p>Now, we <span>only </span>have the build image and have not triggered the deployment yet. Let's add another stage to our build. We will use <kbd>openshiftDeploy</kbd>, <kbd>openshiftScale</kbd>, <kbd>openShiftVerifyDeployment</kbd>, and <kbd>openShiftVerifyService</kbd>. Before doing that, let's introduce each of these commands.</p>
<p>The <kbd>openshiftDeploy</kbd> command takes a mandatory parameter—<kbd>dplCfg</kbd><span>—</span>which is the name of the deployment. It runs the deployment of an application.</p>
<p><kbd>openshiftScale</kbd>, irrespective of a mandatory <kbd>dplCfg</kbd> parameter, takes the <kbd>replicaCount</kbd> parameter, which specifies the number of replicas of the application. Since we are using this command to scale the application, we will change the number of instance deployments in the <kbd>deploymentConfig</kbd> to zero. As a result, the pods will be started only after the <kbd>openshiftScale</kbd> operation has been executed without an unnecessary rescale.</p>
<p><kbd>openShiftVerifyDeployment</kbd> has the same mandatory parameter as the two previous commands—<kbd>dplCfg</kbd>. This command has three optional parameters, and we will use all of them:</p>
<ul>
<li><span><kbd>replicaCount</kbd></span>:<span> This parameter specifies the expected number of replicas</span></li>
<li><span><kbd>verifyReplicaCount</kbd>: This is a Boolean parameter, which specifies whether the replica count should be checked</span></li>
<li><span><kbd>waitTime</kbd></span>:<span> This indicates the time in milliseconds in which we should wait for the verification</span></li>
<li><kbd>openshiftVerifyService</kbd>: This command checks whether the service is available</li>
</ul>
<p><span><kbd>openshiftVerifyService</kbd></span> has one mandatory parameter:</p>
<ul>
<li><kbd>svcName</kbd></li>
<li>One optional parameter <kbd>retryCount</kbd> specifies how many times the connection is attempted before declaring the verification invalid</li>
</ul>
<p>Before showing you the new script, we will introduce one more concept. As we mentioned in the theoretical section of this chapter, the build should give immediate feedback to its authors regarding its status. In order to react to the <kbd>Build</kbd> status, the DPL provides the ability to perform an action after the <kbd>pipeline</kbd> is finished based on the status of the build. The construct that allows doing that is post directive.</p>
<p>A post directive enables us to perform an action after the build has been finished. It can be placed at the end of the <kbd>pipeline</kbd> or at the end of each stage. The post directive provides a bunch of subdirectories: <span>always, success, failure, unstable, (runs if the build is unstable—the result changes during the build) aborted, and changed. </span></p>
<p>In our script, for the sake of simplicity, we will echo the build status to the console, but we can use the available Jenkins plugins0; to configure email, HipChat, or slack notification.</p>
<p>Let's take a look at the build:</p>
<pre>pipeline { <br/> agent any<br/> tools {<br/>    maven 'maven3.5.2'<br/>    jdk 'jdk8u152'<br/>    git 'Default'<br/> }<br/> stages {<br/> stage('Test') {<br/>     steps {<br/>      git url: 'https://github.com/PacktPublishing/Hands-On-Cloud-Development-with-WildFly.git'<br/>      sh 'mvn clean install -Dswarm.build.modules=target/test-classes/modules'<br/>     }<br/>    }<br/> stage('Build') {<br/>     steps {<br/>        openshiftBuild(bldCfg: 'pricing-service', showBuildLogs: 'true')<br/>        openshiftVerifyBuild(bldCfg: 'pricing-service', waitTime: '300000')<br/>    }<br/>    }<br/>//1<br/> stage('Deploy'){<br/>     steps {<br/>         //2<br/>         openshiftDeploy(depCfg: 'pricing-service')<br/>         //3<br/>         openshiftScale(depCfg: 'pricing-service',replicaCount:'3')<br/>         //4<br/>         openshiftVerifyDeployment(depCfg: 'pricing-service',verifyReplicaCount:'true',replicaCount:'3', waitTime: '300000')<br/>         //5<br/>         openshiftVerifyService(svcName: 'pricing-service')<br/>     }<br/> }<br/> }<br/> post {<br/>    //6<br/>    success {<br/>        echo "Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' result: SUCCESS"<br/>    }<br/>    //7<br/>    failure {<br/>        echo "Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' result: FAILURE"<br/>    }<br/> }<br/>}</pre>
<p>We have extended our <kbd>pipeline</kbd> in a way described previously:</p>
<ol>
<li>We have added the <kbd>Deploy</kbd> stage (1), which deploys the application (2)</li>
<li>Then, it scales the application (3)</li>
<li>It verifies that the deployment succeeded (4) and that the service is available (5)</li>
<li>After each build, the result of the test is echoed to the output, depending on whether the test succeeded (6) or failed (7)</li>
</ol>
<p>If you look at the console output, you will be able to see that all the steps that we have implemented have been executed successfully.</p>
<p>You can also verify this in the web console <kbd>pipeline</kbd> view:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/235d9bdc-5bae-453a-9d72-f950ae6a6935.png" style=""/></div>
<p>Finally, you can verify in the web console that the service has indeed been created and that the corresponding pods are running.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned the build infrastructure provided by OpenShift. You then learned to use source-to-image build, which abstracts away the details of Kubernetes internals from the developer and lets them build the image based solely on code with minimal configuration.</p>
<p>In the second part of this chapter, you learned about the <kbd>pipeline</kbd> build, which, effectively, is a way to integrate Jenkins <kbd>pipelines</kbd> with OpenShift infrastructures. You also learned how to create the <kbd>pipeline</kbd> build and the basics of the DPL syntax. Hence, you were able to create a CD <kbd>pipeline</kbd> for your petstore's <kbd>pricing-service.</kbd></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p class="mce-root"><a href="https://jenkins.io/doc/book/pipeline/syntax/"> https://jenkins.io/doc/book/pipeline/syntax/</a></p>


            </article>

            
        </section>
    </body></html>