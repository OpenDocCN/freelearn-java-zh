<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Build Flights Performance Prediction Model</h1>
                </header>
            
            <article>
                
<p>Flight delays and cancellations are travel annoyances. Will a Chicago-bound flight arrive late causing a traveler to miss their connecting flight to Denver? Another traveler at Chicago Airport just learned that their connecting flight to Philly was delayed, perhaps even canceled. If both travelers<span> could predict the odds of their respective experiences actually occurring, travel would get so much better.</span></p>
<p>That said, implementing a f<span>light delay pipeline that can </span>predict outcomes on the lines just described is the overarching learning objective of this chapter. The next section lists all the learning objectives in terms of topics covered in this chapter.</p>
<p><span>All learning objectives in this chapter depend on the following datasets compiled by the United States Department of Transportation. These are flight data, airline carrier data, and flight performance data, respectively.</span></p>
<p><span>Each topic covered in this chapter has specific learning objectives, broken down into two categories:</span></p>
<ul>
<li>Background theory, starting with coverage of the years 2007 and 2008 flights, carrier, and flight performance datasets </li>
<li>A Spark-Scala implementation of a flight delay prediction model</li>
</ul>
<p><span>That said, the immediate learning objective is to gain an understanding of the flight on-time performance dataset for 2007 and 2008. A good place to start is the <em>Flight dataset at a glance</em> section:</span></p>
<ul>
<li>Understanding background theory relevant to understanding flight</li>
<li>Formulating the flights performance problem by applying the background theory</li>
<li>We learn which dataset to pick from the US Department of Transportation website the dataset we pick belong to the years 2007 and 2008</li>
<li>We want to learn what we can from the data, by conducing data exploratory steps</li>
<li>Dividing data into test and training datasets</li>
<li>Implementation of a model in Scala and Spark to predict flights performance</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of flight delay prediction</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will implement a logistic regression-based machine learning model to predict flight delays. This model learns from flight data described in the next section, <em>Flight dataset at a glance</em>. </p>
<p><span>A real-life situation goes like this—travel company T has a new prediction feature in their booking system that is designed to enhance a customer's travel experience. How so? For example, say traveler <em>X</em> wants to get on Southwest flight <em>SW1</em> from origin <em>A</em> (St Louis) to destination <em>C</em> (Denver) with a connection at city <em>B</em> (Chicago). If T's flight booking system could predict the odds of <em>X</em>'s flight arriving late at Chicago, and furthermore the odds of missing the connecting flight as well, <em>X</em> has information at their disposal that lets him or her decide the next course of action. </span></p>
<p><span>With these opening point made, let's take a look at our flight dataset. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The flight dataset at a glance</h1>
                </header>
            
            <article>
                
<p>Data analysis in this chapter relies on a flight dataset, a dataset consisting of the following individual datasets. <span>Download these datasets from the </span><kbd>ModernScalaProjects</kbd> folder:</p>
<ul>
<li><span><kbd>Airports.csv</kbd></span></li>
<li><kbd>AirlineCarriers.csv</kbd></li>
<li><kbd>Flights.csv</kbd></li>
<li><kbd><span>OnTime2007Short.xlsx</span></kbd></li>
<li><kbd><span>OnTime2008Short.xlsx</span></kbd></li>
</ul>
<p>The following screenshot is an overall view of the airports and airline carrier's datasets:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/788e8fd3-2ab7-4026-89e1-ddd8cac17941.jpg" style="width:41.92em;height:38.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The airport and airline dataset</div>
<p>The following table describes the structure of the on-time dataset (<kbd>OnTime2008Short.xlsx</kbd>). It lists all the 28 fields. The table consists of denormalized, semi-structured data:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a6d43da8-0a38-4d61-9ce3-4dcb0c09b318.jpg" style="width:41.33em;height:39.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The OnTime2008Short file dataset</span></div>
<p>The description of the fields are into the following categories:</p>
<ul>
<li><strong>Causes of delay on account of the airline (carrier) (in minutes)</strong>:</li>
<li style="padding-left: 30px"><kbd>FlightCarrierDelay</kbd>: It denotes the delay caused by the carrier</li>
<li style="padding-left: 30px"><kbd>FlightWeatherDelay</kbd>: It denotes the delay caused by weather</li>
<li style="padding-left: 30px"><kbd>FlightNASDelay</kbd>: It denotes the delay caused by the National Air System</li>
<li style="padding-left: 30px"><kbd>FlightSecurityDelay</kbd>: It denotes the delay on account of security checks or other security reasons</li>
<li style="padding-left: 30px"><kbd>FlightLateAircraftDelay</kbd>: It denotes that the aircraft arrives late for reasons other than the preceding causes described </li>
</ul>
<ul>
<li><strong>Flight aircraft data</strong>:</li>
<li style="padding-left: 30px"><kbd>FlightUniqueCarrier</kbd>: A unique two-letter sequence in uppercase, or a o<span>ne-number-one-letter</span> sequence (for example, US, DL, 9E)</li>
</ul>
<p>The section represents a comprehensive overview of the project. To start with, we formulate at a high level the nature of the underlying problem we want to solve. The problem formulation step paves the way for implementation. First, let's formulate the flight delay prediction problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem formulation of flight delay prediction</h1>
                </header>
            
            <article>
                
<p>A high-level description of the problem of flight delays is summed up in one statement—we want to implement a prediction mode that will make predictions on flight delays. In short, a traveler with an itinerary wants to know whether his/her flight or flights are running late.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started</h1>
                </header>
            
            <article>
                
<p>This section starts by laying out the implementation infrastructure for <a href="9608dfbf-246b-4df2-83f6-9e4967b99b0a.xhtml" target="_blank">Chapter 4</a>, <em>Building a Spam Classification Pipeline</em>. The goal of this section will be to get started on developing one data pipeline to analyze the<span> flight-on-time dataset. The first step is to set up </span>prerequisites, before implementation. That is the goal of the next subsection.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up prerequisite software</h1>
                </header>
            
            <article>
                
<p>The following prerequisites or prerequisite checks are recommended. A new prerequisite on this list is MongoDB:</p>
<ul>
<li>Increase Java memory</li>
<li>Review JDK version</li>
<li>Self-contained Scala application based on <strong>Simple Build Tool</strong> (<strong>SBT</strong>), where all dependencies are wired into the <kbd>build.sbt</kbd> file</li>
<li>MongoDB</li>
</ul>
<p>We start by detailing the steps to increase the memory available to the Spark application. Why would we want to do that? This and other points related to Java heap space memory are explored in the following topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Increasing Java memory</h1>
                </header>
            
            <article>
                
<p><span>Flight on-time records, compiled over a period of time, say, month by month, become big or medium data. Processing such volumes of data on a local machine is not trivial. In most cases, a local machine with limited RAM simply won't cut it. </span></p>
<p><span>As challenging as this situation can be, we want to make the best use of our local machine. That brings us to why we want to increase Java memory. For example, trying to process a<br/>
typical one-time dataset file of 27 columns and 509,520 rows, is enough to cause Java<br/>
to run out of memory (see the following screenshot)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/80d225f7-60c9-4119-9b95-05de182cee0d.jpg"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">GC overhead limit exceeded</div>
<p>Firstly, <kbd>java.lang.OutOfMemory</kbd> occurs when the Java VM on your machine tries to go over its threshold memory allocation, as set by the <kbd>-Xmx</kbd> parameter.</p>
<p>The <kbd>-Xmx</kbd> parameter has to do with memory management. It is used to set the maximum Java heap size. From Java 1.8 onwards, the JVM will allocate heap size proportional to the physical memory on the machine</p>
<p>To address this situation, here are a few different ways to increase Java memory: </p>
<ul>
<li>Method 1: On the command line, we pass into SBT the following runtime parameters:</li>
<li style="padding-left: 30px">Maximum allowable heap size</li>
<li style="padding-left: 30px">Java thread stack size</li>
<li style="padding-left: 30px">Initial heap size </li>
</ul>
<ul>
<li>Method 2: Setting maximum Java heap size in the Java control panel.</li>
<li><span>Method 3: Globally setting these parameters in the environment variable, <kbd>JAVA_OPTS</kbd></span>.</li>
</ul>
<p>To address the <kbd>GC Overhead Limit exceeded</kbd> problem illustrated in the preceding screenshot, we can quickly allocate more heap space right on the command line, like this: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ed2792bb-ecf1-4803-ba03-2450ba0eca15.jpg" style="width:41.50em;height:9.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Allocating heap space</div>
<p><span>Note the <kbd>-Xmx2G</kbd> setting. We set the <kbd>SBT_OPTS</kbd> environment variable with the value of <kbd>-Xmx2G</kbd>, the maximum allocated Java heap space memory. We set that and then run SBT.</span></p>
<p>Before we move on to the next method, it might be useful to know the following JVM heap allocation statistics:</p>
<ul>
<li>Total memory</li>
<li>Maximum memory</li>
<li>Free memory</li>
</ul>
<p>This is useful. Heap memory utilization numbers are revealing. The following screenshot shows how to do this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44620101-b200-4a99-9ac6-103abb14e30d.jpg" style="width:30.75em;height:18.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Heap memory</div>
<p>Next up, we will talk about method 2, where we go through the steps to set Java runtime parameters globally.</p>
<p>The following steps apply to Windows machines.</p>
<ol>
<li>Navigate to <span class="packt_screen">Start</span> | <span class="packt_screen">Control Panel</span>, and under <span class="packt_screen">Category</span>, choose <span class="packt_screen">Small icons</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a137b5f1-f4fb-4f1d-b1c1-1c35aedf2b3d.jpg" style="width:43.75em;height:17.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Control Panel</span></div>
<ol start="2">
<li>The ensuing panel allows you to make changes to your computer's settings. The Java setting is one of those. <span>Locate <span class="packt_screen">Java</span> in the <span class="packt_screen">Control Panel</span>:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4f83b333-f01f-48a6-a6a8-6643788465b3.jpg" style="width:44.00em;height:19.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">All Control Panel Items</div>
<ol start="3">
<li>Clicking on <span class="packt_screen">Java</span>, as in the preceding screenshot, will take you to the <span class="packt_screen">Java Control Panel</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/09ee4547-315c-48d0-870b-0ad59cdb8eed.jpg"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Java Control Panel</div>
<ol start="4">
<li>Select the <span class="packt_screen">Java</span> tab results in the <span class="packt_screen">Java Runtime Environment Settings</span> panel, where you may inspect the <span class="packt_screen">Runtime Parameters</span>, such as the Java heap size:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/71434e7d-31c0-463e-aabb-21668310285c.jpg" style="width:37.83em;height:32.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Java Control Panel's User tab</div>
<p style="padding-left: 60px"><span>Referring to the <span class="packt_screen">Java Control Panel</span> representations, we want to set the maximum Java heap size in the <span class="packt_screen">Runtime Parameters</span> box. <kbd>Xmx2048m</kbd> is the new value of the maximum heap space, where</span> <kbd>m</kbd> <span>stands for megabytes. It is easy to modify the value of the <kbd>-Xmx</kbd> parameter. Click on it, then change the value to <kbd>2048</kbd> and click <span class="packt_screen">OK</span>.</span></p>
<div class="packt_infobox">There is no space between <kbd>-Xmx</kbd> and <kbd>2048m</kbd> or <kbd>2 GB</kbd>.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>That's it. Exit the Control Panel:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2264256-f855-49a9-9aa9-933f3d8e6814.jpg" style="width:40.17em;height:35.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Java Control Panel Runtime Parameters</span></div>
<p>Speaking of Java memory management and the settings that are available to help us manage Java memory usage in our Spark application, here is a list of command line options available on running the <kbd>java -X</kbd> command line:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/10a42cc6-32f3-4477-8d18-f9d24b89c9d7.jpg" style="width:43.25em;height:22.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">java -X command line</div>
<p>The preceding screenshot illustrates a comprehensive list of command line options. These options let you tweak different Java environment settings related to memory usage of your JVM-based Spark application. We are interested in the Xmx setting.</p>
<p>We just described method 2, where we outlined how to set the Java runtime parameter, <kbd>-Xmx</kbd>, in the Java Control Panel.</p>
<p>That leaves us with method 3, where we describe how to set three runtime parameters globally. In reference to the preceding screenshot, these are:</p>
<ul>
<li><kbd>-Xmx</kbd>: Sets (or allocates) the size in megabytes that the Java heap space is allowed to grow to. A typical default setting is <kbd>64m</kbd>.</li>
<li><kbd>-Xms</kbd>: Sets the initial Java heap size. The default is 2 MB. </li>
<li><kbd>-Xss</kbd>: Sets the Java thread stack size.</li>
</ul>
<p>We will set these parameters in an environmental variable called <kbd>JAVA_OPTS</kbd>.</p>
<p>The following steps illustrate how to do just this:</p>
<ol>
<li>To start with, we right-click on <span class="packt_screen">This PC</span> and select <span class="packt_screen">Properties</span>: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5810c859-4525-4e59-86de-f7717d9479b8.jpg" style="width:22.00em;height:24.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Properties option tab</span></div>
<ol start="2">
<li>Clicking on <span class="packt_screen">Properties</span> takes us to the following screen:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/990e58e7-3925-4b97-a2cb-bfe84ed9ce96.jpg" style="width:41.17em;height:18.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">System tab</div>
<ol start="3">
<li>Clicking on <span class="packt_screen">Advanced</span> <span class="packt_screen">system settings</span> takes us to the following screen:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f61bc110-b8e4-4e70-860a-27a48517f73c.jpg" style="width:32.00em;height:36.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>System Properties tab</span></div>
<ol start="4">
<li>Click on <span class="packt_screen">Environment Variables...</span> next. In the ensuing screen, we will be able to set <kbd>JAVA_OPTS</kbd>. If <kbd>JAVA_OPTS</kbd> is not present, create a new one. Click on <span class="packt_screen">New</span> and enter the appropriate values in the <span class="packt_screen">Variable name</span> and <span class="packt_screen">Variable value</span> boxes. Dismiss the box by clicking <span class="packt_screen">OK</span>: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e16ed0c-52c5-4dbc-898d-854149cdb9b1.jpg" style="width:40.42em;height:37.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>New System Variable</span></div>
<ol start="5">
<li>Your new <kbd>JAVA_OPTS</kbd> variable is now ready:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4e479571-ecb3-48e1-a3f9-810d316f3c4f.jpg" style="width:41.75em;height:39.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Environment Variables</div>
<p style="padding-left: 60px">In the environment setting we just made, set the <kbd>JAVA_OPTS</kbd> environment variable to the value <kbd>JAVA_OPTS = =Xmx2048M -Xms64M -Xss16M</kbd>.</p>
<p class="mce-root"/>
<p><span>Refer back to the preceding screenshot for a quick refresher on what those settings are.</span></p>
<p>To take stock of all environment variables, launch the Windows PowerShell (there should be a PowerShell app on the desktop). The following is a complete listing of all the environment variables. Note the ones that are relevant:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e83668e4-5e6b-4b19-aed7-db322d4e7926.jpg" style="width:75.08em;height:36.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Hadoop Environment settings</div>
<p>To recap, here is a list of considerations when selecting an <span>appropriate Java (maximum Java heap size):</span></p>
<ul>
<li>Setting maximum heap space, in bytes, and initial heap size, also in bytes. These are appropriate memory allocation pool values that help control the amount of memory usage for our JVM-based Spark application.</li>
<li><span>The <kbd>-Xmx</kbd> option changes the maximum heap space for the VM. Some example settings are </span><kbd>-Xmx2048</kbd>, <kbd>-Xmx81920k</kbd>, and  <kbd>-Xmx1024m</kbd>.</li>
</ul>
<div class="packt_tip packt_infobox"><kbd>-Xmx10G</kbd> is the same as <kbd>-Xmx1024m</kbd> or <kbd>-Xmx1024g</kbd>.</div>
<p class="mce-root"/>
<p>The <kbd>-Xms</kbd> option allows us to set an initial heap size. The default value is 64 MB or 640 KB, for example, <kbd>Xms64m</kbd>. Consider the following:</p>
<ul>
<li>To determine how much can be a higher heap size setting, we recommend increasing the Java heap space to no more than 50% of the total RAM available. For example, if your machine has 32 GB of available RAM, we recommend setting the maximum heap no higher than 16 GB.</li>
<li>Setting the maximum heap space to a value above 16 GB in our example would cause problems with performance.</li>
</ul>
<p>Next, we will review your system JDK.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviewing the JDK version</h1>
                </header>
            
            <article>
                
<p>If you have JDK 8, that is all you need to safely skip this section. If you want to install JDK 9, do not. Spark is incompatible with any JDK version greater than 8. Also, please ensure that you did not install the JDK into a path that has spaces in it. This is a minor detail, but we want to make sure.</p>
<p>In the next section, we get into the installation of MongoDB. We will talk about the why and the how.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MongoDB installation</h1>
                </header>
            
            <article>
                
<p>What is MongoDB and why do we even need it? Firstly, MongoDB's document model makes it easy to map objects in application code to equivalent JSON representations in MongoDB. There is more to this. Spark has good integration with MongoDB. One clear advantage is being able to publish our on-time dataframe into MongoDB as a document. Fetching a dataframe document from MongoDB is good from a performance standpoint too.</p>
<p>There are two prerequisites to installing MongoDB (on Windows):</p>
<ul>
<li>Only 64-bit machines are able to support MongoDB</li>
<li>Be sure to get the latest Windows updates</li>
</ul>
<p>To get started, download the latest stable version of the MongoDB Community Server from the <span class="packt_screen">MongoDB Download Center</span> page on the <a href="https://www.mongodb.com/">mongodb.com</a> website. That will be 4.0. Depending on whichever operating system you have, download the appropriate version. The instructions here are for Windows 10, 64-bit users.</p>
<div class="packt_infobox">The MongoDB product no longer supports 32-bit x86 operating system platforms.</div>
<p>In the next few steps, we will install MongoDB as a service: </p>
<ol>
<li>Click on the MongoDB installer, an MSI file:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9db19f8d-88e0-42b3-aac8-5f6b010d74bd.jpg" style="width:22.67em;height:24.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">MSI file of MongoDB</div>
<ol start="2">
<li>Click<span> </span><span class="packt_screen">Install</span><span>, </span>as shown in the <span>following </span>screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b847d3ed-63ee-4bee-b681-ae039692673d.jpg" style="width:39.25em;height:30.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Install screen of MongoDB</span></div>
<ol start="3">
<li><span>Click <span class="packt_screen">Next</span> and proceed with the complete setup type of installation:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d3fd7825-33b7-4013-b8d4-4ffdc84b47ea.jpg" style="width:39.25em;height:30.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The Next button of MongoDB</div>
<ol start="4">
<li>Click on <span class="packt_screen">Complete</span> and proceed with the complete setup type of installation:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a60e2eda-f2af-40c6-8d32-3c6fcf447d39.jpg" style="width:39.42em;height:30.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The Complete option of MongoDB</div>
<ol start="5">
<li>As already stated, we will choose not to install MongoDB as a service. Therefore, uncheck the <span class="packt_screen">Install MongoDB as a Service</span> option: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/01655a8d-35da-4e57-b947-c6c6a44e7860.jpg" style="width:40.67em;height:31.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Service Configuration MongoDB</span></div>
<p style="padding-left: 90px">Note where you are installing MongoDB into. The server is installed at <kbd>C:\MongoDB\Server\4.0</kbd>. The data folder is at <kbd>C:\MongoDB\Server\4.0\data</kbd>.</p>
<ol start="6">
<li>Next, you will see the screen of the MongoDB Compass:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e997fbf2-b701-4632-bd3a-69e50af17951.jpg" style="width:36.75em;height:28.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Install MongoDB Compass</span></div>
<p>In the next section, we will show you how and why we put MongoDB to work. With the prerequisites out of the way and the application building infrastructure in place, we proceed to the <em>Implementation and deployment</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation and deployment</h1>
                </header>
            
            <article>
                
<p>Implementation depends on setting up the big data infrastructure. Please verify that your MongoDB installation is running properly. Now we shall list implementation objectives as follows:</p>
<ul>
<li>Splitting data into test, train and validation datasets</li>
<li>Data ingestion</li>
<li>Data analysis</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation objectives</h1>
                </header>
            
            <article>
                
<p>The overall objective is to perform data analysis on an on-time flight dataset corresponding to the year 2007-2008. Of the 2007 flight data, 80% will be used as the training dataset and the rest as a validation dataset. In so far as model performance evaluation is concerned, 100% of the 2008 flight data becomes the testing dataset.</p>
<p>The following are the implementation objectives required to implement the flight prediction model:</p>
<ul>
<li>Download the flight dataset.</li>
<li> You may develop the pipeline in four ways:</li>
<li style="padding-left: 30px">Incrementally in your local Spark shell</li>
<li style="padding-left: 30px">By firing up your Horton Sandbox on your h<span>ost machine managed virtual machine, and developing code in a powerful </span>Zeppelin Notebook environment</li>
<li style="padding-left: 30px">Developing everything on the Azure Cloud</li>
<li style="padding-left: 30px">Developing the application as a self-contained SBT application and deploying it to your local Spark cluster using <kbd>spark-submit</kbd></li>
</ul>
<ul>
<li>Flesh out your code in IntelliJ and wire up all the necessary dependencies in the <kbd>build.sbt</kbd> file.</li>
<li>Run the application and interpret the results.</li>
</ul>
<p>In the next subsection, we will document <span>step-by-step instructions for implementing the project. In the succeeding step, we will create a new Scala project in IntelliJ and call it <kbd>Chapter6</kbd>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new Scala project</h1>
                </header>
            
            <article>
                
<p>Let's create<span> </span><span>a Scala project called </span><kbd>Chapter6</kbd>, with the following artifacts:</p>
<ul>
<li><kbd>AirlineWrapper.scala</kbd></li>
<li><kbd>Aircraft.scala</kbd></li>
</ul>
<p>The following screenshot is representative of what our project looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c73be01-6e7d-4cc0-8a80-b4087781895f.jpg" style="width:30.33em;height:25.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">IntelliJ project structure</div>
<p>Let's break down the project structure:</p>
<ul>
<li class="mce-root"><kbd>.idea</kbd>: These are the generated IntelliJ configuration files.</li>
<li class="mce-root"><kbd>project</kbd>: Contains a <kbd>build.properties</kbd> and <kbd>plugins.sbt</kbd>. For example, <kbd>plugins.sbt</kbd> may be used to specify the SBT assembly plugin.</li>
<li class="mce-root"><kbd>src/main/scala</kbd>: A folder that houses Scala source files in the <kbd>com.packt.modern.chapter6</kbd> package.</li>
<li><kbd>src/main/resources</kbd>: Any data or configuration files; for example, a log4j configuration file called <kbd>log4j.xml</kbd>.</li>
<li class="mce-root"><kbd>target</kbd>: This is where artifacts of the compile process are stored. Any generated assembly JAR files go there.</li>
<li class="mce-root"><kbd>build.sbt</kbd>: This is the main SBT configuration file. Spark and its dependencies are specified here.</li>
</ul>
<p>At this point, we will start developing. We start with the <kbd>AirlineWrapper.scala</kbd> file and end with the deployment of the final application JAR into Spark with <kbd>spark-submit</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the AirlineWrapper Scala trait</h1>
                </header>
            
            <article>
                
<p>The <kbd>AirlineWrapper</kbd> contains code to create a <kbd>SparkSession</kbd> instance called <kbd>session</kbd>. It also declares case classes to represent our flights dataset. </p>
<p>Let's create the <kbd>trait</kbd> definition first:</p>
<pre><span>trait </span>AirlineWrapper {  }</pre>
<p>The entry point to programming is as follows: The first thing we do in the <kbd>trait</kbd> is to declare a <kbd>lazy val</kbd> called <kbd>session</kbd>. This is where we lazily create an instance of <kbd>SparkSession</kbd>. Lazily implies that the <kbd>val</kbd> is only executed when it is encountered the first time around. The session is our entry point to programming Spark with the <kbd>DataSet</kbd> and <kbd>DataFrame</kbd> API is <kbd>SparkSession</kbd>:</p>
<pre><span>lazy val </span><span>session </span>= { SparkSession.<span>builder</span>()..getOrCreate() }</pre>
<p>In the following code snippet, <kbd>CarrierCode</kbd> is an identification number assigned by US DoT to identify a unique airline (carrier):</p>
<pre class="mce-root">case class AirlineCarrier(uniqueCarrierCode: String)</pre>
<p class="mce-root">In the following code, <kbd>originOfFlight</kbd> is the origin of the flight (IATA airport code) and <kbd>destOfFlight</kbd> is the destination of the flight (IATA airport code):</p>
<pre class="mce-root"><span>case class Flight(monthOfFlight: Int, /* Number between 1 and 12 */<br/></span>                 dayOfFlight: Int, /*Number between 1 and 31 */<br/>                 uniqueCarrierCode: String,<br/>                 arrDelay: Int, /* Arrival Delay - Field # 15*/<br/>                 depDelay: Int, /* Departure Delay - Field # 16 */<br/>                 originAirportCodeOfFlight: String, /* An identification number assigned by US DOT to identify a unique airport. */<br/>                destAirportCodeOfFlight: String, /* An identification number assigned by US DOT to  identify a unique airport.*/<br/>                carrierDelay: Int, /* Field # 25*/<br/>                weatherDelay: Int, /* Field # 26*/<br/>                lateAircraftDelay: Int /* Field # 29*/<br/>)  </pre>
<p class="mce-root">In the following code snippet, <kbd>iataAirportCode</kbd> is the international airport abbreviation code:</p>
<pre class="mce-root">case class Airports(iataAirportCode: String, airportCity: String, airportCountry: String)</pre>
<p class="mce-root">Load and create a <kbd>File</kbd> object out of the airport's dataset:</p>
<pre class="mce-root">val airportsData: String = loadData("data/airports.csv") </pre>
<p class="mce-root">Load and create a <kbd>File</kbd> object out of the airline carrier dataset:</p>
<pre class="mce-root">val carriersData: String = loadData("data/airlines.csv") </pre>
<p class="mce-root">Create a <kbd>File</kbd> object out of the main FAA dataset:</p>
<pre class="mce-root">val faaFlightsData: String = loadData("data/faa.csv")</pre>
<p class="mce-root">This method takes in a relative path to the data inside the <kbd>resources</kbd> folder of your folder:</p>
<pre class="mce-root">def loadData(dataset: String) = {<br/>                     //Get file from resources folder<br/>                    val classLoader: ClassLoader = getClass.getClassLoader<br/>                    val file: File = new File(classLoader.getResource(dataset).getFile)<br/>                    val filePath = file.getPath<br/>                    println("File path is: " + filePath)<br/>                   filePath<br/> } </pre>
<p class="mce-root">Next, we will write a method called <kbd>buildDataFrame</kbd>:</p>
<pre>import org.apache.spark.sql.SparkSession<br/>import org.apache.spark.SparkConf<br/>import org.apache.spark.ml.linalg.{Vector, Vectors}<br/>import org.apache.spark.rdd.RDD</pre>
<p><span>Remember to update your import statements. The necessary input statements look like the following code. </span>This is all we need to be able to compile all of the code that we developed up until now:</p>
<pre class="mce-root">def buildDataFrame(dataSet: String): RDD[Array[String]] = {<br/>//def getRows2: Array[(org.apache.spark.ml.linalg.Vector, String)] = {<br/>def getRows2: RDD[Array[String]] = {session.sparkContext.<br/>textFile(dataSet).flatMap {<br/>                        partitionLine =&gt;  <br/>                        partitionLine.split("\n").toList<br/>                        }.map(_.split(","))<br/> } <br/>   //Create a dataframe by transforming an Array of a tuple of Feature <br/>   Vectors and the Label<br/>   val dataFrame = session.createDataFrame(getRows2).<br/>   toDF(bcwFeatures_IndexedLabel._1, bcwFeatures_IndexedLabel._2)<br/>   //dataFrame<br/>   //val dataFrame = session.createDataFrame(getRows2)<br/>   getRows2<br/>}</pre>
<p><span>Import the MongoDB packages, including the connector package, in particular:</span></p>
<pre><span>/*<br/></span><span>Import the MongoDB Connector Package<br/></span><span>*/<br/></span><span><br/></span><span>import </span>com.mongodb.spark._<br/><span>import </span>com.mongodb.spark.config._<br/><span>import </span>org.bson.Document</pre>
<p>Create the <kbd>Aircraft</kbd> object:</p>
<pre><span>object </span>Aircraft <span>extends </span>AirlineWrapper {</pre>
<p>Create a <kbd>main</kbd> method inside the <kbd>Aircraft</kbd> object, like this:</p>
<pre><span>def </span>main(args: Array[<span>String</span>]): Unit = {<br/><br/>}</pre>
<p>The <kbd>object</kbd> now looks like this:</p>
<pre><span>object </span>Aircraft <span>extends </span>AirlineWrapper {<br/><span>      def </span>main(args: Array[<span>String</span>]): Unit = {<br/><br/>}<br/><br/>}</pre>
<p>Create a <kbd>case class</kbd> to represent carefully selected features in the dataset that we decide will contribute most to this data analysis:</p>
<pre><span>case class FlightsData</span>(<span>flightYear: String, /* 1 */ <br/>flightMonth : String, /* 2 */ <br/>flightDayOfmonth : String, /* 3 */ <br/>flightDayOfweek : String, /* 4 */ <br/>flightDepTime : String, /* 5 */ <br/>flightCrsDeptime : String, /* 6 */ <br/>flightArrtime : String, /* 7 */ <br/>flightCrsArrTime : String, /* 8 */ <br/>flightUniqueCarrier : String,/* 9 */ <br/>flightNumber : String, /* 10 */ <br/>flightTailNumber : String, /* 11 */ <br/>flightActualElapsedTime : String, /* 12 */ <br/>flightCrsElapsedTime : String, /* 13 */ <br/>flightAirTime : String, /* 14 */ <br/>flightArrDelay : String, /* 15 */ <br/>flightDepDelay : String, /* 16 */ <br/>flightOrigin : String, /* 17 */ <br/>flightDest : String, /* 18 */ <br/>flightDistance : String, /* 19 */ <br/>flightTaxiin : String, /* 20 */<br/>flightTaxiout : String, /* 21 */ <br/>flightCancelled : String, /* 22 */ <br/>flightCancellationCode : String, /* 23 */ <br/>flightDiverted : String, /* 24 */ <br/>flightCarrierDelay : String, /* 25 */ <br/>flightWeatherDelay : String, /* 26 */ <br/>flightNasDelay : String, /* 27 */ <br/>flightSecuritDelay : String, /* 28 */ <br/>flightLateAircraftDelay : String, /* 29 */ record_insertion_time: String, /* 30 */ uuid : String /* 31 */</span><span><br/></span><span>                    </span>)</pre>
<p>Next, create a dataframe to represent the <kbd>FlightData</kbd>:</p>
<pre><span>val airFrame: DataFrame = session.read .format("com.databricks.spark.csv") .option("header", true).option("inferSchema", "true").option("treatEmptyValuesAsNulls", true) .load("2008.csv")</span></pre>
<p>We just loaded the dataset and created a dataframe. Now, we are able to print the schema:</p>
<pre><span>println</span>(<span>"The schema of the raw Airline Dataframe is: "</span>)<br/><span>airFrame.printSchema()</span></pre>
<p class="mce-root"/>
<p>The <kbd>printschema()</kbd> method displays the following schema:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/53042a67-d6a3-4ab1-8562-2c1e5893552b.jpg" style="width:35.67em;height:47.08em;"/></p>
<p><span>We will need a cast on some fields. To call the <kbd>cast</kbd> method, we call in the following import:</span></p>
<pre><span>import </span>org.apache.spark.sql.functions._</pre>
<p>Now, we will create a local temporary view and give it the name <kbd>airline_onTime</kbd>. This temporary view only exists for as long as the lifespan of the <kbd>SparkSession</kbd> that we used to create our dataframe:</p>
<pre>airFrame.createOrReplaceTempView("airline_onTime")</pre>
<p>Run a <kbd>count</kbd> on the number of rows in the dataframe:</p>
<pre>print("size of one-time dataframe is: " + airFrame.count())</pre>
<p>Create a local temporary view using the given name. The lifetime of this temporary view is<br/>
tied to the <kbd>SparkSession</kbd> that was used to create this dataset:</p>
<pre class="mce-root">airFrame.createOrReplaceTempView("airline_ontime")<br/>print("size of one-time dataframe is: " + airFrame.count()) </pre>
<p class="mce-root">Create a local temporary view using the given name. The lifetime of this temporary view is tied to the <kbd>SparkSession</kbd> that was used to create this dataset:</p>
<pre class="mce-root">airFrame.createOrReplaceTempView("airline_ontime") <br/>print("size of one-time dataframe is: " + airFrame.count()) </pre>
<p class="mce-root">Having trimmed and cast our fields and made sure the numeric columns work, we can now save our data as JSON lines and parquet. Call the <kbd>toJSON</kbd> method to return the content of the dataset as a dataset of JSON strings:</p>
<pre class="mce-root">val airFrameJSON: Dataset[String] = clippedAirFrameForDisplay.toJSON</pre>
<p class="mce-root">Display the new dataset in JSON format:</p>
<pre class="mce-root">println("Airline Dataframe as JSON is: ")<br/>airFrameJSON.show(10) </pre>
<p class="mce-root">Save our JSON airline dataframe as a <kbd>.gzip</kbd> JSON file:</p>
<pre class="mce-root">airFrameJSON.rdd.saveAsTextFile("json/airlineOnTimeDataShort.json.gz", classOf[org.apache.hadoop.io.compress.GzipCodec]) </pre>
<p class="mce-root"/>
<p class="mce-root">Next, we need to convert our dataframe to <kbd>parquet</kbd> records. The following code does just that:</p>
<pre class="mce-root">clippedAirFrameForDisplay.write.format("parquet").save("parquet/airlineOnTimeDataShort.parquet")</pre>
<p class="mce-root">Let's read our newly created JSON archive and display the first 20 rows of it:</p>
<pre class="mce-root">val airlineOnTime_Json_Frame: DataFrame = session.read.json("json/airlineOnTimeDataShort.json.gz")<br/>println("JSON version of the Airline dataframe is: ")<br/>airlineOnTime_Json_Frame.show()</pre>
<p class="mce-root">Let's load the <kbd>parquet</kbd> version as well:</p>
<pre class="mce-root">val airlineOnTime_Parquet_Frame: DataFrame = session.read.format("parquet").load("parquet/airlineOnTimeDataShort.parquet") </pre>
<p class="mce-root">Print out the <kbd>parquet</kbd> version of the airline dataframe:</p>
<pre class="mce-root"> println("Parquet version of the Airline dataframe is: ")<br/> airlineOnTime_Parquet_Frame.show(10) </pre>
<p class="mce-root">Next, write to the MongoDB database, <kbd>airlineOnTimeData</kbd>. The call to the <kbd>save</kbd> method produces a <kbd>DataFrameWriter</kbd> that contains a <kbd>.mode</kbd> method; <kbd>mode</kbd> takes in an <kbd>"overwrite"</kbd> parameter. Thus, if the <kbd>collection</kbd> already exists in Mongo, the new records will still be written into the MongoDB database:</p>
<pre class="mce-root">MongoSpark.save( airlineOnTime_Parquet_Frame.write.option("collection", "airlineOnTimeData").mode("overwrite") )</pre>
<p class="mce-root">To confirm that the data was written into MongoDB, launch the MongoDB Compass Community app. In the <span class="packt_screen">Connect to Host</span> opening screen, click on <span class="packt_screen">Connect</span> and in the resulting screen click on database test. The benefit of writing to MongoDB is that, it gives us a easy way to retrieve our data and import it into Spark if something were to corrupt our data <kbd>airlineOnTimeData</kbd> collection.</p>
<p>Finally, submit the application into a Spark local cluster using the <kbd>spark-submit</kbd> command.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we carried out <strong>machine learning</strong> (<strong>ML</strong>) data analysis tasks on flight performance data. One such task is the implementation of a regression model <span>fitted on a training subset of data. </span>Given a new or unknown flight with delayed departure data, this model was able to predict whether the flight under investigation made up for time lost and arrived at the destination on time. One important takeaway from this ML exercise is this—the origin to destination distance contributed most toward predicting time gained. Carrier delays contributed least toward a prediction. A longer flight, it turns out, is able to gain more time. </p>
<p class="mce-root">This chapter provided the foundation to build more sophisticated models. A model with more predictor variables (for example, taking into account, the weather and security delays) could yield deeper, sharper predictions. That said, this chapter hopefully opens a window for opportunity readers to understand how flight performance insights could help travelers <span>snag an optimal travel experience in terms of money and time spent.</span></p>
<p>In the next chapter and the last one, we will develop a recommender system. Get inspired by Amazon's recommendation algorithms and Netflix's ratings system for bringing us relevant movies. The recommendation system that we build will take advantage of all our accumulated skills in Spark ML this far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>Before readers head to the next chapter, we invite readers to attempt an upgrade on the flight performance model. The idea is this—feed in a couple more predictors that enhance the flight delay ML process in a way that makes predictions deeper and more incisive. </p>
<p>Here are a few questions to open further vistas of learning:</p>
<ol>
<li>What is a <kbd>parquet</kbd> file and what are its advantages, especially when a dataset becomes larger, and data shuffling between nodes becomes necessary?</li>
<li>What are the advantages of data compressed in a columnar format?</li>
</ol>
<ol start="3">
<li>Occasionally, you might run into this error: "<kbd>Unable to find encoder stored in Dataset. Primitive types (Int, String, and so on) and Product types (case classes) are supported by importing spark.implicits._</kbd>". How do you get around this error? What is the root cause? Hint—build a simple dataframe with a dataset from the first chapter. Use the <kbd>spark.read</kbd> approach and attempt a <kbd>printSchema</kbd> on it. If that produces the aforementioned error, investigate if it could be that an explicit schema is required</li>
<li>As an alternative to MongoDB, would you rather submit flight performance data to HDFS?</li>
<li>Why did MongoDB prove to be useful in this chapter? </li>
<li>What is semi-structured data?</li>
<li>Name one big benefit of Spark that sets it apart from Hadoop? For example, think programming paradigms.</li>
<li>Can you read in the flight's data from Kafka? If so, how and what might be a reason to do this?</li>
<li>What is data enrichment and how is it related to munging if both the terms are related?</li>
<li>Create a dataframe with two case classes, each with a small subset from the carriers CSV and airports CSV datasets respectively. How would you write this to MongoDB?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>The following article on <em>Introduction to Multivariate Regression Analysis</em> is about the importance of regression analysis: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049417/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049417/</a></p>


            </article>

            
        </section>
    </body></html>